GitDiffStart: 69c6779625319387589d4580db0f824b56c1c719 | Fri Jan 18 15:09:50 2013 +0000
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java
index d0f97aa..a75ee8e 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 import java.util.Iterator;
 
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.SimpleDVConsumer;
+import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentWriteState;
@@ -32,7 +32,7 @@ import org.apache.lucene.util.packed.PackedInts;
 import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
 
 // nocommit fix exception handling (make sure tests find problems first)
-class DiskDocValuesConsumer extends SimpleDVConsumer {
+class DiskDocValuesConsumer extends DocValuesConsumer {
   final IndexOutput data, meta;
   final int maxDoc;
   
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java
index 50f7ce2..a599abb 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java
@@ -19,9 +19,9 @@ package org.apache.lucene.codecs.diskdv;
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.SimpleDVConsumer;
-import org.apache.lucene.codecs.SimpleDVProducer;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 
@@ -37,19 +37,19 @@ import org.apache.lucene.index.SegmentWriteState;
  * fixedLength SortedField = BINARY + NUMERIC (ords)
  * variableLength SortedField = BINARY + NUMERIC (addresses) + NUMERIC (ords) 
  */
-public class DiskDocValuesFormat extends SimpleDocValuesFormat {
+public class DiskDocValuesFormat extends DocValuesFormat {
 
   public DiskDocValuesFormat() {
     super("Disk");
   }
 
   @Override
-  public SimpleDVConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
     return new DiskDocValuesConsumer(state);
   }
 
   @Override
-  public SimpleDVProducer fieldsProducer(SegmentReadState state) throws IOException {
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
     return new DiskDocValuesProducer(state);
   }
   
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java
index 26391f5..b44603d 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java
@@ -22,7 +22,7 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.SimpleDVProducer;
+import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.FieldInfo;
@@ -37,7 +37,7 @@ import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.packed.PackedInts;
 
-class DiskDocValuesProducer extends SimpleDVProducer {
+class DiskDocValuesProducer extends DocValuesProducer {
   private final Map<Integer,NumericEntry> numerics;
   private final Map<Integer,NumericEntry> ords;
   private final Map<Integer,BinaryEntry> binaries;
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesFormat.java
index 74ae8bd..921eda5 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesFormat.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesFormat.java
@@ -19,11 +19,11 @@ package org.apache.lucene.codecs.memory;
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.SimpleDVConsumer;
-import org.apache.lucene.codecs.SimpleDVProducer;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
-import org.apache.lucene.codecs.simpletext.SimpleTextSimpleDocValuesFormat.SimpleTextDocValuesReader;
-import org.apache.lucene.codecs.simpletext.SimpleTextSimpleDocValuesFormat.SimpleTextDocValuesWriter;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.simpletext.SimpleTextDocValuesFormat.SimpleTextDocValuesReader;
+import org.apache.lucene.codecs.simpletext.SimpleTextDocValuesFormat.SimpleTextDocValuesWriter;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.NumericDocValues;
@@ -37,14 +37,14 @@ import org.apache.lucene.util.packed.PackedInts;
  *  search time. */
 
 // nocommit: nuke this wrapper and just make a nice impl for 4.1 (e.g. FST for sortedbytes)
-public class MemoryDocValuesFormat extends SimpleDocValuesFormat {
+public class MemoryDocValuesFormat extends DocValuesFormat {
 
   public MemoryDocValuesFormat() {
     super("Memory");
   }
 
   @Override
-  public SimpleDVConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
     // nocommit use a more efficient format ;):
     return new SimpleTextDocValuesWriter(state, "dat");
   }
@@ -53,11 +53,11 @@ public class MemoryDocValuesFormat extends SimpleDocValuesFormat {
   // per-thread!
   
   @Override
-  public SimpleDVProducer fieldsProducer(SegmentReadState state) throws IOException {
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
     final int maxDoc = state.segmentInfo.getDocCount();
-    final SimpleDVProducer producer = new SimpleTextDocValuesReader(state, "dat");
+    final DocValuesProducer producer = new SimpleTextDocValuesReader(state, "dat");
 
-    return new SimpleDVProducer() {
+    return new DocValuesProducer() {
 
       @Override
       public NumericDocValues getNumeric(FieldInfo field) throws IOException {
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
index 2397f01..0ee84e7 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
@@ -22,8 +22,8 @@ import org.apache.lucene.codecs.FieldInfosFormat;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
-import org.apache.lucene.codecs.SimpleNormsFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 
@@ -39,11 +39,11 @@ public final class SimpleTextCodec extends Codec {
   private final SegmentInfoFormat segmentInfos = new SimpleTextSegmentInfoFormat();
   private final FieldInfosFormat fieldInfosFormat = new SimpleTextFieldInfosFormat();
   private final TermVectorsFormat vectorsFormat = new SimpleTextTermVectorsFormat();
-  private final SimpleNormsFormat simpleNormsFormat = new SimpleTextSimpleNormsFormat();
+  private final NormsFormat simpleNormsFormat = new SimpleTextNormsFormat();
   private final LiveDocsFormat liveDocs = new SimpleTextLiveDocsFormat();
 
   // nocommit rename
-  private final SimpleDocValuesFormat simpleDVFormat = new SimpleTextSimpleDocValuesFormat();
+  private final DocValuesFormat simpleDVFormat = new SimpleTextDocValuesFormat();
   
   public SimpleTextCodec() {
     super("SimpleText");
@@ -75,7 +75,7 @@ public final class SimpleTextCodec extends Codec {
   }
 
   @Override
-  public SimpleNormsFormat simpleNormsFormat() {
+  public NormsFormat normsFormat() {
     return simpleNormsFormat;
   }
   
@@ -85,7 +85,7 @@ public final class SimpleTextCodec extends Codec {
   }
 
   @Override
-  public SimpleDocValuesFormat simpleDocValuesFormat() {
+  public DocValuesFormat docValuesFormat() {
     return simpleDVFormat;
   }
 }
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
new file mode 100644
index 0000000..041e1ff
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
@@ -0,0 +1,641 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.text.DecimalFormat;
+import java.text.DecimalFormatSymbols;
+import java.text.ParseException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+
+
+/**
+ * plain text doc values format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextDocValuesFormat extends DocValuesFormat {
+  final static BytesRef END     = new BytesRef("END");
+  final static BytesRef FIELD   = new BytesRef("field ");
+  // used for numerics
+  final static BytesRef MINVALUE = new BytesRef("  minvalue ");
+  final static BytesRef PATTERN  = new BytesRef("  pattern ");
+  // used for bytes
+  final static BytesRef LENGTH = new BytesRef("length ");
+  final static BytesRef MAXLENGTH = new BytesRef("  maxlength ");
+  // used for sorted bytes
+  final static BytesRef FIXEDLENGTH = new BytesRef("  fixedlength ");
+  final static BytesRef NUMVALUES = new BytesRef("  numvalues ");
+  final static BytesRef ORDPATTERN = new BytesRef("  ordpattern ");
+  
+  public SimpleTextDocValuesFormat() {
+    super("SimpleText");
+  }
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return new SimpleTextDocValuesWriter(state, "dat");
+  }
+
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new SimpleTextDocValuesReader(state, "dat");
+  }
+  
+  /** the .dat file contains the data.
+   *  for numbers this is a "fixed-width" file, for example a single byte range:
+   *  <pre>
+   *  field myField
+   *    minvalue 0
+   *    pattern 000
+   *  005
+   *  234
+   *  123
+   *  ...
+   *  </pre>
+   *  so a document's value (delta encoded from minvalue) can be retrieved by 
+   *  seeking to startOffset + (1+pattern.length())*docid. The extra 1 is the newline.
+   *  
+   *  for bytes this is also a "fixed-width" file, for example:
+   *  <pre>
+   *  field myField
+   *    maxlength 6
+   *    pattern 0
+   *  length 6
+   *  foobar[space][space]
+   *  length 3
+   *  baz[space][space][space][space][space]
+   *  ...
+   *  </pre>
+   *  so a doc's value can be retrieved by seeking to startOffset + (9+pattern.length+maxlength)*doc
+   *  the extra 9 is 2 newlines, plus "length " itself.
+   *  
+   *  for sorted bytes this is a fixed-width file, for example:
+   *  <pre>
+   *  field myField
+   *    numvalues 10
+   *    maxLength 8
+   *    pattern 0
+   *    ordpattern 00
+   *  length 6
+   *  foobar[space][space]
+   *  length 3
+   *  baz[space][space][space][space][space]
+   *  ...
+   *  03
+   *  06
+   *  01
+   *  10
+   *  ...
+   *  </pre>
+   *  so the "ord section" begins at startOffset + (9+pattern.length+maxlength)*numValues.
+   *  a document's ord can be retrieved by seeking to "ord section" + (1+ordpattern.length())*docid
+   *  an ord's value can be retrieved by seeking to startOffset + (9+pattern.length+maxlength)*ord
+   *   
+   *  the reader can just scan this file when it opens, skipping over the data blocks
+   *  and saving the offset/etc for each field. 
+   */
+  // nocommit not public
+  public static class SimpleTextDocValuesWriter extends DocValuesConsumer {
+    final IndexOutput data;
+    final BytesRef scratch = new BytesRef();
+    final int numDocs;
+    // nocommit
+    final boolean isNorms;
+    private final Set<String> fieldsSeen = new HashSet<String>(); // for asserting
+    
+    public SimpleTextDocValuesWriter(SegmentWriteState state, String ext) throws IOException {
+      //System.out.println("WRITE: " + IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext) + " " + state.segmentInfo.getDocCount() + " docs");
+      data = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext), state.context);
+      numDocs = state.segmentInfo.getDocCount();
+      isNorms = ext.equals("slen");
+    }
+
+    // for asserting
+    private boolean fieldSeen(String field) {
+      assert !fieldsSeen.contains(field): "field \"" + field + "\" was added more than once during flush";
+      fieldsSeen.add(field);
+      return true;
+    }
+
+    @Override
+    public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+      assert fieldSeen(field.name);
+      // nocommit: this must be multiple asserts
+      //assert (field.getDocValuesType() != null && (DocValues.isNumber(field.getDocValuesType()) || DocValues.isFloat(field.getDocValuesType()))) ||
+      //  (field.getNormType() != null && (DocValues.isNumber(field.getNormType()) || DocValues.isFloat(field.getNormType()))): "field=" + field.name;
+      writeFieldEntry(field);
+
+      // first pass to find min/max
+      long minValue = Long.MAX_VALUE;
+      long maxValue = Long.MIN_VALUE;
+      for(Number n : values) {
+        long v = n.longValue();
+        minValue = Math.min(minValue, v);
+        maxValue = Math.max(maxValue, v);
+      }
+      
+      // write our minimum value to the .dat, all entries are deltas from that
+      SimpleTextUtil.write(data, MINVALUE);
+      SimpleTextUtil.write(data, Long.toString(minValue), scratch);
+      SimpleTextUtil.writeNewline(data);
+      
+      // build up our fixed-width "simple text packed ints"
+      // format
+      BigInteger maxBig = BigInteger.valueOf(maxValue);
+      BigInteger minBig = BigInteger.valueOf(minValue);
+      BigInteger diffBig = maxBig.subtract(minBig);
+      int maxBytesPerValue = diffBig.toString().length();
+      StringBuilder sb = new StringBuilder();
+      for (int i = 0; i < maxBytesPerValue; i++) {
+        sb.append('0');
+      }
+      
+      // write our pattern to the .dat
+      SimpleTextUtil.write(data, PATTERN);
+      SimpleTextUtil.write(data, sb.toString(), scratch);
+      SimpleTextUtil.writeNewline(data);
+
+      final String patternString = sb.toString();
+      
+      final DecimalFormat encoder = new DecimalFormat(patternString, new DecimalFormatSymbols(Locale.ROOT));
+      
+      int numDocsWritten = 0;
+
+      // second pass to write the values
+      for(Number n : values) {
+        long value = n.longValue();
+        assert value >= minValue;
+        Number delta = BigInteger.valueOf(value).subtract(BigInteger.valueOf(minValue));
+        String s = encoder.format(delta);
+        assert s.length() == patternString.length();
+        SimpleTextUtil.write(data, s, scratch);
+        SimpleTextUtil.writeNewline(data);
+        numDocsWritten++;
+        assert numDocsWritten <= numDocs;
+      }
+
+      assert numDocs == numDocsWritten: "numDocs=" + numDocs + " numDocsWritten=" + numDocsWritten;
+    }
+
+    @Override
+    public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
+      assert fieldSeen(field.name);
+      assert field.getDocValuesType() == DocValuesType.BINARY;
+      assert !isNorms;
+      int maxLength = 0;
+      for(BytesRef value : values) {
+        maxLength = Math.max(maxLength, value.length);
+      }
+      writeFieldEntry(field);
+
+      // write maxLength
+      SimpleTextUtil.write(data, MAXLENGTH);
+      SimpleTextUtil.write(data, Integer.toString(maxLength), scratch);
+      SimpleTextUtil.writeNewline(data);
+      
+      int maxBytesLength = Long.toString(maxLength).length();
+      StringBuilder sb = new StringBuilder();
+      for (int i = 0; i < maxBytesLength; i++) {
+        sb.append('0');
+      }
+      // write our pattern for encoding lengths
+      SimpleTextUtil.write(data, PATTERN);
+      SimpleTextUtil.write(data, sb.toString(), scratch);
+      SimpleTextUtil.writeNewline(data);
+      final DecimalFormat encoder = new DecimalFormat(sb.toString(), new DecimalFormatSymbols(Locale.ROOT));
+
+      int numDocsWritten = 0;
+      for(BytesRef value : values) {
+        // write length
+        SimpleTextUtil.write(data, LENGTH);
+        SimpleTextUtil.write(data, encoder.format(value.length), scratch);
+        SimpleTextUtil.writeNewline(data);
+          
+        // write bytes -- don't use SimpleText.write
+        // because it escapes:
+        data.writeBytes(value.bytes, value.offset, value.length);
+
+        // pad to fit
+        for (int i = value.length; i < maxLength; i++) {
+          data.writeByte((byte)' ');
+        }
+        SimpleTextUtil.writeNewline(data);
+        numDocsWritten++;
+      }
+
+      assert numDocs == numDocsWritten;
+    }
+    
+    @Override
+    public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+      assert fieldSeen(field.name);
+      assert field.getDocValuesType() == DocValuesType.SORTED;
+      assert !isNorms;
+      writeFieldEntry(field);
+
+      int valueCount = 0;
+      int maxLength = -1;
+      for(BytesRef value : values) {
+        maxLength = Math.max(maxLength, value.length);
+        valueCount++;
+      }
+
+      // write numValues
+      SimpleTextUtil.write(data, NUMVALUES);
+      SimpleTextUtil.write(data, Integer.toString(valueCount), scratch);
+      SimpleTextUtil.writeNewline(data);
+      
+      // write maxLength
+      SimpleTextUtil.write(data, MAXLENGTH);
+      SimpleTextUtil.write(data, Integer.toString(maxLength), scratch);
+      SimpleTextUtil.writeNewline(data);
+      
+      int maxBytesLength = Integer.toString(maxLength).length();
+      StringBuilder sb = new StringBuilder();
+      for (int i = 0; i < maxBytesLength; i++) {
+        sb.append('0');
+      }
+      
+      // write our pattern for encoding lengths
+      SimpleTextUtil.write(data, PATTERN);
+      SimpleTextUtil.write(data, sb.toString(), scratch);
+      SimpleTextUtil.writeNewline(data);
+      final DecimalFormat encoder = new DecimalFormat(sb.toString(), new DecimalFormatSymbols(Locale.ROOT));
+      
+      int maxOrdBytes = Integer.toString(valueCount).length();
+      sb.setLength(0);
+      for (int i = 0; i < maxOrdBytes; i++) {
+        sb.append('0');
+      }
+      
+      // write our pattern for ords
+      SimpleTextUtil.write(data, ORDPATTERN);
+      SimpleTextUtil.write(data, sb.toString(), scratch);
+      SimpleTextUtil.writeNewline(data);
+      final DecimalFormat ordEncoder = new DecimalFormat(sb.toString(), new DecimalFormatSymbols(Locale.ROOT));
+
+      // for asserts:
+      int valuesSeen = 0;
+
+      for(BytesRef value : values) {
+        // write length
+        SimpleTextUtil.write(data, LENGTH);
+        SimpleTextUtil.write(data, encoder.format(value.length), scratch);
+        SimpleTextUtil.writeNewline(data);
+          
+        // write bytes -- don't use SimpleText.write
+        // because it escapes:
+        data.writeBytes(value.bytes, value.offset, value.length);
+
+        // pad to fit
+        for (int i = value.length; i < maxLength; i++) {
+          data.writeByte((byte)' ');
+        }
+        SimpleTextUtil.writeNewline(data);
+        valuesSeen++;
+        assert valuesSeen <= valueCount;
+      }
+
+      assert valuesSeen == valueCount;
+
+      for(Number ord : docToOrd) {
+        SimpleTextUtil.write(data, ordEncoder.format(ord.intValue()), scratch);
+        SimpleTextUtil.writeNewline(data);
+      }
+    }
+
+    /** write the header for this field */
+    private void writeFieldEntry(FieldInfo field) throws IOException {
+      SimpleTextUtil.write(data, FIELD);
+      SimpleTextUtil.write(data, field.name, scratch);
+      SimpleTextUtil.writeNewline(data);
+    }
+    
+    @Override
+    public void close() throws IOException {
+      boolean success = false;
+      try {
+        assert !fieldsSeen.isEmpty();
+        // TODO: sheisty to do this here?
+        SimpleTextUtil.write(data, END);
+        SimpleTextUtil.writeNewline(data);
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(data);
+        } else {
+          IOUtils.closeWhileHandlingException(data);
+        }
+      }
+    }
+  };
+
+  // nocommit once we do "in ram cache of direct source"
+  // ... and hopeuflly under SCR control ... then if app
+  // asks for direct soruce but it was already cached in ram
+  // ... we should use the ram cached one!  we don't do this
+  // correctly today ...
+
+  // nocommit make sure we test "all docs have 0 value",
+  // "all docs have empty BytesREf"
+
+  // nocommit not public
+  public static class SimpleTextDocValuesReader extends DocValuesProducer {
+
+    static class OneField {
+      FieldInfo fieldInfo;
+      long dataStartFilePointer;
+      String pattern;
+      String ordPattern;
+      int maxLength;
+      boolean fixedLength;
+      long minValue;
+      int numValues;
+    };
+
+    final int maxDoc;
+    final IndexInput data;
+    final BytesRef scratch = new BytesRef();
+    final Map<String,OneField> fields = new HashMap<String,OneField>();
+    
+    public SimpleTextDocValuesReader(SegmentReadState state, String ext) throws IOException {
+      //System.out.println("dir=" + state.directory + " seg=" + state.segmentInfo.name + " ext=" + ext);
+      data = state.directory.openInput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext), state.context);
+      maxDoc = state.segmentInfo.getDocCount();
+      while(true) {
+        readLine();
+        //System.out.println("READ field=" + scratch.utf8ToString());
+        if (scratch.equals(END)) {
+          break;
+        }
+        assert startsWith(FIELD) : scratch.utf8ToString();
+        String fieldName = stripPrefix(FIELD);
+        //System.out.println("  field=" + fieldName);
+        FieldInfo fieldInfo = state.fieldInfos.fieldInfo(fieldName);
+        assert fieldInfo != null;
+
+        OneField field = new OneField();
+        fields.put(fieldName, field);
+
+        field.fieldInfo = fieldInfo;
+        //System.out.println("  field=" + fieldName);
+
+        // nocommit hack hack hack!!:
+        DocValuesType dvType = ext.equals("slen") ? DocValuesType.NUMERIC : fieldInfo.getDocValuesType();
+        assert dvType != null;
+        if (dvType == DocValuesType.NUMERIC) {
+          readLine();
+          assert startsWith(MINVALUE): "got " + scratch.utf8ToString() + " field=" + fieldName + " ext=" + ext;
+          field.minValue = Long.parseLong(stripPrefix(MINVALUE));
+          readLine();
+          assert startsWith(PATTERN);
+          field.pattern = stripPrefix(PATTERN);
+          field.dataStartFilePointer = data.getFilePointer();
+          data.seek(data.getFilePointer() + (1+field.pattern.length()) * maxDoc);
+        } else if (dvType == DocValuesType.BINARY) {
+          readLine();
+          assert startsWith(MAXLENGTH);
+          field.maxLength = Integer.parseInt(stripPrefix(MAXLENGTH));
+          readLine();
+          assert startsWith(PATTERN);
+          field.pattern = stripPrefix(PATTERN);
+          field.dataStartFilePointer = data.getFilePointer();
+          data.seek(data.getFilePointer() + (9+field.pattern.length()+field.maxLength) * maxDoc);
+        } else if (dvType == DocValuesType.SORTED) {
+          readLine();
+          assert startsWith(NUMVALUES);
+          field.numValues = Integer.parseInt(stripPrefix(NUMVALUES));
+          readLine();
+          assert startsWith(MAXLENGTH);
+          field.maxLength = Integer.parseInt(stripPrefix(MAXLENGTH));
+          readLine();
+          assert startsWith(PATTERN);
+          field.pattern = stripPrefix(PATTERN);
+          readLine();
+          assert startsWith(ORDPATTERN);
+          field.ordPattern = stripPrefix(ORDPATTERN);
+          field.dataStartFilePointer = data.getFilePointer();
+          data.seek(data.getFilePointer() + (9+field.pattern.length()+field.maxLength) * field.numValues + (1+field.ordPattern.length())*maxDoc);
+        } else {
+          throw new AssertionError();
+        }
+      }
+
+      // We should only be called from above if at least one
+      // field has DVs:
+      assert !fields.isEmpty();
+    }
+
+    @Override
+    public NumericDocValues getNumeric(FieldInfo fieldInfo) throws IOException {
+      final OneField field = fields.get(fieldInfo.name);
+      assert field != null;
+
+      // SegmentCoreReaders already verifies this field is
+      // valid:
+      assert field != null: "field=" + fieldInfo.name + " fields=" + fields;
+
+      final IndexInput in = data.clone();
+      final BytesRef scratch = new BytesRef();
+      final DecimalFormat decoder = new DecimalFormat(field.pattern, new DecimalFormatSymbols(Locale.ROOT));
+
+      decoder.setParseBigDecimal(true);
+
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          try {
+            //System.out.println(Thread.currentThread().getName() + ": get docID=" + docID + " in=" + in);
+            if (docID < 0 || docID >= maxDoc) {
+              throw new IndexOutOfBoundsException("docID must be 0 .. " + (maxDoc-1) + "; got " + docID);
+            }
+            in.seek(field.dataStartFilePointer + (1+field.pattern.length())*docID);
+            SimpleTextUtil.readLine(in, scratch);
+            //System.out.println("parsing delta: " + scratch.utf8ToString());
+            BigDecimal bd;
+            try {
+              bd = (BigDecimal) decoder.parse(scratch.utf8ToString());
+            } catch (ParseException pe) {
+              CorruptIndexException e = new CorruptIndexException("failed to parse BigDecimal value");
+              e.initCause(pe);
+              throw e;
+            }
+            return BigInteger.valueOf(field.minValue).add(bd.toBigIntegerExact()).longValue();
+          } catch (IOException ioe) {
+            throw new RuntimeException(ioe);
+          }
+        }
+      };
+    }
+
+    @Override
+    public BinaryDocValues getBinary(FieldInfo fieldInfo) throws IOException {
+      final OneField field = fields.get(fieldInfo.name);
+
+      // SegmentCoreReaders already verifies this field is
+      // valid:
+      assert field != null;
+
+      final IndexInput in = data.clone();
+      final BytesRef scratch = new BytesRef();
+      final DecimalFormat decoder = new DecimalFormat(field.pattern, new DecimalFormatSymbols(Locale.ROOT));
+
+      return new BinaryDocValues() {
+        @Override
+        public void get(int docID, BytesRef result) {
+          try {
+            if (docID < 0 || docID >= maxDoc) {
+              throw new IndexOutOfBoundsException("docID must be 0 .. " + (maxDoc-1) + "; got " + docID);
+            }
+            in.seek(field.dataStartFilePointer + (9+field.pattern.length() + field.maxLength)*docID);
+            SimpleTextUtil.readLine(in, scratch);
+            assert StringHelper.startsWith(scratch, LENGTH);
+            int len;
+            try {
+              len = decoder.parse(new String(scratch.bytes, scratch.offset + LENGTH.length, scratch.length - LENGTH.length, "UTF-8")).intValue();
+            } catch (ParseException pe) {
+              CorruptIndexException e = new CorruptIndexException("failed to parse int length");
+              e.initCause(pe);
+              throw e;
+            }
+            result.bytes = new byte[len];
+            result.offset = 0;
+            result.length = len;
+            in.readBytes(result.bytes, 0, len);
+          } catch (IOException ioe) {
+            throw new RuntimeException(ioe);
+          }
+        }
+      };
+    }
+
+    @Override
+    public SortedDocValues getSorted(FieldInfo fieldInfo) throws IOException {
+      final OneField field = fields.get(fieldInfo.name);
+
+      // SegmentCoreReaders already verifies this field is
+      // valid:
+      assert field != null;
+
+      final IndexInput in = data.clone();
+      final BytesRef scratch = new BytesRef();
+      final DecimalFormat decoder = new DecimalFormat(field.pattern, new DecimalFormatSymbols(Locale.ROOT));
+      final DecimalFormat ordDecoder = new DecimalFormat(field.ordPattern, new DecimalFormatSymbols(Locale.ROOT));
+
+      return new SortedDocValues() {
+        @Override
+        public int getOrd(int docID) {
+          if (docID < 0 || docID >= maxDoc) {
+            throw new IndexOutOfBoundsException("docID must be 0 .. " + (maxDoc-1) + "; got " + docID);
+          }
+          try {
+            in.seek(field.dataStartFilePointer + field.numValues * (9 + field.pattern.length() + field.maxLength) + docID * (1 + field.ordPattern.length()));
+            SimpleTextUtil.readLine(in, scratch);
+            try {
+              return ordDecoder.parse(scratch.utf8ToString()).intValue();
+            } catch (ParseException pe) {
+              CorruptIndexException e = new CorruptIndexException("failed to parse ord");
+              e.initCause(pe);
+              throw e;
+            }
+          } catch (IOException ioe) {
+            throw new RuntimeException(ioe);
+          }
+        }
+
+        @Override
+        public void lookupOrd(int ord, BytesRef result) {
+          try {
+            if (ord < 0 || ord >= field.numValues) {
+              throw new IndexOutOfBoundsException("ord must be 0 .. " + (field.numValues-1) + "; got " + ord);
+            }
+            in.seek(field.dataStartFilePointer + ord * (9 + field.pattern.length() + field.maxLength));
+            SimpleTextUtil.readLine(in, scratch);
+            assert StringHelper.startsWith(scratch, LENGTH): "got " + scratch.utf8ToString() + " in=" + in;
+            int len;
+            try {
+              len = decoder.parse(new String(scratch.bytes, scratch.offset + LENGTH.length, scratch.length - LENGTH.length, "UTF-8")).intValue();
+            } catch (ParseException pe) {
+              CorruptIndexException e = new CorruptIndexException("failed to parse int length");
+              e.initCause(pe);
+              throw e;
+            }
+            result.bytes = new byte[len];
+            result.offset = 0;
+            result.length = len;
+            in.readBytes(result.bytes, 0, len);
+          } catch (IOException ioe) {
+            throw new RuntimeException(ioe);
+          }
+        }
+
+        @Override
+        public int getValueCount() {
+          return field.numValues;
+        }
+      };
+    }
+
+    @Override
+    public void close() throws IOException {
+      data.close();
+    }
+
+    /** Used only in ctor: */
+    private void readLine() throws IOException {
+      SimpleTextUtil.readLine(data, scratch);
+      //System.out.println("line: " + scratch.utf8ToString());
+    }
+
+    /** Used only in ctor: */
+    private boolean startsWith(BytesRef prefix) {
+      return StringHelper.startsWith(scratch, prefix);
+    }
+
+    /** Used only in ctor: */
+    private String stripPrefix(BytesRef prefix) throws IOException {
+      return new String(scratch.bytes, scratch.offset + prefix.length, scratch.length - prefix.length, "UTF-8");
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java
new file mode 100644
index 0000000..ecb537a
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java
@@ -0,0 +1,85 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.simpletext.SimpleTextDocValuesFormat.SimpleTextDocValuesReader;
+import org.apache.lucene.codecs.simpletext.SimpleTextDocValuesFormat.SimpleTextDocValuesWriter;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * plain-text norms format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * 
+ * @lucene.experimental
+ */
+public class SimpleTextNormsFormat extends NormsFormat {
+  // nocommit put back to len once we replace current norms format:
+  private static final String NORMS_SEG_EXTENSION = "slen";
+  
+  @Override
+  public DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    return new SimpleTextSimpleNormsConsumer(state);
+  }
+  
+  @Override
+  public DocValuesProducer normsProducer(SegmentReadState state) throws IOException {
+    return new SimpleTextSimpleNormsProducer(state);
+  }
+  
+  /**
+   * Reads plain-text norms.
+   * <p>
+   * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+   * 
+   * @lucene.experimental
+   */
+  public static class SimpleTextSimpleNormsProducer extends SimpleTextDocValuesReader {
+    public SimpleTextSimpleNormsProducer(SegmentReadState state) throws IOException {
+      // All we do is change the extension from .dat -> .len;
+      // otherwise this is a normal simple doc values file:
+      super(state, NORMS_SEG_EXTENSION);
+    }
+  }
+  
+  /**
+   * Writes plain-text norms.
+   * <p>
+   * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+   * 
+   * @lucene.experimental
+   */
+  public static class SimpleTextSimpleNormsConsumer extends SimpleTextDocValuesWriter {
+    public SimpleTextSimpleNormsConsumer(SegmentWriteState state) throws IOException {
+      // All we do is change the extension from .dat -> .len;
+      // otherwise this is a normal simple doc values file:
+      super(state, NORMS_SEG_EXTENSION);
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSimpleDocValuesFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSimpleDocValuesFormat.java
deleted file mode 100644
index a638855..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSimpleDocValuesFormat.java
+++ /dev/null
@@ -1,641 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.math.BigDecimal;
-import java.math.BigInteger;
-import java.text.DecimalFormat;
-import java.text.DecimalFormatSymbols;
-import java.text.ParseException;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.codecs.SimpleDVConsumer;
-import org.apache.lucene.codecs.SimpleDVProducer;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-
-/**
- * plain text doc values format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextSimpleDocValuesFormat extends SimpleDocValuesFormat {
-  final static BytesRef END     = new BytesRef("END");
-  final static BytesRef FIELD   = new BytesRef("field ");
-  // used for numerics
-  final static BytesRef MINVALUE = new BytesRef("  minvalue ");
-  final static BytesRef PATTERN  = new BytesRef("  pattern ");
-  // used for bytes
-  final static BytesRef LENGTH = new BytesRef("length ");
-  final static BytesRef MAXLENGTH = new BytesRef("  maxlength ");
-  // used for sorted bytes
-  final static BytesRef FIXEDLENGTH = new BytesRef("  fixedlength ");
-  final static BytesRef NUMVALUES = new BytesRef("  numvalues ");
-  final static BytesRef ORDPATTERN = new BytesRef("  ordpattern ");
-  
-  public SimpleTextSimpleDocValuesFormat() {
-    super("SimpleText");
-  }
-
-  @Override
-  public SimpleDVConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new SimpleTextDocValuesWriter(state, "dat");
-  }
-
-  @Override
-  public SimpleDVProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new SimpleTextDocValuesReader(state, "dat");
-  }
-  
-  /** the .dat file contains the data.
-   *  for numbers this is a "fixed-width" file, for example a single byte range:
-   *  <pre>
-   *  field myField
-   *    minvalue 0
-   *    pattern 000
-   *  005
-   *  234
-   *  123
-   *  ...
-   *  </pre>
-   *  so a document's value (delta encoded from minvalue) can be retrieved by 
-   *  seeking to startOffset + (1+pattern.length())*docid. The extra 1 is the newline.
-   *  
-   *  for bytes this is also a "fixed-width" file, for example:
-   *  <pre>
-   *  field myField
-   *    maxlength 6
-   *    pattern 0
-   *  length 6
-   *  foobar[space][space]
-   *  length 3
-   *  baz[space][space][space][space][space]
-   *  ...
-   *  </pre>
-   *  so a doc's value can be retrieved by seeking to startOffset + (9+pattern.length+maxlength)*doc
-   *  the extra 9 is 2 newlines, plus "length " itself.
-   *  
-   *  for sorted bytes this is a fixed-width file, for example:
-   *  <pre>
-   *  field myField
-   *    numvalues 10
-   *    maxLength 8
-   *    pattern 0
-   *    ordpattern 00
-   *  length 6
-   *  foobar[space][space]
-   *  length 3
-   *  baz[space][space][space][space][space]
-   *  ...
-   *  03
-   *  06
-   *  01
-   *  10
-   *  ...
-   *  </pre>
-   *  so the "ord section" begins at startOffset + (9+pattern.length+maxlength)*numValues.
-   *  a document's ord can be retrieved by seeking to "ord section" + (1+ordpattern.length())*docid
-   *  an ord's value can be retrieved by seeking to startOffset + (9+pattern.length+maxlength)*ord
-   *   
-   *  the reader can just scan this file when it opens, skipping over the data blocks
-   *  and saving the offset/etc for each field. 
-   */
-  // nocommit not public
-  public static class SimpleTextDocValuesWriter extends SimpleDVConsumer {
-    final IndexOutput data;
-    final BytesRef scratch = new BytesRef();
-    final int numDocs;
-    // nocommit
-    final boolean isNorms;
-    private final Set<String> fieldsSeen = new HashSet<String>(); // for asserting
-    
-    public SimpleTextDocValuesWriter(SegmentWriteState state, String ext) throws IOException {
-      //System.out.println("WRITE: " + IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext) + " " + state.segmentInfo.getDocCount() + " docs");
-      data = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext), state.context);
-      numDocs = state.segmentInfo.getDocCount();
-      isNorms = ext.equals("slen");
-    }
-
-    // for asserting
-    private boolean fieldSeen(String field) {
-      assert !fieldsSeen.contains(field): "field \"" + field + "\" was added more than once during flush";
-      fieldsSeen.add(field);
-      return true;
-    }
-
-    @Override
-    public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
-      assert fieldSeen(field.name);
-      // nocommit: this must be multiple asserts
-      //assert (field.getDocValuesType() != null && (DocValues.isNumber(field.getDocValuesType()) || DocValues.isFloat(field.getDocValuesType()))) ||
-      //  (field.getNormType() != null && (DocValues.isNumber(field.getNormType()) || DocValues.isFloat(field.getNormType()))): "field=" + field.name;
-      writeFieldEntry(field);
-
-      // first pass to find min/max
-      long minValue = Long.MAX_VALUE;
-      long maxValue = Long.MIN_VALUE;
-      for(Number n : values) {
-        long v = n.longValue();
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-      }
-      
-      // write our minimum value to the .dat, all entries are deltas from that
-      SimpleTextUtil.write(data, MINVALUE);
-      SimpleTextUtil.write(data, Long.toString(minValue), scratch);
-      SimpleTextUtil.writeNewline(data);
-      
-      // build up our fixed-width "simple text packed ints"
-      // format
-      BigInteger maxBig = BigInteger.valueOf(maxValue);
-      BigInteger minBig = BigInteger.valueOf(minValue);
-      BigInteger diffBig = maxBig.subtract(minBig);
-      int maxBytesPerValue = diffBig.toString().length();
-      StringBuilder sb = new StringBuilder();
-      for (int i = 0; i < maxBytesPerValue; i++) {
-        sb.append('0');
-      }
-      
-      // write our pattern to the .dat
-      SimpleTextUtil.write(data, PATTERN);
-      SimpleTextUtil.write(data, sb.toString(), scratch);
-      SimpleTextUtil.writeNewline(data);
-
-      final String patternString = sb.toString();
-      
-      final DecimalFormat encoder = new DecimalFormat(patternString, new DecimalFormatSymbols(Locale.ROOT));
-      
-      int numDocsWritten = 0;
-
-      // second pass to write the values
-      for(Number n : values) {
-        long value = n.longValue();
-        assert value >= minValue;
-        Number delta = BigInteger.valueOf(value).subtract(BigInteger.valueOf(minValue));
-        String s = encoder.format(delta);
-        assert s.length() == patternString.length();
-        SimpleTextUtil.write(data, s, scratch);
-        SimpleTextUtil.writeNewline(data);
-        numDocsWritten++;
-        assert numDocsWritten <= numDocs;
-      }
-
-      assert numDocs == numDocsWritten: "numDocs=" + numDocs + " numDocsWritten=" + numDocsWritten;
-    }
-
-    @Override
-    public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
-      assert fieldSeen(field.name);
-      assert field.getDocValuesType() == DocValuesType.BINARY;
-      assert !isNorms;
-      int maxLength = 0;
-      for(BytesRef value : values) {
-        maxLength = Math.max(maxLength, value.length);
-      }
-      writeFieldEntry(field);
-
-      // write maxLength
-      SimpleTextUtil.write(data, MAXLENGTH);
-      SimpleTextUtil.write(data, Integer.toString(maxLength), scratch);
-      SimpleTextUtil.writeNewline(data);
-      
-      int maxBytesLength = Long.toString(maxLength).length();
-      StringBuilder sb = new StringBuilder();
-      for (int i = 0; i < maxBytesLength; i++) {
-        sb.append('0');
-      }
-      // write our pattern for encoding lengths
-      SimpleTextUtil.write(data, PATTERN);
-      SimpleTextUtil.write(data, sb.toString(), scratch);
-      SimpleTextUtil.writeNewline(data);
-      final DecimalFormat encoder = new DecimalFormat(sb.toString(), new DecimalFormatSymbols(Locale.ROOT));
-
-      int numDocsWritten = 0;
-      for(BytesRef value : values) {
-        // write length
-        SimpleTextUtil.write(data, LENGTH);
-        SimpleTextUtil.write(data, encoder.format(value.length), scratch);
-        SimpleTextUtil.writeNewline(data);
-          
-        // write bytes -- don't use SimpleText.write
-        // because it escapes:
-        data.writeBytes(value.bytes, value.offset, value.length);
-
-        // pad to fit
-        for (int i = value.length; i < maxLength; i++) {
-          data.writeByte((byte)' ');
-        }
-        SimpleTextUtil.writeNewline(data);
-        numDocsWritten++;
-      }
-
-      assert numDocs == numDocsWritten;
-    }
-    
-    @Override
-    public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-      assert fieldSeen(field.name);
-      assert field.getDocValuesType() == DocValuesType.SORTED;
-      assert !isNorms;
-      writeFieldEntry(field);
-
-      int valueCount = 0;
-      int maxLength = -1;
-      for(BytesRef value : values) {
-        maxLength = Math.max(maxLength, value.length);
-        valueCount++;
-      }
-
-      // write numValues
-      SimpleTextUtil.write(data, NUMVALUES);
-      SimpleTextUtil.write(data, Integer.toString(valueCount), scratch);
-      SimpleTextUtil.writeNewline(data);
-      
-      // write maxLength
-      SimpleTextUtil.write(data, MAXLENGTH);
-      SimpleTextUtil.write(data, Integer.toString(maxLength), scratch);
-      SimpleTextUtil.writeNewline(data);
-      
-      int maxBytesLength = Integer.toString(maxLength).length();
-      StringBuilder sb = new StringBuilder();
-      for (int i = 0; i < maxBytesLength; i++) {
-        sb.append('0');
-      }
-      
-      // write our pattern for encoding lengths
-      SimpleTextUtil.write(data, PATTERN);
-      SimpleTextUtil.write(data, sb.toString(), scratch);
-      SimpleTextUtil.writeNewline(data);
-      final DecimalFormat encoder = new DecimalFormat(sb.toString(), new DecimalFormatSymbols(Locale.ROOT));
-      
-      int maxOrdBytes = Integer.toString(valueCount).length();
-      sb.setLength(0);
-      for (int i = 0; i < maxOrdBytes; i++) {
-        sb.append('0');
-      }
-      
-      // write our pattern for ords
-      SimpleTextUtil.write(data, ORDPATTERN);
-      SimpleTextUtil.write(data, sb.toString(), scratch);
-      SimpleTextUtil.writeNewline(data);
-      final DecimalFormat ordEncoder = new DecimalFormat(sb.toString(), new DecimalFormatSymbols(Locale.ROOT));
-
-      // for asserts:
-      int valuesSeen = 0;
-
-      for(BytesRef value : values) {
-        // write length
-        SimpleTextUtil.write(data, LENGTH);
-        SimpleTextUtil.write(data, encoder.format(value.length), scratch);
-        SimpleTextUtil.writeNewline(data);
-          
-        // write bytes -- don't use SimpleText.write
-        // because it escapes:
-        data.writeBytes(value.bytes, value.offset, value.length);
-
-        // pad to fit
-        for (int i = value.length; i < maxLength; i++) {
-          data.writeByte((byte)' ');
-        }
-        SimpleTextUtil.writeNewline(data);
-        valuesSeen++;
-        assert valuesSeen <= valueCount;
-      }
-
-      assert valuesSeen == valueCount;
-
-      for(Number ord : docToOrd) {
-        SimpleTextUtil.write(data, ordEncoder.format(ord.intValue()), scratch);
-        SimpleTextUtil.writeNewline(data);
-      }
-    }
-
-    /** write the header for this field */
-    private void writeFieldEntry(FieldInfo field) throws IOException {
-      SimpleTextUtil.write(data, FIELD);
-      SimpleTextUtil.write(data, field.name, scratch);
-      SimpleTextUtil.writeNewline(data);
-    }
-    
-    @Override
-    public void close() throws IOException {
-      boolean success = false;
-      try {
-        assert !fieldsSeen.isEmpty();
-        // TODO: sheisty to do this here?
-        SimpleTextUtil.write(data, END);
-        SimpleTextUtil.writeNewline(data);
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(data);
-        } else {
-          IOUtils.closeWhileHandlingException(data);
-        }
-      }
-    }
-  };
-
-  // nocommit once we do "in ram cache of direct source"
-  // ... and hopeuflly under SCR control ... then if app
-  // asks for direct soruce but it was already cached in ram
-  // ... we should use the ram cached one!  we don't do this
-  // correctly today ...
-
-  // nocommit make sure we test "all docs have 0 value",
-  // "all docs have empty BytesREf"
-
-  // nocommit not public
-  public static class SimpleTextDocValuesReader extends SimpleDVProducer {
-
-    static class OneField {
-      FieldInfo fieldInfo;
-      long dataStartFilePointer;
-      String pattern;
-      String ordPattern;
-      int maxLength;
-      boolean fixedLength;
-      long minValue;
-      int numValues;
-    };
-
-    final int maxDoc;
-    final IndexInput data;
-    final BytesRef scratch = new BytesRef();
-    final Map<String,OneField> fields = new HashMap<String,OneField>();
-    
-    public SimpleTextDocValuesReader(SegmentReadState state, String ext) throws IOException {
-      //System.out.println("dir=" + state.directory + " seg=" + state.segmentInfo.name + " ext=" + ext);
-      data = state.directory.openInput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, ext), state.context);
-      maxDoc = state.segmentInfo.getDocCount();
-      while(true) {
-        readLine();
-        //System.out.println("READ field=" + scratch.utf8ToString());
-        if (scratch.equals(END)) {
-          break;
-        }
-        assert startsWith(FIELD) : scratch.utf8ToString();
-        String fieldName = stripPrefix(FIELD);
-        //System.out.println("  field=" + fieldName);
-        FieldInfo fieldInfo = state.fieldInfos.fieldInfo(fieldName);
-        assert fieldInfo != null;
-
-        OneField field = new OneField();
-        fields.put(fieldName, field);
-
-        field.fieldInfo = fieldInfo;
-        //System.out.println("  field=" + fieldName);
-
-        // nocommit hack hack hack!!:
-        DocValuesType dvType = ext.equals("slen") ? DocValuesType.NUMERIC : fieldInfo.getDocValuesType();
-        assert dvType != null;
-        if (dvType == DocValuesType.NUMERIC) {
-          readLine();
-          assert startsWith(MINVALUE): "got " + scratch.utf8ToString() + " field=" + fieldName + " ext=" + ext;
-          field.minValue = Long.parseLong(stripPrefix(MINVALUE));
-          readLine();
-          assert startsWith(PATTERN);
-          field.pattern = stripPrefix(PATTERN);
-          field.dataStartFilePointer = data.getFilePointer();
-          data.seek(data.getFilePointer() + (1+field.pattern.length()) * maxDoc);
-        } else if (dvType == DocValuesType.BINARY) {
-          readLine();
-          assert startsWith(MAXLENGTH);
-          field.maxLength = Integer.parseInt(stripPrefix(MAXLENGTH));
-          readLine();
-          assert startsWith(PATTERN);
-          field.pattern = stripPrefix(PATTERN);
-          field.dataStartFilePointer = data.getFilePointer();
-          data.seek(data.getFilePointer() + (9+field.pattern.length()+field.maxLength) * maxDoc);
-        } else if (dvType == DocValuesType.SORTED) {
-          readLine();
-          assert startsWith(NUMVALUES);
-          field.numValues = Integer.parseInt(stripPrefix(NUMVALUES));
-          readLine();
-          assert startsWith(MAXLENGTH);
-          field.maxLength = Integer.parseInt(stripPrefix(MAXLENGTH));
-          readLine();
-          assert startsWith(PATTERN);
-          field.pattern = stripPrefix(PATTERN);
-          readLine();
-          assert startsWith(ORDPATTERN);
-          field.ordPattern = stripPrefix(ORDPATTERN);
-          field.dataStartFilePointer = data.getFilePointer();
-          data.seek(data.getFilePointer() + (9+field.pattern.length()+field.maxLength) * field.numValues + (1+field.ordPattern.length())*maxDoc);
-        } else {
-          throw new AssertionError();
-        }
-      }
-
-      // We should only be called from above if at least one
-      // field has DVs:
-      assert !fields.isEmpty();
-    }
-
-    @Override
-    public NumericDocValues getNumeric(FieldInfo fieldInfo) throws IOException {
-      final OneField field = fields.get(fieldInfo.name);
-      assert field != null;
-
-      // SegmentCoreReaders already verifies this field is
-      // valid:
-      assert field != null: "field=" + fieldInfo.name + " fields=" + fields;
-
-      final IndexInput in = data.clone();
-      final BytesRef scratch = new BytesRef();
-      final DecimalFormat decoder = new DecimalFormat(field.pattern, new DecimalFormatSymbols(Locale.ROOT));
-
-      decoder.setParseBigDecimal(true);
-
-      return new NumericDocValues() {
-        @Override
-        public long get(int docID) {
-          try {
-            //System.out.println(Thread.currentThread().getName() + ": get docID=" + docID + " in=" + in);
-            if (docID < 0 || docID >= maxDoc) {
-              throw new IndexOutOfBoundsException("docID must be 0 .. " + (maxDoc-1) + "; got " + docID);
-            }
-            in.seek(field.dataStartFilePointer + (1+field.pattern.length())*docID);
-            SimpleTextUtil.readLine(in, scratch);
-            //System.out.println("parsing delta: " + scratch.utf8ToString());
-            BigDecimal bd;
-            try {
-              bd = (BigDecimal) decoder.parse(scratch.utf8ToString());
-            } catch (ParseException pe) {
-              CorruptIndexException e = new CorruptIndexException("failed to parse BigDecimal value");
-              e.initCause(pe);
-              throw e;
-            }
-            return BigInteger.valueOf(field.minValue).add(bd.toBigIntegerExact()).longValue();
-          } catch (IOException ioe) {
-            throw new RuntimeException(ioe);
-          }
-        }
-      };
-    }
-
-    @Override
-    public BinaryDocValues getBinary(FieldInfo fieldInfo) throws IOException {
-      final OneField field = fields.get(fieldInfo.name);
-
-      // SegmentCoreReaders already verifies this field is
-      // valid:
-      assert field != null;
-
-      final IndexInput in = data.clone();
-      final BytesRef scratch = new BytesRef();
-      final DecimalFormat decoder = new DecimalFormat(field.pattern, new DecimalFormatSymbols(Locale.ROOT));
-
-      return new BinaryDocValues() {
-        @Override
-        public void get(int docID, BytesRef result) {
-          try {
-            if (docID < 0 || docID >= maxDoc) {
-              throw new IndexOutOfBoundsException("docID must be 0 .. " + (maxDoc-1) + "; got " + docID);
-            }
-            in.seek(field.dataStartFilePointer + (9+field.pattern.length() + field.maxLength)*docID);
-            SimpleTextUtil.readLine(in, scratch);
-            assert StringHelper.startsWith(scratch, LENGTH);
-            int len;
-            try {
-              len = decoder.parse(new String(scratch.bytes, scratch.offset + LENGTH.length, scratch.length - LENGTH.length, "UTF-8")).intValue();
-            } catch (ParseException pe) {
-              CorruptIndexException e = new CorruptIndexException("failed to parse int length");
-              e.initCause(pe);
-              throw e;
-            }
-            result.bytes = new byte[len];
-            result.offset = 0;
-            result.length = len;
-            in.readBytes(result.bytes, 0, len);
-          } catch (IOException ioe) {
-            throw new RuntimeException(ioe);
-          }
-        }
-      };
-    }
-
-    @Override
-    public SortedDocValues getSorted(FieldInfo fieldInfo) throws IOException {
-      final OneField field = fields.get(fieldInfo.name);
-
-      // SegmentCoreReaders already verifies this field is
-      // valid:
-      assert field != null;
-
-      final IndexInput in = data.clone();
-      final BytesRef scratch = new BytesRef();
-      final DecimalFormat decoder = new DecimalFormat(field.pattern, new DecimalFormatSymbols(Locale.ROOT));
-      final DecimalFormat ordDecoder = new DecimalFormat(field.ordPattern, new DecimalFormatSymbols(Locale.ROOT));
-
-      return new SortedDocValues() {
-        @Override
-        public int getOrd(int docID) {
-          if (docID < 0 || docID >= maxDoc) {
-            throw new IndexOutOfBoundsException("docID must be 0 .. " + (maxDoc-1) + "; got " + docID);
-          }
-          try {
-            in.seek(field.dataStartFilePointer + field.numValues * (9 + field.pattern.length() + field.maxLength) + docID * (1 + field.ordPattern.length()));
-            SimpleTextUtil.readLine(in, scratch);
-            try {
-              return ordDecoder.parse(scratch.utf8ToString()).intValue();
-            } catch (ParseException pe) {
-              CorruptIndexException e = new CorruptIndexException("failed to parse ord");
-              e.initCause(pe);
-              throw e;
-            }
-          } catch (IOException ioe) {
-            throw new RuntimeException(ioe);
-          }
-        }
-
-        @Override
-        public void lookupOrd(int ord, BytesRef result) {
-          try {
-            if (ord < 0 || ord >= field.numValues) {
-              throw new IndexOutOfBoundsException("ord must be 0 .. " + (field.numValues-1) + "; got " + ord);
-            }
-            in.seek(field.dataStartFilePointer + ord * (9 + field.pattern.length() + field.maxLength));
-            SimpleTextUtil.readLine(in, scratch);
-            assert StringHelper.startsWith(scratch, LENGTH): "got " + scratch.utf8ToString() + " in=" + in;
-            int len;
-            try {
-              len = decoder.parse(new String(scratch.bytes, scratch.offset + LENGTH.length, scratch.length - LENGTH.length, "UTF-8")).intValue();
-            } catch (ParseException pe) {
-              CorruptIndexException e = new CorruptIndexException("failed to parse int length");
-              e.initCause(pe);
-              throw e;
-            }
-            result.bytes = new byte[len];
-            result.offset = 0;
-            result.length = len;
-            in.readBytes(result.bytes, 0, len);
-          } catch (IOException ioe) {
-            throw new RuntimeException(ioe);
-          }
-        }
-
-        @Override
-        public int getValueCount() {
-          return field.numValues;
-        }
-      };
-    }
-
-    @Override
-    public void close() throws IOException {
-      data.close();
-    }
-
-    /** Used only in ctor: */
-    private void readLine() throws IOException {
-      SimpleTextUtil.readLine(data, scratch);
-      //System.out.println("line: " + scratch.utf8ToString());
-    }
-
-    /** Used only in ctor: */
-    private boolean startsWith(BytesRef prefix) {
-      return StringHelper.startsWith(scratch, prefix);
-    }
-
-    /** Used only in ctor: */
-    private String stripPrefix(BytesRef prefix) throws IOException {
-      return new String(scratch.bytes, scratch.offset + prefix.length, scratch.length - prefix.length, "UTF-8");
-    }
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSimpleNormsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSimpleNormsFormat.java
deleted file mode 100644
index 10816e7..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSimpleNormsFormat.java
+++ /dev/null
@@ -1,85 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.codecs.SimpleDVConsumer;
-import org.apache.lucene.codecs.SimpleDVProducer;
-import org.apache.lucene.codecs.SimpleNormsFormat;
-import org.apache.lucene.codecs.simpletext.SimpleTextSimpleDocValuesFormat.SimpleTextDocValuesReader;
-import org.apache.lucene.codecs.simpletext.SimpleTextSimpleDocValuesFormat.SimpleTextDocValuesWriter;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * plain-text norms format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * 
- * @lucene.experimental
- */
-public class SimpleTextSimpleNormsFormat extends SimpleNormsFormat {
-  // nocommit put back to len once we replace current norms format:
-  private static final String NORMS_SEG_EXTENSION = "slen";
-  
-  @Override
-  public SimpleDVConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    return new SimpleTextSimpleNormsConsumer(state);
-  }
-  
-  @Override
-  public SimpleDVProducer normsProducer(SegmentReadState state) throws IOException {
-    return new SimpleTextSimpleNormsProducer(state);
-  }
-  
-  /**
-   * Reads plain-text norms.
-   * <p>
-   * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
-   * 
-   * @lucene.experimental
-   */
-  public static class SimpleTextSimpleNormsProducer extends SimpleTextDocValuesReader {
-    public SimpleTextSimpleNormsProducer(SegmentReadState state) throws IOException {
-      // All we do is change the extension from .dat -> .len;
-      // otherwise this is a normal simple doc values file:
-      super(state, NORMS_SEG_EXTENSION);
-    }
-  }
-  
-  /**
-   * Writes plain-text norms.
-   * <p>
-   * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
-   * 
-   * @lucene.experimental
-   */
-  public static class SimpleTextSimpleNormsConsumer extends SimpleTextDocValuesWriter {
-    public SimpleTextSimpleNormsConsumer(SegmentWriteState state) throws IOException {
-      // All we do is change the extension from .dat -> .len;
-      // otherwise this is a normal simple doc values file:
-      super(state, NORMS_SEG_EXTENSION);
-    }
-  }
-}
diff --git a/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat b/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
new file mode 100644
index 0000000..5103c52
--- /dev/null
+++ b/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
@@ -0,0 +1,18 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.diskdv.DiskDocValuesFormat
+org.apache.lucene.codecs.memory.MemoryDocValuesFormat
+org.apache.lucene.codecs.simpletext.SimpleTextDocValuesFormat
diff --git a/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.SimpleDocValuesFormat b/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.SimpleDocValuesFormat
deleted file mode 100644
index 2b9372d..0000000
--- a/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.SimpleDocValuesFormat
+++ /dev/null
@@ -1,18 +0,0 @@
-#  Licensed to the Apache Software Foundation (ASF) under one or more
-#  contributor license agreements.  See the NOTICE file distributed with
-#  this work for additional information regarding copyright ownership.
-#  The ASF licenses this file to You under the Apache License, Version 2.0
-#  (the "License"); you may not use this file except in compliance with
-#  the License.  You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#  Unless required by applicable law or agreed to in writing, software
-#  distributed under the License is distributed on an "AS IS" BASIS,
-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#  See the License for the specific language governing permissions and
-#  limitations under the License.
-
-org.apache.lucene.codecs.diskdv.DiskDocValuesFormat
-org.apache.lucene.codecs.memory.MemoryDocValuesFormat
-org.apache.lucene.codecs.simpletext.SimpleTextSimpleDocValuesFormat
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/Codec.java
index 694b9bb..c187ced 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/Codec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/Codec.java
@@ -66,7 +66,7 @@ public abstract class Codec implements NamedSPILoader.NamedSPI {
   public abstract PostingsFormat postingsFormat();
 
   /** Encodes/decodes docvalues */
-  public abstract SimpleDocValuesFormat simpleDocValuesFormat();
+  public abstract DocValuesFormat docValuesFormat();
   
   /** Encodes/decodes stored fields */
   public abstract StoredFieldsFormat storedFieldsFormat();
@@ -81,7 +81,7 @@ public abstract class Codec implements NamedSPILoader.NamedSPI {
   public abstract SegmentInfoFormat segmentInfoFormat();
   
   /** Encodes/decodes document normalization values */
-  public abstract SimpleNormsFormat simpleNormsFormat();
+  public abstract NormsFormat normsFormat();
 
   /** Encodes/decodes live docs */
   public abstract LiveDocsFormat liveDocsFormat();
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
new file mode 100644
index 0000000..839aa02
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
@@ -0,0 +1,423 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.NoSuchElementException;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.PriorityQueue;
+
+// prototype streaming DV api
+public abstract class DocValuesConsumer implements Closeable {
+  // TODO: are any of these params too "infringing" on codec?
+  // we want codec to get necessary stuff from IW, but trading off against merge complexity.
+
+  // nocommit should we pass SegmentWriteState...?
+  public abstract void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException;    
+
+  public abstract void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException;
+
+  public abstract void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException;
+
+  // dead simple impl: codec can optimize
+  public void mergeNumericField(FieldInfo fieldInfo, final MergeState mergeState, final List<NumericDocValues> toMerge) throws IOException {
+
+    addNumericField(fieldInfo,
+                    new Iterable<Number>() {
+                      @Override
+                      public Iterator<Number> iterator() {
+                        return new Iterator<Number>() {
+                          int readerUpto = -1;
+                          int docIDUpto;
+                          long nextValue;
+                          AtomicReader currentReader;
+                          NumericDocValues currentValues;
+                          Bits currentLiveDocs;
+                          boolean nextIsSet;
+
+                          @Override
+                          public boolean hasNext() {
+                            return nextIsSet || setNext();
+                          }
+
+                          @Override
+                          public void remove() {
+                            throw new UnsupportedOperationException();
+                          }
+
+                          @Override
+                          public Number next() {
+                            if (!hasNext()) {
+                              throw new NoSuchElementException();
+                            }
+                            assert nextIsSet;
+                            nextIsSet = false;
+                            // nocommit make a mutable number
+                            return nextValue;
+                          }
+
+                          private boolean setNext() {
+                            while (true) {
+                              if (readerUpto == toMerge.size()) {
+                                return false;
+                              }
+
+                              if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                                readerUpto++;
+                                if (readerUpto < toMerge.size()) {
+                                  currentReader = mergeState.readers.get(readerUpto);
+                                  currentValues = toMerge.get(readerUpto);
+                                  currentLiveDocs = currentReader.getLiveDocs();
+                                }
+                                docIDUpto = 0;
+                                continue;
+                              }
+
+                              if (currentLiveDocs == null || currentLiveDocs.get(docIDUpto)) {
+                                nextIsSet = true;
+                                nextValue = currentValues.get(docIDUpto);
+                                docIDUpto++;
+                                return true;
+                              }
+
+                              docIDUpto++;
+                            }
+                          }
+                        };
+                      }
+                    });
+  }
+  
+  // dead simple impl: codec can optimize
+  public void mergeBinaryField(FieldInfo fieldInfo, final MergeState mergeState, final List<BinaryDocValues> toMerge) throws IOException {
+
+    addBinaryField(fieldInfo,
+                   new Iterable<BytesRef>() {
+                     @Override
+                     public Iterator<BytesRef> iterator() {
+                       return new Iterator<BytesRef>() {
+                         int readerUpto = -1;
+                         int docIDUpto;
+                         BytesRef nextValue = new BytesRef();
+                         AtomicReader currentReader;
+                         BinaryDocValues currentValues;
+                         Bits currentLiveDocs;
+                         boolean nextIsSet;
+
+                         @Override
+                         public boolean hasNext() {
+                           return nextIsSet || setNext();
+                         }
+
+                         @Override
+                         public void remove() {
+                           throw new UnsupportedOperationException();
+                         }
+
+                         @Override
+                         public BytesRef next() {
+                           if (!hasNext()) {
+                             throw new NoSuchElementException();
+                           }
+                           assert nextIsSet;
+                           nextIsSet = false;
+                           // nocommit make a mutable number
+                           return nextValue;
+                         }
+
+                         private boolean setNext() {
+                           while (true) {
+                             if (readerUpto == toMerge.size()) {
+                               return false;
+                             }
+
+                             if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
+                               readerUpto++;
+                               if (readerUpto < toMerge.size()) {
+                                 currentReader = mergeState.readers.get(readerUpto);
+                                 currentValues = toMerge.get(readerUpto);
+                                 currentLiveDocs = currentReader.getLiveDocs();
+                               }
+                               docIDUpto = 0;
+                               continue;
+                             }
+
+                             if (currentLiveDocs == null || currentLiveDocs.get(docIDUpto)) {
+                               nextIsSet = true;
+                               currentValues.get(docIDUpto, nextValue);
+                               docIDUpto++;
+                               return true;
+                             }
+
+                             docIDUpto++;
+                           }
+                         }
+                       };
+                     }
+                   });
+  }
+
+  public static class SortedBytesMerger {
+
+    public int numMergedTerms;
+
+    final List<BytesRef> mergedTerms = new ArrayList<BytesRef>();
+    final List<SegmentState> segStates = new ArrayList<SegmentState>();
+
+    private static class SegmentState {
+      AtomicReader reader;
+      FixedBitSet liveTerms;
+      int ord = -1;
+      SortedDocValues values;
+      BytesRef scratch = new BytesRef();
+
+      // nocommit can we factor out the compressed fields
+      // compression?  ie we have a good idea "roughly" what
+      // the ord should be (linear projection) so we only
+      // need to encode the delta from that ...:        
+      int[] segOrdToMergedOrd;
+
+      public BytesRef nextTerm() {
+        while (ord < values.getValueCount()-1) {
+          ord++;
+          if (liveTerms == null || liveTerms.get(ord)) {
+            values.lookupOrd(ord, scratch);
+            return scratch;
+          } else {
+            // Skip "deleted" terms (ie, terms that were not
+            // referenced by any live docs):
+            values.lookupOrd(ord, scratch);
+          }
+        }
+
+        return null;
+      }
+    }
+
+    private static class TermMergeQueue extends PriorityQueue<SegmentState> {
+      public TermMergeQueue(int maxSize) {
+        super(maxSize);
+      }
+
+      @Override
+      protected boolean lessThan(SegmentState a, SegmentState b) {
+        return a.scratch.compareTo(b.scratch) <= 0;
+      }
+    }
+
+    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {
+
+      // First pass: mark "live" terms
+      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {
+        AtomicReader reader = mergeState.readers.get(readerIDX);      
+        // nocommit what if this is null...?  need default source?
+        int maxDoc = reader.maxDoc();
+
+        SegmentState state = new SegmentState();
+        state.reader = reader;
+        state.values = toMerge.get(readerIDX);
+
+        segStates.add(state);
+        assert state.values.getValueCount() < Integer.MAX_VALUE;
+        if (reader.hasDeletions()) {
+          state.liveTerms = new FixedBitSet(state.values.getValueCount());
+          Bits liveDocs = reader.getLiveDocs();
+          assert liveDocs != null;
+          for(int docID=0;docID<maxDoc;docID++) {
+            if (liveDocs.get(docID)) {
+              state.liveTerms.set(state.values.getOrd(docID));
+            }
+          }
+        }
+
+        // nocommit we can unload the bits to disk to reduce
+        // transient ram spike...
+      }
+
+      // Second pass: merge only the live terms
+
+      TermMergeQueue q = new TermMergeQueue(segStates.size());
+      for(SegmentState segState : segStates) {
+        if (segState.nextTerm() != null) {
+
+          // nocommit we could defer this to 3rd pass (and
+          // reduce transient RAM spike) but then
+          // we'd spend more effort computing the mapping...:
+          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];
+          q.add(segState);
+        }
+      }
+
+      BytesRef lastTerm = null;
+      int ord = 0;
+      while (q.size() != 0) {
+        SegmentState top = q.top();
+        if (lastTerm == null || !lastTerm.equals(top.scratch)) {
+          lastTerm = BytesRef.deepCopyOf(top.scratch);
+          // nocommit we could spill this to disk instead of
+          // RAM, and replay on finish...
+          mergedTerms.add(lastTerm);
+          ord++;
+        }
+
+        top.segOrdToMergedOrd[top.ord] = ord-1;
+        if (top.nextTerm() == null) {
+          q.pop();
+        } else {
+          q.updateTop();
+        }
+      }
+
+      numMergedTerms = ord;
+    }
+
+    /*
+    public void finish(SortedDocValuesConsumer consumer) throws IOException {
+
+      // Third pass: write merged result
+      for(BytesRef term : mergedTerms) {
+        consumer.addValue(term);
+      }
+
+      for(SegmentState segState : segStates) {
+        Bits liveDocs = segState.reader.getLiveDocs();
+        int maxDoc = segState.reader.maxDoc();
+        for(int docID=0;docID<maxDoc;docID++) {
+          if (liveDocs == null || liveDocs.get(docID)) {
+            int segOrd = segState.values.getOrd(docID);
+            int mergedOrd = segState.segOrdToMergedOrd[segOrd];
+            consumer.addDoc(mergedOrd);
+          }
+        }
+      }
+    }
+    */
+  }
+
+  public void mergeSortedField(FieldInfo fieldInfo, final MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {
+    final SortedBytesMerger merger = new SortedBytesMerger();
+
+    // Does the heavy lifting to merge sort all "live" ords:
+    merger.merge(mergeState, toMerge);
+
+    addSortedField(fieldInfo,
+
+                   // ord -> value
+                   new Iterable<BytesRef>() {
+                     @Override
+                     public Iterator<BytesRef> iterator() {
+                       return new Iterator<BytesRef>() {
+                         int ordUpto;
+
+                         @Override
+                         public boolean hasNext() {
+                           return ordUpto < merger.mergedTerms.size();
+                         }
+
+                         @Override
+                         public void remove() {
+                           throw new UnsupportedOperationException();
+                         }
+
+                         @Override
+                         public BytesRef next() {
+                           return merger.mergedTerms.get(ordUpto++);
+                         }
+                       };
+                     }
+                   },
+
+                   // doc -> ord
+                    new Iterable<Number>() {
+                      @Override
+                      public Iterator<Number> iterator() {
+                        return new Iterator<Number>() {
+                          int readerUpto = -1;
+                          int docIDUpto;
+                          int nextValue;
+                          SortedBytesMerger.SegmentState currentReader;
+                          Bits currentLiveDocs;
+                          boolean nextIsSet;
+
+                          @Override
+                          public boolean hasNext() {
+                            return nextIsSet || setNext();
+                          }
+
+                          @Override
+                          public void remove() {
+                            throw new UnsupportedOperationException();
+                          }
+
+                          @Override
+                          public Number next() {
+                            if (!hasNext()) {
+                              throw new NoSuchElementException();
+                            }
+                            assert nextIsSet;
+                            nextIsSet = false;
+                            // nocommit make a mutable number
+                            return nextValue;
+                          }
+
+                          private boolean setNext() {
+                            while (true) {
+                              if (readerUpto == merger.segStates.size()) {
+                                return false;
+                              }
+
+                              if (currentReader == null || docIDUpto == currentReader.reader.maxDoc()) {
+                                readerUpto++;
+                                if (readerUpto < merger.segStates.size()) {
+                                  currentReader = merger.segStates.get(readerUpto);
+                                  currentLiveDocs = currentReader.reader.getLiveDocs();
+                                }
+                                docIDUpto = 0;
+                                continue;
+                              }
+
+                              if (currentLiveDocs == null || currentLiveDocs.get(docIDUpto)) {
+                                nextIsSet = true;
+                                int segOrd = currentReader.values.getOrd(docIDUpto);
+                                nextValue = currentReader.segOrdToMergedOrd[segOrd];
+                                docIDUpto++;
+                                return true;
+                              }
+
+                              docIDUpto++;
+                            }
+                          }
+                        };
+                      }
+                    });
+
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java
new file mode 100644
index 0000000..7d81c2d
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/DocValuesFormat.java
@@ -0,0 +1,97 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.NamedSPILoader;
+
+public abstract class DocValuesFormat implements NamedSPILoader.NamedSPI {
+  
+  private static final NamedSPILoader<DocValuesFormat> loader =
+      new NamedSPILoader<DocValuesFormat>(DocValuesFormat.class);
+  
+  /** Unique name that's used to retrieve this format when
+   *  reading the index.
+   */
+  private final String name;
+
+  /**
+   * Creates a new docvalues format.
+   * <p>
+   * The provided name will be written into the index segment in some configurations
+   * (such as when using {@code PerFieldDocValuesFormat}): in such configurations,
+   * for the segment to be read this class should be registered with Java's
+   * SPI mechanism (registered in META-INF/ of your jar file, etc).
+   * @param name must be all ascii alphanumeric, and less than 128 characters in length.
+   */
+  protected DocValuesFormat(String name) {
+    NamedSPILoader.checkServiceName(name);
+    this.name = name;
+  }
+
+  public abstract DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException;
+
+  public abstract DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException;
+
+  @Override
+  public final String getName() {
+    return name;
+  }
+  
+  @Override
+  public String toString() {
+    return "DocValuesFormat(name=" + name + ")";
+  }
+  
+  /** looks up a format by name */
+  public static DocValuesFormat forName(String name) {
+    if (loader == null) {
+      throw new IllegalStateException("You called DocValuesFormat.forName() before all formats could be initialized. "+
+          "This likely happens if you call it from a DocValuesFormat's ctor.");
+    }
+    return loader.lookup(name);
+  }
+  
+  /** returns a list of all available format names */
+  public static Set<String> availableDocValuesFormats() {
+    if (loader == null) {
+      throw new IllegalStateException("You called DocValuesFormat.availableDocValuesFormats() before all formats could be initialized. "+
+          "This likely happens if you call it from a DocValuesFormat's ctor.");
+    }
+    return loader.availableServices();
+  }
+  
+  /** 
+   * Reloads the DocValues format list from the given {@link ClassLoader}.
+   * Changes to the docvalues formats are visible after the method ends, all
+   * iterators ({@link #availableDocValuesFormats()},...) stay consistent. 
+   * 
+   * <p><b>NOTE:</b> Only new docvalues formats are added, existing ones are
+   * never removed or replaced.
+   * 
+   * <p><em>This method is expensive and should only be called for discovery
+   * of new docvalues formats on the given classpath/classloader!</em>
+   */
+  public static void reloadDocValuesFormats(ClassLoader classloader) {
+    loader.reload(classloader);
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/DocValuesProducer.java
new file mode 100644
index 0000000..50fcd57
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/DocValuesProducer.java
@@ -0,0 +1,40 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SortedDocValues;
+
+// nocommit add javadocs stating that this must open all
+// necessary files "on init", not later eg in .getXXX, else
+// an IW that deletes a commit will cause an SR to hit
+// exceptions....
+
+public abstract class DocValuesProducer implements Closeable {
+
+  public abstract NumericDocValues getNumeric(FieldInfo field) throws IOException;
+
+  public abstract BinaryDocValues getBinary(FieldInfo field) throws IOException;
+
+  public abstract SortedDocValues getSorted(FieldInfo field) throws IOException;
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java b/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java
index 7f690ff..12f1719 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/FilterCodec.java
@@ -60,8 +60,8 @@ public abstract class FilterCodec extends Codec {
   }
 
   @Override
-  public SimpleDocValuesFormat simpleDocValuesFormat() {
-    return delegate.simpleDocValuesFormat();
+  public DocValuesFormat docValuesFormat() {
+    return delegate.docValuesFormat();
   }
 
   @Override
@@ -75,8 +75,8 @@ public abstract class FilterCodec extends Codec {
   }
 
   @Override
-  public SimpleNormsFormat simpleNormsFormat() {
-    return delegate.simpleNormsFormat();
+  public NormsFormat normsFormat() {
+    return delegate.normsFormat();
   }
 
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/NormsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/NormsFormat.java
new file mode 100644
index 0000000..cdf3924
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/NormsFormat.java
@@ -0,0 +1,41 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * format for normalization factors
+ */
+public abstract class NormsFormat {
+  /** Sole constructor. (For invocation by subclass 
+   *  constructors, typically implicit.) */
+  protected NormsFormat() {
+  }
+
+  /** Returns a {@link DocValuesConsumer} to write norms to the
+   *  index. */
+  public abstract DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException;
+
+  /** Returns a {@link DocValuesProducer} to read norms from the
+   *  index. */
+  public abstract DocValuesProducer normsProducer(SegmentReadState state) throws IOException;
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/SimpleDVConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/SimpleDVConsumer.java
deleted file mode 100644
index b1f3fa3..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/SimpleDVConsumer.java
+++ /dev/null
@@ -1,423 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.NoSuchElementException;
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.PriorityQueue;
-
-// prototype streaming DV api
-public abstract class SimpleDVConsumer implements Closeable {
-  // TODO: are any of these params too "infringing" on codec?
-  // we want codec to get necessary stuff from IW, but trading off against merge complexity.
-
-  // nocommit should we pass SegmentWriteState...?
-  public abstract void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException;    
-
-  public abstract void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException;
-
-  public abstract void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException;
-
-  // dead simple impl: codec can optimize
-  public void mergeNumericField(FieldInfo fieldInfo, final MergeState mergeState, final List<NumericDocValues> toMerge) throws IOException {
-
-    addNumericField(fieldInfo,
-                    new Iterable<Number>() {
-                      @Override
-                      public Iterator<Number> iterator() {
-                        return new Iterator<Number>() {
-                          int readerUpto = -1;
-                          int docIDUpto;
-                          long nextValue;
-                          AtomicReader currentReader;
-                          NumericDocValues currentValues;
-                          Bits currentLiveDocs;
-                          boolean nextIsSet;
-
-                          @Override
-                          public boolean hasNext() {
-                            return nextIsSet || setNext();
-                          }
-
-                          @Override
-                          public void remove() {
-                            throw new UnsupportedOperationException();
-                          }
-
-                          @Override
-                          public Number next() {
-                            if (!hasNext()) {
-                              throw new NoSuchElementException();
-                            }
-                            assert nextIsSet;
-                            nextIsSet = false;
-                            // nocommit make a mutable number
-                            return nextValue;
-                          }
-
-                          private boolean setNext() {
-                            while (true) {
-                              if (readerUpto == toMerge.size()) {
-                                return false;
-                              }
-
-                              if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
-                                readerUpto++;
-                                if (readerUpto < toMerge.size()) {
-                                  currentReader = mergeState.readers.get(readerUpto);
-                                  currentValues = toMerge.get(readerUpto);
-                                  currentLiveDocs = currentReader.getLiveDocs();
-                                }
-                                docIDUpto = 0;
-                                continue;
-                              }
-
-                              if (currentLiveDocs == null || currentLiveDocs.get(docIDUpto)) {
-                                nextIsSet = true;
-                                nextValue = currentValues.get(docIDUpto);
-                                docIDUpto++;
-                                return true;
-                              }
-
-                              docIDUpto++;
-                            }
-                          }
-                        };
-                      }
-                    });
-  }
-  
-  // dead simple impl: codec can optimize
-  public void mergeBinaryField(FieldInfo fieldInfo, final MergeState mergeState, final List<BinaryDocValues> toMerge) throws IOException {
-
-    addBinaryField(fieldInfo,
-                   new Iterable<BytesRef>() {
-                     @Override
-                     public Iterator<BytesRef> iterator() {
-                       return new Iterator<BytesRef>() {
-                         int readerUpto = -1;
-                         int docIDUpto;
-                         BytesRef nextValue = new BytesRef();
-                         AtomicReader currentReader;
-                         BinaryDocValues currentValues;
-                         Bits currentLiveDocs;
-                         boolean nextIsSet;
-
-                         @Override
-                         public boolean hasNext() {
-                           return nextIsSet || setNext();
-                         }
-
-                         @Override
-                         public void remove() {
-                           throw new UnsupportedOperationException();
-                         }
-
-                         @Override
-                         public BytesRef next() {
-                           if (!hasNext()) {
-                             throw new NoSuchElementException();
-                           }
-                           assert nextIsSet;
-                           nextIsSet = false;
-                           // nocommit make a mutable number
-                           return nextValue;
-                         }
-
-                         private boolean setNext() {
-                           while (true) {
-                             if (readerUpto == toMerge.size()) {
-                               return false;
-                             }
-
-                             if (currentReader == null || docIDUpto == currentReader.maxDoc()) {
-                               readerUpto++;
-                               if (readerUpto < toMerge.size()) {
-                                 currentReader = mergeState.readers.get(readerUpto);
-                                 currentValues = toMerge.get(readerUpto);
-                                 currentLiveDocs = currentReader.getLiveDocs();
-                               }
-                               docIDUpto = 0;
-                               continue;
-                             }
-
-                             if (currentLiveDocs == null || currentLiveDocs.get(docIDUpto)) {
-                               nextIsSet = true;
-                               currentValues.get(docIDUpto, nextValue);
-                               docIDUpto++;
-                               return true;
-                             }
-
-                             docIDUpto++;
-                           }
-                         }
-                       };
-                     }
-                   });
-  }
-
-  public static class SortedBytesMerger {
-
-    public int numMergedTerms;
-
-    final List<BytesRef> mergedTerms = new ArrayList<BytesRef>();
-    final List<SegmentState> segStates = new ArrayList<SegmentState>();
-
-    private static class SegmentState {
-      AtomicReader reader;
-      FixedBitSet liveTerms;
-      int ord = -1;
-      SortedDocValues values;
-      BytesRef scratch = new BytesRef();
-
-      // nocommit can we factor out the compressed fields
-      // compression?  ie we have a good idea "roughly" what
-      // the ord should be (linear projection) so we only
-      // need to encode the delta from that ...:        
-      int[] segOrdToMergedOrd;
-
-      public BytesRef nextTerm() {
-        while (ord < values.getValueCount()-1) {
-          ord++;
-          if (liveTerms == null || liveTerms.get(ord)) {
-            values.lookupOrd(ord, scratch);
-            return scratch;
-          } else {
-            // Skip "deleted" terms (ie, terms that were not
-            // referenced by any live docs):
-            values.lookupOrd(ord, scratch);
-          }
-        }
-
-        return null;
-      }
-    }
-
-    private static class TermMergeQueue extends PriorityQueue<SegmentState> {
-      public TermMergeQueue(int maxSize) {
-        super(maxSize);
-      }
-
-      @Override
-      protected boolean lessThan(SegmentState a, SegmentState b) {
-        return a.scratch.compareTo(b.scratch) <= 0;
-      }
-    }
-
-    public void merge(MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {
-
-      // First pass: mark "live" terms
-      for (int readerIDX=0;readerIDX<toMerge.size();readerIDX++) {
-        AtomicReader reader = mergeState.readers.get(readerIDX);      
-        // nocommit what if this is null...?  need default source?
-        int maxDoc = reader.maxDoc();
-
-        SegmentState state = new SegmentState();
-        state.reader = reader;
-        state.values = toMerge.get(readerIDX);
-
-        segStates.add(state);
-        assert state.values.getValueCount() < Integer.MAX_VALUE;
-        if (reader.hasDeletions()) {
-          state.liveTerms = new FixedBitSet(state.values.getValueCount());
-          Bits liveDocs = reader.getLiveDocs();
-          assert liveDocs != null;
-          for(int docID=0;docID<maxDoc;docID++) {
-            if (liveDocs.get(docID)) {
-              state.liveTerms.set(state.values.getOrd(docID));
-            }
-          }
-        }
-
-        // nocommit we can unload the bits to disk to reduce
-        // transient ram spike...
-      }
-
-      // Second pass: merge only the live terms
-
-      TermMergeQueue q = new TermMergeQueue(segStates.size());
-      for(SegmentState segState : segStates) {
-        if (segState.nextTerm() != null) {
-
-          // nocommit we could defer this to 3rd pass (and
-          // reduce transient RAM spike) but then
-          // we'd spend more effort computing the mapping...:
-          segState.segOrdToMergedOrd = new int[segState.values.getValueCount()];
-          q.add(segState);
-        }
-      }
-
-      BytesRef lastTerm = null;
-      int ord = 0;
-      while (q.size() != 0) {
-        SegmentState top = q.top();
-        if (lastTerm == null || !lastTerm.equals(top.scratch)) {
-          lastTerm = BytesRef.deepCopyOf(top.scratch);
-          // nocommit we could spill this to disk instead of
-          // RAM, and replay on finish...
-          mergedTerms.add(lastTerm);
-          ord++;
-        }
-
-        top.segOrdToMergedOrd[top.ord] = ord-1;
-        if (top.nextTerm() == null) {
-          q.pop();
-        } else {
-          q.updateTop();
-        }
-      }
-
-      numMergedTerms = ord;
-    }
-
-    /*
-    public void finish(SortedDocValuesConsumer consumer) throws IOException {
-
-      // Third pass: write merged result
-      for(BytesRef term : mergedTerms) {
-        consumer.addValue(term);
-      }
-
-      for(SegmentState segState : segStates) {
-        Bits liveDocs = segState.reader.getLiveDocs();
-        int maxDoc = segState.reader.maxDoc();
-        for(int docID=0;docID<maxDoc;docID++) {
-          if (liveDocs == null || liveDocs.get(docID)) {
-            int segOrd = segState.values.getOrd(docID);
-            int mergedOrd = segState.segOrdToMergedOrd[segOrd];
-            consumer.addDoc(mergedOrd);
-          }
-        }
-      }
-    }
-    */
-  }
-
-  public void mergeSortedField(FieldInfo fieldInfo, final MergeState mergeState, List<SortedDocValues> toMerge) throws IOException {
-    final SortedBytesMerger merger = new SortedBytesMerger();
-
-    // Does the heavy lifting to merge sort all "live" ords:
-    merger.merge(mergeState, toMerge);
-
-    addSortedField(fieldInfo,
-
-                   // ord -> value
-                   new Iterable<BytesRef>() {
-                     @Override
-                     public Iterator<BytesRef> iterator() {
-                       return new Iterator<BytesRef>() {
-                         int ordUpto;
-
-                         @Override
-                         public boolean hasNext() {
-                           return ordUpto < merger.mergedTerms.size();
-                         }
-
-                         @Override
-                         public void remove() {
-                           throw new UnsupportedOperationException();
-                         }
-
-                         @Override
-                         public BytesRef next() {
-                           return merger.mergedTerms.get(ordUpto++);
-                         }
-                       };
-                     }
-                   },
-
-                   // doc -> ord
-                    new Iterable<Number>() {
-                      @Override
-                      public Iterator<Number> iterator() {
-                        return new Iterator<Number>() {
-                          int readerUpto = -1;
-                          int docIDUpto;
-                          int nextValue;
-                          SortedBytesMerger.SegmentState currentReader;
-                          Bits currentLiveDocs;
-                          boolean nextIsSet;
-
-                          @Override
-                          public boolean hasNext() {
-                            return nextIsSet || setNext();
-                          }
-
-                          @Override
-                          public void remove() {
-                            throw new UnsupportedOperationException();
-                          }
-
-                          @Override
-                          public Number next() {
-                            if (!hasNext()) {
-                              throw new NoSuchElementException();
-                            }
-                            assert nextIsSet;
-                            nextIsSet = false;
-                            // nocommit make a mutable number
-                            return nextValue;
-                          }
-
-                          private boolean setNext() {
-                            while (true) {
-                              if (readerUpto == merger.segStates.size()) {
-                                return false;
-                              }
-
-                              if (currentReader == null || docIDUpto == currentReader.reader.maxDoc()) {
-                                readerUpto++;
-                                if (readerUpto < merger.segStates.size()) {
-                                  currentReader = merger.segStates.get(readerUpto);
-                                  currentLiveDocs = currentReader.reader.getLiveDocs();
-                                }
-                                docIDUpto = 0;
-                                continue;
-                              }
-
-                              if (currentLiveDocs == null || currentLiveDocs.get(docIDUpto)) {
-                                nextIsSet = true;
-                                int segOrd = currentReader.values.getOrd(docIDUpto);
-                                nextValue = currentReader.segOrdToMergedOrd[segOrd];
-                                docIDUpto++;
-                                return true;
-                              }
-
-                              docIDUpto++;
-                            }
-                          }
-                        };
-                      }
-                    });
-
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/SimpleDVProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/SimpleDVProducer.java
deleted file mode 100644
index d25a64b..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/SimpleDVProducer.java
+++ /dev/null
@@ -1,40 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SortedDocValues;
-
-// nocommit add javadocs stating that this must open all
-// necessary files "on init", not later eg in .getXXX, else
-// an IW that deletes a commit will cause an SR to hit
-// exceptions....
-
-public abstract class SimpleDVProducer implements Closeable {
-
-  public abstract NumericDocValues getNumeric(FieldInfo field) throws IOException;
-
-  public abstract BinaryDocValues getBinary(FieldInfo field) throws IOException;
-
-  public abstract SortedDocValues getSorted(FieldInfo field) throws IOException;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/SimpleDocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/SimpleDocValuesFormat.java
deleted file mode 100644
index e60ab95..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/SimpleDocValuesFormat.java
+++ /dev/null
@@ -1,97 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.NamedSPILoader;
-
-public abstract class SimpleDocValuesFormat implements NamedSPILoader.NamedSPI {
-  
-  private static final NamedSPILoader<SimpleDocValuesFormat> loader =
-      new NamedSPILoader<SimpleDocValuesFormat>(SimpleDocValuesFormat.class);
-  
-  /** Unique name that's used to retrieve this format when
-   *  reading the index.
-   */
-  private final String name;
-
-  /**
-   * Creates a new docvalues format.
-   * <p>
-   * The provided name will be written into the index segment in some configurations
-   * (such as when using {@code PerFieldDocValuesFormat}): in such configurations,
-   * for the segment to be read this class should be registered with Java's
-   * SPI mechanism (registered in META-INF/ of your jar file, etc).
-   * @param name must be all ascii alphanumeric, and less than 128 characters in length.
-   */
-  protected SimpleDocValuesFormat(String name) {
-    NamedSPILoader.checkServiceName(name);
-    this.name = name;
-  }
-
-  public abstract SimpleDVConsumer fieldsConsumer(SegmentWriteState state) throws IOException;
-
-  public abstract SimpleDVProducer fieldsProducer(SegmentReadState state) throws IOException;
-
-  @Override
-  public final String getName() {
-    return name;
-  }
-  
-  @Override
-  public String toString() {
-    return "DocValuesFormat(name=" + name + ")";
-  }
-  
-  /** looks up a format by name */
-  public static SimpleDocValuesFormat forName(String name) {
-    if (loader == null) {
-      throw new IllegalStateException("You called DocValuesFormat.forName() before all formats could be initialized. "+
-          "This likely happens if you call it from a DocValuesFormat's ctor.");
-    }
-    return loader.lookup(name);
-  }
-  
-  /** returns a list of all available format names */
-  public static Set<String> availableDocValuesFormats() {
-    if (loader == null) {
-      throw new IllegalStateException("You called DocValuesFormat.availableDocValuesFormats() before all formats could be initialized. "+
-          "This likely happens if you call it from a DocValuesFormat's ctor.");
-    }
-    return loader.availableServices();
-  }
-  
-  /** 
-   * Reloads the DocValues format list from the given {@link ClassLoader}.
-   * Changes to the docvalues formats are visible after the method ends, all
-   * iterators ({@link #availableDocValuesFormats()},...) stay consistent. 
-   * 
-   * <p><b>NOTE:</b> Only new docvalues formats are added, existing ones are
-   * never removed or replaced.
-   * 
-   * <p><em>This method is expensive and should only be called for discovery
-   * of new docvalues formats on the given classpath/classloader!</em>
-   */
-  public static void reloadDocValuesFormats(ClassLoader classloader) {
-    loader.reload(classloader);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/SimpleNormsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/SimpleNormsFormat.java
deleted file mode 100644
index 1e09c6b..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/SimpleNormsFormat.java
+++ /dev/null
@@ -1,41 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * format for normalization factors
- */
-public abstract class SimpleNormsFormat {
-  /** Sole constructor. (For invocation by subclass 
-   *  constructors, typically implicit.) */
-  protected SimpleNormsFormat() {
-  }
-
-  /** Returns a {@link SimpleDVConsumer} to write norms to the
-   *  index. */
-  public abstract SimpleDVConsumer normsConsumer(SegmentWriteState state) throws IOException;
-
-  /** Returns a {@link SimpleDVProducer} to read norms from the
-   *  index. */
-  public abstract SimpleDVProducer normsProducer(SegmentReadState state) throws IOException;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
index f1a054d..46a602b 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
@@ -23,8 +23,8 @@ import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
-import org.apache.lucene.codecs.SimpleNormsFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41SimpleNormsFormat;
@@ -88,19 +88,19 @@ public final class Lucene40Codec extends Codec {
   }
   
   // nocommit need a read-only Lucene40SimpleDVFormat
-  private final SimpleDocValuesFormat defaultDVFormat = SimpleDocValuesFormat.forName("Disk");
+  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Disk");
 
   @Override
-  public SimpleDocValuesFormat simpleDocValuesFormat() {
+  public DocValuesFormat docValuesFormat() {
     // nocommit
     return defaultDVFormat;
   }
 
   // nocommit need a read-only Lucene40SimpleNormsFormat:
-  private final SimpleNormsFormat simpleNormsFormat = new Lucene41SimpleNormsFormat();
+  private final NormsFormat simpleNormsFormat = new Lucene41SimpleNormsFormat();
 
   @Override
-  public SimpleNormsFormat simpleNormsFormat() {
+  public NormsFormat normsFormat() {
     return simpleNormsFormat;
   }
 
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
index ef593ea..ebf3aaa 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
@@ -23,8 +23,8 @@ import org.apache.lucene.codecs.FilterCodec;
 import org.apache.lucene.codecs.LiveDocsFormat;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
-import org.apache.lucene.codecs.SimpleNormsFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
@@ -61,9 +61,9 @@ public class Lucene41Codec extends Codec {
   };
   
   
-  private final SimpleDocValuesFormat simpleDocValuesFormat = new PerFieldDocValuesFormat() {
+  private final DocValuesFormat simpleDocValuesFormat = new PerFieldDocValuesFormat() {
     @Override
-    public SimpleDocValuesFormat getDocValuesFormatForField(String field) {
+    public DocValuesFormat getDocValuesFormatForField(String field) {
       return Lucene41Codec.this.getDocValuesFormatForField(field);
     }
   };
@@ -117,23 +117,23 @@ public class Lucene41Codec extends Codec {
    *  
    *  The default implementation always returns "Lucene41"
    */
-  public SimpleDocValuesFormat getDocValuesFormatForField(String field) {
+  public DocValuesFormat getDocValuesFormatForField(String field) {
     return defaultDVFormat;
   }
   
   @Override
-  public SimpleDocValuesFormat simpleDocValuesFormat() {
+  public DocValuesFormat docValuesFormat() {
     return simpleDocValuesFormat;
   }
 
   private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
   // nocommit
-  private final SimpleDocValuesFormat defaultDVFormat = SimpleDocValuesFormat.forName("Lucene41");
+  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene41");
 
-  private final SimpleNormsFormat simpleNormsFormat = new Lucene41SimpleNormsFormat();
+  private final NormsFormat simpleNormsFormat = new Lucene41SimpleNormsFormat();
 
   @Override
-  public SimpleNormsFormat simpleNormsFormat() {
+  public NormsFormat normsFormat() {
     return simpleNormsFormat;
   }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesConsumer.java
index 8587b0b..533c9cf 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesConsumer.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesConsumer.java
@@ -23,7 +23,7 @@ import java.util.HashSet;
 import java.util.Iterator;
 
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.SimpleDVConsumer;
+import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentWriteState;
@@ -47,7 +47,7 @@ import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
  * the latter is typically much smaller with lucene's sims, as only some byte values are used,
  * but its often a nonlinear mapping, especially if you dont use crazy boosts.
  */
-class Lucene41SimpleDocValuesConsumer extends SimpleDVConsumer {
+class Lucene41SimpleDocValuesConsumer extends DocValuesConsumer {
   static final int VERSION_START = 0;
   static final int VERSION_CURRENT = VERSION_START;
   
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesFormat.java
index 498b4d0..f444d41 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesFormat.java
@@ -19,25 +19,25 @@ package org.apache.lucene.codecs.lucene41;
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.SimpleDVConsumer;
-import org.apache.lucene.codecs.SimpleDVProducer;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 
-public class Lucene41SimpleDocValuesFormat extends SimpleDocValuesFormat {
+public class Lucene41SimpleDocValuesFormat extends DocValuesFormat {
 
   public Lucene41SimpleDocValuesFormat() {
     super("Lucene41");
   }
 
   @Override
-  public SimpleDVConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
     return new Lucene41SimpleDocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
   }
   
   @Override
-  public SimpleDVProducer fieldsProducer(SegmentReadState state) throws IOException {
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
     return new Lucene41SimpleDocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
   }
   
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesProducer.java
index c3d1b4c..6af6196 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesProducer.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesProducer.java
@@ -22,7 +22,7 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.SimpleDVProducer;
+import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.FieldInfo;
@@ -45,7 +45,7 @@ import org.apache.lucene.util.fst.PositiveIntOutputs;
 import org.apache.lucene.util.fst.Util;
 import org.apache.lucene.util.packed.PackedInts;
 
-class Lucene41SimpleDocValuesProducer extends SimpleDVProducer {
+class Lucene41SimpleDocValuesProducer extends DocValuesProducer {
   // metadata maps (just file pointers and minimal stuff)
   private final Map<Integer,NumericEntry> numerics;
   private final Map<Integer,BinaryEntry> binaries;
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleNormsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleNormsFormat.java
index 748d083fd..c81eee2 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleNormsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleNormsFormat.java
@@ -19,21 +19,21 @@ package org.apache.lucene.codecs.lucene41;
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.SimpleDVConsumer;
-import org.apache.lucene.codecs.SimpleDVProducer;
-import org.apache.lucene.codecs.SimpleNormsFormat;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
 
-public class Lucene41SimpleNormsFormat extends SimpleNormsFormat {
+public class Lucene41SimpleNormsFormat extends NormsFormat {
 
   @Override
-  public SimpleDVConsumer normsConsumer(SegmentWriteState state) throws IOException {
+  public DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException {
     return new Lucene41SimpleDocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
   }
   
   @Override
-  public SimpleDVProducer normsProducer(SegmentReadState state) throws IOException {
+  public DocValuesProducer normsProducer(SegmentReadState state) throws IOException {
     return new Lucene41SimpleDocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
   }
   
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java
index dd8ee0a..9f36d5a 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/perfield/PerFieldDocValuesFormat.java
@@ -26,9 +26,9 @@ import java.util.ServiceLoader; // javadocs
 import java.util.TreeMap;
 
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SimpleDVConsumer;
-import org.apache.lucene.codecs.SimpleDVProducer;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.NumericDocValues;
@@ -54,7 +54,7 @@ import org.apache.lucene.util.IOUtils;
  * @lucene.experimental
  */
 
-public abstract class PerFieldDocValuesFormat extends SimpleDocValuesFormat {
+public abstract class PerFieldDocValuesFormat extends DocValuesFormat {
   /** Name of this {@link PostingsFormat}. */
   public static final String PER_FIELD_NAME = "PerFieldDV40";
 
@@ -73,13 +73,13 @@ public abstract class PerFieldDocValuesFormat extends SimpleDocValuesFormat {
   }
 
   @Override
-  public final SimpleDVConsumer fieldsConsumer(SegmentWriteState state)
+  public final DocValuesConsumer fieldsConsumer(SegmentWriteState state)
       throws IOException {
     return new FieldsWriter(state);
   }
   
   static class SimpleDVConsumerAndSuffix implements Closeable {
-    SimpleDVConsumer consumer;
+    DocValuesConsumer consumer;
     int suffix;
     
     @Override
@@ -88,9 +88,9 @@ public abstract class PerFieldDocValuesFormat extends SimpleDocValuesFormat {
     }
   }
     
-  private class FieldsWriter extends SimpleDVConsumer {
+  private class FieldsWriter extends DocValuesConsumer {
 
-    private final Map<SimpleDocValuesFormat,SimpleDVConsumerAndSuffix> formats = new HashMap<SimpleDocValuesFormat,SimpleDVConsumerAndSuffix>();
+    private final Map<DocValuesFormat,SimpleDVConsumerAndSuffix> formats = new HashMap<DocValuesFormat,SimpleDVConsumerAndSuffix>();
     private final Map<String,Integer> suffixes = new HashMap<String,Integer>();
     
     private final SegmentWriteState segmentWriteState;
@@ -114,8 +114,8 @@ public abstract class PerFieldDocValuesFormat extends SimpleDocValuesFormat {
       getInstance(field).addSortedField(field, values, docToOrd);
     }
 
-    private SimpleDVConsumer getInstance(FieldInfo field) throws IOException {
-      final SimpleDocValuesFormat format = getDocValuesFormatForField(field.name);
+    private DocValuesConsumer getInstance(FieldInfo field) throws IOException {
+      final DocValuesFormat format = getDocValuesFormatForField(field.name);
       if (format == null) {
         throw new IllegalStateException("invalid null DocValuesFormat for field=\"" + field.name + "\"");
       }
@@ -185,10 +185,10 @@ public abstract class PerFieldDocValuesFormat extends SimpleDocValuesFormat {
   // nocommit what if SimpleNormsFormat wants to use this
   // ...?  we have a "boolean isNorms" issue...?  I guess we
   // just need to make a PerFieldNormsFormat?
-  private class FieldsReader extends SimpleDVProducer {
+  private class FieldsReader extends DocValuesProducer {
 
-    private final Map<String,SimpleDVProducer> fields = new TreeMap<String,SimpleDVProducer>();
-    private final Map<String,SimpleDVProducer> formats = new HashMap<String,SimpleDVProducer>();
+    private final Map<String,DocValuesProducer> fields = new TreeMap<String,DocValuesProducer>();
+    private final Map<String,DocValuesProducer> formats = new HashMap<String,DocValuesProducer>();
 
     public FieldsReader(final SegmentReadState readState) throws IOException {
 
@@ -204,7 +204,7 @@ public abstract class PerFieldDocValuesFormat extends SimpleDocValuesFormat {
               // null formatName means the field is in fieldInfos, but has no docvalues!
               final String suffix = fi.getAttribute(PER_FIELD_SUFFIX_KEY);
               assert suffix != null;
-              SimpleDocValuesFormat format = SimpleDocValuesFormat.forName(formatName);
+              DocValuesFormat format = DocValuesFormat.forName(formatName);
               String segmentSuffix = getSuffix(formatName, suffix);
               if (!formats.containsKey(segmentSuffix)) {
                 formats.put(segmentSuffix, format.fieldsProducer(new SegmentReadState(readState, segmentSuffix)));
@@ -223,17 +223,17 @@ public abstract class PerFieldDocValuesFormat extends SimpleDocValuesFormat {
 
     private FieldsReader(FieldsReader other) {
 
-      Map<SimpleDVProducer,SimpleDVProducer> oldToNew = new IdentityHashMap<SimpleDVProducer,SimpleDVProducer>();
+      Map<DocValuesProducer,DocValuesProducer> oldToNew = new IdentityHashMap<DocValuesProducer,DocValuesProducer>();
       // First clone all formats
-      for(Map.Entry<String,SimpleDVProducer> ent : other.formats.entrySet()) {
-        SimpleDVProducer values = ent.getValue();
+      for(Map.Entry<String,DocValuesProducer> ent : other.formats.entrySet()) {
+        DocValuesProducer values = ent.getValue();
         formats.put(ent.getKey(), values);
         oldToNew.put(ent.getValue(), values);
       }
 
       // Then rebuild fields:
-      for(Map.Entry<String,SimpleDVProducer> ent : other.fields.entrySet()) {
-        SimpleDVProducer producer = oldToNew.get(ent.getValue());
+      for(Map.Entry<String,DocValuesProducer> ent : other.fields.entrySet()) {
+        DocValuesProducer producer = oldToNew.get(ent.getValue());
         assert producer != null;
         fields.put(ent.getKey(), producer);
       }
@@ -241,19 +241,19 @@ public abstract class PerFieldDocValuesFormat extends SimpleDocValuesFormat {
 
     @Override
     public NumericDocValues getNumeric(FieldInfo field) throws IOException {
-      SimpleDVProducer producer = fields.get(field.name);
+      DocValuesProducer producer = fields.get(field.name);
       return producer == null ? null : producer.getNumeric(field);
     }
 
     @Override
     public BinaryDocValues getBinary(FieldInfo field) throws IOException {
-      SimpleDVProducer producer = fields.get(field.name);
+      DocValuesProducer producer = fields.get(field.name);
       return producer == null ? null : producer.getBinary(field);
     }
 
     @Override
     public SortedDocValues getSorted(FieldInfo field) throws IOException {
-      SimpleDVProducer producer = fields.get(field.name);
+      DocValuesProducer producer = fields.get(field.name);
       return producer == null ? null : producer.getSorted(field);
     }
 
@@ -263,13 +263,13 @@ public abstract class PerFieldDocValuesFormat extends SimpleDocValuesFormat {
     }
 
     @Override
-    public SimpleDVProducer clone() {
+    public DocValuesProducer clone() {
       return new FieldsReader(this);
     }
   }
 
   @Override
-  public final SimpleDVProducer fieldsProducer(SegmentReadState state) throws IOException {
+  public final DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
     return new FieldsReader(state);
   }
 
@@ -279,5 +279,5 @@ public abstract class PerFieldDocValuesFormat extends SimpleDocValuesFormat {
    * <p>
    * The field to format mapping is written to the index, so
    * this method is only invoked when writing, not when reading. */
-  public abstract SimpleDocValuesFormat getDocValuesFormatForField(String field);
+  public abstract DocValuesFormat getDocValuesFormatForField(String field);
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java b/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
index beb4b2e..784284e 100644
--- a/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
@@ -179,7 +179,7 @@ public abstract class AtomicReader extends IndexReader {
   /** Returns {@link NumericDocValues} representing norms
    *  for this field, or null if no {@link NumericDocValues}
    *  were indexed. */
-  public abstract NumericDocValues simpleNormValues(String field) throws IOException;
+  public abstract NumericDocValues getNormValues(String field) throws IOException;
 
   /**
    * Get the {@link FieldInfos} describing all fields in
diff --git a/lucene/core/src/java/org/apache/lucene/index/BytesDVWriter.java b/lucene/core/src/java/org/apache/lucene/index/BytesDVWriter.java
index 1e0fad8..7ab8623 100644
--- a/lucene/core/src/java/org/apache/lucene/index/BytesDVWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/BytesDVWriter.java
@@ -20,7 +20,7 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.util.Iterator;
 
-import org.apache.lucene.codecs.SimpleDVConsumer;
+import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefArray;
 import org.apache.lucene.util.Counter;
@@ -65,7 +65,7 @@ class BytesDVWriter extends DocValuesWriter {
   }
 
   @Override
-  public void flush(SegmentWriteState state, SimpleDVConsumer dvConsumer) throws IOException {
+  public void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException {
     final int maxDoc = state.segmentInfo.getDocCount();
 
     dvConsumer.addBinaryField(fieldInfo,
diff --git a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
index 2b8a3ec..44cc6d7 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
@@ -679,7 +679,7 @@ public class CheckIndex {
           checkSimpleNorms(info, reader, infoStream);
           ++status.totFields;
         } else {
-          if (reader.simpleNormValues(info.name) != null) {
+          if (reader.getNormValues(info.name) != null) {
             throw new RuntimeException("field: " + info.name + " should omit norms but has them!");
           }
         }
@@ -1368,7 +1368,7 @@ public class CheckIndex {
   public static void checkSimpleNorms(FieldInfo fi, AtomicReader reader, PrintStream infoStream) throws IOException {
     switch(fi.getNormType()) {
       case NUMERIC:
-        checkNumericDocValues(fi.name, reader, reader.simpleNormValues(fi.name));
+        checkNumericDocValues(fi.name, reader, reader.getNormValues(fi.name));
         break;
       default:
         throw new AssertionError("wtf: " + fi.getNormType());
diff --git a/lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java b/lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java
index 833c2d6..161b939 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DocValuesProcessor.java
@@ -21,8 +21,8 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.codecs.SimpleDVConsumer;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Counter;
@@ -77,7 +77,7 @@ final class DocValuesProcessor extends StoredFieldsConsumer {
   @Override
   void flush(SegmentWriteState state) throws IOException {
     if (!writers.isEmpty()) {
-      SimpleDocValuesFormat fmt = state.segmentInfo.getCodec().simpleDocValuesFormat();
+      DocValuesFormat fmt = state.segmentInfo.getCodec().docValuesFormat();
       // nocommit once we make
       // Codec.simpleDocValuesFormat abstract, change
       // this to assert fmt != null!
@@ -85,7 +85,7 @@ final class DocValuesProcessor extends StoredFieldsConsumer {
         return;
       }
 
-      SimpleDVConsumer dvConsumer = fmt.fieldsConsumer(state);
+      DocValuesConsumer dvConsumer = fmt.fieldsConsumer(state);
       // nocommit change to assert != null:
       if (dvConsumer == null) {
         return;
diff --git a/lucene/core/src/java/org/apache/lucene/index/DocValuesWriter.java b/lucene/core/src/java/org/apache/lucene/index/DocValuesWriter.java
index 25761d2..dc4f48e 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DocValuesWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DocValuesWriter.java
@@ -19,10 +19,10 @@ package org.apache.lucene.index;
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.SimpleDVConsumer;
+import org.apache.lucene.codecs.DocValuesConsumer;
 
 abstract class DocValuesWriter {
   abstract void abort() throws IOException;
   abstract void finish(int numDoc);
-  abstract void flush(SegmentWriteState state, SimpleDVConsumer consumer) throws IOException;
+  abstract void flush(SegmentWriteState state, DocValuesConsumer consumer) throws IOException;
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java b/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java
index 17d2f7a..0247b6c 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java
@@ -424,8 +424,8 @@ public class FilterAtomicReader extends AtomicReader {
   }
 
   @Override
-  public NumericDocValues simpleNormValues(String field) throws IOException {
+  public NumericDocValues getNormValues(String field) throws IOException {
     ensureOpen();
-    return in.simpleNormValues(field);
+    return in.getNormValues(field);
   }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java b/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
new file mode 100644
index 0000000..c7c13fe
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
@@ -0,0 +1,207 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.IndexReader.ReaderClosedListener;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.Version;
+
+// nocommit move this back to test-framework!!!
+public class MultiDocValues {
+  
+  // moved to src/java so SlowWrapper can use it... uggggggh
+  public static NumericDocValues getNormValues(final IndexReader r, final String field) throws IOException {
+    final List<AtomicReaderContext> leaves = r.leaves();
+    if (leaves.size() == 1) {
+      return leaves.get(0).reader().getNormValues(field);
+    }
+    FieldInfo fi = MultiFields.getMergedFieldInfos(r).fieldInfo(field);
+    if (fi == null || fi.hasNorms() == false) {
+      return null;
+    }
+    boolean anyReal = false;
+    for(AtomicReaderContext ctx : leaves) {
+      NumericDocValues norms = ctx.reader().getNormValues(field);
+
+      if (norms != null) {
+        anyReal = true;
+      }
+    }
+
+    // assert anyReal; // nocommit: unsafe until 4.0 is done
+
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        int subIndex = ReaderUtil.subIndex(docID, leaves);
+        NumericDocValues norms;
+        try {
+          norms = leaves.get(subIndex).reader().getNormValues(field);
+        } catch (IOException ioe) {
+          throw new RuntimeException(ioe);
+        }
+        if (norms == null) { // WTF? should be EMPTY?
+          return 0;
+        } else {
+          return norms.get(docID - leaves.get(subIndex).docBase);
+        }
+      }
+    };
+  }
+
+  public static NumericDocValues getNumericValues(final IndexReader r, final String field) throws IOException {
+    final List<AtomicReaderContext> leaves = r.leaves();
+    if (leaves.size() == 1) {
+      return leaves.get(0).reader().getNumericDocValues(field);
+    }
+    boolean anyReal = false;
+    for(AtomicReaderContext ctx : leaves) {
+      NumericDocValues values = ctx.reader().getNumericDocValues(field);
+
+      if (values != null) {
+        anyReal = true;
+      }
+    }
+
+    if (!anyReal) {
+      return null;
+    } else {
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          int subIndex = ReaderUtil.subIndex(docID, leaves);
+          NumericDocValues values;
+          try {
+            values = leaves.get(subIndex).reader().getNumericDocValues(field);
+          } catch (IOException ioe) {
+            throw new RuntimeException(ioe);
+          }
+          if (values == null) {
+            return 0;
+          } else {
+            return values.get(docID - leaves.get(subIndex).docBase);
+          }
+        }
+      };
+    }
+  }
+
+  public static BinaryDocValues getBinaryValues(final IndexReader r, final String field) throws IOException {
+    final List<AtomicReaderContext> leaves = r.leaves();
+    if (leaves.size() == 1) {
+      return leaves.get(0).reader().getBinaryDocValues(field);
+    }
+    boolean anyReal = false;
+
+    for(AtomicReaderContext ctx : leaves) {
+      BinaryDocValues values = ctx.reader().getBinaryDocValues(field);
+
+      if (values != null) {
+        anyReal = true;
+      }
+    }
+
+    if (!anyReal) {
+      return null;
+    } else {
+
+      return new BinaryDocValues() {
+        @Override
+        public void get(int docID, BytesRef result) {
+          int subIndex = ReaderUtil.subIndex(docID, leaves);
+          BinaryDocValues values;
+          try {
+            values = leaves.get(subIndex).reader().getBinaryDocValues(field);
+          } catch (IOException ioe) {
+            throw new RuntimeException(ioe);
+          }
+          if (values != null) {
+            values.get(docID - leaves.get(subIndex).docBase, result);
+          } else {
+            result.length = 0;
+            result.bytes = BinaryDocValues.MISSING;
+          }
+        }
+      };
+    }
+  }
+  
+  public static SortedDocValues getSortedValues(final IndexReader r, final String field) throws IOException {
+    final List<AtomicReaderContext> leaves = r.leaves();
+    if (leaves.size() == 1) {
+      return leaves.get(0).reader().getSortedDocValues(field);
+    }
+    boolean anyReal = false;
+
+    for(AtomicReaderContext ctx : leaves) {
+      SortedDocValues values = ctx.reader().getSortedDocValues(field);
+
+      if (values != null) {
+        anyReal = true;
+      }
+    }
+
+    if (!anyReal) {
+      return null;
+    } else {
+      // its called slow-wrapper for a reason right?
+      final Directory scratch = new RAMDirectory();
+      IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_50, null);
+      config.setCodec(Codec.forName("SimpleText"));
+      IndexWriter writer = new IndexWriter(scratch, config);
+      List<AtomicReader> newLeaves = new ArrayList<AtomicReader>();
+      for (AtomicReaderContext ctx : leaves) {
+        final AtomicReader a = ctx.reader();
+        newLeaves.add(new FilterAtomicReader(a) {
+          @Override
+          public Bits getLiveDocs() {
+            return null; // lie
+          }
+          @Override
+          public int numDocs() {
+            return maxDoc(); // lie
+          }
+          @Override
+          public boolean hasDeletions() {
+            return false; // lie
+          }
+        });
+      }
+      writer.addIndexes(newLeaves.toArray(new AtomicReader[0]));
+      writer.close();
+      final IndexReader newR = DirectoryReader.open(scratch);
+      assert newR.leaves().size() == 1;
+      r.addReaderClosedListener(new ReaderClosedListener() {
+        @Override
+        public void onClose(IndexReader reader) {
+          IOUtils.closeWhileHandlingException(newR, scratch);
+        }
+      });
+      return newR.leaves().get(0).reader().getSortedDocValues(field);
+    }
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/index/MultiSimpleDocValues.java b/lucene/core/src/java/org/apache/lucene/index/MultiSimpleDocValues.java
deleted file mode 100644
index 7848541..0000000
--- a/lucene/core/src/java/org/apache/lucene/index/MultiSimpleDocValues.java
+++ /dev/null
@@ -1,207 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.IndexReader.ReaderClosedListener;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.Version;
-
-// nocommit move this back to test-framework!!!
-public class MultiSimpleDocValues {
-  
-  // moved to src/java so SlowWrapper can use it... uggggggh
-  public static NumericDocValues simpleNormValues(final IndexReader r, final String field) throws IOException {
-    final List<AtomicReaderContext> leaves = r.leaves();
-    if (leaves.size() == 1) {
-      return leaves.get(0).reader().simpleNormValues(field);
-    }
-    FieldInfo fi = MultiFields.getMergedFieldInfos(r).fieldInfo(field);
-    if (fi == null || fi.hasNorms() == false) {
-      return null;
-    }
-    boolean anyReal = false;
-    for(AtomicReaderContext ctx : leaves) {
-      NumericDocValues norms = ctx.reader().simpleNormValues(field);
-
-      if (norms != null) {
-        anyReal = true;
-      }
-    }
-
-    // assert anyReal; // nocommit: unsafe until 4.0 is done
-
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        int subIndex = ReaderUtil.subIndex(docID, leaves);
-        NumericDocValues norms;
-        try {
-          norms = leaves.get(subIndex).reader().simpleNormValues(field);
-        } catch (IOException ioe) {
-          throw new RuntimeException(ioe);
-        }
-        if (norms == null) { // WTF? should be EMPTY?
-          return 0;
-        } else {
-          return norms.get(docID - leaves.get(subIndex).docBase);
-        }
-      }
-    };
-  }
-
-  public static NumericDocValues simpleNumericValues(final IndexReader r, final String field) throws IOException {
-    final List<AtomicReaderContext> leaves = r.leaves();
-    if (leaves.size() == 1) {
-      return leaves.get(0).reader().getNumericDocValues(field);
-    }
-    boolean anyReal = false;
-    for(AtomicReaderContext ctx : leaves) {
-      NumericDocValues values = ctx.reader().getNumericDocValues(field);
-
-      if (values != null) {
-        anyReal = true;
-      }
-    }
-
-    if (!anyReal) {
-      return null;
-    } else {
-      return new NumericDocValues() {
-        @Override
-        public long get(int docID) {
-          int subIndex = ReaderUtil.subIndex(docID, leaves);
-          NumericDocValues values;
-          try {
-            values = leaves.get(subIndex).reader().getNumericDocValues(field);
-          } catch (IOException ioe) {
-            throw new RuntimeException(ioe);
-          }
-          if (values == null) {
-            return 0;
-          } else {
-            return values.get(docID - leaves.get(subIndex).docBase);
-          }
-        }
-      };
-    }
-  }
-
-  public static BinaryDocValues simpleBinaryValues(final IndexReader r, final String field) throws IOException {
-    final List<AtomicReaderContext> leaves = r.leaves();
-    if (leaves.size() == 1) {
-      return leaves.get(0).reader().getBinaryDocValues(field);
-    }
-    boolean anyReal = false;
-
-    for(AtomicReaderContext ctx : leaves) {
-      BinaryDocValues values = ctx.reader().getBinaryDocValues(field);
-
-      if (values != null) {
-        anyReal = true;
-      }
-    }
-
-    if (!anyReal) {
-      return null;
-    } else {
-
-      return new BinaryDocValues() {
-        @Override
-        public void get(int docID, BytesRef result) {
-          int subIndex = ReaderUtil.subIndex(docID, leaves);
-          BinaryDocValues values;
-          try {
-            values = leaves.get(subIndex).reader().getBinaryDocValues(field);
-          } catch (IOException ioe) {
-            throw new RuntimeException(ioe);
-          }
-          if (values != null) {
-            values.get(docID - leaves.get(subIndex).docBase, result);
-          } else {
-            result.length = 0;
-            result.bytes = BinaryDocValues.MISSING;
-          }
-        }
-      };
-    }
-  }
-  
-  public static SortedDocValues simpleSortedValues(final IndexReader r, final String field) throws IOException {
-    final List<AtomicReaderContext> leaves = r.leaves();
-    if (leaves.size() == 1) {
-      return leaves.get(0).reader().getSortedDocValues(field);
-    }
-    boolean anyReal = false;
-
-    for(AtomicReaderContext ctx : leaves) {
-      SortedDocValues values = ctx.reader().getSortedDocValues(field);
-
-      if (values != null) {
-        anyReal = true;
-      }
-    }
-
-    if (!anyReal) {
-      return null;
-    } else {
-      // its called slow-wrapper for a reason right?
-      final Directory scratch = new RAMDirectory();
-      IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_50, null);
-      config.setCodec(Codec.forName("SimpleText"));
-      IndexWriter writer = new IndexWriter(scratch, config);
-      List<AtomicReader> newLeaves = new ArrayList<AtomicReader>();
-      for (AtomicReaderContext ctx : leaves) {
-        final AtomicReader a = ctx.reader();
-        newLeaves.add(new FilterAtomicReader(a) {
-          @Override
-          public Bits getLiveDocs() {
-            return null; // lie
-          }
-          @Override
-          public int numDocs() {
-            return maxDoc(); // lie
-          }
-          @Override
-          public boolean hasDeletions() {
-            return false; // lie
-          }
-        });
-      }
-      writer.addIndexes(newLeaves.toArray(new AtomicReader[0]));
-      writer.close();
-      final IndexReader newR = DirectoryReader.open(scratch);
-      assert newR.leaves().size() == 1;
-      r.addReaderClosedListener(new ReaderClosedListener() {
-        @Override
-        public void onClose(IndexReader reader) {
-          IOUtils.closeWhileHandlingException(newR, scratch);
-        }
-      });
-      return newR.leaves().get(0).reader().getSortedDocValues(field);
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/index/NormsConsumer.java b/lucene/core/src/java/org/apache/lucene/index/NormsConsumer.java
index f442f81..799270f 100644
--- a/lucene/core/src/java/org/apache/lucene/index/NormsConsumer.java
+++ b/lucene/core/src/java/org/apache/lucene/index/NormsConsumer.java
@@ -20,8 +20,8 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.codecs.SimpleDVConsumer;
-import org.apache.lucene.codecs.SimpleNormsFormat;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.util.IOUtils;
 
@@ -40,10 +40,10 @@ final class NormsConsumer extends InvertedDocEndConsumer {
   @Override
   public void flush(Map<String,InvertedDocEndConsumerPerField> fieldsToFlush, SegmentWriteState state) throws IOException {
     boolean success = false;
-    SimpleDVConsumer normsConsumer = null;
+    DocValuesConsumer normsConsumer = null;
     try {
       if (state.fieldInfos.hasNorms()) {
-        SimpleNormsFormat normsFormat = state.segmentInfo.getCodec().simpleNormsFormat();
+        NormsFormat normsFormat = state.segmentInfo.getCodec().normsFormat();
         assert normsFormat != null;
         normsConsumer = normsFormat.normsConsumer(state);
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/NormsConsumerPerField.java b/lucene/core/src/java/org/apache/lucene/index/NormsConsumerPerField.java
index 735d167..868e84b 100644
--- a/lucene/core/src/java/org/apache/lucene/index/NormsConsumerPerField.java
+++ b/lucene/core/src/java/org/apache/lucene/index/NormsConsumerPerField.java
@@ -17,7 +17,7 @@ package org.apache.lucene.index;
  */
 import java.io.IOException;
 
-import org.apache.lucene.codecs.SimpleDVConsumer;
+import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.search.similarities.Similarity;
 
 final class NormsConsumerPerField extends InvertedDocEndConsumerPerField implements Comparable<NormsConsumerPerField> {
@@ -52,7 +52,7 @@ final class NormsConsumerPerField extends InvertedDocEndConsumerPerField impleme
     }
   }
   
-  void flush(SegmentWriteState state, SimpleDVConsumer normsWriter) throws IOException {
+  void flush(SegmentWriteState state, DocValuesConsumer normsWriter) throws IOException {
     int docCount = state.segmentInfo.getDocCount();
     if (consumer == null) {
       return; // null type - not omitted but not written -
diff --git a/lucene/core/src/java/org/apache/lucene/index/NumberDVWriter.java b/lucene/core/src/java/org/apache/lucene/index/NumberDVWriter.java
index d8014af..041780b 100644
--- a/lucene/core/src/java/org/apache/lucene/index/NumberDVWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/NumberDVWriter.java
@@ -20,7 +20,7 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.util.Iterator;
 
-import org.apache.lucene.codecs.SimpleDVConsumer;
+import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.util.Counter;
 import org.apache.lucene.util.packed.AppendingLongBuffer;
 
@@ -72,7 +72,7 @@ class NumberDVWriter extends DocValuesWriter {
   }
 
   @Override
-  public void flush(SegmentWriteState state, SimpleDVConsumer dvConsumer) throws IOException {
+  public void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException {
 
     final int maxDoc = state.segmentInfo.getDocCount();
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java b/lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java
index d3b78e5..4244ecd 100644
--- a/lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/ParallelAtomicReader.java
@@ -285,10 +285,10 @@ public final class ParallelAtomicReader extends AtomicReader {
   }
 
   @Override
-  public NumericDocValues simpleNormValues(String field) throws IOException {
+  public NumericDocValues getNormValues(String field) throws IOException {
     ensureOpen();
     AtomicReader reader = fieldToReader.get(field);
-    NumericDocValues values = reader == null ? null : reader.simpleNormValues(field);
+    NumericDocValues values = reader == null ? null : reader.getNormValues(field);
     return values;
   }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java b/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
index f2ed523..1403cc2 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
@@ -28,7 +28,7 @@ import java.util.concurrent.atomic.AtomicInteger;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SimpleDVProducer;
+import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
@@ -54,8 +54,8 @@ final class SegmentCoreReaders {
   final FieldInfos fieldInfos;
   
   final FieldsProducer fields;
-  final SimpleDVProducer simpleDVProducer;
-  final SimpleDVProducer simpleNormsProducer;
+  final DocValuesProducer simpleDVProducer;
+  final DocValuesProducer simpleNormsProducer;
 
   final int termsIndexDivisor;
   
@@ -132,9 +132,9 @@ final class SegmentCoreReaders {
       // TODO: since we don't write any norms file if there are no norms,
       // kinda jaky to assume the codec handles the case of no norms file at all gracefully?!
       // nocommit shouldn't need null check:
-      if (codec.simpleDocValuesFormat() != null) {
+      if (codec.docValuesFormat() != null) {
         if (fieldInfos.hasDocValues()) {
-          simpleDVProducer = codec.simpleDocValuesFormat().fieldsProducer(segmentReadState);
+          simpleDVProducer = codec.docValuesFormat().fieldsProducer(segmentReadState);
         } else {
           simpleDVProducer = null;
         }
@@ -142,9 +142,9 @@ final class SegmentCoreReaders {
         simpleDVProducer = null;
       }
       // nocommit shouldn't need null check:
-      if (codec.simpleNormsFormat() != null) {
+      if (codec.normsFormat() != null) {
         if (fieldInfos.hasNorms()) {
-          simpleNormsProducer = codec.simpleNormsFormat().normsProducer(segmentReadState);
+          simpleNormsProducer = codec.normsFormat().normsProducer(segmentReadState);
         } else {
           simpleNormsProducer = null;
         }
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
index 4a95ba6..3396567 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -26,7 +26,7 @@ import java.util.Map;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.FieldInfosWriter;
 import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.SimpleDVConsumer;
+import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.codecs.TermVectorsWriter;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
@@ -164,8 +164,8 @@ final class SegmentMerger {
 
   private void mergeSimpleDocValues(SegmentWriteState segmentWriteState) throws IOException {
 
-    if (codec.simpleDocValuesFormat() != null) {
-      SimpleDVConsumer consumer = codec.simpleDocValuesFormat().fieldsConsumer(segmentWriteState);
+    if (codec.docValuesFormat() != null) {
+      DocValuesConsumer consumer = codec.docValuesFormat().fieldsConsumer(segmentWriteState);
       boolean success = false;
       try {
         for (FieldInfo field : mergeState.fieldInfos) {
@@ -218,15 +218,15 @@ final class SegmentMerger {
   }
 
   private void mergeSimpleNorms(SegmentWriteState segmentWriteState) throws IOException {
-    if (codec.simpleNormsFormat() != null) {
-      SimpleDVConsumer consumer = codec.simpleNormsFormat().normsConsumer(segmentWriteState);
+    if (codec.normsFormat() != null) {
+      DocValuesConsumer consumer = codec.normsFormat().normsConsumer(segmentWriteState);
       boolean success = false;
       try {
         for (FieldInfo field : mergeState.fieldInfos) {
           if (field.hasNorms()) {
             List<NumericDocValues> toMerge = new ArrayList<NumericDocValues>();
             for (AtomicReader reader : mergeState.readers) {
-              NumericDocValues norms = reader.simpleNormValues(field.name);
+              NumericDocValues norms = reader.getNormValues(field.name);
               if (norms == null) {
                 norms = NumericDocValues.EMPTY;
               }
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java b/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
index 14c4f9a..0684880 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
@@ -248,7 +248,7 @@ public final class SegmentReader extends AtomicReader {
   }
 
   @Override
-  public NumericDocValues simpleNormValues(String field) throws IOException {
+  public NumericDocValues getNormValues(String field) throws IOException {
     ensureOpen();
     return core.getSimpleNormValues(field);
   }
diff --git a/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java b/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
index d152c6f..39d2cfe 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
@@ -29,7 +29,7 @@ import org.apache.lucene.index.MultiReader; // javadoc
  * MultiReader} or {@link DirectoryReader}) to emulate an
  * atomic reader.  This requires implementing the postings
  * APIs on-the-fly, using the static methods in {@link
- * MultiFields}, {@link MultiSimpleDocValues}, by stepping through
+ * MultiFields}, {@link MultiDocValues}, by stepping through
  * the sub-readers to merge fields/terms, appending docs, etc.
  *
  * <p><b>NOTE</b>: this class almost always results in a
@@ -82,26 +82,26 @@ public final class SlowCompositeReaderWrapper extends AtomicReader {
   @Override
   public NumericDocValues getNumericDocValues(String field) throws IOException {
     ensureOpen();
-    return MultiSimpleDocValues.simpleNumericValues(in, field);
+    return MultiDocValues.getNumericValues(in, field);
   }
 
   @Override
   public BinaryDocValues getBinaryDocValues(String field) throws IOException {
     ensureOpen();
-    return MultiSimpleDocValues.simpleBinaryValues(in, field);
+    return MultiDocValues.getBinaryValues(in, field);
   }
 
   @Override
   public SortedDocValues getSortedDocValues(String field) throws IOException {
     ensureOpen();
-    return MultiSimpleDocValues.simpleSortedValues(in, field);
+    return MultiDocValues.getSortedValues(in, field);
   }
 
   @Override
-  public NumericDocValues simpleNormValues(String field) throws IOException {
+  public NumericDocValues getNormValues(String field) throws IOException {
     ensureOpen();
     // nocommit hmm
-    return MultiSimpleDocValues.simpleNormValues(in, field);
+    return MultiDocValues.getNormValues(in, field);
   }
   
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/index/SortedBytesDVWriter.java b/lucene/core/src/java/org/apache/lucene/index/SortedBytesDVWriter.java
index 9d0c517..8e335eb 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SortedBytesDVWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SortedBytesDVWriter.java
@@ -20,7 +20,7 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.util.Iterator;
 
-import org.apache.lucene.codecs.SimpleDVConsumer;
+import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.ByteBlockPool;
 import org.apache.lucene.util.BytesRef;
@@ -94,7 +94,7 @@ class SortedBytesDVWriter extends DocValuesWriter {
   }
 
   @Override
-  public void flush(SegmentWriteState state, SimpleDVConsumer dvConsumer) throws IOException {
+  public void flush(SegmentWriteState state, DocValuesConsumer dvConsumer) throws IOException {
     final int maxDoc = state.segmentInfo.getDocCount();
 
     final int emptyOrd;
diff --git a/lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java b/lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java
index dcc1465..106f1cb 100644
--- a/lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java
+++ b/lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java
@@ -214,7 +214,7 @@ public class BM25Similarity extends Similarity {
   @Override
   public final ExactSimScorer exactSimScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
     BM25Stats bm25stats = (BM25Stats) stats;
-    final NumericDocValues norms = context.reader().simpleNormValues(bm25stats.field);
+    final NumericDocValues norms = context.reader().getNormValues(bm25stats.field);
     return norms == null 
       ? new ExactBM25DocScorerNoNorms(bm25stats)
       : new ExactBM25DocScorer(bm25stats, norms);
@@ -223,7 +223,7 @@ public class BM25Similarity extends Similarity {
   @Override
   public final SloppySimScorer sloppySimScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
     BM25Stats bm25stats = (BM25Stats) stats;
-    return new SloppyBM25DocScorer(bm25stats, context.reader().simpleNormValues(bm25stats.field));
+    return new SloppyBM25DocScorer(bm25stats, context.reader().getNormValues(bm25stats.field));
   }
   
   private class ExactBM25DocScorer extends ExactSimScorer {
diff --git a/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java b/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
index 7909193..d805cc0 100644
--- a/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
+++ b/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
@@ -53,7 +53,7 @@ import org.apache.lucene.util.SmallFloat; // javadoc
  * <a name="indextime"/>
  * At indexing time, the indexer calls {@link #computeNorm(FieldInvertState)}, allowing
  * the Similarity implementation to set a per-document value for the field that will 
- * be later accessible via {@link AtomicReader#simpleNormValues(String)}.  Lucene makes no assumption
+ * be later accessible via {@link AtomicReader#getNormValues(String)}.  Lucene makes no assumption
  * about what is in this norm, but it is most useful for encoding length normalization 
  * information.
  * <p>
diff --git a/lucene/core/src/java/org/apache/lucene/search/similarities/SimilarityBase.java b/lucene/core/src/java/org/apache/lucene/search/similarities/SimilarityBase.java
index 0bfca3e..4f4f678 100644
--- a/lucene/core/src/java/org/apache/lucene/search/similarities/SimilarityBase.java
+++ b/lucene/core/src/java/org/apache/lucene/search/similarities/SimilarityBase.java
@@ -198,12 +198,12 @@ public abstract class SimilarityBase extends Similarity {
       ExactSimScorer subScorers[] = new ExactSimScorer[subStats.length];
       for (int i = 0; i < subScorers.length; i++) {
         BasicStats basicstats = (BasicStats) subStats[i];
-        subScorers[i] = new BasicExactDocScorer(basicstats, context.reader().simpleNormValues(basicstats.field));
+        subScorers[i] = new BasicExactDocScorer(basicstats, context.reader().getNormValues(basicstats.field));
       }
       return new MultiSimilarity.MultiExactDocScorer(subScorers);
     } else {
       BasicStats basicstats = (BasicStats) stats;
-      return new BasicExactDocScorer(basicstats, context.reader().simpleNormValues(basicstats.field));
+      return new BasicExactDocScorer(basicstats, context.reader().getNormValues(basicstats.field));
     }
   }
   
@@ -216,12 +216,12 @@ public abstract class SimilarityBase extends Similarity {
       SloppySimScorer subScorers[] = new SloppySimScorer[subStats.length];
       for (int i = 0; i < subScorers.length; i++) {
         BasicStats basicstats = (BasicStats) subStats[i];
-        subScorers[i] = new BasicSloppyDocScorer(basicstats, context.reader().simpleNormValues(basicstats.field));
+        subScorers[i] = new BasicSloppyDocScorer(basicstats, context.reader().getNormValues(basicstats.field));
       }
       return new MultiSimilarity.MultiSloppyDocScorer(subScorers);
     } else {
       BasicStats basicstats = (BasicStats) stats;
-      return new BasicSloppyDocScorer(basicstats, context.reader().simpleNormValues(basicstats.field));
+      return new BasicSloppyDocScorer(basicstats, context.reader().getNormValues(basicstats.field));
     }
   }
   
diff --git a/lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java b/lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java
index 22939d8..b7c68e3 100644
--- a/lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java
+++ b/lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java
@@ -757,13 +757,13 @@ public abstract class TFIDFSimilarity extends Similarity {
   @Override
   public final ExactSimScorer exactSimScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
     IDFStats idfstats = (IDFStats) stats;
-    return new ExactTFIDFDocScorer(idfstats, context.reader().simpleNormValues(idfstats.field));
+    return new ExactTFIDFDocScorer(idfstats, context.reader().getNormValues(idfstats.field));
   }
 
   @Override
   public final SloppySimScorer sloppySimScorer(SimWeight stats, AtomicReaderContext context) throws IOException {
     IDFStats idfstats = (IDFStats) stats;
-    return new SloppyTFIDFDocScorer(idfstats, context.reader().simpleNormValues(idfstats.field));
+    return new SloppyTFIDFDocScorer(idfstats, context.reader().getNormValues(idfstats.field));
   }
   
   // TODO: we can specialize these for omitNorms up front, but we should test that it doesn't confuse stupid hotspot.
diff --git a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
new file mode 100644
index 0000000..c5e93af
--- /dev/null
+++ b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
@@ -0,0 +1,16 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.lucene41.Lucene41SimpleDocValuesFormat
diff --git a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.SimpleDocValuesFormat b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.SimpleDocValuesFormat
deleted file mode 100644
index c5e93af..0000000
--- a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.SimpleDocValuesFormat
+++ /dev/null
@@ -1,16 +0,0 @@
-#  Licensed to the Apache Software Foundation (ASF) under one or more
-#  contributor license agreements.  See the NOTICE file distributed with
-#  this work for additional information regarding copyright ownership.
-#  The ASF licenses this file to You under the Apache License, Version 2.0
-#  (the "License"); you may not use this file except in compliance with
-#  the License.  You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#  Unless required by applicable law or agreed to in writing, software
-#  distributed under the License is distributed on an "AS IS" BASIS,
-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#  See the License for the specific language governing permissions and
-#  limitations under the License.
-
-org.apache.lucene.codecs.lucene41.Lucene41SimpleDocValuesFormat
diff --git a/lucene/core/src/test/org/apache/lucene/TestDemoDocValue.java b/lucene/core/src/test/org/apache/lucene/TestDemoDocValue.java
index c6783b5..8192825 100644
--- a/lucene/core/src/test/org/apache/lucene/TestDemoDocValue.java
+++ b/lucene/core/src/test/org/apache/lucene/TestDemoDocValue.java
@@ -21,7 +21,7 @@ import java.io.IOException;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -708,11 +708,11 @@ public class TestDemoDocValue extends LuceneTestCase {
     // we don't use RandomIndexWriter because it might add more docvalues than we expect !!!!1
     IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
     // TODO: Fix the CFS/suffixing of Lucene41DocValues so it actually works with this
-    final SimpleDocValuesFormat fast = SimpleDocValuesFormat.forName("Memory");
-    final SimpleDocValuesFormat slow = SimpleDocValuesFormat.forName("SimpleText");
+    final DocValuesFormat fast = DocValuesFormat.forName("Memory");
+    final DocValuesFormat slow = DocValuesFormat.forName("SimpleText");
     iwc.setCodec(new Lucene41Codec() {
       @Override
-      public SimpleDocValuesFormat getDocValuesFormatForField(String field) {
+      public DocValuesFormat getDocValuesFormatForField(String field) {
         if ("dv1".equals(field)) {
           return fast;
         } else {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java b/lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java
index 039a553..eec412c 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestCustomNorms.java
@@ -64,7 +64,7 @@ public class TestCustomNorms extends LuceneTestCase {
     writer.commit();
     writer.close();
     AtomicReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
-    NumericDocValues norms = open.simpleNormValues(floatTestField);
+    NumericDocValues norms = open.getNormValues(floatTestField);
     assertNotNull(norms);
     for (int i = 0; i < open.maxDoc(); i++) {
       StoredDocument document = open.document(i);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java b/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
index 19ebee6..f4b357a 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
@@ -565,8 +565,8 @@ public void testFilesOpenClose() throws IOException {
     // check norms
     for(FieldInfo fieldInfo : fieldInfos1) {
       String curField = fieldInfo.name;
-      NumericDocValues norms1 = MultiSimpleDocValues.simpleNormValues(index1, curField);
-      NumericDocValues norms2 = MultiSimpleDocValues.simpleNormValues(index2, curField);
+      NumericDocValues norms1 = MultiDocValues.getNormValues(index1, curField);
+      NumericDocValues norms2 = MultiDocValues.getNormValues(index2, curField);
       if (norms1 != null && norms2 != null) {
         // todo: generalize this (like TestDuelingCodecs assert)
         for (int i = 0; i < index1.maxDoc(); i++) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java b/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
index 24f037c..c3f3b52 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
@@ -862,7 +862,7 @@ public class TestDocValuesIndexing extends LuceneTestCase {
       }
       w.commit();
       IndexReader reader = w.getReader();
-      SortedDocValues docValues = MultiSimpleDocValues.simpleSortedValues(reader, "field");
+      SortedDocValues docValues = MultiDocValues.getSortedValues(reader, "field");
       int[] sort = hash.sort(BytesRef.getUTF8SortedAsUnicodeComparator());
       BytesRef expected = new BytesRef();
       BytesRef actual = new BytesRef();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java b/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java
index 7175d64..e36479e 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java
@@ -97,7 +97,7 @@ public class TestDocumentWriter extends LuceneTestCase {
     // omitNorms is true
     for (FieldInfo fi : reader.getFieldInfos()) {
       if (fi.isIndexed()) {
-        assertTrue(fi.omitsNorms() == (reader.simpleNormValues(fi.name) == null));
+        assertTrue(fi.omitsNorms() == (reader.getNormValues(fi.name) == null));
       }
     }
     reader.close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java b/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
index 6123699..1262c82 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
@@ -528,8 +528,8 @@ public class TestDuelingCodecs extends LuceneTestCase {
     
     for (String field : leftFields) {
       // nocommit cutover to per-segment comparison
-      NumericDocValues leftNorms = MultiSimpleDocValues.simpleNormValues(leftReader, field);
-      NumericDocValues rightNorms = MultiSimpleDocValues.simpleNormValues(rightReader, field);
+      NumericDocValues leftNorms = MultiDocValues.getNormValues(leftReader, field);
+      NumericDocValues rightNorms = MultiDocValues.getNormValues(rightReader, field);
       if (leftNorms != null && rightNorms != null) {
         assertDocValues(leftReader.maxDoc(), leftNorms, rightNorms);
       } else {
@@ -617,8 +617,8 @@ public class TestDuelingCodecs extends LuceneTestCase {
     for (String field : leftFields) {
 
       {
-        NumericDocValues leftValues = MultiSimpleDocValues.simpleNumericValues(leftReader, field);
-        NumericDocValues rightValues = MultiSimpleDocValues.simpleNumericValues(rightReader, field);
+        NumericDocValues leftValues = MultiDocValues.getNumericValues(leftReader, field);
+        NumericDocValues rightValues = MultiDocValues.getNumericValues(rightReader, field);
         if (leftValues != null && rightValues != null) {
           assertDocValues(leftReader.maxDoc(), leftValues, rightValues);
         } else {
@@ -628,8 +628,8 @@ public class TestDuelingCodecs extends LuceneTestCase {
       }
 
       {
-        BinaryDocValues leftValues = MultiSimpleDocValues.simpleBinaryValues(leftReader, field);
-        BinaryDocValues rightValues = MultiSimpleDocValues.simpleBinaryValues(rightReader, field);
+        BinaryDocValues leftValues = MultiDocValues.getBinaryValues(leftReader, field);
+        BinaryDocValues rightValues = MultiDocValues.getBinaryValues(rightReader, field);
         if (leftValues != null && rightValues != null) {
           BytesRef scratchLeft = new BytesRef();
           BytesRef scratchRight = new BytesRef();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestMaxTermFrequency.java b/lucene/core/src/test/org/apache/lucene/index/TestMaxTermFrequency.java
index bad5d64..371bb9f 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestMaxTermFrequency.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestMaxTermFrequency.java
@@ -67,7 +67,7 @@ public class TestMaxTermFrequency extends LuceneTestCase {
   }
   
   public void test() throws Exception {
-    NumericDocValues fooNorms = MultiSimpleDocValues.simpleNormValues(reader, "foo");
+    NumericDocValues fooNorms = MultiDocValues.getNormValues(reader, "foo");
     for (int i = 0; i < reader.maxDoc(); i++) {
       assertEquals(expected.get(i).intValue(), fooNorms.get(i) & 0xff);
     }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestNorms.java b/lucene/core/src/test/org/apache/lucene/index/TestNorms.java
index e6378cc..5dea898 100755
--- a/lucene/core/src/test/org/apache/lucene/index/TestNorms.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestNorms.java
@@ -83,12 +83,12 @@ public class TestNorms extends LuceneTestCase {
     IndexReader reader = writer.getReader();
     writer.close();
     
-    NumericDocValues fooNorms = MultiSimpleDocValues.simpleNormValues(reader, "foo");
+    NumericDocValues fooNorms = MultiDocValues.getNormValues(reader, "foo");
     for (int i = 0; i < reader.maxDoc(); i++) {
       assertEquals(0, fooNorms.get(i));
     }
     
-    NumericDocValues barNorms = MultiSimpleDocValues.simpleNormValues(reader, "bar");
+    NumericDocValues barNorms = MultiDocValues.getNormValues(reader, "bar");
     for (int i = 0; i < reader.maxDoc(); i++) {
       assertEquals(1, barNorms.get(i));
     }
@@ -101,7 +101,7 @@ public class TestNorms extends LuceneTestCase {
     Directory dir = newFSDirectory(_TestUtil.getTempDir("TestNorms.testMaxByteNorms"));
     buildIndex(dir);
     AtomicReader open = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir));
-    NumericDocValues normValues = open.simpleNormValues(byteTestField);
+    NumericDocValues normValues = open.getNormValues(byteTestField);
     assertNotNull(normValues);
     for (int i = 0; i < open.maxDoc(); i++) {
       StoredDocument document = open.document(i);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java b/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java
index f86f121..c5459a6 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestOmitNorms.java
@@ -290,12 +290,12 @@ public class TestOmitNorms extends LuceneTestCase {
 
     IndexReader ir1 = riw.getReader();
     // todo: generalize
-    NumericDocValues norms1 = MultiSimpleDocValues.simpleNormValues(ir1, field);
+    NumericDocValues norms1 = MultiDocValues.getNormValues(ir1, field);
     
     // fully merge and validate MultiNorms against single segment.
     riw.forceMerge(1);
     DirectoryReader ir2 = riw.getReader();
-    NumericDocValues norms2 = getOnlySegmentReader(ir2).simpleNormValues(field);
+    NumericDocValues norms2 = getOnlySegmentReader(ir2).getNormValues(field);
 
     if (norms1 == null) {
       assertNull(norms2);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java b/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
index 7d93de3..c216126 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
@@ -178,11 +178,11 @@ public class TestSegmentReader extends LuceneTestCase {
     for (int i=0; i<DocHelper.fields.length; i++) {
       IndexableField f = DocHelper.fields[i];
       if (f.fieldType().indexed()) {
-        assertEquals(reader.simpleNormValues(f.name()) != null, !f.fieldType().omitNorms());
-        assertEquals(reader.simpleNormValues(f.name()) != null, !DocHelper.noNorms.containsKey(f.name()));
-        if (reader.simpleNormValues(f.name()) == null) {
+        assertEquals(reader.getNormValues(f.name()) != null, !f.fieldType().omitNorms());
+        assertEquals(reader.getNormValues(f.name()) != null, !DocHelper.noNorms.containsKey(f.name()));
+        if (reader.getNormValues(f.name()) == null) {
           // test for norms of null
-          NumericDocValues norms = MultiSimpleDocValues.simpleNormValues(reader, f.name());
+          NumericDocValues norms = MultiDocValues.getNormValues(reader, f.name());
           assertNull(norms);
         }
       }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java b/lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java
index 172f5ec..5e1da90 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestUniqueTermCount.java
@@ -69,7 +69,7 @@ public class TestUniqueTermCount extends LuceneTestCase {
   }
   
   public void test() throws Exception {
-    NumericDocValues fooNorms = MultiSimpleDocValues.simpleNormValues(reader, "foo");
+    NumericDocValues fooNorms = MultiDocValues.getNormValues(reader, "foo");
     assertNotNull(fooNorms);
     for (int i = 0; i < reader.maxDoc(); i++) {
       assertEquals(expected.get(i).longValue(), fooNorms.get(i));
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestSimilarityProvider.java b/lucene/core/src/test/org/apache/lucene/search/TestSimilarityProvider.java
index 381e2ea..d3d3a1d 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestSimilarityProvider.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestSimilarityProvider.java
@@ -77,8 +77,8 @@ public class TestSimilarityProvider extends LuceneTestCase {
     // sanity check of norms writer
     // TODO: generalize
     AtomicReader slow = new SlowCompositeReaderWrapper(reader);
-    NumericDocValues fooNorms = slow.simpleNormValues("foo");
-    NumericDocValues barNorms = slow.simpleNormValues("bar");
+    NumericDocValues fooNorms = slow.getNormValues("foo");
+    NumericDocValues barNorms = slow.getNormValues("bar");
     for (int i = 0; i < slow.maxDoc(); i++) {
       assertFalse(fooNorms.get(i) == barNorms.get(i));
     }
diff --git a/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java b/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
index 7f72c0d..b8cdbc6 100644
--- a/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
+++ b/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
@@ -1148,7 +1148,7 @@ public class MemoryIndex {
     private Similarity cachedSimilarity;
     
     @Override
-    public NumericDocValues simpleNormValues(String field) {
+    public NumericDocValues getNormValues(String field) {
       if (fieldInfos.get(field).omitsNorms())
         return null;
       NumericDocValues norms = cachedNormValues;
diff --git a/lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java b/lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
index 8b105f5..993c9c7 100644
--- a/lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
+++ b/lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
@@ -177,8 +177,8 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
       if (iwTerms == null) {
         assertNull(memTerms);
       } else {
-        NumericDocValues normValues = competitor.simpleNormValues(field);
-        NumericDocValues memNormValues = memIndexReader.simpleNormValues(field);
+        NumericDocValues normValues = competitor.getNormValues(field);
+        NumericDocValues memNormValues = memIndexReader.getNormValues(field);
         if (normValues != null) {
           // mem idx always computes norms on the fly
           assertNotNull(memNormValues);
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java
index a7d8496..b71928c 100755
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java
@@ -62,7 +62,7 @@ public class NormValueSource extends ValueSource {
     if (similarity == null) {
       throw new UnsupportedOperationException("requires a TFIDFSimilarity (such as DefaultSimilarity)");
     }
-    final NumericDocValues norms = readerContext.reader().simpleNormValues(field);
+    final NumericDocValues norms = readerContext.reader().getNormValues(field);
 
     if (norms == null) {
       return new ConstDoubleDocValues(0.0, this);
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java b/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java
index 60eefda..a0a2521 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterAtomicReader.java
@@ -132,8 +132,8 @@ public final class FieldFilterAtomicReader extends FilterAtomicReader {
   }
 
   @Override
-  public NumericDocValues simpleNormValues(String field) throws IOException {
-    return hasField(field) ? super.simpleNormValues(field) : null;
+  public NumericDocValues getNormValues(String field) throws IOException {
+    return hasField(field) ? super.getNormValues(field) : null;
   }
 
   @Override
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java b/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
index 8c083fd..81c3370 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
@@ -28,7 +28,7 @@ import java.util.Random;
 import java.util.Set;
 
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.asserting.AssertingPostingsFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41Codec;
 import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
@@ -45,7 +45,7 @@ import org.apache.lucene.codecs.mocksep.MockSepPostingsFormat;
 import org.apache.lucene.codecs.nestedpulsing.NestedPulsingPostingsFormat;
 import org.apache.lucene.codecs.pulsing.Pulsing41PostingsFormat;
 import org.apache.lucene.codecs.simpletext.SimpleTextPostingsFormat;
-import org.apache.lucene.codecs.simpletext.SimpleTextSimpleDocValuesFormat;
+import org.apache.lucene.codecs.simpletext.SimpleTextDocValuesFormat;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
 
@@ -63,7 +63,7 @@ public class RandomCodec extends Lucene41Codec {
   private List<PostingsFormat> formats = new ArrayList<PostingsFormat>();
   
   /** Shuffled list of docvalues formats to use for new mappings */
-  private List<SimpleDocValuesFormat> dvFormats = new ArrayList<SimpleDocValuesFormat>();
+  private List<DocValuesFormat> dvFormats = new ArrayList<DocValuesFormat>();
   
   /** unique set of format names this codec knows about */
   public Set<String> formatNames = new HashSet<String>();
@@ -76,7 +76,7 @@ public class RandomCodec extends Lucene41Codec {
   // otherwise DWPT's .toString() calls that iterate over the map can 
   // cause concurrentmodificationexception if indexwriter's infostream is on
   private Map<String,PostingsFormat> previousMappings = Collections.synchronizedMap(new HashMap<String,PostingsFormat>());
-  private Map<String,SimpleDocValuesFormat> previousDVMappings = Collections.synchronizedMap(new HashMap<String,SimpleDocValuesFormat>());
+  private Map<String,DocValuesFormat> previousDVMappings = Collections.synchronizedMap(new HashMap<String,DocValuesFormat>());
   private final int perFieldSeed;
 
   @Override
@@ -96,11 +96,11 @@ public class RandomCodec extends Lucene41Codec {
   }
 
   @Override
-  public SimpleDocValuesFormat getDocValuesFormatForField(String name) {
-    SimpleDocValuesFormat codec = previousDVMappings.get(name);
+  public DocValuesFormat getDocValuesFormatForField(String name) {
+    DocValuesFormat codec = previousDVMappings.get(name);
     if (codec == null) {
       codec = dvFormats.get(Math.abs(perFieldSeed ^ name.hashCode()) % dvFormats.size());
-      if (codec instanceof SimpleTextSimpleDocValuesFormat && perFieldSeed % 5 != 0) {
+      if (codec instanceof SimpleTextDocValuesFormat && perFieldSeed % 5 != 0) {
         // make simpletext rarer, choose again
         codec = dvFormats.get(Math.abs(perFieldSeed ^ name.toUpperCase(Locale.ROOT).hashCode()) % dvFormats.size());
       }
@@ -143,7 +143,7 @@ public class RandomCodec extends Lucene41Codec {
     
     addDocValues(avoidCodecs,
         new DiskDocValuesFormat(),
-        new SimpleTextSimpleDocValuesFormat(),
+        new SimpleTextDocValuesFormat(),
         new MemoryDocValuesFormat());
 
     Collections.shuffle(formats, random);
@@ -167,8 +167,8 @@ public class RandomCodec extends Lucene41Codec {
     }
   }
   
-  private final void addDocValues(Set<String> avoidCodecs, SimpleDocValuesFormat... docvalues) {
-    for (SimpleDocValuesFormat d : docvalues) {
+  private final void addDocValues(Set<String> avoidCodecs, DocValuesFormat... docvalues) {
+    for (DocValuesFormat d : docvalues) {
       if (!avoidCodecs.contains(d.getName())) {
         dvFormats.add(d);
         dvFormatNames.add(d.getName());
diff --git a/solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java b/solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java
index 04ab0b3..e6e348f 100644
--- a/solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java
+++ b/solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java
@@ -36,7 +36,7 @@ import org.apache.lucene.analysis.util.TokenFilterFactory;
 import org.apache.lucene.analysis.util.TokenizerFactory;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SimpleDocValuesFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.analysis.util.WordlistLoader;
 import org.apache.solr.common.ResourceLoader;
 import org.apache.solr.handler.admin.CoreAdminHandler;
@@ -179,7 +179,7 @@ public class SolrResourceLoader implements ResourceLoader
   void reloadLuceneSPI() {
     // Codecs:
     PostingsFormat.reloadPostingsFormats(this.classLoader);
-    SimpleDocValuesFormat.reloadDocValuesFormats(this.classLoader);
+    DocValuesFormat.reloadDocValuesFormats(this.classLoader);
     Codec.reloadCodecs(this.classLoader);
     // Analysis:
     CharFilterFactory.reloadCharFilters(this.classLoader);
diff --git a/solr/core/src/test/org/apache/solr/search/TestDocSet.java b/solr/core/src/test/org/apache/solr/search/TestDocSet.java
index ff73212..c5769aa 100644
--- a/solr/core/src/test/org/apache/solr/search/TestDocSet.java
+++ b/solr/core/src/test/org/apache/solr/search/TestDocSet.java
@@ -403,7 +403,7 @@ public class TestDocSet extends LuceneTestCase {
       }
 
       @Override
-      public NumericDocValues simpleNormValues(String field) {
+      public NumericDocValues getNormValues(String field) {
         return null;
       }
 
diff --git a/solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java b/solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java
index 2489ca3..71efe81 100644
--- a/solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java
+++ b/solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java
@@ -333,9 +333,9 @@ public class DocumentBuilderTest extends SolrTestCaseJ4 {
 
       DefaultSimilarity sim = (DefaultSimilarity) searcher.getSimilarity();
       
-      NumericDocValues titleNorms = reader.simpleNormValues("title");
-      NumericDocValues fooNorms = reader.simpleNormValues("foo_t");
-      NumericDocValues textNorms =  reader.simpleNormValues("text");
+      NumericDocValues titleNorms = reader.getNormValues("title");
+      NumericDocValues fooNorms = reader.getNormValues("foo_t");
+      NumericDocValues textNorms =  reader.getNormValues("text");
 
       assertEquals(expectedNorm(sim, 2, TITLE_BOOST * DOC_BOOST),
                    titleNorms.get(docid));

