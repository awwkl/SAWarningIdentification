GitDiffStart: 400639f54e54ba2cf90b2436652450ded25861f7 | Sat May 7 13:14:38 2011 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 848977a..a8c19d6 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -472,11 +472,29 @@ Changes in backwards compatibility policy
   a method getHeapArray() was added to retrieve the internal heap array as a
   non-generic Object[].  (Uwe Schindler, Yonik Seeley)
 
+Changes in runtime behavior
+
+* LUCENE-3065: When a NumericField is retrieved from a Document loaded
+  from IndexReader (or IndexSearcher), it will now come back as
+  NumericField not as a Field with a string-ified version of the
+  numeric value you had indexed.  Note that this only applies for
+  newly-indexed Documents; older indices will still return Field
+  with the string-ified numeric value. If you call Document.get(),
+  the value comes still back as String, but Document.getFieldable()
+  returns NumericField instances. (Uwe Schindler, Ryan McKinley,
+  Mike McCandless)
+
 Optimizations
 
 * LUCENE-2990: ArrayUtil/CollectionUtil.*Sort() methods now exit early
   on empty or one-element lists/arrays.  (Uwe Schindler)
 
+API Changes
+
+* LUCENE-3065: Document.getField() was deprecated, as it throws
+  ClassCastException when loading lazy fields or NumericFields.
+  (Uwe Schindler, Ryan McKinley, Mike McCandless)
+
 Bug fixes
 
 * LUCENE-3024: Index with more than 2.1B terms was hitting AIOOBE when
diff --git a/lucene/docs/contributions.html b/lucene/docs/contributions.html
index cc67944..3544745 100644
--- a/lucene/docs/contributions.html
+++ b/lucene/docs/contributions.html
@@ -3,7 +3,7 @@
 <head>
 <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta content="Apache Forrest" name="Generator">
-<meta name="Forrest-version" content="0.8">
+<meta name="Forrest-version" content="0.9">
 <meta name="Forrest-skin-name" content="lucene">
 <title>
 	Apache Lucene - Contributions
@@ -275,7 +275,7 @@ document.write("Last Published: " + document.lastModified);
 <a href="#PDFTextStream -- PDF text and metadata extraction">PDFTextStream -- PDF text and metadata extraction</a>
 </li>
 <li>
-<a href="#PJ Classic & PJ Professional - PDF Document Conversion">PJ Classic &amp; PJ Professional - PDF Document Conversion</a>
+<a href="#PJ Classic &amp; PJ Professional - PDF Document Conversion">PJ Classic &amp; PJ Professional - PDF Document Conversion</a>
 </li>
 </ul>
 </li>
@@ -403,7 +403,7 @@ document.write("Last Published: " + document.lastModified);
                             URL
                         </th>
                         <td>
-                            <a href="http://marc.theaimsgroup.com/?l=lucene-dev&m=100723333506246&w=2">
+                            <a href="http://marc.theaimsgroup.com/?l=lucene-dev&amp;m=100723333506246&amp;w=2">
                                 http://marc.theaimsgroup.com/?l=lucene-dev&amp;m=100723333506246&amp;w=2
                             </a>
                         </td>
@@ -538,7 +538,7 @@ document.write("Last Published: " + document.lastModified);
 </tr>
                 
 </table>
-<a name="N10124"></a><a name="PJ Classic & PJ Professional - PDF Document Conversion"></a>
+<a name="N10124"></a><a name="PJ Classic &amp; PJ Professional - PDF Document Conversion"></a>
 <h3 class="boxed">PJ Classic &amp; PJ Professional - PDF Document Conversion</h3>
 <table class="ForrestTable" cellspacing="1" cellpadding="4">
                     
diff --git a/lucene/docs/contributions.pdf b/lucene/docs/contributions.pdf
index a7937c0..352a0ff 100644
--- a/lucene/docs/contributions.pdf
+++ b/lucene/docs/contributions.pdf
@@ -1,31 +1,31 @@
 Apache Lucene - Contributions
 
 Peter Carlson
-
 Table of contents
 
-   1 Overview............................................................................................................................2
-   2 Lucene Tools......................................................................................................................2
-    2.1 Luke...............................................................................................................................2
-    2.2 LIMO (Lucene Index Monitor)..................................................................................... 2
+   1 Overview............................................................................................................................ 2
+   2 Lucene Tools...................................................................................................................... 2
+
+     2.1 Luke.............................................................................................................................. 2
+    2.2 LIMO (Lucene Index Monitor).................................................................................... 2
    3 Lucene Document Converters............................................................................................2
-    3.1 XML Document #1........................................................................................................2
-    3.2 XML Document #2........................................................................................................2
-    3.3 PDF Box........................................................................................................................ 3
-    3.4 XPDF - PDF Document Conversion............................................................................. 3
-    3.5 PDFTextStream -- PDF text and metadata extraction...................................................3
-    3.6 PJ Classic & PJ Professional - PDF Document Conversion......................................... 3
-   4 Miscellaneous.....................................................................................................................3
-    4.1 Arabic Analyzer for Java...............................................................................................3
-    4.2 Phonetix.........................................................................................................................3
-    4.3 ejIndex - JBoss MBean for Lucene............................................................................... 3
-    4.4 JavaCC...........................................................................................................................4
-    4.5 LuSQL - Index databases with Lucene......................................................................... 4
-
-                   Copyright © 2006 The Apache Software Foundation. All rights reserved.
+     3.1 XML Document #1.......................................................................................................2
+     3.2 XML Document #2.......................................................................................................2
+     3.3 PDF Box....................................................................................................................... 3
+    3.4 XPDF - PDF Document Conversion............................................................................ 3
+    3.5 PDFTextStream -- PDF text and metadata extraction.................................................. 3
+    3.6 PJ Classic & PJ Professional - PDF Document Conversion.........................................3
+   4 Miscellaneous..................................................................................................................... 3
+     4.1 Arabic Analyzer for Java..............................................................................................3
+     4.2 Phonetix........................................................................................................................ 3
+    4.3 ejIndex - JBoss MBean for Lucene.............................................................................. 4
+     4.4 JavaCC.......................................................................................................................... 4
+    4.5 LuSQL - Index databases with Lucene........................................................................ 4
+
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
                                                                Apache Lucene - Contributions
 
-1. Overview
+1 Overview
 
 This page lists external Lucene resources. If you have written something that should be
 included, please post all relevant information to one of the mailing lists. Nothing listed here
@@ -35,98 +35,87 @@ this software, please use the author's contact information to get help.
 If you are looking for information on contributing patches or other improvements to Lucene,
 see How To Contribute on the Lucene Wiki.
 
-2. Lucene Tools
-
+2 Lucene Tools
 Software that works with Lucene indices.
 
-2.1. Luke
+2.1 Luke
 
-            URL                           http://www.getopt.org/luke/
-           author                         Andrzej Bialecki
+          URL                             http://www.getopt.org/luke/
+          author                          Andrzej Bialecki
 
-2.2. LIMO (Lucene Index Monitor)          http://limo.sf.net/
-                                          Julien Nioche
-                              URL
+2.2 LIMO (Lucene Index Monitor)           http://limo.sf.net/
+                              URL         Julien Nioche
                              author
 
-3. Lucene Document Converters
+3 Lucene Document Converters
 
-Lucene requires information you want to index to be converted into a Document class. Here
-are contributions for various solutions that convert different content types to Lucene's
+Lucene requires information you want to index to be converted into a Document class.
+Here are contributions for various solutions that convert different content types to Lucene's
 Document classes.
 
-3.1. XML Document #1                      http://marc.theaimsgroup.com/?l=lucene-dev&m=10072333350624
-                                          Philip Ogren - ogren@mayo.edu
-                              URL
-                             author
+3.1 XML Document #1                       http://marc.theaimsgroup.com/?l=lucene-
+                              URL         dev&m=100723333506246&w=2
 
-3.2. XML Document #2                      http://www.mail-archive.com/lucene-user@jakarta.apache.org/msg
-                                          Peter Carlson - carlson@bookandhammer.com
-                              URL
-                             author
+                             author       Philip Ogren - ogren@mayo.edu
 
-                                                                                  Page 2
+3.2 XML Document #2                       http://www.mail-archive.com/lucene-
+                              URL         user@jakarta.apache.org/msg00346.html
 
-           Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Contributions
+          Copyright © 2006 The Apache Software Foundation. All rights reserved.    Page 2
+                                                             Apache Lucene - Contributions
 
-3.3. PDF Box
+                 author              Peter Carlson - carlson@bookandhammer.com
 
-                URL                    http://www.pdfbox.org/
-               author                  Ben Litchfield - ben@csh.rit.edu
+3.3 PDF Box
 
-3.4. XPDF - PDF Document Conversion
+                 URL                 http://www.pdfbox.org/
+                 author              Ben Litchfield - ben@csh.rit.edu
 
-                  URL                  http://www.foolabs.com/xpdf
+3.4 XPDF - PDF Document Conversion   http://www.foolabs.com/xpdf
+                              URL    N/A
+                             author
 
-               author                  N/A
+3.5 PDFTextStream -- PDF text and metadata extraction
 
-3.5. PDFTextStream -- PDF text and metadata extraction
+                 URL                 http://snowtide.com
+                 author              N/A
 
-                  URL                  http://snowtide.com
+3.6 PJ Classic & PJ Professional - PDF Document Conversion
 
-               author                  N/A
+                 URL                 http://www.etymon.com/
+                 author              N/A
 
-3.6. PJ Classic & PJ Professional - PDF Document Conversion
+4 Miscellaneous
 
-                  URL                  http://www.etymon.com/
+4.1 Arabic Analyzer for Java         http://savannah.nongnu.org/projects/aramorph
+                              URL    Pierrick Brihaye
+                             author
+                                     http://www.companywebstore.de/tangentum/mirror/
+4.2 Phonetix                         en/products/phonetix/index.html
+                                     tangentum technologies
+                 URL
 
-               author                  N/A
+                 author
 
-4. Miscellaneous
+                 Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 3
+                                         Apache Lucene - Contributions
 
-4.1. Arabic Analyzer for Java          http://savannah.nongnu.org/projects/aramorph
-                                       Pierrick Brihaye
+4.3 ejIndex - JBoss MBean for Lucene     http://ejindex.sourceforge.net/
+                                         Andy Scholz
                               URL
                              author
 
-4.2. Phonetix
-
-                URL                    http://www.companywebstore.de/tangentum/mirror/en/products/pho
-               author                  tangentum technologies
-
-4.3. ejIndex - JBoss MBean for Lucene
-
-                URL                    http://ejindex.sourceforge.net/
-               author                  Andy Scholz
-
-Page 3
-
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-                                          Apache Lucene - Contributions
-
-4.4. JavaCC
-
-              URL    https://javacc.dev.java.net/
-             author  Sun Microsystems (java.net)
+4.4 JavaCC
 
-4.5. LuSQL - Index databases with Lucene
+            URL                          https://javacc.dev.java.net/
+            author                       Sun Microsystems (java.net)
 
-             URL     http://lab.cisti-icist.nrc-cnrc.gc.ca/cistilabswiki/index.php/LuSql
+4.5 LuSQL - Index databases with Lucene
 
-             author  Glen Newton
+            URL                          http://lab.cisti-icist.nrc-cnrc.gc.ca/cistilabswiki/
+            author                       index.php/LuSql
 
-                                                                       Page 4
+                                         Glen Newton
 
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
+            Copyright © 2006 The Apache Software Foundation. All rights reserved.              Page 4
 
\ No newline at end of file
diff --git a/lucene/docs/demo.html b/lucene/docs/demo.html
index 4c6a0be..7ddf92a 100644
--- a/lucene/docs/demo.html
+++ b/lucene/docs/demo.html
@@ -3,7 +3,7 @@
 <head>
 <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta content="Apache Forrest" name="Generator">
-<meta name="Forrest-version" content="0.8">
+<meta name="Forrest-version" content="0.9">
 <meta name="Forrest-skin-name" content="lucene">
 <title>
 	Apache Lucene - Building and Installing the Basic Demo
diff --git a/lucene/docs/demo.pdf b/lucene/docs/demo.pdf
index 122697a..48fff58 100644
--- a/lucene/docs/demo.pdf
+++ b/lucene/docs/demo.pdf
@@ -2,44 +2,43 @@ Apache Lucene - Building and
 Installing the Basic Demo
 
 Andrew C. Oliver
-
 Table of contents
 
    1 About this Document......................................................................................................... 2
-   2 About the Demo................................................................................................................. 2
+   2 About the Demo.................................................................................................................2
    3 Setting your CLASSPATH................................................................................................ 2
    4 Indexing Files.....................................................................................................................2
    5 About the code................................................................................................................... 2
 
-                   Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
                                                                                       Apache Lucene - Building and Installing the Basic Demo
 
-1. About this Document
+1 About this Document
 
 This document is intended as a "getting started" guide to using and running the Lucene
 demos. It walks you through some basic installation and configuration.
 
-2. About the Demo
+2 About the Demo
 
 The Lucene command-line demo code consists of an application that demonstrates various
 functionalities of Lucene and how you can add Lucene to your applications.
 
-3. Setting your CLASSPATH
+3 Setting your CLASSPATH
 
 First, you should download the latest Lucene distribution and then extract it to a working
 directory.
 You need three JARs: the Lucene JAR, the common analysis JAR, and the Lucene demo
 JAR. You should see the Lucene JAR file in the directory you created when you extracted
-the archive -- it should be named something like lucene-core-{version}.jar. You
-should also see files called lucene-analyzers-common-{version}.jar and
+the archive -- it should be named something like lucene-core-{version}.jar.
+You should also see files called lucene-analyzers-common-{version}.jar and
 lucene-demo-{version}.jar.
 Put all three of these files in your Java CLASSPATH.
 
-4. Indexing Files
+4 Indexing Files
 
 Once you've gotten this far you're probably itching to go. Let's build an index! Assuming
-you've set your CLASSPATH correctly, just type: java org.apache.lucene.demo.IndexFiles
--docs {path-to-lucene}/src This will produce a subdirectory called index which will contain
+you've set your CLASSPATH correctly, just type: java org.apache.lucene.demo.IndexFiles -
+docs {path-to-lucene}/src This will produce a subdirectory called index which will contain
 an index of all of the Lucene source code.
 To search the index type: java org.apache.lucene.demo.SearchFiles You'll be prompted for a
 query. Type in a swear word and press the enter key. You'll see that the Lucene developers
@@ -47,11 +46,9 @@ are very well mannered and get no results. Now try entering the word "string". T
 return a whole bunch of documents. The results will page at every tenth result and ask you
 whether you want more results.
 
-5. About the code...
+5 About the code...
 
 read on>>>
 
-                                                                       Page 2
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 2
 
\ No newline at end of file
diff --git a/lucene/docs/demo2.html b/lucene/docs/demo2.html
index 0d4791c..f5b568a 100644
--- a/lucene/docs/demo2.html
+++ b/lucene/docs/demo2.html
@@ -3,7 +3,7 @@
 <head>
 <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta content="Apache Forrest" name="Generator">
-<meta name="Forrest-version" content="0.8">
+<meta name="Forrest-version" content="0.9">
 <meta name="Forrest-skin-name" content="lucene">
 <title>
 	Apache Lucene - Basic Demo Sources Walk-through
diff --git a/lucene/docs/demo2.pdf b/lucene/docs/demo2.pdf
index a90050e..6be6790 100644
--- a/lucene/docs/demo2.pdf
+++ b/lucene/docs/demo2.pdf
@@ -2,45 +2,40 @@ Apache Lucene - Basic Demo Sources
 Walk-through
 
 Andrew C. Oliver
-
 Table of contents
 
    1 About the Code.................................................................................................................. 2
-   2 Location of the source........................................................................................................2
-   3 IndexFiles...........................................................................................................................2
-   4 Searching Files...................................................................................................................3
+   2 Location of the source....................................................................................................... 2
+   3 IndexFiles........................................................................................................................... 2
+   4 Searching Files................................................................................................................... 3
 
-                   Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Basic Demo Sources Walk-through
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                                                                           Apache Lucene - Basic Demo Sources Walk-through
 
-1. About the Code
+1 About the Code
 
 In this section we walk through the sources behind the command-line Lucene demo: where to
 find them, their parts and their function. This section is intended for Java developers wishing
 to understand how to use Lucene in their applications.
 
-2. Location of the source
+2 Location of the source
 
 NOTE: to examine the sources, you need to download and extract a source checkout of
 Lucene: (lucene-{version}-src.zip).
-
 Relative to the directory created when you extracted Lucene, you should see a directory
 called lucene/contrib/demo/. This is the root for the Lucene demo. Under this
 directory is src/java/org/apache/lucene/demo/. This is where all the Java
 sources for the demo live.
-
 Within this directory you should see the IndexFiles.java class we executed earlier.
 Bring it up in vi or your editor of choice and let's take a look at it.
 
-3. IndexFiles
+3 IndexFiles
 
 As we discussed in the previous walk-through, the IndexFiles class creates a Lucene Index.
 Let's take a look at how it does this.
-
 The main() method parses the command-line parameters, then in preparation for
 instantiating IndexWriter, opens a Directory and instantiates StandardAnalyzer and
 IndexWriterConfig.
-
 The value of the -index command-line parameter is the name of the filesystem directory
 where all index information should be stored. If IndexFiles is invoked with a relative
 path given in the -index command-line parameter, or if the -index command-line
@@ -48,26 +43,22 @@ parameter is not given, causing the default relative index path "index" to be us
 path will be created as a subdirectory of the current working directory (if it does not already
 exist). On some platforms, the index path may be created in a different directory (such as the
 user's home directory).
-
 The -docs command-line parameter value is the location of the directory containing files to
 be indexed.
-
 The -update command-line parameter tells IndexFiles not to delete the index if it
 already exists. When -update is not given, IndexFiles will first wipe the slate clean
 before indexing any documents.
 
-                                                                       Page 2
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Basic Demo Sources Walk-through
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 2
+                                                                                           Apache Lucene - Basic Demo Sources Walk-through
 
-Lucene Directorys are used by the IndexWriter to store information in the index. In
-addition to the FSDirectory implementation we are using, there are several other
+Lucene Directorys are used by the IndexWriter to store information in the index.
+In addition to the FSDirectory implementation we are using, there are several other
 Directory subclasses that can write to RAM, to databases, etc.
 
-Lucene Analyzers are processing pipelines that break up text into indexed tokens, a.k.a.
-terms, and optionally perform other operations on these tokens, e.g. downcasing, synonym
-insertion, filtering out unwanted tokens, etc. The Analyzer we are using is
+Lucene Analyzers are processing pipelines that break up text into indexed tokens,
+a.k.a. terms, and optionally perform other operations on these tokens, e.g. downcasing,
+synonym insertion, filtering out unwanted tokens, etc. The Analyzer we are using is
 StandardAnalyzer, which creates tokens using the Word Break rules from the Unicode
 Text Segmentation algorithm specified in Unicode Standard Annex #29; converts tokens to
 lowercase; and then filters out stopwords. Stopwords are common language words such as
@@ -77,20 +68,20 @@ for each. Lucene currently provides Analyzers for a number of different language
 javadocs under modules/analysis/common/src/java/org/apache/lucene/analysis).
 
 The IndexWriterConfig instance holds all configuration for IndexWriter. For
-example, we set the OpenMode to use here based on the value of the -update
-command-line parameter.
+example, we set the OpenMode to use here based on the value of the -update command-
+line parameter.
 
 Looking further down in the file, after IndexWriter is instantiated, you should see the
 indexDocs() code. This recursive function crawls the directories and creates Document
 objects. The Document is simply a data object to represent the text content from the file as
 well as its creation time and location. These instances are added to the IndexWriter. If
-the -update command-line parameter is given, the IndexWriter OpenMode will be set
-to OpenMode.CREATE_OR_APPEND, and rather than adding documents to the index, the
-IndexWriter will update them in the index by attempting to find an already-indexed
+the -update command-line parameter is given, the IndexWriter OpenMode will be
+set to OpenMode.CREATE_OR_APPEND, and rather than adding documents to the index,
+the IndexWriter will update them in the index by attempting to find an already-indexed
 document with the same identifier (in our case, the file path serves as the identifier); deleting
 it from the index if it exists; and then adding the new document to the index.
 
-4. Searching Files
+4 Searching Files
 
 The SearchFiles class is quite simple. It primarily collaborates with an IndexSearcher,
 StandardAnalyzer (which is used in the IndexFiles class as well) and a QueryParser. The
@@ -101,15 +92,11 @@ which is passed to the searcher. Note that it's also possible to programmaticall
 rich Query object without using the query parser. The query parser just enables decoding the
 Lucene query syntax into the corresponding Query object.
 
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 3
+                                                                                           Apache Lucene - Basic Demo Sources Walk-through
+
 SearchFiles uses the IndexSearcher.search(query,n) method that returns
 TopDocs with max n hits. The results are printed in pages, sorted by score (i.e. relevance).
 
-Page 3
-
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Basic Demo Sources Walk-through
-
-                                                                       Page 4
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 4
 
\ No newline at end of file
diff --git a/lucene/docs/fileformats.html b/lucene/docs/fileformats.html
index 88fe07f..a3b5ea7 100644
--- a/lucene/docs/fileformats.html
+++ b/lucene/docs/fileformats.html
@@ -3,7 +3,7 @@
 <head>
 <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta content="Apache Forrest" name="Generator">
-<meta name="Forrest-version" content="0.8">
+<meta name="Forrest-version" content="0.9">
 <meta name="Forrest-skin-name" content="lucene">
 <title>
             Apache Lucene - Index File Formats
@@ -429,10 +429,15 @@ document.write("Last Published: " + document.lastModified);
             Additionally segments track explicitly whether or
             not they have term vectors. See LUCENE-2811 for details.
            </p>
+<p>
+            In version 3.2, numeric fields are written as natively
+            to stored fields file, previously they were stored in
+            text format only.
+           </p>
 </div>
 
         
-<a name="N10037"></a><a name="Definitions"></a>
+<a name="N1003A"></a><a name="Definitions"></a>
 <h2 class="boxed">Definitions</h2>
 <div class="section">
 <p>
@@ -473,7 +478,7 @@ document.write("Last Published: " + document.lastModified);
                 strings, the first naming the field, and the second naming text
                 within the field.
             </p>
-<a name="N10057"></a><a name="Inverted Indexing"></a>
+<a name="N1005A"></a><a name="Inverted Indexing"></a>
 <h3 class="boxed">Inverted Indexing</h3>
 <p>
                     The index stores statistics about terms in order
@@ -483,7 +488,7 @@ document.write("Last Published: " + document.lastModified);
                     it.  This is the inverse of the natural relationship, in which
                     documents list terms.
                 </p>
-<a name="N10063"></a><a name="Types of Fields"></a>
+<a name="N10066"></a><a name="Types of Fields"></a>
 <h3 class="boxed">Types of Fields</h3>
 <p>
                     In Lucene, fields may be <i>stored</i>, in which
@@ -497,7 +502,7 @@ document.write("Last Published: " + document.lastModified);
                     to be indexed literally.
                 </p>
 <p>See the <a href="api/core/org/apache/lucene/document/Field.html">Field</a> java docs for more information on Fields.</p>
-<a name="N10080"></a><a name="Segments"></a>
+<a name="N10083"></a><a name="Segments"></a>
 <h3 class="boxed">Segments</h3>
 <p>
                     Lucene indexes may be composed of multiple sub-indexes, or
@@ -523,7 +528,7 @@ document.write("Last Published: " + document.lastModified);
                     Searches may involve multiple segments and/or multiple indexes, each
                     index potentially composed of a set of segments.
                 </p>
-<a name="N1009E"></a><a name="Document Numbers"></a>
+<a name="N100A1"></a><a name="Document Numbers"></a>
 <h3 class="boxed">Document Numbers</h3>
 <p>
                     Internally, Lucene refers to documents by an integer <i>document
@@ -578,7 +583,7 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N100C5"></a><a name="Overview"></a>
+<a name="N100C8"></a><a name="Overview"></a>
 <h2 class="boxed">Overview</h2>
 <div class="section">
 <p>
@@ -677,7 +682,7 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N10108"></a><a name="File Naming"></a>
+<a name="N1010B"></a><a name="File Naming"></a>
 <h2 class="boxed">File Naming</h2>
 <div class="section">
 <p>
@@ -704,7 +709,7 @@ document.write("Last Published: " + document.lastModified);
             </p>
 </div>
       
-<a name="N10117"></a><a name="file-names"></a>
+<a name="N1011A"></a><a name="file-names"></a>
 <h2 class="boxed">Summary of File Extensions</h2>
 <div class="section">
 <p>The following table summarizes the names and extensions of the files in Lucene:
@@ -846,10 +851,10 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N10201"></a><a name="Primitive Types"></a>
+<a name="N10204"></a><a name="Primitive Types"></a>
 <h2 class="boxed">Primitive Types</h2>
 <div class="section">
-<a name="N10206"></a><a name="Byte"></a>
+<a name="N10209"></a><a name="Byte"></a>
 <h3 class="boxed">Byte</h3>
 <p>
                     The most primitive type
@@ -857,7 +862,7 @@ document.write("Last Published: " + document.lastModified);
                     other data types are defined as sequences
                     of bytes, so file formats are byte-order independent.
                 </p>
-<a name="N1020F"></a><a name="UInt32"></a>
+<a name="N10212"></a><a name="UInt32"></a>
 <h3 class="boxed">UInt32</h3>
 <p>
                     32-bit unsigned integers are written as four
@@ -867,7 +872,7 @@ document.write("Last Published: " + document.lastModified);
                     UInt32    --&gt; &lt;Byte&gt;<sup>4</sup>
                 
 </p>
-<a name="N1021E"></a><a name="Uint64"></a>
+<a name="N10221"></a><a name="Uint64"></a>
 <h3 class="boxed">Uint64</h3>
 <p>
                     64-bit unsigned integers are written as eight
@@ -876,7 +881,7 @@ document.write("Last Published: " + document.lastModified);
 <p>UInt64    --&gt; &lt;Byte&gt;<sup>8</sup>
                 
 </p>
-<a name="N1022D"></a><a name="VInt"></a>
+<a name="N10230"></a><a name="VInt"></a>
 <h3 class="boxed">VInt</h3>
 <p>
                     A variable-length format for positive integers is
@@ -1426,13 +1431,13 @@ document.write("Last Published: " + document.lastModified);
                     This provides compression while still being
                     efficient to decode.
                 </p>
-<a name="N10512"></a><a name="Chars"></a>
+<a name="N10515"></a><a name="Chars"></a>
 <h3 class="boxed">Chars</h3>
 <p>
                     Lucene writes unicode
                     character sequences as UTF-8 encoded bytes.
                 </p>
-<a name="N1051B"></a><a name="String"></a>
+<a name="N1051E"></a><a name="String"></a>
 <h3 class="boxed">String</h3>
 <p>
 		    Lucene writes strings as UTF-8 encoded bytes.
@@ -1445,10 +1450,10 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N10528"></a><a name="Compound Types"></a>
+<a name="N1052B"></a><a name="Compound Types"></a>
 <h2 class="boxed">Compound Types</h2>
 <div class="section">
-<a name="N1052D"></a><a name="MapStringString"></a>
+<a name="N10530"></a><a name="MapStringString"></a>
 <h3 class="boxed">Map&lt;String,String&gt;</h3>
 <p>
 		    In a couple places Lucene stores a Map
@@ -1461,13 +1466,13 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N1053D"></a><a name="Per-Index Files"></a>
+<a name="N10540"></a><a name="Per-Index Files"></a>
 <h2 class="boxed">Per-Index Files</h2>
 <div class="section">
 <p>
                 The files in this section exist one-per-index.
             </p>
-<a name="N10545"></a><a name="Segments File"></a>
+<a name="N10548"></a><a name="Segments File"></a>
 <h3 class="boxed">Segments File</h3>
 <p>
                     The active segments in the index are stored in the
@@ -1640,7 +1645,7 @@ document.write("Last Published: " + document.lastModified);
 <p> HasVectors is 1 if this segment stores term vectors,
             else it's 0.
                 </p>
-<a name="N105D0"></a><a name="Lock File"></a>
+<a name="N105D3"></a><a name="Lock File"></a>
 <h3 class="boxed">Lock File</h3>
 <p>
                     The write lock, which is stored in the index
@@ -1654,14 +1659,14 @@ document.write("Last Published: " + document.lastModified);
                     documents).  This lock file ensures that only one
                     writer is modifying the index at a time.
                 </p>
-<a name="N105D9"></a><a name="Deletable File"></a>
+<a name="N105DC"></a><a name="Deletable File"></a>
 <h3 class="boxed">Deletable File</h3>
 <p>
                     A writer dynamically computes
                     the files that are deletable, instead, so no file
                     is written.
                 </p>
-<a name="N105E2"></a><a name="Compound Files"></a>
+<a name="N105E5"></a><a name="Compound Files"></a>
 <h3 class="boxed">Compound Files</h3>
 <p>Starting with Lucene 1.4 the compound file format became default. This
                     is simply a container for all files described in the next section
@@ -1688,14 +1693,14 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N1060A"></a><a name="Per-Segment Files"></a>
+<a name="N1060D"></a><a name="Per-Segment Files"></a>
 <h2 class="boxed">Per-Segment Files</h2>
 <div class="section">
 <p>
                 The remaining files are all per-segment, and are
                 thus defined by suffix.
             </p>
-<a name="N10612"></a><a name="Fields"></a>
+<a name="N10615"></a><a name="Fields"></a>
 <h3 class="boxed">Fields</h3>
 <p>
                     
@@ -1868,13 +1873,29 @@ document.write("Last Published: " + document.lastModified);
 <li>third bit is one for fields with compression option enabled
                                     (if compression is enabled, the algorithm used is ZLIB),
                                     only available for indexes until Lucene version 2.9.x</li>
+                                
+<li>4th to 6th bits (mask: 0x7&lt;&lt;3) define the type of a
+                                numeric field: <ul>
+                                  
+<li>all bits in mask are cleared if no numeric field at all</li>
+                                  
+<li>1&lt;&lt;3: Value is Int</li>
+                                  
+<li>2&lt;&lt;3: Value is Long</li>
+                                  
+<li>3&lt;&lt;3: Value is Int as Float (as of Integer.intBitsToFloat)</li>
+                                  
+<li>4&lt;&lt;3: Value is Long as Double (as of Double.longBitsToDouble)</li>
+                                
+</ul>
+</li>
                             
 </ul>
                         
 </p>
                         
 <p>Value --&gt;
-                            String | BinaryValue (depending on Bits)
+                            String | BinaryValue | Int | Long (depending on Bits)
                         </p>
                         
 <p>BinaryValue --&gt;
@@ -1889,7 +1910,7 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N106B9"></a><a name="Term Dictionary"></a>
+<a name="N106D0"></a><a name="Term Dictionary"></a>
 <h3 class="boxed">Term Dictionary</h3>
 <p>
                     The term dictionary is represented as two files:
@@ -2081,7 +2102,7 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N1073D"></a><a name="Frequencies"></a>
+<a name="N10754"></a><a name="Frequencies"></a>
 <h3 class="boxed">Frequencies</h3>
 <p>
                     The .frq file contains the lists of documents
@@ -2209,7 +2230,7 @@ document.write("Last Published: " + document.lastModified);
                    entry in level-1. In the example has entry 15 on level 1 a pointer to entry 15 on level 0 and entry 31 on level 1 a pointer
                    to entry 31 on level 0.                   
                 </p>
-<a name="N107C5"></a><a name="Positions"></a>
+<a name="N107DC"></a><a name="Positions"></a>
 <h3 class="boxed">Positions</h3>
 <p>
                     The .prx file contains the lists of positions that
@@ -2279,7 +2300,7 @@ document.write("Last Published: " + document.lastModified);
                     Payload. If PayloadLength is not stored, then this Payload has the same
                     length as the Payload at the previous position.
                 </p>
-<a name="N10801"></a><a name="Normalization Factors"></a>
+<a name="N10818"></a><a name="Normalization Factors"></a>
 <h3 class="boxed">Normalization Factors</h3>
 <p>There's a single .nrm file containing all norms:
                 </p>
@@ -2359,7 +2380,7 @@ document.write("Last Published: " + document.lastModified);
                 </p>
 <p>Separate norm files are created (when adequate) for both compound and non compound segments.
                 </p>
-<a name="N10852"></a><a name="Term Vectors"></a>
+<a name="N10869"></a><a name="Term Vectors"></a>
 <h3 class="boxed">Term Vectors</h3>
 <p>
 		  Term Vector support is an optional on a field by
@@ -2495,7 +2516,7 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N108EE"></a><a name="Deleted Documents"></a>
+<a name="N10905"></a><a name="Deleted Documents"></a>
 <h3 class="boxed">Deleted Documents</h3>
 <p>The .del file is
                     optional, and only exists when a segment contains deletions.
@@ -2559,7 +2580,7 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N10928"></a><a name="Limitations"></a>
+<a name="N1093F"></a><a name="Limitations"></a>
 <h2 class="boxed">Limitations</h2>
 <div class="section">
 <p>
diff --git a/lucene/docs/fileformats.pdf b/lucene/docs/fileformats.pdf
index 366927b..8acadfd 100644
--- a/lucene/docs/fileformats.pdf
+++ b/lucene/docs/fileformats.pdf
@@ -1,50 +1,48 @@
 Apache Lucene - Index File Formats
 
 Table of contents
-
    1 Index File Formats............................................................................................................. 3
-   2 Definitions..........................................................................................................................4
-    2.1 Inverted Indexing...........................................................................................................4
-    2.2 Types of Fields.............................................................................................................. 4
-    2.3 Segments........................................................................................................................4
-    2.4 Document Numbers.......................................................................................................5
-   3 Overview............................................................................................................................5
-   4 File Naming....................................................................................................................... 6
-   5 Summary of File Extensions.............................................................................................. 6
+   2 Definitions.......................................................................................................................... 4
+     2.1 Inverted Indexing.......................................................................................................... 4
+     2.2 Types of Fields............................................................................................................. 4
+     2.3 Segments....................................................................................................................... 4
+     2.4 Document Numbers...................................................................................................... 5
+   3 Overview............................................................................................................................ 5
+   4 File Naming........................................................................................................................6
+   5 Summary of File Extensions..............................................................................................6
    6 Primitive Types.................................................................................................................. 7
-    6.1 Byte................................................................................................................................7
-    6.2 UInt32............................................................................................................................8
-    6.3 Uint64............................................................................................................................ 8
-    6.4 VInt................................................................................................................................8
-    6.5 Chars..............................................................................................................................9
-    6.6 String............................................................................................................................. 9
+     6.1 Byte............................................................................................................................... 7
+     6.2 UInt32........................................................................................................................... 7
+     6.3 Uint64............................................................................................................................7
+     6.4 VInt............................................................................................................................... 8
+     6.5 Chars............................................................................................................................. 8
+     6.6 String.............................................................................................................................8
    7 Compound Types............................................................................................................... 9
-    7.1 Map<String,String>.......................................................................................................9
+     7.1 Map<String,String>...................................................................................................... 9
    8 Per-Index Files................................................................................................................... 9
-    8.1 Segments File................................................................................................................ 9
-    8.2 Lock File......................................................................................................................11
-    8.3 Deletable File...............................................................................................................12
-    8.4 Compound Files...........................................................................................................12
-   9 Per-Segment Files............................................................................................................ 12
-
-                   Copyright © 2006 The Apache Software Foundation. All rights reserved.
-                                                                                                            Apache Lucene - Index File Formats
-
- 9.1 Fields........................................................................................................................... 12
- 9.2 Term Dictionary.......................................................................................................... 14
- 9.3 Frequencies..................................................................................................................16
- 9.4 Positions...................................................................................................................... 18
- 9.5 Normalization Factors................................................................................................. 18
- 9.6 Term Vectors............................................................................................................... 19
- 9.7 Deleted Documents..................................................................................................... 21
-10 Limitations..................................................................................................................... 22
-
-                                                                       Page 2
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
+     8.1 Segments File................................................................................................................9
+     8.2 Lock File.....................................................................................................................11
+     8.3 Deletable File..............................................................................................................11
+     8.4 Compound Files.......................................................................................................... 11
+
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                                                                                             Apache Lucene - Index File Formats
+
+9 Per-Segment Files............................................................................................................ 11
+  9.1 Fields...........................................................................................................................12
+  9.2 Term Dictionary..........................................................................................................13
+  9.3 Frequencies................................................................................................................. 15
+  9.4 Positions...................................................................................................................... 17
+  9.5 Normalization Factors.................................................................................................17
+  9.6 Term Vectors.............................................................................................................. 18
+  9.7 Deleted Documents..................................................................................................... 20
+
+10 Limitations...................................................................................................................... 20
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 2
 Apache Lucene - Index File Formats
 
-1. Index File Formats
+1 Index File Formats
 
 This document defines the index file formats used in Lucene version 4.0. If you are using a
 different version of Lucene, please consult the copy of docs/fileformats.html that
@@ -83,24 +81,26 @@ file. See issue LUCENE-1382 for details. Also, diagnostics were added to each se
 written recording details about why it was written (due to flush, merge; which OS/JRE was
 used; etc.). See issue LUCENE-1654 for details.
 
-In version 3.0, compressed fields are no longer written to the index (they can still be read, but
-on merge the new segment will write them, uncompressed). See issue LUCENE-1960 for
+In version 3.0, compressed fields are no longer written to the index (they can still be read,
+but on merge the new segment will write them, uncompressed). See issue LUCENE-1960 for
 details.
 
-Page 3
-
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 In version 3.1, segments records the code version that created them. See LUCENE-2720 for
 details. Additionally segments track explicitly whether or not they have term vectors. See
 LUCENE-2811 for details.
 
-2. Definitions
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 3
+Apache Lucene - Index File Formats
+
+In version 3.2, numeric fields are written as natively to stored fields file, previously they
+were stored in text format only.
+
+2 Definitions
 
 The fundamental concepts in Lucene are index, document, field and term.
 
 An index contains a sequence of documents.
+
 ?? A document is a sequence of fields.
 ?? A field is a named sequence of terms.
 ?? A term is a string.
@@ -109,14 +109,14 @@ The same string in two different fields is considered a different term. Thus ter
 represented as a pair of strings, the first naming the field, and the second naming text within
 the field.
 
-2.1. Inverted Indexing
+2.1 Inverted Indexing
 
 The index stores statistics about terms in order to make term-based search more efficient.
-Lucene's index falls into the family of indexes known as an inverted index. This is because it
-can list, for a term, the documents that contain it. This is the inverse of the natural
+Lucene's index falls into the family of indexes known as an inverted index. This is because
+it can list, for a term, the documents that contain it. This is the inverse of the natural
 relationship, in which documents list terms.
 
-2.2. Types of Fields
+2.2 Types of Fields
 
 In Lucene, fields may be stored, in which case their text is stored in the index literally, in a
 non-inverted manner. Fields that are inverted are called indexed. A field may be both stored
@@ -128,23 +128,20 @@ certain identifier fields to be indexed literally.
 
 See the Field java docs for more information on Fields.
 
-2.3. Segments
+2.3 Segments
 
 Lucene indexes may be composed of multiple sub-indexes, or segments. Each segment is a
 fully independent index, which could be searched separately. Indexes evolve by:
 1. Creating new segments for newly added documents.
-
-                                                                       Page 4
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 2. Merging existing segments.
 
 Searches may involve multiple segments and/or multiple indexes, each index potentially
 composed of a set of segments.
 
-2.4. Document Numbers
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 4
+Apache Lucene - Index File Formats
+
+2.4 Document Numbers
 
 Internally, Lucene refers to documents by an integer document number. The first document
 added to an index is numbered zero, and each subsequent document added gets a number one
@@ -152,23 +149,25 @@ greater than the previous.
 
 Note that a document's number may change, so caution should be taken when storing these
 numbers outside of Lucene. In particular, numbers may change in the following situations:
-?? The numbers stored in each segment are unique only within the segment, and must be
-
-    converted before they can be used in a larger context. The standard technique is to
-    allocate each segment a range of values, based on the range of numbers used in that
-    segment. To convert a document number from a segment to an external value, the
-    segment's base document number is added. To convert an external value back to a
-    segment-specific value, the segment is identified by the range that the external value is in,
-    and the segment's base value is subtracted. For example two five document segments
+
+?? The numbers stored in each segment are unique only within the segment, and must
+    be converted before they can be used in a larger context. The standard technique is
+    to allocate each segment a range of values, based on the range of numbers used in
+    that segment. To convert a document number from a segment to an external value,
+    the segment's base document number is added. To convert an external value back to a
+    segment-specific value, the segment is identified by the range that the external value is
+    in, and the segment's base value is subtracted. For example two five document segments
     might be combined, so that the first segment has a base value of zero, and the second of
     five. Document three from the second segment would have an external value of eight.
+
 ?? When documents are deleted, gaps are created in the numbering. These are eventually
     removed as the index evolves through merging. Deleted documents are dropped when
     segments are merged. A freshly-merged segment thus has no gaps in its numbering.
 
-3. Overview
+3 Overview
 
 Each segment index maintains the following:
+
 ?? Field names. This contains the set of field names used in the index.
 ?? Stored Field values. This contains, for each document, a list of attribute-value pairs,
 
@@ -179,22 +178,17 @@ Each segment index maintains the following:
 ?? Term dictionary. A dictionary containing all of the terms used in all of the indexed fields
     of all of the documents. The dictionary also contains the number of documents which
     contain the term, and pointers to the term's frequency and proximity data.
-
-Page 5
-
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-                                           Apache Lucene - Index File Formats
-
 ?? Term Frequency data. For each term in the dictionary, the numbers of all the documents
     that contain that term, and the frequency of the term in that document if omitTf is false.
-
-?? Term Proximity data. For each term in the dictionary, the positions that the term occurs in
-    each document. Note that this will not exist if all fields in all documents set omitTf to
+?? Term Proximity data. For each term in the dictionary, the positions that the term occurs
+    in each document. Note that this will not exist if all fields in all documents set omitTf to
     true.
-
 ?? Normalization factors. For each field in each document, a value is stored that is
     multiplied into the score for hits on that field.
 
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 5
+                                               Apache Lucene - Index File Formats
+
 ?? Term Vectors. For each field in each document, the term vector (sometimes called
     document vector) may be stored. A term vector consists of term text and term frequency.
     To add Term Vectors to your index see the Field constructors
@@ -203,7 +197,7 @@ Page 5
 
 Details on each of these are provided in subsequent sections.
 
-4. File Naming
+4 File Naming
 
 All files belonging to a segment have the same name with varying extensions. The extensions
 correspond to the different file formats described below. When using the Compound File
@@ -219,87 +213,78 @@ never before used filename. This is achieved using a simple generations approach
 example, the first segments file is segments_1, then segments_2, etc. The generation is a
 sequential long integer represented in alpha-numeric (base 36) form.
 
-5. Summary of File Extensions
+5 Summary of File Extensions
 
 The following table summarizes the names and extensions of the files in Lucene:
 
-           Name                Extension   Brief Description
-
-Segments File    segments.gen, segments_N  Stores information about
-                                           segments
+               Name              Extension     Brief Description
 
-Lock File        write.lock                The Write lock prevents
-                                           multiple IndexWriters from
-                                           writing to the same file.
+Segments File        segments.gen, segments_N  Stores information about segments
 
-                                                                                  Page 6
+Lock File            write.lock                The Write lock prevents multiple
+                                               IndexWriters from writing to the
+                                               same file.
 
-           Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
+Compound File        .cfs                      An optional "virtual" file
+                                               consisting of all the other index
+                                               files for systems that frequently
+                                               run out of file handles.
 
-Compound File                       .cfs  An optional "virtual" file
-                                          consisting of all the other index
-Fields                              .fnm  files for systems that frequently
-Field Index                         .fdx  run out of file handles.
-Field Data                          .fdt
-Term Infos                          .tis  Stores information about the
-Term Info Index                     .tii  fields
-Frequencies                         .frq
-                                          Contains pointers to field data
-Positions                           .prx
-                                          The stored fields for documents
-Norms                               .nrm
-Term Vector Index                   .tvx  Part of the term dictionary,
-Term Vector Documents               .tvd  stores term info
+Fields               .fnm                      Stores information about the fields
 
-Term Vector Fields                  .tvf  The index into the Term Infos
-Deleted Documents                   .del  file
+Field Index          .fdx                      Contains pointers to field data
 
-                                          Contains the list of docs which
-                                          contain each term along with
-                                          frequency
+Field Data           .fdt                      The stored fields for documents
 
-                                          Stores position information
-                                          about where a term occurs in
-                                          the index
+Term Infos           .tis                      Part of the term dictionary, stores
+                                               term info
 
-                                          Encodes length and boost
-                                          factors for docs and fields
+                     Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 6
+                                                 Apache Lucene - Index File Formats
 
-                                          Stores offset into the document
-                                          data file
+                 Name                 Extension            Brief Description
+Term Info Index        .tii
+Frequencies            .frq                      The index into the Term Infos file
 
-                                          Contains information about
-                                          each document that has term
-                                          vectors
+Positions              .prx                      Contains the list of docs which
+Norms                  .nrm                      contain each term along with
+Term Vector Index      .tvx                      frequency
+Term Vector Documents  .tvd
+Term Vector Fields     .tvf                      Stores position information about
+Deleted Documents      .del                      where a term occurs in the index
 
-                                          The field level info about term
-                                          vectors
+                                                 Encodes length and boost factors
+                                                 for docs and fields
 
-                                          Info about what files are
-                                          deleted
+                                                 Stores offset into the document
+                                                 data file
 
-6. Primitive Types
+                                                 Contains information about each
+                                                 document that has term vectors
 
-6.1. Byte
-The most primitive type is an eight-bit byte. Files are accessed as sequences of bytes. All
+                                                 The field level info about term
+                                                 vectors
 
-Page 7
+                                                 Info about what files are deleted
 
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-                                Apache Lucene - Index File Formats
+6 Primitive Types
 
+6.1 Byte
+The most primitive type is an eight-bit byte. Files are accessed as sequences of bytes. All
 other data types are defined as sequences of bytes, so file formats are byte-order independent.
 
-6.2. UInt32
+6.2 UInt32
 32-bit unsigned integers are written as four bytes, high-order bytes first.
 UInt32 --> <Byte>4
 
-6.3. Uint64
+6.3 Uint64
 64-bit unsigned integers are written as eight bytes, high-order bytes first.
 UInt64 --> <Byte>8
 
-6.4. VInt
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 7
+                                                                  Apache Lucene - Index File Formats
+
+6.4 VInt
 
 A variable-length format for positive integers is defined where the high-order bit of each byte
 indicates whether more bytes remain to be read. The low-order seven bits are appended as
@@ -309,7 +294,7 @@ on.
 
 VInt Encoding Example
 
-Value  First byte  Second byte  Third byte
+Value     First byte  Second byte                                  Third byte
 
 0 00000000
 
@@ -321,60 +306,50 @@ Value  First byte  Second byte  Third byte
 
 127 01111111
 
-128    10000000    00000001
+128       10000000    00000001
 
-129    10000001    00000001
+129       10000001    00000001
 
-130    10000010    00000001
-
-                                                                              Page 8
-
-       Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
+130       10000010    00000001
 
 ...
 
-16,383  11111111                                01111111
+16,383    11111111    01111111
 
-16,384  10000000                                10000000           00000001
+16,384    10000000    10000000                                     00000001
 
-16,385  10000001                                10000000           00000001
+16,385    10000001    10000000                                     00000001
 
- ...
+...
 
 This provides compression while still being efficient to decode.
 
-6.5. Chars
+6.5 Chars
 Lucene writes unicode character sequences as UTF-8 encoded bytes.
 
-6.6. String
+6.6 String
 
 Lucene writes strings as UTF-8 encoded bytes. First the length, in bytes, is written as a VInt,
 followed by the bytes.
-
 String --> VInt, Chars
 
-7. Compound Types
+          Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 8
+                                                        Apache Lucene - Index File Formats
 
-7.1. Map<String,String>
+7 Compound Types
+
+7.1 Map<String,String>
 In a couple places Lucene stores a Map String->String.
 Map<String,String> --> Count<String,String>Count
 
-8. Per-Index Files
-
+8 Per-Index Files
 The files in this section exist one-per-index.
 
-8.1. Segments File
+8.1 Segments File
 
 The active segments in the index are stored in the segment info file, segments_N. There may
 be one or more segments_N files in the index; however, the one with the largest generation is
 the active one (when older segments_N files are present it's because they temporarily cannot
-
-Page 9
-
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 be deleted, or, a writer is in the process of committing, or a custom IndexDeletionPolicy is in
 use). This file lists each segment by name, has details about the separate norms and deletion
 files, and also contains the size of each segment.
@@ -405,6 +380,9 @@ Int8
 
 CommitUserData --> Map<String,String>
 
+                   Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 9
+                                                                                                                 Apache Lucene - Index File Formats
+
 Format is -9 (SegmentInfos.FORMAT_DIAGNOSTICS).
 
 Version counts how often the index has been changed by adding or deleting documents.
@@ -422,11 +400,6 @@ DelGen is the generation count of the separate deletes file. If this is -1, ther
 deletes. If it is 0, this is a pre-2.1 segment and you must check filesystem for the existence of
 _X.del. Anything above zero means there are separate deletes (_X_N.del).
 
-                                                                       Page 10
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 NumField is the size of the array for NormGen, or -1 if there are no NormGens stored.
 
 NormGen records the generation of the separate norms files. If NumField is -1, there are no
@@ -442,8 +415,8 @@ If HasSingleNormFile is 1, then the field norms are written as a single joined f
 extension .nrm); if it is 0 then each field's norms are stored as separate .fN files. See
 "Normalization Factors" below for details.
 
-DocStoreOffset, DocStoreSegment, DocStoreIsCompoundFile: If DocStoreOffset is -1, this
-segment has its own doc store (stored fields values and term vectors) files and
+DocStoreOffset, DocStoreSegment, DocStoreIsCompoundFile: If DocStoreOffset is
+-1, this segment has its own doc store (stored fields values and term vectors) files and
 DocStoreSegment and DocStoreIsCompoundFile are not stored. In this case all files for
 stored field values (*.fdt and *.fdx) and term vectors (*.tvf, *.tvd and *.tvx) will be stored
 with this segment. Otherwise, DocStoreSegment is the name of the segment that has the
@@ -459,6 +432,9 @@ DeletionCount records the number of deleted documents in this segment.
 
 HasProx is 1 if any fields in this segment have omitTf set to false; else, it's 0.
 
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 10
+Apache Lucene - Index File Formats
+
 CommitUserData stores an optional user-supplied opaque Map<String,String> that was
 passed to IndexWriter's commit or prepareCommit, or IndexReader's flush methods.
 
@@ -468,25 +444,20 @@ why the segment was created (merge, flush, addIndexes), etc.
 
 HasVectors is 1 if this segment stores term vectors, else it's 0.
 
-8.2. Lock File
-
-The write lock, which is stored in the index directory by default, is named "write.lock". If the
-
-Page 11
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
+8.2 Lock File
 
-lock directory is different from the index directory then the write lock will be named
+The write lock, which is stored in the index directory by default, is named "write.lock". If
+the lock directory is different from the index directory then the write lock will be named
 "XXXX-write.lock" where XXXX is a unique prefix derived from the full path to the index
 directory. When this file is present, a writer is currently modifying the index (adding or
 removing documents). This lock file ensures that only one writer is modifying the index at a
 time.
 
-8.3. Deletable File
+8.3 Deletable File
 A writer dynamically computes the files that are deletable, instead, so no file is written.
 
-8.4. Compound Files
+8.4 Compound Files
+
 Starting with Lucene 1.4 the compound file format became default. This is simply a container
 for all files described in the next section (except for the .del file).
 Compound (.cfs) --> FileCount, <DataOffset, FileName> FileCount , FileData FileCount
@@ -500,26 +471,20 @@ in a single set of files for more than one segment. When compound file is enable
 shared files will be added into a single compound file (same format as above) but with the
 extension .cfx.
 
-9. Per-Segment Files
-
+9 Per-Segment Files
 The remaining files are all per-segment, and are thus defined by suffix.
 
-9.1. Fields
+Copyright © 2006 The Apache Software Foundation. All rights reserved.     Page 11
+Apache Lucene - Index File Formats
+
+9.1 Fields
+
 Field Info
 Field names are stored in the field info file, with suffix .fnm.
 FieldInfos (.fnm) --> FNMVersion,FieldsCount, <FieldName, FieldBits> FieldsCount
-
-                                                                       Page 12
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 FNMVersion, FieldsCount --> VInt
-
 FieldName --> String
-
 FieldBits --> Byte
-
 ?? The low-order bit is one for indexed fields, and zero for non-indexed fields.
 ?? The second lowest-order bit is one for fields that have term vectors stored, and zero for
 
@@ -530,93 +495,73 @@ FieldBits --> Byte
 ?? If the sixth lowest-order bit is set (0x20), payloads are stored for the indexed field.
 
 FNMVersion (added in 2.9) is always -2.
-
-Fields are numbered by their order in this file. Thus field zero is the first field in the file, field
-one the next, and so on. Note that, like document numbers, field numbers are segment
+Fields are numbered by their order in this file. Thus field zero is the first field in the file,
+field one the next, and so on. Note that, like document numbers, field numbers are segment
 relative.
-
 Stored Fields
-
 Stored fields are represented by two files:
 1. The field index, or .fdx file.
 
     This contains, for each document, a pointer to its field data, as follows:
-
     FieldIndex (.fdx) --> <FieldValuesPosition> SegSize
-
     FieldValuesPosition --> Uint64
-
     This is used to find the location within the field data file of the fields of a particular
     document. Because it contains fixed-length data, this file may be easily randomly
     accessed. The position of document n 's field data is the Uint64 at n*8 in this file.
 2. The field data, or .fdt file.
-
     This contains the stored fields of each document, as follows:
-
     FieldData (.fdt) --> <DocFieldData> SegSize
-
     DocFieldData --> FieldCount, <FieldNum, Bits, Value> FieldCount
-
     FieldCount --> VInt
 
-    FieldNum --> VInt
-
-Page 13
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 12
+                                                                                                                 Apache Lucene - Index File Formats
 
+    FieldNum --> VInt
     Bits --> Byte
-
     ?? low order bit is one for tokenized fields
     ?? second bit is one for fields containing binary data
     ?? third bit is one for fields with compression option enabled (if compression is enabled,
 
          the algorithm used is ZLIB), only available for indexes until Lucene version 2.9.x
+    ?? 4th to 6th bits (mask: 0x7<<3) define the type of a numeric field:
 
-    Value --> String | BinaryValue (depending on Bits)
+         ?? all bits in mask are cleared if no numeric field at all
+         ?? 1<<3: Value is Int
+         ?? 2<<3: Value is Long
+         ?? 3<<3: Value is Int as Float (as of Integer.intBitsToFloat)
+         ?? 4<<3: Value is Long as Double (as of Double.longBitsToDouble)
 
+    Value --> String | BinaryValue | Int | Long (depending on Bits)
     BinaryValue --> ValueSize, <Byte>^ValueSize
-
     ValueSize --> VInt
 
-9.2. Term Dictionary
+9.2 Term Dictionary
+
 The term dictionary is represented as two files:
 1. The term infos, or tis file.
 
     TermInfoFile (.tis)--> TIVersion, TermCount, IndexInterval, SkipInterval,
     MaxSkipLevels, TermInfos
-
     TIVersion --> UInt32
-
     TermCount --> UInt64
-
     IndexInterval --> UInt32
-
     SkipInterval --> UInt32
-
     MaxSkipLevels --> UInt32
-
     TermInfos --> <TermInfo> TermCount
-
     TermInfo --> <Term, DocFreq, FreqDelta, ProxDelta, SkipDelta>
-
     Term --> <PrefixLength, Suffix, FieldNum>
-
     Suffix --> String
-
     PrefixLength, DocFreq, FreqDelta, ProxDelta, SkipDelta
     --> VInt
 
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 13
+                                                                                                                 Apache Lucene - Index File Formats
+
     This file is sorted by Term. Terms are ordered first lexicographically (by UTF16
     character code) by the term's field name, and within that lexicographically (by UTF16
     character code) by the term's text.
 
-                                                                       Page 14
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
     TIVersion names the version of the format of this file and is equal to
     TermInfosWriter.FORMAT_CURRENT.
 
@@ -662,67 +607,51 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
 
     SkipInterval --> UInt32
 
-Page 15
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 14
 Apache Lucene - Index File Formats
 
-    TermIndices --> <TermInfo, IndexDelta> IndexTermCount
+TermIndices --> <TermInfo, IndexDelta> IndexTermCount
 
-    IndexDelta --> VLong
+IndexDelta --> VLong
 
-    IndexDelta determines the position of this term's TermInfo within the .tis file. In
-    particular, it is the difference between the position of this term's entry in that file and the
-    position of the previous term's entry.
+IndexDelta determines the position of this term's TermInfo within the .tis file. In
+particular, it is the difference between the position of this term's entry in that file and the
+position of the previous term's entry.
 
-    SkipInterval is the fraction of TermDocs stored in skip tables. It is used to accelerate
-    TermDocs.skipTo(int). Larger values result in smaller indexes, greater acceleration, but
-    fewer accelerable cases, while smaller values result in bigger indexes, less acceleration
-    (in case of a small value for MaxSkipLevels) and more accelerable cases.
+SkipInterval is the fraction of TermDocs stored in skip tables. It is used to accelerate
+TermDocs.skipTo(int). Larger values result in smaller indexes, greater acceleration, but
+fewer accelerable cases, while smaller values result in bigger indexes, less acceleration
+(in case of a small value for MaxSkipLevels) and more accelerable cases.
 
-    MaxSkipLevels is the max. number of skip levels stored for each term in the .frq file. A
-    low value results in smaller indexes but less acceleration, a larger value results in slighly
-    larger indexes but greater acceleration. See format of .frq file for more information about
-    skip levels.
+MaxSkipLevels is the max. number of skip levels stored for each term in the .frq file. A
+low value results in smaller indexes but less acceleration, a larger value results in slighly
+larger indexes but greater acceleration. See format of .frq file for more information about
+skip levels.
 
-9.3. Frequencies
+9.3 Frequencies
 
 The .frq file contains the lists of documents which contain each term, along with the
 frequency of the term in that document (if omitTf is false).
-
 FreqFile (.frq) --> <TermFreqs, SkipData> TermCount
-
 TermFreqs --> <TermFreq> DocFreq
-
 TermFreq --> DocDelta[, Freq?]
-
 SkipData --> <<SkipLevelLength, SkipLevel> NumSkipLevels-1, SkipLevel> <SkipDatum>
-
 SkipLevel --> <SkipDatum> DocFreq/(SkipInterval^(Level + 1))
-
 SkipDatum --> DocSkip,PayloadLength?,FreqSkip,ProxSkip,SkipChildLevelPointer?
-
 DocDelta,Freq,DocSkip,PayloadLength,FreqSkip,ProxSkip --> VInt
-
 SkipChildLevelPointer --> VLong
-
 TermFreqs are ordered by term (the term is implicit, from the .tis file).
-
 TermFreq entries are ordered by increasing document number.
-
-DocDelta: if omitTf is false, this determines both the document number and the frequency. In
-particular, DocDelta/2 is the difference between this document number and the previous
+DocDelta: if omitTf is false, this determines both the document number and the frequency.
+In particular, DocDelta/2 is the difference between this document number and the previous
 document number (or zero when this is the first document in a TermFreqs). When DocDelta
-
-                                                                       Page 16
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 is odd, the frequency is one. When DocDelta is even, the frequency is read as another VInt. If
 omitTf is true, DocDelta contains the gap (not multiplied by 2) between document numbers
 and no frequency information is stored.
 
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 15
+                                                                                                                 Apache Lucene - Index File Formats
+
 For example, the TermFreqs for a term which occurs once in document seven and three times
 in document eleven, with omitTf false, would be the following sequence of VInts:
 
@@ -743,9 +672,9 @@ are relative to the start of TermFreqs and Positions, to the previous SkipDatum
 sequence.
 
 For example, if DocFreq=35 and SkipInterval=16, then there are two SkipData entries,
-containing the 15 th and 31 st document numbers in TermFreqs. The first FreqSkip names the
-number of bytes after the beginning of TermFreqs that the 16 th SkipDatum starts, and the
-second the number of bytes after that that the 32 nd starts. The first ProxSkip names the
+containing the 15 th and 31 st document numbers in TermFreqs. The first FreqSkip names
+the number of bytes after the beginning of TermFreqs that the 16 th SkipDatum starts, and
+the second the number of bytes after that that the 32 nd starts. The first ProxSkip names the
 number of bytes after the beginning of Positions that the 16 th SkipDatum starts, and the
 second the number of bytes after that that the 32 nd starts.
 
@@ -761,12 +690,10 @@ The SkipData entries on all upper levels > 0 contain a SkipChildLevelPointer ref
 corresponding SkipData entry in level-1. In the example has entry 15 on level 1 a pointer to
 entry 15 on level 0 and entry 31 on level 1 a pointer to entry 31 on level 0.
 
-Page 17
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 16
 Apache Lucene - Index File Formats
 
-9.4. Positions
+9.4 Positions
 
 The .prx file contains the lists of positions that each term occurs at within documents. Note
 that fields with omitTf true do not store anything into this file, and if all fields in the index
@@ -808,19 +735,14 @@ PayloadData is metadata associated with the current term position. If PayloadLen
 at the current position, then it indicates the length of this Payload. If PayloadLength is not
 stored, then this Payload has the same length as the Payload at the previous position.
 
-9.5. Normalization Factors
-
+9.5 Normalization Factors                                                                                   Page 17
 There's a single .nrm file containing all norms:
-
-                                                                       Page 18
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 AllNorms (.nrm) --> NormsHeader,<Norms> NumFieldsWithNorms
-
 Norms --> <Byte> SegSize
 
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
 NormsHeader --> 'N','R','M',Version
 
 Version --> Byte
@@ -831,6 +753,7 @@ Each byte encodes a floating point value. Bits 0-2 contain the 3-bit mantissa, a
 contain the 5-bit exponent.
 
 These are converted to an IEEE single float value as follows:
+
 1. If the byte is zero, use a zero float.
 2. Otherwise, set the sign bit of the float to zero;
 3. add 48 to the exponent and use this as the float's exponent;
@@ -844,60 +767,66 @@ that field.
 Separate norm files are created (when adequate) for both compound and non compound
 segments.
 
-9.6. Term Vectors
+9.6 Term Vectors
 
 Term Vector support is an optional on a field by field basis. It consists of 3 files.
 1. The Document Index or .tvx file.
 
     For each document, this stores the offset into the document data (.tvd) and field data (.tvf)
     files.
-
     DocumentIndex (.tvx) --> TVXVersion<DocumentPosition,FieldPosition> NumDocs
-
     TVXVersion --> Int (TermVectorsReader.CURRENT)
-
     DocumentPosition --> UInt64 (offset in the .tvd file)
-
     FieldPosition --> UInt64 (offset in the .tvf file)
 2. The Document or .tvd file.
-
-Page 19
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
     This contains, for each document, the number of fields, a list of the fields with term
     vector info and finally a list of pointers to the field information in the .tvf (Term Vector
     Fields) file.
     Document (.tvd) --> TVDVersion<NumFields, FieldNums, FieldPositions> NumDocs
     TVDVersion --> Int (TermVectorsReader.FORMAT_CURRENT)
     NumFields --> VInt
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 18
+Apache Lucene - Index File Formats
+
     FieldNums --> <FieldNumDelta> NumFields
+
     FieldNumDelta --> VInt
+
     FieldPositions --> <FieldPositionDelta> NumFields-1
+
     FieldPositionDelta --> VLong
+
     The .tvd file is used to map out the fields that have term vectors stored and where the
     field information is in the .tvf file.
 3. The Field or .tvf file.
+
     This file contains, for each field that has a term vector stored, a list of the terms, their
     frequencies and, optionally, position and offest information.
+
     Field (.tvf) --> TVFVersion<NumTerms, Position/Offset, TermFreqs> NumFields
+
     TVFVersion --> Int (TermVectorsReader.FORMAT_CURRENT)
+
     NumTerms --> VInt
+
     Position/Offset --> Byte
+
     TermFreqs --> <TermText, TermFreq, Positions?, Offsets?> NumTerms
+
     TermText --> <PrefixLength, Suffix>
+
     PrefixLength --> VInt
+
     Suffix --> String
+
     TermFreq --> VInt
+
     Positions --> <VInt>TermFreq
-    Offsets --> <VInt, VInt>TermFreq
-    Notes:
 
-                                                                       Page 20
+    Offsets --> <VInt, VInt>TermFreq
 
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
+    Notes:
 
     ?? Position/Offset byte stores whether this term vector has position or offset information
          stored.
@@ -913,60 +842,43 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
     ?? Offsets are stored as delta encoded VInts. The first VInt is the startOffset, the second
          is the endOffset.
 
-9.7. Deleted Documents
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 19
+Apache Lucene - Index File Formats
 
-The .del file is optional, and only exists when a segment contains deletions.
+9.7 Deleted Documents
 
+The .del file is optional, and only exists when a segment contains deletions.
 Although per-segment, this file is maintained exterior to compound segment files.
-
 Deletions (.del) --> [Format],ByteCount,BitCount, Bits | DGaps (depending on Format)
-
 Format,ByteSize,BitCount --> Uint32
-
 Bits --> <Byte> ByteCount
-
 DGaps --> <DGap,NonzeroByte> NonzeroBytesCount
-
 DGap --> VInt
-
 NonzeroByte --> Byte
-
 Format is Optional. -1 indicates DGaps. Non-negative value indicates Bits, and that Format is
 excluded.
-
 ByteCount indicates the number of bytes in Bits. It is typically (SegSize/8)+1.
-
 BitCount indicates the number of bits that are currently set in Bits.
-
 Bits contains one bit for each document indexed. When the bit corresponding to a document
 number is set, that document is marked as deleted. Bit ordering is from least to most
 significant. Thus, if Bits contains two bytes, 0x00 and 0x02, then document 9 is marked as
 deleted.
-
 DGaps represents sparse bit-vectors more efficiently than Bits. It is made of DGaps on
 indexes of nonzero bytes in Bits, and the nonzero bytes themselves. The number of nonzero
 bytes in Bits (NonzeroBytesCount) is not stored.
-
-Page 21
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-                                                                                                                Apache Lucene - Index File Formats
-
 For example, if there are 8000 bits and only bits 10,12,32 are set, DGaps would be used:
 (VInt) 1 , (byte) 20 , (VInt) 3 , (Byte) 1
 
-10. Limitations
+10 Limitations
 
 When referring to term numbers, Lucene's current implementation uses a Java int to hold
 the term index, which means the maximum number of unique terms in any single index
 segment is ~2.1 billion times the term index interval (default 128) = ~274 billion. This is
 technically not a limitation of the index file format, just of Lucene's current implementation.
 Similarly, Lucene uses a Java int to refer to document numbers, and the index file format
-uses an Int32 on-disk to store document numbers. This is a limitation of both the index file
-format and the current implementation. Eventually these should be replaced with either
+uses an Int32 on-disk to store document numbers. This is a limitation of both the index
+file format and the current implementation. Eventually these should be replaced with either
 UInt64 values, or better yet, VInt values which have no limit.
 
-                                                                       Page 22
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 20
 
\ No newline at end of file
diff --git a/lucene/docs/gettingstarted.html b/lucene/docs/gettingstarted.html
index b70b217..29fc8cd 100644
--- a/lucene/docs/gettingstarted.html
+++ b/lucene/docs/gettingstarted.html
@@ -3,7 +3,7 @@
 <head>
 <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta content="Apache Forrest" name="Generator">
-<meta name="Forrest-version" content="0.8">
+<meta name="Forrest-version" content="0.9">
 <meta name="Forrest-skin-name" content="lucene">
 <title>
 	Apache Lucene - Getting Started Guide
@@ -268,15 +268,13 @@ may wish to skip sections.
 	
 <li>
 <a href="demo.html">About the command-line Lucene demo and its usage</a>.  This section
-	is intended for anyone who wants to use the command-line Lucene demo.</li> 
-<p></p>
+	is intended for anyone who wants to use the command-line Lucene demo.</li>
 
 	
 <li>
 <a href="demo2.html">About the sources and implementation for the command-line Lucene
 	demo</a>.  This section walks through the implementation details (sources) of the
-	command-line Lucene demo.  This section is intended for developers.</li> 
-<p></p>
+	command-line Lucene demo.  This section is intended for developers.</li>
 
 </ul>
 </div>
diff --git a/lucene/docs/gettingstarted.pdf b/lucene/docs/gettingstarted.pdf
index d53b62c..b95fccb 100644
--- a/lucene/docs/gettingstarted.pdf
+++ b/lucene/docs/gettingstarted.pdf
@@ -1,26 +1,30 @@
 Apache Lucene - Getting Started Guide
 
 Andrew C. Oliver
-
 Table of contents
 
-   1 Getting Started................................................................................................................... 2
+   1 Getting Started....................................................................................................................2
 
-                   Copyright © 2006 The Apache Software Foundation. All rights reserved.
-                                                                                                            Apache Lucene - Getting Started Guide
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                                                                                             Apache Lucene - Getting Started Guide
 
-1. Getting Started
+1 Getting Started
 
 This document is intended as a "getting started" guide. It has three audiences: first-time users
 looking to install Apache Lucene in their application; developers looking to modify or base
-the applications they develop on Lucene; and developers looking to become involved in and
-contribute to the development of Lucene. This document is written in tutorial and
-walk-through format. The goal is to help you "get started". It does not go into great depth on
-some of the conceptual or inner details of Lucene.
+the applications they develop on Lucene; and developers looking to become involved in
+and contribute to the development of Lucene. This document is written in tutorial and walk-
+through format. The goal is to help you "get started". It does not go into great depth on some
+of the conceptual or inner details of Lucene.
 Each section listed below builds on one another. More advanced users may wish to skip
 sections.
+?? About the command-line Lucene demo and its usage. This section is intended for anyone
+
+    who wants to use the command-line Lucene demo.
+?? About the sources and implementation for the command-line Lucene demo. This section
 
-                                                                       Page 2
+    walks through the implementation details (sources) of the command-line Lucene demo.
+    This section is intended for developers.
 
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 2
 
\ No newline at end of file
diff --git a/lucene/docs/index.html b/lucene/docs/index.html
index a65f277..7c6f8b7 100644
--- a/lucene/docs/index.html
+++ b/lucene/docs/index.html
@@ -3,7 +3,7 @@
 <head>
 <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta content="Apache Forrest" name="Generator">
-<meta name="Forrest-version" content="0.8">
+<meta name="Forrest-version" content="0.9">
 <meta name="Forrest-skin-name" content="lucene">
 <title>Lucene Java Documentation</title>
 <link type="text/css" href="skin/basic.css" rel="stylesheet">
diff --git a/lucene/docs/index.pdf b/lucene/docs/index.pdf
index 4e1a93a..8e10347 100644
--- a/lucene/docs/index.pdf
+++ b/lucene/docs/index.pdf
@@ -4,5 +4,5 @@ This is the official documentation for Lucene Java 4.0 Please use the menu on th
 access the Javadocs and different documents.
 Additional documentation is available in the Wiki.
 
-                   Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
 
\ No newline at end of file
diff --git a/lucene/docs/linkmap.html b/lucene/docs/linkmap.html
index 22af044..89d0b14 100644
--- a/lucene/docs/linkmap.html
+++ b/lucene/docs/linkmap.html
@@ -3,7 +3,7 @@
 <head>
 <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta content="Apache Forrest" name="Generator">
-<meta name="Forrest-version" content="0.8">
+<meta name="Forrest-version" content="0.9">
 <meta name="Forrest-skin-name" content="lucene">
 <title>Site Linkmap Table of Contents</title>
 <link type="text/css" href="skin/basic.css" rel="stylesheet">
diff --git a/lucene/docs/linkmap.pdf b/lucene/docs/linkmap.pdf
index 789b0ce..d05bc8e 100644
--- a/lucene/docs/linkmap.pdf
+++ b/lucene/docs/linkmap.pdf
@@ -4,40 +4,56 @@ This is a map of the complete site and its structure.
 ?? Lucene ___________________ site
 
          ?? Documentation ___________________ docs
+
                   ?? Overview ___________________ overview
                   ?? Changes ___________________ changes
+
                            ?? Core ___________________ changes-core
                            ?? Contrib ___________________ changes-contrib
                   ?? Javadocs ___________________ javadoc
+
                            ?? All ___________________ javadoc-all
                            ?? Core ___________________ javadoc-core
                            ?? Test Framework ___________________ javadoc-test-framework
                            ?? Contrib ___________________ javadoc-contrib
+
                                     ?? Ant ___________________ javadoc-contrib-ant
                                     ?? Bdb ___________________ javadoc-contrib-bdb
                                     ?? Bdb-je ___________________ javadoc-contrib-bdb-je
-                                    ?? Benchmark ___________________ javadoc-contrib-benchmark
+                                    ?? Benchmark ___________________ javadoc-contrib-
+
+                                        benchmark
                                     ?? Demo ___________________ javadoc-contrib-demo
-                                    ?? Highlighter ___________________ javadoc-contrib-highlighter
-                                    ?? Instantiated ___________________ javadoc-contrib-instantiated
+                                    ?? Highlighter ___________________ javadoc-contrib-
+
+                                        highlighter
+                                    ?? Instantiated ___________________ javadoc-contrib-
+
+                                        instantiated
                                     ?? Lucli ___________________ javadoc-contrib-lucli
                                     ?? Memory ___________________ javadoc-contrib-memory
-                                    ?? Miscellaneous ___________________ javadoc-contrib-misc
+                                    ?? Miscellaneous ___________________ javadoc-contrib-
+
+                                        misc
                                     ?? Queries ___________________ javadoc-contrib-queries
                                     ?? Query Parser
 
-                   Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                        Framework ___________________ javadoc-contrib-
+                                        queryparser
+
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
                                                                                               Site Linkmap Table of Contents
 
-                      javadoc-contrib-queryparser
                   ?? Remote ___________________ javadoc-contrib-remote
                   ?? Spatial ___________________ javadoc-contrib-spatial
-                  ?? Spellchecker ___________________ javadoc-contrib-spellchecker
+                  ?? Spellchecker ___________________ javadoc-contrib-
+
+                      spellchecker
                   ?? Swing ___________________ javadoc-contrib-swing
                   ?? Wordnet ___________________ javadoc-contrib-wordnet
-                  ?? XML Query
+                  ?? XML Query Parser ___________________ javadoc-
 
-                      Parser ___________________ javadoc-contrib-xml-query-parser
+                      contrib-xml-query-parser
 ?? System Requirements ___________________ systemrequirements
 ?? Contributions ___________________ contributions
 ?? FAQ ___________________ faq
@@ -48,7 +64,5 @@ This is a map of the complete site and its structure.
 ?? Scoring ___________________ scoring
 ?? Wiki ___________________ wiki
 
-                                                                       Page 2
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 2
 
\ No newline at end of file
diff --git a/lucene/docs/lucene-contrib/index.html b/lucene/docs/lucene-contrib/index.html
index a217a52..b516462 100644
--- a/lucene/docs/lucene-contrib/index.html
+++ b/lucene/docs/lucene-contrib/index.html
@@ -3,7 +3,7 @@
 <head>
 <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta content="Apache Forrest" name="Generator">
-<meta name="Forrest-version" content="0.8">
+<meta name="Forrest-version" content="0.9">
 <meta name="Forrest-skin-name" content="lucene">
 <title>
 	        Apache Lucene - Lucene Contrib
diff --git a/lucene/docs/lucene-contrib/index.pdf b/lucene/docs/lucene-contrib/index.pdf
index d3c4c98..1db16b3 100644
--- a/lucene/docs/lucene-contrib/index.pdf
+++ b/lucene/docs/lucene-contrib/index.pdf
@@ -1,35 +1,34 @@
 Apache Lucene - Lucene Contrib
 
 Table of contents
-
    1 Lucene Contrib...................................................................................................................2
-    1.1 ant.................................................................................................................................. 2
-    1.2 benchmark..................................................................................................................... 2
-    1.3 demo.............................................................................................................................. 2
-    1.4 db................................................................................................................................... 3
-    1.5 highlighter......................................................................................................................3
-    1.6 instantiated.....................................................................................................................3
-    1.7 lucli................................................................................................................................ 3
-    1.8 memory..........................................................................................................................3
-    1.9 misc................................................................................................................................3
-    1.10 queryparser...................................................................................................................3
-    1.11 queries..........................................................................................................................4
-    1.12 remote.......................................................................................................................... 4
-    1.13 spatial........................................................................................................................... 4
-    1.14 spellchecker................................................................................................................. 4
-    1.15 swing............................................................................................................................4
-    1.16 wordnet........................................................................................................................ 4
-    1.17 xml-query-parser..........................................................................................................4
-
-                   Copyright © 2006 The Apache Software Foundation. All rights reserved.
+     1.1 ant..................................................................................................................................2
+     1.2 benchmark..................................................................................................................... 2
+     1.3 demo..............................................................................................................................2
+     1.4 db...................................................................................................................................3
+     1.5 highlighter..................................................................................................................... 3
+     1.6 instantiated.................................................................................................................... 3
+     1.7 lucli................................................................................................................................3
+     1.8 memory......................................................................................................................... 3
+     1.9 misc............................................................................................................................... 3
+     1.10 queryparser.................................................................................................................. 3
+     1.11 queries......................................................................................................................... 3
+     1.12 remote..........................................................................................................................4
+     1.13 spatial.......................................................................................................................... 4
+     1.14 spellchecker................................................................................................................. 4
+     1.15 swing........................................................................................................................... 4
+     1.16 wordnet........................................................................................................................4
+     1.17 xml-query-parser......................................................................................................... 4
+
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Lucene Contrib
 
-1. Lucene Contrib
+1 Lucene Contrib
 
 The Lucene Java project also contains a workspace, Lucene Contrib (formerly known as the
 Lucene Sandbox), that is open both to all Lucene Java core committers and to developers
-whose commit rights are restricted to the Contrib workspace; these developers are referred to
-as "Contrib committers". The Lucene Contrib workspace hosts the following types of
+whose commit rights are restricted to the Contrib workspace; these developers are referred
+to as "Contrib committers". The Lucene Contrib workspace hosts the following types of
 packages:
 ?? Various third party contributions.
 ?? Contributions with third party dependencies - the Lucene Java core distribution has no
@@ -37,116 +36,102 @@ packages:
     external runtime dependencies.
 ?? New ideas that are intended for eventual inclusion into the Lucene Java core.
 
-Users are free to experiment with the components developed in the Contrib workspace, but
-Contrib packages will not necessarily be maintained, particularly in their current state. The
-Lucene Java core backwards compatibility commitments (see
-http://wiki.apache.org/lucene-java/BackwardsCompatibility) do not necessarily extend to the
-packages in the Contrib workspace. See the README.txt file for each Contrib package for
-details. If the README.txt file does not address its backwards compatibility commitments,
-users should assume it does not make any compatibility commitments.
-
+Users are free to experiment with the components developed in the Contrib workspace,
+but Contrib packages will not necessarily be maintained, particularly in their current state.
+The Lucene Java core backwards compatibility commitments (see http://wiki.apache.org/
+lucene-java/BackwardsCompatibility) do not necessarily extend to the packages in the
+Contrib workspace. See the README.txt file for each Contrib package for details. If the
+README.txt file does not address its backwards compatibility commitments, users should
+assume it does not make any compatibility commitments.
 See Contrib CHANGES for changes included in the current release.
+You can access the current trunk Contrib repository at http://svn.apache.org/repos/asf/lucene/
+dev/trunk/lucene/contrib/.
 
-You can access the current trunk Contrib repository at
-http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/contrib/.
-
-1.1. ant
+1.1 ant
 
 Ant task to create Lucene indexes.
-
 See ant javadoc
 
-1.2. benchmark
+1.2 benchmark
 
 The benchmark contribution contains tools for benchmarking Lucene using standard, freely
 available corpora.
-
 See benchmark javadoc
 
-1.3. demo
+1.3 demo
 
 The demo contrib contains the Lucene demo: IndexFiles and SearchFiles, described under
 Getting Started.
-
-                                                                       Page 2
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Lucene Contrib
-
 See demo javadoc
 
-1.4. db
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 2
+                                                                     Apache Lucene - Lucene Contrib
+
+1.4 db
 Provides integration with Berkley DB.
 See db javadoc
 
-1.5. highlighter
+1.5 highlighter
 A set of classes for highlighting matching terms in search results.
 See highlighter javadoc
 
-1.6. instantiated
+1.6 instantiated
+
 RAM-based index that enables much faster searching than RAMDirectory in certain
 situations.
 See instantiated javadoc
 
-1.7. lucli
+1.7 lucli
 An application that allows Lucene index manipulation from the command-line.
 See lucli javadoc
 
-1.8. memory
+1.8 memory
 High-performance single-document main memory index.
 See memory javadoc
 
-1.9. misc
+1.9 misc
+
 A variety of miscellaneous files, including QueryParsers, and other alternate Lucene class
 implementations and tools.
 See misc javadoc
 
-1.10. queryparser
-A new Lucene query parser implementation, which matches the syntax of the core
-
-Page 3
-
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Lucene Contrib
+1.10 queryparser
 
+A new Lucene query parser implementation, which matches the syntax of the core
 QueryParser but offers a more modular architecture to enable customization.
 See queryparser javadoc
 
-1.11. queries
+1.11 queries                                                                                                Page 3
 Additional queries for Lucene.
+
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                                                                                                      Apache Lucene - Lucene Contrib
+
 See queries javadoc
 
-1.12. remote
+1.12 remote
 Classes to help use Lucene with RMI.
 See remote javadoc
 
-1.13. spatial
+1.13 spatial
 Classes to help with efficient distance based sorting.
 See spatial javadoc
 
-1.14. spellchecker
+1.14 spellchecker
 Provides tools for spellchecking and suggestions with Lucene.
 See spellchecker javadoc
 
-1.15. swing
+1.15 swing
 Swing components designed to integrate with Lucene.
 See swing javadoc
 
-1.16. wordnet
+1.16 wordnet
 Tools to help utilize wordnet synonyms with Lucene
 See wordnet javadoc
 
-1.17. xml-query-parser
+1.17 xml-query-parser
 A QueryParser that can read queries written in an XML format.
-
-                                                                             Page 4
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Lucene Contrib
-
 See xml-query-parser javadoc
 
-Page 5
-
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 4
 
\ No newline at end of file
diff --git a/lucene/docs/queryparsersyntax.html b/lucene/docs/queryparsersyntax.html
index 59222f6..fed92a4 100644
--- a/lucene/docs/queryparsersyntax.html
+++ b/lucene/docs/queryparsersyntax.html
@@ -3,7 +3,7 @@
 <head>
 <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta content="Apache Forrest" name="Generator">
-<meta name="Forrest-version" content="0.8">
+<meta name="Forrest-version" content="0.9">
 <meta name="Forrest-skin-name" content="lucene">
 <title>
 	Apache Lucene - Query Parser Syntax
diff --git a/lucene/docs/queryparsersyntax.pdf b/lucene/docs/queryparsersyntax.pdf
index f2b045e..79d6165 100644
--- a/lucene/docs/queryparsersyntax.pdf
+++ b/lucene/docs/queryparsersyntax.pdf
@@ -1,32 +1,32 @@
 Apache Lucene - Query Parser Syntax
 
 Peter Carlson
-
 Table of contents
 
-   1 Overview............................................................................................................................2
-   2 Terms................................................................................................................................. 2
-   3 Fields..................................................................................................................................3
+   1 Overview............................................................................................................................ 2
+   2 Terms.................................................................................................................................. 2
+   3 Fields.................................................................................................................................. 3
    4 Term Modifiers.................................................................................................................. 3
-    4.1 Wildcard Searches......................................................................................................... 3
-    4.2 Fuzzy Searches.............................................................................................................. 4
-    4.3 Proximity Searches........................................................................................................4
-    4.4 Range Searches..............................................................................................................4
-    4.5 Boosting a Term............................................................................................................ 5
+
+     4.1 Wildcard Searches........................................................................................................ 3
+     4.2 Fuzzy Searches............................................................................................................. 4
+     4.3 Proximity Searches....................................................................................................... 4
+     4.4 Range Searches............................................................................................................. 4
+     4.5 Boosting a Term........................................................................................................... 5
    5 Boolean Operators..............................................................................................................5
-    5.1 ....................................................................................................................................... 5
-    5.2 AND.............................................................................................................................. 5
-    5.3 +.....................................................................................................................................6
-    5.4 NOT...............................................................................................................................6
-    5.5 -......................................................................................................................................6
-   6 Grouping............................................................................................................................ 6
+     5.1 .......................................................................................................................................5
+     5.2 AND.............................................................................................................................. 6
+     5.3 +.................................................................................................................................... 6
+     5.4 NOT.............................................................................................................................. 6
+     5.5 -..................................................................................................................................... 6
+   6 Grouping............................................................................................................................. 7
    7 Field Grouping................................................................................................................... 7
    8 Escaping Special Characters.............................................................................................. 7
 
-                   Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Query Parser Syntax
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                                                                                              Apache Lucene - Query Parser Syntax
 
-1. Overview
+1 Overview
 
 Although Lucene provides the ability to create your own queries through its API, it also
 provides a rich query language through the Query Parser, a lexer which interprets a string
@@ -38,10 +38,10 @@ consult the copy of docs/queryparsersyntax.html that was distributed with the
 version you are using.
 
 Before choosing to use the provided Query Parser, please consider the following:
-1. If you are programmatically generating a query string and then parsing it with the query
+1. If you are programmatically generating a query string and then parsing it with the
 
-    parser then you should seriously consider building your queries directly with the query
-    API. In other words, the query parser is designed for human-entered text, not for
+    query parser then you should seriously consider building your queries directly with the
+    query API. In other words, the query parser is designed for human-entered text, not for
     program-generated text.
 2. Untokenized fields are best added directly to queries, and not through the query parser. If
     a field's values are generated programmatically by the application, then so should query
@@ -54,7 +54,7 @@ Before choosing to use the provided Query Parser, please consider the following:
     added to a query string which is subsequently parsed, but rather added as a TermQuery
     clause.
 
-2. Terms
+2 Terms
 
 A query is broken up into terms and operators. There are two types of terms: Single Terms
 and Phrases.
@@ -70,111 +70,99 @@ Note: The analyzer used to create the index will be used on the terms and phrase
 query string. So it is important to choose an analyzer that will not interfere with the terms
 used in the query string.
 
-                                                                       Page 2
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 2
 Apache Lucene - Query Parser Syntax
 
-3. Fields
+3 Fields
 
 Lucene supports fielded data. When performing a search you can either specify a field, or use
 the default field. The field names and default field is implementation specific.
-
 You can search any field by typing the field name followed by a colon ":" and then the term
 you are looking for.
-
 As an example, let's assume a Lucene index contains two fields, title and text and text is the
 default field. If you want to find the document entitled "The Right Way" which contains the
 text "don't go this way", you can enter:
 
-title:"The Right Way" AND text:go
+  title:"The Right Way" AND text:go
 
 or
 
-title:"Do it right" AND right
+  title:"Do it right" AND right
 
 Since text is the default field, the field indicator is not required.
-
 Note: The field is only valid for the term that it directly precedes, so the query
 
-title:Do it right
+  title:Do it right
 
 Will only find "Do" in the title field. It will find "it" and "right" in the default field (in this
 case the text field).
 
-4. Term Modifiers
-
+4 Term Modifiers
 Lucene supports modifying query terms to provide a wide range of searching options.
 
-4.1. Wildcard Searches
+4.1 Wildcard Searches
 
 Lucene supports single and multiple character wildcard searches within single terms (not
 within phrase queries).
-
 To perform a single character wildcard search use the "?" symbol.
-
 To perform a multiple character wildcard search use the "*" symbol.
-
 The single character wildcard search looks for terms that match that with the single character
 replaced. For example, to search for "text" or "test" you can use the search:
 
-te?t
+  te?t
 
 Multiple character wildcard searches looks for 0 or more characters. For example, to search
 for test, tests or tester, you can use the search:
 
-test*
-
-Page 3
+  test*
 
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Query Parser Syntax
+Copyright © 2006 The Apache Software Foundation. All rights reserved.                Page 3
+                                                                                                              Apache Lucene - Query Parser Syntax
 
 You can also use the wildcard searches in the middle of a term.
 
-te*t
+  te*t
 
 Note: You cannot use a * or ? symbol as the first character of a search.
 
-4.2. Fuzzy Searches
-
+4.2 Fuzzy Searches
 Lucene supports fuzzy searches based on the Levenshtein Distance, or Edit Distance
 algorithm. To do a fuzzy search use the tilde, "~", symbol at the end of a Single word Term.
 For example to search for a term similar in spelling to "roam" use the fuzzy search:
 
-roam~
+  roam~
 
 This search will find terms like foam and roams.
-
 Starting with Lucene 1.9 an additional (optional) parameter can specify the required
 similarity. The value is between 0 and 1, with a value closer to 1 only terms with a higher
 similarity will be matched. For example:
 
-roam~0.8
+  roam~0.8
 
 The default that is used if the parameter is not given is 0.5.
 
-4.3. Proximity Searches
-
+4.3 Proximity Searches
 Lucene supports finding words are a within a specific distance away. To do a proximity
 search use the tilde, "~", symbol at the end of a Phrase. For example to search for a "apache"
 and "jakarta" within 10 words of each other in a document use the search:
 
-"jakarta apache"~10
-
-4.4. Range Searches
+  "jakarta apache"~10
 
+4.4 Range Searches
 Range Queries allow one to match documents whose field(s) values are between the lower
 and upper bound specified by the Range Query. Range Queries can be inclusive or exclusive
 of the upper and lower bounds. Sorting is done lexicographically.
 
-mod_date:[20020101 TO 20030101]
+  mod_date:[20020101 TO 20030101]
 
 This will find documents whose mod_date fields have values between 20020101 and
 20030101, inclusive. Note that Range Queries are not reserved for date fields. You could also
 use range queries with non-date fields:
 
-title:{Aida TO Carmen}
+  title:{Aida TO Carmen}
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 4
+Apache Lucene - Query Parser Syntax
 
 This will find all documents whose titles are between Aida and Carmen, but not including
 Aida and Carmen.
@@ -182,141 +170,119 @@ Aida and Carmen.
 Inclusive range queries are denoted by square brackets. Exclusive range queries are denoted
 by curly brackets.
 
-                                                                       Page 4
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Query Parser Syntax
-
-4.5. Boosting a Term
+4.5 Boosting a Term
 
 Lucene provides the relevance level of matching documents based on the terms found. To
 boost a term use the caret, "^", symbol with a boost factor (a number) at the end of the term
 you are searching. The higher the boost factor, the more relevant the term will be.
-
 Boosting allows you to control the relevance of a document by boosting its term. For
 example, if you are searching for
 
-jakarta apache
+  jakarta apache
 
 and you want the term "jakarta" to be more relevant boost it using the ^ symbol along with
 the boost factor next to the term. You would type:
 
-jakarta^4 apache
+  jakarta^4 apache
 
 This will make documents with the term jakarta appear more relevant. You can also boost
 Phrase Terms as in the example:
 
-"jakarta apache"^4 "Apache Lucene"
+  "jakarta apache"^4 "Apache Lucene"
 
 By default, the boost factor is 1. Although the boost factor must be positive, it can be less
 than 1 (e.g. 0.2)
 
-5. Boolean Operators
+5 Boolean Operators
 
 Boolean operators allow terms to be combined through logic operators. Lucene supports
 AND, "+", OR, NOT and "-" as Boolean operators(Note: Boolean operators must be ALL
 CAPS).
 
-5.1.
+5.1
 
 The OR operator is the default conjunction operator. This means that if there is no Boolean
 operator between two terms, the OR operator is used. The OR operator links two terms and
 finds a matching document if either of the terms exist in a document. This is equivalent to a
 union using sets. The symbol || can be used in place of the word OR.
-
 To search for documents that contain either "jakarta apache" or just "jakarta" use the query:
 
-"jakarta apache" jakarta
+  "jakarta apache" jakarta
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 5
+                               Apache Lucene - Query Parser Syntax
 
 or
 
-"jakarta apache" OR jakarta
+  "jakarta apache" OR jakarta
 
-5.2. AND
+5.2 AND
 
 The AND operator matches documents where both terms exist anywhere in the text of a
 single document. This is equivalent to an intersection using sets. The symbol && can be used
-
-Page 5
-
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Query Parser Syntax
-
 in place of the word AND.
-
 To search for documents that contain "jakarta apache" and "Apache Lucene" use the query:
 
-"jakarta apache" AND "Apache Lucene"
+  "jakarta apache" AND "Apache Lucene"
 
-5.3. +
+5.3 +
 
 The "+" or required operator requires that the term after the "+" symbol exist somewhere in a
 the field of a single document.
-
 To search for documents that must contain "jakarta" and may contain "lucene" use the query:
 
-+jakarta lucene
+  +jakarta lucene
 
-5.4. NOT
+5.4 NOT
 
 The NOT operator excludes documents that contain the term after NOT. This is equivalent to
 a difference using sets. The symbol ! can be used in place of the word NOT.
-
 To search for documents that contain "jakarta apache" but not "Apache Lucene" use the
 query:
 
-"jakarta apache" NOT "Apache Lucene"
+  "jakarta apache" NOT "Apache Lucene"
 
 Note: The NOT operator cannot be used with just one term. For example, the following
 search will return no results:
 
-NOT "jakarta apache"
+  NOT "jakarta apache"
 
-5.5. -
+5.5 -
 
 The "-" or prohibit operator excludes documents that contain the term after the "-" symbol.
-
 To search for documents that contain "jakarta apache" but not "Apache Lucene" use the
 query:
 
-"jakarta apache" -"Apache Lucene"
+  "jakarta apache" -"Apache Lucene"
 
-6. Grouping
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 6
+                                                                                                              Apache Lucene - Query Parser Syntax
 
+6 Grouping
 Lucene supports using parentheses to group clauses to form sub queries. This can be very
 useful if you want to control the boolean logic for a query.
-
 To search for either "jakarta" or "apache" and "website" use the query:
 
-(jakarta OR apache) AND website
+  (jakarta OR apache) AND website
 
 This eliminates any confusion and makes sure you that website must exist and either term
 jakarta or apache may exist.
 
-                                                                       Page 6
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Query Parser Syntax
-
-7. Field Grouping
-
+7 Field Grouping
 Lucene supports using parentheses to group multiple clauses to a single field.
 To search for a title that contains both the word "return" and the phrase "pink panther" use
 the query:
 
-title:(+return +"pink panther")
-
-8. Escaping Special Characters
+  title:(+return +"pink panther")
 
+8 Escaping Special Characters
 Lucene supports escaping special characters that are part of the query syntax. The current list
 special characters are
 + - && || ! ( ) { } [ ] ^ " ~ * ? : \
 To escape these character use the \ before the character. For example to search for (1+1):2
 use the query:
 
-\(1\+1\)\:2
-
-Page 7
+  \(1\+1\)\:2
 
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 7
 
\ No newline at end of file
diff --git a/lucene/docs/scoring.html b/lucene/docs/scoring.html
index c30ec4c..a0326be 100644
--- a/lucene/docs/scoring.html
+++ b/lucene/docs/scoring.html
@@ -3,7 +3,7 @@
 <head>
 <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta content="Apache Forrest" name="Generator">
-<meta name="Forrest-version" content="0.8">
+<meta name="Forrest-version" content="0.9">
 <meta name="Forrest-skin-name" content="lucene">
 <title>
 	Apache Lucene - Scoring
diff --git a/lucene/docs/scoring.pdf b/lucene/docs/scoring.pdf
index 1855c61..0fdf791 100644
--- a/lucene/docs/scoring.pdf
+++ b/lucene/docs/scoring.pdf
@@ -1,25 +1,25 @@
 Apache Lucene - Scoring
 
 Grant Ingersoll
-
 Table of contents
 
-   1 Introduction........................................................................................................................2
-   2 Scoring............................................................................................................................... 2
-    2.1 Fields and Documents................................................................................................... 2
-    2.2 Score Boosting...............................................................................................................3
-    2.3 Understanding the Scoring Formula..............................................................................3
-    2.4 The Big Picture..............................................................................................................3
-    2.5 Query Classes................................................................................................................ 4
-    2.6 Changing Similarity.......................................................................................................4
-   3 Changing your Scoring -- Expert Level.............................................................................4
-   4 Appendix............................................................................................................................5
-    4.1 Algorithm...................................................................................................................... 5
+   1 Introduction........................................................................................................................ 2
+   2 Scoring................................................................................................................................2
+
+     2.1 Fields and Documents.................................................................................................. 2
+     2.2 Score Boosting..............................................................................................................3
+    2.3 Understanding the Scoring Formula............................................................................. 3
+     2.4 The Big Picture.............................................................................................................3
+     2.5 Query Classes............................................................................................................... 4
+     2.6 Changing Similarity...................................................................................................... 4
+   3 Changing your Scoring -- Expert Level............................................................................ 4
+   4 Appendix............................................................................................................................ 5
+     4.1 Algorithm...................................................................................................................... 5
 
-                   Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Scoring
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                                                       Apache Lucene - Scoring
 
-1. Introduction
+1 Introduction
 
 Lucene scoring is the heart of why we all love Lucene. It is blazingly fast and it hides almost
 all of the complexity from the user. In a nutshell, it works. At least, that is, until it doesn't
@@ -32,9 +32,9 @@ While this document won't answer your specific scoring issues, it will, hopefull
 to the places that can help you figure out the what and why of Lucene scoring.
 
 Lucene scoring uses a combination of the Vector Space Model (VSM) of Information
-Retrieval and the Boolean model to determine how relevant a given Document is to a User's
-query. In general, the idea behind the VSM is the more times a query term appears in a
-document relative to the number of times the term appears in all the documents in the
+Retrieval and the Boolean model to determine how relevant a given Document is to a
+User's query. In general, the idea behind the VSM is the more times a query term appears
+in a document relative to the number of times the term appears in all the documents in the
 collection, the more relevant that document is to the query. It uses the Boolean model to first
 narrow down the documents that need to be scored based on the use of boolean logic in the
 Query specification. Lucene also adds some capabilities and refinements onto this model to
@@ -42,12 +42,12 @@ support boolean and fuzzy searching, but it essentially remains a VSM based syst
 heart. For some valuable references on VSM and IR in general refer to the Lucene Wiki IR
 references.
 
-The rest of this document will cover Scoring basics and how to change your Similarity. Next
-it will cover ways you can customize the Lucene internals in Changing your Scoring --
-Expert Level which gives details on implementing your own Query class and related
+The rest of this document will cover Scoring basics and how to change your Similarity.
+Next it will cover ways you can customize the Lucene internals in Changing your Scoring
+-- Expert Level which gives details on implementing your own Query class and related
 functionality. Finally, we will finish up with some reference material in the Appendix.
 
-2. Scoring
+2 Scoring
 
 Scoring is very much dependent on the way documents are indexed, so it is important to
 understand indexing (see Apache Lucene - Getting Started Guide and the Lucene file formats
@@ -55,23 +55,21 @@ before continuing on with this section.) It is also assumed that readers know ho
 Searcher.explain(Query query, int doc) functionality, which can go a long way in informing
 why a score is returned.
 
-2.1. Fields and Documents
+2.1 Fields and Documents
 
 In Lucene, the objects we are scoring are Documents. A Document is a collection of Fields.
 Each Field has semantics about how it is created and stored (i.e. tokenized, untokenized, raw
 data, compressed, etc.) It is important to note that Lucene scoring works on Fields and then
+combines the results to return Documents. This is important because two Documents with
+the exact same content, but one having the content in two Fields and the other in one Field
 
-                                                                       Page 2
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Scoring
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 2
+                                                                       Apache Lucene - Scoring
 
-combines the results to return Documents. This is important because two Documents with the
-exact same content, but one having the content in two Fields and the other in one Field will
-return different scores for the same query due to length normalization (assumming the
+will return different scores for the same query due to length normalization (assumming the
 DefaultSimilarity on the Fields).
 
-2.2. Score Boosting
+2.2 Score Boosting
 
 Lucene allows influencing search results by "boosting" in more than one level:
 ?? Document level boosting - while indexing - by calling document.setBoost() before a
@@ -85,10 +83,10 @@ Lucene allows influencing search results by "boosting" in more than one level:
     Query.setBoost().
 
 Indexing time boosts are preprocessed for storage efficiency and written to the directory
-(when writing the document) in a single byte (!) as follows: For each field of a document, all
-boosts of that field (i.e. all boosts under the same field name in that doc) are multiplied. The
-result is multiplied by the boost of the document, and also multiplied by a "field length
-norm" value that represents the length of that field in that doc (so shorter fields are
+(when writing the document) in a single byte (!) as follows: For each field of a document,
+all boosts of that field (i.e. all boosts under the same field name in that doc) are multiplied.
+The result is multiplied by the boost of the document, and also multiplied by a "field
+length norm" value that represents the length of that field in that doc (so shorter fields are
 automatically boosted up). The result is decoded as a single byte (with some precision loss of
 course) and stored in the directory. The similarity object in effect at indexing computes the
 length-norm of the field.
@@ -102,31 +100,28 @@ it is not guaranteed that decode(encode(x)) = x, e.g. decode(encode(0.89)) = 0.7
 (search) time, this norm is brought into the score of document as norm(t, d), as shown by the
 formula in Similarity.
 
-2.3. Understanding the Scoring Formula
+2.3 Understanding the Scoring Formula
 
 This scoring formula is described in the Similarity class. Please take the time to study this
 formula, as it contains much of the information about how the basics of Lucene scoring
 work, especially the TermQuery.
 
-2.4. The Big Picture
-
-Page 3
-
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-                                                                                                                               Apache Lucene - Scoring
+2.4 The Big Picture
 
 OK, so the tf-idf formula and the Similarity is great for understanding the basics of Lucene
 scoring, but what really drives Lucene scoring are the use and interactions between the Query
 classes, as created by each application in response to a user's information need.
 
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 3
+                                                                                                                                Apache Lucene - Scoring
+
 In this regard, Lucene offers a wide variety of Query implementations, most of which are in
 the org.apache.lucene.search package. These implementations can be combined in a wide
 variety of ways to provide complex querying capabilities along with information about where
-matches took place in the document collection. The Query section below highlights some of
-the more important Query classes. For information on the other ones, see the package
+matches took place in the document collection. The Query section below highlights some
+of the more important Query classes. For information on the other ones, see the package
 summary. For details on implementing your own Query class, see Changing your Scoring --
 Expert Level below.
-
 Once a Query has been created and submitted to the IndexSearcher, the scoring process
 begins. (See the Appendix Algorithm section for more notes on the process.) After some
 infrastructure setup, control finally passes to the Weight implementation and its Scorer
@@ -134,36 +129,33 @@ instance. In the case of any type of BooleanQuery, scoring is handled by the Boo
 (link goes to ViewVC BooleanQuery java code which contains the BooleanWeight2 inner
 class) or BooleanWeight (link goes to ViewVC BooleanQuery java code, which contains the
 BooleanWeight inner class).
-
 Assuming the use of the BooleanWeight2, a BooleanScorer2 is created by bringing together
-all of the Scorers from the sub-clauses of the BooleanQuery. When the BooleanScorer2 is
-asked to score it delegates its work to an internal Scorer based on the type of clauses in the
-Query. This internal Scorer essentially loops over the sub scorers and sums the scores
+all of the Scorers from the sub-clauses of the BooleanQuery. When the BooleanScorer2
+is asked to score it delegates its work to an internal Scorer based on the type of clauses in
+the Query. This internal Scorer essentially loops over the sub scorers and sums the scores
 provided by each scorer while factoring in the coord() score.
 
-2.5. Query Classes
+2.5 Query Classes
 
 For information on the Query Classes, refer to the search package javadocs
 
-2.6. Changing Similarity
+2.6 Changing Similarity
 
 One of the ways of changing the scoring characteristics of Lucene is to change the similarity
 factors. For information on how to do this, see the search package javadocs
 
-3. Changing your Scoring -- Expert Level
+3 Changing your Scoring -- Expert Level
 
-At a much deeper level, one can affect scoring by implementing their own Query classes (and
-related scoring classes.) To learn more about how to do this, refer to the search package
+At a much deeper level, one can affect scoring by implementing their own Query classes
+(and related scoring classes.) To learn more about how to do this, refer to the search package
 javadocs
 
-                                                                       Page 4
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Scoring
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 4
+                                                                                   Apache Lucene - Scoring
 
-4. Appendix
+4 Appendix
 
-4.1. Algorithm
+4.1 Algorithm
 
 This section is mostly notes on stepping through the Scoring process and serves as fertilizer
 for the earlier sections.
@@ -182,8 +174,8 @@ These important objects are involved in a search:
 
     method is not desired.
 
-Assuming we are not sorting (since sorting doesn't effect the raw Lucene score), we call one
-of the search methods of the Searcher, passing in the Weight object created by
+Assuming we are not sorting (since sorting doesn't effect the raw Lucene score), we
+call one of the search methods of the Searcher, passing in the Weight object created by
 Searcher.createWeight(Query), Filter and the number of results we want. This method returns
 a TopDocs object, which is an internal collection of search results. The Searcher creates a
 TopScoreDocCollector and passes it along with the Weight, Filter to another expert search
@@ -201,22 +193,18 @@ Weight object depends on what type of Query was submitted. In most real world ap
 with multiple query terms, the Scorer is going to be a BooleanScorer2 (see the section on
 customizing your scoring for info on changing this.)
 
-Assuming a BooleanScorer2 scorer, we first initialize the Coordinator, which is used to apply
-the coord() factor. We then get a internal Scorer based on the required, optional and
+Assuming a BooleanScorer2 scorer, we first initialize the Coordinator, which is used to
+apply the coord() factor. We then get a internal Scorer based on the required, optional and
 prohibited parts of the query. Using this internal Scorer, the BooleanScorer2 then proceeds
-into a while loop based on the Scorer#next() method. The next() method advances to the next
+into a while loop based on the Scorer#next() method. The next() method advances to the
+next document matching the query. This is an abstract method in the Scorer class and is
+thus overriden by all derived implementations. If you have a simple OR query your internal
 
-Page 5
+            Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 5
+                                                                                                                                Apache Lucene - Scoring
 
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-                                                                                                                               Apache Lucene - Scoring
-
-document matching the query. This is an abstract method in the Scorer class and is thus
-overriden by all derived implementations. If you have a simple OR query your internal
 Scorer is most likely a DisjunctionSumScorer, which essentially combines the scorers from
 the sub scorers of the OR'd terms.
 
-                                                                       Page 6
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 6
 
\ No newline at end of file
diff --git a/lucene/docs/skin/images/apache-thanks.png b/lucene/docs/skin/images/apache-thanks.png
new file mode 100644
index 0000000..c0bea09
Binary files /dev/null and b/lucene/docs/skin/images/apache-thanks.png differ
diff --git a/lucene/docs/skin/images/built-with-cocoon.gif b/lucene/docs/skin/images/built-with-cocoon.gif
new file mode 100644
index 0000000..0b38f78
Binary files /dev/null and b/lucene/docs/skin/images/built-with-cocoon.gif differ
diff --git a/lucene/docs/systemrequirements.html b/lucene/docs/systemrequirements.html
index e468f4c..4025906 100644
--- a/lucene/docs/systemrequirements.html
+++ b/lucene/docs/systemrequirements.html
@@ -3,7 +3,7 @@
 <head>
 <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
 <meta content="Apache Forrest" name="Generator">
-<meta name="Forrest-version" content="0.8">
+<meta name="Forrest-version" content="0.9">
 <meta name="Forrest-skin-name" content="lucene">
 <title>Apache Lucene - System Requirements</title>
 <link type="text/css" href="skin/basic.css" rel="stylesheet">
diff --git a/lucene/docs/systemrequirements.pdf b/lucene/docs/systemrequirements.pdf
index f6b2924..1450224 100644
--- a/lucene/docs/systemrequirements.pdf
+++ b/lucene/docs/systemrequirements.pdf
@@ -2,15 +2,14 @@ Apache Lucene - System
 Requirements
 
 Grant Ingersoll, Uwe Schindler
-
 Table of contents
 
-   1 System Requirements.........................................................................................................2
+   1 System Requirements......................................................................................................... 2
 
-                   Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                     Copyright © 2006 The Apache Software Foundation. All rights reserved.
                                                                                                             Apache Lucene - System Requirements
 
-1. System Requirements
+1 System Requirements
 
 Lucene Java 4.0 has the following minimum requirements:
 ?? Java 1.5.x or greater.
@@ -25,7 +24,5 @@ Lucene Java 4.0 has the following minimum requirements:
 Contrib modules may have other requirements, refer to their documentation and build files
 for information.
 
-                                                                       Page 2
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.  Page 2
 
\ No newline at end of file
diff --git a/lucene/src/java/org/apache/lucene/document/Document.java b/lucene/src/java/org/apache/lucene/document/Document.java
index 0343f67..5d8262c 100644
--- a/lucene/src/java/org/apache/lucene/document/Document.java
+++ b/lucene/src/java/org/apache/lucene/document/Document.java
@@ -131,8 +131,13 @@ public final class Document {
   /** Returns a field with the given name if any exist in this document, or
    * null.  If multiple fields exists with this name, this method returns the
    * first value added.
-   * Do not use this method with lazy loaded fields.
+   * Do not use this method with lazy loaded fields or {@link NumericField}.
+   * @deprecated use {@link #getFieldable} instead and cast depending on
+   * data type.
+   * @throws ClassCastException if you try to retrieve a numerical or
+   * lazy loaded field.
    */
+  @Deprecated
   public final Field getField(String name) {
     return (Field) getFieldable(name);
   }
@@ -154,6 +159,8 @@ public final class Document {
    * this document, or null.  If multiple fields exist with this name, this
    * method returns the first value added. If only binary fields with this name
    * exist, returns null.
+   * For {@link NumericField} it returns the string value of the number. If you want
+   * the actual {@code NumericField} instance back, use {@link #getFieldable}.
    */
   public final String get(String name) {
    for (Fieldable field : fields) {
@@ -177,13 +184,18 @@ public final class Document {
   
   /**
    * Returns an array of {@link Field}s with the given name.
-   * Do not use with lazy loaded fields.
    * This method returns an empty array when there are no
    * matching fields.  It never returns null.
+   * Do not use this method with lazy loaded fields or {@link NumericField}.
    *
    * @param name the name of the field
    * @return a <code>Field[]</code> array
+   * @deprecated use {@link #getFieldable} instead and cast depending on
+   * data type.
+   * @throws ClassCastException if you try to retrieve a numerical or
+   * lazy loaded field.
    */
+   @Deprecated
    public final Field[] getFields(String name) {
      List<Field> result = new ArrayList<Field>();
      for (Fieldable field : fields) {
@@ -230,6 +242,8 @@ public final class Document {
    * Returns an array of values of the field specified as the method parameter.
    * This method returns an empty array when there are no
    * matching fields.  It never returns null.
+   * For {@link NumericField}s it returns the string value of the number. If you want
+   * the actual {@code NumericField} instances back, use {@link #getFieldables}.
    * @param name the name of the field
    * @return a <code>String[]</code> of field values
    */
diff --git a/lucene/src/java/org/apache/lucene/document/NumericField.java b/lucene/src/java/org/apache/lucene/document/NumericField.java
index 6cae722..3bd46cf 100644
--- a/lucene/src/java/org/apache/lucene/document/NumericField.java
+++ b/lucene/src/java/org/apache/lucene/document/NumericField.java
@@ -127,18 +127,18 @@ import org.apache.lucene.search.FieldCache; // javadocs
  * class is a wrapper around this token stream type for
  * easier, more intuitive usage.</p>
  *
- * <p><b>NOTE:</b> This class is only used during
- * indexing. When retrieving the stored field value from a
- * {@link Document} instance after search, you will get a
- * conventional {@link Fieldable} instance where the numeric
- * values are returned as {@link String}s (according to
- * <code>toString(value)</code> of the used data type).
- *
  * @since 2.9
  */
 public final class NumericField extends AbstractField {
 
-  private final NumericTokenStream numericTS;
+  /** Data type of the value in {@link NumericField}.
+   * @since 3.2
+   */
+  public static enum DataType { INT, LONG, FLOAT, DOUBLE }
+
+  private transient NumericTokenStream numericTS;
+  private DataType type;
+  private final int precisionStep;
 
   /**
    * Creates a field for numeric values using the default <code>precisionStep</code>
@@ -158,8 +158,8 @@ public final class NumericField extends AbstractField {
    * a numeric value, before indexing a document containing this field,
    * set a value using the various set<em>???</em>Value() methods.
    * @param name the field name
-   * @param store if the field should be stored in plain text form
-   *  (according to <code>toString(value)</code> of the used data type)
+   * @param store if the field should be stored, {@link Document#getFieldable}
+   * then returns {@code NumericField} instances on search results.
    * @param index if the field should be indexed using {@link NumericTokenStream}
    */
   public NumericField(String name, Field.Store store, boolean index) {
@@ -186,19 +186,43 @@ public final class NumericField extends AbstractField {
    * set a value using the various set<em>???</em>Value() methods.
    * @param name the field name
    * @param precisionStep the used <a href="../search/NumericRangeQuery.html#precisionStepDesc">precision step</a>
-   * @param store if the field should be stored in plain text form
-   *  (according to <code>toString(value)</code> of the used data type)
+   * @param store if the field should be stored, {@link Document#getFieldable}
+   * then returns {@code NumericField} instances on search results.
    * @param index if the field should be indexed using {@link NumericTokenStream}
    */
   public NumericField(String name, int precisionStep, Field.Store store, boolean index) {
     super(name, store, index ? Field.Index.ANALYZED_NO_NORMS : Field.Index.NO, Field.TermVector.NO);
+    this.precisionStep = precisionStep;
     setOmitTermFreqAndPositions(true);
-    numericTS = new NumericTokenStream(precisionStep);
   }
 
   /** Returns a {@link NumericTokenStream} for indexing the numeric value. */
   public TokenStream tokenStreamValue()   {
-    return isIndexed() ? numericTS : null;
+    if (!isIndexed())
+      return null;
+    if (numericTS == null) {
+      // lazy init the TokenStream as it is heavy to instantiate (attributes,...),
+      // if not needed (stored field loading)
+      numericTS = new NumericTokenStream(precisionStep);
+      // initialize value in TokenStream
+      if (fieldsData != null) {
+        assert type != null;
+        final Number val = (Number) fieldsData;
+        switch (type) {
+          case INT:
+            numericTS.setIntValue(val.intValue()); break;
+          case LONG:
+            numericTS.setLongValue(val.longValue()); break;
+          case FLOAT:
+            numericTS.setFloatValue(val.floatValue()); break;
+          case DOUBLE:
+            numericTS.setDoubleValue(val.doubleValue()); break;
+          default:
+            assert false : "Should never get here";
+        }
+      }
+    }
+    return numericTS;
   }
   
   /** Returns always <code>null</code> for numeric fields */
@@ -212,7 +236,10 @@ public final class NumericField extends AbstractField {
     return null;
   }
     
-  /** Returns the numeric value as a string (how it is stored, when {@link Field.Store#YES} is chosen). */
+  /** Returns the numeric value as a string. This format is also returned if you call {@link Document#get(String)}
+   * on search results. It is recommended to use {@link Document#getFieldable} instead
+   * that returns {@code NumericField} instances. You can then use {@link #getNumericValue}
+   * to return the stored value. */
   public String stringValue()   {
     return (fieldsData == null) ? null : fieldsData.toString();
   }
@@ -224,7 +251,14 @@ public final class NumericField extends AbstractField {
   
   /** Returns the precision step. */
   public int getPrecisionStep() {
-    return numericTS.getPrecisionStep();
+    return precisionStep;
+  }
+  
+  /** Returns the data type of the current value, {@code null} if not yet set.
+   * @since 3.2
+   */
+  public DataType getDataType() {
+    return type;
   }
   
   /**
@@ -234,8 +268,9 @@ public final class NumericField extends AbstractField {
    * <code>document.add(new NumericField(name, precisionStep).setLongValue(value))</code>
    */
   public NumericField setLongValue(final long value) {
-    numericTS.setLongValue(value);
+    if (numericTS != null) numericTS.setLongValue(value);
     fieldsData = Long.valueOf(value);
+    type = DataType.LONG;
     return this;
   }
   
@@ -246,8 +281,9 @@ public final class NumericField extends AbstractField {
    * <code>document.add(new NumericField(name, precisionStep).setIntValue(value))</code>
    */
   public NumericField setIntValue(final int value) {
-    numericTS.setIntValue(value);
+    if (numericTS != null) numericTS.setIntValue(value);
     fieldsData = Integer.valueOf(value);
+    type = DataType.INT;
     return this;
   }
   
@@ -258,8 +294,9 @@ public final class NumericField extends AbstractField {
    * <code>document.add(new NumericField(name, precisionStep).setDoubleValue(value))</code>
    */
   public NumericField setDoubleValue(final double value) {
-    numericTS.setDoubleValue(value);
+    if (numericTS != null) numericTS.setDoubleValue(value);
     fieldsData = Double.valueOf(value);
+    type = DataType.DOUBLE;
     return this;
   }
   
@@ -270,8 +307,9 @@ public final class NumericField extends AbstractField {
    * <code>document.add(new NumericField(name, precisionStep).setFloatValue(value))</code>
    */
   public NumericField setFloatValue(final float value) {
-    numericTS.setFloatValue(value);
+    if (numericTS != null) numericTS.setFloatValue(value);
     fieldsData = Float.valueOf(value);
+    type = DataType.FLOAT;
     return this;
   }
 
diff --git a/lucene/src/java/org/apache/lucene/index/FieldsReader.java b/lucene/src/java/org/apache/lucene/index/FieldsReader.java
index 76c0ed2..e135d6d 100644
--- a/lucene/src/java/org/apache/lucene/index/FieldsReader.java
+++ b/lucene/src/java/org/apache/lucene/index/FieldsReader.java
@@ -24,10 +24,11 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldSelector;
 import org.apache.lucene.document.FieldSelectorResult;
 import org.apache.lucene.document.Fieldable;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.document.NumericField;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.BufferedIndexInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.CloseableThreadLocal;
 
 import java.io.IOException;
@@ -212,40 +213,39 @@ public final class FieldsReader implements Cloneable {
 
     Document doc = new Document();
     int numFields = fieldsStream.readVInt();
-    for (int i = 0; i < numFields; i++) {
+    out: for (int i = 0; i < numFields; i++) {
       int fieldNumber = fieldsStream.readVInt();
       FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);
       FieldSelectorResult acceptField = fieldSelector == null ? FieldSelectorResult.LOAD : fieldSelector.accept(fi.name);
       
-      byte bits = fieldsStream.readByte();
-      assert bits <= FieldsWriter.FIELD_IS_TOKENIZED + FieldsWriter.FIELD_IS_BINARY;
+      int bits = fieldsStream.readByte() & 0xFF;
+      assert bits <= (FieldsWriter.FIELD_IS_NUMERIC_MASK | FieldsWriter.FIELD_IS_TOKENIZED | FieldsWriter.FIELD_IS_BINARY): "bits=" + Integer.toHexString(bits);
 
       boolean tokenize = (bits & FieldsWriter.FIELD_IS_TOKENIZED) != 0;
       boolean binary = (bits & FieldsWriter.FIELD_IS_BINARY) != 0;
-      //TODO: Find an alternative approach here if this list continues to grow beyond the
-      //list of 5 or 6 currently here.  See Lucene 762 for discussion
-      if (acceptField.equals(FieldSelectorResult.LOAD)) {
-        addField(doc, fi, binary, tokenize);
-      }
-      else if (acceptField.equals(FieldSelectorResult.LOAD_AND_BREAK)){
-        addField(doc, fi, binary, tokenize);
-        break;//Get out of this loop
-      }
-      else if (acceptField.equals(FieldSelectorResult.LAZY_LOAD)) {
-        addFieldLazy(doc, fi, binary, tokenize, true);
-      }
-      else if (acceptField.equals(FieldSelectorResult.LATENT)) {
-        addFieldLazy(doc, fi, binary, tokenize, false);
-      }
-      else if (acceptField.equals(FieldSelectorResult.SIZE)){
-        skipField(addFieldSize(doc, fi, binary));
-      }
-      else if (acceptField.equals(FieldSelectorResult.SIZE_AND_BREAK)){
-        addFieldSize(doc, fi, binary);
-        break;
-      }
-      else {
-        skipField();
+      final int numeric = bits & FieldsWriter.FIELD_IS_NUMERIC_MASK;
+
+      switch (acceptField) {
+        case LOAD:
+          addField(doc, fi, binary, tokenize, numeric);
+          break;
+        case LOAD_AND_BREAK:
+          addField(doc, fi, binary, tokenize, numeric);
+          break out; //Get out of this loop
+        case LAZY_LOAD:
+          addFieldLazy(doc, fi, binary, tokenize, true, numeric);
+          break;
+        case LATENT:
+          addFieldLazy(doc, fi, binary, tokenize, false, numeric);
+          break;
+        case SIZE:
+          skipFieldBytes(addFieldSize(doc, fi, binary, numeric));
+          break;
+        case SIZE_AND_BREAK:
+          addFieldSize(doc, fi, binary, numeric);
+          break out; //Get out of this loop
+        default:
+          skipField(numeric);
       }
     }
 
@@ -282,72 +282,121 @@ public final class FieldsReader implements Cloneable {
    * Skip the field.  We still have to read some of the information about the field, but can skip past the actual content.
    * This will have the most payoff on large fields.
    */
-  private void skipField() throws IOException {
-    skipField(fieldsStream.readVInt());
+  private void skipField(int numeric) throws IOException {
+    final int numBytes;
+    switch(numeric) {
+      case 0:
+        numBytes = fieldsStream.readVInt();
+        break;
+      case FieldsWriter.FIELD_IS_NUMERIC_INT:
+      case FieldsWriter.FIELD_IS_NUMERIC_FLOAT:
+        numBytes = 4;
+        break;
+      case FieldsWriter.FIELD_IS_NUMERIC_LONG:
+      case FieldsWriter.FIELD_IS_NUMERIC_DOUBLE:
+        numBytes = 8;
+        break;
+      default:
+        throw new FieldReaderException("Invalid numeric type: " + Integer.toHexString(numeric));
+    }
+    
+    skipFieldBytes(numBytes);
   }
   
-  private void skipField(int toRead) throws IOException {
+  private void skipFieldBytes(int toRead) throws IOException {
     fieldsStream.seek(fieldsStream.getFilePointer() + toRead);
   }
 
-  private void addFieldLazy(Document doc, FieldInfo fi, boolean binary, boolean tokenize, boolean cacheResult) throws IOException {
+  private NumericField loadNumericField(FieldInfo fi, int numeric) throws IOException {
+    assert numeric != 0;
+    switch(numeric) {
+      case FieldsWriter.FIELD_IS_NUMERIC_INT:
+        return new NumericField(fi.name, Field.Store.YES, fi.isIndexed).setIntValue(fieldsStream.readInt());
+      case FieldsWriter.FIELD_IS_NUMERIC_LONG:
+        return new NumericField(fi.name, Field.Store.YES, fi.isIndexed).setLongValue(fieldsStream.readLong());
+      case FieldsWriter.FIELD_IS_NUMERIC_FLOAT:
+        return new NumericField(fi.name, Field.Store.YES, fi.isIndexed).setFloatValue(Float.intBitsToFloat(fieldsStream.readInt()));
+      case FieldsWriter.FIELD_IS_NUMERIC_DOUBLE:
+        return new NumericField(fi.name, Field.Store.YES, fi.isIndexed).setDoubleValue(Double.longBitsToDouble(fieldsStream.readLong()));
+      default:
+        throw new FieldReaderException("Invalid numeric type: " + Integer.toHexString(numeric));
+    }
+  }
+
+  private void addFieldLazy(Document doc, FieldInfo fi, boolean binary, boolean tokenize, boolean cacheResult, int numeric) throws IOException {
+    final AbstractField f;
     if (binary) {
       int toRead = fieldsStream.readVInt();
       long pointer = fieldsStream.getFilePointer();
-      //was: doc.add(new Fieldable(fi.name, b, Fieldable.Store.YES));
-      doc.add(new LazyField(fi.name, Field.Store.YES, toRead, pointer, binary, cacheResult));
+      f = new LazyField(fi.name, Field.Store.YES, toRead, pointer, binary, cacheResult);
       //Need to move the pointer ahead by toRead positions
       fieldsStream.seek(pointer + toRead);
+    } else if (numeric != 0) {
+      f = loadNumericField(fi, numeric);
     } else {
       Field.Store store = Field.Store.YES;
       Field.Index index = Field.Index.toIndex(fi.isIndexed, tokenize);
       Field.TermVector termVector = Field.TermVector.toTermVector(fi.storeTermVector, fi.storeOffsetWithTermVector, fi.storePositionWithTermVector);
 
-      AbstractField f;
       int length = fieldsStream.readVInt();
       long pointer = fieldsStream.getFilePointer();
       //Skip ahead of where we are by the length of what is stored
       fieldsStream.seek(pointer+length);
       f = new LazyField(fi.name, store, index, termVector, length, pointer, binary, cacheResult);
-      f.setOmitNorms(fi.omitNorms);
-      f.setOmitTermFreqAndPositions(fi.omitTermFreqAndPositions);
-
-      doc.add(f);
     }
-
+    
+    f.setOmitNorms(fi.omitNorms);
+    f.setOmitTermFreqAndPositions(fi.omitTermFreqAndPositions);
+    doc.add(f);
   }
 
-  private void addField(Document doc, FieldInfo fi, boolean binary, boolean tokenize) throws CorruptIndexException, IOException {
+  private void addField(Document doc, FieldInfo fi, boolean binary, boolean tokenize, int numeric) throws CorruptIndexException, IOException {
+    final AbstractField f;
 
     if (binary) {
       int toRead = fieldsStream.readVInt();
       final byte[] b = new byte[toRead];
       fieldsStream.readBytes(b, 0, b.length);
-      doc.add(new Field(fi.name, b));
+      f = new Field(fi.name, b);
+    } else if (numeric != 0) {
+      f = loadNumericField(fi, numeric);
     } else {
-      Field.Store store = Field.Store.YES;
       Field.Index index = Field.Index.toIndex(fi.isIndexed, tokenize);
       Field.TermVector termVector = Field.TermVector.toTermVector(fi.storeTermVector, fi.storeOffsetWithTermVector, fi.storePositionWithTermVector);
-
-      AbstractField f;
       f = new Field(fi.name,     // name
-       false,
-              fieldsStream.readString(), // read value
-              store,
-              index,
-              termVector);
-      f.setOmitTermFreqAndPositions(fi.omitTermFreqAndPositions);
-      f.setOmitNorms(fi.omitNorms);
-
-      doc.add(f);
+        false,
+        fieldsStream.readString(), // read value
+        Field.Store.YES,
+        index,
+        termVector);
     }
+    
+    f.setOmitTermFreqAndPositions(fi.omitTermFreqAndPositions);
+    f.setOmitNorms(fi.omitNorms);
+    doc.add(f);
   }
   
   // Add the size of field as a byte[] containing the 4 bytes of the integer byte size (high order byte first; char = 2 bytes)
   // Read just the size -- caller must skip the field content to continue reading fields
   // Return the size in bytes or chars, depending on field type
-  private int addFieldSize(Document doc, FieldInfo fi, boolean binary) throws IOException {
-    int size = fieldsStream.readVInt(), bytesize = binary ? size : 2*size;
+  private int addFieldSize(Document doc, FieldInfo fi, boolean binary, int numeric) throws IOException {
+    final int bytesize, size;
+    switch(numeric) {
+      case 0:
+        size = fieldsStream.readVInt();
+        bytesize = binary ? size : 2*size;
+        break;
+      case FieldsWriter.FIELD_IS_NUMERIC_INT:
+      case FieldsWriter.FIELD_IS_NUMERIC_FLOAT:
+        size = bytesize = 4;
+        break;
+      case FieldsWriter.FIELD_IS_NUMERIC_LONG:
+      case FieldsWriter.FIELD_IS_NUMERIC_DOUBLE:
+        size = bytesize = 8;
+        break;
+      default:
+        throw new FieldReaderException("Invalid numeric type: " + Integer.toHexString(numeric));
+    }
     byte[] sizebytes = new byte[4];
     sizebytes[0] = (byte) (bytesize>>>24);
     sizebytes[1] = (byte) (bytesize>>>16);
@@ -358,7 +407,7 @@ public final class FieldsReader implements Cloneable {
   }
 
   /**
-   * A Lazy implementation of Fieldable that differs loading of fields until asked for, instead of when the Document is
+   * A Lazy implementation of Fieldable that defers loading of fields until asked for, instead of when the Document is
    * loaded.
    */
   private class LazyField extends AbstractField implements Fieldable {
diff --git a/lucene/src/java/org/apache/lucene/index/FieldsWriter.java b/lucene/src/java/org/apache/lucene/index/FieldsWriter.java
index 303aa91..9efd909 100644
--- a/lucene/src/java/org/apache/lucene/index/FieldsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/FieldsWriter.java
@@ -21,22 +21,40 @@ import java.util.List;
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.document.NumericField;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.IOUtils;
 
 final class FieldsWriter {
-  static final byte FIELD_IS_TOKENIZED = 0x1;
-  static final byte FIELD_IS_BINARY = 0x2;
+  static final int FIELD_IS_TOKENIZED = 1 << 0;
+  static final int FIELD_IS_BINARY = 1 << 1;
 
+  // the old bit 1 << 2 was compressed, is now left out
+
+  private static final int _NUMERIC_BIT_SHIFT = 3;
+  static final int FIELD_IS_NUMERIC_MASK = 0x07 << _NUMERIC_BIT_SHIFT;
+
+  static final int FIELD_IS_NUMERIC_INT = 1 << _NUMERIC_BIT_SHIFT;
+  static final int FIELD_IS_NUMERIC_LONG = 2 << _NUMERIC_BIT_SHIFT;
+  static final int FIELD_IS_NUMERIC_FLOAT = 3 << _NUMERIC_BIT_SHIFT;
+  static final int FIELD_IS_NUMERIC_DOUBLE = 4 << _NUMERIC_BIT_SHIFT;
+  // currently unused: static final int FIELD_IS_NUMERIC_SHORT = 5 << _NUMERIC_BIT_SHIFT;
+  // currently unused: static final int FIELD_IS_NUMERIC_BYTE = 6 << _NUMERIC_BIT_SHIFT;
+
+  // the next possible bits are: 1 << 6; 1 << 7
+  
   // Lucene 3.0: Removal of compressed fields
   static final int FORMAT_LUCENE_3_0_NO_COMPRESSED_FIELDS = 2;
 
+  // Lucene 3.2: NumericFields are stored in binary format
+  static final int FORMAT_LUCENE_3_2_NUMERIC_FIELDS = 3;
+
   // NOTE: if you introduce a new format, make it 1 higher
   // than the current one, and always change this if you
   // switch to a new format!
-  static final int FORMAT_CURRENT = FORMAT_LUCENE_3_0_NO_COMPRESSED_FIELDS;
+  static final int FORMAT_CURRENT = FORMAT_LUCENE_3_2_NUMERIC_FIELDS;
 
   // when removing support for old versions, leave the last supported version here
   static final int FORMAT_MINIMUM = FORMAT_LUCENE_3_0_NO_COMPRESSED_FIELDS;
@@ -121,13 +139,26 @@ final class FieldsWriter {
 
   final void writeField(int fieldNumber, Fieldable field) throws IOException {
     fieldsStream.writeVInt(fieldNumber);
-    byte bits = 0;
+    int bits = 0;
     if (field.isTokenized())
-      bits |= FieldsWriter.FIELD_IS_TOKENIZED;
+      bits |= FIELD_IS_TOKENIZED;
     if (field.isBinary())
-      bits |= FieldsWriter.FIELD_IS_BINARY;
-
-    fieldsStream.writeByte(bits);
+      bits |= FIELD_IS_BINARY;
+    if (field instanceof NumericField) {
+      switch (((NumericField) field).getDataType()) {
+        case INT:
+          bits |= FIELD_IS_NUMERIC_INT; break;
+        case LONG:
+          bits |= FIELD_IS_NUMERIC_LONG; break;
+        case FLOAT:
+          bits |= FIELD_IS_NUMERIC_FLOAT; break;
+        case DOUBLE:
+          bits |= FIELD_IS_NUMERIC_DOUBLE; break;
+        default:
+          assert false : "Should never get here";
+      }
+    }
+    fieldsStream.writeByte((byte) bits);
 
     if (field.isBinary()) {
       final byte[] data;
@@ -139,8 +170,22 @@ final class FieldsWriter {
 
       fieldsStream.writeVInt(len);
       fieldsStream.writeBytes(data, offset, len);
-    }
-    else {
+    } else if (field instanceof NumericField) {
+      final NumericField nf = (NumericField) field;
+      final Number n = nf.getNumericValue();
+      switch (nf.getDataType()) {
+        case INT:
+          fieldsStream.writeInt(n.intValue()); break;
+        case LONG:
+          fieldsStream.writeLong(n.longValue()); break;
+        case FLOAT:
+          fieldsStream.writeInt(Float.floatToIntBits(n.floatValue())); break;
+        case DOUBLE:
+          fieldsStream.writeLong(Double.doubleToLongBits(n.doubleValue())); break;
+        default:
+          assert false : "Should never get here";
+      }
+    } else {
       fieldsStream.writeString(field.stringValue());
     }
   }
diff --git a/lucene/src/site/src/documentation/content/xdocs/fileformats.xml b/lucene/src/site/src/documentation/content/xdocs/fileformats.xml
index 17ca534..228e18a 100644
--- a/lucene/src/site/src/documentation/content/xdocs/fileformats.xml
+++ b/lucene/src/site/src/documentation/content/xdocs/fileformats.xml
@@ -94,6 +94,11 @@
             Additionally segments track explicitly whether or
             not they have term vectors. See LUCENE-2811 for details.
            </p>
+        <p>
+            In version 3.2, numeric fields are written as natively
+            to stored fields file, previously they were stored in
+            text format only.
+           </p>
         </section>
 
         <section id="Definitions"><title>Definitions</title>
@@ -1300,10 +1305,18 @@
                                 <li>third bit is one for fields with compression option enabled
                                     (if compression is enabled, the algorithm used is ZLIB),
                                     only available for indexes until Lucene version 2.9.x</li>
+                                <li>4th to 6th bits (mask: 0x7&lt;&lt;3) define the type of a
+                                numeric field: <ul>
+                                  <li>all bits in mask are cleared if no numeric field at all</li>
+                                  <li>1&lt;&lt;3: Value is Int</li>
+                                  <li>2&lt;&lt;3: Value is Long</li>
+                                  <li>3&lt;&lt;3: Value is Int as Float (as of Integer.intBitsToFloat)</li>
+                                  <li>4&lt;&lt;3: Value is Long as Double (as of Double.longBitsToDouble)</li>
+                                </ul></li>
                             </ul>
                         </p>
                         <p>Value --&gt;
-                            String | BinaryValue (depending on Bits)
+                            String | BinaryValue | Int | Long (depending on Bits)
                         </p>
                         <p>BinaryValue --&gt;
                             ValueSize, &lt;Byte&gt;^ValueSize
diff --git a/lucene/src/site/src/documentation/content/xdocs/gettingstarted.xml b/lucene/src/site/src/documentation/content/xdocs/gettingstarted.xml
index 4dde0f3..7ab6441 100644
--- a/lucene/src/site/src/documentation/content/xdocs/gettingstarted.xml
+++ b/lucene/src/site/src/documentation/content/xdocs/gettingstarted.xml
@@ -28,11 +28,11 @@ may wish to skip sections.
 
 <ul>
 	<li><a href="demo.html">About the command-line Lucene demo and its usage</a>.  This section
-	is intended for anyone who wants to use the command-line Lucene demo.</li> <p/>
+	is intended for anyone who wants to use the command-line Lucene demo.</li>
 
 	<li><a href="demo2.html">About the sources and implementation for the command-line Lucene
 	demo</a>.  This section walks through the implementation details (sources) of the
-	command-line Lucene demo.  This section is intended for developers.</li> <p/>
+	command-line Lucene demo.  This section is intended for developers.</li>
 </ul>
 </section>
 
diff --git a/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java b/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
index e467499..72c702a 100644
--- a/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
+++ b/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
@@ -90,6 +90,8 @@ public class TestBackwardsCompatibility extends LuceneTestCase {
                              "30.nocfs",
                              "31.cfs",
                              "31.nocfs",
+                             "32.cfs",
+                             "32.nocfs",
   };
   
   final String[] unsupportedNames = {"19.cfs",
diff --git a/lucene/src/test/org/apache/lucene/index/TestFieldsReader.java b/lucene/src/test/org/apache/lucene/index/TestFieldsReader.java
index 26b1717..75a9be9 100644
--- a/lucene/src/test/org/apache/lucene/index/TestFieldsReader.java
+++ b/lucene/src/test/org/apache/lucene/index/TestFieldsReader.java
@@ -24,12 +24,14 @@ import java.util.*;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericField;
 import org.apache.lucene.document.FieldSelector;
 import org.apache.lucene.document.FieldSelectorResult;
 import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.document.LoadFirstFieldSelector;
 import org.apache.lucene.document.SetBasedFieldSelector;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.BufferedIndexInput;
 import org.apache.lucene.store.Directory;
@@ -511,4 +513,69 @@ public class TestFieldsReader extends LuceneTestCase {
     }
 
   }
+  
+  public void testNumericField() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random, dir);
+    final int numDocs = _TestUtil.nextInt(random, 500, 1000) * RANDOM_MULTIPLIER;
+    final Number[] answers = new Number[numDocs];
+    final NumericField.DataType[] typeAnswers = new NumericField.DataType[numDocs];
+    for(int id=0;id<numDocs;id++) {
+      Document doc = new Document();
+      NumericField nf = new NumericField("nf", Field.Store.YES, false);
+      doc.add(nf);
+      final Number answer;
+      final NumericField.DataType typeAnswer;
+      if (random.nextBoolean()) {
+        // float/double
+        if (random.nextBoolean()) {
+          final float f = random.nextFloat();
+          nf.setFloatValue(f);
+          answer = Float.valueOf(f);
+          typeAnswer = NumericField.DataType.FLOAT;
+        } else {
+          final double d = random.nextDouble();
+          nf.setDoubleValue(d);
+          answer = Double.valueOf(d);
+          typeAnswer = NumericField.DataType.DOUBLE;
+        }
+      } else {
+        // int/long
+        if (random.nextBoolean()) {
+          final int i = random.nextInt();
+          nf.setIntValue(i);
+          answer = Integer.valueOf(i);
+          typeAnswer = NumericField.DataType.INT;
+        } else {
+          final long l = random.nextLong();
+          nf.setLongValue(l);
+          answer = Long.valueOf(l);
+          typeAnswer = NumericField.DataType.LONG;
+        }
+      }
+      answers[id] = answer;
+      typeAnswers[id] = typeAnswer;
+      doc.add(new NumericField("id", Integer.MAX_VALUE, Field.Store.NO, true).setIntValue(id));
+      w.addDocument(doc);
+    }
+    final IndexReader r = w.getReader();
+    w.close();
+    
+    assertEquals(numDocs, r.numDocs());
+
+    for(IndexReader sub : r.getSequentialSubReaders()) {
+      final int[] ids = FieldCache.DEFAULT.getInts(sub, "id");
+      for(int docID=0;docID<sub.numDocs();docID++) {
+        final Document doc = sub.document(docID);
+        final Fieldable f = doc.getFieldable("nf");
+        assertTrue("got f=" + f, f instanceof NumericField);
+        final NumericField nf = (NumericField) f;
+        assertEquals(answers[ids[docID]], nf.getNumericValue());
+        assertSame(typeAnswers[ids[docID]], nf.getDataType());
+      }
+    }
+    r.close();
+    dir.close();
+  }
+  
 }
diff --git a/lucene/src/test/org/apache/lucene/index/index.31.cfs.zip b/lucene/src/test/org/apache/lucene/index/index.31.cfs.zip
index bfbe179..8f123a7 100644
Binary files a/lucene/src/test/org/apache/lucene/index/index.31.cfs.zip and b/lucene/src/test/org/apache/lucene/index/index.31.cfs.zip differ
diff --git a/lucene/src/test/org/apache/lucene/index/index.31.nocfs.zip b/lucene/src/test/org/apache/lucene/index/index.31.nocfs.zip
index c456941..21434e1 100644
Binary files a/lucene/src/test/org/apache/lucene/index/index.31.nocfs.zip and b/lucene/src/test/org/apache/lucene/index/index.31.nocfs.zip differ
diff --git a/lucene/src/test/org/apache/lucene/index/index.32.cfs.zip b/lucene/src/test/org/apache/lucene/index/index.32.cfs.zip
new file mode 100644
index 0000000..5293983
Binary files /dev/null and b/lucene/src/test/org/apache/lucene/index/index.32.cfs.zip differ
diff --git a/lucene/src/test/org/apache/lucene/index/index.32.nocfs.zip b/lucene/src/test/org/apache/lucene/index/index.32.nocfs.zip
new file mode 100644
index 0000000..c32fbd6
Binary files /dev/null and b/lucene/src/test/org/apache/lucene/index/index.32.nocfs.zip differ
diff --git a/solr/src/java/org/apache/solr/handler/component/TermVectorComponent.java b/solr/src/java/org/apache/solr/handler/component/TermVectorComponent.java
index 56b9d48..24c5256 100644
--- a/solr/src/java/org/apache/solr/handler/component/TermVectorComponent.java
+++ b/solr/src/java/org/apache/solr/handler/component/TermVectorComponent.java
@@ -208,7 +208,7 @@ public class TermVectorComponent extends SearchComponent implements SolrCoreAwar
 
       if (keyField != null) {
         Document document = reader.document(docId, fieldSelector);
-        Fieldable uniqId = document.getField(uniqFieldName);
+        Fieldable uniqId = document.getFieldable(uniqFieldName);
         String uniqVal = null;
         if (uniqId != null) {
           uniqVal = keyField.getType().storedToReadable(uniqId);          
diff --git a/solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java b/solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java
index fbb1489..b0be39f 100644
--- a/solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java
+++ b/solr/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java
@@ -401,13 +401,24 @@ public class DefaultSolrHighlighter extends SolrHighlighter implements PluginInf
   
   private void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,
       int docId, Document doc, String fieldName ) throws IOException {
+    final SolrIndexSearcher searcher = req.getSearcher();
+    final IndexSchema schema = searcher.getSchema();
+    
+    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -
+    // so we disable them until fixed (see LUCENE-3080)!
+    // BEGIN: Hack
+    final SchemaField schemaField = schema.getFieldOrNull(fieldName);
+    if (schemaField != null && (
+      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||
+      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)
+    )) return;
+    // END: Hack
+    
     SolrParams params = req.getParams(); 
     String[] docTexts = doc.getValues(fieldName);
     // according to Document javadoc, doc.getValues() never returns null. check empty instead of null
     if (docTexts.length == 0) return;
     
-    SolrIndexSearcher searcher = req.getSearcher();
-    IndexSchema schema = searcher.getSchema();
     TokenStream tstream = null;
     int numFragments = getMaxSnippets(fieldName, params);
     boolean mergeContiguousFragments = isMergeContiguousFragments(fieldName, params);
diff --git a/solr/src/java/org/apache/solr/schema/SchemaField.java b/solr/src/java/org/apache/solr/schema/SchemaField.java
index 41ad8e0..bb2d3e7 100644
--- a/solr/src/java/org/apache/solr/schema/SchemaField.java
+++ b/solr/src/java/org/apache/solr/schema/SchemaField.java
@@ -19,7 +19,6 @@ package org.apache.solr.schema;
 
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
-import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.search.SortField;
 import org.apache.solr.search.QParser;
diff --git a/solr/src/java/org/apache/solr/schema/TrieDateField.java b/solr/src/java/org/apache/solr/schema/TrieDateField.java
index 7e3b30d..8d58fa5 100755
--- a/solr/src/java/org/apache/solr/schema/TrieDateField.java
+++ b/solr/src/java/org/apache/solr/schema/TrieDateField.java
@@ -18,210 +18,125 @@
 package org.apache.solr.schema;
 
 import org.apache.noggit.CharArr;
-import org.apache.solr.common.SolrException;
-import org.apache.solr.analysis.CharFilterFactory;
-import org.apache.solr.analysis.TokenFilterFactory;
-import org.apache.solr.analysis.TokenizerChain;
-import org.apache.solr.analysis.TrieTokenizerFactory;
-import org.apache.solr.search.function.*;
+import org.apache.solr.search.function.ValueSource;
 import org.apache.solr.search.QParser;
 import org.apache.solr.response.TextResponseWriter;
 import org.apache.lucene.document.Fieldable;
-import org.apache.lucene.document.Field;
 import org.apache.lucene.search.SortField;
-import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.search.cache.CachedArrayCreator;
-import org.apache.lucene.search.cache.LongValuesCreator;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.NumericTokenStream;
 
 import java.util.Map;
 import java.util.Date;
 import java.io.IOException;
 
 public class TrieDateField extends DateField {
-  protected int precisionStepArg = TrieField.DEFAULT_PRECISION_STEP;  // the one passed in or defaulted
-  protected int precisionStep = precisionStepArg;     // normalized
+
+  final TrieField wrappedField = new TrieField() {{
+    type = TrieTypes.DATE;
+  }};
 
   @Override
   protected void init(IndexSchema schema, Map<String, String> args) {
-    String p = args.remove("precisionStep");
-    if (p != null) {
-       precisionStepArg = Integer.parseInt(p);
-    }
-    // normalize the precisionStep
-    precisionStep = precisionStepArg;
-    if (precisionStep<=0 || precisionStep>=64) precisionStep=Integer.MAX_VALUE;
-
-    CharFilterFactory[] filterFactories = new CharFilterFactory[0];
-    TokenFilterFactory[] tokenFilterFactories = new TokenFilterFactory[0];
-    analyzer = new TokenizerChain(filterFactories, new TrieTokenizerFactory(TrieField.TrieTypes.DATE, precisionStep), tokenFilterFactories);
-    // for query time we only need one token, so we use the biggest possible precisionStep:
-    queryAnalyzer = new TokenizerChain(filterFactories, new TrieTokenizerFactory(TrieField.TrieTypes.DATE, Integer.MAX_VALUE), tokenFilterFactories);
+    wrappedField.init(schema, args);
+    analyzer = wrappedField.analyzer;
+    queryAnalyzer = wrappedField.queryAnalyzer;
   }
 
   @Override
   public Date toObject(Fieldable f) {
-    byte[] arr = f.getBinaryValue();
-    if (arr==null) throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,TrieField.badFieldString(f));
-    return new Date(TrieFieldHelper.toLong(arr));
+    return (Date) wrappedField.toObject(f);
   }
 
   @Override
   public Object toObject(SchemaField sf, BytesRef term) {
-    return new Date(NumericUtils.prefixCodedToLong(term));
+    return wrappedField.toObject(sf, term);
   }
 
   @Override
   public SortField getSortField(SchemaField field, boolean top) {
-    field.checkSortability();
-
-    int flags = CachedArrayCreator.CACHE_VALUES_AND_BITS;
-    boolean sortMissingLast  = field.sortMissingLast();
-    boolean sortMissingFirst = field.sortMissingFirst();
-
-    Object missingValue = null;
-    if( sortMissingLast ) {
-      missingValue = top ? Long.MIN_VALUE : Long.MAX_VALUE;
-    } else if( sortMissingFirst ) {
-      missingValue = top ? Long.MAX_VALUE : Long.MIN_VALUE;
-    }
-    return new SortField(new LongValuesCreator(field.getName(), FieldCache.NUMERIC_UTILS_LONG_PARSER, flags), top).setMissingValue(missingValue);
+    return wrappedField.getSortField(field, top);
   }
 
   @Override
   public ValueSource getValueSource(SchemaField field, QParser parser) {
-    field.checkFieldCacheSource(parser);
-    return new TrieDateFieldSource( new LongValuesCreator( field.getName(), FieldCache.NUMERIC_UTILS_LONG_PARSER, CachedArrayCreator.CACHE_VALUES_AND_BITS ));
+    return wrappedField.getValueSource(field, parser);
+  }
+
+  /**
+   * @return the precisionStep used to index values into the field
+   */
+  public int getPrecisionStep() {
+    return wrappedField.getPrecisionStep();
   }
 
+
   @Override
   public void write(TextResponseWriter writer, String name, Fieldable f) throws IOException {
-    byte[] arr = f.getBinaryValue();
-    if (arr==null) {
-      writer.writeStr(name, TrieField.badFieldString(f),true);
-      return;
-    }
-
-    writer.writeDate(name,new Date(TrieFieldHelper.toLong(arr)));
+    wrappedField.write(writer, name, f);
   }
 
   @Override
   public boolean isTokenized() {
-    return true;
+    return wrappedField.isTokenized();
   }
 
-  /**
-   * @return the precisionStep used to index values into the field
-   */
-  public int getPrecisionStep() {
-    return precisionStepArg;
+  @Override
+  public boolean multiValuedFieldCache() {
+    return wrappedField.multiValuedFieldCache();
   }
 
-
-
   @Override
   public String storedToReadable(Fieldable f) {
-    return toExternal(f);
+    return wrappedField.storedToReadable(f);
   }
 
   @Override
   public String readableToIndexed(String val) {  
-    // TODO: Numeric should never be handled as String, that may break in future lucene versions! Change to use BytesRef for term texts!
-    BytesRef bytes = new BytesRef(NumericUtils.BUF_SIZE_LONG);
-    NumericUtils.longToPrefixCoded(super.parseMath(null, val).getTime(), 0, bytes);
-    return bytes.utf8ToString();
+    return wrappedField.readableToIndexed(val);
   }
 
   @Override
   public String toInternal(String val) {
-    return readableToIndexed(val);
+    return wrappedField.toInternal(val);
   }
 
   @Override
   public String toExternal(Fieldable f) {
-    byte[] arr = f.getBinaryValue();
-    if (arr==null) return TrieField.badFieldString(f);
-     return super.toExternal(new Date(TrieFieldHelper.toLong(arr)));
+    return wrappedField.toExternal(f);
   }
 
   @Override
   public String indexedToReadable(String _indexedForm) {
-    final BytesRef indexedForm = new BytesRef(_indexedForm);
-    return super.toExternal( new Date(NumericUtils.prefixCodedToLong(indexedForm)) );
+    return wrappedField.indexedToReadable(_indexedForm);
   }
 
   @Override
   public void indexedToReadable(BytesRef input, CharArr out) {
-    String ext =  super.toExternal( new Date(NumericUtils.prefixCodedToLong(input)) );
-    out.write(ext);
+    wrappedField.indexedToReadable(input, out);
   }
 
   @Override
   public String storedToIndexed(Fieldable f) {
-    // TODO: optimize to remove redundant string conversion
-    return readableToIndexed(storedToReadable(f));
+    return wrappedField.storedToIndexed(f);
   }
 
   @Override
   public Fieldable createField(SchemaField field, Object value, float boost) {
-    boolean indexed = field.indexed();
-    boolean stored = field.stored();
-
-    if (!indexed && !stored) {
-      if (log.isTraceEnabled())
-        log.trace("Ignoring unindexed/unstored field: " + field);
-      return null;
-    }
-
-    int ps = precisionStep;
-
-    byte[] arr=null;
-    TokenStream ts=null;
-
-    long time = (value instanceof Date) 
-      ? ((Date)value).getTime() 
-      : super.parseMath(null, value.toString()).getTime();
-      
-    if (stored) arr = TrieFieldHelper.toArr(time);
-    if (indexed) ts = new NumericTokenStream(ps).setLongValue(time);
-
-    Field f;
-    if (stored) {
-      f = new Field(field.getName(), arr);
-      if (indexed) f.setTokenStream(ts);
-    } else {
-      f = new Field(field.getName(), ts);
-    }
-
-    // term vectors aren't supported
-
-    f.setOmitNorms(field.omitNorms());
-    f.setOmitTermFreqAndPositions(field.omitTf());
-    f.setBoost(boost);
-    return f;
+    return wrappedField.createField(field, value, boost);
   }
 
   @Override
   public Query getRangeQuery(QParser parser, SchemaField field, String min, String max, boolean minInclusive, boolean maxInclusive) {
-    return getRangeQuery(parser, field,
-            min==null ? null : super.parseMath(null,min),
-            max==null ? null : super.parseMath(null,max),
-            minInclusive, maxInclusive);
+    return wrappedField.getRangeQuery(parser, field, min, max, minInclusive, maxInclusive);
   }
   
   @Override
   public Query getRangeQuery(QParser parser, SchemaField sf, Date min, Date max, boolean minInclusive, boolean maxInclusive) {
-    int ps = precisionStep;
-    Query query = NumericRangeQuery.newLongRange(sf.getName(), ps,
+    return NumericRangeQuery.newLongRange(sf.getName(), wrappedField.precisionStep,
               min == null ? null : min.getTime(),
               max == null ? null : max.getTime(),
               minInclusive, maxInclusive);
-
-    return query;
   }
 }
diff --git a/solr/src/java/org/apache/solr/schema/TrieField.java b/solr/src/java/org/apache/solr/schema/TrieField.java
index e670ba0..eb78e1b 100644
--- a/solr/src/java/org/apache/solr/schema/TrieField.java
+++ b/solr/src/java/org/apache/solr/schema/TrieField.java
@@ -17,6 +17,8 @@
 package org.apache.solr.schema;
 
 import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.NumericField;
 import org.apache.lucene.search.*;
 import org.apache.lucene.search.cache.CachedArrayCreator;
 import org.apache.lucene.search.cache.DoubleValuesCreator;
@@ -40,17 +42,17 @@ import java.util.Map;
 import java.util.Date;
 
 /**
- * Provides field types to support for Lucene's Trie Range Queries.
+ * Provides field types to support for Lucene's {@link NumericField}.
  * See {@link org.apache.lucene.search.NumericRangeQuery} for more details.
  * It supports integer, float, long, double and date types.
  * <p/>
  * For each number being added to this field, multiple terms are generated as per the algorithm described in the above
- * link. The possible number of terms increases dramatically with higher precision steps (factor 2^precisionStep). For
+ * link. The possible number of terms increases dramatically with lower precision steps. For
  * the fast range search to work, trie fields must be indexed.
  * <p/>
  * Trie fields are sortable in numerical order and can be used in function queries.
  * <p/>
- * Note that if you use a precisionStep of 32 for int/float and 64 for long/double, then multiple terms will not be
+ * Note that if you use a precisionStep of 32 for int/float and 64 for long/double/date, then multiple terms will not be
  * generated, range search will be no faster than any other number field, but sorting will still be possible.
  *
  * @version $Id$
@@ -101,21 +103,28 @@ public class TrieField extends FieldType {
 
   @Override
   public Object toObject(Fieldable f) {
-    byte[] arr = f.getBinaryValue();
-    if (arr==null) return badFieldString(f);
-    switch (type) {
-      case INTEGER:
-        return TrieFieldHelper.toInt(arr);
-      case FLOAT:
-        return TrieFieldHelper.toFloat(arr);
-      case LONG:
-        return TrieFieldHelper.toLong(arr);
-      case DOUBLE:
-        return TrieFieldHelper.toDouble(arr);
-      case DATE:
-        return new Date(TrieFieldHelper.toLong(arr));
-      default:
-        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unknown type for trie field: " + f.name());
+    if (f instanceof NumericField) {
+      final Number val = ((NumericField) f).getNumericValue();
+      if (val==null) return badFieldString(f);
+      return (type == TrieTypes.DATE) ? new Date(val.longValue()) : val;
+    } else {
+      // the following code is "deprecated" and only to support pre-3.2 indexes using the old BinaryField encoding:
+      final byte[] arr = f.getBinaryValue();
+      if (arr==null) return badFieldString(f);
+      switch (type) {
+        case INTEGER:
+          return toInt(arr);
+        case FLOAT:
+          return Float.intBitsToFloat(toInt(arr));
+        case LONG:
+          return toLong(arr);
+        case DOUBLE:
+          return Double.longBitsToDouble(toLong(arr));
+        case DATE:
+          return new Date(toLong(arr));
+        default:
+          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unknown type for trie field: " + f.name());
+      }
     }
   }
 
@@ -198,30 +207,7 @@ public class TrieField extends FieldType {
 
   @Override
   public void write(TextResponseWriter writer, String name, Fieldable f) throws IOException {
-    byte[] arr = f.getBinaryValue();
-    if (arr==null) {
-      writer.writeStr(name, badFieldString(f),true);
-      return;
-    }
-    switch (type) {
-      case INTEGER:
-        writer.writeInt(name,TrieFieldHelper.toInt(arr));
-        break;
-      case FLOAT:
-        writer.writeFloat(name,TrieFieldHelper.toFloat(arr));
-        break;
-      case LONG:
-        writer.writeLong(name,TrieFieldHelper.toLong(arr));
-        break;
-      case DOUBLE:
-        writer.writeDouble(name,TrieFieldHelper.toDouble(arr));
-        break;
-      case DATE:
-        writer.writeDate(name,new Date(TrieFieldHelper.toLong(arr)));
-        break;
-      default:
-        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unknown type for trie field: " + f.name());
-    }
+    writer.writeVal(name, toObject(f));
   }
 
   @Override
@@ -290,6 +276,17 @@ public class TrieField extends FieldType {
     return query;
   }
 
+  @Deprecated
+  static int toInt(byte[] arr) {
+    return (arr[0]<<24) | ((arr[1]&0xff)<<16) | ((arr[2]&0xff)<<8) | (arr[3]&0xff);
+  }
+  
+  @Deprecated
+  static long toLong(byte[] arr) {
+    int high = (arr[0]<<24) | ((arr[1]&0xff)<<16) | ((arr[2]&0xff)<<8) | (arr[3]&0xff);
+    int low = (arr[4]<<24) | ((arr[5]&0xff)<<16) | ((arr[6]&0xff)<<8) | (arr[7]&0xff);
+    return (((long)high)<<32) | (low&0x0ffffffffL);
+  }
 
   @Override
   public String storedToReadable(Fieldable f) {
@@ -341,22 +338,9 @@ public class TrieField extends FieldType {
 
   @Override
   public String toExternal(Fieldable f) {
-    byte[] arr = f.getBinaryValue();
-    if (arr==null) return badFieldString(f);
-    switch (type) {
-      case INTEGER:
-        return Integer.toString(TrieFieldHelper.toInt(arr));
-      case FLOAT:
-        return Float.toString(TrieFieldHelper.toFloat(arr));
-      case LONG:
-        return Long.toString(TrieFieldHelper.toLong(arr));
-      case DOUBLE:
-        return Double.toString(TrieFieldHelper.toDouble(arr));
-      case DATE:
-        return dateField.formatDate(new Date(TrieFieldHelper.toLong(arr)));
-      default:
-        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unknown type for trie field: " + f.name());
-    }
+    return (type == TrieTypes.DATE)
+      ? dateField.toExternal((Date) toObject(f)) 
+      : toObject(f).toString();
   }
 
   @Override
@@ -372,7 +356,7 @@ public class TrieField extends FieldType {
       case DOUBLE:
         return Double.toString( NumericUtils.sortableLongToDouble(NumericUtils.prefixCodedToLong(indexedForm)) );
       case DATE:
-        return dateField.formatDate( new Date(NumericUtils.prefixCodedToLong(indexedForm)) );
+        return dateField.toExternal( new Date(NumericUtils.prefixCodedToLong(indexedForm)) );
       default:
         throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unknown type for trie field: " + type);
     }
@@ -397,7 +381,7 @@ public class TrieField extends FieldType {
         s = Double.toString( NumericUtils.sortableLongToDouble(NumericUtils.prefixCodedToLong(indexedForm)) );
         break;
       case DATE:
-        s = dateField.formatDate( new Date(NumericUtils.prefixCodedToLong(indexedForm)) );
+        s = dateField.toExternal( new Date(NumericUtils.prefixCodedToLong(indexedForm)) );
         break;
       default:
         throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unknown type for trie field: " + type);
@@ -426,59 +410,117 @@ public class TrieField extends FieldType {
 
   @Override
   public String storedToIndexed(Fieldable f) {
-    // TODO: optimize to remove redundant string conversion
-    return readableToIndexed(storedToReadable(f));
+    final BytesRef bytes = new BytesRef(NumericUtils.BUF_SIZE_LONG);
+    if (f instanceof NumericField) {
+      final Number val = ((NumericField) f).getNumericValue();
+      if (val==null)
+        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Invalid field contents: "+f.name());
+      switch (type) {
+        case INTEGER:
+          NumericUtils.intToPrefixCoded(val.intValue(), 0, bytes);
+          break;
+        case FLOAT:
+          NumericUtils.intToPrefixCoded(NumericUtils.floatToSortableInt(val.floatValue()), 0, bytes);
+          break;
+        case LONG: //fallthrough!
+        case DATE:
+          NumericUtils.longToPrefixCoded(val.longValue(), 0, bytes);
+          break;
+        case DOUBLE:
+          NumericUtils.longToPrefixCoded(NumericUtils.doubleToSortableLong(val.doubleValue()), 0, bytes);
+          break;
+        default:
+          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unknown type for trie field: " + f.name());
+      }
+    } else {
+      // the following code is "deprecated" and only to support pre-3.2 indexes using the old BinaryField encoding:
+      final byte[] arr = f.getBinaryValue();
+      if (arr==null)
+        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Invalid field contents: "+f.name());
+      switch (type) {
+        case INTEGER:
+          NumericUtils.intToPrefixCoded(toInt(arr), 0, bytes);
+          break;
+        case FLOAT: {
+          // WARNING: Code Duplication! Keep in sync with o.a.l.util.NumericUtils!
+          // copied from NumericUtils to not convert to/from float two times
+          // code in next 2 lines is identical to: int v = NumericUtils.floatToSortableInt(Float.intBitsToFloat(toInt(arr)));
+          int v = toInt(arr);
+          if (v<0) v ^= 0x7fffffff;
+          NumericUtils.intToPrefixCoded(v, 0, bytes);
+          break;
+        }
+        case LONG: //fallthrough!
+        case DATE:
+          NumericUtils.longToPrefixCoded(toLong(arr), 0, bytes);
+          break;
+        case DOUBLE: {
+          // WARNING: Code Duplication! Keep in sync with o.a.l.util.NumericUtils!
+          // copied from NumericUtils to not convert to/from double two times
+          // code in next 2 lines is identical to: long v = NumericUtils.doubleToSortableLong(Double.longBitsToDouble(toLong(arr)));
+          long v = toLong(arr);
+          if (v<0) v ^= 0x7fffffffffffffffL;
+          NumericUtils.longToPrefixCoded(v, 0, bytes);
+          break;
+        }
+        default:
+          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unknown type for trie field: " + f.name());
+      }
+    }
+    return bytes.utf8ToString();
   }
 
   @Override
   public Fieldable createField(SchemaField field, Object value, float boost) {
-    TrieFieldHelper.FieldInfo info = new TrieFieldHelper.FieldInfo();
-    info.index = field.indexed();
-    info.store = field.stored();
-    info.precisionStep = precisionStep;
-    info.omitNorms = field.omitNorms();
-    info.omitTF = field.omitTf();
-    
-    if (!info.index && !info.store) {
+    boolean indexed = field.indexed();
+    boolean stored = field.stored();
+
+    if (!indexed && !stored) {
       if (log.isTraceEnabled())
         log.trace("Ignoring unindexed/unstored field: " + field);
       return null;
     }
 
+    final NumericField f = new NumericField(field.getName(), precisionStep, stored ? Field.Store.YES : Field.Store.NO, indexed);
     switch (type) {
       case INTEGER:
         int i = (value instanceof Number)
           ? ((Number)value).intValue()
           : Integer.parseInt(value.toString());
-        return TrieFieldHelper.createIntField(field.getName(), i, info, boost);
-
+        f.setIntValue(i);
+        break;
       case FLOAT:
-        float f = (value instanceof Number)
+        float fl = (value instanceof Number)
           ? ((Number)value).floatValue()
           : Float.parseFloat(value.toString());
-        return TrieFieldHelper.createFloatField(field.getName(), f, info, boost);
-        
+        f.setFloatValue(fl);
+        break;
       case LONG:
         long l = (value instanceof Number)
           ? ((Number)value).longValue()
           : Long.parseLong(value.toString());
-        return TrieFieldHelper.createLongField(field.getName(), l, info, boost);
-          
+        f.setLongValue(l);
+        break;
       case DOUBLE:
         double d = (value instanceof Number)
           ? ((Number)value).doubleValue()
           : Double.parseDouble(value.toString());
-        return TrieFieldHelper.createDoubleField(field.getName(), d, info, boost);
-        
+        f.setDoubleValue(d);
+        break;
       case DATE:
         Date date = (value instanceof Date)
           ? ((Date)value)
           : dateField.parseMath(null, value.toString());
-        return TrieFieldHelper.createDateField(field.getName(), date, info, boost);
-        
+        f.setLongValue(date.getTime());
+        break;
       default:
         throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Unknown type for trie field: " + type);
     }
+
+    f.setOmitNorms(field.omitNorms());
+    f.setOmitTermFreqAndPositions(field.omitTf());
+    f.setBoost(boost);
+    return f;
   }
 
   public enum TrieTypes {
@@ -498,14 +540,12 @@ public class TrieField extends FieldType {
    * that indexes multiple precisions per value.
    */
   public static String getMainValuePrefix(FieldType ft) {
-    if (ft instanceof TrieDateField) {
-      int step = ((TrieDateField)ft).getPrecisionStep();
-      if (step <= 0 || step >=64) return null;
-      return LONG_PREFIX;
-    } else if (ft instanceof TrieField) {
-      TrieField trie = (TrieField)ft;
-      if (trie.precisionStep  == Integer.MAX_VALUE) return null;
-
+    if (ft instanceof TrieDateField)
+      ft = ((TrieDateField) ft).wrappedField;
+    if (ft instanceof TrieField) {
+      final TrieField trie = (TrieField)ft;
+      if (trie.precisionStep  == Integer.MAX_VALUE)
+        return null;
       switch (trie.type) {
         case INTEGER:
         case FLOAT:
diff --git a/solr/src/java/org/apache/solr/schema/TrieFieldHelper.java b/solr/src/java/org/apache/solr/schema/TrieFieldHelper.java
deleted file mode 100644
index c40ecd8..0000000
--- a/solr/src/java/org/apache/solr/schema/TrieFieldHelper.java
+++ /dev/null
@@ -1,166 +0,0 @@
-/**
- * Copyright 2005 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.schema;
-
-import java.util.Date;
-
-import org.apache.lucene.analysis.NumericTokenStream;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Fieldable;
-
-/**
- * Helper class to make TrieFields compatible with ones written in solr
- * 
- * TODO -- Something like this should be in in lucene
- *  see: LUCENE-3001
- */
-public class TrieFieldHelper {
-  
-  private TrieFieldHelper() {}
-  
-  public static class FieldInfo {
-    public int precisionStep = 8; // same as solr default
-    public boolean store = true;
-    public boolean index = true;
-    public boolean omitNorms = true;
-    public boolean omitTF = true;
-  }
-
-  //----------------------------------------------
-  // Create Field
-  //----------------------------------------------
-
-  private static Fieldable createField(String name, byte[] arr, TokenStream ts, FieldInfo info, float boost) {
-
-    Field f;
-    if (info.store) {
-      f = new Field(name, arr);
-      if (info.index) f.setTokenStream(ts);
-    } else {
-      f = new Field(name, ts);
-    }
-
-    // term vectors aren't supported
-    f.setOmitNorms(info.omitNorms);
-    f.setOmitTermFreqAndPositions(info.omitTF);
-    f.setBoost(boost);
-    return f;
-  }
-
-  public static Fieldable createIntField(String name, int value, FieldInfo info, float boost) {
-
-    byte[] arr=null;
-    TokenStream ts=null;
-
-    if (info.store) arr = TrieFieldHelper.toArr(value);
-    if (info.index) ts = new NumericTokenStream(info.precisionStep).setIntValue(value);
-    
-    return createField(name, arr, ts, info, boost);
-  }
-
-  public static Fieldable createFloatField(String name, float value, FieldInfo info, float boost) {
-
-    byte[] arr=null;
-    TokenStream ts=null;
-
-    if (info.store) arr = TrieFieldHelper.toArr(value);
-    if (info.index) ts = new NumericTokenStream(info.precisionStep).setFloatValue(value);
-    
-    return createField(name, arr, ts, info, boost);
-  }
-
-  public static Fieldable createLongField(String name, long value, FieldInfo info, float boost) {
-
-    byte[] arr=null;
-    TokenStream ts=null;
-
-    if (info.store) arr = TrieFieldHelper.toArr(value);
-    if (info.index) ts = new NumericTokenStream(info.precisionStep).setLongValue(value);
-    
-    return createField(name, arr, ts, info, boost);
-  }
-
-  public static Fieldable createDoubleField(String name, double value, FieldInfo info, float boost) {
-
-    byte[] arr=null;
-    TokenStream ts=null;
-
-    if (info.store) arr = TrieFieldHelper.toArr(value);
-    if (info.index) ts = new NumericTokenStream(info.precisionStep).setDoubleValue(value);
-    
-    return createField(name, arr, ts, info, boost);
-  }
-
-  public static Fieldable createDateField(String name, Date value, FieldInfo info, float boost) {
-    // TODO, make sure the date is within long range!
-    return createLongField(name, value.getTime(), info, boost);
-  }
-  
-  
-  //----------------------------------------------
-  // number <=> byte[]
-  //----------------------------------------------
-
-  public static int toInt(byte[] arr) {
-    return (arr[0]<<24) | ((arr[1]&0xff)<<16) | ((arr[2]&0xff)<<8) | (arr[3]&0xff);
-  }
-  
-  public static long toLong(byte[] arr) {
-    int high = (arr[0]<<24) | ((arr[1]&0xff)<<16) | ((arr[2]&0xff)<<8) | (arr[3]&0xff);
-    int low = (arr[4]<<24) | ((arr[5]&0xff)<<16) | ((arr[6]&0xff)<<8) | (arr[7]&0xff);
-    return (((long)high)<<32) | (low&0x0ffffffffL);
-  }
-
-  public static float toFloat(byte[] arr) {
-    return Float.intBitsToFloat(toInt(arr));
-  }
-
-  public static double toDouble(byte[] arr) {
-    return Double.longBitsToDouble(toLong(arr));
-  }
-
-  public static byte[] toArr(int val) {
-    byte[] arr = new byte[4];
-    arr[0] = (byte)(val>>>24);
-    arr[1] = (byte)(val>>>16);
-    arr[2] = (byte)(val>>>8);
-    arr[3] = (byte)(val);
-    return arr;
-  }
-
-  public static byte[] toArr(long val) {
-    byte[] arr = new byte[8];
-    arr[0] = (byte)(val>>>56);
-    arr[1] = (byte)(val>>>48);
-    arr[2] = (byte)(val>>>40);
-    arr[3] = (byte)(val>>>32);
-    arr[4] = (byte)(val>>>24);
-    arr[5] = (byte)(val>>>16);
-    arr[6] = (byte)(val>>>8);
-    arr[7] = (byte)(val);
-    return arr;
-  }
-
-  public static byte[] toArr(float val) {
-    return toArr(Float.floatToRawIntBits(val));
-  }
-
-  public static byte[] toArr(double val) {
-    return toArr(Double.doubleToRawLongBits(val));
-  }
-}
diff --git a/solr/src/java/org/apache/solr/update/AddUpdateCommand.java b/solr/src/java/org/apache/solr/update/AddUpdateCommand.java
index 84632ee..6a02010 100644
--- a/solr/src/java/org/apache/solr/update/AddUpdateCommand.java
+++ b/solr/src/java/org/apache/solr/update/AddUpdateCommand.java
@@ -18,7 +18,7 @@
 package org.apache.solr.update;
 
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.index.Term;
 import org.apache.solr.common.SolrInputDocument;
 import org.apache.solr.common.SolrInputField;
@@ -74,7 +74,7 @@ public class AddUpdateCommand extends UpdateCommand {
        if (sf != null) {
          if (doc != null) {
            schema.getUniqueKeyField();
-           Field storedId = doc.getField(sf.getName());
+           Fieldable storedId = doc.getFieldable(sf.getName());
            indexedId = sf.getType().storedToIndexed(storedId);
          }
          if (solrDoc != null) {
diff --git a/solr/src/java/org/apache/solr/update/DocumentBuilder.java b/solr/src/java/org/apache/solr/update/DocumentBuilder.java
index e78e18d..a7b80e0 100644
--- a/solr/src/java/org/apache/solr/update/DocumentBuilder.java
+++ b/solr/src/java/org/apache/solr/update/DocumentBuilder.java
@@ -159,7 +159,7 @@ public class DocumentBuilder {
     // default value are defacto 'required' fields.  
     List<String> missingFields = null;
     for (SchemaField field : schema.getRequiredFields()) {
-      if (doc.getField(field.getName() ) == null) {
+      if (doc.getFieldable(field.getName() ) == null) {
         if (field.getDefaultValue() != null) {
           addField(doc, field, field.getDefaultValue(), 1.0f);
         } else {
@@ -313,7 +313,7 @@ public class DocumentBuilder {
     // Now validate required fields or add default values
     // fields with default values are defacto 'required'
     for (SchemaField field : schema.getRequiredFields()) {
-      if (out.getField(field.getName() ) == null) {
+      if (out.getFieldable(field.getName() ) == null) {
         if (field.getDefaultValue() != null) {
           addField(out, field, field.getDefaultValue(), 1.0f);
         } 
@@ -339,8 +339,7 @@ public class DocumentBuilder {
    */
   public SolrDocument loadStoredFields( SolrDocument doc, Document luceneDoc  )
   {
-    for( Object f : luceneDoc.getFields() ) {
-      Fieldable field = (Fieldable)f;
+    for( Fieldable field : luceneDoc.getFields() ) {
       if( field.isStored() ) {
         SchemaField sf = schema.getField( field.name() );
         if( !schema.isCopyFieldTarget( sf ) ) {
diff --git a/solr/src/java/org/apache/solr/update/UpdateHandler.java b/solr/src/java/org/apache/solr/update/UpdateHandler.java
index e733234..cd13a49 100644
--- a/solr/src/java/org/apache/solr/update/UpdateHandler.java
+++ b/solr/src/java/org/apache/solr/update/UpdateHandler.java
@@ -21,7 +21,6 @@ package org.apache.solr.update;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
 import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.Scorer;
@@ -125,7 +124,7 @@ public abstract class UpdateHandler implements SolrInfoMBean {
 
   protected final String getIndexedIdOptional(Document doc) {
     if (idField == null) return null;
-    Field f = doc.getField(idField.getName());
+    Fieldable f = doc.getFieldable(idField.getName());
     if (f == null) return null;
     return idFieldType.storedToIndexed(f);
   }
diff --git a/solr/src/test/org/apache/solr/BasicFunctionalityTest.java b/solr/src/test/org/apache/solr/BasicFunctionalityTest.java
index f19d9b2..3b12f79 100644
--- a/solr/src/test/org/apache/solr/BasicFunctionalityTest.java
+++ b/solr/src/test/org/apache/solr/BasicFunctionalityTest.java
@@ -561,7 +561,7 @@ public class BasicFunctionalityTest extends SolrTestCaseJ4 {
 
     DocList dl = ((ResultContext) rsp.getValues().get("response")).docs;
     org.apache.lucene.document.Document d = req.getSearcher().doc(dl.iterator().nextDoc());
-    // ensure field is not lazy
+    // ensure field is not lazy, only works for Non-Numeric fields currently (if you change schema behind test, this may fail)
     assertTrue( d.getFieldable("test_hlt") instanceof Field );
     assertTrue( d.getFieldable("title") instanceof Field );
     req.close();
diff --git a/solr/src/test/org/apache/solr/handler/MoreLikeThisHandlerTest.java b/solr/src/test/org/apache/solr/handler/MoreLikeThisHandlerTest.java
index 6dbae21..c7d8a39 100644
--- a/solr/src/test/org/apache/solr/handler/MoreLikeThisHandlerTest.java
+++ b/solr/src/test/org/apache/solr/handler/MoreLikeThisHandlerTest.java
@@ -79,7 +79,7 @@ public class MoreLikeThisHandlerTest extends SolrTestCaseJ4 {
 
     params.set(CommonParams.Q, "id:42");
     params.set(MoreLikeThisParams.MLT, "true");
-    params.set(MoreLikeThisParams.SIMILARITY_FIELDS, "name,subword,foo_ti");
+    params.set(MoreLikeThisParams.SIMILARITY_FIELDS, "name,subword");
     params.set(MoreLikeThisParams.INTERESTING_TERMS, "details");
     params.set(MoreLikeThisParams.MIN_TERM_FREQ,"1");
     params.set(MoreLikeThisParams.MIN_DOC_FREQ,"1");
diff --git a/solr/src/test/org/apache/solr/update/DocumentBuilderTest.java b/solr/src/test/org/apache/solr/update/DocumentBuilderTest.java
index 4a4df13..991295d 100644
--- a/solr/src/test/org/apache/solr/update/DocumentBuilderTest.java
+++ b/solr/src/test/org/apache/solr/update/DocumentBuilderTest.java
@@ -109,8 +109,8 @@ public class DocumentBuilderTest extends SolrTestCaseJ4 {
     doc.addField( "home", "2.2,3.3", 1.0f );
     Document out = DocumentBuilder.toDocument( doc, core.getSchema() );
     assertNotNull( out.get( "home" ) );//contains the stored value and term vector, if there is one
-    assertNotNull( out.getField( "home_0" + FieldType.POLY_FIELD_SEPARATOR + "double" ) );
-    assertNotNull( out.getField( "home_1" + FieldType.POLY_FIELD_SEPARATOR + "double" ) );
+    assertNotNull( out.getFieldable( "home_0" + FieldType.POLY_FIELD_SEPARATOR + "double" ) );
+    assertNotNull( out.getFieldable( "home_1" + FieldType.POLY_FIELD_SEPARATOR + "double" ) );
   }
 
 }

