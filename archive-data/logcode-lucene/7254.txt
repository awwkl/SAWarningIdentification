GitDiffStart: cfe57ec320b52a83f4c79ef4c87e763103450ad6 | Tue Dec 31 07:33:38 2013 +0000
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
index 08aa78f..8b17e3c 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
@@ -23,17 +23,15 @@ import java.util.List;
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.Facets;
-import org.apache.lucene.facet.FacetsConfig;
-import org.apache.lucene.facet.FloatAssociationFacetField;
-import org.apache.lucene.facet.IntAssociationFacetField;
 import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.Facets;
 import org.apache.lucene.facet.FacetsCollector;
-import org.apache.lucene.facet.TaxonomyFacetSumFloatAssociations;
-import org.apache.lucene.facet.TaxonomyFacetSumIntAssociations;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.FloatAssociationFacetField;
+import org.apache.lucene.facet.taxonomy.IntAssociationFacetField;
+import org.apache.lucene.facet.taxonomy.TaxonomyFacetSumFloatAssociations;
+import org.apache.lucene.facet.taxonomy.TaxonomyFacetSumIntAssociations;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.index.DirectoryReader;
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java
index cf13c0f..061fe82 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/DistanceFacetsExample.java
@@ -28,12 +28,12 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.expressions.Expression;
 import org.apache.lucene.expressions.SimpleBindings;
 import org.apache.lucene.expressions.js.JavascriptCompiler;
-import org.apache.lucene.facet.DoubleRange;
-import org.apache.lucene.facet.DoubleRangeFacetCounts;
 import org.apache.lucene.facet.DrillDownQuery;
 import org.apache.lucene.facet.FacetResult;
 import org.apache.lucene.facet.Facets;
 import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.range.DoubleRange;
+import org.apache.lucene.facet.range.DoubleRangeFacetCounts;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java
index e6088c9..45d71e4 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java
@@ -2,8 +2,6 @@ package org.apache.lucene.demo.facet;
 
 import java.io.IOException;
 import java.text.ParseException;
-import java.util.Collections;
-import java.util.List;
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
@@ -14,12 +12,11 @@ import org.apache.lucene.expressions.Expression;
 import org.apache.lucene.expressions.SimpleBindings;
 import org.apache.lucene.expressions.js.JavascriptCompiler;
 import org.apache.lucene.facet.FacetField;
-import org.apache.lucene.facet.Facets;
-import org.apache.lucene.facet.FacetsConfig;
 import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.Facets;
 import org.apache.lucene.facet.FacetsCollector;
-import org.apache.lucene.facet.TaxonomyFacetSumValueSource;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.TaxonomyFacetSumValueSource;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java
index 11d8eb7..a638312 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java
@@ -19,21 +19,17 @@ package org.apache.lucene.demo.facet;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.facet.FacetField;
-import org.apache.lucene.facet.Facets;
-import org.apache.lucene.facet.FacetsConfig;
-import org.apache.lucene.facet.FastTaxonomyFacetCounts;
 import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.Facets;
 import org.apache.lucene.facet.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.FastTaxonomyFacetCounts;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.index.DirectoryReader;
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
index d84d0f6..3fce57e 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
@@ -30,8 +30,8 @@ import org.apache.lucene.facet.FacetResult;
 import org.apache.lucene.facet.Facets;
 import org.apache.lucene.facet.FacetsCollector;
 import org.apache.lucene.facet.FacetsConfig;
-import org.apache.lucene.facet.LongRange;
-import org.apache.lucene.facet.LongRangeFacetCounts;
+import org.apache.lucene.facet.range.LongRange;
+import org.apache.lucene.facet.range.LongRangeFacetCounts;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java
index 8f9de58..7357545 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java
@@ -23,16 +23,14 @@ import java.util.List;
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetField;
-import org.apache.lucene.facet.Facets;
-import org.apache.lucene.facet.FacetsConfig;
-import org.apache.lucene.facet.FastTaxonomyFacetCounts;
 import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetField;
 import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.Facets;
 import org.apache.lucene.facet.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.FastTaxonomyFacetCounts;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.index.DirectoryReader;
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
index a89ade0..021bd4b 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
@@ -28,9 +28,9 @@ import org.apache.lucene.facet.FacetResult;
 import org.apache.lucene.facet.Facets;
 import org.apache.lucene.facet.FacetsCollector;
 import org.apache.lucene.facet.FacetsConfig;
-import org.apache.lucene.facet.SortedSetDocValuesFacetCounts;
-import org.apache.lucene.facet.SortedSetDocValuesFacetField;
-import org.apache.lucene.facet.SortedSetDocValuesReaderState;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetCounts;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetField;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/AssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/AssociationFacetField.java
deleted file mode 100644
index ff71fdf..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/AssociationFacetField.java
+++ /dev/null
@@ -1,73 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-
-import org.apache.lucene.document.Document; // javadocs
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.util.BytesRef;
-
-/** Add an instance of this to your {@link Document} to add
- *  a facet label associated with an arbitrary byte[].
- *  This will require a custom {@link Facets}
- *  implementation at search time; see {@link
- *  IntAssociationFacetField} and {@link
- *  FloatAssociationFacetField} to use existing {@link
- *  Facets} implementations.
- * 
- *  @lucene.experimental */
-public class AssociationFacetField extends Field {
-  
-  static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setIndexed(true);
-    TYPE.freeze();
-  }
-  protected final String dim;
-  protected final String[] path;
-  protected final BytesRef assoc;
-
-  /** Creates this from {@code dim} and {@code path} and an
-   *  association */
-  public AssociationFacetField(BytesRef assoc, String dim, String... path) {
-    super("dummy", TYPE);
-    this.dim = dim;
-    this.assoc = assoc;
-    if (path.length == 0) {
-      throw new IllegalArgumentException("path must have at least one element");
-    }
-    this.path = path;
-  }
-
-//  private static BytesRef intToBytesRef(int v) {
-//    byte[] bytes = new byte[4];
-//    // big-endian:
-//    bytes[0] = (byte) (v >> 24);
-//    bytes[1] = (byte) (v >> 16);
-//    bytes[2] = (byte) (v >> 8);
-//    bytes[3] = (byte) v;
-//    return new BytesRef(bytes);
-//  }
-
-  @Override
-  public String toString() {
-    return "AssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " bytes=" + assoc + ")";
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/DoubleRange.java b/lucene/facet/src/java/org/apache/lucene/facet/DoubleRange.java
deleted file mode 100644
index 98df59e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/DoubleRange.java
+++ /dev/null
@@ -1,162 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.NumericRangeFilter;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.NumericUtils;
-
-/** Represents a range over double values. */
-public final class DoubleRange extends Range {
-  final double minIncl;
-  final double maxIncl;
-
-  public final double min;
-  public final double max;
-  public final boolean minInclusive;
-  public final boolean maxInclusive;
-
-  /** Create a DoubleRange. */
-  public DoubleRange(String label, double minIn, boolean minInclusive, double maxIn, boolean maxInclusive) {
-    super(label);
-    this.min = minIn;
-    this.max = maxIn;
-    this.minInclusive = minInclusive;
-    this.maxInclusive = maxInclusive;
-
-    // TODO: if DoubleDocValuesField used
-    // NumericUtils.doubleToSortableLong format (instead of
-    // Double.doubleToRawLongBits) we could do comparisons
-    // in long space 
-
-    if (Double.isNaN(min)) {
-      throw new IllegalArgumentException("min cannot be NaN");
-    }
-    if (!minInclusive) {
-      minIn = Math.nextUp(minIn);
-    }
-
-    if (Double.isNaN(max)) {
-      throw new IllegalArgumentException("max cannot be NaN");
-    }
-    if (!maxInclusive) {
-      // Why no Math.nextDown?
-      maxIn = Math.nextAfter(maxIn, Double.NEGATIVE_INFINITY);
-    }
-
-    if (minIn > maxIn) {
-      failNoMatch();
-    }
-
-    this.minIncl = minIn;
-    this.maxIncl = maxIn;
-  }
-
-  public boolean accept(double value) {
-    return value >= minIncl && value <= maxIncl;
-  }
-
-  LongRange toLongRange() {
-    return new LongRange(label,
-                         NumericUtils.doubleToSortableLong(minIncl), true,
-                         NumericUtils.doubleToSortableLong(maxIncl), true);
-  }
-
-  @Override
-  public String toString() {
-    return "DoubleRange(" + minIncl + " to " + maxIncl + ")";
-  }
-
-  /** Returns a new {@link Filter} accepting only documents
-   *  in this range.  Note that this filter is not
-   *  efficient: it's a linear scan of all docs, testing
-   *  each value.  If the {@link ValueSource} is static,
-   *  e.g. an indexed numeric field, then it's more
-   *  efficient to use {@link NumericRangeFilter}. */
-  public Filter getFilter(final ValueSource valueSource) {
-    return new Filter() {
-      @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
-
-        // TODO: this is just like ValueSourceScorer,
-        // ValueSourceFilter (spatial),
-        // ValueSourceRangeFilter (solr); also,
-        // https://issues.apache.org/jira/browse/LUCENE-4251
-
-        final FunctionValues values = valueSource.getValues(Collections.emptyMap(), context);
-
-        final int maxDoc = context.reader().maxDoc();
-
-        return new DocIdSet() {
-
-          @Override
-          public DocIdSetIterator iterator() {
-            return new DocIdSetIterator() {
-              int doc = -1;
-
-              @Override
-              public int nextDoc() throws IOException {
-                while (true) {
-                  doc++;
-                  if (doc == maxDoc) {
-                    return doc = NO_MORE_DOCS;
-                  }
-                  if (acceptDocs != null && acceptDocs.get(doc) == false) {
-                    continue;
-                  }
-                  double v = values.doubleVal(doc);
-                  if (accept(v)) {
-                    return doc;
-                  }
-                }
-              }
-
-              @Override
-              public int advance(int target) throws IOException {
-                doc = target-1;
-                return nextDoc();
-              }
-
-              @Override
-              public int docID() {
-                return doc;
-              }
-
-              @Override
-              public long cost() {
-                // Since we do a linear scan over all
-                // documents, our cost is O(maxDoc):
-                return maxDoc;
-              }
-            };
-          }
-        };
-      }
-    };
-  }
-}
-
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/DoubleRangeFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/DoubleRangeFacetCounts.java
deleted file mode 100644
index 5f9eb69..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/DoubleRangeFacetCounts.java
+++ /dev/null
@@ -1,107 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.document.DoubleDocValuesField; // javadocs
-import org.apache.lucene.document.FloatDocValuesField; // javadocs
-import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.valuesource.DoubleFieldSource;
-import org.apache.lucene.queries.function.valuesource.FloatFieldSource; // javadocs
-import org.apache.lucene.util.NumericUtils;
-
-/** {@link Facets} implementation that computes counts for
- *  dynamic double ranges from a provided {@link
- *  ValueSource}, using {@link FunctionValues#doubleVal}.  Use
- *  this for dimensions that change in real-time (e.g. a
- *  relative time based dimension like "Past day", "Past 2
- *  days", etc.) or that change for each request (e.g.
- *  distance from the user's location, "< 1 km", "< 2 km",
- *  etc.).
- *
- *  <p> If you had indexed your field using {@link
- *  FloatDocValuesField} then pass {@link FloatFieldSource}
- *  as the {@link ValueSource}; if you used {@link
- *  DoubleDocValuesField} then pass {@link
- *  DoubleFieldSource} (this is the default used when you
- *  pass just a the field name).
- *
- *  @lucene.experimental */
-public class DoubleRangeFacetCounts extends RangeFacetCounts {
-
-  /** Create {@code RangeFacetCounts}, using {@link
-   *  DoubleFieldSource} from the specified field. */
-  public DoubleRangeFacetCounts(String field, FacetsCollector hits, DoubleRange... ranges) throws IOException {
-    this(field, new DoubleFieldSource(field), hits, ranges);
-  }
-
-  /** Create {@code RangeFacetCounts}, using the provided
-   *  {@link ValueSource}. */
-  public DoubleRangeFacetCounts(String field, ValueSource valueSource, FacetsCollector hits, DoubleRange... ranges) throws IOException {
-    super(field, ranges);
-    count(valueSource, hits.getMatchingDocs());
-  }
-
-  private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
-
-    DoubleRange[] ranges = (DoubleRange[]) this.ranges;
-
-    LongRange[] longRanges = new LongRange[ranges.length];
-    for(int i=0;i<ranges.length;i++) {
-      DoubleRange range = ranges[i];
-      longRanges[i] =  new LongRange(range.label,
-                                     NumericUtils.doubleToSortableLong(range.minIncl), true,
-                                     NumericUtils.doubleToSortableLong(range.maxIncl), true);
-    }
-
-    LongRangeCounter counter = new LongRangeCounter(longRanges);
-
-    // Compute min & max over all ranges:
-    double minIncl = Double.POSITIVE_INFINITY;
-    double maxIncl = Double.NEGATIVE_INFINITY;
-    for(DoubleRange range : ranges) {
-      minIncl = Math.min(minIncl, range.minIncl);
-      maxIncl = Math.max(maxIncl, range.maxIncl);
-    }
-
-    int missingCount = 0;
-    for (MatchingDocs hits : matchingDocs) {
-      FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
-      final int length = hits.bits.length();
-      int doc = 0;
-      totCount += hits.totalHits;
-      while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
-        // Skip missing docs:
-        if (fv.exists(doc)) {
-          counter.add(NumericUtils.doubleToSortableLong(fv.doubleVal(doc)));
-        } else {
-          missingCount++;
-        }
-        doc++;
-      }
-    }
-
-    missingCount += counter.fillCounts(counts);
-    totCount -= missingCount;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java b/lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java
index 674a8b6..9556f18 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java
@@ -22,6 +22,8 @@ import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.lucene.facet.range.DoubleRangeFacetCounts;
+import org.apache.lucene.facet.range.LongRangeFacetCounts;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.search.BooleanClause;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java b/lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java
index f719470..92c5229 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java
@@ -21,6 +21,10 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetCounts;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetField;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
+import org.apache.lucene.facet.taxonomy.FastTaxonomyFacetCounts;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FacetPackage.java b/lucene/facet/src/java/org/apache/lucene/facet/FacetPackage.java
deleted file mode 100644
index 861b577..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/FacetPackage.java
+++ /dev/null
@@ -1,25 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Required for javadocs generation. */
-public final class FacetPackage {
-  
-  private FacetPackage() {}
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java b/lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java
index 3cc649f..1152a57 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java
@@ -31,7 +31,11 @@ import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetField;
+import org.apache.lucene.facet.taxonomy.AssociationFacetField;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.FloatAssociationFacetField;
+import org.apache.lucene.facet.taxonomy.IntAssociationFacetField;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.index.IndexDocument;
 import org.apache.lucene.index.IndexableField;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FastTaxonomyFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/FastTaxonomyFacetCounts.java
deleted file mode 100644
index 4d151a7..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/FastTaxonomyFacetCounts.java
+++ /dev/null
@@ -1,85 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-
-/** Computes facets counts, assuming the default encoding
- *  into DocValues was used.
- *
- * @lucene.experimental */
-public class FastTaxonomyFacetCounts extends IntTaxonomyFacets {
-
-  /** Create {@code FastTaxonomyFacetCounts}, which also
-   *  counts all facet labels. */
-  public FastTaxonomyFacetCounts(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
-    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
-  }
-
-  /** Create {@code FastTaxonomyFacetCounts}, using the
-   *  specified {@code indexFieldName} for ordinals.  Use
-   *  this if you had set {@link
-   *  FacetsConfig#setIndexFieldName} to change the index
-   *  field name for certain dimensions. */
-  public FastTaxonomyFacetCounts(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
-    super(indexFieldName, taxoReader, config);
-    count(fc.getMatchingDocs());
-  }
-
-  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
-    for(MatchingDocs hits : matchingDocs) {
-      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
-      if (dv == null) { // this reader does not have DocValues for the requested category list
-        continue;
-      }
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      BytesRef scratch = new BytesRef();
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        dv.get(doc, scratch);
-        byte[] bytes = scratch.bytes;
-        int end = scratch.offset + scratch.length;
-        int ord = 0;
-        int offset = scratch.offset;
-        int prev = 0;
-        while (offset < end) {
-          byte b = bytes[offset++];
-          if (b >= 0) {
-            prev = ord = ((ord << 7) | b) + prev;
-            ++values[ord];
-            ord = 0;
-          } else {
-            ord = (ord << 7) | (b & 0x7F);
-          }
-        }
-        ++doc;
-      }
-    }
-
-    rollup();
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FloatAssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/FloatAssociationFacetField.java
deleted file mode 100644
index d3a05a3..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/FloatAssociationFacetField.java
+++ /dev/null
@@ -1,53 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.util.BytesRef;
-
-/** Add an instance of this to your {@link Document} to add
- *  a facet label associated with a float.  Use {@link
- *  TaxonomyFacetSumFloatAssociations} to aggregate float values
- *  per facet label at search time.
- * 
- *  @lucene.experimental */
-public class FloatAssociationFacetField extends AssociationFacetField {
-
-  /** Creates this from {@code dim} and {@code path} and a
-   *  float association */
-  public FloatAssociationFacetField(float assoc, String dim, String... path) {
-    super(floatToBytesRef(assoc), dim, path);
-  }
-
-  /** Encodes a {@code float} as a 4-byte {@link BytesRef}. */
-  public static BytesRef floatToBytesRef(float v) {
-    return IntAssociationFacetField.intToBytesRef(Float.floatToIntBits(v));
-  }
-
-  /** Decodes a previously encoded {@code float}. */
-  public static float bytesRefToFloat(BytesRef b) {
-    return Float.intBitsToFloat(IntAssociationFacetField.bytesRefToInt(b));
-  }
-
-  @Override
-  public String toString() {
-    return "FloatAssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " value=" + bytesRefToFloat(assoc) + ")";
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FloatTaxonomyFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/FloatTaxonomyFacets.java
deleted file mode 100644
index 2b97b54..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/FloatTaxonomyFacets.java
+++ /dev/null
@@ -1,146 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.facet.FacetsConfig.DimConfig;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/** Base class for all taxonomy-based facets that aggregate
- *  to a per-ords float[]. */
-
-public abstract class FloatTaxonomyFacets extends TaxonomyFacets {
-
-  protected final float[] values;
-
-  protected FloatTaxonomyFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config) throws IOException {
-    super(indexFieldName, taxoReader, config);
-    values = new float[taxoReader.getSize()];
-  }
-  
-  protected void rollup() throws IOException {
-    // Rollup any necessary dims:
-    for(Map.Entry<String,DimConfig> ent : config.getDimConfigs().entrySet()) {
-      String dim = ent.getKey();
-      DimConfig ft = ent.getValue();
-      if (ft.hierarchical && ft.multiValued == false) {
-        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
-        assert dimRootOrd > 0;
-        values[dimRootOrd] += rollup(children[dimRootOrd]);
-      }
-    }
-  }
-
-  private float rollup(int ord) {
-    float sum = 0;
-    while (ord != TaxonomyReader.INVALID_ORDINAL) {
-      float childValue = values[ord] + rollup(children[ord]);
-      values[ord] = childValue;
-      sum += childValue;
-      ord = siblings[ord];
-    }
-    return sum;
-  }
-
-  @Override
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    DimConfig dimConfig = verifyDim(dim);
-    if (path.length == 0) {
-      if (dimConfig.hierarchical && dimConfig.multiValued == false) {
-        // ok: rolled up at search time
-      } else if (dimConfig.requireDimCount && dimConfig.multiValued) {
-        // ok: we indexed all ords at index time
-      } else {
-        throw new IllegalArgumentException("cannot return dimension-level value alone; use getTopChildren instead");
-      }
-    }
-    int ord = taxoReader.getOrdinal(new FacetLabel(dim, path));
-    if (ord < 0) {
-      return -1;
-    }
-    return values[ord];
-  }
-
-  @Override
-  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    if (topN <= 0) {
-      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
-    }
-    DimConfig dimConfig = verifyDim(dim);
-    FacetLabel cp = new FacetLabel(dim, path);
-    int dimOrd = taxoReader.getOrdinal(cp);
-    if (dimOrd == -1) {
-      return null;
-    }
-
-    TopOrdAndFloatQueue q = new TopOrdAndFloatQueue(Math.min(taxoReader.getSize(), topN));
-    float bottomValue = 0;
-
-    int ord = children[dimOrd];
-    float sumValues = 0;
-    int childCount = 0;
-
-    TopOrdAndFloatQueue.OrdAndValue reuse = null;
-    while(ord != TaxonomyReader.INVALID_ORDINAL) {
-      if (values[ord] > 0) {
-        sumValues += values[ord];
-        childCount++;
-        if (values[ord] > bottomValue) {
-          if (reuse == null) {
-            reuse = new TopOrdAndFloatQueue.OrdAndValue();
-          }
-          reuse.ord = ord;
-          reuse.value = values[ord];
-          reuse = q.insertWithOverflow(reuse);
-          if (q.size() == topN) {
-            bottomValue = q.top().value;
-          }
-        }
-      }
-
-      ord = siblings[ord];
-    }
-
-    if (sumValues == 0) {
-      return null;
-    }
-
-    if (dimConfig.multiValued) {
-      if (dimConfig.requireDimCount) {
-        sumValues = values[dimOrd];
-      } else {
-        // Our sum'd count is not correct, in general:
-        sumValues = -1;
-      }
-    } else {
-      // Our sum'd dim count is accurate, so we keep it
-    }
-
-    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
-    for(int i=labelValues.length-1;i>=0;i--) {
-      TopOrdAndFloatQueue.OrdAndValue ordAndValue = q.pop();
-      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
-      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
-    }
-
-    return new FacetResult(dim, path, sumValues, labelValues, childCount);
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/IntAssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/IntAssociationFacetField.java
deleted file mode 100644
index 8508c09..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/IntAssociationFacetField.java
+++ /dev/null
@@ -1,63 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.util.BytesRef;
-
-/** Add an instance of this to your {@link Document} to add
- *  a facet label associated with an int.  Use {@link
- *  TaxonomyFacetSumIntAssociations} to aggregate int values
- *  per facet label at search time.
- * 
- *  @lucene.experimental */
-public class IntAssociationFacetField extends AssociationFacetField {
-
-  /** Creates this from {@code dim} and {@code path} and an
-   *  int association */
-  public IntAssociationFacetField(int assoc, String dim, String... path) {
-    super(intToBytesRef(assoc), dim, path);
-  }
-
-  /** Encodes an {@code int} as a 4-byte {@link BytesRef},
-   *  big-endian. */
-  public static BytesRef intToBytesRef(int v) {
-    byte[] bytes = new byte[4];
-    // big-endian:
-    bytes[0] = (byte) (v >> 24);
-    bytes[1] = (byte) (v >> 16);
-    bytes[2] = (byte) (v >> 8);
-    bytes[3] = (byte) v;
-    return new BytesRef(bytes);
-  }
-
-  /** Decodes a previously encoded {@code int}. */
-  public static int bytesRefToInt(BytesRef b) {
-    return ((b.bytes[b.offset]&0xFF) << 24) |
-      ((b.bytes[b.offset+1]&0xFF) << 16) |
-      ((b.bytes[b.offset+2]&0xFF) << 8) |
-      (b.bytes[b.offset+3]&0xFF);
-  }
-
-  @Override
-  public String toString() {
-    return "IntAssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " value=" + bytesRefToInt(assoc) + ")";
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/IntTaxonomyFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/IntTaxonomyFacets.java
deleted file mode 100644
index 50579c8..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/IntTaxonomyFacets.java
+++ /dev/null
@@ -1,150 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.facet.FacetsConfig.DimConfig;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/** Base class for all taxonomy-based facets that aggregate
- *  to a per-ords int[]. */
-
-public abstract class IntTaxonomyFacets extends TaxonomyFacets {
-
-  protected final int[] values;
-
-  protected IntTaxonomyFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config) throws IOException {
-    super(indexFieldName, taxoReader, config);
-    values = new int[taxoReader.getSize()];
-  }
-  
-  protected void rollup() throws IOException {
-    // Rollup any necessary dims:
-    for(Map.Entry<String,DimConfig> ent : config.getDimConfigs().entrySet()) {
-      String dim = ent.getKey();
-      DimConfig ft = ent.getValue();
-      if (ft.hierarchical && ft.multiValued == false) {
-        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
-        // It can be -1 if this field was declared in the
-        // config but never indexed:
-        if (dimRootOrd > 0) {
-          values[dimRootOrd] += rollup(children[dimRootOrd]);
-        }
-      }
-    }
-  }
-
-  private int rollup(int ord) {
-    int sum = 0;
-    while (ord != TaxonomyReader.INVALID_ORDINAL) {
-      int childValue = values[ord] + rollup(children[ord]);
-      values[ord] = childValue;
-      sum += childValue;
-      ord = siblings[ord];
-    }
-    return sum;
-  }
-
-  @Override
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    DimConfig dimConfig = verifyDim(dim);
-    if (path.length == 0) {
-      if (dimConfig.hierarchical && dimConfig.multiValued == false) {
-        // ok: rolled up at search time
-      } else if (dimConfig.requireDimCount && dimConfig.multiValued) {
-        // ok: we indexed all ords at index time
-      } else {
-        throw new IllegalArgumentException("cannot return dimension-level value alone; use getTopChildren instead");
-      }
-    }
-    int ord = taxoReader.getOrdinal(new FacetLabel(dim, path));
-    if (ord < 0) {
-      return -1;
-    }
-    return values[ord];
-  }
-
-  @Override
-  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    if (topN <= 0) {
-      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
-    }
-    DimConfig dimConfig = verifyDim(dim);
-    FacetLabel cp = new FacetLabel(dim, path);
-    int dimOrd = taxoReader.getOrdinal(cp);
-    if (dimOrd == -1) {
-      return null;
-    }
-
-    TopOrdAndIntQueue q = new TopOrdAndIntQueue(Math.min(taxoReader.getSize(), topN));
-    
-    int bottomValue = 0;
-
-    int ord = children[dimOrd];
-    int totValue = 0;
-    int childCount = 0;
-
-    TopOrdAndIntQueue.OrdAndValue reuse = null;
-    while(ord != TaxonomyReader.INVALID_ORDINAL) {
-      if (values[ord] > 0) {
-        totValue += values[ord];
-        childCount++;
-        if (values[ord] > bottomValue) {
-          if (reuse == null) {
-            reuse = new TopOrdAndIntQueue.OrdAndValue();
-          }
-          reuse.ord = ord;
-          reuse.value = values[ord];
-          reuse = q.insertWithOverflow(reuse);
-          if (q.size() == topN) {
-            bottomValue = q.top().value;
-          }
-        }
-      }
-
-      ord = siblings[ord];
-    }
-
-    if (totValue == 0) {
-      return null;
-    }
-
-    if (dimConfig.multiValued) {
-      if (dimConfig.requireDimCount) {
-        totValue = values[dimOrd];
-      } else {
-        // Our sum'd value is not correct, in general:
-        totValue = -1;
-      }
-    } else {
-      // Our sum'd dim value is accurate, so we keep it
-    }
-
-    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
-    for(int i=labelValues.length-1;i>=0;i--) {
-      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
-      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
-      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
-    }
-
-    return new FacetResult(dim, path, totValue, labelValues, childCount);
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/LongRange.java b/lucene/facet/src/java/org/apache/lucene/facet/LongRange.java
deleted file mode 100644
index aeeb4b3..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/LongRange.java
+++ /dev/null
@@ -1,153 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.NumericRangeFilter;
-import org.apache.lucene.util.Bits;
-
-/** Represents a range over long values. */
-public final class LongRange extends Range {
-  final long minIncl;
-  final long maxIncl;
-
-  public final long min;
-  public final long max;
-  public final boolean minInclusive;
-  public final boolean maxInclusive;
-
-  // TODO: can we require fewer args? (same for
-  // Double/FloatRange too)
-
-  /** Create a LongRange. */
-  public LongRange(String label, long minIn, boolean minInclusive, long maxIn, boolean maxInclusive) {
-    super(label);
-    this.min = minIn;
-    this.max = maxIn;
-    this.minInclusive = minInclusive;
-    this.maxInclusive = maxInclusive;
-
-    if (!minInclusive) {
-      if (minIn != Long.MAX_VALUE) {
-        minIn++;
-      } else {
-        failNoMatch();
-      }
-    }
-
-    if (!maxInclusive) {
-      if (maxIn != Long.MIN_VALUE) {
-        maxIn--;
-      } else {
-        failNoMatch();
-      }
-    }
-
-    if (minIn > maxIn) {
-      failNoMatch();
-    }
-
-    this.minIncl = minIn;
-    this.maxIncl = maxIn;
-  }
-
-  public boolean accept(long value) {
-    return value >= minIncl && value <= maxIncl;
-  }
-
-  @Override
-  public String toString() {
-    return "LongRange(" + minIncl + " to " + maxIncl + ")";
-  }
-
-  /** Returns a new {@link Filter} accepting only documents
-   *  in this range.  Note that this filter is not
-   *  efficient: it's a linear scan of all docs, testing
-   *  each value.  If the {@link ValueSource} is static,
-   *  e.g. an indexed numeric field, then it's more
-   *  efficient to use {@link NumericRangeFilter}. */
-  public Filter getFilter(final ValueSource valueSource) {
-    return new Filter() {
-      @Override
-      public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
-
-        // TODO: this is just like ValueSourceScorer,
-        // ValueSourceFilter (spatial),
-        // ValueSourceRangeFilter (solr); also,
-        // https://issues.apache.org/jira/browse/LUCENE-4251
-
-        final FunctionValues values = valueSource.getValues(Collections.emptyMap(), context);
-
-        final int maxDoc = context.reader().maxDoc();
-
-        return new DocIdSet() {
-
-          @Override
-          public DocIdSetIterator iterator() {
-            return new DocIdSetIterator() {
-              int doc = -1;
-
-              @Override
-              public int nextDoc() throws IOException {
-                while (true) {
-                  doc++;
-                  if (doc == maxDoc) {
-                    return doc = NO_MORE_DOCS;
-                  }
-                  if (acceptDocs != null && acceptDocs.get(doc) == false) {
-                    continue;
-                  }
-                  long v = values.longVal(doc);
-                  if (accept(v)) {
-                    return doc;
-                  }
-                }
-              }
-
-              @Override
-              public int advance(int target) throws IOException {
-                doc = target-1;
-                return nextDoc();
-              }
-
-              @Override
-              public int docID() {
-                return doc;
-              }
-
-              @Override
-              public long cost() {
-                // Since we do a linear scan over all
-                // documents, our cost is O(maxDoc):
-                return maxDoc;
-              }
-            };
-          }
-        };
-      }
-    };
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/LongRangeCounter.java b/lucene/facet/src/java/org/apache/lucene/facet/LongRangeCounter.java
deleted file mode 100644
index 264926f..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/LongRangeCounter.java
+++ /dev/null
@@ -1,316 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-/** Counts how many times each range was seen;
- *  per-hit it's just a binary search ({@link #add})
- *  against the elementary intervals, and in the end we
- *  rollup back to the original ranges. */
-
-final class LongRangeCounter {
-
-  final LongRangeNode root;
-  final long[] boundaries;
-  final int[] leafCounts;
-
-  // Used during rollup
-  private int leafUpto;
-  private int missingCount;
-
-  public LongRangeCounter(LongRange[] ranges) {
-    // Maps all range inclusive endpoints to int flags; 1
-    // = start of interval, 2 = end of interval.  We need to
-    // track the start vs end case separately because if a
-    // given point is both, then it must be its own
-    // elementary interval:
-    Map<Long,Integer> endsMap = new HashMap<Long,Integer>();
-
-    endsMap.put(Long.MIN_VALUE, 1);
-    endsMap.put(Long.MAX_VALUE, 2);
-
-    for(LongRange range : ranges) {
-      Integer cur = endsMap.get(range.minIncl);
-      if (cur == null) {
-        endsMap.put(range.minIncl, 1);
-      } else {
-        endsMap.put(range.minIncl, cur.intValue() | 1);
-      }
-      cur = endsMap.get(range.maxIncl);
-      if (cur == null) {
-        endsMap.put(range.maxIncl, 2);
-      } else {
-        endsMap.put(range.maxIncl, cur.intValue() | 2);
-      }
-    }
-
-    List<Long> endsList = new ArrayList<Long>(endsMap.keySet());
-    Collections.sort(endsList);
-
-    // Build elementaryIntervals (a 1D Venn diagram):
-    List<InclusiveRange> elementaryIntervals = new ArrayList<InclusiveRange>();
-    int upto0 = 1;
-    long v = endsList.get(0);
-    long prev;
-    if (endsMap.get(v) == 3) {
-      elementaryIntervals.add(new InclusiveRange(v, v));
-      prev = v+1;
-    } else {
-      prev = v;
-    }
-
-    while (upto0 < endsList.size()) {
-      v = endsList.get(upto0);
-      int flags = endsMap.get(v);
-      //System.out.println("  v=" + v + " flags=" + flags);
-      if (flags == 3) {
-        // This point is both an end and a start; we need to
-        // separate it:
-        if (v > prev) {
-          elementaryIntervals.add(new InclusiveRange(prev, v-1));
-        }
-        elementaryIntervals.add(new InclusiveRange(v, v));
-        prev = v+1;
-      } else if (flags == 1) {
-        // This point is only the start of an interval;
-        // attach it to next interval:
-        if (v > prev) {
-          elementaryIntervals.add(new InclusiveRange(prev, v-1));
-        }
-        prev = v;
-      } else {
-        assert flags == 2;
-        // This point is only the end of an interval; attach
-        // it to last interval:
-        elementaryIntervals.add(new InclusiveRange(prev, v));
-        prev = v+1;
-      }
-      //System.out.println("    ints=" + elementaryIntervals);
-      upto0++;
-    }
-
-    // Build binary tree on top of intervals:
-    root = split(0, elementaryIntervals.size(), elementaryIntervals);
-
-    // Set outputs, so we know which range to output for
-    // each node in the tree:
-    for(int i=0;i<ranges.length;i++) {
-      root.addOutputs(i, ranges[i]);
-    }
-
-    // Set boundaries (ends of each elementary interval):
-    boundaries = new long[elementaryIntervals.size()];
-    for(int i=0;i<boundaries.length;i++) {
-      boundaries[i] = elementaryIntervals.get(i).end;
-    }
-
-    leafCounts = new int[boundaries.length];
-
-    //System.out.println("ranges: " + Arrays.toString(ranges));
-    //System.out.println("intervals: " + elementaryIntervals);
-    //System.out.println("boundaries: " + Arrays.toString(boundaries));
-    //System.out.println("root:\n" + root);
-  }
-
-  public void add(long v) {
-    //System.out.println("add v=" + v);
-
-    // NOTE: this works too, but it's ~6% slower on a simple
-    // test with a high-freq TermQuery w/ range faceting on
-    // wikimediumall:
-    /*
-    int index = Arrays.binarySearch(boundaries, v);
-    if (index < 0) {
-      index = -index-1;
-    }
-    leafCounts[index]++;
-    */
-
-    // Binary search to find matched elementary range; we
-    // are guaranteed to find a match because the last
-    // boundary is Long.MAX_VALUE:
-
-    int lo = 0;
-    int hi = boundaries.length - 1;
-    while (true) {
-      int mid = (lo + hi) >>> 1;
-      //System.out.println("  cycle lo=" + lo + " hi=" + hi + " mid=" + mid + " boundary=" + boundaries[mid] + " to " + boundaries[mid+1]);
-      if (v <= boundaries[mid]) {
-        if (mid == 0) {
-          leafCounts[0]++;
-          return;
-        } else {
-          hi = mid - 1;
-        }
-      } else if (v > boundaries[mid+1]) {
-        lo = mid + 1;
-      } else {
-        leafCounts[mid+1]++;
-        //System.out.println("  incr @ " + (mid+1) + "; now " + leafCounts[mid+1]);
-        return;
-      }
-    }
-  }
-
-  /** Fills counts corresponding to the original input
-   *  ranges, returning the missing count (how many hits
-   *  didn't match any ranges). */
-  public int fillCounts(int[] counts) {
-    //System.out.println("  rollup");
-    missingCount = 0;
-    leafUpto = 0;
-    rollup(root, counts, false);
-    return missingCount;
-  }
-
-  private int rollup(LongRangeNode node, int[] counts, boolean sawOutputs) {
-    int count;
-    sawOutputs |= node.outputs != null;
-    if (node.left != null) {
-      count = rollup(node.left, counts, sawOutputs);
-      count += rollup(node.right, counts, sawOutputs);
-    } else {
-      // Leaf:
-      count = leafCounts[leafUpto];
-      leafUpto++;
-      if (!sawOutputs) {
-        // This is a missing count (no output ranges were
-        // seen "above" us):
-        missingCount += count;
-      }
-    }
-    if (node.outputs != null) {
-      for(int rangeIndex : node.outputs) {
-        counts[rangeIndex] += count;
-      }
-    }
-    //System.out.println("  rollup node=" + node.start + " to " + node.end + ": count=" + count);
-    return count;
-  }
-
-  private static LongRangeNode split(int start, int end, List<InclusiveRange> elementaryIntervals) {
-    if (start == end-1) {
-      // leaf
-      InclusiveRange range = elementaryIntervals.get(start);
-      return new LongRangeNode(range.start, range.end, null, null, start);
-    } else {
-      int mid = (start + end) >>> 1;
-      LongRangeNode left = split(start, mid, elementaryIntervals);
-      LongRangeNode right = split(mid, end, elementaryIntervals);
-      return new LongRangeNode(left.start, right.end, left, right, -1);
-    }
-  }
-
-  private static final class InclusiveRange {
-    public final long start;
-    public final long end;
-
-    public InclusiveRange(long start, long end) {
-      assert end >= start;
-      this.start = start;
-      this.end = end;
-    }
-
-    @Override
-    public String toString() {
-      return start + " to " + end;
-    }
-  }
-
-  /** Holds one node of the segment tree. */
-  public static final class LongRangeNode {
-    final LongRangeNode left;
-    final LongRangeNode right;
-
-    // Our range, inclusive:
-    final long start;
-    final long end;
-
-    // If we are a leaf, the index into elementary ranges that
-    // we point to:
-    final int leafIndex;
-
-    // Which range indices to output when a query goes
-    // through this node:
-    List<Integer> outputs;
-
-    public LongRangeNode(long start, long end, LongRangeNode left, LongRangeNode right, int leafIndex) {
-      this.start = start;
-      this.end = end;
-      this.left = left;
-      this.right = right;
-      this.leafIndex = leafIndex;
-    }
-
-    @Override
-    public String toString() {
-      StringBuilder sb = new StringBuilder();
-      toString(sb, 0);
-      return sb.toString();
-    }
-
-    static void indent(StringBuilder sb, int depth) {
-      for(int i=0;i<depth;i++) {
-        sb.append("  ");
-      }
-    }
-
-    /** Recursively assigns range outputs to each node. */
-    void addOutputs(int index, LongRange range) {
-      if (start >= range.minIncl && end <= range.maxIncl) {
-        // Our range is fully included in the incoming
-        // range; add to our output list:
-        if (outputs == null) {
-          outputs = new ArrayList<Integer>();
-        }
-        outputs.add(index);
-      } else if (left != null) {
-        assert right != null;
-        // Recurse:
-        left.addOutputs(index, range);
-        right.addOutputs(index, range);
-      }
-    }
-
-    void toString(StringBuilder sb, int depth) {
-      indent(sb, depth);
-      if (left == null) {
-        assert right == null;
-        sb.append("leaf: " + start + " to " + end);
-      } else {
-        sb.append("node: " + start + " to " + end);
-      }
-      if (outputs != null) {
-        sb.append(" outputs=");
-        sb.append(outputs);
-      }
-      sb.append('\n');
-
-      if (left != null) {
-        assert right != null;
-        left.toString(sb, depth+1);
-        right.toString(sb, depth+1);
-      }
-    }
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/LongRangeFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/LongRangeFacetCounts.java
deleted file mode 100644
index 7f1ce9e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/LongRangeFacetCounts.java
+++ /dev/null
@@ -1,93 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.valuesource.LongFieldSource;
-
-/** {@link Facets} implementation that computes counts for
- *  dynamic long ranges from a provided {@link ValueSource},
- *  using {@link FunctionValues#longVal}.  Use
- *  this for dimensions that change in real-time (e.g. a
- *  relative time based dimension like "Past day", "Past 2
- *  days", etc.) or that change for each request (e.g. 
- *  distance from the user's location, "< 1 km", "< 2 km",
- *  etc.).
- *
- *  @lucene.experimental */
-public class LongRangeFacetCounts extends RangeFacetCounts {
-
-  /** Create {@code LongRangeFacetCounts}, using {@link
-   *  LongFieldSource} from the specified field. */
-  public LongRangeFacetCounts(String field, FacetsCollector hits, LongRange... ranges) throws IOException {
-    this(field, new LongFieldSource(field), hits, ranges);
-  }
-
-  /** Create {@code RangeFacetCounts}, using the provided
-   *  {@link ValueSource}. */
-  public LongRangeFacetCounts(String field, ValueSource valueSource, FacetsCollector hits, LongRange... ranges) throws IOException {
-    super(field, ranges);
-    count(valueSource, hits.getMatchingDocs());
-  }
-
-  private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
-
-    LongRange[] ranges = (LongRange[]) this.ranges;
-
-    // Compute min & max over all ranges:
-    long minIncl = Long.MAX_VALUE;
-    long maxIncl = Long.MIN_VALUE;
-    for(LongRange range : ranges) {
-      minIncl = Math.min(minIncl, range.minIncl);
-      maxIncl = Math.max(maxIncl, range.maxIncl);
-    }
-
-    LongRangeCounter counter = new LongRangeCounter(ranges);
-
-    int missingCount = 0;
-    for (MatchingDocs hits : matchingDocs) {
-      FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
-      final int length = hits.bits.length();
-      int doc = 0;
-      totCount += hits.totalHits;
-      while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
-        // Skip missing docs:
-        if (fv.exists(doc)) {
-          counter.add(fv.longVal(doc));
-        } else {
-          missingCount++;
-        }
-
-        doc++;
-      }
-    }
-    
-    int x = counter.fillCounts(counts);
-
-    missingCount += x;
-
-    //System.out.println("totCount " + totCount + " missingCount " + counter.missingCount);
-    totCount -= missingCount;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/Range.java b/lucene/facet/src/java/org/apache/lucene/facet/Range.java
deleted file mode 100644
index 1ab9105..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/Range.java
+++ /dev/null
@@ -1,36 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Base class for a single labeled range.
- *
- *  @lucene.experimental */
-public abstract class Range {
-  public final String label;
-
-  protected Range(String label) {
-    if (label == null) {
-      throw new NullPointerException("label cannot be null");
-    }
-    this.label = label;
-  }
-
-  protected void failNoMatch() {
-    throw new IllegalArgumentException("range \"" + label + "\" matches nothing");
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/RangeFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/RangeFacetCounts.java
deleted file mode 100644
index e58bf6a..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/RangeFacetCounts.java
+++ /dev/null
@@ -1,69 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.queries.function.valuesource.LongFieldSource;
-
-
-/** Base class for range faceting.
- *
- *  @lucene.experimental */
-abstract class RangeFacetCounts extends Facets {
-  protected final Range[] ranges;
-  protected final int[] counts;
-  protected final String field;
-  protected int totCount;
-
-  /** Create {@code RangeFacetCounts}, using {@link
-   *  LongFieldSource} from the specified field. */
-  protected RangeFacetCounts(String field, Range[] ranges) throws IOException {
-    this.field = field;
-    this.ranges = ranges;
-    counts = new int[ranges.length];
-  }
-
-  @Override
-  public FacetResult getTopChildren(int topN, String dim, String... path) {
-    if (dim.equals(field) == false) {
-      throw new IllegalArgumentException("invalid dim \"" + dim + "\"; should be \"" + field + "\"");
-    }
-    if (path.length != 0) {
-      throw new IllegalArgumentException("path.length should be 0");
-    }
-    LabelAndValue[] labelValues = new LabelAndValue[counts.length];
-    for(int i=0;i<counts.length;i++) {
-      labelValues[i] = new LabelAndValue(ranges[i].label, counts[i]);
-    }
-    return new FacetResult(dim, path, totCount, labelValues, labelValues.length);
-  }
-
-  @Override
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    // TODO: should we impl this?
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public List<FacetResult> getAllDims(int topN) throws IOException {
-    return Collections.singletonList(getTopChildren(topN, null));
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/SearcherTaxonomyManager.java b/lucene/facet/src/java/org/apache/lucene/facet/SearcherTaxonomyManager.java
deleted file mode 100644
index c4b303e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/SearcherTaxonomyManager.java
+++ /dev/null
@@ -1,123 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.ReferenceManager;
-import org.apache.lucene.search.SearcherFactory;
-import org.apache.lucene.search.SearcherManager;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Manages near-real-time reopen of both an IndexSearcher
- * and a TaxonomyReader.
- *
- * <p><b>NOTE</b>: If you call {@link
- * DirectoryTaxonomyWriter#replaceTaxonomy} then you must
- * open a new {@code SearcherTaxonomyManager} afterwards.
- */
-public class SearcherTaxonomyManager extends ReferenceManager<SearcherTaxonomyManager.SearcherAndTaxonomy> {
-
-  /** Holds a matched pair of {@link IndexSearcher} and
-   *  {@link TaxonomyReader} */
-  public static class SearcherAndTaxonomy {
-    public final IndexSearcher searcher;
-    public final DirectoryTaxonomyReader taxonomyReader;
-
-    /** Create a SearcherAndTaxonomy */
-    public SearcherAndTaxonomy(IndexSearcher searcher, DirectoryTaxonomyReader taxonomyReader) {
-      this.searcher = searcher;
-      this.taxonomyReader = taxonomyReader;
-    }
-  }
-
-  private final SearcherFactory searcherFactory;
-  private final long taxoEpoch;
-  private final DirectoryTaxonomyWriter taxoWriter;
-
-  /** Creates near-real-time searcher and taxonomy reader
-   *  from the corresponding writers. */
-  public SearcherTaxonomyManager(IndexWriter writer, boolean applyAllDeletes, SearcherFactory searcherFactory, DirectoryTaxonomyWriter taxoWriter) throws IOException {
-    if (searcherFactory == null) {
-      searcherFactory = new SearcherFactory();
-    }
-    this.searcherFactory = searcherFactory;
-    this.taxoWriter = taxoWriter;
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    current = new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, DirectoryReader.open(writer, applyAllDeletes)),
-                                      taxoReader);
-    taxoEpoch = taxoWriter.getTaxonomyEpoch();
-  }
-
-  @Override
-  protected void decRef(SearcherAndTaxonomy ref) throws IOException {
-    ref.searcher.getIndexReader().decRef();
-
-    // This decRef can fail, and then in theory we should
-    // tryIncRef the searcher to put back the ref count
-    // ... but 1) the below decRef should only fail because
-    // it decRef'd to 0 and closed and hit some IOException
-    // during close, in which case 2) very likely the
-    // searcher was also just closed by the above decRef and
-    // a tryIncRef would fail:
-    ref.taxonomyReader.decRef();
-  }
-
-  @Override
-  protected boolean tryIncRef(SearcherAndTaxonomy ref) throws IOException {
-    if (ref.searcher.getIndexReader().tryIncRef()) {
-      if (ref.taxonomyReader.tryIncRef()) {
-        return true;
-      } else {
-        ref.searcher.getIndexReader().decRef();
-      }
-    }
-    return false;
-  }
-
-  @Override
-  protected SearcherAndTaxonomy refreshIfNeeded(SearcherAndTaxonomy ref) throws IOException {
-    // Must re-open searcher first, otherwise we may get a
-    // new reader that references ords not yet known to the
-    // taxonomy reader:
-    final IndexReader r = ref.searcher.getIndexReader();
-    final IndexReader newReader = DirectoryReader.openIfChanged((DirectoryReader) r);
-    if (newReader == null) {
-      return null;
-    } else {
-      DirectoryTaxonomyReader tr = TaxonomyReader.openIfChanged(ref.taxonomyReader);
-      if (tr == null) {
-        ref.taxonomyReader.incRef();
-        tr = ref.taxonomyReader;
-      } else if (taxoWriter.getTaxonomyEpoch() != taxoEpoch) {
-        IOUtils.close(newReader, tr);
-        throw new IllegalStateException("DirectoryTaxonomyWriter.replaceTaxonomy was called, which is not allowed when using SearcherTaxonomyManager");
-      }
-
-      return new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, newReader), tr);
-    }
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts.java
deleted file mode 100644
index edf759e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts.java
+++ /dev/null
@@ -1,291 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.SortedSetDocValuesReaderState.OrdRange;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.ReaderUtil;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.util.BytesRef;
-
-/** Compute facets counts from previously
- *  indexed {@link SortedSetDocValuesFacetField},
- *  without require a separate taxonomy index.  Faceting is
- *  a bit slower (~25%), and there is added cost on every
- *  {@link IndexReader} open to create a new {@link
- *  SortedSetDocValuesReaderState}.  Furthermore, this does
- *  not support hierarchical facets; only flat (dimension +
- *  label) facets, but it uses quite a bit less RAM to do
- *  so.
- *
- *  <p><b>NOTE</b>: this class should be instantiated and
- *  then used from a single thread, because it holds a
- *  thread-private instance of {@link SortedSetDocValues}.
- * 
- * <p><b>NOTE:<b/>: tie-break is by unicode sort order
- *
- * @lucene.experimental */
-public class SortedSetDocValuesFacetCounts extends Facets {
-
-  final SortedSetDocValuesReaderState state;
-  final SortedSetDocValues dv;
-  final String field;
-  final int[] counts;
-
-  /** Sparse faceting: returns any dimension that had any
-   *  hits, topCount labels per dimension. */
-  public SortedSetDocValuesFacetCounts(SortedSetDocValuesReaderState state, FacetsCollector hits)
-      throws IOException {
-    this.state = state;
-    this.field = state.getField();
-    counts = new int[state.getSize()];
-    dv = state.getDocValues();
-    //System.out.println("field=" + field);
-    count(hits.getMatchingDocs());
-  }
-
-  @Override
-  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    if (topN <= 0) {
-      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
-    }
-    if (path.length > 0) {
-      throw new IllegalArgumentException("path should be 0 length");
-    }
-    OrdRange ordRange = state.getOrdRange(dim);
-    if (ordRange == null) {
-      throw new IllegalArgumentException("dimension \"" + dim + "\" was not indexed");
-    }
-    return getDim(dim, ordRange, topN);
-  }
-
-  private final FacetResult getDim(String dim, OrdRange ordRange, int topN) {
-
-    TopOrdAndIntQueue q = null;
-
-    int bottomCount = 0;
-
-    int dimCount = 0;
-    int childCount = 0;
-
-    TopOrdAndIntQueue.OrdAndValue reuse = null;
-    //System.out.println("getDim : " + ordRange.start + " - " + ordRange.end);
-    for(int ord=ordRange.start; ord<=ordRange.end; ord++) {
-      //System.out.println("  ord=" + ord + " count=" + counts[ord]);
-      if (counts[ord] > 0) {
-        dimCount += counts[ord];
-        childCount++;
-        if (counts[ord] > bottomCount) {
-          if (reuse == null) {
-            reuse = new TopOrdAndIntQueue.OrdAndValue();
-          }
-          reuse.ord = ord;
-          reuse.value = counts[ord];
-          if (q == null) {
-            // Lazy init, so we don't create this for the
-            // sparse case unnecessarily
-            q = new TopOrdAndIntQueue(topN);
-          }
-          reuse = q.insertWithOverflow(reuse);
-          if (q.size() == topN) {
-            bottomCount = q.top().value;
-          }
-        }
-      }
-    }
-
-    if (q == null) {
-      return null;
-    }
-
-    BytesRef scratch = new BytesRef();
-
-    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
-    for(int i=labelValues.length-1;i>=0;i--) {
-      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
-      dv.lookupOrd(ordAndValue.ord, scratch);
-      String[] parts = FacetsConfig.stringToPath(scratch.utf8ToString());
-      labelValues[i] = new LabelAndValue(parts[1], ordAndValue.value);
-    }
-
-    return new FacetResult(dim, new String[0], dimCount, labelValues, childCount);
-  }
-
-  /** Does all the "real work" of tallying up the counts. */
-  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
-    //System.out.println("ssdv count");
-
-    MultiDocValues.OrdinalMap ordinalMap;
-
-    // TODO: is this right?  really, we need a way to
-    // verify that this ordinalMap "matches" the leaves in
-    // matchingDocs...
-    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {
-      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;
-    } else {
-      ordinalMap = null;
-    }
-
-    for(MatchingDocs hits : matchingDocs) {
-
-      AtomicReader reader = hits.context.reader();
-      //System.out.println("  reader=" + reader);
-      // LUCENE-5090: make sure the provided reader context "matches"
-      // the top-level reader passed to the
-      // SortedSetDocValuesReaderState, else cryptic
-      // AIOOBE can happen:
-      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {
-        throw new IllegalStateException("the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader");
-      }
-      
-      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);
-      if (segValues == null) {
-        continue;
-      }
-
-      final int maxDoc = reader.maxDoc();
-      assert maxDoc == hits.bits.length();
-      //System.out.println("  dv=" + dv);
-
-      // TODO: yet another option is to count all segs
-      // first, only in seg-ord space, and then do a
-      // merge-sort-PQ in the end to only "resolve to
-      // global" those seg ords that can compete, if we know
-      // we just want top K?  ie, this is the same algo
-      // that'd be used for merging facets across shards
-      // (distributed faceting).  but this has much higher
-      // temp ram req'ts (sum of number of ords across all
-      // segs)
-      if (ordinalMap != null) {
-        int segOrd = hits.context.ord;
-
-        int numSegOrds = (int) segValues.getValueCount();
-
-        if (hits.totalHits < numSegOrds/10) {
-          //System.out.println("    remap as-we-go");
-          // Remap every ord to global ord as we iterate:
-          int doc = 0;
-          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
-            //System.out.println("    doc=" + doc);
-            segValues.setDocument(doc);
-            int term = (int) segValues.nextOrd();
-            while (term != SortedSetDocValues.NO_MORE_ORDS) {
-              //System.out.println("      segOrd=" + segOrd + " ord=" + term + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, term));
-              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;
-              term = (int) segValues.nextOrd();
-            }
-            ++doc;
-          }
-        } else {
-          //System.out.println("    count in seg ord first");
-
-          // First count in seg-ord space:
-          final int[] segCounts = new int[numSegOrds];
-          int doc = 0;
-          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
-            //System.out.println("    doc=" + doc);
-            segValues.setDocument(doc);
-            int term = (int) segValues.nextOrd();
-            while (term != SortedSetDocValues.NO_MORE_ORDS) {
-              //System.out.println("      ord=" + term);
-              segCounts[term]++;
-              term = (int) segValues.nextOrd();
-            }
-            ++doc;
-          }
-
-          // Then, migrate to global ords:
-          for(int ord=0;ord<numSegOrds;ord++) {
-            int count = segCounts[ord];
-            if (count != 0) {
-              //System.out.println("    migrate segOrd=" + segOrd + " ord=" + ord + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, ord));
-              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;
-            }
-          }
-        }
-      } else {
-        // No ord mapping (e.g., single segment index):
-        // just aggregate directly into counts:
-
-        int doc = 0;
-        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
-          segValues.setDocument(doc);
-          int term = (int) segValues.nextOrd();
-          while (term != SortedSetDocValues.NO_MORE_ORDS) {
-            counts[term]++;
-            term = (int) segValues.nextOrd();
-          }
-          ++doc;
-        }
-      }
-    }
-  }
-
-  @Override
-  public Number getSpecificValue(String dim, String... path) {
-    if (path.length != 1) {
-      throw new IllegalArgumentException("path must be length=1");
-    }
-    int ord = (int) dv.lookupTerm(new BytesRef(FacetsConfig.pathToString(dim, path)));
-    if (ord < 0) {
-      return -1;
-    }
-
-    return counts[ord];
-  }
-
-  @Override
-  public List<FacetResult> getAllDims(int topN) throws IOException {
-
-    List<FacetResult> results = new ArrayList<FacetResult>();
-    for(Map.Entry<String,OrdRange> ent : state.getPrefixToOrdRange().entrySet()) {
-      FacetResult fr = getDim(ent.getKey(), ent.getValue(), topN);
-      if (fr != null) {
-        results.add(fr);
-      }
-    }
-
-    // Sort by highest count:
-    Collections.sort(results,
-                     new Comparator<FacetResult>() {
-                       @Override
-                       public int compare(FacetResult a, FacetResult b) {
-                         if (a.value.intValue() > b.value.intValue()) {
-                           return -1;
-                         } else if (b.value.intValue() > a.value.intValue()) {
-                           return 1;
-                         } else {
-                           return a.dim.compareTo(b.dim);
-                         }
-                       }
-                     });
-
-    return results;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetField.java
deleted file mode 100644
index 1297963..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetField.java
+++ /dev/null
@@ -1,44 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-
-/** Add an instance of this to your Document for every facet
- *  label to be indexed via SortedSetDocValues. */
-public class SortedSetDocValuesFacetField extends Field {
-  static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setIndexed(true);
-    TYPE.freeze();
-  }
-  final String dim;
-  final String label;
-
-  public SortedSetDocValuesFacetField(String dim, String label) {
-    super("dummy", TYPE);
-    this.dim = dim;
-    this.label = label;
-  }
-
-  @Override
-  public String toString() {
-    return "SortedSetDocValuesFacetField(dim=" + dim + " label=" + label + ")";
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesReaderState.java b/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesReaderState.java
deleted file mode 100644
index 459b853..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesReaderState.java
+++ /dev/null
@@ -1,147 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.util.BytesRef;
-
-/** Wraps a {@link IndexReader} and resolves ords
- *  using existing {@link SortedSetDocValues} APIs without a
- *  separate taxonomy index.  This only supports flat facets
- *  (dimension + label), and it makes faceting a bit
- *  slower, adds some cost at reopen time, but avoids
- *  managing the separate taxonomy index.  It also requires
- *  less RAM than the taxonomy index, as it manages the flat
- *  (2-level) hierarchy more efficiently.  In addition, the
- *  tie-break during faceting is now meaningful (in label
- *  sorted order).
- *
- *  <p><b>NOTE</b>: creating an instance of this class is
- *  somewhat costly, as it computes per-segment ordinal maps,
- *  so you should create it once and re-use that one instance
- *  for a given {@link IndexReader}. */
-
-public final class SortedSetDocValuesReaderState {
-
-  private final String field;
-  private final AtomicReader topReader;
-  private final int valueCount;
-  public final IndexReader origReader;
-
-  /** Holds start/end range of ords, which maps to one
-   *  dimension (someday we may generalize it to map to
-   *  hierarchies within one dimension). */
-  public static final class OrdRange {
-    /** Start of range, inclusive: */
-    public final int start;
-    /** End of range, inclusive: */
-    public final int end;
-
-    /** Start and end are inclusive. */
-    public OrdRange(int start, int end) {
-      this.start = start;
-      this.end = end;
-    }
-  }
-
-  private final Map<String,OrdRange> prefixToOrdRange = new HashMap<String,OrdRange>();
-
-  /** Creates this, pulling doc values from the default {@link
-   *  FacetsConfig#DEFAULT_INDEX_FIELD_NAME}. */ 
-  public SortedSetDocValuesReaderState(IndexReader reader) throws IOException {
-    this(reader, FacetsConfig.DEFAULT_INDEX_FIELD_NAME);
-  }
-
-  /** Creates this, pulling doc values from the specified
-   *  field. */
-  public SortedSetDocValuesReaderState(IndexReader reader, String field) throws IOException {
-
-    this.field = field;
-    this.origReader = reader;
-
-    // We need this to create thread-safe MultiSortedSetDV
-    // per collector:
-    topReader = SlowCompositeReaderWrapper.wrap(reader);
-    SortedSetDocValues dv = topReader.getSortedSetDocValues(field);
-    if (dv == null) {
-      throw new IllegalArgumentException("field \"" + field + "\" was not indexed with SortedSetDocValues");
-    }
-    if (dv.getValueCount() > Integer.MAX_VALUE) {
-      throw new IllegalArgumentException("can only handle valueCount < Integer.MAX_VALUE; got " + dv.getValueCount());
-    }
-    valueCount = (int) dv.getValueCount();
-
-    // TODO: we can make this more efficient if eg we can be
-    // "involved" when OrdinalMap is being created?  Ie see
-    // each term/ord it's assigning as it goes...
-    String lastDim = null;
-    int startOrd = -1;
-    BytesRef spare = new BytesRef();
-
-    // TODO: this approach can work for full hierarchy?;
-    // TaxoReader can't do this since ords are not in
-    // "sorted order" ... but we should generalize this to
-    // support arbitrary hierarchy:
-    for(int ord=0;ord<valueCount;ord++) {
-      dv.lookupOrd(ord, spare);
-      String[] components = FacetsConfig.stringToPath(spare.utf8ToString());
-      if (components.length != 2) {
-        throw new IllegalArgumentException("this class can only handle 2 level hierarchy (dim/value); got: " + Arrays.toString(components) + " " + spare.utf8ToString());
-      }
-      if (!components[0].equals(lastDim)) {
-        if (lastDim != null) {
-          prefixToOrdRange.put(lastDim, new OrdRange(startOrd, ord-1));
-        }
-        startOrd = ord;
-        lastDim = components[0];
-      }
-    }
-
-    if (lastDim != null) {
-      prefixToOrdRange.put(lastDim, new OrdRange(startOrd, valueCount-1));
-    }
-  }
-
-  public SortedSetDocValues getDocValues() throws IOException {
-    return topReader.getSortedSetDocValues(field);
-  }
-
-  public Map<String,OrdRange> getPrefixToOrdRange() {
-    return prefixToOrdRange;
-  }
-
-  public OrdRange getOrdRange(String dim) {
-    return prefixToOrdRange.get(dim);
-  }
-
-  public String getField() {
-    return field;
-  }
-
-  public int getSize() {
-    return valueCount;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetCounts.java
deleted file mode 100644
index 40c11e5..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetCounts.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.IntsRef;
-
-/** Reads from any {@link OrdinalsReader}; use {@link
- *  FastTaxonomyFacetCounts} if you are using the
- *  default encoding from {@link BinaryDocValues}.
- * 
- * @lucene.experimental */
-public class TaxonomyFacetCounts extends IntTaxonomyFacets {
-  private final OrdinalsReader ordinalsReader;
-
-  /** Create {@code TaxonomyFacetCounts}, which also
-   *  counts all facet labels.  Use this for a non-default
-   *  {@link OrdinalsReader}; otherwise use {@link
-   *  FastTaxonomyFacetCounts}. */
-  public TaxonomyFacetCounts(OrdinalsReader ordinalsReader, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
-    super(ordinalsReader.getIndexFieldName(), taxoReader, config);
-    this.ordinalsReader = ordinalsReader;
-    count(fc.getMatchingDocs());
-  }
-
-  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
-    IntsRef scratch  = new IntsRef();
-    for(MatchingDocs hits : matchingDocs) {
-      OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        ords.get(doc, scratch);
-        for(int i=0;i<scratch.length;i++) {
-          values[scratch.ints[scratch.offset+i]]++;
-        }
-        ++doc;
-      }
-    }
-
-    rollup();
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumFloatAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumFloatAssociations.java
deleted file mode 100644
index 05c4e10..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumFloatAssociations.java
+++ /dev/null
@@ -1,89 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-
-/** Aggregates sum of int values previously indexed with
- *  {@link FloatAssociationFacetField}, assuming the default
- *  encoding.
- *
- *  @lucene.experimental */
-public class TaxonomyFacetSumFloatAssociations extends FloatTaxonomyFacets {
-
-  /** Create {@code TaxonomyFacetSumFloatAssociations} against
-   *  the default index field. */
-  public TaxonomyFacetSumFloatAssociations(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
-    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
-  }
-
-  /** Create {@code TaxonomyFacetSumFloatAssociations} against
-   *  the specified index field. */
-  public TaxonomyFacetSumFloatAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
-    super(indexFieldName, taxoReader, config);
-    sumValues(fc.getMatchingDocs());
-  }
-
-  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
-    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
-    for(MatchingDocs hits : matchingDocs) {
-      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
-      if (dv == null) { // this reader does not have DocValues for the requested category list
-        continue;
-      }
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      BytesRef scratch = new BytesRef();
-      //System.out.println("count seg=" + hits.context.reader());
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        //System.out.println("  doc=" + doc);
-        // TODO: use OrdinalsReader?  we'd need to add a
-        // BytesRef getAssociation()?
-        dv.get(doc, scratch);
-        byte[] bytes = scratch.bytes;
-        int end = scratch.offset + scratch.length;
-        int offset = scratch.offset;
-        while (offset < end) {
-          int ord = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          int value = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          values[ord] += Float.intBitsToFloat(value);
-        }
-        ++doc;
-      }
-    }
-
-    rollup();
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumIntAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumIntAssociations.java
deleted file mode 100644
index a2e4495..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumIntAssociations.java
+++ /dev/null
@@ -1,89 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-
-/** Aggregates sum of int values previously indexed with
- *  {@link IntAssociationFacetField}, assuming the default
- *  encoding.
- *
- *  @lucene.experimental */
-public class TaxonomyFacetSumIntAssociations extends IntTaxonomyFacets {
-
-  /** Create {@code TaxonomyFacetSumIntAssociations} against
-   *  the default index field. */
-  public TaxonomyFacetSumIntAssociations(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
-    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
-  }
-
-  /** Create {@code TaxonomyFacetSumIntAssociations} against
-   *  the specified index field. */
-  public TaxonomyFacetSumIntAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
-    super(indexFieldName, taxoReader, config);
-    sumValues(fc.getMatchingDocs());
-  }
-
-  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
-    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
-    for(MatchingDocs hits : matchingDocs) {
-      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
-      if (dv == null) { // this reader does not have DocValues for the requested category list
-        continue;
-      }
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      BytesRef scratch = new BytesRef();
-      //System.out.println("count seg=" + hits.context.reader());
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        //System.out.println("  doc=" + doc);
-        // TODO: use OrdinalsReader?  we'd need to add a
-        // BytesRef getAssociation()?
-        dv.get(doc, scratch);
-        byte[] bytes = scratch.bytes;
-        int end = scratch.offset + scratch.length;
-        int offset = scratch.offset;
-        while (offset < end) {
-          int ord = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          int value = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          values[ord] += value;
-        }
-        ++doc;
-      }
-    }
-
-    rollup();
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumValueSource.java b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumValueSource.java
deleted file mode 100644
index ba70426..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumValueSource.java
+++ /dev/null
@@ -1,134 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.IntsRef;
-
-/** Aggregates sum of values from {@link
- *  FunctionValues#doubleVal}, for each facet label.
- *
- *  @lucene.experimental */
-public class TaxonomyFacetSumValueSource extends FloatTaxonomyFacets {
-  private final OrdinalsReader ordinalsReader;
-
-  /** Aggreggates float facet values from the provided
-   *  {@link ValueSource}, pulling ordinals using {@link
-   *  DocValuesOrdinalsReader} against the default indexed
-   *  facet field {@link
-   *  FacetsConfig#DEFAULT_INDEX_FIELD_NAME}. */
-  public TaxonomyFacetSumValueSource(TaxonomyReader taxoReader, FacetsConfig config,
-                                     FacetsCollector fc, ValueSource valueSource) throws IOException {
-    this(new DocValuesOrdinalsReader(FacetsConfig.DEFAULT_INDEX_FIELD_NAME), taxoReader, config, fc, valueSource);
-  }
-
-  /** Aggreggates float facet values from the provided
-   *  {@link ValueSource}, and pulls ordinals from the
-   *  provided {@link OrdinalsReader}. */
-  public TaxonomyFacetSumValueSource(OrdinalsReader ordinalsReader, TaxonomyReader taxoReader,
-                                     FacetsConfig config, FacetsCollector fc, ValueSource valueSource) throws IOException {
-    super(ordinalsReader.getIndexFieldName(), taxoReader, config);
-    this.ordinalsReader = ordinalsReader;
-    sumValues(fc.getMatchingDocs(), fc.getKeepScores(), valueSource);
-  }
-
-  private static final class FakeScorer extends Scorer {
-    float score;
-    int docID;
-    FakeScorer() { super(null); }
-    @Override public float score() throws IOException { return score; }
-    @Override public int freq() throws IOException { throw new UnsupportedOperationException(); }
-    @Override public int docID() { return docID; }
-    @Override public int nextDoc() throws IOException { throw new UnsupportedOperationException(); }
-    @Override public int advance(int target) throws IOException { throw new UnsupportedOperationException(); }
-    @Override public long cost() { return 0; }
-  }
-
-  private final void sumValues(List<MatchingDocs> matchingDocs, boolean keepScores, ValueSource valueSource) throws IOException {
-    final FakeScorer scorer = new FakeScorer();
-    Map<String, Scorer> context = new HashMap<String, Scorer>();
-    if (keepScores) {
-      context.put("scorer", scorer);
-    }
-    IntsRef scratch = new IntsRef();
-    for(MatchingDocs hits : matchingDocs) {
-      OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      int scoresIdx = 0;
-      float[] scores = hits.scores;
-
-      FunctionValues functionValues = valueSource.getValues(context, hits.context);
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        ords.get(doc, scratch);
-        if (keepScores) {
-          scorer.docID = doc;
-          scorer.score = scores[scoresIdx++];
-        }
-        float value = (float) functionValues.doubleVal(doc);
-        for(int i=0;i<scratch.length;i++) {
-          values[scratch.ints[i]] += value;
-        }
-        ++doc;
-      }
-    }
-
-    rollup();
-  }
-
-  /** {@link ValueSource} that returns the score for each
-   *  hit; use this to aggregate the sum of all hit scores
-   *  for each facet label.  */
-  public static class ScoreValueSource extends ValueSource {
-    @Override
-    public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
-      final Scorer scorer = (Scorer) context.get("scorer");
-      if (scorer == null) {
-        throw new IllegalStateException("scores are missing; be sure to pass keepScores=true to FacetsCollector");
-      }
-      return new DoubleDocValues(this) {
-        @Override
-        public double doubleVal(int document) {
-          try {
-            return scorer.score();
-          } catch (IOException exception) {
-            throw new RuntimeException(exception);
-          }
-        }
-      };
-    }
-
-    @Override public boolean equals(Object o) { return o == this; }
-    @Override public int hashCode() { return System.identityHashCode(this); }
-    @Override public String description() { return "score()"; }
-    };
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacets.java
deleted file mode 100644
index 4c1b366..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacets.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/** Base class for all taxonomy-based facets impls. */
-public abstract class TaxonomyFacets extends Facets {
-  protected final String indexFieldName;
-  protected final TaxonomyReader taxoReader;
-  protected final FacetsConfig config;
-  protected final int[] children;
-  protected final int[] siblings;
-
-  protected TaxonomyFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config) throws IOException {
-    this.indexFieldName = indexFieldName;
-    this.taxoReader = taxoReader;
-    this.config = config;
-    ParallelTaxonomyArrays pta = taxoReader.getParallelTaxonomyArrays();
-    children = pta.children();
-    siblings = pta.siblings();
-  }
-
-  protected FacetsConfig.DimConfig verifyDim(String dim) {
-    FacetsConfig.DimConfig dimConfig = config.getDimConfig(dim);
-    if (!dimConfig.indexFieldName.equals(indexFieldName)) {
-      throw new IllegalArgumentException("dimension \"" + dim + "\" was not indexed into field \"" + indexFieldName);
-    }
-    return dimConfig;
-  }
-
-  @Override
-  public List<FacetResult> getAllDims(int topN) throws IOException {
-    int ord = children[TaxonomyReader.ROOT_ORDINAL];
-    List<FacetResult> results = new ArrayList<FacetResult>();
-    while (ord != TaxonomyReader.INVALID_ORDINAL) {
-      String dim = taxoReader.getPath(ord).components[0];
-      FacetsConfig.DimConfig dimConfig = config.getDimConfig(dim);
-      if (dimConfig.indexFieldName.equals(indexFieldName)) {
-        FacetResult result = getTopChildren(topN, dim);
-        if (result != null) {
-          results.add(result);
-        }
-      }
-      ord = siblings[ord];
-    }
-
-    // Sort by highest value, tie break by value:
-    Collections.sort(results,
-                     new Comparator<FacetResult>() {
-                       @Override
-                       public int compare(FacetResult a, FacetResult b) {
-                         if (a.value.doubleValue() > b.value.doubleValue()) {
-                           return -1;
-                         } else if (b.value.doubleValue() > a.value.doubleValue()) {
-                           return 1;
-                         } else {
-                           return a.dim.compareTo(b.dim);
-                         }
-                       }
-                     });
-    return results;
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndFloatQueue.java b/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndFloatQueue.java
index af9fecf..91d9e6d 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndFloatQueue.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndFloatQueue.java
@@ -21,11 +21,11 @@ import org.apache.lucene.util.PriorityQueue;
 
 /** Keeps highest results, first by largest float value,
  *  then tie break by smallest ord. */
-class TopOrdAndFloatQueue extends PriorityQueue<TopOrdAndFloatQueue.OrdAndValue> {
+public class TopOrdAndFloatQueue extends PriorityQueue<TopOrdAndFloatQueue.OrdAndValue> {
 
   public static final class OrdAndValue {
-    int ord;
-    float value;
+    public int ord;
+    public float value;
   }
 
   public TopOrdAndFloatQueue(int topN) {
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndIntQueue.java b/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndIntQueue.java
index f1c8cde..06fffb7 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndIntQueue.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndIntQueue.java
@@ -21,11 +21,11 @@ import org.apache.lucene.util.PriorityQueue;
 
 /** Keeps highest results, first by largest int value,
  *  then tie break by smallest ord. */
-class TopOrdAndIntQueue extends PriorityQueue<TopOrdAndIntQueue.OrdAndValue> {
+public class TopOrdAndIntQueue extends PriorityQueue<TopOrdAndIntQueue.OrdAndValue> {
 
   public static final class OrdAndValue {
-    int ord;
-    int value;
+    public int ord;
+    public int value;
   }
 
   public TopOrdAndIntQueue(int topN) {
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/package.html b/lucene/facet/src/java/org/apache/lucene/facet/package.html
index 4d8f500..be1de9a 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/package.html
+++ b/lucene/facet/src/java/org/apache/lucene/facet/package.html
@@ -27,23 +27,23 @@
 	<li> Taxonomy-based methods rely on a separate taxonomy index to
           map hierarchical facet paths to global int ordinals for fast
           counting at search time; these methods can compute counts
-          (({@link org.apache.lucene.facet.FastTaxonomyFacetCounts}, {@link
-          org.apache.lucene.facet.TaxonomyFacetCounts}) aggregate long or double values {@link
-          org.apache.lucene.facet.TaxonomyFacetSumIntAssociations}, {@link
-          org.apache.lucene.facet.TaxonomyFacetSumFloatAssociations}, {@link
-          org.apache.lucene.facet.TaxonomyFacetSumValueSource}.  Add {@link org.apache.lucene.facet.FacetField} or
-          {@link org.apache.lucene.facet.AssociationFacetField} to your documents at index time
+          (({@link org.apache.lucene.facet.taxonomy.FastTaxonomyFacetCounts}, {@link
+          org.apache.lucene.facet.taxonomy.TaxonomyFacetCounts}) aggregate long or double values {@link
+          org.apache.lucene.facet.taxonomy.TaxonomyFacetSumIntAssociations}, {@link
+          org.apache.lucene.facet.taxonomy.TaxonomyFacetSumFloatAssociations}, {@link
+          org.apache.lucene.facet.taxonomy.TaxonomyFacetSumValueSource}.  Add {@link org.apache.lucene.facet.FacetField} or
+          {@link org.apache.lucene.facet.taxonomy.AssociationFacetField} to your documents at index time
           to use taxonomy-based methods.
 
 	<li> Sorted-set doc values method does not require a separate
           taxonomy index, and computes counts based on sorted set doc
-          values fields ({@link org.apache.lucene.facet.SortedSetDocValuesFacetCounts}).  Add
-          {@link org.apache.lucene.facet.SortedSetDocValuesFacetField} to your documents at
+          values fields ({@link org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetCounts}).  Add
+          {@link org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetField} to your documents at
           index time to use sorted set facet counts.
 
-	<li> Range faceting {@link org.apache.lucene.facet.LongRangeFacetCounts}, {@link
-          org.apache.lucene.facet.DoubleRangeFacetCounts} compute counts for a dynamic numeric
-          range from a provided {@link org.apache.lucene.facet.ValueSource} (previously indexed
+	<li> Range faceting {@link org.apache.lucene.facet.range.LongRangeFacetCounts}, {@link
+          org.apache.lucene.facet.range.DoubleRangeFacetCounts} compute counts for a dynamic numeric
+          range from a provided {@link org.apache.lucene.queries.function.ValueSource} (previously indexed
           numeric field, or a dynamic expression such as distance).
       </ul>
     </p>
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRange.java b/lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRange.java
new file mode 100644
index 0000000..55f5747
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRange.java
@@ -0,0 +1,162 @@
+package org.apache.lucene.facet.range;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.NumericRangeFilter;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.NumericUtils;
+
+/** Represents a range over double values. */
+public final class DoubleRange extends Range {
+  final double minIncl;
+  final double maxIncl;
+
+  public final double min;
+  public final double max;
+  public final boolean minInclusive;
+  public final boolean maxInclusive;
+
+  /** Create a DoubleRange. */
+  public DoubleRange(String label, double minIn, boolean minInclusive, double maxIn, boolean maxInclusive) {
+    super(label);
+    this.min = minIn;
+    this.max = maxIn;
+    this.minInclusive = minInclusive;
+    this.maxInclusive = maxInclusive;
+
+    // TODO: if DoubleDocValuesField used
+    // NumericUtils.doubleToSortableLong format (instead of
+    // Double.doubleToRawLongBits) we could do comparisons
+    // in long space 
+
+    if (Double.isNaN(min)) {
+      throw new IllegalArgumentException("min cannot be NaN");
+    }
+    if (!minInclusive) {
+      minIn = Math.nextUp(minIn);
+    }
+
+    if (Double.isNaN(max)) {
+      throw new IllegalArgumentException("max cannot be NaN");
+    }
+    if (!maxInclusive) {
+      // Why no Math.nextDown?
+      maxIn = Math.nextAfter(maxIn, Double.NEGATIVE_INFINITY);
+    }
+
+    if (minIn > maxIn) {
+      failNoMatch();
+    }
+
+    this.minIncl = minIn;
+    this.maxIncl = maxIn;
+  }
+
+  public boolean accept(double value) {
+    return value >= minIncl && value <= maxIncl;
+  }
+
+  LongRange toLongRange() {
+    return new LongRange(label,
+                         NumericUtils.doubleToSortableLong(minIncl), true,
+                         NumericUtils.doubleToSortableLong(maxIncl), true);
+  }
+
+  @Override
+  public String toString() {
+    return "DoubleRange(" + minIncl + " to " + maxIncl + ")";
+  }
+
+  /** Returns a new {@link Filter} accepting only documents
+   *  in this range.  Note that this filter is not
+   *  efficient: it's a linear scan of all docs, testing
+   *  each value.  If the {@link ValueSource} is static,
+   *  e.g. an indexed numeric field, then it's more
+   *  efficient to use {@link NumericRangeFilter}. */
+  public Filter getFilter(final ValueSource valueSource) {
+    return new Filter() {
+      @Override
+      public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+
+        // TODO: this is just like ValueSourceScorer,
+        // ValueSourceFilter (spatial),
+        // ValueSourceRangeFilter (solr); also,
+        // https://issues.apache.org/jira/browse/LUCENE-4251
+
+        final FunctionValues values = valueSource.getValues(Collections.emptyMap(), context);
+
+        final int maxDoc = context.reader().maxDoc();
+
+        return new DocIdSet() {
+
+          @Override
+          public DocIdSetIterator iterator() {
+            return new DocIdSetIterator() {
+              int doc = -1;
+
+              @Override
+              public int nextDoc() throws IOException {
+                while (true) {
+                  doc++;
+                  if (doc == maxDoc) {
+                    return doc = NO_MORE_DOCS;
+                  }
+                  if (acceptDocs != null && acceptDocs.get(doc) == false) {
+                    continue;
+                  }
+                  double v = values.doubleVal(doc);
+                  if (accept(v)) {
+                    return doc;
+                  }
+                }
+              }
+
+              @Override
+              public int advance(int target) throws IOException {
+                doc = target-1;
+                return nextDoc();
+              }
+
+              @Override
+              public int docID() {
+                return doc;
+              }
+
+              @Override
+              public long cost() {
+                // Since we do a linear scan over all
+                // documents, our cost is O(maxDoc):
+                return maxDoc;
+              }
+            };
+          }
+        };
+      }
+    };
+  }
+}
+
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRangeFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRangeFacetCounts.java
new file mode 100644
index 0000000..74eef29
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/range/DoubleRangeFacetCounts.java
@@ -0,0 +1,109 @@
+package org.apache.lucene.facet.range;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.lucene.document.DoubleDocValuesField; // javadocs
+import org.apache.lucene.document.FloatDocValuesField; // javadocs
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.valuesource.DoubleFieldSource;
+import org.apache.lucene.queries.function.valuesource.FloatFieldSource; // javadocs
+import org.apache.lucene.util.NumericUtils;
+
+/** {@link Facets} implementation that computes counts for
+ *  dynamic double ranges from a provided {@link
+ *  ValueSource}, using {@link FunctionValues#doubleVal}.  Use
+ *  this for dimensions that change in real-time (e.g. a
+ *  relative time based dimension like "Past day", "Past 2
+ *  days", etc.) or that change for each request (e.g.
+ *  distance from the user's location, "< 1 km", "< 2 km",
+ *  etc.).
+ *
+ *  <p> If you had indexed your field using {@link
+ *  FloatDocValuesField} then pass {@link FloatFieldSource}
+ *  as the {@link ValueSource}; if you used {@link
+ *  DoubleDocValuesField} then pass {@link
+ *  DoubleFieldSource} (this is the default used when you
+ *  pass just a the field name).
+ *
+ *  @lucene.experimental */
+public class DoubleRangeFacetCounts extends RangeFacetCounts {
+
+  /** Create {@code RangeFacetCounts}, using {@link
+   *  DoubleFieldSource} from the specified field. */
+  public DoubleRangeFacetCounts(String field, FacetsCollector hits, DoubleRange... ranges) throws IOException {
+    this(field, new DoubleFieldSource(field), hits, ranges);
+  }
+
+  /** Create {@code RangeFacetCounts}, using the provided
+   *  {@link ValueSource}. */
+  public DoubleRangeFacetCounts(String field, ValueSource valueSource, FacetsCollector hits, DoubleRange... ranges) throws IOException {
+    super(field, ranges);
+    count(valueSource, hits.getMatchingDocs());
+  }
+
+  private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
+
+    DoubleRange[] ranges = (DoubleRange[]) this.ranges;
+
+    LongRange[] longRanges = new LongRange[ranges.length];
+    for(int i=0;i<ranges.length;i++) {
+      DoubleRange range = ranges[i];
+      longRanges[i] =  new LongRange(range.label,
+                                     NumericUtils.doubleToSortableLong(range.minIncl), true,
+                                     NumericUtils.doubleToSortableLong(range.maxIncl), true);
+    }
+
+    LongRangeCounter counter = new LongRangeCounter(longRanges);
+
+    // Compute min & max over all ranges:
+    double minIncl = Double.POSITIVE_INFINITY;
+    double maxIncl = Double.NEGATIVE_INFINITY;
+    for(DoubleRange range : ranges) {
+      minIncl = Math.min(minIncl, range.minIncl);
+      maxIncl = Math.max(maxIncl, range.maxIncl);
+    }
+
+    int missingCount = 0;
+    for (MatchingDocs hits : matchingDocs) {
+      FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
+      final int length = hits.bits.length();
+      int doc = 0;
+      totCount += hits.totalHits;
+      while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
+        // Skip missing docs:
+        if (fv.exists(doc)) {
+          counter.add(NumericUtils.doubleToSortableLong(fv.doubleVal(doc)));
+        } else {
+          missingCount++;
+        }
+        doc++;
+      }
+    }
+
+    missingCount += counter.fillCounts(counts);
+    totCount -= missingCount;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/LongRange.java b/lucene/facet/src/java/org/apache/lucene/facet/range/LongRange.java
new file mode 100644
index 0000000..209d780
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/range/LongRange.java
@@ -0,0 +1,153 @@
+package org.apache.lucene.facet.range;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.NumericRangeFilter;
+import org.apache.lucene.util.Bits;
+
+/** Represents a range over long values. */
+public final class LongRange extends Range {
+  final long minIncl;
+  final long maxIncl;
+
+  public final long min;
+  public final long max;
+  public final boolean minInclusive;
+  public final boolean maxInclusive;
+
+  // TODO: can we require fewer args? (same for
+  // Double/FloatRange too)
+
+  /** Create a LongRange. */
+  public LongRange(String label, long minIn, boolean minInclusive, long maxIn, boolean maxInclusive) {
+    super(label);
+    this.min = minIn;
+    this.max = maxIn;
+    this.minInclusive = minInclusive;
+    this.maxInclusive = maxInclusive;
+
+    if (!minInclusive) {
+      if (minIn != Long.MAX_VALUE) {
+        minIn++;
+      } else {
+        failNoMatch();
+      }
+    }
+
+    if (!maxInclusive) {
+      if (maxIn != Long.MIN_VALUE) {
+        maxIn--;
+      } else {
+        failNoMatch();
+      }
+    }
+
+    if (minIn > maxIn) {
+      failNoMatch();
+    }
+
+    this.minIncl = minIn;
+    this.maxIncl = maxIn;
+  }
+
+  public boolean accept(long value) {
+    return value >= minIncl && value <= maxIncl;
+  }
+
+  @Override
+  public String toString() {
+    return "LongRange(" + minIncl + " to " + maxIncl + ")";
+  }
+
+  /** Returns a new {@link Filter} accepting only documents
+   *  in this range.  Note that this filter is not
+   *  efficient: it's a linear scan of all docs, testing
+   *  each value.  If the {@link ValueSource} is static,
+   *  e.g. an indexed numeric field, then it's more
+   *  efficient to use {@link NumericRangeFilter}. */
+  public Filter getFilter(final ValueSource valueSource) {
+    return new Filter() {
+      @Override
+      public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+
+        // TODO: this is just like ValueSourceScorer,
+        // ValueSourceFilter (spatial),
+        // ValueSourceRangeFilter (solr); also,
+        // https://issues.apache.org/jira/browse/LUCENE-4251
+
+        final FunctionValues values = valueSource.getValues(Collections.emptyMap(), context);
+
+        final int maxDoc = context.reader().maxDoc();
+
+        return new DocIdSet() {
+
+          @Override
+          public DocIdSetIterator iterator() {
+            return new DocIdSetIterator() {
+              int doc = -1;
+
+              @Override
+              public int nextDoc() throws IOException {
+                while (true) {
+                  doc++;
+                  if (doc == maxDoc) {
+                    return doc = NO_MORE_DOCS;
+                  }
+                  if (acceptDocs != null && acceptDocs.get(doc) == false) {
+                    continue;
+                  }
+                  long v = values.longVal(doc);
+                  if (accept(v)) {
+                    return doc;
+                  }
+                }
+              }
+
+              @Override
+              public int advance(int target) throws IOException {
+                doc = target-1;
+                return nextDoc();
+              }
+
+              @Override
+              public int docID() {
+                return doc;
+              }
+
+              @Override
+              public long cost() {
+                // Since we do a linear scan over all
+                // documents, our cost is O(maxDoc):
+                return maxDoc;
+              }
+            };
+          }
+        };
+      }
+    };
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/LongRangeCounter.java b/lucene/facet/src/java/org/apache/lucene/facet/range/LongRangeCounter.java
new file mode 100644
index 0000000..3e9db73
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/range/LongRangeCounter.java
@@ -0,0 +1,316 @@
+package org.apache.lucene.facet.range;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+/** Counts how many times each range was seen;
+ *  per-hit it's just a binary search ({@link #add})
+ *  against the elementary intervals, and in the end we
+ *  rollup back to the original ranges. */
+
+final class LongRangeCounter {
+
+  final LongRangeNode root;
+  final long[] boundaries;
+  final int[] leafCounts;
+
+  // Used during rollup
+  private int leafUpto;
+  private int missingCount;
+
+  public LongRangeCounter(LongRange[] ranges) {
+    // Maps all range inclusive endpoints to int flags; 1
+    // = start of interval, 2 = end of interval.  We need to
+    // track the start vs end case separately because if a
+    // given point is both, then it must be its own
+    // elementary interval:
+    Map<Long,Integer> endsMap = new HashMap<Long,Integer>();
+
+    endsMap.put(Long.MIN_VALUE, 1);
+    endsMap.put(Long.MAX_VALUE, 2);
+
+    for(LongRange range : ranges) {
+      Integer cur = endsMap.get(range.minIncl);
+      if (cur == null) {
+        endsMap.put(range.minIncl, 1);
+      } else {
+        endsMap.put(range.minIncl, cur.intValue() | 1);
+      }
+      cur = endsMap.get(range.maxIncl);
+      if (cur == null) {
+        endsMap.put(range.maxIncl, 2);
+      } else {
+        endsMap.put(range.maxIncl, cur.intValue() | 2);
+      }
+    }
+
+    List<Long> endsList = new ArrayList<Long>(endsMap.keySet());
+    Collections.sort(endsList);
+
+    // Build elementaryIntervals (a 1D Venn diagram):
+    List<InclusiveRange> elementaryIntervals = new ArrayList<InclusiveRange>();
+    int upto0 = 1;
+    long v = endsList.get(0);
+    long prev;
+    if (endsMap.get(v) == 3) {
+      elementaryIntervals.add(new InclusiveRange(v, v));
+      prev = v+1;
+    } else {
+      prev = v;
+    }
+
+    while (upto0 < endsList.size()) {
+      v = endsList.get(upto0);
+      int flags = endsMap.get(v);
+      //System.out.println("  v=" + v + " flags=" + flags);
+      if (flags == 3) {
+        // This point is both an end and a start; we need to
+        // separate it:
+        if (v > prev) {
+          elementaryIntervals.add(new InclusiveRange(prev, v-1));
+        }
+        elementaryIntervals.add(new InclusiveRange(v, v));
+        prev = v+1;
+      } else if (flags == 1) {
+        // This point is only the start of an interval;
+        // attach it to next interval:
+        if (v > prev) {
+          elementaryIntervals.add(new InclusiveRange(prev, v-1));
+        }
+        prev = v;
+      } else {
+        assert flags == 2;
+        // This point is only the end of an interval; attach
+        // it to last interval:
+        elementaryIntervals.add(new InclusiveRange(prev, v));
+        prev = v+1;
+      }
+      //System.out.println("    ints=" + elementaryIntervals);
+      upto0++;
+    }
+
+    // Build binary tree on top of intervals:
+    root = split(0, elementaryIntervals.size(), elementaryIntervals);
+
+    // Set outputs, so we know which range to output for
+    // each node in the tree:
+    for(int i=0;i<ranges.length;i++) {
+      root.addOutputs(i, ranges[i]);
+    }
+
+    // Set boundaries (ends of each elementary interval):
+    boundaries = new long[elementaryIntervals.size()];
+    for(int i=0;i<boundaries.length;i++) {
+      boundaries[i] = elementaryIntervals.get(i).end;
+    }
+
+    leafCounts = new int[boundaries.length];
+
+    //System.out.println("ranges: " + Arrays.toString(ranges));
+    //System.out.println("intervals: " + elementaryIntervals);
+    //System.out.println("boundaries: " + Arrays.toString(boundaries));
+    //System.out.println("root:\n" + root);
+  }
+
+  public void add(long v) {
+    //System.out.println("add v=" + v);
+
+    // NOTE: this works too, but it's ~6% slower on a simple
+    // test with a high-freq TermQuery w/ range faceting on
+    // wikimediumall:
+    /*
+    int index = Arrays.binarySearch(boundaries, v);
+    if (index < 0) {
+      index = -index-1;
+    }
+    leafCounts[index]++;
+    */
+
+    // Binary search to find matched elementary range; we
+    // are guaranteed to find a match because the last
+    // boundary is Long.MAX_VALUE:
+
+    int lo = 0;
+    int hi = boundaries.length - 1;
+    while (true) {
+      int mid = (lo + hi) >>> 1;
+      //System.out.println("  cycle lo=" + lo + " hi=" + hi + " mid=" + mid + " boundary=" + boundaries[mid] + " to " + boundaries[mid+1]);
+      if (v <= boundaries[mid]) {
+        if (mid == 0) {
+          leafCounts[0]++;
+          return;
+        } else {
+          hi = mid - 1;
+        }
+      } else if (v > boundaries[mid+1]) {
+        lo = mid + 1;
+      } else {
+        leafCounts[mid+1]++;
+        //System.out.println("  incr @ " + (mid+1) + "; now " + leafCounts[mid+1]);
+        return;
+      }
+    }
+  }
+
+  /** Fills counts corresponding to the original input
+   *  ranges, returning the missing count (how many hits
+   *  didn't match any ranges). */
+  public int fillCounts(int[] counts) {
+    //System.out.println("  rollup");
+    missingCount = 0;
+    leafUpto = 0;
+    rollup(root, counts, false);
+    return missingCount;
+  }
+
+  private int rollup(LongRangeNode node, int[] counts, boolean sawOutputs) {
+    int count;
+    sawOutputs |= node.outputs != null;
+    if (node.left != null) {
+      count = rollup(node.left, counts, sawOutputs);
+      count += rollup(node.right, counts, sawOutputs);
+    } else {
+      // Leaf:
+      count = leafCounts[leafUpto];
+      leafUpto++;
+      if (!sawOutputs) {
+        // This is a missing count (no output ranges were
+        // seen "above" us):
+        missingCount += count;
+      }
+    }
+    if (node.outputs != null) {
+      for(int rangeIndex : node.outputs) {
+        counts[rangeIndex] += count;
+      }
+    }
+    //System.out.println("  rollup node=" + node.start + " to " + node.end + ": count=" + count);
+    return count;
+  }
+
+  private static LongRangeNode split(int start, int end, List<InclusiveRange> elementaryIntervals) {
+    if (start == end-1) {
+      // leaf
+      InclusiveRange range = elementaryIntervals.get(start);
+      return new LongRangeNode(range.start, range.end, null, null, start);
+    } else {
+      int mid = (start + end) >>> 1;
+      LongRangeNode left = split(start, mid, elementaryIntervals);
+      LongRangeNode right = split(mid, end, elementaryIntervals);
+      return new LongRangeNode(left.start, right.end, left, right, -1);
+    }
+  }
+
+  private static final class InclusiveRange {
+    public final long start;
+    public final long end;
+
+    public InclusiveRange(long start, long end) {
+      assert end >= start;
+      this.start = start;
+      this.end = end;
+    }
+
+    @Override
+    public String toString() {
+      return start + " to " + end;
+    }
+  }
+
+  /** Holds one node of the segment tree. */
+  public static final class LongRangeNode {
+    final LongRangeNode left;
+    final LongRangeNode right;
+
+    // Our range, inclusive:
+    final long start;
+    final long end;
+
+    // If we are a leaf, the index into elementary ranges that
+    // we point to:
+    final int leafIndex;
+
+    // Which range indices to output when a query goes
+    // through this node:
+    List<Integer> outputs;
+
+    public LongRangeNode(long start, long end, LongRangeNode left, LongRangeNode right, int leafIndex) {
+      this.start = start;
+      this.end = end;
+      this.left = left;
+      this.right = right;
+      this.leafIndex = leafIndex;
+    }
+
+    @Override
+    public String toString() {
+      StringBuilder sb = new StringBuilder();
+      toString(sb, 0);
+      return sb.toString();
+    }
+
+    static void indent(StringBuilder sb, int depth) {
+      for(int i=0;i<depth;i++) {
+        sb.append("  ");
+      }
+    }
+
+    /** Recursively assigns range outputs to each node. */
+    void addOutputs(int index, LongRange range) {
+      if (start >= range.minIncl && end <= range.maxIncl) {
+        // Our range is fully included in the incoming
+        // range; add to our output list:
+        if (outputs == null) {
+          outputs = new ArrayList<Integer>();
+        }
+        outputs.add(index);
+      } else if (left != null) {
+        assert right != null;
+        // Recurse:
+        left.addOutputs(index, range);
+        right.addOutputs(index, range);
+      }
+    }
+
+    void toString(StringBuilder sb, int depth) {
+      indent(sb, depth);
+      if (left == null) {
+        assert right == null;
+        sb.append("leaf: " + start + " to " + end);
+      } else {
+        sb.append("node: " + start + " to " + end);
+      }
+      if (outputs != null) {
+        sb.append(" outputs=");
+        sb.append(outputs);
+      }
+      sb.append('\n');
+
+      if (left != null) {
+        assert right != null;
+        left.toString(sb, depth+1);
+        right.toString(sb, depth+1);
+      }
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/LongRangeFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/range/LongRangeFacetCounts.java
new file mode 100644
index 0000000..6e8d6d6
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/range/LongRangeFacetCounts.java
@@ -0,0 +1,95 @@
+package org.apache.lucene.facet.range;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.valuesource.LongFieldSource;
+
+/** {@link Facets} implementation that computes counts for
+ *  dynamic long ranges from a provided {@link ValueSource},
+ *  using {@link FunctionValues#longVal}.  Use
+ *  this for dimensions that change in real-time (e.g. a
+ *  relative time based dimension like "Past day", "Past 2
+ *  days", etc.) or that change for each request (e.g. 
+ *  distance from the user's location, "< 1 km", "< 2 km",
+ *  etc.).
+ *
+ *  @lucene.experimental */
+public class LongRangeFacetCounts extends RangeFacetCounts {
+
+  /** Create {@code LongRangeFacetCounts}, using {@link
+   *  LongFieldSource} from the specified field. */
+  public LongRangeFacetCounts(String field, FacetsCollector hits, LongRange... ranges) throws IOException {
+    this(field, new LongFieldSource(field), hits, ranges);
+  }
+
+  /** Create {@code RangeFacetCounts}, using the provided
+   *  {@link ValueSource}. */
+  public LongRangeFacetCounts(String field, ValueSource valueSource, FacetsCollector hits, LongRange... ranges) throws IOException {
+    super(field, ranges);
+    count(valueSource, hits.getMatchingDocs());
+  }
+
+  private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
+
+    LongRange[] ranges = (LongRange[]) this.ranges;
+
+    // Compute min & max over all ranges:
+    long minIncl = Long.MAX_VALUE;
+    long maxIncl = Long.MIN_VALUE;
+    for(LongRange range : ranges) {
+      minIncl = Math.min(minIncl, range.minIncl);
+      maxIncl = Math.max(maxIncl, range.maxIncl);
+    }
+
+    LongRangeCounter counter = new LongRangeCounter(ranges);
+
+    int missingCount = 0;
+    for (MatchingDocs hits : matchingDocs) {
+      FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
+      final int length = hits.bits.length();
+      int doc = 0;
+      totCount += hits.totalHits;
+      while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
+        // Skip missing docs:
+        if (fv.exists(doc)) {
+          counter.add(fv.longVal(doc));
+        } else {
+          missingCount++;
+        }
+
+        doc++;
+      }
+    }
+    
+    int x = counter.fillCounts(counts);
+
+    missingCount += x;
+
+    //System.out.println("totCount " + totCount + " missingCount " + counter.missingCount);
+    totCount -= missingCount;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/Range.java b/lucene/facet/src/java/org/apache/lucene/facet/range/Range.java
new file mode 100644
index 0000000..2e191aa
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/range/Range.java
@@ -0,0 +1,36 @@
+package org.apache.lucene.facet.range;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Base class for a single labeled range.
+ *
+ *  @lucene.experimental */
+public abstract class Range {
+  public final String label;
+
+  protected Range(String label) {
+    if (label == null) {
+      throw new NullPointerException("label cannot be null");
+    }
+    this.label = label;
+  }
+
+  protected void failNoMatch() {
+    throw new IllegalArgumentException("range \"" + label + "\" matches nothing");
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetCounts.java
new file mode 100644
index 0000000..39257a7
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetCounts.java
@@ -0,0 +1,71 @@
+package org.apache.lucene.facet.range;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.LabelAndValue;
+import org.apache.lucene.queries.function.valuesource.LongFieldSource;
+
+/** Base class for range faceting.
+ *
+ *  @lucene.experimental */
+abstract class RangeFacetCounts extends Facets {
+  protected final Range[] ranges;
+  protected final int[] counts;
+  protected final String field;
+  protected int totCount;
+
+  /** Create {@code RangeFacetCounts}, using {@link
+   *  LongFieldSource} from the specified field. */
+  protected RangeFacetCounts(String field, Range[] ranges) throws IOException {
+    this.field = field;
+    this.ranges = ranges;
+    counts = new int[ranges.length];
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) {
+    if (dim.equals(field) == false) {
+      throw new IllegalArgumentException("invalid dim \"" + dim + "\"; should be \"" + field + "\"");
+    }
+    if (path.length != 0) {
+      throw new IllegalArgumentException("path.length should be 0");
+    }
+    LabelAndValue[] labelValues = new LabelAndValue[counts.length];
+    for(int i=0;i<counts.length;i++) {
+      labelValues[i] = new LabelAndValue(ranges[i].label, counts[i]);
+    }
+    return new FacetResult(dim, path, totCount, labelValues, labelValues.length);
+  }
+
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    // TODO: should we impl this?
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public List<FacetResult> getAllDims(int topN) throws IOException {
+    return Collections.singletonList(getTopChildren(topN, null));
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/package.html b/lucene/facet/src/java/org/apache/lucene/facet/range/package.html
new file mode 100644
index 0000000..fc2ba10
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/range/package.html
@@ -0,0 +1,24 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+<title>Range Facets</title>
+</head>
+<body>
+Provides range faceting capabilities. 
+</body>
+</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java
new file mode 100644
index 0000000..7491817
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetCounts.java
@@ -0,0 +1,297 @@
+package org.apache.lucene.facet.sortedset;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.LabelAndValue;
+import org.apache.lucene.facet.TopOrdAndIntQueue;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState.OrdRange;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
+import org.apache.lucene.index.ReaderUtil;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.util.BytesRef;
+
+/** Compute facets counts from previously
+ *  indexed {@link SortedSetDocValuesFacetField},
+ *  without require a separate taxonomy index.  Faceting is
+ *  a bit slower (~25%), and there is added cost on every
+ *  {@link IndexReader} open to create a new {@link
+ *  SortedSetDocValuesReaderState}.  Furthermore, this does
+ *  not support hierarchical facets; only flat (dimension +
+ *  label) facets, but it uses quite a bit less RAM to do
+ *  so.
+ *
+ *  <p><b>NOTE</b>: this class should be instantiated and
+ *  then used from a single thread, because it holds a
+ *  thread-private instance of {@link SortedSetDocValues}.
+ * 
+ * <p><b>NOTE:<b/>: tie-break is by unicode sort order
+ *
+ * @lucene.experimental */
+public class SortedSetDocValuesFacetCounts extends Facets {
+
+  final SortedSetDocValuesReaderState state;
+  final SortedSetDocValues dv;
+  final String field;
+  final int[] counts;
+
+  /** Sparse faceting: returns any dimension that had any
+   *  hits, topCount labels per dimension. */
+  public SortedSetDocValuesFacetCounts(SortedSetDocValuesReaderState state, FacetsCollector hits)
+      throws IOException {
+    this.state = state;
+    this.field = state.getField();
+    counts = new int[state.getSize()];
+    dv = state.getDocValues();
+    //System.out.println("field=" + field);
+    count(hits.getMatchingDocs());
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    if (path.length > 0) {
+      throw new IllegalArgumentException("path should be 0 length");
+    }
+    OrdRange ordRange = state.getOrdRange(dim);
+    if (ordRange == null) {
+      throw new IllegalArgumentException("dimension \"" + dim + "\" was not indexed");
+    }
+    return getDim(dim, ordRange, topN);
+  }
+
+  private final FacetResult getDim(String dim, OrdRange ordRange, int topN) {
+
+    TopOrdAndIntQueue q = null;
+
+    int bottomCount = 0;
+
+    int dimCount = 0;
+    int childCount = 0;
+
+    TopOrdAndIntQueue.OrdAndValue reuse = null;
+    //System.out.println("getDim : " + ordRange.start + " - " + ordRange.end);
+    for(int ord=ordRange.start; ord<=ordRange.end; ord++) {
+      //System.out.println("  ord=" + ord + " count=" + counts[ord]);
+      if (counts[ord] > 0) {
+        dimCount += counts[ord];
+        childCount++;
+        if (counts[ord] > bottomCount) {
+          if (reuse == null) {
+            reuse = new TopOrdAndIntQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = counts[ord];
+          if (q == null) {
+            // Lazy init, so we don't create this for the
+            // sparse case unnecessarily
+            q = new TopOrdAndIntQueue(topN);
+          }
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomCount = q.top().value;
+          }
+        }
+      }
+    }
+
+    if (q == null) {
+      return null;
+    }
+
+    BytesRef scratch = new BytesRef();
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
+      dv.lookupOrd(ordAndValue.ord, scratch);
+      String[] parts = FacetsConfig.stringToPath(scratch.utf8ToString());
+      labelValues[i] = new LabelAndValue(parts[1], ordAndValue.value);
+    }
+
+    return new FacetResult(dim, new String[0], dimCount, labelValues, childCount);
+  }
+
+  /** Does all the "real work" of tallying up the counts. */
+  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("ssdv count");
+
+    MultiDocValues.OrdinalMap ordinalMap;
+
+    // TODO: is this right?  really, we need a way to
+    // verify that this ordinalMap "matches" the leaves in
+    // matchingDocs...
+    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {
+      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;
+    } else {
+      ordinalMap = null;
+    }
+
+    for(MatchingDocs hits : matchingDocs) {
+
+      AtomicReader reader = hits.context.reader();
+      //System.out.println("  reader=" + reader);
+      // LUCENE-5090: make sure the provided reader context "matches"
+      // the top-level reader passed to the
+      // SortedSetDocValuesReaderState, else cryptic
+      // AIOOBE can happen:
+      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {
+        throw new IllegalStateException("the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader");
+      }
+      
+      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);
+      if (segValues == null) {
+        continue;
+      }
+
+      final int maxDoc = reader.maxDoc();
+      assert maxDoc == hits.bits.length();
+      //System.out.println("  dv=" + dv);
+
+      // TODO: yet another option is to count all segs
+      // first, only in seg-ord space, and then do a
+      // merge-sort-PQ in the end to only "resolve to
+      // global" those seg ords that can compete, if we know
+      // we just want top K?  ie, this is the same algo
+      // that'd be used for merging facets across shards
+      // (distributed faceting).  but this has much higher
+      // temp ram req'ts (sum of number of ords across all
+      // segs)
+      if (ordinalMap != null) {
+        int segOrd = hits.context.ord;
+
+        int numSegOrds = (int) segValues.getValueCount();
+
+        if (hits.totalHits < numSegOrds/10) {
+          //System.out.println("    remap as-we-go");
+          // Remap every ord to global ord as we iterate:
+          int doc = 0;
+          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
+            //System.out.println("    doc=" + doc);
+            segValues.setDocument(doc);
+            int term = (int) segValues.nextOrd();
+            while (term != SortedSetDocValues.NO_MORE_ORDS) {
+              //System.out.println("      segOrd=" + segOrd + " ord=" + term + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, term));
+              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;
+              term = (int) segValues.nextOrd();
+            }
+            ++doc;
+          }
+        } else {
+          //System.out.println("    count in seg ord first");
+
+          // First count in seg-ord space:
+          final int[] segCounts = new int[numSegOrds];
+          int doc = 0;
+          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
+            //System.out.println("    doc=" + doc);
+            segValues.setDocument(doc);
+            int term = (int) segValues.nextOrd();
+            while (term != SortedSetDocValues.NO_MORE_ORDS) {
+              //System.out.println("      ord=" + term);
+              segCounts[term]++;
+              term = (int) segValues.nextOrd();
+            }
+            ++doc;
+          }
+
+          // Then, migrate to global ords:
+          for(int ord=0;ord<numSegOrds;ord++) {
+            int count = segCounts[ord];
+            if (count != 0) {
+              //System.out.println("    migrate segOrd=" + segOrd + " ord=" + ord + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, ord));
+              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;
+            }
+          }
+        }
+      } else {
+        // No ord mapping (e.g., single segment index):
+        // just aggregate directly into counts:
+
+        int doc = 0;
+        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
+          segValues.setDocument(doc);
+          int term = (int) segValues.nextOrd();
+          while (term != SortedSetDocValues.NO_MORE_ORDS) {
+            counts[term]++;
+            term = (int) segValues.nextOrd();
+          }
+          ++doc;
+        }
+      }
+    }
+  }
+
+  @Override
+  public Number getSpecificValue(String dim, String... path) {
+    if (path.length != 1) {
+      throw new IllegalArgumentException("path must be length=1");
+    }
+    int ord = (int) dv.lookupTerm(new BytesRef(FacetsConfig.pathToString(dim, path)));
+    if (ord < 0) {
+      return -1;
+    }
+
+    return counts[ord];
+  }
+
+  @Override
+  public List<FacetResult> getAllDims(int topN) throws IOException {
+
+    List<FacetResult> results = new ArrayList<FacetResult>();
+    for(Map.Entry<String,OrdRange> ent : state.getPrefixToOrdRange().entrySet()) {
+      FacetResult fr = getDim(ent.getKey(), ent.getValue(), topN);
+      if (fr != null) {
+        results.add(fr);
+      }
+    }
+
+    // Sort by highest count:
+    Collections.sort(results,
+                     new Comparator<FacetResult>() {
+                       @Override
+                       public int compare(FacetResult a, FacetResult b) {
+                         if (a.value.intValue() > b.value.intValue()) {
+                           return -1;
+                         } else if (b.value.intValue() > a.value.intValue()) {
+                           return 1;
+                         } else {
+                           return a.dim.compareTo(b.dim);
+                         }
+                       }
+                     });
+
+    return results;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetField.java
new file mode 100644
index 0000000..9bdf243
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesFacetField.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.facet.sortedset;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+
+/** Add an instance of this to your Document for every facet
+ *  label to be indexed via SortedSetDocValues. */
+public class SortedSetDocValuesFacetField extends Field {
+  
+  /** Indexed {@link FieldType}. */
+  public static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setIndexed(true);
+    TYPE.freeze();
+  }
+  
+  public final String dim;
+  public final String label;
+
+  public SortedSetDocValuesFacetField(String dim, String label) {
+    super("dummy", TYPE);
+    this.dim = dim;
+    this.label = label;
+  }
+
+  @Override
+  public String toString() {
+    return "SortedSetDocValuesFacetField(dim=" + dim + " label=" + label + ")";
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesReaderState.java b/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesReaderState.java
new file mode 100644
index 0000000..442d778
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesReaderState.java
@@ -0,0 +1,148 @@
+package org.apache.lucene.facet.sortedset;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.util.BytesRef;
+
+/** Wraps a {@link IndexReader} and resolves ords
+ *  using existing {@link SortedSetDocValues} APIs without a
+ *  separate taxonomy index.  This only supports flat facets
+ *  (dimension + label), and it makes faceting a bit
+ *  slower, adds some cost at reopen time, but avoids
+ *  managing the separate taxonomy index.  It also requires
+ *  less RAM than the taxonomy index, as it manages the flat
+ *  (2-level) hierarchy more efficiently.  In addition, the
+ *  tie-break during faceting is now meaningful (in label
+ *  sorted order).
+ *
+ *  <p><b>NOTE</b>: creating an instance of this class is
+ *  somewhat costly, as it computes per-segment ordinal maps,
+ *  so you should create it once and re-use that one instance
+ *  for a given {@link IndexReader}. */
+
+public final class SortedSetDocValuesReaderState {
+
+  private final String field;
+  private final AtomicReader topReader;
+  private final int valueCount;
+  public final IndexReader origReader;
+
+  /** Holds start/end range of ords, which maps to one
+   *  dimension (someday we may generalize it to map to
+   *  hierarchies within one dimension). */
+  public static final class OrdRange {
+    /** Start of range, inclusive: */
+    public final int start;
+    /** End of range, inclusive: */
+    public final int end;
+
+    /** Start and end are inclusive. */
+    public OrdRange(int start, int end) {
+      this.start = start;
+      this.end = end;
+    }
+  }
+
+  private final Map<String,OrdRange> prefixToOrdRange = new HashMap<String,OrdRange>();
+
+  /** Creates this, pulling doc values from the default {@link
+   *  FacetsConfig#DEFAULT_INDEX_FIELD_NAME}. */ 
+  public SortedSetDocValuesReaderState(IndexReader reader) throws IOException {
+    this(reader, FacetsConfig.DEFAULT_INDEX_FIELD_NAME);
+  }
+
+  /** Creates this, pulling doc values from the specified
+   *  field. */
+  public SortedSetDocValuesReaderState(IndexReader reader, String field) throws IOException {
+
+    this.field = field;
+    this.origReader = reader;
+
+    // We need this to create thread-safe MultiSortedSetDV
+    // per collector:
+    topReader = SlowCompositeReaderWrapper.wrap(reader);
+    SortedSetDocValues dv = topReader.getSortedSetDocValues(field);
+    if (dv == null) {
+      throw new IllegalArgumentException("field \"" + field + "\" was not indexed with SortedSetDocValues");
+    }
+    if (dv.getValueCount() > Integer.MAX_VALUE) {
+      throw new IllegalArgumentException("can only handle valueCount < Integer.MAX_VALUE; got " + dv.getValueCount());
+    }
+    valueCount = (int) dv.getValueCount();
+
+    // TODO: we can make this more efficient if eg we can be
+    // "involved" when OrdinalMap is being created?  Ie see
+    // each term/ord it's assigning as it goes...
+    String lastDim = null;
+    int startOrd = -1;
+    BytesRef spare = new BytesRef();
+
+    // TODO: this approach can work for full hierarchy?;
+    // TaxoReader can't do this since ords are not in
+    // "sorted order" ... but we should generalize this to
+    // support arbitrary hierarchy:
+    for(int ord=0;ord<valueCount;ord++) {
+      dv.lookupOrd(ord, spare);
+      String[] components = FacetsConfig.stringToPath(spare.utf8ToString());
+      if (components.length != 2) {
+        throw new IllegalArgumentException("this class can only handle 2 level hierarchy (dim/value); got: " + Arrays.toString(components) + " " + spare.utf8ToString());
+      }
+      if (!components[0].equals(lastDim)) {
+        if (lastDim != null) {
+          prefixToOrdRange.put(lastDim, new OrdRange(startOrd, ord-1));
+        }
+        startOrd = ord;
+        lastDim = components[0];
+      }
+    }
+
+    if (lastDim != null) {
+      prefixToOrdRange.put(lastDim, new OrdRange(startOrd, valueCount-1));
+    }
+  }
+
+  public SortedSetDocValues getDocValues() throws IOException {
+    return topReader.getSortedSetDocValues(field);
+  }
+
+  public Map<String,OrdRange> getPrefixToOrdRange() {
+    return prefixToOrdRange;
+  }
+
+  public OrdRange getOrdRange(String dim) {
+    return prefixToOrdRange.get(dim);
+  }
+
+  public String getField() {
+    return field;
+  }
+
+  public int getSize() {
+    return valueCount;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sortedset/package.html b/lucene/facet/src/java/org/apache/lucene/facet/sortedset/package.html
new file mode 100644
index 0000000..08a4363
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sortedset/package.html
@@ -0,0 +1,25 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+<title>SortedSet Facets</title>
+</head>
+<body>
+Provides faceting capabilities over facets that were indexed with {@link org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetField}.
+</body>
+
+</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/AssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/AssociationFacetField.java
new file mode 100644
index 0000000..b217c36
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/AssociationFacetField.java
@@ -0,0 +1,66 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Document; // javadocs
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.util.BytesRef;
+
+/** Add an instance of this to your {@link Document} to add
+ *  a facet label associated with an arbitrary byte[].
+ *  This will require a custom {@link Facets}
+ *  implementation at search time; see {@link
+ *  IntAssociationFacetField} and {@link
+ *  FloatAssociationFacetField} to use existing {@link
+ *  Facets} implementations.
+ * 
+ *  @lucene.experimental */
+public class AssociationFacetField extends Field {
+  
+  /** Indexed {@link FieldType}. */
+  public static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setIndexed(true);
+    TYPE.freeze();
+  }
+  
+  public final String dim;
+  public final String[] path;
+  public final BytesRef assoc;
+
+  /** Creates this from {@code dim} and {@code path} and an
+   *  association */
+  public AssociationFacetField(BytesRef assoc, String dim, String... path) {
+    super("dummy", TYPE);
+    this.dim = dim;
+    this.assoc = assoc;
+    if (path.length == 0) {
+      throw new IllegalArgumentException("path must have at least one element");
+    }
+    this.path = path;
+  }
+
+  @Override
+  public String toString() {
+    return "AssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " bytes=" + assoc + ")";
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FastTaxonomyFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FastTaxonomyFacetCounts.java
new file mode 100644
index 0000000..bfe276d
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FastTaxonomyFacetCounts.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+/** Computes facets counts, assuming the default encoding
+ *  into DocValues was used.
+ *
+ * @lucene.experimental */
+public class FastTaxonomyFacetCounts extends IntTaxonomyFacets {
+
+  /** Create {@code FastTaxonomyFacetCounts}, which also
+   *  counts all facet labels. */
+  public FastTaxonomyFacetCounts(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
+  }
+
+  /** Create {@code FastTaxonomyFacetCounts}, using the
+   *  specified {@code indexFieldName} for ordinals.  Use
+   *  this if you had set {@link
+   *  FacetsConfig#setIndexFieldName} to change the index
+   *  field name for certain dimensions. */
+  public FastTaxonomyFacetCounts(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    count(fc.getMatchingDocs());
+  }
+
+  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
+    for(MatchingDocs hits : matchingDocs) {
+      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
+      if (dv == null) { // this reader does not have DocValues for the requested category list
+        continue;
+      }
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      BytesRef scratch = new BytesRef();
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        dv.get(doc, scratch);
+        byte[] bytes = scratch.bytes;
+        int end = scratch.offset + scratch.length;
+        int ord = 0;
+        int offset = scratch.offset;
+        int prev = 0;
+        while (offset < end) {
+          byte b = bytes[offset++];
+          if (b >= 0) {
+            prev = ord = ((ord << 7) | b) + prev;
+            ++values[ord];
+            ord = 0;
+          } else {
+            ord = (ord << 7) | (b & 0x7F);
+          }
+        }
+        ++doc;
+      }
+    }
+
+    rollup();
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FloatAssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FloatAssociationFacetField.java
new file mode 100644
index 0000000..d89b49e
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FloatAssociationFacetField.java
@@ -0,0 +1,53 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.util.BytesRef;
+
+/** Add an instance of this to your {@link Document} to add
+ *  a facet label associated with a float.  Use {@link
+ *  TaxonomyFacetSumFloatAssociations} to aggregate float values
+ *  per facet label at search time.
+ * 
+ *  @lucene.experimental */
+public class FloatAssociationFacetField extends AssociationFacetField {
+
+  /** Creates this from {@code dim} and {@code path} and a
+   *  float association */
+  public FloatAssociationFacetField(float assoc, String dim, String... path) {
+    super(floatToBytesRef(assoc), dim, path);
+  }
+
+  /** Encodes a {@code float} as a 4-byte {@link BytesRef}. */
+  public static BytesRef floatToBytesRef(float v) {
+    return IntAssociationFacetField.intToBytesRef(Float.floatToIntBits(v));
+  }
+
+  /** Decodes a previously encoded {@code float}. */
+  public static float bytesRefToFloat(BytesRef b) {
+    return Float.intBitsToFloat(IntAssociationFacetField.bytesRefToInt(b));
+  }
+
+  @Override
+  public String toString() {
+    return "FloatAssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " value=" + bytesRefToFloat(assoc) + ")";
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FloatTaxonomyFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FloatTaxonomyFacets.java
new file mode 100644
index 0000000..db4413c
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FloatTaxonomyFacets.java
@@ -0,0 +1,147 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FacetsConfig.DimConfig;
+import org.apache.lucene.facet.LabelAndValue;
+import org.apache.lucene.facet.TopOrdAndFloatQueue;
+
+/** Base class for all taxonomy-based facets that aggregate
+ *  to a per-ords float[]. */
+public abstract class FloatTaxonomyFacets extends TaxonomyFacets {
+
+  protected final float[] values;
+
+  protected FloatTaxonomyFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    values = new float[taxoReader.getSize()];
+  }
+  
+  protected void rollup() throws IOException {
+    // Rollup any necessary dims:
+    for(Map.Entry<String,DimConfig> ent : config.getDimConfigs().entrySet()) {
+      String dim = ent.getKey();
+      DimConfig ft = ent.getValue();
+      if (ft.hierarchical && ft.multiValued == false) {
+        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
+        assert dimRootOrd > 0;
+        values[dimRootOrd] += rollup(children[dimRootOrd]);
+      }
+    }
+  }
+
+  private float rollup(int ord) {
+    float sum = 0;
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      float childValue = values[ord] + rollup(children[ord]);
+      values[ord] = childValue;
+      sum += childValue;
+      ord = siblings[ord];
+    }
+    return sum;
+  }
+
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    DimConfig dimConfig = verifyDim(dim);
+    if (path.length == 0) {
+      if (dimConfig.hierarchical && dimConfig.multiValued == false) {
+        // ok: rolled up at search time
+      } else if (dimConfig.requireDimCount && dimConfig.multiValued) {
+        // ok: we indexed all ords at index time
+      } else {
+        throw new IllegalArgumentException("cannot return dimension-level value alone; use getTopChildren instead");
+      }
+    }
+    int ord = taxoReader.getOrdinal(new FacetLabel(dim, path));
+    if (ord < 0) {
+      return -1;
+    }
+    return values[ord];
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    DimConfig dimConfig = verifyDim(dim);
+    FacetLabel cp = new FacetLabel(dim, path);
+    int dimOrd = taxoReader.getOrdinal(cp);
+    if (dimOrd == -1) {
+      return null;
+    }
+
+    TopOrdAndFloatQueue q = new TopOrdAndFloatQueue(Math.min(taxoReader.getSize(), topN));
+    float bottomValue = 0;
+
+    int ord = children[dimOrd];
+    float sumValues = 0;
+    int childCount = 0;
+
+    TopOrdAndFloatQueue.OrdAndValue reuse = null;
+    while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      if (values[ord] > 0) {
+        sumValues += values[ord];
+        childCount++;
+        if (values[ord] > bottomValue) {
+          if (reuse == null) {
+            reuse = new TopOrdAndFloatQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = values[ord];
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomValue = q.top().value;
+          }
+        }
+      }
+
+      ord = siblings[ord];
+    }
+
+    if (sumValues == 0) {
+      return null;
+    }
+
+    if (dimConfig.multiValued) {
+      if (dimConfig.requireDimCount) {
+        sumValues = values[dimOrd];
+      } else {
+        // Our sum'd count is not correct, in general:
+        sumValues = -1;
+      }
+    } else {
+      // Our sum'd dim count is accurate, so we keep it
+    }
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndFloatQueue.OrdAndValue ordAndValue = q.pop();
+      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
+      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
+    }
+
+    return new FacetResult(dim, path, sumValues, labelValues, childCount);
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/IntAssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/IntAssociationFacetField.java
new file mode 100644
index 0000000..e2f953f
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/IntAssociationFacetField.java
@@ -0,0 +1,63 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.util.BytesRef;
+
+/** Add an instance of this to your {@link Document} to add
+ *  a facet label associated with an int.  Use {@link
+ *  TaxonomyFacetSumIntAssociations} to aggregate int values
+ *  per facet label at search time.
+ * 
+ *  @lucene.experimental */
+public class IntAssociationFacetField extends AssociationFacetField {
+
+  /** Creates this from {@code dim} and {@code path} and an
+   *  int association */
+  public IntAssociationFacetField(int assoc, String dim, String... path) {
+    super(intToBytesRef(assoc), dim, path);
+  }
+
+  /** Encodes an {@code int} as a 4-byte {@link BytesRef},
+   *  big-endian. */
+  public static BytesRef intToBytesRef(int v) {
+    byte[] bytes = new byte[4];
+    // big-endian:
+    bytes[0] = (byte) (v >> 24);
+    bytes[1] = (byte) (v >> 16);
+    bytes[2] = (byte) (v >> 8);
+    bytes[3] = (byte) v;
+    return new BytesRef(bytes);
+  }
+
+  /** Decodes a previously encoded {@code int}. */
+  public static int bytesRefToInt(BytesRef b) {
+    return ((b.bytes[b.offset]&0xFF) << 24) |
+      ((b.bytes[b.offset+1]&0xFF) << 16) |
+      ((b.bytes[b.offset+2]&0xFF) << 8) |
+      (b.bytes[b.offset+3]&0xFF);
+  }
+
+  @Override
+  public String toString() {
+    return "IntAssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " value=" + bytesRefToInt(assoc) + ")";
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/IntTaxonomyFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/IntTaxonomyFacets.java
new file mode 100644
index 0000000..498e638
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/IntTaxonomyFacets.java
@@ -0,0 +1,152 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FacetsConfig.DimConfig;
+import org.apache.lucene.facet.LabelAndValue;
+import org.apache.lucene.facet.TopOrdAndIntQueue;
+
+/** Base class for all taxonomy-based facets that aggregate
+ *  to a per-ords int[]. */
+
+public abstract class IntTaxonomyFacets extends TaxonomyFacets {
+
+  protected final int[] values;
+
+  protected IntTaxonomyFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    values = new int[taxoReader.getSize()];
+  }
+  
+  protected void rollup() throws IOException {
+    // Rollup any necessary dims:
+    for(Map.Entry<String,DimConfig> ent : config.getDimConfigs().entrySet()) {
+      String dim = ent.getKey();
+      DimConfig ft = ent.getValue();
+      if (ft.hierarchical && ft.multiValued == false) {
+        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
+        // It can be -1 if this field was declared in the
+        // config but never indexed:
+        if (dimRootOrd > 0) {
+          values[dimRootOrd] += rollup(children[dimRootOrd]);
+        }
+      }
+    }
+  }
+
+  private int rollup(int ord) {
+    int sum = 0;
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      int childValue = values[ord] + rollup(children[ord]);
+      values[ord] = childValue;
+      sum += childValue;
+      ord = siblings[ord];
+    }
+    return sum;
+  }
+
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    DimConfig dimConfig = verifyDim(dim);
+    if (path.length == 0) {
+      if (dimConfig.hierarchical && dimConfig.multiValued == false) {
+        // ok: rolled up at search time
+      } else if (dimConfig.requireDimCount && dimConfig.multiValued) {
+        // ok: we indexed all ords at index time
+      } else {
+        throw new IllegalArgumentException("cannot return dimension-level value alone; use getTopChildren instead");
+      }
+    }
+    int ord = taxoReader.getOrdinal(new FacetLabel(dim, path));
+    if (ord < 0) {
+      return -1;
+    }
+    return values[ord];
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    DimConfig dimConfig = verifyDim(dim);
+    FacetLabel cp = new FacetLabel(dim, path);
+    int dimOrd = taxoReader.getOrdinal(cp);
+    if (dimOrd == -1) {
+      return null;
+    }
+
+    TopOrdAndIntQueue q = new TopOrdAndIntQueue(Math.min(taxoReader.getSize(), topN));
+    
+    int bottomValue = 0;
+
+    int ord = children[dimOrd];
+    int totValue = 0;
+    int childCount = 0;
+
+    TopOrdAndIntQueue.OrdAndValue reuse = null;
+    while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      if (values[ord] > 0) {
+        totValue += values[ord];
+        childCount++;
+        if (values[ord] > bottomValue) {
+          if (reuse == null) {
+            reuse = new TopOrdAndIntQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = values[ord];
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomValue = q.top().value;
+          }
+        }
+      }
+
+      ord = siblings[ord];
+    }
+
+    if (totValue == 0) {
+      return null;
+    }
+
+    if (dimConfig.multiValued) {
+      if (dimConfig.requireDimCount) {
+        totValue = values[dimOrd];
+      } else {
+        // Our sum'd value is not correct, in general:
+        totValue = -1;
+      }
+    } else {
+      // Our sum'd dim value is accurate, so we keep it
+    }
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
+      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
+      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
+    }
+
+    return new FacetResult(dim, path, totValue, labelValues, childCount);
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/SearcherTaxonomyManager.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/SearcherTaxonomyManager.java
new file mode 100644
index 0000000..bb868ff
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/SearcherTaxonomyManager.java
@@ -0,0 +1,122 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.ReferenceManager;
+import org.apache.lucene.search.SearcherFactory;
+import org.apache.lucene.search.SearcherManager;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Manages near-real-time reopen of both an IndexSearcher
+ * and a TaxonomyReader.
+ *
+ * <p><b>NOTE</b>: If you call {@link
+ * DirectoryTaxonomyWriter#replaceTaxonomy} then you must
+ * open a new {@code SearcherTaxonomyManager} afterwards.
+ */
+public class SearcherTaxonomyManager extends ReferenceManager<SearcherTaxonomyManager.SearcherAndTaxonomy> {
+
+  /** Holds a matched pair of {@link IndexSearcher} and
+   *  {@link TaxonomyReader} */
+  public static class SearcherAndTaxonomy {
+    public final IndexSearcher searcher;
+    public final DirectoryTaxonomyReader taxonomyReader;
+
+    /** Create a SearcherAndTaxonomy */
+    public SearcherAndTaxonomy(IndexSearcher searcher, DirectoryTaxonomyReader taxonomyReader) {
+      this.searcher = searcher;
+      this.taxonomyReader = taxonomyReader;
+    }
+  }
+
+  private final SearcherFactory searcherFactory;
+  private final long taxoEpoch;
+  private final DirectoryTaxonomyWriter taxoWriter;
+
+  /** Creates near-real-time searcher and taxonomy reader
+   *  from the corresponding writers. */
+  public SearcherTaxonomyManager(IndexWriter writer, boolean applyAllDeletes, SearcherFactory searcherFactory, DirectoryTaxonomyWriter taxoWriter) throws IOException {
+    if (searcherFactory == null) {
+      searcherFactory = new SearcherFactory();
+    }
+    this.searcherFactory = searcherFactory;
+    this.taxoWriter = taxoWriter;
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    current = new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, DirectoryReader.open(writer, applyAllDeletes)),
+                                      taxoReader);
+    taxoEpoch = taxoWriter.getTaxonomyEpoch();
+  }
+
+  @Override
+  protected void decRef(SearcherAndTaxonomy ref) throws IOException {
+    ref.searcher.getIndexReader().decRef();
+
+    // This decRef can fail, and then in theory we should
+    // tryIncRef the searcher to put back the ref count
+    // ... but 1) the below decRef should only fail because
+    // it decRef'd to 0 and closed and hit some IOException
+    // during close, in which case 2) very likely the
+    // searcher was also just closed by the above decRef and
+    // a tryIncRef would fail:
+    ref.taxonomyReader.decRef();
+  }
+
+  @Override
+  protected boolean tryIncRef(SearcherAndTaxonomy ref) throws IOException {
+    if (ref.searcher.getIndexReader().tryIncRef()) {
+      if (ref.taxonomyReader.tryIncRef()) {
+        return true;
+      } else {
+        ref.searcher.getIndexReader().decRef();
+      }
+    }
+    return false;
+  }
+
+  @Override
+  protected SearcherAndTaxonomy refreshIfNeeded(SearcherAndTaxonomy ref) throws IOException {
+    // Must re-open searcher first, otherwise we may get a
+    // new reader that references ords not yet known to the
+    // taxonomy reader:
+    final IndexReader r = ref.searcher.getIndexReader();
+    final IndexReader newReader = DirectoryReader.openIfChanged((DirectoryReader) r);
+    if (newReader == null) {
+      return null;
+    } else {
+      DirectoryTaxonomyReader tr = TaxonomyReader.openIfChanged(ref.taxonomyReader);
+      if (tr == null) {
+        ref.taxonomyReader.incRef();
+        tr = ref.taxonomyReader;
+      } else if (taxoWriter.getTaxonomyEpoch() != taxoEpoch) {
+        IOUtils.close(newReader, tr);
+        throw new IllegalStateException("DirectoryTaxonomyWriter.replaceTaxonomy was called, which is not allowed when using SearcherTaxonomyManager");
+      }
+
+      return new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, newReader), tr);
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetCounts.java
new file mode 100644
index 0000000..aceb5ce
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetCounts.java
@@ -0,0 +1,68 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.OrdinalsReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IntsRef;
+
+/** Reads from any {@link OrdinalsReader}; use {@link
+ *  FastTaxonomyFacetCounts} if you are using the
+ *  default encoding from {@link BinaryDocValues}.
+ * 
+ * @lucene.experimental */
+public class TaxonomyFacetCounts extends IntTaxonomyFacets {
+  private final OrdinalsReader ordinalsReader;
+
+  /** Create {@code TaxonomyFacetCounts}, which also
+   *  counts all facet labels.  Use this for a non-default
+   *  {@link OrdinalsReader}; otherwise use {@link
+   *  FastTaxonomyFacetCounts}. */
+  public TaxonomyFacetCounts(OrdinalsReader ordinalsReader, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(ordinalsReader.getIndexFieldName(), taxoReader, config);
+    this.ordinalsReader = ordinalsReader;
+    count(fc.getMatchingDocs());
+  }
+
+  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
+    IntsRef scratch  = new IntsRef();
+    for(MatchingDocs hits : matchingDocs) {
+      OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        ords.get(doc, scratch);
+        for(int i=0;i<scratch.length;i++) {
+          values[scratch.ints[scratch.offset+i]]++;
+        }
+        ++doc;
+      }
+    }
+
+    rollup();
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumFloatAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumFloatAssociations.java
new file mode 100644
index 0000000..b9ca10b
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumFloatAssociations.java
@@ -0,0 +1,90 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+/** Aggregates sum of int values previously indexed with
+ *  {@link FloatAssociationFacetField}, assuming the default
+ *  encoding.
+ *
+ *  @lucene.experimental */
+public class TaxonomyFacetSumFloatAssociations extends FloatTaxonomyFacets {
+
+  /** Create {@code TaxonomyFacetSumFloatAssociations} against
+   *  the default index field. */
+  public TaxonomyFacetSumFloatAssociations(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
+  }
+
+  /** Create {@code TaxonomyFacetSumFloatAssociations} against
+   *  the specified index field. */
+  public TaxonomyFacetSumFloatAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    sumValues(fc.getMatchingDocs());
+  }
+
+  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
+    for(MatchingDocs hits : matchingDocs) {
+      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
+      if (dv == null) { // this reader does not have DocValues for the requested category list
+        continue;
+      }
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      BytesRef scratch = new BytesRef();
+      //System.out.println("count seg=" + hits.context.reader());
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        //System.out.println("  doc=" + doc);
+        // TODO: use OrdinalsReader?  we'd need to add a
+        // BytesRef getAssociation()?
+        dv.get(doc, scratch);
+        byte[] bytes = scratch.bytes;
+        int end = scratch.offset + scratch.length;
+        int offset = scratch.offset;
+        while (offset < end) {
+          int ord = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          int value = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          values[ord] += Float.intBitsToFloat(value);
+        }
+        ++doc;
+      }
+    }
+
+    rollup();
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumIntAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumIntAssociations.java
new file mode 100644
index 0000000..801e6f6
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumIntAssociations.java
@@ -0,0 +1,90 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+/** Aggregates sum of int values previously indexed with
+ *  {@link IntAssociationFacetField}, assuming the default
+ *  encoding.
+ *
+ *  @lucene.experimental */
+public class TaxonomyFacetSumIntAssociations extends IntTaxonomyFacets {
+
+  /** Create {@code TaxonomyFacetSumIntAssociations} against
+   *  the default index field. */
+  public TaxonomyFacetSumIntAssociations(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
+  }
+
+  /** Create {@code TaxonomyFacetSumIntAssociations} against
+   *  the specified index field. */
+  public TaxonomyFacetSumIntAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    sumValues(fc.getMatchingDocs());
+  }
+
+  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
+    for(MatchingDocs hits : matchingDocs) {
+      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
+      if (dv == null) { // this reader does not have DocValues for the requested category list
+        continue;
+      }
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      BytesRef scratch = new BytesRef();
+      //System.out.println("count seg=" + hits.context.reader());
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        //System.out.println("  doc=" + doc);
+        // TODO: use OrdinalsReader?  we'd need to add a
+        // BytesRef getAssociation()?
+        dv.get(doc, scratch);
+        byte[] bytes = scratch.bytes;
+        int end = scratch.offset + scratch.length;
+        int offset = scratch.offset;
+        while (offset < end) {
+          int ord = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          int value = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          values[ord] += value;
+        }
+        ++doc;
+      }
+    }
+
+    rollup();
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumValueSource.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumValueSource.java
new file mode 100644
index 0000000..dca4d4c
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacetSumValueSource.java
@@ -0,0 +1,138 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.DocValuesOrdinalsReader;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.OrdinalsReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IntsRef;
+
+/** Aggregates sum of values from {@link
+ *  FunctionValues#doubleVal}, for each facet label.
+ *
+ *  @lucene.experimental */
+public class TaxonomyFacetSumValueSource extends FloatTaxonomyFacets {
+  private final OrdinalsReader ordinalsReader;
+
+  /** Aggreggates float facet values from the provided
+   *  {@link ValueSource}, pulling ordinals using {@link
+   *  DocValuesOrdinalsReader} against the default indexed
+   *  facet field {@link
+   *  FacetsConfig#DEFAULT_INDEX_FIELD_NAME}. */
+  public TaxonomyFacetSumValueSource(TaxonomyReader taxoReader, FacetsConfig config,
+                                     FacetsCollector fc, ValueSource valueSource) throws IOException {
+    this(new DocValuesOrdinalsReader(FacetsConfig.DEFAULT_INDEX_FIELD_NAME), taxoReader, config, fc, valueSource);
+  }
+
+  /** Aggreggates float facet values from the provided
+   *  {@link ValueSource}, and pulls ordinals from the
+   *  provided {@link OrdinalsReader}. */
+  public TaxonomyFacetSumValueSource(OrdinalsReader ordinalsReader, TaxonomyReader taxoReader,
+                                     FacetsConfig config, FacetsCollector fc, ValueSource valueSource) throws IOException {
+    super(ordinalsReader.getIndexFieldName(), taxoReader, config);
+    this.ordinalsReader = ordinalsReader;
+    sumValues(fc.getMatchingDocs(), fc.getKeepScores(), valueSource);
+  }
+
+  private static final class FakeScorer extends Scorer {
+    float score;
+    int docID;
+    FakeScorer() { super(null); }
+    @Override public float score() throws IOException { return score; }
+    @Override public int freq() throws IOException { throw new UnsupportedOperationException(); }
+    @Override public int docID() { return docID; }
+    @Override public int nextDoc() throws IOException { throw new UnsupportedOperationException(); }
+    @Override public int advance(int target) throws IOException { throw new UnsupportedOperationException(); }
+    @Override public long cost() { return 0; }
+  }
+
+  private final void sumValues(List<MatchingDocs> matchingDocs, boolean keepScores, ValueSource valueSource) throws IOException {
+    final FakeScorer scorer = new FakeScorer();
+    Map<String, Scorer> context = new HashMap<String, Scorer>();
+    if (keepScores) {
+      context.put("scorer", scorer);
+    }
+    IntsRef scratch = new IntsRef();
+    for(MatchingDocs hits : matchingDocs) {
+      OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      int scoresIdx = 0;
+      float[] scores = hits.scores;
+
+      FunctionValues functionValues = valueSource.getValues(context, hits.context);
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        ords.get(doc, scratch);
+        if (keepScores) {
+          scorer.docID = doc;
+          scorer.score = scores[scoresIdx++];
+        }
+        float value = (float) functionValues.doubleVal(doc);
+        for(int i=0;i<scratch.length;i++) {
+          values[scratch.ints[i]] += value;
+        }
+        ++doc;
+      }
+    }
+
+    rollup();
+  }
+
+  /** {@link ValueSource} that returns the score for each
+   *  hit; use this to aggregate the sum of all hit scores
+   *  for each facet label.  */
+  public static class ScoreValueSource extends ValueSource {
+    @Override
+    public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
+      final Scorer scorer = (Scorer) context.get("scorer");
+      if (scorer == null) {
+        throw new IllegalStateException("scores are missing; be sure to pass keepScores=true to FacetsCollector");
+      }
+      return new DoubleDocValues(this) {
+        @Override
+        public double doubleVal(int document) {
+          try {
+            return scorer.score();
+          } catch (IOException exception) {
+            throw new RuntimeException(exception);
+          }
+        }
+      };
+    }
+    
+    @Override public boolean equals(Object o) { return o == this; }
+    @Override public int hashCode() { return System.identityHashCode(this); }
+    @Override public String description() { return "score()"; }
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacets.java
new file mode 100644
index 0000000..c4ace0b
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/TaxonomyFacets.java
@@ -0,0 +1,90 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+
+/** Base class for all taxonomy-based facets impls. */
+public abstract class TaxonomyFacets extends Facets {
+
+  private static final Comparator<FacetResult> BY_VALUE_THEN_DIM = new Comparator<FacetResult>() {
+    @Override
+    public int compare(FacetResult a, FacetResult b) {
+      if (a.value.doubleValue() > b.value.doubleValue()) {
+        return -1;
+      } else if (b.value.doubleValue() > a.value.doubleValue()) {
+        return 1;
+      } else {
+        return a.dim.compareTo(b.dim);
+      }
+    }
+  };
+
+  protected final String indexFieldName;
+  protected final TaxonomyReader taxoReader;
+  protected final FacetsConfig config;
+  protected final int[] children;
+  protected final int[] siblings;
+
+  protected TaxonomyFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config) throws IOException {
+    this.indexFieldName = indexFieldName;
+    this.taxoReader = taxoReader;
+    this.config = config;
+    ParallelTaxonomyArrays pta = taxoReader.getParallelTaxonomyArrays();
+    children = pta.children();
+    siblings = pta.siblings();
+  }
+
+  protected FacetsConfig.DimConfig verifyDim(String dim) {
+    FacetsConfig.DimConfig dimConfig = config.getDimConfig(dim);
+    if (!dimConfig.indexFieldName.equals(indexFieldName)) {
+      throw new IllegalArgumentException("dimension \"" + dim + "\" was not indexed into field \"" + indexFieldName);
+    }
+    return dimConfig;
+  }
+
+  @Override
+  public List<FacetResult> getAllDims(int topN) throws IOException {
+    int ord = children[TaxonomyReader.ROOT_ORDINAL];
+    List<FacetResult> results = new ArrayList<FacetResult>();
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      String dim = taxoReader.getPath(ord).components[0];
+      FacetsConfig.DimConfig dimConfig = config.getDimConfig(dim);
+      if (dimConfig.indexFieldName.equals(indexFieldName)) {
+        FacetResult result = getTopChildren(topN, dim);
+        if (result != null) {
+          results.add(result);
+        }
+      }
+      ord = siblings[ord];
+    }
+
+    // Sort by highest value, tie break by dim:
+    Collections.sort(results, BY_VALUE_THEN_DIM);
+    return results;
+  }
+  
+}
\ No newline at end of file
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
index a691de0..4df86e5 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
@@ -25,8 +25,9 @@ import java.util.Comparator;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
-import java.util.Random;
 
+import org.apache.lucene.facet.taxonomy.FastTaxonomyFacetCounts;
+import org.apache.lucene.facet.taxonomy.TaxonomyFacetCounts;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.LuceneTestCase;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java b/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
index 200cf31..f966f91 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
@@ -32,6 +32,8 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.facet.DrillSideways.DrillSidewaysResult;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetField;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
@@ -624,7 +626,6 @@ public class TestDrillSideways extends FacetTestCase {
 
       for(int dim=0;dim<numDims;dim++) {
         if (drillDowns[dim] != null) {
-          int upto = 0;
           for(String value : drillDowns[dim]) {
             ddq.add("dim" + dim, value);
           }
@@ -786,6 +787,7 @@ public class TestDrillSideways extends FacetTestCase {
     List<Doc> hits;
     int[][] counts;
     int[] uniqueCounts;
+    public TestFacetResult() {}
   }
   
   private int[] getTopNOrds(final int[] counts, final String[] values, int topN) {
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestRangeFacetCounts.java b/lucene/facet/src/test/org/apache/lucene/facet/TestRangeFacetCounts.java
deleted file mode 100644
index 2b23ea5..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/TestRangeFacetCounts.java
+++ /dev/null
@@ -1,797 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
-import org.apache.lucene.document.DoubleField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.FloatField;
-import org.apache.lucene.document.LongField;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.facet.DrillSideways.DrillSidewaysResult;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
-import org.apache.lucene.queries.function.valuesource.FloatFieldSource;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-
-public class TestRangeFacetCounts extends FacetTestCase {
-
-  public void testBasicLong() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      field.setLongValue(l);
-      w.addDocument(doc);
-    }
-
-    // Also add Long.MAX_VALUE
-    field.setLongValue(Long.MAX_VALUE);
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-    w.close();
-
-    FacetsCollector fc = new FacetsCollector();
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-
-    Facets facets = new LongRangeFacetCounts("field", fc,
-        new LongRange("less than 10", 0L, true, 10L, false),
-        new LongRange("less than or equal to 10", 0L, true, 10L, true),
-        new LongRange("over 90", 90L, false, 100L, false),
-        new LongRange("90 or above", 90L, true, 100L, false),
-        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, true));
-
-    FacetResult result = facets.getTopChildren(10, "field");
-    assertEquals("dim=field path=[] value=22 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (1)\n",
-                 result.toString());
-    
-    r.close();
-    d.close();
-  }
-
-  public void testUselessRange() {
-    try {
-      new LongRange("useless", 7, true, 6, true);
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    try {
-      new LongRange("useless", 7, true, 7, false);
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    try {
-      new DoubleRange("useless", 7.0, true, 6.0, true);
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    try {
-      new DoubleRange("useless", 7.0, true, 7.0, false);
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-  }
-
-  public void testLongMinMax() throws Exception {
-
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
-    doc.add(field);
-    field.setLongValue(Long.MIN_VALUE);
-    w.addDocument(doc);
-    field.setLongValue(0);
-    w.addDocument(doc);
-    field.setLongValue(Long.MAX_VALUE);
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-    w.close();
-
-    FacetsCollector fc = new FacetsCollector();
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-
-    Facets facets = new LongRangeFacetCounts("field", fc,
-        new LongRange("min", Long.MIN_VALUE, true, Long.MIN_VALUE, true),
-        new LongRange("max", Long.MAX_VALUE, true, Long.MAX_VALUE, true),
-        new LongRange("all0", Long.MIN_VALUE, true, Long.MAX_VALUE, true),
-        new LongRange("all1", Long.MIN_VALUE, false, Long.MAX_VALUE, true),
-        new LongRange("all2", Long.MIN_VALUE, true, Long.MAX_VALUE, false),
-        new LongRange("all3", Long.MIN_VALUE, false, Long.MAX_VALUE, false));
-
-    FacetResult result = facets.getTopChildren(10, "field");
-    assertEquals("dim=field path=[] value=3 childCount=6\n  min (1)\n  max (1)\n  all0 (3)\n  all1 (2)\n  all2 (2)\n  all3 (1)\n",
-                 result.toString());
-    
-    r.close();
-    d.close();
-  }
-
-  public void testOverlappedEndStart() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      field.setLongValue(l);
-      w.addDocument(doc);
-    }
-    field.setLongValue(Long.MAX_VALUE);
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-    w.close();
-
-    FacetsCollector fc = new FacetsCollector();
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-
-    Facets facets = new LongRangeFacetCounts("field", fc,
-        new LongRange("0-10", 0L, true, 10L, true),
-        new LongRange("10-20", 10L, true, 20L, true),
-        new LongRange("20-30", 20L, true, 30L, true),
-        new LongRange("30-40", 30L, true, 40L, true));
-    
-    FacetResult result = facets.getTopChildren(10, "field");
-    assertEquals("dim=field path=[] value=41 childCount=4\n  0-10 (11)\n  10-20 (11)\n  20-30 (11)\n  30-40 (11)\n",
-                 result.toString());
-    
-    r.close();
-    d.close();
-  }
-
-  /** Tests single request that mixes Range and non-Range
-   *  faceting, with DrillSideways and taxonomy. */
-  public void testMixedRangeAndNonRangeTaxonomy() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Directory td = newDirectory();
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig();
-
-    for (long l = 0; l < 100; l++) {
-      Document doc = new Document();
-      // For computing range facet counts:
-      doc.add(new NumericDocValuesField("field", l));
-      // For drill down by numeric range:
-      doc.add(new LongField("field", l, Field.Store.NO));
-
-      if ((l&3) == 0) {
-        doc.add(new FacetField("dim", "a"));
-      } else {
-        doc.add(new FacetField("dim", "b"));
-      }
-      w.addDocument(config.build(tw, doc));
-    }
-
-    final IndexReader r = w.getReader();
-
-    final TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
-
-    IndexSearcher s = newSearcher(r);
-
-    DrillSideways ds = new DrillSideways(s, config, tr) {
-
-        @Override
-        protected Facets buildFacetsResult(FacetsCollector drillDowns, FacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {        
-          FacetsCollector dimFC = drillDowns;
-          FacetsCollector fieldFC = drillDowns;
-          if (drillSideways != null) {
-            for(int i=0;i<drillSideways.length;i++) {
-              String dim = drillSidewaysDims[i];
-              if (dim.equals("field")) {
-                fieldFC = drillSideways[i];
-              } else {
-                dimFC = drillSideways[i];
-              }
-            }
-          }
-
-          Map<String,Facets> byDim = new HashMap<String,Facets>();
-          byDim.put("field",
-                    new LongRangeFacetCounts("field", fieldFC,
-                          new LongRange("less than 10", 0L, true, 10L, false),
-                          new LongRange("less than or equal to 10", 0L, true, 10L, true),
-                          new LongRange("over 90", 90L, false, 100L, false),
-                          new LongRange("90 or above", 90L, true, 100L, false),
-                          new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false)));
-          byDim.put("dim", getTaxonomyFacetCounts(taxoReader, config, dimFC));
-          return new MultiFacets(byDim, null);
-        }
-
-        @Override
-        protected boolean scoreSubDocsAtOnce() {
-          return random().nextBoolean();
-        }
-      };
-
-    // First search, no drill downs:
-    DrillDownQuery ddq = new DrillDownQuery(config);
-    DrillSidewaysResult dsr = ds.search(null, ddq, 10);
-
-    assertEquals(100, dsr.hits.totalHits);
-    assertEquals("dim=dim path=[] value=100 childCount=2\n  b (75)\n  a (25)\n", dsr.facets.getTopChildren(10, "dim").toString());
-    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
-                 dsr.facets.getTopChildren(10, "field").toString());
-
-    // Second search, drill down on dim=b:
-    ddq = new DrillDownQuery(config);
-    ddq.add("dim", "b");
-    dsr = ds.search(null, ddq, 10);
-
-    assertEquals(75, dsr.hits.totalHits);
-    assertEquals("dim=dim path=[] value=100 childCount=2\n  b (75)\n  a (25)\n", dsr.facets.getTopChildren(10, "dim").toString());
-    assertEquals("dim=field path=[] value=16 childCount=5\n  less than 10 (7)\n  less than or equal to 10 (8)\n  over 90 (7)\n  90 or above (8)\n  over 1000 (0)\n",
-                 dsr.facets.getTopChildren(10, "field").toString());
-
-    // Third search, drill down on "less than or equal to 10":
-    ddq = new DrillDownQuery(config);
-    ddq.add("field", NumericRangeQuery.newLongRange("field", 0L, 10L, true, true));
-    dsr = ds.search(null, ddq, 10);
-
-    assertEquals(11, dsr.hits.totalHits);
-    assertEquals("dim=dim path=[] value=11 childCount=2\n  b (8)\n  a (3)\n", dsr.facets.getTopChildren(10, "dim").toString());
-    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
-                 dsr.facets.getTopChildren(10, "field").toString());
-    IOUtils.close(tw, tr, td, w, r, d);
-  }
-
-  public void testBasicDouble() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    DoubleDocValuesField field = new DoubleDocValuesField("field", 0.0);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      field.setDoubleValue(l);
-      w.addDocument(doc);
-    }
-
-    IndexReader r = w.getReader();
-
-    FacetsCollector fc = new FacetsCollector();
-
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-    Facets facets = new DoubleRangeFacetCounts("field", fc,
-        new DoubleRange("less than 10", 0.0, true, 10.0, false),
-        new DoubleRange("less than or equal to 10", 0.0, true, 10.0, true),
-        new DoubleRange("over 90", 90.0, false, 100.0, false),
-        new DoubleRange("90 or above", 90.0, true, 100.0, false),
-        new DoubleRange("over 1000", 1000.0, false, Double.POSITIVE_INFINITY, false));
-                                         
-    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
-                 facets.getTopChildren(10, "field").toString());
-
-    IOUtils.close(w, r, d);
-  }
-
-  public void testBasicFloat() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    FloatDocValuesField field = new FloatDocValuesField("field", 0.0f);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      field.setFloatValue(l);
-      w.addDocument(doc);
-    }
-
-    IndexReader r = w.getReader();
-
-    FacetsCollector fc = new FacetsCollector();
-
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-
-    Facets facets = new DoubleRangeFacetCounts("field", new FloatFieldSource("field"), fc,
-        new DoubleRange("less than 10", 0.0f, true, 10.0f, false),
-        new DoubleRange("less than or equal to 10", 0.0f, true, 10.0f, true),
-        new DoubleRange("over 90", 90.0f, false, 100.0f, false),
-        new DoubleRange("90 or above", 90.0f, true, 100.0f, false),
-        new DoubleRange("over 1000", 1000.0f, false, Double.POSITIVE_INFINITY, false));
-    
-    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
-                 facets.getTopChildren(10, "field").toString());
-    
-    IOUtils.close(w, r, d);
-  }
-
-  public void testRandomLongs() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
-
-    int numDocs = atLeast(1000);
-    if (VERBOSE) {
-      System.out.println("TEST: numDocs=" + numDocs);
-    }
-    long[] values = new long[numDocs];
-    for(int i=0;i<numDocs;i++) {
-      Document doc = new Document();
-      long v = random().nextLong();
-      values[i] = v;
-      doc.add(new NumericDocValuesField("field", v));
-      doc.add(new LongField("field", v, Field.Store.NO));
-      w.addDocument(doc);
-    }
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = newSearcher(r);
-    FacetsConfig config = new FacetsConfig();
-    
-    int numIters = atLeast(10);
-    for(int iter=0;iter<numIters;iter++) {
-      if (VERBOSE) {
-        System.out.println("TEST: iter=" + iter);
-      }
-      int numRange = _TestUtil.nextInt(random(), 1, 100);
-      LongRange[] ranges = new LongRange[numRange];
-      int[] expectedCounts = new int[numRange];
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        long min;
-        if (rangeID > 0 && random().nextInt(10) == 7) {
-          // Use an existing boundary:
-          LongRange prevRange = ranges[random().nextInt(rangeID)];
-          if (random().nextBoolean()) {
-            min = prevRange.min;
-          } else {
-            min = prevRange.max;
-          }
-        } else {
-          min = random().nextLong();
-        }
-        long max;
-        if (rangeID > 0 && random().nextInt(10) == 7) {
-          // Use an existing boundary:
-          LongRange prevRange = ranges[random().nextInt(rangeID)];
-          if (random().nextBoolean()) {
-            max = prevRange.min;
-          } else {
-            max = prevRange.max;
-          }
-        } else {
-          max = random().nextLong();
-        }
-
-        if (min > max) {
-          long x = min;
-          min = max;
-          max = x;
-        }
-        boolean minIncl;
-        boolean maxIncl;
-        if (min == max) {
-          minIncl = true;
-          maxIncl = true;
-        } else {
-          minIncl = random().nextBoolean();
-          maxIncl = random().nextBoolean();
-        }
-        ranges[rangeID] = new LongRange("r" + rangeID, min, minIncl, max, maxIncl);
-        if (VERBOSE) {
-          System.out.println("  range " + rangeID + ": " + ranges[rangeID]);      
-        }
-
-        // Do "slow but hopefully correct" computation of
-        // expected count:
-        for(int i=0;i<numDocs;i++) {
-          boolean accept = true;
-          if (minIncl) {
-            accept &= values[i] >= min;
-          } else {
-            accept &= values[i] > min;
-          }
-          if (maxIncl) {
-            accept &= values[i] <= max;
-          } else {
-            accept &= values[i] < max;
-          }
-          if (accept) {
-            expectedCounts[rangeID]++;
-          }
-        }
-      }
-
-      FacetsCollector sfc = new FacetsCollector();
-      s.search(new MatchAllDocsQuery(), sfc);
-      Facets facets = new LongRangeFacetCounts("field", sfc, ranges);
-      FacetResult result = facets.getTopChildren(10, "field");
-      assertEquals(numRange, result.labelValues.length);
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        if (VERBOSE) {
-          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
-        }
-        LabelAndValue subNode = result.labelValues[rangeID];
-        assertEquals("r" + rangeID, subNode.label);
-        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
-
-        LongRange range = ranges[rangeID];
-
-        // Test drill-down:
-        DrillDownQuery ddq = new DrillDownQuery(config);
-        ddq.add("field", NumericRangeQuery.newLongRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
-        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
-      }
-    }
-
-    IOUtils.close(w, r, dir);
-  }
-
-  public void testRandomFloats() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
-
-    int numDocs = atLeast(1000);
-    float[] values = new float[numDocs];
-    for(int i=0;i<numDocs;i++) {
-      Document doc = new Document();
-      float v = random().nextFloat();
-      values[i] = v;
-      doc.add(new FloatDocValuesField("field", v));
-      doc.add(new FloatField("field", v, Field.Store.NO));
-      w.addDocument(doc);
-    }
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = newSearcher(r);
-    FacetsConfig config = new FacetsConfig();
-    
-    int numIters = atLeast(10);
-    for(int iter=0;iter<numIters;iter++) {
-      if (VERBOSE) {
-        System.out.println("TEST: iter=" + iter);
-      }
-      int numRange = _TestUtil.nextInt(random(), 1, 5);
-      DoubleRange[] ranges = new DoubleRange[numRange];
-      int[] expectedCounts = new int[numRange];
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        double min;
-        if (rangeID > 0 && random().nextInt(10) == 7) {
-          // Use an existing boundary:
-          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
-          if (random().nextBoolean()) {
-            min = prevRange.min;
-          } else {
-            min = prevRange.max;
-          }
-        } else {
-          min = random().nextDouble();
-        }
-        double max;
-        if (rangeID > 0 && random().nextInt(10) == 7) {
-          // Use an existing boundary:
-          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
-          if (random().nextBoolean()) {
-            max = prevRange.min;
-          } else {
-            max = prevRange.max;
-          }
-        } else {
-          max = random().nextDouble();
-        }
-
-        if (min > max) {
-          double x = min;
-          min = max;
-          max = x;
-        }
-
-        boolean minIncl;
-        boolean maxIncl;
-        if (min == max) {
-          minIncl = true;
-          maxIncl = true;
-        } else {
-          minIncl = random().nextBoolean();
-          maxIncl = random().nextBoolean();
-        }
-        ranges[rangeID] = new DoubleRange("r" + rangeID, min, minIncl, max, maxIncl);
-
-        // Do "slow but hopefully correct" computation of
-        // expected count:
-        for(int i=0;i<numDocs;i++) {
-          boolean accept = true;
-          if (minIncl) {
-            accept &= values[i] >= min;
-          } else {
-            accept &= values[i] > min;
-          }
-          if (maxIncl) {
-            accept &= values[i] <= max;
-          } else {
-            accept &= values[i] < max;
-          }
-          if (accept) {
-            expectedCounts[rangeID]++;
-          }
-        }
-      }
-
-      FacetsCollector sfc = new FacetsCollector();
-      s.search(new MatchAllDocsQuery(), sfc);
-      Facets facets = new DoubleRangeFacetCounts("field", new FloatFieldSource("field"), sfc, ranges);
-      FacetResult result = facets.getTopChildren(10, "field");
-      assertEquals(numRange, result.labelValues.length);
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        if (VERBOSE) {
-          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
-        }
-        LabelAndValue subNode = result.labelValues[rangeID];
-        assertEquals("r" + rangeID, subNode.label);
-        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
-
-        DoubleRange range = ranges[rangeID];
-
-        // Test drill-down:
-        DrillDownQuery ddq = new DrillDownQuery(config);
-        ddq.add("field", NumericRangeQuery.newFloatRange("field", (float) range.min, (float) range.max, range.minInclusive, range.maxInclusive));
-        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
-      }
-    }
-
-    IOUtils.close(w, r, dir);
-  }
-
-  public void testRandomDoubles() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
-
-    int numDocs = atLeast(1000);
-    double[] values = new double[numDocs];
-    for(int i=0;i<numDocs;i++) {
-      Document doc = new Document();
-      double v = random().nextDouble();
-      values[i] = v;
-      doc.add(new DoubleDocValuesField("field", v));
-      doc.add(new DoubleField("field", v, Field.Store.NO));
-      w.addDocument(doc);
-    }
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = newSearcher(r);
-    FacetsConfig config = new FacetsConfig();
-    
-    int numIters = atLeast(10);
-    for(int iter=0;iter<numIters;iter++) {
-      if (VERBOSE) {
-        System.out.println("TEST: iter=" + iter);
-      }
-      int numRange = _TestUtil.nextInt(random(), 1, 5);
-      DoubleRange[] ranges = new DoubleRange[numRange];
-      int[] expectedCounts = new int[numRange];
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        double min;
-        if (rangeID > 0 && random().nextInt(10) == 7) {
-          // Use an existing boundary:
-          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
-          if (random().nextBoolean()) {
-            min = prevRange.min;
-          } else {
-            min = prevRange.max;
-          }
-        } else {
-          min = random().nextDouble();
-        }
-        double max;
-        if (rangeID > 0 && random().nextInt(10) == 7) {
-          // Use an existing boundary:
-          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
-          if (random().nextBoolean()) {
-            max = prevRange.min;
-          } else {
-            max = prevRange.max;
-          }
-        } else {
-          max = random().nextDouble();
-        }
-
-        if (min > max) {
-          double x = min;
-          min = max;
-          max = x;
-        }
-
-        boolean minIncl;
-        boolean maxIncl;
-        if (min == max) {
-          minIncl = true;
-          maxIncl = true;
-        } else {
-          minIncl = random().nextBoolean();
-          maxIncl = random().nextBoolean();
-        }
-        ranges[rangeID] = new DoubleRange("r" + rangeID, min, minIncl, max, maxIncl);
-
-        // Do "slow but hopefully correct" computation of
-        // expected count:
-        for(int i=0;i<numDocs;i++) {
-          boolean accept = true;
-          if (minIncl) {
-            accept &= values[i] >= min;
-          } else {
-            accept &= values[i] > min;
-          }
-          if (maxIncl) {
-            accept &= values[i] <= max;
-          } else {
-            accept &= values[i] < max;
-          }
-          if (accept) {
-            expectedCounts[rangeID]++;
-          }
-        }
-      }
-
-      FacetsCollector sfc = new FacetsCollector();
-      s.search(new MatchAllDocsQuery(), sfc);
-      Facets facets = new DoubleRangeFacetCounts("field", sfc, ranges);
-      FacetResult result = facets.getTopChildren(10, "field");
-      assertEquals(numRange, result.labelValues.length);
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        if (VERBOSE) {
-          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
-        }
-        LabelAndValue subNode = result.labelValues[rangeID];
-        assertEquals("r" + rangeID, subNode.label);
-        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
-
-        DoubleRange range = ranges[rangeID];
-
-        // Test drill-down:
-        DrillDownQuery ddq = new DrillDownQuery(config);
-        ddq.add("field", NumericRangeQuery.newDoubleRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
-        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
-      }
-    }
-
-    IOUtils.close(w, r, dir);
-  }
-
-  // LUCENE-5178
-  public void testMissingValues() throws Exception {
-    assumeTrue("codec does not support docsWithField", defaultCodecSupportsDocsWithField());
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      if (l % 5 == 0) {
-        // Every 5th doc is missing the value:
-        w.addDocument(new Document());
-        continue;
-      }
-      field.setLongValue(l);
-      w.addDocument(doc);
-    }
-
-    IndexReader r = w.getReader();
-
-    FacetsCollector fc = new FacetsCollector();
-
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-    Facets facets = new LongRangeFacetCounts("field", fc,
-        new LongRange("less than 10", 0L, true, 10L, false),
-        new LongRange("less than or equal to 10", 0L, true, 10L, true),
-        new LongRange("over 90", 90L, false, 100L, false),
-        new LongRange("90 or above", 90L, true, 100L, false),
-        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false));
-    
-    assertEquals("dim=field path=[] value=16 childCount=5\n  less than 10 (8)\n  less than or equal to 10 (8)\n  over 90 (8)\n  90 or above (8)\n  over 1000 (0)\n",
-                 facets.getTopChildren(10, "field").toString());
-
-    IOUtils.close(w, r, d);
-  }
-
-  public void testCustomDoublesValueSource() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    
-    Document doc = new Document();
-    writer.addDocument(doc);
-    
-    doc = new Document();
-    writer.addDocument(doc);
-    
-    doc = new Document();
-    writer.addDocument(doc);
-
-    writer.forceMerge(1);
-
-    ValueSource vs = new ValueSource() {
-        @Override
-        public FunctionValues getValues(Map ignored, AtomicReaderContext ignored2) {
-          return new DoubleDocValues(null) {
-            public double doubleVal(int doc) {
-              return doc+1;
-            }
-          };
-        }
-
-        @Override
-        public boolean equals(Object o) {
-          throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public int hashCode() {
-          throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public String description() {
-          throw new UnsupportedOperationException();
-        }
-      };
-    
-    FacetsCollector fc = new FacetsCollector();
-
-    IndexReader r = writer.getReader();
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-
-    Facets facets = new DoubleRangeFacetCounts("field", vs, fc,
-        new DoubleRange("< 1", 0.0, true, 1.0, false),
-        new DoubleRange("< 2", 0.0, true, 2.0, false),
-        new DoubleRange("< 5", 0.0, true, 5.0, false),
-        new DoubleRange("< 10", 0.0, true, 10.0, false),
-        new DoubleRange("< 20", 0.0, true, 20.0, false),
-        new DoubleRange("< 50", 0.0, true, 50.0, false));
-
-    assertEquals("dim=field path=[] value=3 childCount=6\n  < 1 (0)\n  < 2 (1)\n  < 5 (3)\n  < 10 (3)\n  < 20 (3)\n  < 50 (3)\n", facets.getTopChildren(10, "field").toString());
-
-    // Test drill-down:
-    assertEquals(1, s.search(new ConstantScoreQuery(new DoubleRange("< 2", 0.0, true, 2.0, false).getFilter(vs)), 10).totalHits);
-
-    IOUtils.close(r, writer, dir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestSearcherTaxonomyManager.java b/lucene/facet/src/test/org/apache/lucene/facet/TestSearcherTaxonomyManager.java
deleted file mode 100644
index 6982d87..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/TestSearcherTaxonomyManager.java
+++ /dev/null
@@ -1,183 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.SearcherTaxonomyManager.SearcherAndTaxonomy;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestSearcherTaxonomyManager extends FacetTestCase {
-  public void test() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    final IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    final DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-    final FacetsConfig config = new FacetsConfig();
-    config.setMultiValued("field", true);
-    final AtomicBoolean stop = new AtomicBoolean();
-
-    // How many unique facets to index before stopping:
-    final int ordLimit = TEST_NIGHTLY ? 100000 : 6000;
-
-    Thread indexer = new Thread() {
-        @Override
-        public void run() {
-          try {
-            Set<String> seen = new HashSet<String>();
-            List<String> paths = new ArrayList<String>();
-            while (true) {
-              Document doc = new Document();
-              int numPaths = _TestUtil.nextInt(random(), 1, 5);
-              for(int i=0;i<numPaths;i++) {
-                String path;
-                if (!paths.isEmpty() && random().nextInt(5) != 4) {
-                  // Use previous path
-                  path = paths.get(random().nextInt(paths.size()));
-                } else {
-                  // Create new path
-                  path = null;
-                  while (true) {
-                    path = _TestUtil.randomRealisticUnicodeString(random());
-                    if (path.length() != 0 && !seen.contains(path)) {
-                      seen.add(path);
-                      paths.add(path);
-                      break;
-                    }
-                  }
-                }
-                doc.add(new FacetField("field", path));
-              }
-              try {
-                w.addDocument(config.build(tw, doc));
-              } catch (IOException ioe) {
-                throw new RuntimeException(ioe);
-              }
-
-              if (tw.getSize() >= ordLimit) {
-                break;
-              }
-            }
-          } finally {
-            stop.set(true);
-          }
-        }
-      };
-
-    final SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
-
-    Thread reopener = new Thread() {
-        @Override
-        public void run() {
-          while(!stop.get()) {
-            try {
-              // Sleep for up to 20 msec:
-              Thread.sleep(random().nextInt(20));
-
-              if (VERBOSE) {
-                System.out.println("TEST: reopen");
-              }
-
-              mgr.maybeRefresh();
-
-              if (VERBOSE) {
-                System.out.println("TEST: reopen done");
-              }
-            } catch (Exception ioe) {
-              throw new RuntimeException(ioe);
-            }
-          }
-        }
-      };
-    reopener.start();
-
-    indexer.start();
-
-    try {
-      while (!stop.get()) {
-        SearcherAndTaxonomy pair = mgr.acquire();
-        try {
-          //System.out.println("search maxOrd=" + pair.taxonomyReader.getSize());
-          int topN = _TestUtil.nextInt(random(), 1, 20);
-          
-          FacetsCollector sfc = new FacetsCollector();
-          pair.searcher.search(new MatchAllDocsQuery(), sfc);
-          Facets facets = getTaxonomyFacetCounts(pair.taxonomyReader, config, sfc);
-          FacetResult result = facets.getTopChildren(10, "field");
-          if (pair.searcher.getIndexReader().numDocs() > 0) { 
-            //System.out.println(pair.taxonomyReader.getSize());
-            assertTrue(result.childCount > 0);
-            assertTrue(result.labelValues.length > 0);
-          }
-
-          //if (VERBOSE) {
-          //System.out.println("TEST: facets=" + FacetTestUtils.toString(results.get(0)));
-          //}
-        } finally {
-          mgr.release(pair);
-        }
-      }
-    } finally {
-      indexer.join();
-      reopener.join();
-    }
-
-    if (VERBOSE) {
-      System.out.println("TEST: now stop");
-    }
-
-    IOUtils.close(mgr, tw, w, taxoDir, dir);
-  }
-
-  public void testReplaceTaxonomy() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-
-    Directory taxoDir2 = newDirectory();
-    DirectoryTaxonomyWriter tw2 = new DirectoryTaxonomyWriter(taxoDir2);
-    tw2.close();
-
-    SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
-    w.addDocument(new Document());
-    tw.replaceTaxonomy(taxoDir2);
-    taxoDir2.close();
-
-    try {
-      mgr.maybeRefresh();
-      fail("should have hit exception");
-    } catch (IllegalStateException ise) {
-      // expected
-    }
-
-    IOUtils.close(mgr, tw, w, taxoDir, dir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestSortedSetDocValuesFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/TestSortedSetDocValuesFacets.java
deleted file mode 100644
index 4232509..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/TestSortedSetDocValuesFacets.java
+++ /dev/null
@@ -1,350 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestSortedSetDocValuesFacets extends FacetTestCase {
-
-  // NOTE: TestDrillSideways.testRandom also sometimes
-  // randomly uses SortedSetDV
-
-  public void testBasic() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory dir = newDirectory();
-
-    FacetsConfig config = new FacetsConfig();
-    config.setMultiValued("a", true);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
-    doc.add(new SortedSetDocValuesFacetField("a", "bar"));
-    doc.add(new SortedSetDocValuesFacetField("a", "zoo"));
-    doc.add(new SortedSetDocValuesFacetField("b", "baz"));
-    writer.addDocument(config.build(doc));
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // Per-top-reader state:
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
-    
-    FacetsCollector c = new FacetsCollector();
-
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
-
-    assertEquals("dim=a path=[] value=4 childCount=3\n  foo (2)\n  bar (1)\n  zoo (1)\n", facets.getTopChildren(10, "a").toString());
-    assertEquals("dim=b path=[] value=1 childCount=1\n  baz (1)\n", facets.getTopChildren(10, "b").toString());
-
-    // DrillDown:
-    DrillDownQuery q = new DrillDownQuery(config);
-    q.add("a", "foo");
-    q.add("b", "baz");
-    TopDocs hits = searcher.search(q, 1);
-    assertEquals(1, hits.totalHits);
-
-    IOUtils.close(writer, searcher.getIndexReader(), dir);
-  }
-
-  // LUCENE-5090
-  public void testStaleState() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory dir = newDirectory();
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
-    writer.addDocument(config.build(doc));
-
-    IndexReader r = writer.getReader();
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(r);
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "bar"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "baz"));
-    writer.addDocument(config.build(doc));
-
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    FacetsCollector c = new FacetsCollector();
-
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    try {
-      new SortedSetDocValuesFacetCounts(state, c);
-      fail("did not hit expected exception");
-    } catch (IllegalStateException ise) {
-      // expected
-    }
-
-    r.close();
-    writer.close();
-    searcher.getIndexReader().close();
-    dir.close();
-  }
-
-  // LUCENE-5333
-  public void testSparseFacets() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory dir = newDirectory();
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
-    writer.addDocument(config.build(doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
-    doc.add(new SortedSetDocValuesFacetField("b", "bar1"));
-    writer.addDocument(config.build(doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo3"));
-    doc.add(new SortedSetDocValuesFacetField("b", "bar2"));
-    doc.add(new SortedSetDocValuesFacetField("c", "baz1"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // Per-top-reader state:
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
-
-    FacetsCollector c = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<FacetResult> results = facets.getAllDims(10);
-
-    assertEquals(3, results.size());
-    assertEquals("dim=a path=[] value=3 childCount=3\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
-    assertEquals("dim=b path=[] value=2 childCount=2\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
-    assertEquals("dim=c path=[] value=1 childCount=1\n  baz1 (1)\n", results.get(2).toString());
-
-    searcher.getIndexReader().close();
-    dir.close();
-  }
-
-  public void testSomeSegmentsMissing() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory dir = newDirectory();
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
-    writer.addDocument(config.build(doc));
-    writer.commit();
-
-    doc = new Document();
-    writer.addDocument(config.build(doc));
-    writer.commit();
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
-    writer.addDocument(config.build(doc));
-    writer.commit();
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // Per-top-reader state:
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
-
-    FacetsCollector c = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
-
-    // Ask for top 10 labels for any dims that have counts:
-    assertEquals("dim=a path=[] value=2 childCount=2\n  foo1 (1)\n  foo2 (1)\n", facets.getTopChildren(10, "a").toString());
-
-    searcher.getIndexReader().close();
-    dir.close();
-  }
-
-  public void testSlowCompositeReaderWrapper() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory dir = newDirectory();
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
-    writer.addDocument(config.build(doc));
-
-    writer.commit();
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = new IndexSearcher(SlowCompositeReaderWrapper.wrap(writer.getReader()));
-
-    // Per-top-reader state:
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
-
-    FacetsCollector c = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-    Facets facets = new SortedSetDocValuesFacetCounts(state, c);
-
-    // Ask for top 10 labels for any dims that have counts:
-    assertEquals("dim=a path=[] value=2 childCount=2\n  foo1 (1)\n  foo2 (1)\n", facets.getTopChildren(10, "a").toString());
-
-    IOUtils.close(writer, searcher.getIndexReader(), dir);
-  }
-
-
-  public void testRandom() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    String[] tokens = getRandomTokens(10);
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    RandomIndexWriter w = new RandomIndexWriter(random(), indexDir);
-    FacetsConfig config = new FacetsConfig();
-    int numDocs = atLeast(1000);
-    int numDims = _TestUtil.nextInt(random(), 1, 7);
-    List<TestDoc> testDocs = getRandomDocs(tokens, numDocs, numDims);
-    for(TestDoc testDoc : testDocs) {
-      Document doc = new Document();
-      doc.add(newStringField("content", testDoc.content, Field.Store.NO));
-      for(int j=0;j<numDims;j++) {
-        if (testDoc.dims[j] != null) {
-          doc.add(new SortedSetDocValuesFacetField("dim" + j, testDoc.dims[j]));
-        }
-      }
-      w.addDocument(config.build(doc));
-    }
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(w.getReader());
-    
-    // Per-top-reader state:
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
-
-    int iters = atLeast(100);
-    for(int iter=0;iter<iters;iter++) {
-      String searchToken = tokens[random().nextInt(tokens.length)];
-      if (VERBOSE) {
-        System.out.println("\nTEST: iter content=" + searchToken);
-      }
-      FacetsCollector fc = new FacetsCollector();
-      TopDocs hits = FacetsCollector.search(searcher, new TermQuery(new Term("content", searchToken)), 10, fc);
-      Facets facets = new SortedSetDocValuesFacetCounts(state, fc);
-
-      // Slow, yet hopefully bug-free, faceting:
-      @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Integer>[] expectedCounts = new HashMap[numDims];
-      for(int i=0;i<numDims;i++) {
-        expectedCounts[i] = new HashMap<String,Integer>();
-      }
-
-      for(TestDoc doc : testDocs) {
-        if (doc.content.equals(searchToken)) {
-          for(int j=0;j<numDims;j++) {
-            if (doc.dims[j] != null) {
-              Integer v = expectedCounts[j].get(doc.dims[j]);
-              if (v == null) {
-                expectedCounts[j].put(doc.dims[j], 1);
-              } else {
-                expectedCounts[j].put(doc.dims[j], v.intValue() + 1);
-              }
-            }
-          }
-        }
-      }
-
-      List<FacetResult> expected = new ArrayList<FacetResult>();
-      for(int i=0;i<numDims;i++) {
-        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
-        int totCount = 0;
-        for(Map.Entry<String,Integer> ent : expectedCounts[i].entrySet()) {
-          labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
-          totCount += ent.getValue();
-        }
-        sortLabelValues(labelValues);
-        if (totCount > 0) {
-          expected.add(new FacetResult("dim" + i, new String[0], totCount, labelValues.toArray(new LabelAndValue[labelValues.size()]), labelValues.size()));
-        }
-      }
-
-      // Sort by highest value, tie break by value:
-      sortFacetResults(expected);
-
-      List<FacetResult> actual = facets.getAllDims(10);
-
-      // Messy: fixup ties
-      //sortTies(actual);
-
-      assertEquals(expected, actual);
-    }
-
-    IOUtils.close(w, searcher.getIndexReader(), indexDir, taxoDir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetAssociations.java b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetAssociations.java
deleted file mode 100644
index 2886a16..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetAssociations.java
+++ /dev/null
@@ -1,218 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-
-/** Test for associations */
-public class TestTaxonomyFacetAssociations extends FacetTestCase {
-  
-  private static Directory dir;
-  private static IndexReader reader;
-  private static Directory taxoDir;
-  private static TaxonomyReader taxoReader;
-
-  private static FacetsConfig config;
-
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    dir = newDirectory();
-    taxoDir = newDirectory();
-    // preparations - index, taxonomy, content
-    
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-
-    // Cannot mix ints & floats in the same indexed field:
-    config = new FacetsConfig();
-    config.setIndexFieldName("int", "$facets.int");
-    config.setMultiValued("int", true);
-    config.setIndexFieldName("float", "$facets.float");
-    config.setMultiValued("float", true);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    // index documents, 50% have only 'b' and all have 'a'
-    for (int i = 0; i < 110; i++) {
-      Document doc = new Document();
-      // every 11th document is added empty, this used to cause the association
-      // aggregators to go into an infinite loop
-      if (i % 11 != 0) {
-        doc.add(new IntAssociationFacetField(2, "int", "a"));
-        doc.add(new FloatAssociationFacetField(0.5f, "float", "a"));
-        if (i % 2 == 0) { // 50
-          doc.add(new IntAssociationFacetField(3, "int", "b"));
-          doc.add(new FloatAssociationFacetField(0.2f, "float", "b"));
-        }
-      }
-      writer.addDocument(config.build(taxoWriter, doc));
-    }
-    
-    taxoWriter.close();
-    reader = writer.getReader();
-    writer.close();
-    taxoReader = new DirectoryTaxonomyReader(taxoDir);
-  }
-  
-  @AfterClass
-  public static void afterClass() throws Exception {
-    reader.close();
-    reader = null;
-    dir.close();
-    dir = null;
-    taxoReader.close();
-    taxoReader = null;
-    taxoDir.close();
-    taxoDir = null;
-  }
-  
-  public void testIntSumAssociation() throws Exception {
-    
-    FacetsCollector fc = new FacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-
-    Facets facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
-    assertEquals("dim=int path=[] value=-1 childCount=2\n  a (200)\n  b (150)\n", facets.getTopChildren(10, "int").toString());
-    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
-    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
-  }
-
-  public void testFloatSumAssociation() throws Exception {
-    FacetsCollector fc = new FacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
-    assertEquals("dim=float path=[] value=-1.0 childCount=2\n  a (50.0)\n  b (9.999995)\n", facets.getTopChildren(10, "float").toString());
-    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
-    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
-  }  
-
-  /** Make sure we can test both int and float assocs in one
-   *  index, as long as we send each to a different field. */
-  public void testIntAndFloatAssocation() throws Exception {
-    FacetsCollector fc = new FacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
-    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
-    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
-    
-    facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
-    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
-    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
-  }
-
-  public void testWrongIndexFieldName() throws Exception {
-    FacetsCollector fc = new FacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    Facets facets = new TaxonomyFacetSumFloatAssociations(taxoReader, config, fc);
-    try {
-      facets.getSpecificValue("float");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    try {
-      facets.getTopChildren(10, "float");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-  }
-
-  public void testMixedTypesInSameIndexField() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetsConfig config = new FacetsConfig();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new IntAssociationFacetField(14, "a", "x"));
-    doc.add(new FloatAssociationFacetField(55.0f, "b", "y"));
-    try {
-      writer.addDocument(config.build(taxoWriter, doc));
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException exc) {
-      // expected
-    }
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-
-  public void testNoHierarchy() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetsConfig config = new FacetsConfig();
-    config.setHierarchical("a", true);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new IntAssociationFacetField(14, "a", "x"));
-    try {
-      writer.addDocument(config.build(taxoWriter, doc));
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException exc) {
-      // expected
-    }
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-
-  public void testRequireDimCount() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetsConfig config = new FacetsConfig();
-    config.setRequireDimCount("a", true);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new IntAssociationFacetField(14, "a", "x"));
-    try {
-      writer.addDocument(config.build(taxoWriter, doc));
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException exc) {
-      // expected
-    }
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts.java b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts.java
deleted file mode 100644
index 8c6a11e..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts.java
+++ /dev/null
@@ -1,754 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.taxonomy.PrintTaxonomyStats;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.similarities.DefaultSimilarity;
-import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
-import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestTaxonomyFacetCounts extends FacetTestCase {
-
-  public void testBasic() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig();
-    config.setHierarchical("Publish Date", true);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new FacetField("Author", "Bob"));
-    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Lisa"));
-    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Lisa"));
-    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Susan"));
-    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Frank"));
-    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    // Aggregate the facet counts:
-    FacetsCollector c = new FacetsCollector();
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
-
-    // Retrieve & verify results:
-    assertEquals("dim=Publish Date path=[] value=5 childCount=3\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", facets.getTopChildren(10, "Publish Date").toString());
-    assertEquals("dim=Author path=[] value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", facets.getTopChildren(10, "Author").toString());
-
-    // Now user drills down on Publish Date/2010:
-    DrillDownQuery q2 = new DrillDownQuery(config);
-    q2.add("Publish Date", "2010");
-    c = new FacetsCollector();
-    searcher.search(q2, c);
-    facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
-    assertEquals("dim=Author path=[] value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", facets.getTopChildren(10, "Author").toString());
-
-    assertEquals(1, facets.getSpecificValue("Author", "Lisa"));
-
-    assertNull(facets.getTopChildren(10, "Non exitent dim"));
-
-    // Smoke test PrintTaxonomyStats:
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintTaxonomyStats.printStats(taxoReader, new PrintStream(bos, false, "UTF-8"), true);
-    String result = bos.toString("UTF-8");
-    assertTrue(result.indexOf("/Author: 4 immediate children; 5 total categories") != -1);
-    assertTrue(result.indexOf("/Publish Date: 3 immediate children; 12 total categories") != -1);
-    // Make sure at least a few nodes of the tree came out:
-    assertTrue(result.indexOf("  /1999") != -1);
-    assertTrue(result.indexOf("  /2012") != -1);
-    assertTrue(result.indexOf("      /20") != -1);
-
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
-  }
-
-  // LUCENE-5333
-  public void testSparseFacets() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new FacetField("a", "foo2"));
-    doc.add(new FacetField("b", "bar1"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new FacetField("a", "foo3"));
-    doc.add(new FacetField("b", "bar2"));
-    doc.add(new FacetField("c", "baz1"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    FacetsCollector c = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    Facets facets = getTaxonomyFacetCounts(taxoReader, new FacetsConfig(), c);
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<FacetResult> results = facets.getAllDims(10);
-
-    assertEquals(3, results.size());
-    assertEquals("dim=a path=[] value=3 childCount=3\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
-    assertEquals("dim=b path=[] value=2 childCount=2\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
-    assertEquals("dim=c path=[] value=1 childCount=1\n  baz1 (1)\n", results.get(2).toString());
-
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
-  }
-
-  public void testWrongIndexFieldName() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig();
-    config.setIndexFieldName("a", "$facets2");
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    FacetsCollector c = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    // Uses default $facets field:
-    Facets facets;
-    if (random().nextBoolean()) {
-      facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
-    } else {
-      OrdinalsReader ordsReader = new DocValuesOrdinalsReader();
-      if (random().nextBoolean()) {
-        ordsReader = new CachedOrdinalsReader(ordsReader);
-      }
-      facets = new TaxonomyFacetCounts(ordsReader, taxoReader, config, c);
-    }
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<FacetResult> results = facets.getAllDims(10);
-    assertTrue(results.isEmpty());
-
-    try {
-      facets.getSpecificValue("a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    try {
-      facets.getTopChildren(10, "a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
-  }
-
-  public void testReallyNoNormsForDrillDown() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setSimilarity(new PerFieldSimilarityWrapper() {
-        final Similarity sim = new DefaultSimilarity();
-
-        @Override
-        public Similarity get(String name) {
-          assertEquals("field", name);
-          return sim;
-        }
-      });
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("a", "path"));
-    writer.addDocument(config.build(taxoWriter, doc));
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-
-  public void testMultiValuedHierarchy() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    FacetsConfig config = new FacetsConfig();
-    config.setHierarchical("a", true);
-    config.setMultiValued("a", true);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("a", "path", "x"));
-    doc.add(new FacetField("a", "path", "y"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    // Aggregate the facet counts:
-    FacetsCollector c = new FacetsCollector();
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
-
-    try {
-      facets.getSpecificValue("a");
-      fail("didn't hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    FacetResult result = facets.getTopChildren(10, "a");
-    assertEquals(1, result.labelValues.length);
-    assertEquals(1, result.labelValues[0].value.intValue());
-
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-
-  public void testLabelWithDelimiter() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig();
-    config.setMultiValued("dim", true);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("dim", "test\u001Fone"));
-    doc.add(new FacetField("dim", "test\u001Etwo"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    FacetsCollector c = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);
-    
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
-    assertEquals(1, facets.getSpecificValue("dim", "test\u001Fone"));
-    assertEquals(1, facets.getSpecificValue("dim", "test\u001Etwo"));
-
-    FacetResult result = facets.getTopChildren(10, "dim");
-    assertEquals("dim=dim path=[] value=-1 childCount=2\n  test\u001Fone (1)\n  test\u001Etwo (1)\n", result.toString());
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-
-  public void testRequireDimCount() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig();
-    config.setRequireDimCount("dim", true);
-
-    config.setMultiValued("dim2", true);
-    config.setRequireDimCount("dim2", true);
-
-    config.setMultiValued("dim3", true);
-    config.setHierarchical("dim3", true);
-    config.setRequireDimCount("dim3", true);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("dim", "a"));
-    doc.add(new FacetField("dim2", "a"));
-    doc.add(new FacetField("dim2", "b"));
-    doc.add(new FacetField("dim3", "a", "b"));
-    doc.add(new FacetField("dim3", "a", "c"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    FacetsCollector c = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);
-    
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
-    assertEquals(1, facets.getTopChildren(10, "dim").value);
-    assertEquals(1, facets.getTopChildren(10, "dim2").value);
-    assertEquals(1, facets.getTopChildren(10, "dim3").value);
-    try {
-      assertEquals(1, facets.getSpecificValue("dim"));
-      fail("didn't hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    assertEquals(1, facets.getSpecificValue("dim2"));
-    assertEquals(1, facets.getSpecificValue("dim3"));
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-
-  // LUCENE-4583: make sure if we require > 32 KB for one
-  // document, we don't hit exc when using Facet42DocValuesFormat
-  public void testManyFacetsInOneDocument() throws Exception {
-    assumeTrue("default Codec doesn't support huge BinaryDocValues", _TestUtil.fieldSupportsHugeBinaryDocValues(FacetsConfig.DEFAULT_INDEX_FIELD_NAME));
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig();
-    config.setMultiValued("dim", true);
-    
-    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);
-    
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    for (int i = 0; i < numLabels; i++) {
-      doc.add(new FacetField("dim", "" + i));
-    }
-    writer.addDocument(config.build(taxoWriter, doc));
-    
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    // Aggregate the facet counts:
-    FacetsCollector c = new FacetsCollector();
-    
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
-
-    FacetResult result = facets.getTopChildren(Integer.MAX_VALUE, "dim");
-    assertEquals(numLabels, result.labelValues.length);
-    Set<String> allLabels = new HashSet<String>();
-    for (LabelAndValue labelValue : result.labelValues) {
-      allLabels.add(labelValue.label);
-      assertEquals(1, labelValue.value.intValue());
-    }
-    assertEquals(numLabels, allLabels.size());
-    
-    IOUtils.close(searcher.getIndexReader(), taxoWriter, writer, taxoReader, dir, taxoDir);
-  }
-
-  // Make sure we catch when app didn't declare field as
-  // hierarchical but it was:
-  public void testDetectHierarchicalField() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("a", "path", "other"));
-    try {
-      config.build(taxoWriter, doc);
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-
-  // Make sure we catch when app didn't declare field as
-  // multi-valued but it was:
-  public void testDetectMultiValuedField() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("a", "path"));
-    doc.add(new FacetField("a", "path2"));
-    try {
-      config.build(taxoWriter, doc);
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-
-  public void testSeparateIndexedFields() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig();
-    config.setIndexFieldName("b", "$b");
-    
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      doc.add(new StringField("f", "v", Field.Store.NO));
-      doc.add(new FacetField("a", "1"));
-      doc.add(new FacetField("b", "1"));
-      iw.addDocument(config.build(taxoWriter, doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    FacetsCollector sfc = new FacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-    Facets facets1 = getTaxonomyFacetCounts(taxoReader, config, sfc);
-    Facets facets2 = getTaxonomyFacetCounts(taxoReader, config, sfc, "$b");
-    assertEquals(r.maxDoc(), facets1.getTopChildren(10, "a").value.intValue());
-    assertEquals(r.maxDoc(), facets2.getTopChildren(10, "b").value.intValue());
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-  
-  public void testCountRoot() throws Exception {
-    // LUCENE-4882: FacetsAccumulator threw NPE if a FacetRequest was defined on CP.EMPTY
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig();
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      doc.add(new FacetField("a", "1"));
-      doc.add(new FacetField("b", "1"));
-      iw.addDocument(config.build(taxoWriter, doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    FacetsCollector sfc = new FacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
-    for (FacetResult result : facets.getAllDims(10)) {
-      assertEquals(r.numDocs(), result.value.intValue());
-    }
-    
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  public void testGetFacetResultsTwice() throws Exception {
-    // LUCENE-4893: counts were multiplied as many times as getFacetResults was called.
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(new FacetField("a", "1"));
-    doc.add(new FacetField("b", "1"));
-    iw.addDocument(config.build(taxoWriter, doc));
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    final FacetsCollector sfc = new FacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
-    List<FacetResult> res1 = facets.getAllDims(10);
-    List<FacetResult> res2 = facets.getAllDims(10);
-    assertEquals("calling getFacetResults twice should return the .equals()=true result", res1, res2);
-    
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-  
-  public void testChildCount() throws Exception {
-    // LUCENE-4885: FacetResult.numValidDescendants was not set properly by FacetsAccumulator
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig();
-    for (int i = 0; i < 10; i++) {
-      Document doc = new Document();
-      doc.add(new FacetField("a", Integer.toString(i)));
-      iw.addDocument(config.build(taxoWriter, doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    FacetsCollector sfc = new FacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
-    
-    assertEquals(10, facets.getTopChildren(2, "a").childCount);
-
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  private void indexTwoDocs(TaxonomyWriter taxoWriter, IndexWriter indexWriter, FacetsConfig config, boolean withContent) throws Exception {
-    for (int i = 0; i < 2; i++) {
-      Document doc = new Document();
-      if (withContent) {
-        doc.add(new StringField("f", "a", Field.Store.NO));
-      }
-      if (config != null) {
-        doc.add(new FacetField("A", Integer.toString(i)));
-        indexWriter.addDocument(config.build(taxoWriter, doc));
-      } else {
-        indexWriter.addDocument(doc);
-      }
-    }
-    
-    indexWriter.commit();
-  }
-  
-  public void testSegmentsWithoutCategoriesOrResults() throws Exception {
-    // tests the accumulator when there are segments with no results
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges
-    IndexWriter indexWriter = new IndexWriter(indexDir, iwc);
-
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetsConfig config = new FacetsConfig();
-    indexTwoDocs(taxoWriter, indexWriter, config, false); // 1st segment, no content, with categories
-    indexTwoDocs(taxoWriter, indexWriter, null, true);         // 2nd segment, with content, no categories
-    indexTwoDocs(taxoWriter, indexWriter, config, true);  // 3rd segment ok
-    indexTwoDocs(taxoWriter, indexWriter, null, false);        // 4th segment, no content, or categories
-    indexTwoDocs(taxoWriter, indexWriter, null, true);         // 5th segment, with content, no categories
-    indexTwoDocs(taxoWriter, indexWriter, config, true);  // 6th segment, with content, with categories
-    indexTwoDocs(taxoWriter, indexWriter, null, true);         // 7th segment, with content, no categories
-    IOUtils.close(indexWriter, taxoWriter);
-
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher indexSearcher = newSearcher(indexReader);
-    
-    // search for "f:a", only segments 1 and 3 should match results
-    Query q = new TermQuery(new Term("f", "a"));
-    FacetsCollector sfc = new FacetsCollector();
-    indexSearcher.search(q, sfc);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
-    FacetResult result = facets.getTopChildren(10, "A");
-    assertEquals("wrong number of children", 2, result.labelValues.length);
-    for (LabelAndValue labelValue : result.labelValues) {
-      assertEquals("wrong weight for child " + labelValue.label, 2, labelValue.value.intValue());
-    }
-
-    IOUtils.close(indexReader, taxoReader, indexDir, taxoDir);
-  }
-
-  public void testRandom() throws Exception {
-    String[] tokens = getRandomTokens(10);
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    RandomIndexWriter w = new RandomIndexWriter(random(), indexDir);
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-    FacetsConfig config = new FacetsConfig();
-    int numDocs = atLeast(1000);
-    int numDims = _TestUtil.nextInt(random(), 1, 7);
-    List<TestDoc> testDocs = getRandomDocs(tokens, numDocs, numDims);
-    for(TestDoc testDoc : testDocs) {
-      Document doc = new Document();
-      doc.add(newStringField("content", testDoc.content, Field.Store.NO));
-      for(int j=0;j<numDims;j++) {
-        if (testDoc.dims[j] != null) {
-          doc.add(new FacetField("dim" + j, testDoc.dims[j]));
-        }
-      }
-      w.addDocument(config.build(tw, doc));
-    }
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(w.getReader());
-    
-    // NRT open
-    TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
-
-    int iters = atLeast(100);
-    for(int iter=0;iter<iters;iter++) {
-      String searchToken = tokens[random().nextInt(tokens.length)];
-      if (VERBOSE) {
-        System.out.println("\nTEST: iter content=" + searchToken);
-      }
-      FacetsCollector fc = new FacetsCollector();
-      TopDocs hits = FacetsCollector.search(searcher, new TermQuery(new Term("content", searchToken)), 10, fc);
-      Facets facets = getTaxonomyFacetCounts(tr, config, fc);
-
-      // Slow, yet hopefully bug-free, faceting:
-      @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Integer>[] expectedCounts = new HashMap[numDims];
-      for(int i=0;i<numDims;i++) {
-        expectedCounts[i] = new HashMap<String,Integer>();
-      }
-
-      for(TestDoc doc : testDocs) {
-        if (doc.content.equals(searchToken)) {
-          for(int j=0;j<numDims;j++) {
-            if (doc.dims[j] != null) {
-              Integer v = expectedCounts[j].get(doc.dims[j]);
-              if (v == null) {
-                expectedCounts[j].put(doc.dims[j], 1);
-              } else {
-                expectedCounts[j].put(doc.dims[j], v.intValue() + 1);
-              }
-            }
-          }
-        }
-      }
-
-      List<FacetResult> expected = new ArrayList<FacetResult>();
-      for(int i=0;i<numDims;i++) {
-        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
-        int totCount = 0;
-        for(Map.Entry<String,Integer> ent : expectedCounts[i].entrySet()) {
-          labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
-          totCount += ent.getValue();
-        }
-        sortLabelValues(labelValues);
-        if (totCount > 0) {
-          expected.add(new FacetResult("dim" + i, new String[0], totCount, labelValues.toArray(new LabelAndValue[labelValues.size()]), labelValues.size()));
-        }
-      }
-
-      // Sort by highest value, tie break by value:
-      sortFacetResults(expected);
-
-      List<FacetResult> actual = facets.getAllDims(10);
-
-      // Messy: fixup ties
-      sortTies(actual);
-
-      assertEquals(expected, actual);
-    }
-
-    IOUtils.close(w, tw, searcher.getIndexReader(), tr, indexDir, taxoDir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts2.java b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts2.java
deleted file mode 100644
index 4563d5c..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts2.java
+++ /dev/null
@@ -1,366 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestTaxonomyFacetCounts2 extends FacetTestCase {
-  
-  private static final Term A = new Term("f", "a");
-  private static final String CP_A = "A", CP_B = "B";
-  private static final String CP_C = "C", CP_D = "D"; // indexed w/ NO_PARENTS
-  private static final int NUM_CHILDREN_CP_A = 5, NUM_CHILDREN_CP_B = 3;
-  private static final int NUM_CHILDREN_CP_C = 5, NUM_CHILDREN_CP_D = 5;
-  private static final FacetField[] CATEGORIES_A, CATEGORIES_B;
-  private static final FacetField[] CATEGORIES_C, CATEGORIES_D;
-  static {
-    CATEGORIES_A = new FacetField[NUM_CHILDREN_CP_A];
-    for (int i = 0; i < NUM_CHILDREN_CP_A; i++) {
-      CATEGORIES_A[i] = new FacetField(CP_A, Integer.toString(i));
-    }
-    CATEGORIES_B = new FacetField[NUM_CHILDREN_CP_B];
-    for (int i = 0; i < NUM_CHILDREN_CP_B; i++) {
-      CATEGORIES_B[i] = new FacetField(CP_B, Integer.toString(i));
-    }
-    
-    // NO_PARENTS categories
-    CATEGORIES_C = new FacetField[NUM_CHILDREN_CP_C];
-    for (int i = 0; i < NUM_CHILDREN_CP_C; i++) {
-      CATEGORIES_C[i] = new FacetField(CP_C, Integer.toString(i));
-    }
-    
-    // Multi-level categories
-    CATEGORIES_D = new FacetField[NUM_CHILDREN_CP_D];
-    for (int i = 0; i < NUM_CHILDREN_CP_D; i++) {
-      String val = Integer.toString(i);
-      CATEGORIES_D[i] = new FacetField(CP_D, val, val + val); // e.g. D/1/11, D/2/22...
-    }
-  }
-  
-  private static Directory indexDir, taxoDir;
-  private static Map<String,Integer> allExpectedCounts, termExpectedCounts;
-
-  @AfterClass
-  public static void afterClassCountingFacetsAggregatorTest() throws Exception {
-    IOUtils.close(indexDir, taxoDir); 
-  }
-  
-  private static List<FacetField> randomCategories(Random random) {
-    // add random categories from the two dimensions, ensuring that the same
-    // category is not added twice.
-    int numFacetsA = random.nextInt(3) + 1; // 1-3
-    int numFacetsB = random.nextInt(2) + 1; // 1-2
-    ArrayList<FacetField> categories_a = new ArrayList<FacetField>();
-    categories_a.addAll(Arrays.asList(CATEGORIES_A));
-    ArrayList<FacetField> categories_b = new ArrayList<FacetField>();
-    categories_b.addAll(Arrays.asList(CATEGORIES_B));
-    Collections.shuffle(categories_a, random);
-    Collections.shuffle(categories_b, random);
-
-    ArrayList<FacetField> categories = new ArrayList<FacetField>();
-    categories.addAll(categories_a.subList(0, numFacetsA));
-    categories.addAll(categories_b.subList(0, numFacetsB));
-    
-    // add the NO_PARENT categories
-    categories.add(CATEGORIES_C[random().nextInt(NUM_CHILDREN_CP_C)]);
-    categories.add(CATEGORIES_D[random().nextInt(NUM_CHILDREN_CP_D)]);
-
-    return categories;
-  }
-
-  private static void addField(Document doc) {
-    doc.add(new StringField(A.field(), A.text(), Store.NO));
-  }
-
-  private static void addFacets(Document doc, FacetsConfig config, boolean updateTermExpectedCounts) 
-      throws IOException {
-    List<FacetField> docCategories = randomCategories(random());
-    for (FacetField ff : docCategories) {
-      doc.add(ff);
-      String cp = ff.dim + "/" + ff.path[0];
-      allExpectedCounts.put(cp, allExpectedCounts.get(cp) + 1);
-      if (updateTermExpectedCounts) {
-        termExpectedCounts.put(cp, termExpectedCounts.get(cp) + 1);
-      }
-    }
-    // add 1 to each NO_PARENTS dimension
-    allExpectedCounts.put(CP_B, allExpectedCounts.get(CP_B) + 1);
-    allExpectedCounts.put(CP_C, allExpectedCounts.get(CP_C) + 1);
-    allExpectedCounts.put(CP_D, allExpectedCounts.get(CP_D) + 1);
-    if (updateTermExpectedCounts) {
-      termExpectedCounts.put(CP_B, termExpectedCounts.get(CP_B) + 1);
-      termExpectedCounts.put(CP_C, termExpectedCounts.get(CP_C) + 1);
-      termExpectedCounts.put(CP_D, termExpectedCounts.get(CP_D) + 1);
-    }
-  }
-
-  private static FacetsConfig getConfig() {
-    FacetsConfig config = new FacetsConfig();
-    config.setMultiValued("A", true);
-    config.setMultiValued("B", true);
-    config.setRequireDimCount("B", true);
-    config.setHierarchical("D", true);
-    return config;
-  }
-
-  private static void indexDocsNoFacets(IndexWriter indexWriter) throws IOException {
-    int numDocs = atLeast(2);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      addField(doc);
-      indexWriter.addDocument(doc);
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  private static void indexDocsWithFacetsNoTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
-                                                 Map<String,Integer> expectedCounts) throws IOException {
-    Random random = random();
-    int numDocs = atLeast(random, 2);
-    FacetsConfig config = getConfig();
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      addFacets(doc, config, false);
-      indexWriter.addDocument(config.build(taxoWriter, doc));
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  private static void indexDocsWithFacetsAndTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
-                                                  Map<String,Integer> expectedCounts) throws IOException {
-    Random random = random();
-    int numDocs = atLeast(random, 2);
-    FacetsConfig config = getConfig();
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      addFacets(doc, config, true);
-      addField(doc);
-      indexWriter.addDocument(config.build(taxoWriter, doc));
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  private static void indexDocsWithFacetsAndSomeTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
-                                                      Map<String,Integer> expectedCounts) throws IOException {
-    Random random = random();
-    int numDocs = atLeast(random, 2);
-    FacetsConfig config = getConfig();
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      boolean hasContent = random.nextBoolean();
-      if (hasContent) {
-        addField(doc);
-      }
-      addFacets(doc, config, hasContent);
-      indexWriter.addDocument(config.build(taxoWriter, doc));
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  // initialize expectedCounts w/ 0 for all categories
-  private static Map<String,Integer> newCounts() {
-    Map<String,Integer> counts = new HashMap<String,Integer>();
-    counts.put(CP_A, 0);
-    counts.put(CP_B, 0);
-    counts.put(CP_C, 0);
-    counts.put(CP_D, 0);
-    for (FacetField ff : CATEGORIES_A) {
-      counts.put(ff.dim + "/" + ff.path[0], 0);
-    }
-    for (FacetField ff : CATEGORIES_B) {
-      counts.put(ff.dim + "/" + ff.path[0], 0);
-    }
-    for (FacetField ff : CATEGORIES_C) {
-      counts.put(ff.dim + "/" + ff.path[0], 0);
-    }
-    for (FacetField ff : CATEGORIES_D) {
-      counts.put(ff.dim + "/" + ff.path[0], 0);
-    }
-    return counts;
-  }
-  
-  @BeforeClass
-  public static void beforeClassCountingFacetsAggregatorTest() throws Exception {
-    indexDir = newDirectory();
-    taxoDir = newDirectory();
-    
-    // create an index which has:
-    // 1. Segment with no categories, but matching results
-    // 2. Segment w/ categories, but no results
-    // 3. Segment w/ categories and results
-    // 4. Segment w/ categories, but only some results
-    
-    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges, so we can control the index segments
-    IndexWriter indexWriter = new IndexWriter(indexDir, conf);
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-
-    allExpectedCounts = newCounts();
-    termExpectedCounts = newCounts();
-    
-    // segment w/ no categories
-    indexDocsNoFacets(indexWriter);
-
-    // segment w/ categories, no content
-    indexDocsWithFacetsNoTerms(indexWriter, taxoWriter, allExpectedCounts);
-
-    // segment w/ categories and content
-    indexDocsWithFacetsAndTerms(indexWriter, taxoWriter, allExpectedCounts);
-    
-    // segment w/ categories and some content
-    indexDocsWithFacetsAndSomeTerms(indexWriter, taxoWriter, allExpectedCounts);
-    
-    IOUtils.close(indexWriter, taxoWriter);
-  }
-  
-  @Test
-  public void testDifferentNumResults() throws Exception {
-    // test the collector w/ FacetRequests and different numResults
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    FacetsCollector sfc = new FacetsCollector();
-    TermQuery q = new TermQuery(A);
-    searcher.search(q, sfc);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
-    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_A, CP_A);
-    assertEquals(-1, result.value.intValue());
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(termExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
-    }
-    result = facets.getTopChildren(NUM_CHILDREN_CP_B, CP_B);
-    assertEquals(termExpectedCounts.get(CP_B), result.value);
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(termExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  @Test
-  public void testAllCounts() throws Exception {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    FacetsCollector sfc = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), sfc);
-
-    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
-    
-    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_A, CP_A);
-    assertEquals(-1, result.value.intValue());
-    int prevValue = Integer.MAX_VALUE;
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
-      assertTrue("wrong sort order of sub results: labelValue.value=" + labelValue.value + " prevValue=" + prevValue, labelValue.value.intValue() <= prevValue);
-      prevValue = labelValue.value.intValue();
-    }
-
-    result = facets.getTopChildren(NUM_CHILDREN_CP_B, CP_B);
-    assertEquals(allExpectedCounts.get(CP_B), result.value);
-    prevValue = Integer.MAX_VALUE;
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
-      assertTrue("wrong sort order of sub results: labelValue.value=" + labelValue.value + " prevValue=" + prevValue, labelValue.value.intValue() <= prevValue);
-      prevValue = labelValue.value.intValue();
-    }
-
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  @Test
-  public void testBigNumResults() throws Exception {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    FacetsCollector sfc = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), sfc);
-
-    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
-
-    FacetResult result = facets.getTopChildren(Integer.MAX_VALUE, CP_A);
-    assertEquals(-1, result.value.intValue());
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
-    }
-    result = facets.getTopChildren(Integer.MAX_VALUE, CP_B);
-    assertEquals(allExpectedCounts.get(CP_B), result.value);
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  @Test
-  public void testNoParents() throws Exception {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    FacetsCollector sfc = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), sfc);
-
-    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
-
-    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_C, CP_C);
-    assertEquals(allExpectedCounts.get(CP_C), result.value);
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_C + "/" + labelValue.label), labelValue.value);
-    }
-    result = facets.getTopChildren(NUM_CHILDREN_CP_D, CP_D);
-    assertEquals(allExpectedCounts.get(CP_C), result.value);
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_D + "/" + labelValue.label), labelValue.value);
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetSumValueSource.java b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetSumValueSource.java
deleted file mode 100644
index 5a5ecd3..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetSumValueSource.java
+++ /dev/null
@@ -1,516 +0,0 @@
-package org.apache.lucene.facet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.IntField;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queries.function.FunctionQuery;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
-import org.apache.lucene.queries.function.valuesource.DoubleFieldSource;
-import org.apache.lucene.queries.function.valuesource.FloatFieldSource;
-import org.apache.lucene.queries.function.valuesource.IntFieldSource;
-import org.apache.lucene.queries.function.valuesource.LongFieldSource;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.TopScoreDocCollector;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
-
-  public void testBasic() throws Exception {
-
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    FacetsConfig config = new FacetsConfig();
-
-    // Reused across documents, to add the necessary facet
-    // fields:
-    Document doc = new Document();
-    doc.add(new IntField("num", 10, Field.Store.NO));
-    doc.add(new FacetField("Author", "Bob"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    doc = new Document();
-    doc.add(new IntField("num", 20, Field.Store.NO));
-    doc.add(new FacetField("Author", "Lisa"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    doc = new Document();
-    doc.add(new IntField("num", 30, Field.Store.NO));
-    doc.add(new FacetField("Author", "Lisa"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    doc = new Document();
-    doc.add(new IntField("num", 40, Field.Store.NO));
-    doc.add(new FacetField("Author", "Susan"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    doc = new Document();
-    doc.add(new IntField("num", 45, Field.Store.NO));
-    doc.add(new FacetField("Author", "Frank"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Aggregate the facet counts:
-    FacetsCollector c = new FacetsCollector();
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query and one of the
-    // Facets.search utility methods:
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
-
-    // Retrieve & verify results:
-    assertEquals("dim=Author path=[] value=145.0 childCount=4\n  Lisa (50.0)\n  Frank (45.0)\n  Susan (40.0)\n  Bob (10.0)\n", facets.getTopChildren(10, "Author").toString());
-
-    taxoReader.close();
-    searcher.getIndexReader().close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  // LUCENE-5333
-  public void testSparseFacets() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(new IntField("num", 10, Field.Store.NO));
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new IntField("num", 20, Field.Store.NO));
-    doc.add(new FacetField("a", "foo2"));
-    doc.add(new FacetField("b", "bar1"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new IntField("num", 30, Field.Store.NO));
-    doc.add(new FacetField("a", "foo3"));
-    doc.add(new FacetField("b", "bar2"));
-    doc.add(new FacetField("c", "baz1"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    FacetsCollector c = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<FacetResult> results = facets.getAllDims(10);
-
-    assertEquals(3, results.size());
-    assertEquals("dim=a path=[] value=60.0 childCount=3\n  foo3 (30.0)\n  foo2 (20.0)\n  foo1 (10.0)\n", results.get(0).toString());
-    assertEquals("dim=b path=[] value=50.0 childCount=2\n  bar2 (30.0)\n  bar1 (20.0)\n", results.get(1).toString());
-    assertEquals("dim=c path=[] value=30.0 childCount=1\n  baz1 (30.0)\n", results.get(2).toString());
-
-    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-
-  public void testWrongIndexFieldName() throws Exception {
-
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig();
-    config.setIndexFieldName("a", "$facets2");
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new IntField("num", 10, Field.Store.NO));
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(config.build(taxoWriter, doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    FacetsCollector c = new FacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, config, c, new IntFieldSource("num"));
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<FacetResult> results = facets.getAllDims(10);
-    assertTrue(results.isEmpty());
-
-    try {
-      facets.getSpecificValue("a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    try {
-      facets.getTopChildren(10, "a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-
-  public void testSumScoreAggregator() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-
-    FacetsConfig config = new FacetsConfig();
-
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      if (random().nextBoolean()) { // don't match all documents
-        doc.add(new StringField("f", "v", Field.Store.NO));
-      }
-      doc.add(new FacetField("dim", "a"));
-      iw.addDocument(config.build(taxoWriter, doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    FacetsCollector fc = new FacetsCollector(true);
-    ConstantScoreQuery csq = new ConstantScoreQuery(new MatchAllDocsQuery());
-    csq.setBoost(2.0f);
-    
-    TopDocs td = FacetsCollector.search(newSearcher(r), csq, 10, fc);
-
-    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, fc, new TaxonomyFacetSumValueSource.ScoreValueSource());
-    
-    int expected = (int) (td.getMaxScore() * td.totalHits);
-    assertEquals(expected, facets.getSpecificValue("dim", "a").intValue());
-    
-    IOUtils.close(iw, taxoWriter, taxoReader, taxoDir, r, indexDir);
-  }
-  
-  public void testNoScore() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig();
-    for (int i = 0; i < 4; i++) {
-      Document doc = new Document();
-      doc.add(new NumericDocValuesField("price", (i+1)));
-      doc.add(new FacetField("a", Integer.toString(i % 2)));
-      iw.addDocument(config.build(taxoWriter, doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    FacetsCollector sfc = new FacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, new LongFieldSource("price"));
-    assertEquals("dim=a path=[] value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
-    
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  public void testWithScore() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-
-    FacetsConfig config = new FacetsConfig();
-    for (int i = 0; i < 4; i++) {
-      Document doc = new Document();
-      doc.add(new NumericDocValuesField("price", (i+1)));
-      doc.add(new FacetField("a", Integer.toString(i % 2)));
-      iw.addDocument(config.build(taxoWriter, doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    ValueSource valueSource = new ValueSource() {
-      @Override
-      public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
-        final Scorer scorer = (Scorer) context.get("scorer");
-        assert scorer != null;
-        return new DoubleDocValues(this) {
-          @Override
-          public double doubleVal(int document) {
-            try {
-              return scorer.score();
-            } catch (IOException exception) {
-              throw new RuntimeException(exception);
-            }
-          }
-        };
-      }
-
-      @Override public boolean equals(Object o) { return o == this; }
-      @Override public int hashCode() { return System.identityHashCode(this); }
-      @Override public String description() { return "score()"; }
-    };
-    
-    FacetsCollector fc = new FacetsCollector(true);
-    TopScoreDocCollector tsdc = TopScoreDocCollector.create(10, true);
-    // score documents by their 'price' field - makes asserting the correct counts for the categories easier
-    Query q = new FunctionQuery(new LongFieldSource("price"));
-    FacetsCollector.search(newSearcher(r), q, 10, fc);
-    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, fc, valueSource);
-    
-    assertEquals("dim=a path=[] value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
-    
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  public void testRollupValues() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig();
-    config.setHierarchical("a", true);
-    //config.setRequireDimCount("a", true);
-    
-    for (int i = 0; i < 4; i++) {
-      Document doc = new Document();
-      doc.add(new NumericDocValuesField("price", (i+1)));
-      doc.add(new FacetField("a", Integer.toString(i % 2), "1"));
-      iw.addDocument(config.build(taxoWriter, doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    ValueSource valueSource = new LongFieldSource("price");
-    FacetsCollector sfc = new FacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, valueSource);
-    
-    assertEquals("dim=a path=[] value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
-    
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  public void testCountAndSumScore() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig();
-    config.setIndexFieldName("b", "$b");
-    
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      doc.add(new StringField("f", "v", Field.Store.NO));
-      doc.add(new FacetField("a", "1"));
-      doc.add(new FacetField("b", "1"));
-      iw.addDocument(config.build(taxoWriter, doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    FacetsCollector fc = new FacetsCollector(true);
-    TopDocs hits = FacetsCollector.search(newSearcher(r), new MatchAllDocsQuery(), 10, fc);
-    
-    Facets facets1 = getTaxonomyFacetCounts(taxoReader, config, fc);
-    Facets facets2 = new TaxonomyFacetSumValueSource(new DocValuesOrdinalsReader("$b"), taxoReader, config, fc, new TaxonomyFacetSumValueSource.ScoreValueSource());
-
-    assertEquals(r.maxDoc(), facets1.getTopChildren(10, "a").value.intValue());
-    double expected = hits.getMaxScore() * r.numDocs();
-    assertEquals(r.maxDoc(), facets2.getTopChildren(10, "b").value.doubleValue(), 1E-10);
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  public void testRandom() throws Exception {
-    String[] tokens = getRandomTokens(10);
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    RandomIndexWriter w = new RandomIndexWriter(random(), indexDir);
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-    FacetsConfig config = new FacetsConfig();
-    int numDocs = atLeast(1000);
-    int numDims = _TestUtil.nextInt(random(), 1, 7);
-    List<TestDoc> testDocs = getRandomDocs(tokens, numDocs, numDims);
-    for(TestDoc testDoc : testDocs) {
-      Document doc = new Document();
-      doc.add(newStringField("content", testDoc.content, Field.Store.NO));
-      testDoc.value = random().nextFloat();
-      doc.add(new FloatDocValuesField("value", testDoc.value));
-      for(int j=0;j<numDims;j++) {
-        if (testDoc.dims[j] != null) {
-          doc.add(new FacetField("dim" + j, testDoc.dims[j]));
-        }
-      }
-      w.addDocument(config.build(tw, doc));
-    }
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(w.getReader());
-    
-    // NRT open
-    TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
-
-    ValueSource values = new FloatFieldSource("value");
-
-    int iters = atLeast(100);
-    for(int iter=0;iter<iters;iter++) {
-      String searchToken = tokens[random().nextInt(tokens.length)];
-      if (VERBOSE) {
-        System.out.println("\nTEST: iter content=" + searchToken);
-      }
-      FacetsCollector fc = new FacetsCollector();
-      TopDocs hits = FacetsCollector.search(searcher, new TermQuery(new Term("content", searchToken)), 10, fc);
-      Facets facets = new TaxonomyFacetSumValueSource(tr, config, fc, values);
-
-      // Slow, yet hopefully bug-free, faceting:
-      @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Float>[] expectedValues = new HashMap[numDims];
-      for(int i=0;i<numDims;i++) {
-        expectedValues[i] = new HashMap<String,Float>();
-      }
-
-      for(TestDoc doc : testDocs) {
-        if (doc.content.equals(searchToken)) {
-          for(int j=0;j<numDims;j++) {
-            if (doc.dims[j] != null) {
-              Float v = expectedValues[j].get(doc.dims[j]);
-              if (v == null) {
-                expectedValues[j].put(doc.dims[j], doc.value);
-              } else {
-                expectedValues[j].put(doc.dims[j], v.floatValue() + doc.value);
-              }
-            }
-          }
-        }
-      }
-
-      List<FacetResult> expected = new ArrayList<FacetResult>();
-      for(int i=0;i<numDims;i++) {
-        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
-        double totValue = 0;
-        for(Map.Entry<String,Float> ent : expectedValues[i].entrySet()) {
-          labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
-          totValue += ent.getValue();
-        }
-        sortLabelValues(labelValues);
-        if (totValue > 0) {
-          expected.add(new FacetResult("dim" + i, new String[0], totValue, labelValues.toArray(new LabelAndValue[labelValues.size()]), labelValues.size()));
-        }
-      }
-
-      // Sort by highest value, tie break by value:
-      sortFacetResults(expected);
-
-      List<FacetResult> actual = facets.getAllDims(10);
-
-      // Messy: fixup ties
-      sortTies(actual);
-
-      if (VERBOSE) {
-        System.out.println("expected=\n" + expected.toString());
-        System.out.println("actual=\n" + actual.toString());
-      }
-
-      assertFloatValuesEquals(expected, actual);
-    }
-
-    IOUtils.close(w, tw, searcher.getIndexReader(), tr, indexDir, taxoDir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java b/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java
new file mode 100644
index 0000000..d5a2343
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeFacetCounts.java
@@ -0,0 +1,814 @@
+package org.apache.lucene.facet.range;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.DoubleDocValuesField;
+import org.apache.lucene.document.DoubleField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FloatDocValuesField;
+import org.apache.lucene.document.FloatField;
+import org.apache.lucene.document.LongField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.DrillSideways;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.LabelAndValue;
+import org.apache.lucene.facet.MultiFacets;
+import org.apache.lucene.facet.DrillSideways.DrillSidewaysResult;
+import org.apache.lucene.facet.range.DoubleRange;
+import org.apache.lucene.facet.range.DoubleRangeFacetCounts;
+import org.apache.lucene.facet.range.LongRange;
+import org.apache.lucene.facet.range.LongRangeFacetCounts;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
+import org.apache.lucene.queries.function.valuesource.FloatFieldSource;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.NumericRangeQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+
+public class TestRangeFacetCounts extends FacetTestCase {
+
+  public void testBasicLong() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      field.setLongValue(l);
+      w.addDocument(doc);
+    }
+
+    // Also add Long.MAX_VALUE
+    field.setLongValue(Long.MAX_VALUE);
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+    w.close();
+
+    FacetsCollector fc = new FacetsCollector();
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new LongRangeFacetCounts("field", fc,
+        new LongRange("less than 10", 0L, true, 10L, false),
+        new LongRange("less than or equal to 10", 0L, true, 10L, true),
+        new LongRange("over 90", 90L, false, 100L, false),
+        new LongRange("90 or above", 90L, true, 100L, false),
+        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, true));
+
+    FacetResult result = facets.getTopChildren(10, "field");
+    assertEquals("dim=field path=[] value=22 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (1)\n",
+                 result.toString());
+    
+    r.close();
+    d.close();
+  }
+
+  @SuppressWarnings("unused")
+  public void testUselessRange() {
+    try {
+      new LongRange("useless", 7, true, 6, true);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    try {
+      new LongRange("useless", 7, true, 7, false);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    try {
+      new DoubleRange("useless", 7.0, true, 6.0, true);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    try {
+      new DoubleRange("useless", 7.0, true, 7.0, false);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+  }
+
+  public void testLongMinMax() throws Exception {
+
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
+    doc.add(field);
+    field.setLongValue(Long.MIN_VALUE);
+    w.addDocument(doc);
+    field.setLongValue(0);
+    w.addDocument(doc);
+    field.setLongValue(Long.MAX_VALUE);
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+    w.close();
+
+    FacetsCollector fc = new FacetsCollector();
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new LongRangeFacetCounts("field", fc,
+        new LongRange("min", Long.MIN_VALUE, true, Long.MIN_VALUE, true),
+        new LongRange("max", Long.MAX_VALUE, true, Long.MAX_VALUE, true),
+        new LongRange("all0", Long.MIN_VALUE, true, Long.MAX_VALUE, true),
+        new LongRange("all1", Long.MIN_VALUE, false, Long.MAX_VALUE, true),
+        new LongRange("all2", Long.MIN_VALUE, true, Long.MAX_VALUE, false),
+        new LongRange("all3", Long.MIN_VALUE, false, Long.MAX_VALUE, false));
+
+    FacetResult result = facets.getTopChildren(10, "field");
+    assertEquals("dim=field path=[] value=3 childCount=6\n  min (1)\n  max (1)\n  all0 (3)\n  all1 (2)\n  all2 (2)\n  all3 (1)\n",
+                 result.toString());
+    
+    r.close();
+    d.close();
+  }
+
+  public void testOverlappedEndStart() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      field.setLongValue(l);
+      w.addDocument(doc);
+    }
+    field.setLongValue(Long.MAX_VALUE);
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+    w.close();
+
+    FacetsCollector fc = new FacetsCollector();
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new LongRangeFacetCounts("field", fc,
+        new LongRange("0-10", 0L, true, 10L, true),
+        new LongRange("10-20", 10L, true, 20L, true),
+        new LongRange("20-30", 20L, true, 30L, true),
+        new LongRange("30-40", 30L, true, 40L, true));
+    
+    FacetResult result = facets.getTopChildren(10, "field");
+    assertEquals("dim=field path=[] value=41 childCount=4\n  0-10 (11)\n  10-20 (11)\n  20-30 (11)\n  30-40 (11)\n",
+                 result.toString());
+    
+    r.close();
+    d.close();
+  }
+
+  /** Tests single request that mixes Range and non-Range
+   *  faceting, with DrillSideways and taxonomy. */
+  public void testMixedRangeAndNonRangeTaxonomy() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Directory td = newDirectory();
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+
+    for (long l = 0; l < 100; l++) {
+      Document doc = new Document();
+      // For computing range facet counts:
+      doc.add(new NumericDocValuesField("field", l));
+      // For drill down by numeric range:
+      doc.add(new LongField("field", l, Field.Store.NO));
+
+      if ((l&3) == 0) {
+        doc.add(new FacetField("dim", "a"));
+      } else {
+        doc.add(new FacetField("dim", "b"));
+      }
+      w.addDocument(config.build(tw, doc));
+    }
+
+    final IndexReader r = w.getReader();
+
+    final TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
+
+    IndexSearcher s = newSearcher(r);
+
+    DrillSideways ds = new DrillSideways(s, config, tr) {
+
+        @Override
+        protected Facets buildFacetsResult(FacetsCollector drillDowns, FacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {        
+          FacetsCollector dimFC = drillDowns;
+          FacetsCollector fieldFC = drillDowns;
+          if (drillSideways != null) {
+            for(int i=0;i<drillSideways.length;i++) {
+              String dim = drillSidewaysDims[i];
+              if (dim.equals("field")) {
+                fieldFC = drillSideways[i];
+              } else {
+                dimFC = drillSideways[i];
+              }
+            }
+          }
+
+          Map<String,Facets> byDim = new HashMap<String,Facets>();
+          byDim.put("field",
+                    new LongRangeFacetCounts("field", fieldFC,
+                          new LongRange("less than 10", 0L, true, 10L, false),
+                          new LongRange("less than or equal to 10", 0L, true, 10L, true),
+                          new LongRange("over 90", 90L, false, 100L, false),
+                          new LongRange("90 or above", 90L, true, 100L, false),
+                          new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false)));
+          byDim.put("dim", getTaxonomyFacetCounts(taxoReader, config, dimFC));
+          return new MultiFacets(byDim, null);
+        }
+
+        @Override
+        protected boolean scoreSubDocsAtOnce() {
+          return random().nextBoolean();
+        }
+      };
+
+    // First search, no drill downs:
+    DrillDownQuery ddq = new DrillDownQuery(config);
+    DrillSidewaysResult dsr = ds.search(null, ddq, 10);
+
+    assertEquals(100, dsr.hits.totalHits);
+    assertEquals("dim=dim path=[] value=100 childCount=2\n  b (75)\n  a (25)\n", dsr.facets.getTopChildren(10, "dim").toString());
+    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 dsr.facets.getTopChildren(10, "field").toString());
+
+    // Second search, drill down on dim=b:
+    ddq = new DrillDownQuery(config);
+    ddq.add("dim", "b");
+    dsr = ds.search(null, ddq, 10);
+
+    assertEquals(75, dsr.hits.totalHits);
+    assertEquals("dim=dim path=[] value=100 childCount=2\n  b (75)\n  a (25)\n", dsr.facets.getTopChildren(10, "dim").toString());
+    assertEquals("dim=field path=[] value=16 childCount=5\n  less than 10 (7)\n  less than or equal to 10 (8)\n  over 90 (7)\n  90 or above (8)\n  over 1000 (0)\n",
+                 dsr.facets.getTopChildren(10, "field").toString());
+
+    // Third search, drill down on "less than or equal to 10":
+    ddq = new DrillDownQuery(config);
+    ddq.add("field", NumericRangeQuery.newLongRange("field", 0L, 10L, true, true));
+    dsr = ds.search(null, ddq, 10);
+
+    assertEquals(11, dsr.hits.totalHits);
+    assertEquals("dim=dim path=[] value=11 childCount=2\n  b (8)\n  a (3)\n", dsr.facets.getTopChildren(10, "dim").toString());
+    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 dsr.facets.getTopChildren(10, "field").toString());
+    IOUtils.close(tw, tr, td, w, r, d);
+  }
+
+  public void testBasicDouble() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    DoubleDocValuesField field = new DoubleDocValuesField("field", 0.0);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      field.setDoubleValue(l);
+      w.addDocument(doc);
+    }
+
+    IndexReader r = w.getReader();
+
+    FacetsCollector fc = new FacetsCollector();
+
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+    Facets facets = new DoubleRangeFacetCounts("field", fc,
+        new DoubleRange("less than 10", 0.0, true, 10.0, false),
+        new DoubleRange("less than or equal to 10", 0.0, true, 10.0, true),
+        new DoubleRange("over 90", 90.0, false, 100.0, false),
+        new DoubleRange("90 or above", 90.0, true, 100.0, false),
+        new DoubleRange("over 1000", 1000.0, false, Double.POSITIVE_INFINITY, false));
+                                         
+    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 facets.getTopChildren(10, "field").toString());
+
+    IOUtils.close(w, r, d);
+  }
+
+  public void testBasicFloat() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    FloatDocValuesField field = new FloatDocValuesField("field", 0.0f);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      field.setFloatValue(l);
+      w.addDocument(doc);
+    }
+
+    IndexReader r = w.getReader();
+
+    FacetsCollector fc = new FacetsCollector();
+
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new DoubleRangeFacetCounts("field", new FloatFieldSource("field"), fc,
+        new DoubleRange("less than 10", 0.0f, true, 10.0f, false),
+        new DoubleRange("less than or equal to 10", 0.0f, true, 10.0f, true),
+        new DoubleRange("over 90", 90.0f, false, 100.0f, false),
+        new DoubleRange("90 or above", 90.0f, true, 100.0f, false),
+        new DoubleRange("over 1000", 1000.0f, false, Double.POSITIVE_INFINITY, false));
+    
+    assertEquals("dim=field path=[] value=21 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 facets.getTopChildren(10, "field").toString());
+    
+    IOUtils.close(w, r, d);
+  }
+
+  public void testRandomLongs() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+
+    int numDocs = atLeast(1000);
+    if (VERBOSE) {
+      System.out.println("TEST: numDocs=" + numDocs);
+    }
+    long[] values = new long[numDocs];
+    for(int i=0;i<numDocs;i++) {
+      Document doc = new Document();
+      long v = random().nextLong();
+      values[i] = v;
+      doc.add(new NumericDocValuesField("field", v));
+      doc.add(new LongField("field", v, Field.Store.NO));
+      w.addDocument(doc);
+    }
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+    FacetsConfig config = new FacetsConfig();
+    
+    int numIters = atLeast(10);
+    for(int iter=0;iter<numIters;iter++) {
+      if (VERBOSE) {
+        System.out.println("TEST: iter=" + iter);
+      }
+      int numRange = _TestUtil.nextInt(random(), 1, 100);
+      LongRange[] ranges = new LongRange[numRange];
+      int[] expectedCounts = new int[numRange];
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        long min;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          LongRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            min = prevRange.min;
+          } else {
+            min = prevRange.max;
+          }
+        } else {
+          min = random().nextLong();
+        }
+        long max;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          LongRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            max = prevRange.min;
+          } else {
+            max = prevRange.max;
+          }
+        } else {
+          max = random().nextLong();
+        }
+
+        if (min > max) {
+          long x = min;
+          min = max;
+          max = x;
+        }
+        boolean minIncl;
+        boolean maxIncl;
+        if (min == max) {
+          minIncl = true;
+          maxIncl = true;
+        } else {
+          minIncl = random().nextBoolean();
+          maxIncl = random().nextBoolean();
+        }
+        ranges[rangeID] = new LongRange("r" + rangeID, min, minIncl, max, maxIncl);
+        if (VERBOSE) {
+          System.out.println("  range " + rangeID + ": " + ranges[rangeID]);      
+        }
+
+        // Do "slow but hopefully correct" computation of
+        // expected count:
+        for(int i=0;i<numDocs;i++) {
+          boolean accept = true;
+          if (minIncl) {
+            accept &= values[i] >= min;
+          } else {
+            accept &= values[i] > min;
+          }
+          if (maxIncl) {
+            accept &= values[i] <= max;
+          } else {
+            accept &= values[i] < max;
+          }
+          if (accept) {
+            expectedCounts[rangeID]++;
+          }
+        }
+      }
+
+      FacetsCollector sfc = new FacetsCollector();
+      s.search(new MatchAllDocsQuery(), sfc);
+      Facets facets = new LongRangeFacetCounts("field", sfc, ranges);
+      FacetResult result = facets.getTopChildren(10, "field");
+      assertEquals(numRange, result.labelValues.length);
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        if (VERBOSE) {
+          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
+        }
+        LabelAndValue subNode = result.labelValues[rangeID];
+        assertEquals("r" + rangeID, subNode.label);
+        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
+
+        LongRange range = ranges[rangeID];
+
+        // Test drill-down:
+        DrillDownQuery ddq = new DrillDownQuery(config);
+        ddq.add("field", NumericRangeQuery.newLongRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
+        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
+      }
+    }
+
+    IOUtils.close(w, r, dir);
+  }
+
+  public void testRandomFloats() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+
+    int numDocs = atLeast(1000);
+    float[] values = new float[numDocs];
+    for(int i=0;i<numDocs;i++) {
+      Document doc = new Document();
+      float v = random().nextFloat();
+      values[i] = v;
+      doc.add(new FloatDocValuesField("field", v));
+      doc.add(new FloatField("field", v, Field.Store.NO));
+      w.addDocument(doc);
+    }
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+    FacetsConfig config = new FacetsConfig();
+    
+    int numIters = atLeast(10);
+    for(int iter=0;iter<numIters;iter++) {
+      if (VERBOSE) {
+        System.out.println("TEST: iter=" + iter);
+      }
+      int numRange = _TestUtil.nextInt(random(), 1, 5);
+      DoubleRange[] ranges = new DoubleRange[numRange];
+      int[] expectedCounts = new int[numRange];
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        double min;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            min = prevRange.min;
+          } else {
+            min = prevRange.max;
+          }
+        } else {
+          min = random().nextDouble();
+        }
+        double max;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            max = prevRange.min;
+          } else {
+            max = prevRange.max;
+          }
+        } else {
+          max = random().nextDouble();
+        }
+
+        if (min > max) {
+          double x = min;
+          min = max;
+          max = x;
+        }
+
+        boolean minIncl;
+        boolean maxIncl;
+        if (min == max) {
+          minIncl = true;
+          maxIncl = true;
+        } else {
+          minIncl = random().nextBoolean();
+          maxIncl = random().nextBoolean();
+        }
+        ranges[rangeID] = new DoubleRange("r" + rangeID, min, minIncl, max, maxIncl);
+
+        // Do "slow but hopefully correct" computation of
+        // expected count:
+        for(int i=0;i<numDocs;i++) {
+          boolean accept = true;
+          if (minIncl) {
+            accept &= values[i] >= min;
+          } else {
+            accept &= values[i] > min;
+          }
+          if (maxIncl) {
+            accept &= values[i] <= max;
+          } else {
+            accept &= values[i] < max;
+          }
+          if (accept) {
+            expectedCounts[rangeID]++;
+          }
+        }
+      }
+
+      FacetsCollector sfc = new FacetsCollector();
+      s.search(new MatchAllDocsQuery(), sfc);
+      Facets facets = new DoubleRangeFacetCounts("field", new FloatFieldSource("field"), sfc, ranges);
+      FacetResult result = facets.getTopChildren(10, "field");
+      assertEquals(numRange, result.labelValues.length);
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        if (VERBOSE) {
+          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
+        }
+        LabelAndValue subNode = result.labelValues[rangeID];
+        assertEquals("r" + rangeID, subNode.label);
+        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
+
+        DoubleRange range = ranges[rangeID];
+
+        // Test drill-down:
+        DrillDownQuery ddq = new DrillDownQuery(config);
+        ddq.add("field", NumericRangeQuery.newFloatRange("field", (float) range.min, (float) range.max, range.minInclusive, range.maxInclusive));
+        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
+      }
+    }
+
+    IOUtils.close(w, r, dir);
+  }
+
+  public void testRandomDoubles() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+
+    int numDocs = atLeast(1000);
+    double[] values = new double[numDocs];
+    for(int i=0;i<numDocs;i++) {
+      Document doc = new Document();
+      double v = random().nextDouble();
+      values[i] = v;
+      doc.add(new DoubleDocValuesField("field", v));
+      doc.add(new DoubleField("field", v, Field.Store.NO));
+      w.addDocument(doc);
+    }
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+    FacetsConfig config = new FacetsConfig();
+    
+    int numIters = atLeast(10);
+    for(int iter=0;iter<numIters;iter++) {
+      if (VERBOSE) {
+        System.out.println("TEST: iter=" + iter);
+      }
+      int numRange = _TestUtil.nextInt(random(), 1, 5);
+      DoubleRange[] ranges = new DoubleRange[numRange];
+      int[] expectedCounts = new int[numRange];
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        double min;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            min = prevRange.min;
+          } else {
+            min = prevRange.max;
+          }
+        } else {
+          min = random().nextDouble();
+        }
+        double max;
+        if (rangeID > 0 && random().nextInt(10) == 7) {
+          // Use an existing boundary:
+          DoubleRange prevRange = ranges[random().nextInt(rangeID)];
+          if (random().nextBoolean()) {
+            max = prevRange.min;
+          } else {
+            max = prevRange.max;
+          }
+        } else {
+          max = random().nextDouble();
+        }
+
+        if (min > max) {
+          double x = min;
+          min = max;
+          max = x;
+        }
+
+        boolean minIncl;
+        boolean maxIncl;
+        if (min == max) {
+          minIncl = true;
+          maxIncl = true;
+        } else {
+          minIncl = random().nextBoolean();
+          maxIncl = random().nextBoolean();
+        }
+        ranges[rangeID] = new DoubleRange("r" + rangeID, min, minIncl, max, maxIncl);
+
+        // Do "slow but hopefully correct" computation of
+        // expected count:
+        for(int i=0;i<numDocs;i++) {
+          boolean accept = true;
+          if (minIncl) {
+            accept &= values[i] >= min;
+          } else {
+            accept &= values[i] > min;
+          }
+          if (maxIncl) {
+            accept &= values[i] <= max;
+          } else {
+            accept &= values[i] < max;
+          }
+          if (accept) {
+            expectedCounts[rangeID]++;
+          }
+        }
+      }
+
+      FacetsCollector sfc = new FacetsCollector();
+      s.search(new MatchAllDocsQuery(), sfc);
+      Facets facets = new DoubleRangeFacetCounts("field", sfc, ranges);
+      FacetResult result = facets.getTopChildren(10, "field");
+      assertEquals(numRange, result.labelValues.length);
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        if (VERBOSE) {
+          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
+        }
+        LabelAndValue subNode = result.labelValues[rangeID];
+        assertEquals("r" + rangeID, subNode.label);
+        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
+
+        DoubleRange range = ranges[rangeID];
+
+        // Test drill-down:
+        DrillDownQuery ddq = new DrillDownQuery(config);
+        ddq.add("field", NumericRangeQuery.newDoubleRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
+        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
+      }
+    }
+
+    IOUtils.close(w, r, dir);
+  }
+
+  // LUCENE-5178
+  public void testMissingValues() throws Exception {
+    assumeTrue("codec does not support docsWithField", defaultCodecSupportsDocsWithField());
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      if (l % 5 == 0) {
+        // Every 5th doc is missing the value:
+        w.addDocument(new Document());
+        continue;
+      }
+      field.setLongValue(l);
+      w.addDocument(doc);
+    }
+
+    IndexReader r = w.getReader();
+
+    FacetsCollector fc = new FacetsCollector();
+
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+    Facets facets = new LongRangeFacetCounts("field", fc,
+        new LongRange("less than 10", 0L, true, 10L, false),
+        new LongRange("less than or equal to 10", 0L, true, 10L, true),
+        new LongRange("over 90", 90L, false, 100L, false),
+        new LongRange("90 or above", 90L, true, 100L, false),
+        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false));
+    
+    assertEquals("dim=field path=[] value=16 childCount=5\n  less than 10 (8)\n  less than or equal to 10 (8)\n  over 90 (8)\n  90 or above (8)\n  over 1000 (0)\n",
+                 facets.getTopChildren(10, "field").toString());
+
+    IOUtils.close(w, r, d);
+  }
+
+  public void testCustomDoublesValueSource() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    
+    Document doc = new Document();
+    writer.addDocument(doc);
+    
+    doc = new Document();
+    writer.addDocument(doc);
+    
+    doc = new Document();
+    writer.addDocument(doc);
+
+    writer.forceMerge(1);
+
+    ValueSource vs = new ValueSource() {
+        @SuppressWarnings("rawtypes")
+        @Override
+        public FunctionValues getValues(Map ignored, AtomicReaderContext ignored2) {
+          return new DoubleDocValues(null) {
+            @Override
+            public double doubleVal(int doc) {
+              return doc+1;
+            }
+          };
+        }
+
+        @Override
+        public boolean equals(Object o) {
+          throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public int hashCode() {
+          throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public String description() {
+          throw new UnsupportedOperationException();
+        }
+      };
+    
+    FacetsCollector fc = new FacetsCollector();
+
+    IndexReader r = writer.getReader();
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new DoubleRangeFacetCounts("field", vs, fc,
+        new DoubleRange("< 1", 0.0, true, 1.0, false),
+        new DoubleRange("< 2", 0.0, true, 2.0, false),
+        new DoubleRange("< 5", 0.0, true, 5.0, false),
+        new DoubleRange("< 10", 0.0, true, 10.0, false),
+        new DoubleRange("< 20", 0.0, true, 20.0, false),
+        new DoubleRange("< 50", 0.0, true, 50.0, false));
+
+    assertEquals("dim=field path=[] value=3 childCount=6\n  < 1 (0)\n  < 2 (1)\n  < 5 (3)\n  < 10 (3)\n  < 20 (3)\n  < 50 (3)\n", facets.getTopChildren(10, "field").toString());
+
+    // Test drill-down:
+    assertEquals(1, s.search(new ConstantScoreQuery(new DoubleRange("< 2", 0.0, true, 2.0, false).getFilter(vs)), 10).totalHits);
+
+    IOUtils.close(r, writer, dir);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java
new file mode 100644
index 0000000..b3f08f2
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java
@@ -0,0 +1,358 @@
+package org.apache.lucene.facet.sortedset;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.LabelAndValue;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestSortedSetDocValuesFacets extends FacetTestCase {
+
+  // NOTE: TestDrillSideways.testRandom also sometimes
+  // randomly uses SortedSetDV
+
+  public void testBasic() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    FacetsConfig config = new FacetsConfig();
+    config.setMultiValued("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
+    doc.add(new SortedSetDocValuesFacetField("a", "bar"));
+    doc.add(new SortedSetDocValuesFacetField("a", "zoo"));
+    doc.add(new SortedSetDocValuesFacetField("b", "baz"));
+    writer.addDocument(config.build(doc));
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+    
+    FacetsCollector c = new FacetsCollector();
+
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    assertEquals("dim=a path=[] value=4 childCount=3\n  foo (2)\n  bar (1)\n  zoo (1)\n", facets.getTopChildren(10, "a").toString());
+    assertEquals("dim=b path=[] value=1 childCount=1\n  baz (1)\n", facets.getTopChildren(10, "b").toString());
+
+    // DrillDown:
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("a", "foo");
+    q.add("b", "baz");
+    TopDocs hits = searcher.search(q, 1);
+    assertEquals(1, hits.totalHits);
+
+    IOUtils.close(writer, searcher.getIndexReader(), dir);
+  }
+
+  // LUCENE-5090
+  @SuppressWarnings("unused")
+  public void testStaleState() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
+    writer.addDocument(config.build(doc));
+
+    IndexReader r = writer.getReader();
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(r);
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "bar"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "baz"));
+    writer.addDocument(config.build(doc));
+
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    FacetsCollector c = new FacetsCollector();
+
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    try {
+      new SortedSetDocValuesFacetCounts(state, c);
+      fail("did not hit expected exception");
+    } catch (IllegalStateException ise) {
+      // expected
+    }
+
+    r.close();
+    writer.close();
+    searcher.getIndexReader().close();
+    dir.close();
+  }
+
+  // LUCENE-5333
+  public void testSparseFacets() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
+    doc.add(new SortedSetDocValuesFacetField("b", "bar1"));
+    writer.addDocument(config.build(doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo3"));
+    doc.add(new SortedSetDocValuesFacetField("b", "bar2"));
+    doc.add(new SortedSetDocValuesFacetField("c", "baz1"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+
+    assertEquals(3, results.size());
+    assertEquals("dim=a path=[] value=3 childCount=3\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
+    assertEquals("dim=b path=[] value=2 childCount=2\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
+    assertEquals("dim=c path=[] value=1 childCount=1\n  baz1 (1)\n", results.get(2).toString());
+
+    searcher.getIndexReader().close();
+    dir.close();
+  }
+
+  public void testSomeSegmentsMissing() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+    writer.commit();
+
+    doc = new Document();
+    writer.addDocument(config.build(doc));
+    writer.commit();
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
+    writer.addDocument(config.build(doc));
+    writer.commit();
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    assertEquals("dim=a path=[] value=2 childCount=2\n  foo1 (1)\n  foo2 (1)\n", facets.getTopChildren(10, "a").toString());
+
+    searcher.getIndexReader().close();
+    dir.close();
+  }
+
+  public void testSlowCompositeReaderWrapper() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+
+    writer.commit();
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = new IndexSearcher(SlowCompositeReaderWrapper.wrap(writer.getReader()));
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+    Facets facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    assertEquals("dim=a path=[] value=2 childCount=2\n  foo1 (1)\n  foo2 (1)\n", facets.getTopChildren(10, "a").toString());
+
+    IOUtils.close(writer, searcher.getIndexReader(), dir);
+  }
+
+
+  public void testRandom() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    String[] tokens = getRandomTokens(10);
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    RandomIndexWriter w = new RandomIndexWriter(random(), indexDir);
+    FacetsConfig config = new FacetsConfig();
+    int numDocs = atLeast(1000);
+    int numDims = _TestUtil.nextInt(random(), 1, 7);
+    List<TestDoc> testDocs = getRandomDocs(tokens, numDocs, numDims);
+    for(TestDoc testDoc : testDocs) {
+      Document doc = new Document();
+      doc.add(newStringField("content", testDoc.content, Field.Store.NO));
+      for(int j=0;j<numDims;j++) {
+        if (testDoc.dims[j] != null) {
+          doc.add(new SortedSetDocValuesFacetField("dim" + j, testDoc.dims[j]));
+        }
+      }
+      w.addDocument(config.build(doc));
+    }
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(w.getReader());
+    
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+
+    int iters = atLeast(100);
+    for(int iter=0;iter<iters;iter++) {
+      String searchToken = tokens[random().nextInt(tokens.length)];
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter content=" + searchToken);
+      }
+      FacetsCollector fc = new FacetsCollector();
+      FacetsCollector.search(searcher, new TermQuery(new Term("content", searchToken)), 10, fc);
+      Facets facets = new SortedSetDocValuesFacetCounts(state, fc);
+
+      // Slow, yet hopefully bug-free, faceting:
+      @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Integer>[] expectedCounts = new HashMap[numDims];
+      for(int i=0;i<numDims;i++) {
+        expectedCounts[i] = new HashMap<String,Integer>();
+      }
+
+      for(TestDoc doc : testDocs) {
+        if (doc.content.equals(searchToken)) {
+          for(int j=0;j<numDims;j++) {
+            if (doc.dims[j] != null) {
+              Integer v = expectedCounts[j].get(doc.dims[j]);
+              if (v == null) {
+                expectedCounts[j].put(doc.dims[j], 1);
+              } else {
+                expectedCounts[j].put(doc.dims[j], v.intValue() + 1);
+              }
+            }
+          }
+        }
+      }
+
+      List<FacetResult> expected = new ArrayList<FacetResult>();
+      for(int i=0;i<numDims;i++) {
+        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
+        int totCount = 0;
+        for(Map.Entry<String,Integer> ent : expectedCounts[i].entrySet()) {
+          labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
+          totCount += ent.getValue();
+        }
+        sortLabelValues(labelValues);
+        if (totCount > 0) {
+          expected.add(new FacetResult("dim" + i, new String[0], totCount, labelValues.toArray(new LabelAndValue[labelValues.size()]), labelValues.size()));
+        }
+      }
+
+      // Sort by highest value, tie break by value:
+      sortFacetResults(expected);
+
+      List<FacetResult> actual = facets.getAllDims(10);
+
+      // Messy: fixup ties
+      //sortTies(actual);
+
+      assertEquals(expected, actual);
+    }
+
+    IOUtils.close(w, searcher.getIndexReader(), indexDir, taxoDir);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestSearcherTaxonomyManager.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestSearcherTaxonomyManager.java
new file mode 100644
index 0000000..f14dfde
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestSearcherTaxonomyManager.java
@@ -0,0 +1,188 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.SearcherTaxonomyManager;
+import org.apache.lucene.facet.taxonomy.SearcherTaxonomyManager.SearcherAndTaxonomy;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestSearcherTaxonomyManager extends FacetTestCase {
+  public void test() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    final IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    final DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+    final FacetsConfig config = new FacetsConfig();
+    config.setMultiValued("field", true);
+    final AtomicBoolean stop = new AtomicBoolean();
+
+    // How many unique facets to index before stopping:
+    final int ordLimit = TEST_NIGHTLY ? 100000 : 6000;
+
+    Thread indexer = new Thread() {
+        @Override
+        public void run() {
+          try {
+            Set<String> seen = new HashSet<String>();
+            List<String> paths = new ArrayList<String>();
+            while (true) {
+              Document doc = new Document();
+              int numPaths = _TestUtil.nextInt(random(), 1, 5);
+              for(int i=0;i<numPaths;i++) {
+                String path;
+                if (!paths.isEmpty() && random().nextInt(5) != 4) {
+                  // Use previous path
+                  path = paths.get(random().nextInt(paths.size()));
+                } else {
+                  // Create new path
+                  path = null;
+                  while (true) {
+                    path = _TestUtil.randomRealisticUnicodeString(random());
+                    if (path.length() != 0 && !seen.contains(path)) {
+                      seen.add(path);
+                      paths.add(path);
+                      break;
+                    }
+                  }
+                }
+                doc.add(new FacetField("field", path));
+              }
+              try {
+                w.addDocument(config.build(tw, doc));
+              } catch (IOException ioe) {
+                throw new RuntimeException(ioe);
+              }
+
+              if (tw.getSize() >= ordLimit) {
+                break;
+              }
+            }
+          } finally {
+            stop.set(true);
+          }
+        }
+      };
+
+    final SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
+
+    Thread reopener = new Thread() {
+        @Override
+        public void run() {
+          while(!stop.get()) {
+            try {
+              // Sleep for up to 20 msec:
+              Thread.sleep(random().nextInt(20));
+
+              if (VERBOSE) {
+                System.out.println("TEST: reopen");
+              }
+
+              mgr.maybeRefresh();
+
+              if (VERBOSE) {
+                System.out.println("TEST: reopen done");
+              }
+            } catch (Exception ioe) {
+              throw new RuntimeException(ioe);
+            }
+          }
+        }
+      };
+    reopener.start();
+
+    indexer.start();
+
+    try {
+      while (!stop.get()) {
+        SearcherAndTaxonomy pair = mgr.acquire();
+        try {
+          //System.out.println("search maxOrd=" + pair.taxonomyReader.getSize());
+          FacetsCollector sfc = new FacetsCollector();
+          pair.searcher.search(new MatchAllDocsQuery(), sfc);
+          Facets facets = getTaxonomyFacetCounts(pair.taxonomyReader, config, sfc);
+          FacetResult result = facets.getTopChildren(10, "field");
+          if (pair.searcher.getIndexReader().numDocs() > 0) { 
+            //System.out.println(pair.taxonomyReader.getSize());
+            assertTrue(result.childCount > 0);
+            assertTrue(result.labelValues.length > 0);
+          }
+
+          //if (VERBOSE) {
+          //System.out.println("TEST: facets=" + FacetTestUtils.toString(results.get(0)));
+          //}
+        } finally {
+          mgr.release(pair);
+        }
+      }
+    } finally {
+      indexer.join();
+      reopener.join();
+    }
+
+    if (VERBOSE) {
+      System.out.println("TEST: now stop");
+    }
+
+    IOUtils.close(mgr, tw, w, taxoDir, dir);
+  }
+
+  public void testReplaceTaxonomy() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+
+    Directory taxoDir2 = newDirectory();
+    DirectoryTaxonomyWriter tw2 = new DirectoryTaxonomyWriter(taxoDir2);
+    tw2.close();
+
+    SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
+    w.addDocument(new Document());
+    tw.replaceTaxonomy(taxoDir2);
+    taxoDir2.close();
+
+    try {
+      mgr.maybeRefresh();
+      fail("should have hit exception");
+    } catch (IllegalStateException ise) {
+      // expected
+    }
+
+    IOUtils.close(mgr, tw, w, taxoDir, dir);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetAssociations.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetAssociations.java
new file mode 100644
index 0000000..bcb1f25
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetAssociations.java
@@ -0,0 +1,226 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.taxonomy.FloatAssociationFacetField;
+import org.apache.lucene.facet.taxonomy.IntAssociationFacetField;
+import org.apache.lucene.facet.taxonomy.TaxonomyFacetSumFloatAssociations;
+import org.apache.lucene.facet.taxonomy.TaxonomyFacetSumIntAssociations;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+
+/** Test for associations */
+public class TestTaxonomyFacetAssociations extends FacetTestCase {
+  
+  private static Directory dir;
+  private static IndexReader reader;
+  private static Directory taxoDir;
+  private static TaxonomyReader taxoReader;
+
+  private static FacetsConfig config;
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    dir = newDirectory();
+    taxoDir = newDirectory();
+    // preparations - index, taxonomy, content
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+
+    // Cannot mix ints & floats in the same indexed field:
+    config = new FacetsConfig();
+    config.setIndexFieldName("int", "$facets.int");
+    config.setMultiValued("int", true);
+    config.setIndexFieldName("float", "$facets.float");
+    config.setMultiValued("float", true);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    // index documents, 50% have only 'b' and all have 'a'
+    for (int i = 0; i < 110; i++) {
+      Document doc = new Document();
+      // every 11th document is added empty, this used to cause the association
+      // aggregators to go into an infinite loop
+      if (i % 11 != 0) {
+        doc.add(new IntAssociationFacetField(2, "int", "a"));
+        doc.add(new FloatAssociationFacetField(0.5f, "float", "a"));
+        if (i % 2 == 0) { // 50
+          doc.add(new IntAssociationFacetField(3, "int", "b"));
+          doc.add(new FloatAssociationFacetField(0.2f, "float", "b"));
+        }
+      }
+      writer.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    taxoWriter.close();
+    reader = writer.getReader();
+    writer.close();
+    taxoReader = new DirectoryTaxonomyReader(taxoDir);
+  }
+  
+  @AfterClass
+  public static void afterClass() throws Exception {
+    reader.close();
+    reader = null;
+    dir.close();
+    dir = null;
+    taxoReader.close();
+    taxoReader = null;
+    taxoDir.close();
+    taxoDir = null;
+  }
+  
+  public void testIntSumAssociation() throws Exception {
+    
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
+    assertEquals("dim=int path=[] value=-1 childCount=2\n  a (200)\n  b (150)\n", facets.getTopChildren(10, "int").toString());
+    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
+    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
+  }
+
+  public void testFloatSumAssociation() throws Exception {
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    
+    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
+    assertEquals("dim=float path=[] value=-1.0 childCount=2\n  a (50.0)\n  b (9.999995)\n", facets.getTopChildren(10, "float").toString());
+    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
+    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
+  }  
+
+  /** Make sure we can test both int and float assocs in one
+   *  index, as long as we send each to a different field. */
+  public void testIntAndFloatAssocation() throws Exception {
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    
+    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
+    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
+    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
+    
+    facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
+    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
+    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    Facets facets = new TaxonomyFacetSumFloatAssociations(taxoReader, config, fc);
+    try {
+      facets.getSpecificValue("float");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "float");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+  }
+
+  public void testMixedTypesInSameIndexField() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntAssociationFacetField(14, "a", "x"));
+    doc.add(new FloatAssociationFacetField(55.0f, "b", "y"));
+    try {
+      writer.addDocument(config.build(taxoWriter, doc));
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException exc) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testNoHierarchy() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntAssociationFacetField(14, "a", "x"));
+    try {
+      writer.addDocument(config.build(taxoWriter, doc));
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException exc) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testRequireDimCount() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    config.setRequireDimCount("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntAssociationFacetField(14, "a", "x"));
+    try {
+      writer.addDocument(config.build(taxoWriter, doc));
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException exc) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts.java
new file mode 100644
index 0000000..3682e7e
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts.java
@@ -0,0 +1,761 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayOutputStream;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.CachedOrdinalsReader;
+import org.apache.lucene.facet.DocValuesOrdinalsReader;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.LabelAndValue;
+import org.apache.lucene.facet.OrdinalsReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.similarities.DefaultSimilarity;
+import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
+import org.apache.lucene.search.similarities.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestTaxonomyFacetCounts extends FacetTestCase {
+
+  public void testBasic() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("Publish Date", true);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Susan"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Frank"));
+    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+
+    // Retrieve & verify results:
+    assertEquals("dim=Publish Date path=[] value=5 childCount=3\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", facets.getTopChildren(10, "Publish Date").toString());
+    assertEquals("dim=Author path=[] value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", facets.getTopChildren(10, "Author").toString());
+
+    // Now user drills down on Publish Date/2010:
+    DrillDownQuery q2 = new DrillDownQuery(config);
+    q2.add("Publish Date", "2010");
+    c = new FacetsCollector();
+    searcher.search(q2, c);
+    facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+    assertEquals("dim=Author path=[] value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", facets.getTopChildren(10, "Author").toString());
+
+    assertEquals(1, facets.getSpecificValue("Author", "Lisa"));
+
+    assertNull(facets.getTopChildren(10, "Non exitent dim"));
+
+    // Smoke test PrintTaxonomyStats:
+    ByteArrayOutputStream bos = new ByteArrayOutputStream();
+    PrintTaxonomyStats.printStats(taxoReader, new PrintStream(bos, false, "UTF-8"), true);
+    String result = bos.toString("UTF-8");
+    assertTrue(result.indexOf("/Author: 4 immediate children; 5 total categories") != -1);
+    assertTrue(result.indexOf("/Publish Date: 3 immediate children; 12 total categories") != -1);
+    // Make sure at least a few nodes of the tree came out:
+    assertTrue(result.indexOf("  /1999") != -1);
+    assertTrue(result.indexOf("  /2012") != -1);
+    assertTrue(result.indexOf("      /20") != -1);
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
+  }
+
+  // LUCENE-5333
+  public void testSparseFacets() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new FacetField("a", "foo2"));
+    doc.add(new FacetField("b", "bar1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new FacetField("a", "foo3"));
+    doc.add(new FacetField("b", "bar2"));
+    doc.add(new FacetField("c", "baz1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, new FacetsConfig(), c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+
+    assertEquals(3, results.size());
+    assertEquals("dim=a path=[] value=3 childCount=3\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
+    assertEquals("dim=b path=[] value=2 childCount=2\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
+    assertEquals("dim=c path=[] value=1 childCount=1\n  baz1 (1)\n", results.get(2).toString());
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setIndexFieldName("a", "$facets2");
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    // Uses default $facets field:
+    Facets facets;
+    if (random().nextBoolean()) {
+      facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+    } else {
+      OrdinalsReader ordsReader = new DocValuesOrdinalsReader();
+      if (random().nextBoolean()) {
+        ordsReader = new CachedOrdinalsReader(ordsReader);
+      }
+      facets = new TaxonomyFacetCounts(ordsReader, taxoReader, config, c);
+    }
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+    assertTrue(results.isEmpty());
+
+    try {
+      facets.getSpecificValue("a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
+  }
+
+  public void testReallyNoNormsForDrillDown() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setSimilarity(new PerFieldSimilarityWrapper() {
+        final Similarity sim = new DefaultSimilarity();
+
+        @Override
+        public Similarity get(String name) {
+          assertEquals("field", name);
+          return sim;
+        }
+      });
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path"));
+    writer.addDocument(config.build(taxoWriter, doc));
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testMultiValuedHierarchy() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("a", true);
+    config.setMultiValued("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path", "x"));
+    doc.add(new FacetField("a", "path", "y"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+
+    try {
+      facets.getSpecificValue("a");
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    FacetResult result = facets.getTopChildren(10, "a");
+    assertEquals(1, result.labelValues.length);
+    assertEquals(1, result.labelValues[0].value.intValue());
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testLabelWithDelimiter() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setMultiValued("dim", true);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("dim", "test\u001Fone"));
+    doc.add(new FacetField("dim", "test\u001Etwo"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);
+    
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+    assertEquals(1, facets.getSpecificValue("dim", "test\u001Fone"));
+    assertEquals(1, facets.getSpecificValue("dim", "test\u001Etwo"));
+
+    FacetResult result = facets.getTopChildren(10, "dim");
+    assertEquals("dim=dim path=[] value=-1 childCount=2\n  test\u001Fone (1)\n  test\u001Etwo (1)\n", result.toString());
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testRequireDimCount() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setRequireDimCount("dim", true);
+
+    config.setMultiValued("dim2", true);
+    config.setRequireDimCount("dim2", true);
+
+    config.setMultiValued("dim3", true);
+    config.setHierarchical("dim3", true);
+    config.setRequireDimCount("dim3", true);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("dim", "a"));
+    doc.add(new FacetField("dim2", "a"));
+    doc.add(new FacetField("dim2", "b"));
+    doc.add(new FacetField("dim3", "a", "b"));
+    doc.add(new FacetField("dim3", "a", "c"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);
+    
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+    assertEquals(1, facets.getTopChildren(10, "dim").value);
+    assertEquals(1, facets.getTopChildren(10, "dim2").value);
+    assertEquals(1, facets.getTopChildren(10, "dim3").value);
+    try {
+      assertEquals(1, facets.getSpecificValue("dim"));
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    assertEquals(1, facets.getSpecificValue("dim2"));
+    assertEquals(1, facets.getSpecificValue("dim3"));
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  // LUCENE-4583: make sure if we require > 32 KB for one
+  // document, we don't hit exc when using Facet42DocValuesFormat
+  public void testManyFacetsInOneDocument() throws Exception {
+    assumeTrue("default Codec doesn't support huge BinaryDocValues", _TestUtil.fieldSupportsHugeBinaryDocValues(FacetsConfig.DEFAULT_INDEX_FIELD_NAME));
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setMultiValued("dim", true);
+    
+    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);
+    
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    for (int i = 0; i < numLabels; i++) {
+      doc.add(new FacetField("dim", "" + i));
+    }
+    writer.addDocument(config.build(taxoWriter, doc));
+    
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+    
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+
+    FacetResult result = facets.getTopChildren(Integer.MAX_VALUE, "dim");
+    assertEquals(numLabels, result.labelValues.length);
+    Set<String> allLabels = new HashSet<String>();
+    for (LabelAndValue labelValue : result.labelValues) {
+      allLabels.add(labelValue.label);
+      assertEquals(1, labelValue.value.intValue());
+    }
+    assertEquals(numLabels, allLabels.size());
+    
+    IOUtils.close(searcher.getIndexReader(), taxoWriter, writer, taxoReader, dir, taxoDir);
+  }
+
+  // Make sure we catch when app didn't declare field as
+  // hierarchical but it was:
+  public void testDetectHierarchicalField() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path", "other"));
+    try {
+      config.build(taxoWriter, doc);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  // Make sure we catch when app didn't declare field as
+  // multi-valued but it was:
+  public void testDetectMultiValuedField() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path"));
+    doc.add(new FacetField("a", "path2"));
+    try {
+      config.build(taxoWriter, doc);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testSeparateIndexedFields() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    config.setIndexFieldName("b", "$b");
+    
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      doc.add(new StringField("f", "v", Field.Store.NO));
+      doc.add(new FacetField("a", "1"));
+      doc.add(new FacetField("b", "1"));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets1 = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    Facets facets2 = getTaxonomyFacetCounts(taxoReader, config, sfc, "$b");
+    assertEquals(r.maxDoc(), facets1.getTopChildren(10, "a").value.intValue());
+    assertEquals(r.maxDoc(), facets2.getTopChildren(10, "b").value.intValue());
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+  
+  public void testCountRoot() throws Exception {
+    // LUCENE-4882: FacetsAccumulator threw NPE if a FacetRequest was defined on CP.EMPTY
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      doc.add(new FacetField("a", "1"));
+      doc.add(new FacetField("b", "1"));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    for (FacetResult result : facets.getAllDims(10)) {
+      assertEquals(r.numDocs(), result.value.intValue());
+    }
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testGetFacetResultsTwice() throws Exception {
+    // LUCENE-4893: counts were multiplied as many times as getFacetResults was called.
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new FacetField("a", "1"));
+    doc.add(new FacetField("b", "1"));
+    iw.addDocument(config.build(taxoWriter, doc));
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    final FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    List<FacetResult> res1 = facets.getAllDims(10);
+    List<FacetResult> res2 = facets.getAllDims(10);
+    assertEquals("calling getFacetResults twice should return the .equals()=true result", res1, res2);
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+  
+  public void testChildCount() throws Exception {
+    // LUCENE-4885: FacetResult.numValidDescendants was not set properly by FacetsAccumulator
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    for (int i = 0; i < 10; i++) {
+      Document doc = new Document();
+      doc.add(new FacetField("a", Integer.toString(i)));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    
+    assertEquals(10, facets.getTopChildren(2, "a").childCount);
+
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  private void indexTwoDocs(TaxonomyWriter taxoWriter, IndexWriter indexWriter, FacetsConfig config, boolean withContent) throws Exception {
+    for (int i = 0; i < 2; i++) {
+      Document doc = new Document();
+      if (withContent) {
+        doc.add(new StringField("f", "a", Field.Store.NO));
+      }
+      if (config != null) {
+        doc.add(new FacetField("A", Integer.toString(i)));
+        indexWriter.addDocument(config.build(taxoWriter, doc));
+      } else {
+        indexWriter.addDocument(doc);
+      }
+    }
+    
+    indexWriter.commit();
+  }
+  
+  public void testSegmentsWithoutCategoriesOrResults() throws Exception {
+    // tests the accumulator when there are segments with no results
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges
+    IndexWriter indexWriter = new IndexWriter(indexDir, iwc);
+
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    indexTwoDocs(taxoWriter, indexWriter, config, false); // 1st segment, no content, with categories
+    indexTwoDocs(taxoWriter, indexWriter, null, true);         // 2nd segment, with content, no categories
+    indexTwoDocs(taxoWriter, indexWriter, config, true);  // 3rd segment ok
+    indexTwoDocs(taxoWriter, indexWriter, null, false);        // 4th segment, no content, or categories
+    indexTwoDocs(taxoWriter, indexWriter, null, true);         // 5th segment, with content, no categories
+    indexTwoDocs(taxoWriter, indexWriter, config, true);  // 6th segment, with content, with categories
+    indexTwoDocs(taxoWriter, indexWriter, null, true);         // 7th segment, with content, no categories
+    IOUtils.close(indexWriter, taxoWriter);
+
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher indexSearcher = newSearcher(indexReader);
+    
+    // search for "f:a", only segments 1 and 3 should match results
+    Query q = new TermQuery(new Term("f", "a"));
+    FacetsCollector sfc = new FacetsCollector();
+    indexSearcher.search(q, sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    FacetResult result = facets.getTopChildren(10, "A");
+    assertEquals("wrong number of children", 2, result.labelValues.length);
+    for (LabelAndValue labelValue : result.labelValues) {
+      assertEquals("wrong weight for child " + labelValue.label, 2, labelValue.value.intValue());
+    }
+
+    IOUtils.close(indexReader, taxoReader, indexDir, taxoDir);
+  }
+
+  public void testRandom() throws Exception {
+    String[] tokens = getRandomTokens(10);
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    RandomIndexWriter w = new RandomIndexWriter(random(), indexDir);
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    int numDocs = atLeast(1000);
+    int numDims = _TestUtil.nextInt(random(), 1, 7);
+    List<TestDoc> testDocs = getRandomDocs(tokens, numDocs, numDims);
+    for(TestDoc testDoc : testDocs) {
+      Document doc = new Document();
+      doc.add(newStringField("content", testDoc.content, Field.Store.NO));
+      for(int j=0;j<numDims;j++) {
+        if (testDoc.dims[j] != null) {
+          doc.add(new FacetField("dim" + j, testDoc.dims[j]));
+        }
+      }
+      w.addDocument(config.build(tw, doc));
+    }
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(w.getReader());
+    
+    // NRT open
+    TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
+
+    int iters = atLeast(100);
+    for(int iter=0;iter<iters;iter++) {
+      String searchToken = tokens[random().nextInt(tokens.length)];
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter content=" + searchToken);
+      }
+      FacetsCollector fc = new FacetsCollector();
+      FacetsCollector.search(searcher, new TermQuery(new Term("content", searchToken)), 10, fc);
+      Facets facets = getTaxonomyFacetCounts(tr, config, fc);
+
+      // Slow, yet hopefully bug-free, faceting:
+      @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Integer>[] expectedCounts = new HashMap[numDims];
+      for(int i=0;i<numDims;i++) {
+        expectedCounts[i] = new HashMap<String,Integer>();
+      }
+
+      for(TestDoc doc : testDocs) {
+        if (doc.content.equals(searchToken)) {
+          for(int j=0;j<numDims;j++) {
+            if (doc.dims[j] != null) {
+              Integer v = expectedCounts[j].get(doc.dims[j]);
+              if (v == null) {
+                expectedCounts[j].put(doc.dims[j], 1);
+              } else {
+                expectedCounts[j].put(doc.dims[j], v.intValue() + 1);
+              }
+            }
+          }
+        }
+      }
+
+      List<FacetResult> expected = new ArrayList<FacetResult>();
+      for(int i=0;i<numDims;i++) {
+        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
+        int totCount = 0;
+        for(Map.Entry<String,Integer> ent : expectedCounts[i].entrySet()) {
+          labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
+          totCount += ent.getValue();
+        }
+        sortLabelValues(labelValues);
+        if (totCount > 0) {
+          expected.add(new FacetResult("dim" + i, new String[0], totCount, labelValues.toArray(new LabelAndValue[labelValues.size()]), labelValues.size()));
+        }
+      }
+
+      // Sort by highest value, tie break by value:
+      sortFacetResults(expected);
+
+      List<FacetResult> actual = facets.getAllDims(10);
+
+      // Messy: fixup ties
+      sortTies(actual);
+
+      assertEquals(expected, actual);
+    }
+
+    IOUtils.close(w, tw, searcher.getIndexReader(), tr, indexDir, taxoDir);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts2.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts2.java
new file mode 100644
index 0000000..f60dfad
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetCounts2.java
@@ -0,0 +1,373 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.LabelAndValue;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestTaxonomyFacetCounts2 extends FacetTestCase {
+  
+  private static final Term A = new Term("f", "a");
+  private static final String CP_A = "A", CP_B = "B";
+  private static final String CP_C = "C", CP_D = "D"; // indexed w/ NO_PARENTS
+  private static final int NUM_CHILDREN_CP_A = 5, NUM_CHILDREN_CP_B = 3;
+  private static final int NUM_CHILDREN_CP_C = 5, NUM_CHILDREN_CP_D = 5;
+  private static final FacetField[] CATEGORIES_A, CATEGORIES_B;
+  private static final FacetField[] CATEGORIES_C, CATEGORIES_D;
+  static {
+    CATEGORIES_A = new FacetField[NUM_CHILDREN_CP_A];
+    for (int i = 0; i < NUM_CHILDREN_CP_A; i++) {
+      CATEGORIES_A[i] = new FacetField(CP_A, Integer.toString(i));
+    }
+    CATEGORIES_B = new FacetField[NUM_CHILDREN_CP_B];
+    for (int i = 0; i < NUM_CHILDREN_CP_B; i++) {
+      CATEGORIES_B[i] = new FacetField(CP_B, Integer.toString(i));
+    }
+    
+    // NO_PARENTS categories
+    CATEGORIES_C = new FacetField[NUM_CHILDREN_CP_C];
+    for (int i = 0; i < NUM_CHILDREN_CP_C; i++) {
+      CATEGORIES_C[i] = new FacetField(CP_C, Integer.toString(i));
+    }
+    
+    // Multi-level categories
+    CATEGORIES_D = new FacetField[NUM_CHILDREN_CP_D];
+    for (int i = 0; i < NUM_CHILDREN_CP_D; i++) {
+      String val = Integer.toString(i);
+      CATEGORIES_D[i] = new FacetField(CP_D, val, val + val); // e.g. D/1/11, D/2/22...
+    }
+  }
+  
+  private static Directory indexDir, taxoDir;
+  private static Map<String,Integer> allExpectedCounts, termExpectedCounts;
+
+  @AfterClass
+  public static void afterClassCountingFacetsAggregatorTest() throws Exception {
+    IOUtils.close(indexDir, taxoDir); 
+  }
+  
+  private static List<FacetField> randomCategories(Random random) {
+    // add random categories from the two dimensions, ensuring that the same
+    // category is not added twice.
+    int numFacetsA = random.nextInt(3) + 1; // 1-3
+    int numFacetsB = random.nextInt(2) + 1; // 1-2
+    ArrayList<FacetField> categories_a = new ArrayList<FacetField>();
+    categories_a.addAll(Arrays.asList(CATEGORIES_A));
+    ArrayList<FacetField> categories_b = new ArrayList<FacetField>();
+    categories_b.addAll(Arrays.asList(CATEGORIES_B));
+    Collections.shuffle(categories_a, random);
+    Collections.shuffle(categories_b, random);
+
+    ArrayList<FacetField> categories = new ArrayList<FacetField>();
+    categories.addAll(categories_a.subList(0, numFacetsA));
+    categories.addAll(categories_b.subList(0, numFacetsB));
+    
+    // add the NO_PARENT categories
+    categories.add(CATEGORIES_C[random().nextInt(NUM_CHILDREN_CP_C)]);
+    categories.add(CATEGORIES_D[random().nextInt(NUM_CHILDREN_CP_D)]);
+
+    return categories;
+  }
+
+  private static void addField(Document doc) {
+    doc.add(new StringField(A.field(), A.text(), Store.NO));
+  }
+
+  private static void addFacets(Document doc, FacetsConfig config, boolean updateTermExpectedCounts) 
+      throws IOException {
+    List<FacetField> docCategories = randomCategories(random());
+    for (FacetField ff : docCategories) {
+      doc.add(ff);
+      String cp = ff.dim + "/" + ff.path[0];
+      allExpectedCounts.put(cp, allExpectedCounts.get(cp) + 1);
+      if (updateTermExpectedCounts) {
+        termExpectedCounts.put(cp, termExpectedCounts.get(cp) + 1);
+      }
+    }
+    // add 1 to each NO_PARENTS dimension
+    allExpectedCounts.put(CP_B, allExpectedCounts.get(CP_B) + 1);
+    allExpectedCounts.put(CP_C, allExpectedCounts.get(CP_C) + 1);
+    allExpectedCounts.put(CP_D, allExpectedCounts.get(CP_D) + 1);
+    if (updateTermExpectedCounts) {
+      termExpectedCounts.put(CP_B, termExpectedCounts.get(CP_B) + 1);
+      termExpectedCounts.put(CP_C, termExpectedCounts.get(CP_C) + 1);
+      termExpectedCounts.put(CP_D, termExpectedCounts.get(CP_D) + 1);
+    }
+  }
+
+  private static FacetsConfig getConfig() {
+    FacetsConfig config = new FacetsConfig();
+    config.setMultiValued("A", true);
+    config.setMultiValued("B", true);
+    config.setRequireDimCount("B", true);
+    config.setHierarchical("D", true);
+    return config;
+  }
+
+  private static void indexDocsNoFacets(IndexWriter indexWriter) throws IOException {
+    int numDocs = atLeast(2);
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      addField(doc);
+      indexWriter.addDocument(doc);
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  private static void indexDocsWithFacetsNoTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
+                                                 Map<String,Integer> expectedCounts) throws IOException {
+    Random random = random();
+    int numDocs = atLeast(random, 2);
+    FacetsConfig config = getConfig();
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      addFacets(doc, config, false);
+      indexWriter.addDocument(config.build(taxoWriter, doc));
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  private static void indexDocsWithFacetsAndTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
+                                                  Map<String,Integer> expectedCounts) throws IOException {
+    Random random = random();
+    int numDocs = atLeast(random, 2);
+    FacetsConfig config = getConfig();
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      addFacets(doc, config, true);
+      addField(doc);
+      indexWriter.addDocument(config.build(taxoWriter, doc));
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  private static void indexDocsWithFacetsAndSomeTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
+                                                      Map<String,Integer> expectedCounts) throws IOException {
+    Random random = random();
+    int numDocs = atLeast(random, 2);
+    FacetsConfig config = getConfig();
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      boolean hasContent = random.nextBoolean();
+      if (hasContent) {
+        addField(doc);
+      }
+      addFacets(doc, config, hasContent);
+      indexWriter.addDocument(config.build(taxoWriter, doc));
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  // initialize expectedCounts w/ 0 for all categories
+  private static Map<String,Integer> newCounts() {
+    Map<String,Integer> counts = new HashMap<String,Integer>();
+    counts.put(CP_A, 0);
+    counts.put(CP_B, 0);
+    counts.put(CP_C, 0);
+    counts.put(CP_D, 0);
+    for (FacetField ff : CATEGORIES_A) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    for (FacetField ff : CATEGORIES_B) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    for (FacetField ff : CATEGORIES_C) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    for (FacetField ff : CATEGORIES_D) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    return counts;
+  }
+  
+  @BeforeClass
+  public static void beforeClassCountingFacetsAggregatorTest() throws Exception {
+    indexDir = newDirectory();
+    taxoDir = newDirectory();
+    
+    // create an index which has:
+    // 1. Segment with no categories, but matching results
+    // 2. Segment w/ categories, but no results
+    // 3. Segment w/ categories and results
+    // 4. Segment w/ categories, but only some results
+    
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges, so we can control the index segments
+    IndexWriter indexWriter = new IndexWriter(indexDir, conf);
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+
+    allExpectedCounts = newCounts();
+    termExpectedCounts = newCounts();
+    
+    // segment w/ no categories
+    indexDocsNoFacets(indexWriter);
+
+    // segment w/ categories, no content
+    indexDocsWithFacetsNoTerms(indexWriter, taxoWriter, allExpectedCounts);
+
+    // segment w/ categories and content
+    indexDocsWithFacetsAndTerms(indexWriter, taxoWriter, allExpectedCounts);
+    
+    // segment w/ categories and some content
+    indexDocsWithFacetsAndSomeTerms(indexWriter, taxoWriter, allExpectedCounts);
+    
+    IOUtils.close(indexWriter, taxoWriter);
+  }
+  
+  @Test
+  public void testDifferentNumResults() throws Exception {
+    // test the collector w/ FacetRequests and different numResults
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    TermQuery q = new TermQuery(A);
+    searcher.search(q, sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_A, CP_A);
+    assertEquals(-1, result.value.intValue());
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(termExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
+    }
+    result = facets.getTopChildren(NUM_CHILDREN_CP_B, CP_B);
+    assertEquals(termExpectedCounts.get(CP_B), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(termExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
+    }
+    
+    IOUtils.close(indexReader, taxoReader);
+  }
+  
+  @Test
+  public void testAllCounts() throws Exception {
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+    
+    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_A, CP_A);
+    assertEquals(-1, result.value.intValue());
+    int prevValue = Integer.MAX_VALUE;
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
+      assertTrue("wrong sort order of sub results: labelValue.value=" + labelValue.value + " prevValue=" + prevValue, labelValue.value.intValue() <= prevValue);
+      prevValue = labelValue.value.intValue();
+    }
+
+    result = facets.getTopChildren(NUM_CHILDREN_CP_B, CP_B);
+    assertEquals(allExpectedCounts.get(CP_B), result.value);
+    prevValue = Integer.MAX_VALUE;
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
+      assertTrue("wrong sort order of sub results: labelValue.value=" + labelValue.value + " prevValue=" + prevValue, labelValue.value.intValue() <= prevValue);
+      prevValue = labelValue.value.intValue();
+    }
+
+    IOUtils.close(indexReader, taxoReader);
+  }
+  
+  @Test
+  public void testBigNumResults() throws Exception {
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+
+    FacetResult result = facets.getTopChildren(Integer.MAX_VALUE, CP_A);
+    assertEquals(-1, result.value.intValue());
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
+    }
+    result = facets.getTopChildren(Integer.MAX_VALUE, CP_B);
+    assertEquals(allExpectedCounts.get(CP_B), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
+    }
+    
+    IOUtils.close(indexReader, taxoReader);
+  }
+  
+  @Test
+  public void testNoParents() throws Exception {
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+
+    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_C, CP_C);
+    assertEquals(allExpectedCounts.get(CP_C), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_C + "/" + labelValue.label), labelValue.value);
+    }
+    result = facets.getTopChildren(NUM_CHILDREN_CP_D, CP_D);
+    assertEquals(allExpectedCounts.get(CP_C), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_D + "/" + labelValue.label), labelValue.value);
+    }
+    
+    IOUtils.close(indexReader, taxoReader);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java
new file mode 100644
index 0000000..8a5b1a8
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyFacetSumValueSource.java
@@ -0,0 +1,518 @@
+package org.apache.lucene.facet.taxonomy;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FloatDocValuesField;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.DocValuesOrdinalsReader;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.LabelAndValue;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queries.function.FunctionQuery;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
+import org.apache.lucene.queries.function.valuesource.FloatFieldSource;
+import org.apache.lucene.queries.function.valuesource.IntFieldSource;
+import org.apache.lucene.queries.function.valuesource.LongFieldSource;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
+
+  public void testBasic() throws Exception {
+
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig();
+
+    // Reused across documents, to add the necessary facet
+    // fields:
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("Author", "Bob"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 20, Field.Store.NO));
+    doc.add(new FacetField("Author", "Lisa"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 30, Field.Store.NO));
+    doc.add(new FacetField("Author", "Lisa"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 40, Field.Store.NO));
+    doc.add(new FacetField("Author", "Susan"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 45, Field.Store.NO));
+    doc.add(new FacetField("Author", "Frank"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query and one of the
+    // Facets.search utility methods:
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
+
+    // Retrieve & verify results:
+    assertEquals("dim=Author path=[] value=145.0 childCount=4\n  Lisa (50.0)\n  Frank (45.0)\n  Susan (40.0)\n  Bob (10.0)\n", facets.getTopChildren(10, "Author").toString());
+
+    taxoReader.close();
+    searcher.getIndexReader().close();
+    dir.close();
+    taxoDir.close();
+  }
+
+  // LUCENE-5333
+  public void testSparseFacets() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new IntField("num", 20, Field.Store.NO));
+    doc.add(new FacetField("a", "foo2"));
+    doc.add(new FacetField("b", "bar1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new IntField("num", 30, Field.Store.NO));
+    doc.add(new FacetField("a", "foo3"));
+    doc.add(new FacetField("b", "bar2"));
+    doc.add(new FacetField("c", "baz1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+
+    assertEquals(3, results.size());
+    assertEquals("dim=a path=[] value=60.0 childCount=3\n  foo3 (30.0)\n  foo2 (20.0)\n  foo1 (10.0)\n", results.get(0).toString());
+    assertEquals("dim=b path=[] value=50.0 childCount=2\n  bar2 (30.0)\n  bar1 (20.0)\n", results.get(1).toString());
+    assertEquals("dim=c path=[] value=30.0 childCount=1\n  baz1 (30.0)\n", results.get(2).toString());
+
+    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setIndexFieldName("a", "$facets2");
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(taxoWriter, doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, config, c, new IntFieldSource("num"));
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+    assertTrue(results.isEmpty());
+
+    try {
+      facets.getSpecificValue("a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testSumScoreAggregator() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+
+    FacetsConfig config = new FacetsConfig();
+
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      if (random().nextBoolean()) { // don't match all documents
+        doc.add(new StringField("f", "v", Field.Store.NO));
+      }
+      doc.add(new FacetField("dim", "a"));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector fc = new FacetsCollector(true);
+    ConstantScoreQuery csq = new ConstantScoreQuery(new MatchAllDocsQuery());
+    csq.setBoost(2.0f);
+    
+    TopDocs td = FacetsCollector.search(newSearcher(r), csq, 10, fc);
+
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, fc, new TaxonomyFacetSumValueSource.ScoreValueSource());
+    
+    int expected = (int) (td.getMaxScore() * td.totalHits);
+    assertEquals(expected, facets.getSpecificValue("dim", "a").intValue());
+    
+    IOUtils.close(iw, taxoWriter, taxoReader, taxoDir, r, indexDir);
+  }
+  
+  public void testNoScore() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    for (int i = 0; i < 4; i++) {
+      Document doc = new Document();
+      doc.add(new NumericDocValuesField("price", (i+1)));
+      doc.add(new FacetField("a", Integer.toString(i % 2)));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, new LongFieldSource("price"));
+    assertEquals("dim=a path=[] value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testWithScore() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+
+    FacetsConfig config = new FacetsConfig();
+    for (int i = 0; i < 4; i++) {
+      Document doc = new Document();
+      doc.add(new NumericDocValuesField("price", (i+1)));
+      doc.add(new FacetField("a", Integer.toString(i % 2)));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    ValueSource valueSource = new ValueSource() {
+      @Override
+      public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
+        final Scorer scorer = (Scorer) context.get("scorer");
+        assert scorer != null;
+        return new DoubleDocValues(this) {
+          @Override
+          public double doubleVal(int document) {
+            try {
+              return scorer.score();
+            } catch (IOException exception) {
+              throw new RuntimeException(exception);
+            }
+          }
+        };
+      }
+
+      @Override public boolean equals(Object o) { return o == this; }
+      @Override public int hashCode() { return System.identityHashCode(this); }
+      @Override public String description() { return "score()"; }
+    };
+    
+    FacetsCollector fc = new FacetsCollector(true);
+    // score documents by their 'price' field - makes asserting the correct counts for the categories easier
+    Query q = new FunctionQuery(new LongFieldSource("price"));
+    FacetsCollector.search(newSearcher(r), q, 10, fc);
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, fc, valueSource);
+    
+    assertEquals("dim=a path=[] value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testRollupValues() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("a", true);
+    //config.setRequireDimCount("a", true);
+    
+    for (int i = 0; i < 4; i++) {
+      Document doc = new Document();
+      doc.add(new NumericDocValuesField("price", (i+1)));
+      doc.add(new FacetField("a", Integer.toString(i % 2), "1"));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    ValueSource valueSource = new LongFieldSource("price");
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, valueSource);
+    
+    assertEquals("dim=a path=[] value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testCountAndSumScore() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig();
+    config.setIndexFieldName("b", "$b");
+    
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      doc.add(new StringField("f", "v", Field.Store.NO));
+      doc.add(new FacetField("a", "1"));
+      doc.add(new FacetField("b", "1"));
+      iw.addDocument(config.build(taxoWriter, doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector fc = new FacetsCollector(true);
+    FacetsCollector.search(newSearcher(r), new MatchAllDocsQuery(), 10, fc);
+    
+    Facets facets1 = getTaxonomyFacetCounts(taxoReader, config, fc);
+    Facets facets2 = new TaxonomyFacetSumValueSource(new DocValuesOrdinalsReader("$b"), taxoReader, config, fc, new TaxonomyFacetSumValueSource.ScoreValueSource());
+
+    assertEquals(r.maxDoc(), facets1.getTopChildren(10, "a").value.intValue());
+    assertEquals(r.maxDoc(), facets2.getTopChildren(10, "b").value.doubleValue(), 1E-10);
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testRandom() throws Exception {
+    String[] tokens = getRandomTokens(10);
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    RandomIndexWriter w = new RandomIndexWriter(random(), indexDir);
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig();
+    int numDocs = atLeast(1000);
+    int numDims = _TestUtil.nextInt(random(), 1, 7);
+    List<TestDoc> testDocs = getRandomDocs(tokens, numDocs, numDims);
+    for(TestDoc testDoc : testDocs) {
+      Document doc = new Document();
+      doc.add(newStringField("content", testDoc.content, Field.Store.NO));
+      testDoc.value = random().nextFloat();
+      doc.add(new FloatDocValuesField("value", testDoc.value));
+      for(int j=0;j<numDims;j++) {
+        if (testDoc.dims[j] != null) {
+          doc.add(new FacetField("dim" + j, testDoc.dims[j]));
+        }
+      }
+      w.addDocument(config.build(tw, doc));
+    }
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(w.getReader());
+    
+    // NRT open
+    TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
+
+    ValueSource values = new FloatFieldSource("value");
+
+    int iters = atLeast(100);
+    for(int iter=0;iter<iters;iter++) {
+      String searchToken = tokens[random().nextInt(tokens.length)];
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter content=" + searchToken);
+      }
+      FacetsCollector fc = new FacetsCollector();
+      FacetsCollector.search(searcher, new TermQuery(new Term("content", searchToken)), 10, fc);
+      Facets facets = new TaxonomyFacetSumValueSource(tr, config, fc, values);
+
+      // Slow, yet hopefully bug-free, faceting:
+      @SuppressWarnings({"rawtypes","unchecked"}) Map<String,Float>[] expectedValues = new HashMap[numDims];
+      for(int i=0;i<numDims;i++) {
+        expectedValues[i] = new HashMap<String,Float>();
+      }
+
+      for(TestDoc doc : testDocs) {
+        if (doc.content.equals(searchToken)) {
+          for(int j=0;j<numDims;j++) {
+            if (doc.dims[j] != null) {
+              Float v = expectedValues[j].get(doc.dims[j]);
+              if (v == null) {
+                expectedValues[j].put(doc.dims[j], doc.value);
+              } else {
+                expectedValues[j].put(doc.dims[j], v.floatValue() + doc.value);
+              }
+            }
+          }
+        }
+      }
+
+      List<FacetResult> expected = new ArrayList<FacetResult>();
+      for(int i=0;i<numDims;i++) {
+        List<LabelAndValue> labelValues = new ArrayList<LabelAndValue>();
+        double totValue = 0;
+        for(Map.Entry<String,Float> ent : expectedValues[i].entrySet()) {
+          labelValues.add(new LabelAndValue(ent.getKey(), ent.getValue()));
+          totValue += ent.getValue();
+        }
+        sortLabelValues(labelValues);
+        if (totValue > 0) {
+          expected.add(new FacetResult("dim" + i, new String[0], totValue, labelValues.toArray(new LabelAndValue[labelValues.size()]), labelValues.size()));
+        }
+      }
+
+      // Sort by highest value, tie break by value:
+      sortFacetResults(expected);
+
+      List<FacetResult> actual = facets.getAllDims(10);
+
+      // Messy: fixup ties
+      sortTies(actual);
+
+      if (VERBOSE) {
+        System.out.println("expected=\n" + expected.toString());
+        System.out.println("actual=\n" + actual.toString());
+      }
+
+      assertFloatValuesEquals(expected, actual);
+    }
+
+    IOUtils.close(w, tw, searcher.getIndexReader(), tr, indexDir, taxoDir);
+  }
+}
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java b/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java
index c4ae135..ef54c35 100644
--- a/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java
+++ b/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java
@@ -32,7 +32,7 @@ import org.apache.lucene.facet.FacetResult;
 import org.apache.lucene.facet.Facets;
 import org.apache.lucene.facet.FacetsCollector;
 import org.apache.lucene.facet.FacetsConfig;
-import org.apache.lucene.facet.FastTaxonomyFacetCounts;
+import org.apache.lucene.facet.taxonomy.FastTaxonomyFacetCounts;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;

