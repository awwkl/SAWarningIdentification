GitDiffStart: 98d3461f68596a877a1b93592d21177e1a31ba3e | Tue Dec 21 23:49:03 2010 +0000
diff --git a/lucene/src/java/org/apache/lucene/index/BufferedDeletesInRAM.java b/lucene/src/java/org/apache/lucene/index/BufferedDeletesInRAM.java
deleted file mode 100644
index 21ef5d5..0000000
--- a/lucene/src/java/org/apache/lucene/index/BufferedDeletesInRAM.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.lucene.index;
-
-import java.util.TreeMap;
-
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.ThreadSafeCloneableSortedMap;
-
-public class BufferedDeletesInRAM {
-  static class Delete {
-    int flushCount;
-
-    public Delete(int flushCount) {
-      this.flushCount = flushCount;
-    }
-  }
-
-  final static class DeleteTerm extends Delete {
-    final Term term;
-
-    public DeleteTerm(Term term, int flushCount) {
-      super(flushCount);
-      this.term = term;
-    }
-  }
-
-  final static class DeleteTerms extends Delete {
-    final Term[] terms;
-
-    public DeleteTerms(Term[] terms, int flushCount) {
-      super(flushCount);
-      this.terms = terms;
-    }
-  }
-  
-  final static class DeleteQuery extends Delete {
-    final Query query;
-
-    public DeleteQuery(Query query, int flushCount) {
-      super(flushCount);
-      this.query = query;
-    }
-  }
-
-  final ThreadSafeCloneableSortedMap<Long, Delete> deletes = ThreadSafeCloneableSortedMap
-      .getThreadSafeSortedMap(new TreeMap<Long, Delete>());
-
-  final void addDeleteTerm(Term term, long sequenceID, int numThreadStates) {
-    deletes.put(sequenceID, new DeleteTerm(term, numThreadStates));
-  }
-
-  final void addDeleteTerms(Term[] terms, long sequenceID, int numThreadStates) {
-    deletes.put(sequenceID, new DeleteTerms(terms, numThreadStates));
-  }
-
-  final void addDeleteQuery(Query query, long sequenceID, int numThreadStates) {
-    deletes.put(sequenceID, new DeleteQuery(query, numThreadStates));
-  }
-
-  boolean hasDeletes() {
-    return !deletes.isEmpty();
-  }
-
-  void clear() {
-    deletes.clear();
-  }
-
-  int getNumDeletes() {
-    return this.deletes.size();
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/ByteBlockPool.java b/lucene/src/java/org/apache/lucene/index/ByteBlockPool.java
deleted file mode 100644
index 651c89d..0000000
--- a/lucene/src/java/org/apache/lucene/index/ByteBlockPool.java
+++ /dev/null
@@ -1,169 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/* Class that Posting and PostingVector use to write byte
- * streams into shared fixed-size byte[] arrays.  The idea
- * is to allocate slices of increasing lengths For
- * example, the first slice is 5 bytes, the next slice is
- * 14, etc.  We start by writing our bytes into the first
- * 5 bytes.  When we hit the end of the slice, we allocate
- * the next slice and then write the address of the new
- * slice into the last 4 bytes of the previous slice (the
- * "forwarding address").
- *
- * Each slice is filled with 0's initially, and we mark
- * the end with a non-zero byte.  This way the methods
- * that are writing into the slice don't need to record
- * its length and instead allocate a new slice once they
- * hit a non-zero byte. */
-
-import java.util.Arrays;
-import org.apache.lucene.util.BytesRef;
-import java.util.List;
-import static org.apache.lucene.util.RamUsageEstimator.NUM_BYTES_OBJECT_REF;
-import org.apache.lucene.util.ArrayUtil;
-
-final class ByteBlockPool {
-
-  abstract static class Allocator {
-    abstract void recycleByteBlocks(byte[][] blocks, int start, int end);
-    abstract void recycleByteBlocks(List<byte[]> blocks);
-    abstract byte[] getByteBlock();
-  }
-
-  public byte[][] buffers = new byte[10][];
-
-  int bufferUpto = -1;                        // Which buffer we are upto
-  public int byteUpto = DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE;             // Where we are in head buffer
-
-  public byte[] buffer;                              // Current head buffer
-  public int byteOffset = -DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE;          // Current head offset
-
-  private final Allocator allocator;
-
-  public ByteBlockPool(Allocator allocator) {
-    this.allocator = allocator;
-  }
-
-  public void reset() {
-    if (bufferUpto != -1) {
-      // We allocated at least one buffer
-
-      for(int i=0;i<bufferUpto;i++)
-        // Fully zero fill buffers that we fully used
-        Arrays.fill(buffers[i], (byte) 0);
-
-      // Partial zero fill the final buffer
-      Arrays.fill(buffers[bufferUpto], 0, byteUpto, (byte) 0);
-          
-      if (bufferUpto > 0)
-        // Recycle all but the first buffer
-        allocator.recycleByteBlocks(buffers, 1, 1+bufferUpto);
-
-      // Re-use the first buffer
-      bufferUpto = 0;
-      byteUpto = 0;
-      byteOffset = 0;
-      buffer = buffers[0];
-    }
-  }
-
-  public void nextBuffer() {
-    if (1+bufferUpto == buffers.length) {
-      byte[][] newBuffers = new byte[ArrayUtil.oversize(buffers.length+1,
-                                                        NUM_BYTES_OBJECT_REF)][];
-      System.arraycopy(buffers, 0, newBuffers, 0, buffers.length);
-      buffers = newBuffers;
-    }
-    buffer = buffers[1+bufferUpto] = allocator.getByteBlock();
-    bufferUpto++;
-
-    byteUpto = 0;
-    byteOffset += DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE;
-  }
-
-  public int newSlice(final int size) {
-    if (byteUpto > DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE-size)
-      nextBuffer();
-    final int upto = byteUpto;
-    byteUpto += size;
-    buffer[byteUpto-1] = 16;
-    return upto;
-  }
-
-  // Size of each slice.  These arrays should be at most 16
-  // elements (index is encoded with 4 bits).  First array
-  // is just a compact way to encode X+1 with a max.  Second
-  // array is the length of each slice, ie first slice is 5
-  // bytes, next slice is 14 bytes, etc.
-  final static int[] nextLevelArray = {1, 2, 3, 4, 5, 6, 7, 8, 9, 9};
-  final static int[] levelSizeArray = {5, 14, 20, 30, 40, 40, 80, 80, 120, 200};
-  final static int FIRST_LEVEL_SIZE = levelSizeArray[0];
-
-  public int allocSlice(final byte[] slice, final int upto) {
-
-    final int level = slice[upto] & 15;
-    final int newLevel = nextLevelArray[level];
-    final int newSize = levelSizeArray[newLevel];
-
-    // Maybe allocate another block
-    if (byteUpto > DocumentsWriterRAMAllocator.BYTE_BLOCK_SIZE-newSize)
-      nextBuffer();
-
-    final int newUpto = byteUpto;
-    final int offset = newUpto + byteOffset;
-    byteUpto += newSize;
-
-    // Copy forward the past 3 bytes (which we are about
-    // to overwrite with the forwarding address):
-    buffer[newUpto] = slice[upto-3];
-    buffer[newUpto+1] = slice[upto-2];
-    buffer[newUpto+2] = slice[upto-1];
-
-    // Write forwarding address at end of last slice:
-    slice[upto-3] = (byte) (offset >>> 24);
-    slice[upto-2] = (byte) (offset >>> 16);
-    slice[upto-1] = (byte) (offset >>> 8);
-    slice[upto] = (byte) offset;
-        
-    // Write new level:
-    buffer[byteUpto-1] = (byte) (16|newLevel);
-
-    return newUpto+3;
-  }
-
-  // Fill in a BytesRef from term's length & bytes encoded in
-  // byte block
-  final BytesRef setBytesRef(BytesRef term, int textStart) {
-    final byte[] bytes = term.bytes = buffers[textStart >> DocumentsWriterRAMAllocator.BYTE_BLOCK_SHIFT];
-    int pos = textStart & DocumentsWriterRAMAllocator.BYTE_BLOCK_MASK;
-    if ((bytes[pos] & 0x80) == 0) {
-      // length is 1 byte
-      term.length = bytes[pos];
-      term.offset = pos+1;
-    } else {
-      // length is 2 bytes
-      term.length = (bytes[pos]&0x7f) + ((bytes[pos+1]&0xff)<<7);
-      term.offset = pos+2;
-    }
-    assert term.length >= 0;
-    return term;
-  }
-}
-
diff --git a/lucene/src/java/org/apache/lucene/index/DocumentsWriterRAMAllocator.java b/lucene/src/java/org/apache/lucene/index/DocumentsWriterRAMAllocator.java
deleted file mode 100644
index 0ece8de..0000000
--- a/lucene/src/java/org/apache/lucene/index/DocumentsWriterRAMAllocator.java
+++ /dev/null
@@ -1,147 +0,0 @@
-package org.apache.lucene.index;
-
-import java.text.NumberFormat;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.util.Constants;
-
-class DocumentsWriterRAMAllocator {
-  final ByteBlockAllocator byteBlockAllocator = new ByteBlockAllocator(BYTE_BLOCK_SIZE);
-  final ByteBlockAllocator perDocAllocator = new ByteBlockAllocator(PER_DOC_BLOCK_SIZE);
-
-  
-  class ByteBlockAllocator extends ByteBlockPool.Allocator {
-    final int blockSize;
-
-    ByteBlockAllocator(int blockSize) {
-      this.blockSize = blockSize;
-    }
-
-    ArrayList<byte[]> freeByteBlocks = new ArrayList<byte[]>();
-    
-    /* Allocate another byte[] from the shared pool */
-    @Override
-    byte[] getByteBlock() {
-      final int size = freeByteBlocks.size();
-      final byte[] b;
-      if (0 == size) {
-        b = new byte[blockSize];
-        // Always record a block allocated, even if
-        // trackAllocations is false.  This is necessary
-        // because this block will be shared between
-        // things that don't track allocations (term
-        // vectors) and things that do (freq/prox
-        // postings).
-        numBytesUsed += blockSize;
-      } else
-        b = freeByteBlocks.remove(size-1);
-      return b;
-    }
-
-    /* Return byte[]'s to the pool */
-    @Override
-    void recycleByteBlocks(byte[][] blocks, int start, int end) {
-      for(int i=start;i<end;i++) {
-        freeByteBlocks.add(blocks[i]);
-      }
-    }
-
-    @Override
-    void recycleByteBlocks(List<byte[]> blocks) {
-      final int size = blocks.size();
-      for(int i=0;i<size;i++) {
-        freeByteBlocks.add(blocks.get(i));
-      }
-    }
-  }
-
-  private ArrayList<int[]> freeIntBlocks = new ArrayList<int[]>();
-
-  /* Allocate another int[] from the shared pool */
-  int[] getIntBlock() {
-    final int size = freeIntBlocks.size();
-    final int[] b;
-    if (0 == size) {
-      b = new int[INT_BLOCK_SIZE];
-      // Always record a block allocated, even if
-      // trackAllocations is false.  This is necessary
-      // because this block will be shared between
-      // things that don't track allocations (term
-      // vectors) and things that do (freq/prox
-      // postings).
-      numBytesUsed += INT_BLOCK_SIZE*INT_NUM_BYTE;
-    } else
-      b = freeIntBlocks.remove(size-1);
-    return b;
-  }
-
-  void bytesUsed(long numBytes) {
-    numBytesUsed += numBytes;
-  }
-
-  /* Return int[]s to the pool */
-  void recycleIntBlocks(int[][] blocks, int start, int end) {
-    for(int i=start;i<end;i++)
-      freeIntBlocks.add(blocks[i]);
-  }
-
-  long getRAMUsed() {
-    return numBytesUsed;
-  }
-
-  long numBytesUsed;
-
-  NumberFormat nf = NumberFormat.getInstance();
-
-  final static int PER_DOC_BLOCK_SIZE = 1024;
-  
-  // Coarse estimates used to measure RAM usage of buffered deletes
-  final static int OBJECT_HEADER_BYTES = 8;
-  final static int POINTER_NUM_BYTE = Constants.JRE_IS_64BIT ? 8 : 4;
-  final static int INT_NUM_BYTE = 4;
-  final static int CHAR_NUM_BYTE = 2;
-
-  /* Rough logic: HashMap has an array[Entry] w/ varying
-     load factor (say 2 * POINTER).  Entry is object w/ Term
-     key, BufferedDeletes.Num val, int hash, Entry next
-     (OBJ_HEADER + 3*POINTER + INT).  Term is object w/
-     String field and String text (OBJ_HEADER + 2*POINTER).
-     We don't count Term's field since it's interned.
-     Term's text is String (OBJ_HEADER + 4*INT + POINTER +
-     OBJ_HEADER + string.length*CHAR).  BufferedDeletes.num is
-     OBJ_HEADER + INT. */
- 
-  final static int BYTES_PER_DEL_TERM = 8*POINTER_NUM_BYTE + 5*OBJECT_HEADER_BYTES + 6*INT_NUM_BYTE;
-
-  /* Rough logic: del docIDs are List<Integer>.  Say list
-     allocates ~2X size (2*POINTER).  Integer is OBJ_HEADER
-     + int */
-  final static int BYTES_PER_DEL_DOCID = 2*POINTER_NUM_BYTE + OBJECT_HEADER_BYTES + INT_NUM_BYTE;
-
-  /* Rough logic: HashMap has an array[Entry] w/ varying
-     load factor (say 2 * POINTER).  Entry is object w/
-     Query key, Integer val, int hash, Entry next
-     (OBJ_HEADER + 3*POINTER + INT).  Query we often
-     undercount (say 24 bytes).  Integer is OBJ_HEADER + INT. */
-  final static int BYTES_PER_DEL_QUERY = 5*POINTER_NUM_BYTE + 2*OBJECT_HEADER_BYTES + 2*INT_NUM_BYTE + 24;
-
-  /* Initial chunks size of the shared byte[] blocks used to
-     store postings data */
-  final static int BYTE_BLOCK_SHIFT = 15;
-  final static int BYTE_BLOCK_SIZE = 1 << BYTE_BLOCK_SHIFT;
-  final static int BYTE_BLOCK_MASK = BYTE_BLOCK_SIZE - 1;
-  final static int BYTE_BLOCK_NOT_MASK = ~BYTE_BLOCK_MASK;
-
-  final static int MAX_TERM_LENGTH_UTF8 = BYTE_BLOCK_SIZE-2;
-
-  /* Initial chunks size of the shared int[] blocks used to
-     store postings data */
-  final static int INT_BLOCK_SHIFT = 13;
-  final static int INT_BLOCK_SIZE = 1 << INT_BLOCK_SHIFT;
-  final static int INT_BLOCK_MASK = INT_BLOCK_SIZE - 1;
-
-  String toMB(long v) {
-    return nf.format(v/1024./1024.);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/IndexReaderPool.java b/lucene/src/java/org/apache/lucene/index/IndexReaderPool.java
deleted file mode 100644
index 0d7752d..0000000
--- a/lucene/src/java/org/apache/lucene/index/IndexReaderPool.java
+++ /dev/null
@@ -1,263 +0,0 @@
-package org.apache.lucene.index;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-
-import org.apache.lucene.store.BufferedIndexInput;
-import org.apache.lucene.store.Directory;
-
-/** Holds shared SegmentReader instances. IndexWriter uses
- *  SegmentReaders for 1) applying deletes, 2) doing
- *  merges, 3) handing out a real-time reader.  This pool
- *  reuses instances of the SegmentReaders in all these
- *  places if it is in "near real-time mode" (getReader()
- *  has been called on this instance). */
-public class IndexReaderPool {
-
-  private final Map<SegmentInfo,SegmentReader> readerMap = new HashMap<SegmentInfo,SegmentReader>();
-
-  private final Directory directory;
-  private final IndexWriterConfig config;
-  private final IndexWriter writer;
-  
-  public IndexReaderPool(IndexWriter writer, Directory directory, IndexWriterConfig config) {
-    this.directory = directory;
-    this.config = config;
-    this.writer = writer;
-  }
-  
-  /** Forcefully clear changes for the specified segments,
-   *  and remove from the pool.   This is called on successful merge. */
-  synchronized void clear(SegmentInfos infos) throws IOException {
-    if (infos == null) {
-      for (Map.Entry<SegmentInfo,SegmentReader> ent: readerMap.entrySet()) {
-        ent.getValue().hasChanges = false;
-      }
-    } else {
-      for (final SegmentInfo info: infos) {
-        if (readerMap.containsKey(info)) {
-          readerMap.get(info).hasChanges = false;
-        }
-      }     
-    }
-  }
-  
-  /**
-   * Release the segment reader (i.e. decRef it and close if there
-   * are no more references.
-   * @param sr
-   * @throws IOException
-   */
-  public synchronized void release(SegmentReader sr) throws IOException {
-    release(sr, false);
-  }
-  
-  /**
-   * Release the segment reader (i.e. decRef it and close if there
-   * are no more references.
-   * @param sr
-   * @throws IOException
-   */
-  public synchronized void release(SegmentReader sr, boolean drop) throws IOException {
-
-    final boolean pooled = readerMap.containsKey(sr.getSegmentInfo());
-
-    assert !pooled | readerMap.get(sr.getSegmentInfo()) == sr;
-
-    // Drop caller's ref; for an external reader (not
-    // pooled), this decRef will close it
-    sr.decRef();
-
-    if (pooled && (drop || (!writer.poolReaders && sr.getRefCount() == 1))) {
-
-      // We are the last ref to this reader; since we're
-      // not pooling readers, we release it:
-      readerMap.remove(sr.getSegmentInfo());
-
-      // nocommit
-      //assert !sr.hasChanges || Thread.holdsLock(IndexWriter.this);
-
-      // Drop our ref -- this will commit any pending
-      // changes to the dir
-      boolean success = false;
-      try {
-        sr.close();
-        success = true;
-      } finally {
-        if (!success && sr.hasChanges) {
-          // Abandon the changes & retry closing:
-          sr.hasChanges = false;
-          try {
-            sr.close();
-          } catch (Throwable ignore) {
-            // Keep throwing original exception
-          }
-        }
-      }
-    }
-  }
-  
-  /** Remove all our references to readers, and commits
-   *  any pending changes. */
-  synchronized void close() throws IOException {
-    Iterator<Map.Entry<SegmentInfo,SegmentReader>> iter = readerMap.entrySet().iterator();
-    while (iter.hasNext()) {
-      
-      Map.Entry<SegmentInfo,SegmentReader> ent = iter.next();
-
-      SegmentReader sr = ent.getValue();
-      if (sr.hasChanges) {
-        assert writer.infoIsLive(sr.getSegmentInfo());
-        sr.startCommit();
-        boolean success = false;
-        try {
-          sr.doCommit(null);
-          success = true;
-        } finally {
-          if (!success) {
-            sr.rollbackCommit();
-          }
-        }
-      }
-
-      iter.remove();
-
-      // NOTE: it is allowed that this decRef does not
-      // actually close the SR; this can happen when a
-      // near real-time reader is kept open after the
-      // IndexWriter instance is closed
-      sr.decRef();
-    }
-  }
-  
-  /**
-   * Commit all segment reader in the pool.
-   * @throws IOException
-   */
-  synchronized void commit() throws IOException {
-    for (Map.Entry<SegmentInfo,SegmentReader> ent : readerMap.entrySet()) {
-
-      SegmentReader sr = ent.getValue();
-      if (sr.hasChanges) {
-        assert writer.infoIsLive(sr.getSegmentInfo());
-        sr.startCommit();
-        boolean success = false;
-        try {
-          sr.doCommit(null);
-          success = true;
-        } finally {
-          if (!success) {
-            sr.rollbackCommit();
-          }
-        }
-      }
-    }
-  }
-  
-  /**
-   * Returns a ref to a clone.  NOTE: this clone is not
-   * enrolled in the pool, so you should simply close()
-   * it when you're done (ie, do not call release()).
-   */
-  public synchronized SegmentReader getReadOnlyClone(SegmentInfo info, boolean doOpenStores, int termInfosIndexDivisor) throws IOException {
-    SegmentReader sr = get(info, doOpenStores, BufferedIndexInput.BUFFER_SIZE, termInfosIndexDivisor);
-    try {
-      return (SegmentReader) sr.clone(true);
-    } finally {
-      sr.decRef();
-    }
-  }
- 
-  /**
-   * Obtain a SegmentReader from the readerPool.  The reader
-   * must be returned by calling {@link #release(SegmentReader)}
-   * @see #release(SegmentReader)
-   * @param info
-   * @param doOpenStores
-   * @throws IOException
-   */
-  public synchronized SegmentReader get(SegmentInfo info, boolean doOpenStores) throws IOException {
-    return get(info, doOpenStores, BufferedIndexInput.BUFFER_SIZE, config.getReaderTermsIndexDivisor());
-  }
-
-  /**
-   * Obtain a SegmentReader from the readerPool.  The reader
-   * must be returned by calling {@link #release(SegmentReader)}
-   * 
-   * @see #release(SegmentReader)
-   * @param info
-   * @param doOpenStores
-   * @param readBufferSize
-   * @param termsIndexDivisor
-   * @throws IOException
-   */
-  public synchronized SegmentReader get(SegmentInfo info, boolean doOpenStores, int readBufferSize, int termsIndexDivisor) throws IOException {
-
-    if (writer.poolReaders) {
-      readBufferSize = BufferedIndexInput.BUFFER_SIZE;
-    }
-
-    SegmentReader sr = readerMap.get(info);
-    if (sr == null) {
-      // TODO: we may want to avoid doing this while
-      // synchronized
-      // Returns a ref, which we xfer to readerMap:
-      sr = SegmentReader.get(false, info.dir, info, readBufferSize, doOpenStores, termsIndexDivisor, config.getCodecProvider());
-
-      if (info.dir == directory) {
-        // Only pool if reader is not external
-        readerMap.put(info, sr);
-      }
-    } else {
-      if (doOpenStores) {
-        sr.openDocStores();
-      }
-      if (termsIndexDivisor != -1) {
-        // If this reader was originally opened because we
-        // needed to merge it, we didn't load the terms
-        // index.  But now, if the caller wants the terms
-        // index (eg because it's doing deletes, or an NRT
-        // reader is being opened) we ask the reader to
-        // load its terms index.
-        sr.loadTermsIndex(termsIndexDivisor);
-      }
-    }
-
-    // Return a ref to our caller
-    if (info.dir == directory) {
-      // Only incRef if we pooled (reader is not external)
-      sr.incRef();
-    }
-    return sr;
-  }
-
-  // Returns a ref
-  public synchronized SegmentReader getIfExists(SegmentInfo info) throws IOException {
-    SegmentReader sr = readerMap.get(info);
-    if (sr != null) {
-      sr.incRef();
-    }
-    return sr;
-  }
-}
-
-
diff --git a/lucene/src/java/org/apache/lucene/store/FilterDirectory.java b/lucene/src/java/org/apache/lucene/store/FilterDirectory.java
deleted file mode 100644
index 30c860b..0000000
--- a/lucene/src/java/org/apache/lucene/store/FilterDirectory.java
+++ /dev/null
@@ -1,111 +0,0 @@
-package org.apache.lucene.store;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-
-public abstract class FilterDirectory extends Directory {
-  private final Directory delegate;
-  
-  public FilterDirectory(Directory delegate) {
-    this.delegate = delegate;
-  }
-  
-  @Override
-  public String[] listAll() throws IOException {
-    return delegate.listAll();
-  }
-
-  @Override
-  public boolean fileExists(String name) throws IOException {
-    return delegate.fileExists(name);
-  }
-
-  @Override
-  public long fileModified(String name) throws IOException {
-    return delegate.fileModified(name);
-  }
-  
-  @Override
-  public void touchFile(String name) throws IOException {
-    delegate.touchFile(name);
-  }
-
-  @Override
-  public void deleteFile(String name) throws IOException {
-    delegate.deleteFile(name);
-  }
-
-  @Override
-  public long fileLength(String name) throws IOException {
-    return delegate.fileLength(name);
-  }
-
-  @Override
-  public IndexOutput createOutput(String name) throws IOException {
-    return delegate.createOutput(name);
-  }
-
-  @Override
-  public IndexInput openInput(String name) throws IOException {
-    return delegate.openInput(name);
-  }
-
-  @Override
-  public void close() throws IOException {
-    delegate.close();
-  }
-  
-  @Deprecated @Override
-  public void sync(String name) throws IOException { // TODO 4.0 kill me
-    delegate.sync(name);
-  }
-  
-  public void sync(Collection<String> names) throws IOException { // TODO 4.0 make me abstract
-    delegate.sync(names);
-  }
-
-  public IndexInput openInput(String name, int bufferSize) throws IOException {
-    return delegate.openInput(name, bufferSize);
-  }
-  
-  public Lock makeLock(String name) {
-    return delegate.makeLock(name);
-  }
-  
-  public void clearLock(String name) throws IOException {
-    delegate.clearLock(name);
-  }
-  
-  public void setLockFactory(LockFactory lockFactory) {
-    delegate.setLockFactory(lockFactory);
-  }
-  
-  public LockFactory getLockFactory() {
-    return delegate.getLockFactory();
-  }
-  
-  public String getLockID() {
-    return delegate.getLockID();
-  }
-  
-  public void copy(Directory to, String src, String dest) throws IOException {
-    delegate.copy(to, src, dest);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/util/ThreadSafeCloneableSortedMap.java b/lucene/src/java/org/apache/lucene/util/ThreadSafeCloneableSortedMap.java
deleted file mode 100644
index 7ff5acf..0000000
--- a/lucene/src/java/org/apache/lucene/util/ThreadSafeCloneableSortedMap.java
+++ /dev/null
@@ -1,156 +0,0 @@
-package org.apache.lucene.util;
-
-import java.util.*;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-
-public class ThreadSafeCloneableSortedMap<K, V> implements SortedMap<K, V>, Cloneable {
-
-  private volatile SortedMap<K, V> copy;
-  private Lock cloneLock = new ReentrantLock();
-  private final SortedMap<K, V> delegate;
-
-  private ThreadSafeCloneableSortedMap(SortedMap<K, V> delegate) {this.delegate = delegate;}
-
-  public static <K, V> ThreadSafeCloneableSortedMap<K, V> getThreadSafeSortedMap(
-      SortedMap<K, V> delegate) {
-    return new ThreadSafeCloneableSortedMap<K, V>(delegate);
-  }
-
-  public SortedMap<K, V> getReadCopy() {
-    SortedMap<K, V> m = copy;
-    if (m != null) {
-      return m;
-    }
-
-    // we have to clone
-    cloneLock.lock();
-    try {
-      // check again - maybe a different thread was faster
-      m = copy;
-      if (m != null) {
-        return m;
-      }
-
-      // still no copy there - create one now
-      SortedMap<K, V> clone = clone(delegate);
-      copy = clone;
-      return clone;
-    } finally {
-      cloneLock.unlock();
-    }
-
-  }
-
-  protected SortedMap<K, V> clone(SortedMap<K, V> map) {
-    if (map instanceof TreeMap<?, ?>) {
-      return (TreeMap<K,V>) ((TreeMap<?,?>) map).clone();
-    }
-    
-    throw new IllegalArgumentException(map.getClass() + " not supported. Overwrite clone(SortedMap<K, V> map) in a custom subclass to support this map.");
-  }
-  
-  private abstract static class Task<T> {
-    abstract T run();
-  }
-
-  private final <T> T withLock(Task<T> task) {
-    copy = null;
-    cloneLock.lock();
-    try {
-      return task.run();
-    } finally {
-      cloneLock.unlock();
-    }
-  }
-
-  @Override public Comparator<? super K> comparator() {
-    return delegate.comparator();
-  }
-
-  @Override public SortedMap<K, V> subMap(K fromKey, K toKey) {
-    return delegate.subMap(fromKey, toKey);
-  }
-
-  @Override public SortedMap<K, V> headMap(K toKey) {
-    return delegate.headMap(toKey);
-  }
-
-  @Override public SortedMap<K, V> tailMap(K fromKey) {
-    return delegate.tailMap(fromKey);
-  }
-
-  @Override public K firstKey() {
-    return delegate.firstKey();
-  }
-
-  @Override public K lastKey() {
-    return delegate.lastKey();
-  }
-
-  @Override public int size() {
-    return delegate.size();
-  }
-
-  @Override public boolean isEmpty() {
-    return delegate.isEmpty();
-  }
-
-  @Override public boolean containsKey(Object key) {
-    return delegate.containsKey(key);
-  }
-
-  @Override public boolean containsValue(Object value) {
-    return delegate.containsValue(value);
-  }
-
-  @Override public V get(Object key) {
-    return delegate.get(key);
-  }
-
-  @Override public V put(final K key, final V value) {
-    return withLock(new Task<V>() {
-      @Override V run() {return delegate.put(key, value);}
-    });
-  }
-
-  @Override public V remove(final Object key) {
-    return withLock(new Task<V>() {
-      @Override V run() {return delegate.remove(key);}
-    });
-  }
-
-  @Override public void putAll(final Map<? extends K, ? extends V> m) {
-    withLock(new Task<V>() {
-      @Override V run() {
-        delegate.putAll(m);
-        return null;
-      }
-    });
-  }
-
-  @Override public void clear() {
-    withLock(new Task<V>() {
-      @Override V run() {
-        delegate.clear();
-        return null;
-      }
-    });
-  }
-
-  //
-  // nocommit : don't use these methods to modify the map.
-  // TODO implement Set and Collection that acquire lock for modifications
-  //
-  @Override public Set<K> keySet() {
-    return delegate.keySet();
-  }
-
-  @Override public Collection<V> values() {
-    return delegate.values();
-  }
-
-  @Override public Set<Entry<K, V>> entrySet() {
-    return delegate.entrySet();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/util/LuceneTestCaseJ4.java b/lucene/src/test/org/apache/lucene/util/LuceneTestCaseJ4.java
deleted file mode 100644
index 40ba071..0000000
--- a/lucene/src/test/org/apache/lucene/util/LuceneTestCaseJ4.java
+++ /dev/null
@@ -1,513 +0,0 @@
-package org.apache.lucene.util;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.index.ConcurrentMergeScheduler;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LogDocMergePolicy;
-import org.apache.lucene.index.LogMergePolicy;
-import org.apache.lucene.index.SerialMergeScheduler;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FieldCache;
-import org.apache.lucene.search.FieldCache.CacheEntry;
-import org.apache.lucene.util.FieldCacheSanityChecker.Insanity;
-import org.apache.lucene.index.codecs.CodecProvider;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.preflexrw.PreFlexRWCodec;
-
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Assert;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestWatchman;
-import org.junit.runners.model.FrameworkMethod;
-
-import java.io.File;
-import java.io.PrintStream;
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Iterator;
-import java.util.Random;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-import java.util.WeakHashMap;
-import java.util.Collections;
-import java.lang.reflect.Method;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.fail;
-
-/**
- * Base class for all Lucene unit tests, Junit4 variant.
- * Replaces LuceneTestCase.
- * <p>
- * </p>
- * <p>
- * If you
- * override either <code>setUp()</code> or
- * <code>tearDown()</code> in your unit test, make sure you
- * call <code>super.setUp()</code> and
- * <code>super.tearDown()</code>
- * </p>
- *
- * @After - replaces setup
- * @Before - replaces teardown
- * @Test - any public method with this annotation is a test case, regardless
- * of its name
- * <p>
- * <p>
- * See Junit4 <a href="http://junit.org/junit/javadoc/4.7/">documentation</a> for a complete list of features.
- * <p>
- * Import from org.junit rather than junit.framework.
- * <p>
- * You should be able to use this class anywhere you used LuceneTestCase
- * if you annotate your derived class correctly with the annotations above
- * @see #assertSaneFieldCaches(String)
- */
-
-
-// If we really need functionality in runBare override from LuceneTestCase,
-// we can introduce RunBareWrapper and override runChild, and add the
-// @RunWith annotation as below. runChild will be called for
-// every test. But the functionality we used to
-// get from that override is provided by InterceptTestCaseEvents
-//@RunWith(RunBareWrapper.class)
-public class LuceneTestCaseJ4 {
-
-  /**
-   * true iff tests are run in verbose mode. Note: if it is false, tests are not
-   * expected to print any messages.
-   */
-  public static final boolean VERBOSE = Boolean.getBoolean("tests.verbose");
-
-  /** Use this constant when creating Analyzers and any other version-dependent stuff.
-   * <p><b>NOTE:</b> Change this when development starts for new Lucene version:
-   */
-  public static final Version TEST_VERSION_CURRENT = Version.LUCENE_40;
-
-  /** Create indexes in this directory, optimally use a subdir, named after the test */
-  public static final File TEMP_DIR;
-  static {
-    String s = System.getProperty("tempDir", System.getProperty("java.io.tmpdir"));
-    if (s == null)
-      throw new RuntimeException("To run tests, you need to define system property 'tempDir' or 'java.io.tmpdir'.");
-    TEMP_DIR = new File(s);
-  }
-
-  // by default we randomly pick a different codec for
-  // each test case (non-J4 tests) and each test class (J4
-  // tests)
-  /** Gets the codec to run tests with. */
-  static final String TEST_CODEC = System.getProperty("tests.codec", "random");
-
-  /**
-   * A random multiplier which you should use when writing random tests:
-   * multiply it by the number of iterations
-   */
-  public static final int RANDOM_MULTIPLIER = Integer.parseInt(System.getProperty("random.multiplier", "1"));
-  
-  private int savedBoolMaxClauseCount;
-
-  private volatile Thread.UncaughtExceptionHandler savedUncaughtExceptionHandler = null;
-  
-  /** Used to track if setUp and tearDown are called correctly from subclasses */
-  private boolean setup;
-
-  private static class UncaughtExceptionEntry {
-    public final Thread thread;
-    public final Throwable exception;
-    
-    public UncaughtExceptionEntry(Thread thread, Throwable exception) {
-      this.thread = thread;
-      this.exception = exception;
-    }
-  }
-  private List<UncaughtExceptionEntry> uncaughtExceptions = Collections.synchronizedList(new ArrayList<UncaughtExceptionEntry>());
-  
-  // checks if class correctly annotated
-  private static final Object PLACEHOLDER = new Object();
-  private static final Map<Class<? extends LuceneTestCaseJ4>,Object> checkedClasses =
-    Collections.synchronizedMap(new WeakHashMap<Class<? extends LuceneTestCaseJ4>,Object>());
-  
-  // saves default codec: we do this statically as many build indexes in @beforeClass
-  private static String savedDefaultCodec;
-  private static String codec;
-  private static Codec preFlexSav;
-  
-  // returns current PreFlex codec
-  public static Codec installPreFlexRW() {
-    final Codec preFlex = CodecProvider.getDefault().lookup("PreFlex");
-    if (preFlex != null) {
-      CodecProvider.getDefault().unregister(preFlex);
-    }
-    CodecProvider.getDefault().register(new PreFlexRWCodec());
-    return preFlex;
-  }
-
-  // returns current PreFlex codec
-  public static void restorePreFlex(Codec preFlex) {
-    Codec preFlexRW = CodecProvider.getDefault().lookup("PreFlex");
-    if (preFlexRW != null) {
-      CodecProvider.getDefault().unregister(preFlexRW);
-    }
-    CodecProvider.getDefault().register(preFlex);
-  }
-
-  @BeforeClass
-  public static void beforeClassLuceneTestCaseJ4() {
-    savedDefaultCodec = CodecProvider.getDefaultCodec();
-    codec = TEST_CODEC;
-    if (codec.equals("random"))
-      codec = CodecProvider.CORE_CODECS[seedRnd.nextInt(CodecProvider.CORE_CODECS.length)];
-
-    // If we're running w/ PreFlex codec we must swap in the
-    // test-only PreFlexRW codec (since core PreFlex can
-    // only read segments):
-    if (codec.equals("PreFlex")) {
-      preFlexSav = installPreFlexRW();
-    } 
-
-    CodecProvider.setDefaultCodec(codec);
-  }
-  
-  @AfterClass
-  public static void afterClassLuceneTestCaseJ4() {
-    // Restore read-only PreFlex codec:
-    if (codec.equals("PreFlex")) {
-      restorePreFlex(preFlexSav);
-    } 
-    CodecProvider.setDefaultCodec(savedDefaultCodec);
-  }
-
-  // This is how we get control when errors occur.
-  // Think of this as start/end/success/failed
-  // events.
-  @Rule
-  public final TestWatchman intercept = new TestWatchman() {
-
-    @Override
-    public void failed(Throwable e, FrameworkMethod method) {
-      reportAdditionalFailureInfo();
-      super.failed(e, method);
-    }
-
-    @Override
-    public void starting(FrameworkMethod method) {
-      // set current method name for logging
-      LuceneTestCaseJ4.this.name = method.getName();
-      // check if the current test's class annotated all test* methods with @Test
-      final Class<? extends LuceneTestCaseJ4> clazz = LuceneTestCaseJ4.this.getClass();
-      if (!checkedClasses.containsKey(clazz)) {
-        checkedClasses.put(clazz, PLACEHOLDER);
-        for (Method m : clazz.getMethods()) {
-          if (m.getName().startsWith("test") && m.getAnnotation(Test.class) == null) {
-            fail("In class '" + clazz.getName() + "' the method '" + m.getName() + "' is not annotated with @Test.");
-          }
-        }
-      }
-      super.starting(method);
-    }
-    
-  };
-
-  @Before
-  public void setUp() throws Exception {
-    Assert.assertFalse("ensure your tearDown() calls super.tearDown()!!!", setup);
-    setup = true;
-    savedUncaughtExceptionHandler = Thread.getDefaultUncaughtExceptionHandler();
-    Thread.setDefaultUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
-      public void uncaughtException(Thread t, Throwable e) {
-        uncaughtExceptions.add(new UncaughtExceptionEntry(t, e));
-        if (savedUncaughtExceptionHandler != null)
-          savedUncaughtExceptionHandler.uncaughtException(t, e);
-      }
-    });
-    
-    ConcurrentMergeScheduler.setTestMode();
-    savedBoolMaxClauseCount = BooleanQuery.getMaxClauseCount();
-    seed = null;
-  }
-
-
-  /**
-   * Forcible purges all cache entries from the FieldCache.
-   * <p>
-   * This method will be called by tearDown to clean up FieldCache.DEFAULT.
-   * If a (poorly written) test has some expectation that the FieldCache
-   * will persist across test methods (ie: a static IndexReader) this
-   * method can be overridden to do nothing.
-   * </p>
-   *
-   * @see FieldCache#purgeAllCaches()
-   */
-  protected void purgeFieldCache(final FieldCache fc) {
-    fc.purgeAllCaches();
-  }
-
-  protected String getTestLabel() {
-    return getClass().getName() + "." + getName();
-  }
-
-  @After
-  public void tearDown() throws Exception {
-    Assert.assertTrue("ensure your setUp() calls super.setUp()!!!", setup);
-    setup = false;
-    BooleanQuery.setMaxClauseCount(savedBoolMaxClauseCount);
-    try {
-
-      if (!uncaughtExceptions.isEmpty()) {
-        System.err.println("The following exceptions were thrown by threads:");
-        for (UncaughtExceptionEntry entry : uncaughtExceptions) {
-          System.err.println("*** Thread: " + entry.thread.getName() + " ***");
-          entry.exception.printStackTrace(System.err);
-        }
-        fail("Some threads threw uncaught exceptions!");
-      }
-
-      // calling assertSaneFieldCaches here isn't as useful as having test 
-      // classes call it directly from the scope where the index readers 
-      // are used, because they could be gc'ed just before this tearDown 
-      // method is called.
-      //
-      // But it's better then nothing.
-      //
-      // If you are testing functionality that you know for a fact 
-      // "violates" FieldCache sanity, then you should either explicitly 
-      // call purgeFieldCache at the end of your test method, or refactor
-      // your Test class so that the inconsistant FieldCache usages are 
-      // isolated in distinct test methods  
-      assertSaneFieldCaches(getTestLabel());
-
-      if (ConcurrentMergeScheduler.anyUnhandledExceptions()) {
-        // Clear the failure so that we don't just keep
-        // failing subsequent test cases
-        ConcurrentMergeScheduler.clearUnhandledExceptions();
-        fail("ConcurrentMergeScheduler hit unhandled exceptions");
-      }
-    } finally {
-      purgeFieldCache(FieldCache.DEFAULT);
-    }
-    
-    Thread.setDefaultUncaughtExceptionHandler(savedUncaughtExceptionHandler);
-  }
-
-  /**
-   * Asserts that FieldCacheSanityChecker does not detect any
-   * problems with FieldCache.DEFAULT.
-   * <p>
-   * If any problems are found, they are logged to System.err
-   * (allong with the msg) when the Assertion is thrown.
-   * </p>
-   * <p>
-   * This method is called by tearDown after every test method,
-   * however IndexReaders scoped inside test methods may be garbage
-   * collected prior to this method being called, causing errors to
-   * be overlooked. Tests are encouraged to keep their IndexReaders
-   * scoped at the class level, or to explicitly call this method
-   * directly in the same scope as the IndexReader.
-   * </p>
-   *
-   * @see FieldCacheSanityChecker
-   */
-  protected void assertSaneFieldCaches(final String msg) {
-    final CacheEntry[] entries = FieldCache.DEFAULT.getCacheEntries();
-    Insanity[] insanity = null;
-    try {
-      try {
-        insanity = FieldCacheSanityChecker.checkSanity(entries);
-      } catch (RuntimeException e) {
-        dumpArray(msg + ": FieldCache", entries, System.err);
-        throw e;
-      }
-
-      assertEquals(msg + ": Insane FieldCache usage(s) found",
-              0, insanity.length);
-      insanity = null;
-    } finally {
-
-      // report this in the event of any exception/failure
-      // if no failure, then insanity will be null anyway
-      if (null != insanity) {
-        dumpArray(msg + ": Insane FieldCache usage(s)", insanity, System.err);
-      }
-
-    }
-  }
-
-  /**
-   * Convinience method for logging an iterator.
-   *
-   * @param label  String logged before/after the items in the iterator
-   * @param iter   Each next() is toString()ed and logged on it's own line. If iter is null this is logged differnetly then an empty iterator.
-   * @param stream Stream to log messages to.
-   */
-  public static void dumpIterator(String label, Iterator<?> iter,
-                                  PrintStream stream) {
-    stream.println("*** BEGIN " + label + " ***");
-    if (null == iter) {
-      stream.println(" ... NULL ...");
-    } else {
-      while (iter.hasNext()) {
-        stream.println(iter.next().toString());
-      }
-    }
-    stream.println("*** END " + label + " ***");
-  }
-
-  /**
-   * Convinience method for logging an array.  Wraps the array in an iterator and delegates
-   *
-   * @see #dumpIterator(String,Iterator,PrintStream)
-   */
-  public static void dumpArray(String label, Object[] objs,
-                               PrintStream stream) {
-    Iterator<?> iter = (null == objs) ? null : Arrays.asList(objs).iterator();
-    dumpIterator(label, iter, stream);
-  }
-
-  /**
-   * Returns a {@link Random} instance for generating random numbers during the test.
-   * The random seed is logged during test execution and printed to System.out on any failure
-   * for reproducing the test using {@link #newRandom(long)} with the recorded seed
-   * .
-   */
-  public Random newRandom() {
-    if (seed != null) {
-      throw new IllegalStateException("please call LuceneTestCaseJ4.newRandom only once per test");
-    }
-    this.seed = Long.valueOf(seedRnd.nextLong());
-    if (VERBOSE) {
-      System.out.println("NOTE: random seed of testcase '" + getName() + "' is: " + this.seed);
-    }
-    return new Random(seed);
-  }
-
-  /**
-   * Returns a {@link Random} instance for generating random numbers during the test.
-   * If an error occurs in the test that is not reproducible, you can use this method to
-   * initialize the number generator with the seed that was printed out during the failing test.
-   */
-  public Random newRandom(long seed) {
-    if (this.seed != null) {
-      throw new IllegalStateException("please call LuceneTestCaseJ4.newRandom only once per test");
-    }
-    System.out.println("WARNING: random seed of testcase '" + getName() + "' is fixed to: " + seed);
-    this.seed = Long.valueOf(seed);
-    return new Random(seed);
-  }
-
-  private static final Map<Class<? extends LuceneTestCaseJ4>,Long> staticSeeds =
-    Collections.synchronizedMap(new WeakHashMap<Class<? extends LuceneTestCaseJ4>,Long>());
-
-  /**
-   * Returns a {@link Random} instance for generating random numbers from a beforeclass
-   * annotated method.
-   * The random seed is logged during test execution and printed to System.out on any failure
-   * for reproducing the test using {@link #newStaticRandom(Class, long)} with the recorded seed
-   * .
-   */
-  public static Random newStaticRandom(Class<? extends LuceneTestCaseJ4> clazz) {
-    Long seed = seedRnd.nextLong();
-    staticSeeds.put(clazz, seed);
-    return new Random(seed);
-  }
-  
-  /**
-   * Returns a {@link Random} instance for generating random numbers from a beforeclass
-   * annotated method.
-   * If an error occurs in the test that is not reproducible, you can use this method to
-   * initialize the number generator with the seed that was printed out during the failing test.
-   */
-  public static Random newStaticRandom(Class<? extends LuceneTestCaseJ4> clazz, long seed) {
-    staticSeeds.put(clazz, Long.valueOf(seed));
-    System.out.println("WARNING: random static seed of testclass '" + clazz + "' is fixed to: " + seed);
-    return new Random(seed);
-  }
-
-  /** create a new index writer config with random defaults */
-  public static IndexWriterConfig newIndexWriterConfig(Random r, Version v, Analyzer a) {
-    IndexWriterConfig c = new IndexWriterConfig(v, a);
-    if (r.nextBoolean()) {
-      c.setMergePolicy(new LogDocMergePolicy());
-    }
-    if (r.nextBoolean()) {
-      c.setMergeScheduler(new SerialMergeScheduler());
-    }
-    if (r.nextBoolean()) {
-      c.setMaxBufferedDocs(_TestUtil.nextInt(r, 2, 1000));
-    }
-    if (r.nextBoolean()) {
-      c.setTermIndexInterval(_TestUtil.nextInt(r, 1, 1000));
-    }
-    
-    if (c.getMergePolicy() instanceof LogMergePolicy) {
-      LogMergePolicy logmp = (LogMergePolicy) c.getMergePolicy();
-      logmp.setUseCompoundFile(r.nextBoolean());
-      logmp.setCalibrateSizeByDeletes(r.nextBoolean());
-      logmp.setMergeFactor(_TestUtil.nextInt(r, 2, 20));
-    }
-    
-    c.setReaderPooling(r.nextBoolean());
-    return c;
-  }
-
-  public String getName() {
-    return this.name;
-  }
-  
-  /** Gets a resource from the classpath as {@link File}. This method should only be used,
-   * if a real file is needed. To get a stream, code should prefer
-   * {@link Class#getResourceAsStream} using {@code this.getClass()}.
-   */
-  protected File getDataFile(String name) throws IOException {
-    try {
-      return new File(this.getClass().getResource(name).toURI());
-    } catch (Exception e) {
-      throw new IOException("Cannot find resource: " + name);
-    }
-  }
-
-  // We get here from InterceptTestCaseEvents on the 'failed' event....
-  public void reportAdditionalFailureInfo() {
-    Long staticSeed = staticSeeds.get(getClass());
-    if (staticSeed != null) {
-      System.out.println("NOTE: random static seed of testclass '" + getName() + "' was: " + staticSeed);
-    }
-    
-    if (TEST_CODEC.equals("random")) {
-      System.out.println("NOTE: random codec of testcase '" + getName() + "' was: " + codec);
-    }
-
-    if (seed != null) {
-      System.out.println("NOTE: random seed of testcase '" + getName() + "' was: " + seed);
-    }
-  }
-
-  // recorded seed
-  protected Long seed = null;
-
-  // static members
-  private static final Random seedRnd = new Random();
-
-  private String name = "<unknown>";
-}

