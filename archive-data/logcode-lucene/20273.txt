GitDiffStart: e01aad89fe24f2482eb2e8c0e39f8272704abe80 | Thu May 14 10:09:22 2009 +0000
diff --git a/BUILD.txt b/BUILD.txt
index f5167e1..3b471f6 100644
--- a/BUILD.txt
+++ b/BUILD.txt
@@ -3,14 +3,14 @@ Lucene Build Instructions
 $Id$
 
 Basic steps:
-  0) Install JDK 1.4 (or greater), Ant 1.6.2 (or greater)
+  0) Install JDK 1.4 (or greater), Ant 1.6.3 (or greater)
   1) Download Lucene from Apache and unpack it
   2) Connect to the top-level of your Lucene installation
   3) Install JavaCC (optional)
   4) Run ant
 
 Step 0) Set up your development environment (JDK 1.4 or greater,
-Ant 1.6.2 or greater)
+Ant 1.6.3 or greater)
 
 We'll assume that you know how to get and set up the JDK - if you
 don't, then we suggest starting at http://java.sun.com and learning
@@ -18,7 +18,7 @@ more about Java, before returning to this README. Lucene runs with
 JDK 1.4 and later.
 
 Like many Open Source java projects, Lucene uses Apache Ant for build
-control.  Specifically, you MUST use Ant version 1.6.2 or greater.
+control.  Specifically, you MUST use Ant version 1.6.3 or greater.
 
 Ant is "kind of like make without make's wrinkles".  Ant is
 implemented in java and uses XML-based configuration files.  You can
diff --git a/CHANGES.txt b/CHANGES.txt
index 46d00cc..f1c93a1 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -308,6 +308,12 @@ Bug fixes
     cross-correlate Spans from different fields.
     (Paul Cowan and Chris Hostetter)
     
+25. LUCENE-1629: Add SmartChineseAnalyzer to contrib/analyzers.  It
+    improves on CJKAnalyzer and ChineseAnalyzer by handling Chinese
+    sentences properly.  SmartChineseAnalyzer uses a Hidden Markov
+    Model to tokenize Chinese words in a more intelligent way.
+    (Xiaoping Gao via Mike McCandless)
+  
 Optimizations
 
  1. LUCENE-1427: Fixed QueryWrapperFilter to not waste time computing
diff --git a/common-build.xml b/common-build.xml
index 654afa7..2cd086a 100644
--- a/common-build.xml
+++ b/common-build.xml
@@ -233,6 +233,12 @@
       destdir="${build.dir}/classes/java">
       <classpath refid="classpath"/>
     </compile>
+
+    <!-- Copy the resources folder (if existent) -->
+    <copy todir="${build.dir}/classes/java" includeEmptyDirs="false">
+      <globmapper from="resources/*" to="*" handledirsep="yes"/>
+      <fileset dir="src" includes="resources/**"/>
+    </copy>
   </target>
 
   <target name="compile" depends="compile-core">
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/ar/stopwords.txt b/contrib/analyzers/src/java/org/apache/lucene/analysis/ar/stopwords.txt
deleted file mode 100644
index 4bb557b..0000000
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/ar/stopwords.txt
+++ /dev/null
@@ -1,350 +0,0 @@
-# This file was created by Jacques Savoy and is distributed under the BSD license.
-# See http://members.unine.ch/jacques.savoy/clef/index.html.
-# Also see http://www.opensource.org/licenses/bsd-license.html
-Ø¨
-Ø§
-Ø£
-?
-Ø¹Ø´Ø±
-Ø¹Ø¨Ø¯
-Ø¹Ø¯Ø¯
-Ø¹Ø¯Ø©
-Ø¹Ø´Ø±Ø©
-Ø¹Ø¯?
-Ø¹Ø§?
-Ø¹Ø§?Ø§
-Ø¹Ø±?Ø§Øª
-Ø¹?
-Ø¹?Ø¯
-Ø¹?Ø§?
-Ø¹?Ø¯?Ø§
-Ø¹??
-Ø¹??
-Ø¹???
-Ø¹???Ø§
-Ø¹???Ø©
-Ø²?Ø§Ø±Ø©
-Ø³Ø¨Øª?Ø¨Ø±
-Ø³Ø§Ø±Ø§????
-Ø³?Ø©
-Ø³?Ø±?Ø§
-Ø³??Ø§Øª
-ØªØ´Ø±??
-Øª?
-Øª??Ø²
-Ø¶Ø¯
-Ø¨Ø¹Ø¯
-Ø¨Ø¹Ø¶
-Ø§Ø¹Ø§Ø¯Ø©
-Ø§Ø¹??
-Ø§Ø¹??Øª
-Ø­Ø²Ø¨
-Ø­Ø²?Ø±Ø§?
-Ø¨Ø³Ø¨Ø¨
-Ø§Ø³Ø±Ø§Ø¦??
-Ø­Ø³??
-Ø­Øª?
-Ø§Øª?Ø§?
-ØµØ±Ø¨
-Ø§Ø°Ø§
-Ø§Ø­Ø¯
-Ø§Ø«Ø±
-ØºØ²Ø©
-Ø¨Ø±Ø³
-Ø¨Ø§Ø³?
-Ø§Ø¬Øª?Ø§Ø¹
-ØºØ¯Ø§
-Ø´Ø®ØµØ§
-ØµØ¨Ø§Ø­
-Ø§Ø·Ø§Ø±
-Ø§Ø±Ø¨Ø¹Ø©
-Ø¨ØºØ¯Ø§Ø¯
-Ø§Ø®Ø±?
-Ø¨Ø§Ø±?Ø³
-Ø±Ø§Ø¨??
-Ø´Ø±?
-Ø¨Ø§?
-Ø§Ø¨?
-Ø§Ø¬?
-Øº?Ø±
-Ø­Ø±?Ø©
-Ø±Ø¦?Ø³
-Ø¬Ø¯?Ø¯Ø©
-Ø§Ø·?Ø§?
-Ø¨Ø´??
-Ø¨Ø·??Ø©
-ØµØ­??Ø©
-Ø­Ø§??Ø§
-Ø¨?
-Ø¨?
-Ø«?
-Ø§?
-Ø§?
-Ø§?
-Ø§?
-Ø¨?Ø§
-Ø¬?Ø©
-Øµ?Ø±
-Ø­?Ø«
-Ø§?Ø¯
-Ø§?Ø§
-Ø§?Ø§
-Ø§?Ø¹Ø³?Ø±?Ø©
-Ø§?Ø¹Ø±Ø§?
-Ø§?Ø¹Ø§Øµ?Ø©
-Ø§?Ø¹Ø±Ø¨?Ø©
-Ø§?Ø¹Ø±Ø§??
-Ø§?Ø¹Ø±Ø§??Ø©
-Ø§?Ø¹Ø§?
-Ø§?Ø¹Ø§??
-Ø§?Ø¹?Ø§?Ø§Øª
-Ø§?Ø¹??
-Ø§?Ø³
-Ø§?Ø³Ø¹?Ø¯?Ø©
-Ø§?Ø³Ø§Ø¹Ø©
-Ø§?Ø³Ø¨Øª
-Ø§?Ø³Ø§Ø¨?
-Ø±?Ø³?Ø§
-Ø§?Ø³?Ø·Ø©
-Ø§?Ø³?Ø·Ø§Øª
-Ø§?Ø³?Ø§?
-Ø§?ØªØ¹Ø§??
-Ø§?ØªØ­Ø±?Ø±
-Ø§?Øª?
-Ø§?Øª?
-Ø§?Øª?Ø¨Ø±
-Ø¯?Ø±Ø©
-Ø§?Ø«Ø±
-Ø§?Ø§Ø±
-Ø§?Ø¶Ø§
-Ø§?Ø¬Ø²Ø§Ø¦Ø±
-Ø­?Ø§Ø³
-Ø§?Ø§Ø³Ø±Ø§Ø¦???
-Ø§?Ø§Ø³Ø±Ø§Ø¦???Ø©
-Ø§?Ø§Ø³Ø¨?Ø¹
-Ø§?Ø§Ø³?Ø­Ø©
-Ø§?Ø§Ø³?Ø§??Ø©
-Ø°?Ø±Øª
-Ø§?Ø§ØªØ­Ø§Ø¯
-Ø§?Ø§Øª?Ø§?
-Ø«?Ø§Ø«Ø©
-Ø§?Ø­Ø±Ø¨
-Ø§?Ø§Ø­Ø¯
-Ø§?Ø°Ø§Øª?
-Ø§?Ø´Ø±Ø·Ø©
-Ø§?Ø§Ø±Ø¨Ø¹Ø§Ø¡
-Ø§?ØºØ±Ø¨?Ø©
-Ø§?Ø®Ø§Ø±Ø¬?Ø©
-Ø§?Ø§Ø±Ø¯?
-Ø§?Ø´Ø±?
-Ø§?Ø±Ø§?
-Ø§?Ø­Ø¯?Ø¯
-Ø§?Ø±Ø¦?Ø³
-Ø§?Ø§Ø®?Ø±Ø©
-Ø§?Ø«Ø§??
-Ø§?Ø«Ø§??Ø©
-Ø§?Ø§Ø«???
-Ø´?Ø§?
-Ø¨?Ø§?
-Ø¯?Ø´?
-Ø§?Ø°?
-Ø§?Ø°?
-Ø§?Ø§?
-Ø§?Ø§?
-Ø§?Ø§?
-Ø®?Ø§?
-Ø§?Ø´?Ø®
-Ø§?Ø¬?Ø´
-Ø§?Ø¯?Ø±
-Ø§?Ø¶?Ø©
-Ø§?Ø¬?Ø¹Ø©
-Ø¨?Ø±?Ø²
-Ø§?Ø§?Ø³Ø·
-Ø§?Ø±?Ø³?
-Ø§?Ø¨?Ø³?Ø©
-Ø§?Ø±?Ø³?Ø©
-Ø¨?Ø±?Øª
-Ø§?Ø§?ØªØ®Ø§Ø¨Ø§Øª
-Ø§?Ø¨?Ø§Ø¯
-Ø§?Ø¯?Ø§Ø¹
-Ø§?Ø«?Ø«Ø§Ø¡
-Ø§?Ø§?Ø¨Ø§Ø¡
-Ø§?Ø«?Ø§Ø«Ø§Ø¡
-Ø§?Ø§?Ø±?Ø¨?
-Ø­?Ø§??
-Ø§?Ø°??
-Ø§?Ø¯??
-Ø§?Ø­??
-Ø§?Ø§??
-Ø§?Ø§??
-Ø§?Ø§??
-Ø§?Ø¯??Ø©
-Ø§?Ø®??Ø¬
-Ø§?Ø®??Ø³
-Ø§?Ø§??Ø±??
-Ø§?Ø§??Ø±??Ø©
-Ø§?Ø¯???
-Ø§?Ø§???
-Ø§?Ø¯???Ø©
-Ø§?Ø­???Ø©
-Ø¨??
-Ø°??
-Ø¯??
-Ø¯??
-Ø­??
-Ø­??
-Ø§??
-Ø§??
-Ø§??
-Ø§??
-Ø¶??
-Ø¬??Ø¨
-Ø¯??Ø©
-Ø§??Ø§
-Ø¬??Ø¹
-Ø§??Ø²Ø±Ø§Ø¡
-Ø§??ØªØ­Ø¯Ø«
-Ø§??ØªØ­Ø¯Ø©
-Ø¯??Ø§Ø±
-Ø§??Ø§Ø±
-Ø§??Ø¶Ø¹
-Ø§??Ø¯Ø³
-Ø§??Ø­Øª?Ø©
-Ø§??ØµØ¯Ø±
-Ø§??Ø¨Ø§Ø±Ø§Ø©
-Ø§??ØµØ±?
-Ø§??Ø§Ø¶?
-Ø§??ØµØ±?Ø©
-Ø§??Ø±Ø­?Ø©
-Ø§??Ø¯?
-Ø§??Ø¬?Ø©
-Ø§??Ø¬?Ø³
-Ø§??Ø±?Ø³?
-Ø§??Ø±?Ø³?Ø©
-Ø§??Ø§?Ø±Ø©
-Ø§??Ø¯??Ø©
-Ø§??Ø§??Ø§
-Ø§??Ø·??Ø©
-Ø§??Ø¬??Ø¹Ø©
-Ø§???
-Ø§???Ø³Ø·???
-Ø§???Ø³Ø·???Ø©
-Ø§???Ø³Ø·?????
-Ø§???Øª
-Ø§???Ø±Ø±
-Ø§???Ø§Øª
-Ø§???Ø§Ø¦?
-Ø§???Ø¨?
-Ø§???Ø·?Ø©
-Ø§???Ø§?Ø§Øª
-Ø§???Ø§?Ø¶Ø§Øª
-Ø§????
-Ø§????
-Ø§????
-Ø§????
-Ø§????Øª
-?
-?
-?
-?6
-?Ø¯
-?Ø§
-?Ø§
-?Ø¹
-?Ø²Ø§Ø±Ø©
-?Ø²?Ø±
-?Ø³Ø§Ø¡
-?Øª?
-?Ø±Ø©
-?ØµØ±
-?Ø°Ø§
-?Ø§Ø²
-?Ø£Ø³
-?Ø§Ø³Ø±
-?Ø±Ø§Ø±
-?ØµØ¯Ø±
-?Ø§Ø­Ø¯
-?Ø·Ø§Ø¹
-?ØµØ§Ø¯Ø±
-?Ø¨Ø§Ø±Ø§Ø©
-?Ø¨Ø§Ø±?
-?Ø§Ø¶Ø§?
-?Ø§Ø¶Ø§?Øª
-?Ø±Ø§?Ø³
-?Ø§Ø´?Ø·?
-?Ø§?
-?Ø¨?
-?Ø§?
-?Ø§?
-?Ø¯?
-?Ø­?
-?Ø°?
-?Ø§?
-?Ø­?Ø¯
-?Ø§?Ø¯
-?Ø°?Ø±
-?Ø¬?Ø³
-?Ø±?Ø³Ø§
-?Ø±?Ø³Øª??Ø±
-?Ø§?Øª
-?Ø§?Ø¶Ø­
-?Ø¨?Ø§?
-?Ø§??
-?Ø¯??Ø©
-?Ø¬??Ø¹Ø©
-?Ø§???
-??
-??
-??
-??
-??
-??
-??
-??
-??
-??Ø©
-??Ø§
-??Ø§
-??Ø°
-??Ø¯
-??Ø§
-??Ø³?
-??Ø³??
-??Øª?
-??Ø§Ø¡
-??Ø±Ø©
-??Ø·Ø©
-??Ø§Øª
-??Ø§Ø¨?
-??Ø¯?
-??Ø§?
-??Ø§?
-??Ø§?
-??Ø·?Ø©
-??Ø¸?Ø©
-??Ø§?Ø©
-??Ø§?Ø©
-??Ø§?Øª
-??Ø§?Øª
-??Ø§??
-???
-???
-???
-???
-???
-???
-???
-???
-???
-???
-???Ø§
-???Ø§
-???Ø§Ø±
-???Ø§?Ø©
-????
-????
-????Øª??
-?????
-?????
-?????
-?????Ø±?
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/SmartChineseAnalyzer.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/SmartChineseAnalyzer.java
new file mode 100644
index 0000000..9737541
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/SmartChineseAnalyzer.java
@@ -0,0 +1,129 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.io.Reader;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.PorterStemFilter;
+import org.apache.lucene.analysis.StopFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.cn.smart.SentenceTokenizer;
+import org.apache.lucene.analysis.cn.smart.WordSegmenter;
+import org.apache.lucene.analysis.cn.smart.WordTokenizer;
+
+/**
+ * 
+ * SmartChineseAnalyzer ???ä¸???½ä¸­???è¯?¨¡??? ?½å??©ç?æ¦??å¯¹æ?è¯??å­??è¡??ä¼?????
+ * å¹¶å?åµ????okenizerï¼?????å¤??ä¸????··???????????
+ * 
+ * å®??????ºä????è¯??å¤??é¢?????é©??ç§?¤«æ¨¡å?(HMM)ï¼? ?©ç?å¤§é?è¯??åº??è®???¥ç?è®¡æ?è¯??æ±??è¯????·³è½?????
+ * ä»???????äº??è®¡ç?????´ä¸ªæ±???¥å?è®¡ç???ä¼¼ç?(likelihood)???????
+ * 
+ * ??¸º?ºè??????è¦???¸æ?ä¿??è¯?????è®¡å?¼ï?SmartChineseAnalyzer???è¡??è¦??å®???¸ä?ç½??å¦?????è¯??ä½?½®è¯·å???
+ * org.apache.lucene.analysis.cn.smart.AnalyzerProfile
+ * 
+ * SmartChineseAnalyzer???æ³??è¯??åº???¸æ????ictclas1.0é¡¹ç?(http://www.ictclas.org)ï¼?
+ * ?¶ä¸­è¯??å·²è???ww.ictclas.org??pache license v2(APLv2)?????????µå¾ªAPLv2???ä»¶ä?ï¼??è¿???·ä½¿?¨ã??
+ * ?¨æ???°¢www.ictclas.orgä»¥å?ictclas???è½?»¶??·¥ä½?ºº??????å¥??ï¼?
+ * 
+ * @see org.apache.lucene.analysis.cn.smart.AnalyzerProfile
+ * 
+ */
+public class SmartChineseAnalyzer extends Analyzer {
+
+  private Set stopWords = null;
+
+  private WordSegmenter wordSegment;
+
+  public SmartChineseAnalyzer() {
+    this(false);
+  }
+
+  /**
+   * SmartChineseAnalyzer???å¸??é»?????è¯??ï¼?¸»è¦?????ç¬????????å¸??ç»??ä¸???°æ??¹ç??·ï?
+   * ??»¥å°?seDefaultStopWordsè®¾ä¸ºtrueï¼? useDefaultStopWordsä¸?alse?¶ä?ä½¿ç?ä»»ä????è¯?
+   * 
+   * @param useDefaultStopWords
+   */
+  public SmartChineseAnalyzer(boolean useDefaultStopWords) {
+    if (useDefaultStopWords) {
+      stopWords = loadStopWords(this.getClass().getResourceAsStream(
+          "stopwords.txt"));
+    }
+    wordSegment = new WordSegmenter();
+  }
+
+  /**
+   * ä½¿ç????ä¹?????ä½¿ç???½®???æ­¢è?åº?????è¯??ä»¥ä½¿??martChineseAnalyzer.loadStopWords(InputStream)??½½
+   * 
+   * @param stopWords
+   * @see SmartChineseAnalyzer.loadStopWords(InputStream)
+   */
+  public SmartChineseAnalyzer(Set stopWords) {
+    this.stopWords = stopWords;
+    wordSegment = new WordSegmenter();
+  }
+
+  public TokenStream tokenStream(String fieldName, Reader reader) {
+    TokenStream result = new SentenceTokenizer(reader);
+    result = new WordTokenizer(result, wordSegment);
+    // result = new LowerCaseFilter(result);
+    // ä¸????è¦?owerCaseFilterï¼??ä¸?egTokenFilterå·²ç?å°????????ç¬?½¬?¢æ?å°??
+    // stemå¤?¸¥?¼ä?, This is not bug, this feature:)
+    result = new PorterStemFilter(result);
+    if (stopWords != null) {
+      result = new StopFilter(result, stopWords, false);
+    }
+    return result;
+  }
+
+  /**
+   * ä»???¨è???»¶ä¸??è½½å??¨è?ï¼? ???è¯??ä»¶æ?????TF-8ç¼????????ä»¶ï? æ¯??è¡??ä¸?ä¸???¨è?ï¼?³¨????¨â??//??? ???è¯?¸­???ä¸?????ç¬??ï¼? ä¸??ç©ºæ?ï¼?
+   * ä»¥å?ä½¿ç???¤ªé«????ç´¢å????ä¸?¤§?????
+   * 
+   * @param input ???è¯??ä»?
+   * @return ???è¯?????HashSet
+   */
+  public static Set loadStopWords(InputStream input) {
+    String line;
+    Set stopWords = new HashSet();
+    try {
+      BufferedReader br = new BufferedReader(new InputStreamReader(input,
+          "UTF-8"));
+      while ((line = br.readLine()) != null) {
+        if (line.indexOf("//") != -1) {
+          line = line.substring(0, line.indexOf("//"));
+        }
+        line = line.trim();
+        if (line.length() != 0)
+          stopWords.add(line.toLowerCase());
+      }
+      br.close();
+    } catch (IOException e) {
+      System.err.println("WARNING: cannot open stop words list!");
+    }
+    return stopWords;
+  }
+
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/package.html b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/package.html
index f88efb3..0a3201c 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/package.html
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/package.html
@@ -1,5 +1,51 @@
-<html><head></head>
+<html>
+<head></head>
 <body>
 Analyzer for Chinese.
+
+
+<h2>About SmartChineseAnalyzer</h2>
+<p>SmartChineseAnalyzer ???ä¸???½ä¸­???è¯?¨¡??? ä¸? ChineseAnalyzer ï¼?????ä¸??å­????
+CJKAnalyzer ï¼?????ä¸¤ä¸ªæ±??ï¼????? å®??å¤???¨æ????æ±???¥å?è¿????ä¼????? å¹¶å?åµ????okenizerï¼?
+?½æ??????¸­?±æ?æ··å???????å®¹ã?????martChineseAnalyzer????¸å???????ä½?¸­????</p>
+
+<p>å®??????ºä????è¯??å¤??é¢?????é©??ç§?¤«æ¨¡å?(HMM)ï¼? ?©ç?å¤§é?è¯??åº??è®???¥ç?è®¡æ?è¯??æ±??è¯????·³è½?????
+ä»???????äº??è®¡ç?????´ä¸ªæ±???¥å?è®¡ç???ä¼¼ç?(likelihood)???????</p>
+
+<p>ä¸?????æ¨¡å????è¯?????è¾?, ?±æ???»¥????ºè?????´ç????å­?????è¯??ï¼? ä»????é«??ç´¢ç???¡®????
+<pre>è¯??ï¼? ???ä¸??äº?</pre>
+<ol>
+	<li>SmartChineseAnalyzer: ??????ä¸??ï¼?ºº</li>
+	<li>ChineseAnalyzer: ??????ä¸???½ï?äº?</li>
+	<li>CJKAnalyzer: ???ï¼??ä¸??ä¸??ï¼??äº?</li>
+</ol>
+</p>
+
+<h3>???è¯?????ç½?</h3>
+<p>??¸º?ºè??????è¦???¸æ?ä¿??è¯?????è®¡å?¼ï?é»?????ä¸??SmartChineseAnalyzerä½¿ç???½®????¸å?ï¼????è¦??å®??è¯??åº??ï¼??è¦??å®???¸ä?ç½??å¦?????è¯??ä½?½®è¯·å???
+org.apache.lucene.analysis.cn.smart.AnalyzerProfile??</p>
+
+<p><b>è¯?????è½½å???ä¸ºï?<a
+	href="http://code.google.com/p/imdict-chinese-analyzer/downloads/list">http://code.google.com/p/imdict-chinese-analyzer/downloads/list</a>
+</b> ä¸?½½??»¶analysis-data.zipä¿???°æ??°ï?è§£å??³å?ä½¿ç???</p>
+
+<p>??ç®???????è¯??åº?????å°±æ?è¿???¶å?ä¸????-Danalysis.data.dir
+<pre>å¦?? java -Danalysis.data.dir=/path/to/analysis-data com.example.YourApplication</pre>
+</p>
+
+<h3>???è¦??</h3>
+<p>SmartChineseAnalyzer??VMè¦??java 1.4??»¥ä¸?????Lucene
+è¦??2.4.0??»¥ä¸?????Lucene 2.3.X???è¯¥ä???»¥ä½¿ç?ï¼?????æµ??ï¼????è¦???¨æ????è¡??è¯???</p>
+
+<h3>æº??ä»¶å????ç¼??</h3>
+?¤ç?å®??äº???¶ç???»¶å¤??SmartChineseAnalyzer?????????Javaæº???½é???TF-8ç¼??ï¼?
+????¨è???????ç¼??Javaæº?????æ³¨æ????æ­£ç¡®???å¼??ä»¥é???º§??¹±???è¯???
+
+<h3>SmartChineseAnalyzer?????</h3>
+<p>SmartChineseAnalyzer???æ³??è¯??åº???¸æ????ictclas1.0é¡¹ç?(<a
+	href="http://www.ictclas.org">http://www.ictclas.org</a>)ï¼?
+?¶ä¸­è¯??å·²ç??????ººwww.ictclas.org???ï¼?»¥apache license
+v2(APLv2)??????????µå¾ªAPLv2???ä»¶ä?ï¼??è¿???·ä½¿?¨ã??
+?¨æ???°¢www.ictclas.orgä»¥å?ictclas???è½?»¶??·¥ä½?ºº???è¾??å·¥ä????ç§?????</p>
 </body>
 </html>
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java
new file mode 100644
index 0000000..09f2dea
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java
@@ -0,0 +1,112 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.util.Properties;
+
+/**
+ * ?¨é?è®¤æ??µä?ï¼?martChineseAnalyzer??½®????¸å????è®¤å?æ­¢è?åº??å·²ç?ç»??å°??ï¼???·å?ä»¥ç??¥ä½¿?¨ã??
+ * 
+ * ?¹æ????ä¸???¨æ???è¦?½¿?¨æ?å®??è¯??åº?????è¯??ï¼???¶é?è¦????rg.apache.lucene.analysis.cn.smart. hhmmä¸??
+ * coredict.mem ?? bigramdict.memï¼? ?¶å?ä½¿ç?AnalyzerProfile?¥æ?å®???¸å??????
+ * 
+ * AnalyzerProfile ?¨æ?å¯»æ?å­?????è¯???°æ? ????¨è??°æ????å½?? è¯¥ç?å½??åº???? bigramdict.dct, coredict.dct,
+ * stopwords_utf8.txt, ?¥æ?è¿??ä¾??å¦??ï¼?
+ * 
+ * <ol>
+ * <li>è¯»å?ç³»ç?è¿???¶å??°ï?-Danalysis.data.dir=/path/to/analysis-dataï¼????²¡???ç»§ç»­ä¸????</li>
+ * <li>?§è??½ä»¤??????å½?¸­???å­??analysis-data???</li>
+ * <li>?§è??½ä»¤??ib/???ä¸???????nalysis-data???</li>
+ * <li>?§è??½ä»¤??????å½?¸­???å­??analysis.properties??»¶</li>
+ * <li>?§è??½ä»¤??ib/???ä¸???????nalysis.properties??»¶</li>
+ * </ol>
+ * 
+ * ?¶ä¸­analysis.properties??»¶analysis.data.dir???analysis-data??????¨ä?ç½?.
+ * analysis.properties??»¶???å®¹ç¤ºä¾??
+ * 
+ * <pre>
+ * analysis.data.dir=D:/path/to/analysis-data/
+ * </pre>
+ * 
+ * å½??ä¸??analysis-data????¶ï?ANALYSIS_DATA_DIRè®¾ç½®ä¸?""ï¼??æ­¤å?ä½¿ç????å¿?¡»?¨ç?åº???¾å????data???ï¼??å¦??
+ * 
+ * <pre>
+ * AnalyzerProfile.ANALYSIS_DATA_DIR = &quot;/path/to/analysis-data&quot;;
+ * </pre>
+ * 
+ */
+public class AnalyzerProfile {
+
+  public static String ANALYSIS_DATA_DIR = "";
+
+  static {
+    init();
+  }
+
+  private static void init() {
+    String dirName = "analysis-data";
+    String propName = "analysis.properties";
+
+    // è¯»å?ç³»ç?è®¾ç½®ï¼??è¿???¶å??¥å??°ï?-Danalysis.data.dir=/path/to/analysis-data
+    ANALYSIS_DATA_DIR = System.getProperty("analysis.data.dir", "");
+    if (ANALYSIS_DATA_DIR.length() != 0)
+      return;
+
+    File[] cadidateFiles = new File[] { new File("./" + dirName),
+        new File("./lib/" + dirName), new File("./" + propName),
+        new File("./lib/" + propName) };
+    for (int i = 0; i < cadidateFiles.length; i++) {
+      File file = cadidateFiles[i];
+      if (file.exists()) {
+        if (file.isDirectory()) {
+          ANALYSIS_DATA_DIR = file.getAbsolutePath();
+        } else if (file.isFile() && getAnalysisDataDir(file).length() != 0) {
+          ANALYSIS_DATA_DIR = getAnalysisDataDir(file);
+        }
+        break;
+      }
+    }
+
+    if (ANALYSIS_DATA_DIR.length() == 0) {
+      // ??¤º?¨æ?????°è??¸æ?ä»¶å¤¹
+      System.err
+          .println("WARNING: Can not found lexical dictionary directory!");
+      System.err
+          .println("WARNING: This will cause unpredictable exceptions in your application!");
+      System.err
+          .println("WARNING: Please refer to the manual to download the dictionaries.");
+    }
+
+  }
+
+  private static String getAnalysisDataDir(File propFile) {
+    Properties prop = new Properties();
+    try {
+      FileInputStream input = new FileInputStream(propFile);
+      prop.load(input);
+      String dir = prop.getProperty("analysis.data.dir", "");
+      input.close();
+      return dir;
+    } catch (IOException e) {
+    }
+    return "";
+  }
+
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/CharType.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/CharType.java
new file mode 100644
index 0000000..c3e448b
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/CharType.java
@@ -0,0 +1,38 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart;
+
+public class CharType {
+
+  public final static int DELIMITER = 0;
+
+  public final static int LETTER = 1;
+
+  public final static int DIGIT = 2;
+
+  public final static int HANZI = 3;
+
+  public final static int SPACE_LIKE = 4;
+
+  // (?¨è????)???ç¬??ï¼??è§??å­??ï¼??å­??ï¼??å­??ç©ºæ?ï¼?"\t\r\n"ç­?©º?¼æ??¢è?å­??
+  public final static int FULLWIDTH_LETTER = 5;
+
+  public final static int FULLWIDTH_DIGIT = 6; // ?¨è?å­??ï¼??æ¯???°å?
+
+  public final static int OTHER = 7;
+
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java
new file mode 100644
index 0000000..7ac7c07
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java
@@ -0,0 +1,102 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart;
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.Reader;
+
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.Tokenizer;
+
+/**
+ * 
+ * ???ä¸?ä¸???´å?å­??Tokenï¼????»¶ä¸???ºï????ä¸?æ­¥å?è¯??å¯¹è±¡
+ * 
+ */
+public class SentenceTokenizer extends Tokenizer {
+
+  /**
+   * ?¨æ?????¥å?????¹ç??? ???ï¼??ï¼?,!?;
+   */
+  public final static String PUNCTION = "???ï¼??ï¼?,!?;";
+
+  private StringBuffer buffer = new StringBuffer();
+
+  private BufferedReader bufferInput;
+
+  private int tokenStart = 0, tokenEnd = 0;
+
+  private Token t = new Token();
+
+  public SentenceTokenizer(Reader reader) {
+    bufferInput = new BufferedReader(reader, 2048);
+  }
+
+  public Token next() throws IOException {
+    buffer.setLength(0);
+    int ci;
+    char ch, pch;
+    boolean atBegin = true;
+    tokenStart = tokenEnd;
+    ci = bufferInput.read();
+    ch = (char) ci;
+
+    while (true) {
+      if (ci == -1) {
+        break;
+      } else if (PUNCTION.indexOf(ch) != -1) {
+        // ?¾å?äº??å­??å°?
+        buffer.append(ch);
+        tokenEnd++;
+        break;
+      } else if (atBegin && Utility.SPACES.indexOf(ch) != -1) {
+        tokenStart++;
+        tokenEnd++;
+        ci = bufferInput.read();
+        ch = (char) ci;
+      } else {
+        buffer.append(ch);
+        atBegin = false;
+        tokenEnd++;
+        pch = ch;
+        ci = bufferInput.read();
+        ch = (char) ci;
+        // å¦??ç¢°ä?äº?¸¤ä¸??ç»??skipå­??ï¼??å¦?¸¤ä¸??è½??ä¸¤ä¸ªç©ºæ??????
+        // ä¸?ä¸??è½??ä¸?ä¸?©º?¼ç?ç­??å°??è§?¸º?¥å?ç»??ï¼?»¥???å­?¤ª?¿è???å­??è¶?
+        if (Utility.SPACES.indexOf(ch) != -1
+            && Utility.SPACES.indexOf(pch) != -1) {
+          // buffer.append(ch);
+          tokenEnd++;
+          break;
+        }
+      }
+    }
+    if (buffer.length() == 0)
+      return null;
+    else {
+      t.clear();
+      t.reinit(buffer.toString(), tokenStart, tokenEnd, "sentence");
+      return t;
+    }
+  }
+
+  public void close() throws IOException {
+    bufferInput.close();
+  }
+
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/Utility.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/Utility.java
new file mode 100644
index 0000000..59a4037
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/Utility.java
@@ -0,0 +1,165 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart;
+
+public class Utility {
+
+  public static final char[] STRING_CHAR_ARRAY = new String("??##ä¸?")
+      .toCharArray();
+
+  public static final char[] NUMBER_CHAR_ARRAY = new String("??##??")
+      .toCharArray();
+
+  public static final char[] START_CHAR_ARRAY = new String("å§?##å§?")
+      .toCharArray();
+
+  public static final char[] END_CHAR_ARRAY = new String("??##??").toCharArray();
+
+  public static final char[] COMMON_DELIMITER = new char[] { ',' };
+
+  /**
+   * ??è¦?·³è¿??ç¬??ï¼??å¦??è¡¨ç?ï¼??è½???¢è?ç­????
+   */
+  public static final String SPACES = " ??\t\r\n";
+
+  public static final int MAX_FREQUENCE = 2079997 + 80000;
+
+  /**
+   * æ¯??ä¸¤ä¸ª?´æ??°ç???¤§å°?, ???ä»??ç»??ä¸?å®??ç½??å§???¸ªæ¯??, å½??æ¬¡ç?ç­???½å?è¾¾æ?å°¾æ?, è¿???¸ç?, ??????è¾¾æ?å°¾ç?å¤§ä??°è¾¾??°¾??;
+   * å½???°è¾¾??°¾?¶æ?ä¸?ä½???¸ç?, è¯¥ä?ç½???¼å¤§???ç»?¤§äº????
+   * 
+   * @param larray
+   * @param lstartIndex larray??µ·å§??ç½?
+   * @param rarray
+   * @param rstartIndex rarray??µ·å§??ç½?
+   * @return 0è¡¨ç¤º?¸ç?ï¼?1è¡¨ç¤ºlarray > rarray, -1è¡¨ç¤ºlarray < rarray
+   */
+  public static int compareArray(char[] larray, int lstartIndex, char[] rarray,
+      int rstartIndex) {
+
+    if (larray == null) {
+      if (rarray == null || rstartIndex >= rarray.length)
+        return 0;
+      else
+        return -1;
+    } else {
+      // larray != null
+      if (rarray == null) {
+        if (lstartIndex >= larray.length)
+          return 0;
+        else
+          return 1;
+      }
+    }
+
+    int li = lstartIndex, ri = rstartIndex;
+    while (li < larray.length && ri < rarray.length && larray[li] == rarray[ri]) {
+      li++;
+      ri++;
+    }
+    if (li == larray.length) {
+      if (ri == rarray.length) {
+        // ä¸¤è????´ç?ç­????°¾ï¼??æ­¤è????ç­??ä¹?°±?????0
+        return 0;
+      } else {
+        // æ­¤æ?ä¸????i>rarray.length??????ri<rarray.length
+        // è¡¨ç¤ºlarrayå·²ç?ç»??ï¼?arrayæ²¡æ?ç»??ï¼??æ­?array < rarrayï¼????-1
+        return -1;
+      }
+    } else {
+      // æ­¤æ?ä¸????i>larray.length??????li < larray.lengthï¼?¡¨ç¤?iæ²¡æ??°è¾¾larray??°¾
+      if (ri == rarray.length) {
+        // larrayæ²¡æ?ç»??ï¼????arrayå·²ç?ç»??ï¼??æ­?array > rarray
+        return 1;
+      } else {
+        // æ­¤æ?ä¸????i>rarray.length??????ri < rarray.length
+        // è¡¨ç¤ºlarray??array?½æ²¡????????????ä¸?ä¸????¤§å°????
+        if (larray[li] > rarray[ri])
+          return 1;
+        else
+          return -1;
+      }
+    }
+  }
+
+  /**
+   * ?¹æ?????¥å???¸¤ä¸??ç¬??ç»??å¤§å?ï¼??????¸º?????????¶ï?è¡¨ç¤º?¸ç?ï¼??ä¸?¸º????¶ï?????????ç¬?¸²?¹å?æ¯??
+   * 
+   * @param shortArray
+   * @param shortIndex
+   * @param longArray
+   * @param longIndex
+   * @return
+   */
+  public static int compareArrayByPrefix(char[] shortArray, int shortIndex,
+      char[] longArray, int longIndex) {
+
+    // ç©ºæ?ç»???????ç»?????ï¼?????index
+    if (shortArray == null)
+      return 0;
+    else if (longArray == null)
+      return (shortIndex < shortArray.length) ? 1 : 0;
+
+    int si = shortIndex, li = longIndex;
+    while (si < shortArray.length && li < longArray.length
+        && shortArray[si] == longArray[li]) {
+      si++;
+      li++;
+    }
+    if (si == shortArray.length) {
+      // shortArray ?? longArray??refix
+      return 0;
+    } else {
+      // æ­¤æ?ä¸????i>shortArray.length??????si <
+      // shortArray.lengthï¼?¡¨ç¤?iæ²¡æ??°è¾¾shortArray??°¾
+
+      // shortArrayæ²¡æ?ç»??ï¼????ongArrayå·²ç?ç»??ï¼??æ­?hortArray > longArray
+      if (li == longArray.length)
+        return 1;
+      else
+        // æ­¤æ?ä¸????i>longArray.length??????li < longArray.length
+        // è¡¨ç¤ºshortArray??ongArray?½æ²¡????????????ä¸?ä¸????¤§å°????
+        return (shortArray[si] > longArray[li]) ? 1 : -1;
+    }
+  }
+
+  public static int getCharType(char ch) {
+    // ??å¤?????å­?
+    if (ch >= 0x4E00 && ch <= 0x9FA5)
+      return CharType.HANZI;
+    if ((ch >= 0x0041 && ch <= 0x005A) || (ch >= 0x0061 && ch <= 0x007A))
+      return CharType.LETTER;
+    if (ch >= 0x0030 && ch <= 0x0039)
+      return CharType.DIGIT;
+    if (ch == ' ' || ch == '\t' || ch == '\r' || ch == '\n' || ch == '??')
+      return CharType.SPACE_LIKE;
+    // ????????å®???½æ????ç¬??äº?
+    if ((ch >= 0x0021 && ch <= 0x00BB) || (ch >= 0x2010 && ch <= 0x2642)
+        || (ch >= 0x3001 && ch <= 0x301E))
+      return CharType.DELIMITER;
+
+    // ?¨è?å­???ºå?
+    if ((ch >= 0xFF21 && ch <= 0xFF3A) || (ch >= 0xFF41 && ch <= 0xFF5A))
+      return CharType.FULLWIDTH_LETTER;
+    if (ch >= 0xFF10 && ch <= 0xFF19)
+      return CharType.FULLWIDTH_DIGIT;
+    if (ch >= 0xFE30 && ch <= 0xFF63)
+      return CharType.DELIMITER;
+    return CharType.OTHER;
+
+  }
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java
new file mode 100644
index 0000000..2a98ae6
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java
@@ -0,0 +1,87 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.cn.smart.hhmm.HHMMSegmenter;
+import org.apache.lucene.analysis.cn.smart.hhmm.SegToken;
+import org.apache.lucene.analysis.cn.smart.hhmm.SegTokenFilter;
+
+public class WordSegmenter {
+
+  private HHMMSegmenter hhmmSegmenter = new HHMMSegmenter();
+
+  private SegTokenFilter tokenFilter = new SegTokenFilter();
+
+  /**
+   * è°??HHMMSegmentç¨??å°?????sentence Token???ï¼?????è¯?????ä¿????oken Listä¸?
+   * 
+   * @param sentenceToken ?¥å???oken
+   * @param shortPathCount HHMMç®?????????è¦??ä¼?????????·¯å¾?¸ª?°ã??????å¤§å?è¯?????ç²¾ç¡®ï¼?????ç®?»£ä»·ä?è¾????
+   * @return ???ç»????oken List
+   */
+  public List segmentSentence(Token sentenceToken, int shortPathCount) {
+    String sentence = sentenceToken.term();
+
+    List segTokenList = hhmmSegmenter.process(sentence);
+
+    List result = new ArrayList();
+
+    // iä»?1??awTokens.length-2ï¼??å°±æ?è¯´å????##å§???????##????¸¤ä¸?awToken?»æ?
+    for (int i = 1; i < segTokenList.size() - 1; i++) {
+      result.add(convertSegToken((SegToken) segTokenList.get(i), sentence,
+          sentenceToken.startOffset(), "word"));
+    }
+    return result;
+
+  }
+
+  /**
+   * 
+   * å°?awTokenç±»å?è½????´¢å¼??è¦??Tokenç±»å?ï¼? ??¸ºç´¢å???è¦?awToken?¨å??¥ä¸­???å®¹ï? ???è½???¶é?è¦??å®???¥å???
+   * 
+   * @param rt
+   * @param sentence è½????è¦???¥å????
+   * @param sentenceStartOffset sentence?¨æ?ç«?¸­???å§??ç½?
+   * @param type tokenç±»å?ï¼??è®¤å?è¯¥æ?word
+   * @return
+   */
+  public Token convertSegToken(SegToken st, String sentence,
+      int sentenceStartOffset, String type) {
+    Token result;
+    switch (st.wordType) {
+      case WordType.STRING:
+      case WordType.NUMBER:
+      case WordType.FULLWIDTH_NUMBER:
+      case WordType.FULLWIDTH_STRING:
+        st.charArray = sentence.substring(st.startOffset, st.endOffset)
+            .toCharArray();
+        break;
+      default:
+        break;
+    }
+
+    st = tokenFilter.filter(st);
+
+    result = new Token(st.charArray, 0, st.charArray.length, st.startOffset
+        + sentenceStartOffset, st.endOffset + sentenceStartOffset);
+    return result;
+  }
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer.java
new file mode 100644
index 0000000..d2dd452
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer.java
@@ -0,0 +1,87 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart;
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+
+public class WordTokenizer extends Tokenizer {
+
+  /**
+   * ???ä¸»ç?åº??WordTokenizer????????½½??
+   */
+  private WordSegmenter wordSegmenter;
+
+  private TokenStream in;
+
+  private Iterator tokenIter;
+
+  private List tokenBuffer;
+
+  private Token sentenceToken = new Token();
+
+  /**
+   * è®¾è?ä¸??SentenceTokenizer???ä¸?å¤??å±????SentenceTokenizer???å­???ºï?
+   * ?©ç?HHMMSegmentä¸»ç?åº???¥å????ï¼????????ç»??è¿????
+   * 
+   * @param in ?¥å???oken
+   * @param smooth å¹³æ??½æ?
+   * @param dataPath è£?½½?¸å?å­??ä¸??????¸ç????
+   * @see init()
+   */
+  public WordTokenizer(TokenStream in, WordSegmenter wordSegmenter) {
+    this.in = in;
+    this.wordSegmenter = wordSegmenter;
+  }
+
+  public Token next() throws IOException {
+    if (tokenIter != null && tokenIter.hasNext())
+      return (Token) tokenIter.next();
+    else {
+      if (processNextSentence()) {
+        return (Token) tokenIter.next();
+      } else
+        return null;
+    }
+  }
+
+  /**
+   * å½??????¥å????å¹¶ç´¢å¼??æ¯??ï¼??è¦?????ä¸?ä¸??å­?okenï¼? ????°è?è´£è??¨ä?ä¸?å±??SentenceTokenizer?»å?è½½ä?ä¸?ä¸??å­?? å¹¶å??¶å?è¯??
+   * å°??è¯?????å­??Token?¾å?tokenBufferä¸?
+   * 
+   * @return è¯»å?å¹¶å????ä¸?ä¸??å­????????å¦??æ²¡æ????ï¼?????ä»¶å????æ¯?????æ²¡æ?Tokenäº?
+   * @throws IOException
+   */
+  private boolean processNextSentence() throws IOException {
+    sentenceToken = in.next(sentenceToken);
+    if (sentenceToken == null)
+      return false;
+    tokenBuffer = wordSegmenter.segmentSentence(sentenceToken, 1);
+    tokenIter = tokenBuffer.iterator();
+    return tokenBuffer != null && tokenIter.hasNext();
+  }
+
+  public void close() throws IOException {
+    in.close();
+  }
+
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordType.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordType.java
new file mode 100644
index 0000000..52e0317
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordType.java
@@ -0,0 +1,37 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart;
+
+public class WordType {
+
+  public final static int SENTENCE_BEGIN = 0;
+
+  public final static int SENTENCE_END = 1;// ?¥å????å¤´å?ç»??
+
+  public final static int CHINESE_WORD = 2;// ä¸??è¯?
+
+  public final static int STRING = 3;
+
+  public final static int NUMBER = 4; // asciiå­??ä¸²å??°å?
+
+  public final static int DELIMITER = 5; // ??????¹ç???
+
+  public final static int FULLWIDTH_STRING = 6;
+
+  public final static int FULLWIDTH_NUMBER = 7;// ????¨è?å­?????ç¬?¸²ï¼???¨è??°å????å­?
+
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java
new file mode 100644
index 0000000..30a21e1
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java
@@ -0,0 +1,195 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+import java.io.UnsupportedEncodingException;
+
+public abstract class AbstractDictionary {
+  /**
+   * ç¬??ä¸??å­?¸º??????ä»???¢æ?15ä¸??ï¼??15*94ä¸??ç¬?
+   */
+  public static final int GB2312_FIRST_CHAR = 1410;
+
+  /**
+   * GB2312å­????¸­01~87???ç¬??????½æ??????8178ä¸?
+   */
+  public static final int GB2312_CHAR_NUM = 87 * 94;
+
+  /**
+   * è¯????»¶ä¸??å½??6768ä¸??å­??è¯??ç»??
+   */
+  public static final int CHAR_NUM_IN_FILE = 6768;
+
+  // =====================================================
+  // code +0 +1 +2 +3 +4 +5 +6 +7 +8 +9 +A +B +C +D +E +F
+  // B0A0 ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ç¢? ?? ??
+  // B0B0 ?? æ°? å®? ä¿? ?? ?? å²? ?? æ¡? ?? ?? ?? ?? ?? ?? ç¿?
+  // B0C0 è¢? ?? å¥? ?? æ¾? ?? ?? ?? ?? ?? ç¬? ?? ?? å·? ?? è·?
+  // B0D0 ?? ?? ?? ?? ?? ç½? ?? ?? ?? ?? ?? ä½? è´? ?? ç¨? ??
+  // B0E0 ?? ?? ?? ?? é¢? ?? ?? ?? ?? ä¼? ?? ?? ?? ç»? ?? å¸?
+  // B0F0 æ¢? æ¦? ?? ç»? æ£? ç£? ?? ?? ?? è°? ?? ?? ?? è¤? ??
+  // =====================================================
+  //
+  // GB2312 å­??????ºä????è¡??
+  // ?ºå? å­?? å­??ç±»å?
+  // 01 94 ä¸??????
+  // 02 72 é¡ºå??·ç?
+  // 03 94 ???å­??
+  // 04 83 ?¥æ????
+  // 05 86 Katakana
+  // 06 48 å¸??å­??
+  // 07 66 ä¿??å­??
+  // 08 63 æ±???¼é?ç¬??
+  // 09 76 ?¾å½¢ç¬??
+  // 10-15 å¤????
+  // 16-55 3755 ä¸?çº§æ?å­??ä»¥æ??³ä¸ºåº?
+  // 56-87 3008 äº?º§æ±??ï¼?»¥ç¬??ä¸ºå?
+  // 88-94 å¤????
+  // ======================================================
+
+  /**
+   * GB2312 ?±æ?å½?? 7445 ä¸??ç¬???¶ä¸­ç®????å­? 6763 ä¸??å­??????? 682 ä¸???
+   * 
+   * GB2312 å°???¶å????ç¬??ä¸? 94 ä¸??ï¼???·ä¸º 01 ?ºè? 94 ?ºï?æ¯?¸ª?ºæ?å½? 94 ä¸??ç¬??ç¼??ä¸? 01 ä½?? 94
+   * ä½??01ä¸ºèµ·å§??0xA1ï¼?94ä½??äº?0xFE??B2312 ???ä¸?ä¸??ç¬???±ä??¶å?ä¸?å¯¹å?????·å?ä½????ç¡?????å¦??æ±????????ç¼??ä¸? 16 ?? 01
+   * ä½???
+   */
+  /**
+   * @param ccid
+   * @return
+   */
+  public String getCCByGB2312Id(int ccid) {
+    if (ccid < 0 || ccid > WordDictionary.GB2312_CHAR_NUM)
+      return "";
+    int cc1 = ccid / 94 + 161;
+    int cc2 = ccid % 94 + 161;
+    byte[] buffer = new byte[2];
+    buffer[0] = (byte) cc1;
+    buffer[1] = (byte) cc2;
+    try {
+      String cchar = new String(buffer, "GB2312");
+      return cchar;
+    } catch (UnsupportedEncodingException e) {
+      return "";
+    }
+  }
+
+  /**
+   * ?¹æ?è¾????nicodeå­??ï¼???????B2312ç¼??????sciiç¼??ï¼?
+   * 
+   * @param ch è¾????B2312ä¸??å­??????SCIIå­??(128ä¸?)
+   * @return ch??B2312ä¸??ä½?½®ï¼?-1è¡¨ç¤ºè¯¥å?ç¬??è®¤è?
+   */
+  public short getGB2312Id(char ch) {
+    try {
+      byte[] buffer = Character.toString(ch).getBytes("GB2312");
+      if (buffer.length != 2) {
+        // æ­£å¸¸???ä¸?ufferåº????¸¤ä¸????????è¯´æ?chä¸??äº?B2312ç¼??ï¼??è¿??'?'ï¼???¶è????è®¤è?è¯¥å?ç¬?
+        return -1;
+      }
+      int b0 = (int) (buffer[0] & 0x0FF) - 161; // ç¼??ä»?1å¼?å§????????0xA1=161
+      int b1 = (int) (buffer[1] & 0x0FF) - 161; // ç¬??ä¸??ç¬???????ä¸??ç¬?²¡???å­?????æ¯?¸ª?ºå???16*6-2=94ä¸??å­?
+      return (short) (b0 * 94 + b1);
+    } catch (UnsupportedEncodingException e) {
+      e.printStackTrace();
+    }
+    return -1;
+  }
+
+  /**
+   * ?¹è???32ä½?NV hashç®??ï¼??ä½??ç¨??ä¸??ç¬??hash?½æ?.ç¬?????äº?ash?½æ??¨æ????è®¡ç?hashè¡?? ä½¿å???????ï¼?
+   * å¹¶è??¿å???ashè¡¨è?å¯?????´ç??¿æ??´è?ç®?????
+   * 
+   * @param c å¾?ash??nicodeå­??
+   * @return c???å¸???
+   * @see Utility.hash2()
+   */
+  public long hash1(char c) {
+    final long p = 1099511628211L;
+    long hash = 0xcbf29ce484222325L;
+    hash = (hash ^ (c & 0x00FF)) * p;
+    hash = (hash ^ (c >> 8)) * p;
+    hash += hash << 13;
+    hash ^= hash >> 7;
+    hash += hash << 3;
+    hash ^= hash >> 17;
+    hash += hash << 5;
+    return hash;
+  }
+
+  /**
+   * @see Utility.hash1(char[])
+   * @param carray
+   * @return
+   */
+  public long hash1(char carray[]) {
+    final long p = 1099511628211L;
+    long hash = 0xcbf29ce484222325L;
+    for (int i = 0; i < carray.length; i++) {
+      char d = carray[i];
+      hash = (hash ^ (d & 0x00FF)) * p;
+      hash = (hash ^ (d >> 8)) * p;
+    }
+
+    // hash += hash << 13;
+    // hash ^= hash >> 7;
+    // hash += hash << 3;
+    // hash ^= hash >> 17;
+    // hash += hash << 5;
+    return hash;
+  }
+
+  /**
+   * djb2???ç®??ï¼??ä½??ç¨??ä¸??ç¬??hash?½æ?
+   * 
+   * djb2 hash algorithmï¼?his algorithm (k=33) was first reported by dan
+   * bernstein many years ago in comp.lang.c. another version of this algorithm
+   * (now favored by bernstein) uses xor: hash(i) = hash(i - 1) * 33 ^ str[i];
+   * the magic of number 33 (why it works better than many other constants,
+   * prime or not) has never been adequately explained.
+   * 
+   * @param c
+   * @return
+   */
+  public int hash2(char c) {
+    int hash = 5381;
+
+    /* hash 33 + c */
+    hash = ((hash << 5) + hash) + c & 0x00FF;
+    hash = ((hash << 5) + hash) + c >> 8;
+
+    return hash;
+  }
+
+  /**
+   * @see Utility.hash2(char[])
+   * @param carray
+   * @return
+   */
+  public int hash2(char carray[]) {
+    int hash = 5381;
+
+    /* hash 33 + c */
+    for (int i = 0; i < carray.length; i++) {
+      char d = carray[i];
+      hash = ((hash << 5) + hash) + d & 0x00FF;
+      hash = ((hash << 5) + hash) + d >> 8;
+    }
+
+    return hash;
+  }
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java
new file mode 100644
index 0000000..c7b97e1
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java
@@ -0,0 +1,237 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.analysis.cn.smart.Utility;
+
+public class BiSegGraph {
+
+  private Map tokenPairListTable = new HashMap();
+
+  private List segTokenList;
+
+  private static BigramDictionary bigramDict = BigramDictionary.getInstance();
+
+  public BiSegGraph(SegGraph segGraph) {
+    segTokenList = segGraph.makeIndex();
+    generateBiSegGraph(segGraph);
+  }
+
+  /**
+   * ???ä¸¤ä¸¤è¯???´ç?äº???¾è¡¨ï¼??ç»??ä¿???¨ä?ä¸?ultiTokenPairMapä¸?
+   * 
+   * @param segGraph ?????Token??¡¨
+   * @param smooth å¹³æ?ç³»æ?
+   * @param biDict äº??è¯??
+   * @return
+   * 
+   * @see MultiTokenPairMap
+   */
+  private void generateBiSegGraph(SegGraph segGraph) {
+    double smooth = 0.1;
+    int wordPairFreq = 0;
+    int maxStart = segGraph.getMaxStart();
+    double oneWordFreq, weight, tinyDouble = 1.0 / Utility.MAX_FREQUENCE;
+
+    int next;
+    char[] idBuffer;
+    // ä¸?egGraphä¸??æ¯?¸ª???èµ?»¥ä¸?ä¸????
+    segTokenList = segGraph.makeIndex();
+    // ??¸ºstartTokenï¼?"å§?##å§?"ï¼??èµ·å?ä½?½®??-1???keyä¸?-1?¶å?ä»¥å???tartToken
+    int key = -1;
+    List nextTokens = null;
+    while (key < maxStart) {
+      if (segGraph.isStartExist(key)) {
+
+        List tokenList = segGraph.getStartList(key);
+
+        // ä¸ºæ?ä¸?ä¸?eyå¯¹å??????oken?½è?ç®??æ¬?
+        for (Iterator iter = tokenList.iterator(); iter.hasNext();) {
+          SegToken t1 = (SegToken) iter.next();
+          oneWordFreq = t1.weight;
+          next = t1.endOffset;
+          nextTokens = null;
+          // ?¾å?ä¸??ä¸??åº??Tokenï¼??å¦??????µ·å²¸â???å½??Token?????????? ä¸??ä¸?oken??»¥????µ·???????µ·å²¸â??
+          // å¦???¾ä??°ä?ä¸?ä¸?okenï¼??è¯´æ??°ä???°¾ï¼???°å¾ª????
+          while (next <= maxStart) {
+            // ??¸ºendToken??µ·å§??ç½??sentenceLenï¼??æ­¤ç?äº?entenceLen???ä»¥æ???ndToken
+            if (segGraph.isStartExist(next)) {
+              nextTokens = segGraph.getStartList(next);
+              break;
+            }
+            next++;
+          }
+          if (nextTokens == null) {
+            break;
+          }
+          for (Iterator iter2 = nextTokens.iterator(); iter2.hasNext();) {
+            SegToken t2 = (SegToken) iter2.next();
+            idBuffer = new char[t1.charArray.length + t2.charArray.length + 1];
+            System.arraycopy(t1.charArray, 0, idBuffer, 0, t1.charArray.length);
+            idBuffer[t1.charArray.length] = BigramDictionary.WORD_SEGMENT_CHAR;
+            System.arraycopy(t2.charArray, 0, idBuffer,
+                t1.charArray.length + 1, t2.charArray.length);
+
+            // Two linked Words frequency
+            wordPairFreq = bigramDict.getFrequency(idBuffer);
+
+            // Smoothing
+
+            // -log{a*P(Ci-1)+(1-a)P(Ci|Ci-1)} Note 0<a<1
+            weight = -Math
+                .log(smooth
+                    * (1.0 + oneWordFreq)
+                    / (Utility.MAX_FREQUENCE + 0.0)
+                    + (1.0 - smooth)
+                    * ((1.0 - tinyDouble) * wordPairFreq / (1.0 + oneWordFreq) + tinyDouble));
+
+            SegTokenPair tokenPair = new SegTokenPair(idBuffer, t1.index,
+                t2.index, weight);
+            this.addSegTokenPair(tokenPair);
+          }
+        }
+      }
+      key++;
+    }
+
+  }
+
+  /**
+   * ?¥ç?SegTokenPair??????ç½?¸ºto(SegTokenPair.toä¸?o)???å­??SegTokenPairï¼?
+   * å¦??æ²¡æ??????oå¤?²¡??egTokenPair?????æ²¡æ?æ·»å?
+   * 
+   * @param to SegTokenPair.to
+   * @return
+   */
+  public boolean isToExist(int to) {
+    return tokenPairListTable.get(new Integer(to)) != null;
+  }
+
+  /**
+   * ???SegTokenPair.toä¸?o?????egTokenPairï¼????²¡???è¿??null
+   * 
+   * @param to
+   * @return ???????egTokenPair.to??egTokenPair?????
+   */
+  public List getToList(int to) {
+    return (List) tokenPairListTable.get(new Integer(to));
+  }
+
+  /**
+   * ??iSegGraphä¸?????ä¸?egTokenPairï¼??äº?egTokenPair????¸å?SegTokenPair.
+   * to?¾å????ä¸?rrayListä¸?
+   * 
+   * @param tokenPair
+   */
+  public void addSegTokenPair(SegTokenPair tokenPair) {
+    int to = tokenPair.to;
+    if (!isToExist(to)) {
+      ArrayList newlist = new ArrayList();
+      newlist.add(tokenPair);
+      tokenPairListTable.put(new Integer(to), newlist);
+    } else {
+      List tokenPairList = (List) tokenPairListTable.get(new Integer(to));
+      tokenPairList.add(tokenPair);
+    }
+  }
+
+  /**
+   * @return TokenPair????°ï?ä¹?°±??apä¸??????·ç?TokenPairç§????
+   */
+  public int getToCount() {
+    return tokenPairListTable.size();
+  }
+
+  /**
+   * ??eterbiç®??è®¡ç?ä»?µ·?¹å?ç»???????·¯å¾?
+   * 
+   * @return
+   */
+  public List getShortPath() {
+    int current;
+    int nodeCount = getToCount();
+    List path = new ArrayList();
+    PathNode zeroPath = new PathNode();
+    zeroPath.weight = 0;
+    zeroPath.preNode = 0;
+    path.add(zeroPath);
+    for (current = 1; current <= nodeCount; current++) {
+      double weight;
+      List edges = getToList(current);
+
+      double minWeight = Double.MAX_VALUE;
+      SegTokenPair minEdge = null;
+      for (Iterator iter1 = edges.iterator(); iter1.hasNext();) {
+        SegTokenPair edge = (SegTokenPair) iter1.next();
+        weight = edge.weight;
+        PathNode preNode = (PathNode) path.get(edge.from);
+        if (preNode.weight + weight < minWeight) {
+          minWeight = preNode.weight + weight;
+          minEdge = edge;
+        }
+      }
+      PathNode newNode = new PathNode();
+      newNode.weight = minWeight;
+      newNode.preNode = minEdge.from;
+      path.add(newNode);
+    }
+
+    // ?¥ä??¥ä?nodePathsä¸??ç®??èµ·ç??°ç??¹ç????è·??
+    int preNode, lastNode;
+    lastNode = path.size() - 1;
+    current = lastNode;
+    List rpath = new ArrayList();
+    List resultPath = new ArrayList();
+
+    rpath.add(new Integer(current));
+    while (current != 0) {
+      PathNode currentPathNode = (PathNode) path.get(current);
+      preNode = currentPathNode.preNode;
+      rpath.add(new Integer(preNode));
+      current = preNode;
+    }
+    for (int j = rpath.size() - 1; j >= 0; j--) {
+      Integer idInteger = (Integer) rpath.get(j);
+      int id = idInteger.intValue();
+      SegToken t = (SegToken) segTokenList.get(id);
+      resultPath.add(t);
+    }
+    return resultPath;
+
+  }
+
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    Collection values = tokenPairListTable.values();
+    for (Iterator iter1 = values.iterator(); iter1.hasNext();) {
+      List segList = (List) iter1.next();
+      for (Iterator iter2 = segList.iterator(); iter2.hasNext();) {
+        SegTokenPair pair = (SegTokenPair) iter2.next();
+        sb.append(pair + "\n");
+      }
+    }
+    return sb.toString();
+  }
+
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java
new file mode 100644
index 0000000..5414bef
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java
@@ -0,0 +1,321 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.io.RandomAccessFile;
+import java.io.UnsupportedEncodingException;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+
+import org.apache.lucene.analysis.cn.smart.AnalyzerProfile;
+
+public class BigramDictionary extends AbstractDictionary {
+
+  private BigramDictionary() {
+  }
+
+  public static final char WORD_SEGMENT_CHAR = '@';
+
+  private static BigramDictionary singleInstance;
+
+  public static final int PRIME_BIGRAM_LENGTH = 402137;
+
+  /**
+   * bigramTable ?¥å??¨è?ä¸??ä¹????·³è½????? bigramHashTable ?? frequencyTable
+   * å°±æ??¨æ?å­??è¿??é¢???????????? ä¸ºä?????¥è???º¦??????å­?? ??? hash ?¼æ?ä»£æ??³è?è¯??ä¸ºæ?è¯????? ?³è?è¯?°±??
+   * (formWord+'@'+toWord) ï¼? ?©ç? FNV1 hash ç®???¥è?ç®???????ash?? ï¼?¹¶ä¿???? bigramHashTable
+   * ä¸???©ç? hash ?¼æ?ä»£æ??³è?è¯?????ä¼?º§???å°??????²ç?ï¼? ä½?? long ç±»å?
+   * (64bit)??ash?¼æ????å°??æ¦??????????igramHashTable[i]ä¸?requencyTable[i]ä¸?ä¸?å¯¹å?
+   */
+  private long[] bigramHashTable;
+
+  private int[] frequencyTable;
+
+  private int max = 0;
+
+  private int repeat = 0;
+
+  // static Logger log = Logger.getLogger(BigramDictionary.class);
+
+  public synchronized static BigramDictionary getInstance() {
+    if (singleInstance == null) {
+      singleInstance = new BigramDictionary();
+      try {
+        singleInstance.load();
+      } catch (IOException e) {
+        String dictRoot = AnalyzerProfile.ANALYSIS_DATA_DIR;
+        singleInstance.load(dictRoot);
+      } catch (ClassNotFoundException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    return singleInstance;
+  }
+
+  private boolean loadFromObj(File serialObj) {
+    try {
+      loadFromInputStream(new FileInputStream(serialObj));
+      return true;
+    } catch (FileNotFoundException e) {
+      e.printStackTrace();
+    } catch (IOException e) {
+      e.printStackTrace();
+    } catch (ClassNotFoundException e) {
+      e.printStackTrace();
+    }
+    return false;
+  }
+
+  private void loadFromInputStream(InputStream serialObjectInputStream)
+      throws IOException, ClassNotFoundException {
+    ObjectInputStream input = new ObjectInputStream(serialObjectInputStream);
+    bigramHashTable = (long[]) input.readObject();
+    frequencyTable = (int[]) input.readObject();
+    // log.info("load bigram dict from serialization.");
+    input.close();
+  }
+
+  private void saveToObj(File serialObj) {
+    try {
+      ObjectOutputStream output = new ObjectOutputStream(new FileOutputStream(
+          serialObj));
+      output.writeObject(bigramHashTable);
+      output.writeObject(frequencyTable);
+      output.close();
+      // log.info("serialize bigram dict.");
+    } catch (Exception e) {
+      // log.warn(e.getMessage());
+    }
+  }
+
+  private void load() throws IOException, ClassNotFoundException {
+    InputStream input = this.getClass().getResourceAsStream("bigramdict.mem");
+    loadFromInputStream(input);
+  }
+
+  private void load(String dictRoot) {
+    String bigramDictPath = dictRoot + "/bigramdict.dct";
+
+    File serialObj = new File(dictRoot + "/bigramdict.mem");
+
+    if (serialObj.exists() && loadFromObj(serialObj)) {
+
+    } else {
+      try {
+        bigramHashTable = new long[PRIME_BIGRAM_LENGTH];
+        frequencyTable = new int[PRIME_BIGRAM_LENGTH];
+        for (int i = 0; i < PRIME_BIGRAM_LENGTH; i++) {
+          // å®??ä¸??0ä½?¸º????¼æ?ä¸??¹é?é¢????¸º??¸ªå­??ä¸²å???ash?¼ä¸º0ï¼????????å¸¸å?ï¼??æ­¤å½±???å¤?
+          bigramHashTable[i] = 0;
+          frequencyTable[i] = 0;
+        }
+        loadFromFile(bigramDictPath);
+      } catch (IOException e) {
+        throw new RuntimeException(e.getMessage());
+      }
+      saveToObj(serialObj);
+    }
+  }
+
+  /**
+   * å°??åº??ä»¶å?è½½å?WordDictionary????³æ??????¸­ï¼?????è½½ï?æ²¡æ?è¿????¹¶??¿®?¹æ?ä½?
+   * 
+   * @param dctFilePath
+   * @return
+   * @throws FileNotFoundException
+   * @throws IOException
+   * @throws UnsupportedEncodingException
+   */
+  public void loadFromFile(String dctFilePath) throws FileNotFoundException,
+      IOException, UnsupportedEncodingException {
+
+    int i, cnt, length, total = 0;
+    // ??»¶ä¸??ç»??äº?6763ä¸??å­??5ä¸?©ºæ±??ç¬?3756~3760ï¼??ä¸??3756ä¸???¥å??¨ç??·ä¿¡????
+    int[] buffer = new int[3];
+    byte[] intBuffer = new byte[4];
+    String tmpword;
+    RandomAccessFile dctFile = new RandomAccessFile(dctFilePath, "r");
+
+    // å­????»¶ä¸??ä¸?ä¸??å­???°ç?ä½?½®??0ï¼?????ä¸??6768
+    for (i = GB2312_FIRST_CHAR; i < GB2312_FIRST_CHAR + CHAR_NUM_IN_FILE; i++) {
+      String currentStr = getCCByGB2312Id(i);
+      // if (i == 5231)
+      // System.out.println(i);
+
+      dctFile.read(intBuffer);// ???åº??ä»¶å?cä¸???????ä»¥å??¥ç???»¶ä¸?ittle
+      // endianç¼??ï¼???avaä¸?ig endianï¼??é¡»è½¬?¢è???
+      cnt = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN).getInt();
+      if (cnt <= 0) {
+        continue;
+      }
+      total += cnt;
+      int j = 0;
+      while (j < cnt) {
+        dctFile.read(intBuffer);
+        buffer[0] = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN)
+            .getInt();// frequency
+        dctFile.read(intBuffer);
+        buffer[1] = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN)
+            .getInt();// length
+        dctFile.read(intBuffer);
+        // buffer[2] = ByteBuffer.wrap(intBuffer).order(
+        // ByteOrder.LITTLE_ENDIAN).getInt();// handle
+
+        length = buffer[1];
+        if (length > 0) {
+          byte[] lchBuffer = new byte[length];
+          dctFile.read(lchBuffer);
+          tmpword = new String(lchBuffer, "GB2312");
+          if (i != 3755 + GB2312_FIRST_CHAR) {
+            tmpword = currentStr + tmpword;
+          }
+          char carray[] = tmpword.toCharArray();
+          long hashId = hash1(carray);
+          int index = getAvaliableIndex(hashId, carray);
+          if (index != -1) {
+            if (bigramHashTable[index] == 0) {
+              bigramHashTable[index] = hashId;
+              // bigramStringTable[index] = tmpword;
+            }
+            frequencyTable[index] += buffer[0];
+          }
+        }
+        j++;
+      }
+    }
+    dctFile.close();
+    // log.info("load dictionary done! " + dctFilePath + " total:" + total);
+  }
+
+  /*
+   * public void test(String dctFilePath) throws IOException { int i, cnt,
+   * length, total = 0; int corrupt = 0, notFound = 0; //
+   * ??»¶ä¸??ç»??äº?6763ä¸??å­??5ä¸?©ºæ±??ç¬?3756~3760ï¼??ä¸??3756ä¸???¥å??¨ç??·ä¿¡???? int[] buffer = new int[3];
+   * byte[] intBuffer = new byte[4]; String tmpword; RandomAccessFile dctFile =
+   * new RandomAccessFile(dctFilePath, "r");
+   * 
+   * // å­????»¶ä¸??ä¸?ä¸??å­???°ç?ä½?½®??0ï¼?????ä¸??6768 for (i = GB2312_FIRST_CHAR; i <
+   * GB2312_FIRST_CHAR + CHAR_NUM_IN_FILE; i++) { String currentStr =
+   * getCCByGB2312Id(i); // if (i == 5231) // System.out.println(i);
+   * 
+   * dctFile.read(intBuffer);// ???åº??ä»¶å?cä¸???????ä»¥å??¥ç???»¶ä¸?ittle // endianç¼??ï¼???avaä¸?ig
+   * endianï¼??é¡»è½¬?¢è??? cnt =
+   * ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN) .getInt(); if
+   * (cnt <= 0) { continue; } total += cnt; int j = 0; while (j < cnt) {
+   * dctFile.read(intBuffer); buffer[0] = ByteBuffer.wrap(intBuffer).order(
+   * ByteOrder.LITTLE_ENDIAN).getInt();// frequency dctFile.read(intBuffer);
+   * buffer[1] = ByteBuffer.wrap(intBuffer).order(
+   * ByteOrder.LITTLE_ENDIAN).getInt();// length dctFile.read(intBuffer); //
+   * buffer[2] = ByteBuffer.wrap(intBuffer).order( //
+   * ByteOrder.LITTLE_ENDIAN).getInt();// handle
+   * 
+   * length = buffer[1]; if (length > 0) { byte[] lchBuffer = new byte[length];
+   * dctFile.read(lchBuffer); tmpword = new String(lchBuffer, "GB2312"); if (i
+   * != 3755 + GB2312_FIRST_CHAR) { tmpword = currentStr + tmpword; } char
+   * carray[] = tmpword.toCharArray(); int index = getBigramItemIndex(carray);
+   * if (index != -1) { // if (!bigramStringTable[index].equals(tmpword)) { //
+   * System.out.println("corrupt: " + tmpword + "<->" // +
+   * bigramStringTable[index]); // corrupt++; // } } else {
+   * System.out.println("not found: " + tmpword); notFound++; } } j++; } }
+   * dctFile.close(); System.out.println("num not found:" + notFound);
+   * System.out.println("num corrupt:" + corrupt);
+   * 
+   * log.info("test dictionary done! " + dctFilePath + " total:" + total); cnt =
+   * 0; for (int j = 0; j < PRIME_BIGRAM_LENGTH; j++) { if (bigramHashTable[j]
+   * != 0) { cnt++; } } System.out.println("total num in bigramTable: " + cnt);
+   * }
+   */
+
+  private int getAvaliableIndex(long hashId, char carray[]) {
+    int hash1 = (int) (hashId % PRIME_BIGRAM_LENGTH);
+    int hash2 = hash2(carray) % PRIME_BIGRAM_LENGTH;
+    if (hash1 < 0)
+      hash1 = PRIME_BIGRAM_LENGTH + hash1;
+    if (hash2 < 0)
+      hash2 = PRIME_BIGRAM_LENGTH + hash2;
+    int index = hash1;
+    int i = 1;
+    while (bigramHashTable[index] != 0 && bigramHashTable[index] != hashId
+        && i < PRIME_BIGRAM_LENGTH) {
+      index = (hash1 + i * hash2) % PRIME_BIGRAM_LENGTH;
+      i++;
+    }
+    // System.out.println(i - 1);
+
+    if (i < PRIME_BIGRAM_LENGTH
+        && (bigramHashTable[index] == 0 || bigramHashTable[index] == hashId)) {
+      return index;
+    } else
+      return -1;
+  }
+
+  /**
+   * @param c
+   * @return
+   */
+  private int getBigramItemIndex(char carray[]) {
+    long hashId = hash1(carray);
+    int hash1 = (int) (hashId % PRIME_BIGRAM_LENGTH);
+    int hash2 = hash2(carray) % PRIME_BIGRAM_LENGTH;
+    if (hash1 < 0)
+      hash1 = PRIME_BIGRAM_LENGTH + hash1;
+    if (hash2 < 0)
+      hash2 = PRIME_BIGRAM_LENGTH + hash2;
+    int index = hash1;
+    int i = 1;
+    repeat++;
+    while (bigramHashTable[index] != 0 && bigramHashTable[index] != hashId
+        && i < PRIME_BIGRAM_LENGTH) {
+      index = (hash1 + i * hash2) % PRIME_BIGRAM_LENGTH;
+      i++;
+      repeat++;
+      if (i > max)
+        max = i;
+    }
+    // System.out.println(i - 1);
+
+    if (i < PRIME_BIGRAM_LENGTH && bigramHashTable[index] == hashId) {
+      return index;
+    } else
+      return -1;
+  }
+
+  public int getFrequency(char[] carray) {
+    int index = getBigramItemIndex(carray);
+    if (index != -1)
+      return frequencyTable[index];
+    return 0;
+  }
+
+  public static void main(String[] args) throws FileNotFoundException,
+      UnsupportedEncodingException, IOException {
+    BigramDictionary dic = new BigramDictionary();
+    dic.load("D:/analysis-data");
+    // dic.test("D:/analysis-data/BigramDict.dct");
+    System.out.println("max:" + dic.max);
+    System.out.println("average repeat:" + (double) dic.repeat / 328856);
+    System.out.println("end");
+  }
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/CopyOfBigramDictionary.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/CopyOfBigramDictionary.java
new file mode 100644
index 0000000..2cdd078
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/CopyOfBigramDictionary.java
@@ -0,0 +1,302 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.io.RandomAccessFile;
+import java.io.UnsupportedEncodingException;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+
+import org.apache.lucene.analysis.cn.smart.AnalyzerProfile;
+
+public class CopyOfBigramDictionary extends AbstractDictionary {
+
+  private CopyOfBigramDictionary() {
+  }
+
+  public static final char WORD_SEGMENT_CHAR = '@';
+
+  private static CopyOfBigramDictionary singleInstance;
+
+  public static final int PRIME_BIGRAM_LENGTH = 402137;
+
+  /**
+   * bigramTable ?¥å??¨è?ä¸??ä¹????·³è½????? bigramHashTable ?? frequencyTable
+   * å°±æ??¨æ?å­??è¿??é¢???????????? ä¸ºä?????¥è???º¦??????å­?? ??? hash ?¼æ?ä»£æ??³è?è¯??ä¸ºæ?è¯????? ?³è?è¯?°±??
+   * (formWord+'@'+toWord) ï¼? ?©ç? FNV1 hash ç®???¥è?ç®???????ash?? ï¼?¹¶ä¿???? bigramHashTable
+   * ä¸???©ç? hash ?¼æ?ä»£æ??³è?è¯?????ä¼?º§???å°??????²ç?ï¼? ä½?? long ç±»å?
+   * (64bit)??ash?¼æ????å°??æ¦??????????igramHashTable[i]ä¸?requencyTable[i]ä¸?ä¸?å¯¹å?
+   */
+  private long[] bigramHashTable;
+
+  private int[] frequencyTable;
+
+  private int max = 0;
+
+  private int repeat = 0;
+
+  // static Logger log = Logger.getLogger(BigramDictionary.class);
+
+  public synchronized static CopyOfBigramDictionary getInstance() {
+    if (singleInstance == null) {
+      String dictRoot = AnalyzerProfile.ANALYSIS_DATA_DIR;
+      singleInstance = new CopyOfBigramDictionary();
+      singleInstance.load(dictRoot);
+    }
+    return singleInstance;
+  }
+
+  private boolean loadFromObj(File serialObj) {
+    boolean loadFromObject = false;
+    try {
+      ObjectInputStream input = new ObjectInputStream(new FileInputStream(
+          serialObj));
+      bigramHashTable = (long[]) input.readObject();
+      frequencyTable = (int[]) input.readObject();
+      // log.info("load bigram dict from serialization.");
+      loadFromObject = true;
+      input.close();
+    } catch (Exception e) {
+      // log.warn(e.getMessage());
+    }
+    return loadFromObject;
+  }
+
+  private void saveToObj(File serialObj) {
+    try {
+      ObjectOutputStream output = new ObjectOutputStream(new FileOutputStream(
+          serialObj));
+      output.writeObject(bigramHashTable);
+      output.writeObject(frequencyTable);
+      output.close();
+      // log.info("serialize bigram dict.");
+    } catch (Exception e) {
+      // log.warn(e.getMessage());
+    }
+  }
+
+  private void load(String dictRoot) {
+    String bigramDictPath = dictRoot + "/bigramdict.dct";
+
+    File serialObj = new File(dictRoot + "/bigramdict.mem");
+
+    if (serialObj.exists() && loadFromObj(serialObj)) {
+
+    } else {
+      try {
+        bigramHashTable = new long[PRIME_BIGRAM_LENGTH];
+        frequencyTable = new int[PRIME_BIGRAM_LENGTH];
+        for (int i = 0; i < PRIME_BIGRAM_LENGTH; i++) {
+          // å®??ä¸??0ä½?¸º????¼æ?ä¸??¹é?é¢????¸º??¸ªå­??ä¸²å???ash?¼ä¸º0ï¼????????å¸¸å?ï¼??æ­¤å½±???å¤?
+          bigramHashTable[i] = 0;
+          frequencyTable[i] = 0;
+        }
+        loadFromFile(bigramDictPath);
+      } catch (IOException e) {
+        throw new RuntimeException(e.getMessage());
+      }
+      saveToObj(serialObj);
+    }
+  }
+
+  /**
+   * å°??åº??ä»¶å?è½½å?WordDictionary????³æ??????¸­ï¼?????è½½ï?æ²¡æ?è¿????¹¶??¿®?¹æ?ä½?
+   * 
+   * @param dctFilePath
+   * @return
+   * @throws FileNotFoundException
+   * @throws IOException
+   * @throws UnsupportedEncodingException
+   */
+  public void loadFromFile(String dctFilePath) throws FileNotFoundException,
+      IOException, UnsupportedEncodingException {
+
+    int i, cnt, length, total = 0;
+    // ??»¶ä¸??ç»??äº?6763ä¸??å­??5ä¸?©ºæ±??ç¬?3756~3760ï¼??ä¸??3756ä¸???¥å??¨ç??·ä¿¡????
+    int[] buffer = new int[3];
+    byte[] intBuffer = new byte[4];
+    String tmpword;
+    RandomAccessFile dctFile = new RandomAccessFile(dctFilePath, "r");
+
+    // å­????»¶ä¸??ä¸?ä¸??å­???°ç?ä½?½®??0ï¼?????ä¸??6768
+    for (i = GB2312_FIRST_CHAR; i < GB2312_FIRST_CHAR + CHAR_NUM_IN_FILE; i++) {
+      String currentStr = getCCByGB2312Id(i);
+      // if (i == 5231)
+      // System.out.println(i);
+
+      dctFile.read(intBuffer);// ???åº??ä»¶å?cä¸???????ä»¥å??¥ç???»¶ä¸?ittle
+      // endianç¼??ï¼???avaä¸?ig endianï¼??é¡»è½¬?¢è???
+      cnt = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN).getInt();
+      if (cnt <= 0) {
+        continue;
+      }
+      total += cnt;
+      int j = 0;
+      while (j < cnt) {
+        dctFile.read(intBuffer);
+        buffer[0] = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN)
+            .getInt();// frequency
+        dctFile.read(intBuffer);
+        buffer[1] = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN)
+            .getInt();// length
+        dctFile.read(intBuffer);
+        // buffer[2] = ByteBuffer.wrap(intBuffer).order(
+        // ByteOrder.LITTLE_ENDIAN).getInt();// handle
+
+        length = buffer[1];
+        if (length > 0) {
+          byte[] lchBuffer = new byte[length];
+          dctFile.read(lchBuffer);
+          tmpword = new String(lchBuffer, "GB2312");
+          if (i != 3755 + GB2312_FIRST_CHAR) {
+            tmpword = currentStr + tmpword;
+          }
+          char carray[] = tmpword.toCharArray();
+          long hashId = hash1(carray);
+          int index = getAvaliableIndex(hashId, carray);
+          if (index != -1) {
+            if (bigramHashTable[index] == 0) {
+              bigramHashTable[index] = hashId;
+              // bigramStringTable[index] = tmpword;
+            }
+            frequencyTable[index] += buffer[0];
+          }
+        }
+        j++;
+      }
+    }
+    dctFile.close();
+    // log.info("load dictionary done! " + dctFilePath + " total:" + total);
+  }
+
+  /*
+   * public void test(String dctFilePath) throws IOException { int i, cnt,
+   * length, total = 0; int corrupt = 0, notFound = 0; //
+   * ??»¶ä¸??ç»??äº?6763ä¸??å­??5ä¸?©ºæ±??ç¬?3756~3760ï¼??ä¸??3756ä¸???¥å??¨ç??·ä¿¡???? int[] buffer = new int[3];
+   * byte[] intBuffer = new byte[4]; String tmpword; RandomAccessFile dctFile =
+   * new RandomAccessFile(dctFilePath, "r");
+   * 
+   * // å­????»¶ä¸??ä¸?ä¸??å­???°ç?ä½?½®??0ï¼?????ä¸??6768 for (i = GB2312_FIRST_CHAR; i <
+   * GB2312_FIRST_CHAR + CHAR_NUM_IN_FILE; i++) { String currentStr =
+   * getCCByGB2312Id(i); // if (i == 5231) // System.out.println(i);
+   * 
+   * dctFile.read(intBuffer);// ???åº??ä»¶å?cä¸???????ä»¥å??¥ç???»¶ä¸?ittle // endianç¼??ï¼???avaä¸?ig
+   * endianï¼??é¡»è½¬?¢è??? cnt =
+   * ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN) .getInt(); if
+   * (cnt <= 0) { continue; } total += cnt; int j = 0; while (j < cnt) {
+   * dctFile.read(intBuffer); buffer[0] = ByteBuffer.wrap(intBuffer).order(
+   * ByteOrder.LITTLE_ENDIAN).getInt();// frequency dctFile.read(intBuffer);
+   * buffer[1] = ByteBuffer.wrap(intBuffer).order(
+   * ByteOrder.LITTLE_ENDIAN).getInt();// length dctFile.read(intBuffer); //
+   * buffer[2] = ByteBuffer.wrap(intBuffer).order( //
+   * ByteOrder.LITTLE_ENDIAN).getInt();// handle
+   * 
+   * length = buffer[1]; if (length > 0) { byte[] lchBuffer = new byte[length];
+   * dctFile.read(lchBuffer); tmpword = new String(lchBuffer, "GB2312"); if (i
+   * != 3755 + GB2312_FIRST_CHAR) { tmpword = currentStr + tmpword; } char
+   * carray[] = tmpword.toCharArray(); int index = getBigramItemIndex(carray);
+   * if (index != -1) { // if (!bigramStringTable[index].equals(tmpword)) { //
+   * System.out.println("corrupt: " + tmpword + "<->" // +
+   * bigramStringTable[index]); // corrupt++; // } } else {
+   * System.out.println("not found: " + tmpword); notFound++; } } j++; } }
+   * dctFile.close(); System.out.println("num not found:" + notFound);
+   * System.out.println("num corrupt:" + corrupt);
+   * 
+   * log.info("test dictionary done! " + dctFilePath + " total:" + total); cnt =
+   * 0; for (int j = 0; j < PRIME_BIGRAM_LENGTH; j++) { if (bigramHashTable[j]
+   * != 0) { cnt++; } } System.out.println("total num in bigramTable: " + cnt);
+   * }
+   */
+
+  private int getAvaliableIndex(long hashId, char carray[]) {
+    int hash1 = (int) (hashId % PRIME_BIGRAM_LENGTH);
+    int hash2 = hash2(carray) % PRIME_BIGRAM_LENGTH;
+    if (hash1 < 0)
+      hash1 = PRIME_BIGRAM_LENGTH + hash1;
+    if (hash2 < 0)
+      hash2 = PRIME_BIGRAM_LENGTH + hash2;
+    int index = hash1;
+    int i = 1;
+    while (bigramHashTable[index] != 0 && bigramHashTable[index] != hashId
+        && i < PRIME_BIGRAM_LENGTH) {
+      index = (hash1 + i * hash2) % PRIME_BIGRAM_LENGTH;
+      i++;
+    }
+    // System.out.println(i - 1);
+
+    if (i < PRIME_BIGRAM_LENGTH
+        && (bigramHashTable[index] == 0 || bigramHashTable[index] == hashId)) {
+      return index;
+    } else
+      return -1;
+  }
+
+  /**
+   * @param c
+   * @return
+   */
+  private int getBigramItemIndex(char carray[]) {
+    long hashId = hash1(carray);
+    int hash1 = (int) (hashId % PRIME_BIGRAM_LENGTH);
+    int hash2 = hash2(carray) % PRIME_BIGRAM_LENGTH;
+    if (hash1 < 0)
+      hash1 = PRIME_BIGRAM_LENGTH + hash1;
+    if (hash2 < 0)
+      hash2 = PRIME_BIGRAM_LENGTH + hash2;
+    int index = hash1;
+    int i = 1;
+    repeat++;
+    while (bigramHashTable[index] != 0 && bigramHashTable[index] != hashId
+        && i < PRIME_BIGRAM_LENGTH) {
+      index = (hash1 + i * hash2) % PRIME_BIGRAM_LENGTH;
+      i++;
+      repeat++;
+      if (i > max)
+        max = i;
+    }
+    // System.out.println(i - 1);
+
+    if (i < PRIME_BIGRAM_LENGTH && bigramHashTable[index] == hashId) {
+      return index;
+    } else
+      return -1;
+  }
+
+  public int getFrequency(char[] carray) {
+    int index = getBigramItemIndex(carray);
+    if (index != -1)
+      return frequencyTable[index];
+    return 0;
+  }
+
+  public static void main(String[] args) throws FileNotFoundException,
+      UnsupportedEncodingException, IOException {
+    CopyOfBigramDictionary dic = new CopyOfBigramDictionary();
+    dic.load("D:/analysis-data");
+    // dic.test("D:/analysis-data/BigramDict.dct");
+    System.out.println("max:" + dic.max);
+    System.out.println("average repeat:" + (double) dic.repeat / 328856);
+    System.out.println("end");
+  }
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/CopyOfWordDictionary.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/CopyOfWordDictionary.java
new file mode 100644
index 0000000..e63e578
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/CopyOfWordDictionary.java
@@ -0,0 +1,541 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.io.RandomAccessFile;
+import java.io.UnsupportedEncodingException;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+
+import org.apache.lucene.analysis.cn.smart.AnalyzerProfile;
+import org.apache.lucene.analysis.cn.smart.Utility;
+
+public class CopyOfWordDictionary extends AbstractDictionary {
+
+  private CopyOfWordDictionary() {
+  }
+
+  private static CopyOfWordDictionary singleInstance;
+
+  /**
+   * ä¸?ä¸??å¤§ç?ç´??ï¼??è¯?ash?¥æ??½å?????????ç½?
+   */
+  public static final int PRIME_INDEX_LENGTH = 12071;
+
+  /**
+   * wordIndexTableä¿??å°?nicodeä¸???????å­????ash??RIME_INDEX_LENGTH?¿åº¦???ç»?¸­ï¼?
+   * å½??ä¼???²ç?ï¼??å®??ä¸??ç¨???????B2312å­???¨å?ï¼?6768ä¸??ç¬??ä¸??äº?SCIIå­??ï¼?
+   * ???å¯¹è?äº??ç¬????????ä¸ºä?ä¿??æ¯?????ç¡??§ï?ä¿????????ç¬??charIndexTableä¸?»¥ç¡???¥æ????ç¡???
+   */
+  private short[] wordIndexTable;
+
+  private char[] charIndexTable;
+
+  /**
+   * å­???????åº??????°æ?ç»??ï¼?¸ºäº??????¨ç©º?´å¤ªå¤???¨ä?ä¸¤ä¸ª??????ç»´æ?ç»??å­??è¯?????????
+   * æ¯?¸ªè¯???¨ä?ä¸?har[]ä¸??æ¯?¸ªcharå¯¹å?ä¸?ä¸??å­???¶ä?å­??ï¼??ä¸??????¨ä?ä¸?ntä¸??
+   * è¿?¸¤ä¸??ç»????¸¤ä¸??è¡¨æ?ä¸?ä¸?å¯¹å??????æ­¤å?ä»¥å???ordItem_charArrayTable[i][j]?¥æ?è¯??
+   * ??ordItem_frequencyTable[i][j]?¥æ?è¯¢å?åº??é¢??
+   */
+  private char[][][] wordItem_charArrayTable;
+
+  private int[][] wordItem_frequencyTable;
+
+  // static Logger log = Logger.getLogger(WordDictionary.class);
+
+  public synchronized static CopyOfWordDictionary getInstance() {
+    if (singleInstance == null) {
+      singleInstance = new CopyOfWordDictionary();
+      String wordDictRoot = AnalyzerProfile.ANALYSIS_DATA_DIR;
+      singleInstance.load(wordDictRoot);
+    }
+    return singleInstance;
+  }
+
+  /**
+   * ???è¯??åº??ä»¶ï?
+   * 
+   * @param dctFileName è¯??åº??ä»¶ç?è·??
+   */
+  public void load(String dctFileRoot) {
+    String dctFilePath = dctFileRoot + "/coredict.dct";
+    File serialObj = new File(dctFileRoot + "/coredict.mem");
+
+    if (serialObj.exists() && loadFromObj(serialObj)) {
+
+    } else {
+      try {
+        wordIndexTable = new short[PRIME_INDEX_LENGTH];
+        charIndexTable = new char[PRIME_INDEX_LENGTH];
+        for (int i = 0; i < PRIME_INDEX_LENGTH; i++) {
+          charIndexTable[i] = 0;
+          wordIndexTable[i] = -1;
+        }
+        wordItem_charArrayTable = new char[GB2312_CHAR_NUM][][];
+        wordItem_frequencyTable = new int[GB2312_CHAR_NUM][];
+        // int total =
+        loadMainDataFromFile(dctFilePath);
+        expandDelimiterData();
+        mergeSameWords();
+        sortEachItems();
+        // log.info("load dictionary: " + dctFilePath + " total:" + total);
+      } catch (IOException e) {
+        throw new RuntimeException(e.getMessage());
+      }
+
+      saveToObj(serialObj);
+    }
+
+  }
+
+  private boolean loadFromObj(File serialObj) {
+    boolean loadFromObject = false;
+    try {
+      ObjectInputStream input = new ObjectInputStream(new FileInputStream(
+          serialObj));
+      wordIndexTable = (short[]) input.readObject();
+      charIndexTable = (char[]) input.readObject();
+      wordItem_charArrayTable = (char[][][]) input.readObject();
+      wordItem_frequencyTable = (int[][]) input.readObject();
+      // log.info("load core dict from serialization.");
+      input.close();
+      loadFromObject = true;
+    } catch (Exception e) {
+      // log.warn(e.getMessage());
+    }
+    return loadFromObject;
+  }
+
+  private void saveToObj(File serialObj) {
+    try {
+      ObjectOutputStream output = new ObjectOutputStream(new FileOutputStream(
+          serialObj));
+      output.writeObject(wordIndexTable);
+      output.writeObject(charIndexTable);
+      output.writeObject(wordItem_charArrayTable);
+      output.writeObject(wordItem_frequencyTable);
+      output.close();
+      // log.info("serialize core dict.");
+    } catch (Exception e) {
+      // log.warn(e.getMessage());
+    }
+  }
+
+  /**
+   * å°??åº??ä»¶å?è½½å?WordDictionary????³æ??????¸­ï¼?????è½½ï?æ²¡æ?è¿????¹¶??¿®?¹æ?ä½?
+   * 
+   * @param dctFilePath
+   * @return
+   * @throws FileNotFoundException
+   * @throws IOException
+   * @throws UnsupportedEncodingException
+   */
+  private int loadMainDataFromFile(String dctFilePath)
+      throws FileNotFoundException, IOException, UnsupportedEncodingException {
+    int i, cnt, length, total = 0;
+    // ??»¶ä¸??ç»??äº?6763ä¸??å­??5ä¸?©ºæ±??ç¬?3756~3760ï¼??ä¸??3756ä¸???¥å??¨ç??·ä¿¡????
+    int[] buffer = new int[3];
+    byte[] intBuffer = new byte[4];
+    String tmpword;
+    RandomAccessFile dctFile = new RandomAccessFile(dctFilePath, "r");
+
+    // å­????»¶ä¸??ä¸?ä¸??å­???°ç?ä½?½®??0ï¼?????ä¸??6768
+    for (i = GB2312_FIRST_CHAR; i < GB2312_FIRST_CHAR + CHAR_NUM_IN_FILE; i++) {
+      // if (i == 5231)
+      // System.out.println(i);
+
+      dctFile.read(intBuffer);// ???åº??ä»¶å?cä¸???????ä»¥å??¥ç???»¶ä¸?ittle
+      // endianç¼??ï¼???avaä¸?ig endianï¼??é¡»è½¬?¢è???
+      cnt = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN).getInt();
+      if (cnt <= 0) {
+        wordItem_charArrayTable[i] = null;
+        wordItem_frequencyTable[i] = null;
+        continue;
+      }
+      wordItem_charArrayTable[i] = new char[cnt][];
+      wordItem_frequencyTable[i] = new int[cnt];
+      total += cnt;
+      int j = 0;
+      while (j < cnt) {
+        // wordItemTable[i][j] = new WordItem();
+        dctFile.read(intBuffer);
+        buffer[0] = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN)
+            .getInt();// frequency
+        dctFile.read(intBuffer);
+        buffer[1] = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN)
+            .getInt();// length
+        dctFile.read(intBuffer);
+        buffer[2] = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN)
+            .getInt();// handle
+
+        // wordItemTable[i][j].frequency = buffer[0];
+        wordItem_frequencyTable[i][j] = buffer[0];
+
+        length = buffer[1];
+        if (length > 0) {
+          byte[] lchBuffer = new byte[length];
+          dctFile.read(lchBuffer);
+          tmpword = new String(lchBuffer, "GB2312");
+          // indexTable[i].wordItems[j].word = tmpword;
+          // wordItemTable[i][j].charArray = tmpword.toCharArray();
+          wordItem_charArrayTable[i][j] = tmpword.toCharArray();
+        } else {
+          // wordItemTable[i][j].charArray = null;
+          wordItem_charArrayTable[i][j] = null;
+        }
+        // System.out.println(indexTable[i].wordItems[j]);
+        j++;
+      }
+
+      String str = getCCByGB2312Id(i);
+      setTableIndex(str.charAt(0), i);
+    }
+    dctFile.close();
+    return total;
+  }
+
+  /**
+   * ???åº????????¹ç??·ç?ä¿¡æ???¹¶?°ä?ä¸??è¡¨é?(ä»?1å¼?å§??3755å¤?)???????¶å?å¼?ï¼??????°å?ä¸???·å?åº????¡¨ä¸?
+   */
+  private void expandDelimiterData() {
+    int i;
+    int cnt;
+    // ???ç¬???¨ä?1å¼?å§??3755å¤??å°??å§?????ç¬??å¯¹å?????¸å????å¯¹å?????¹ç??·ä¸­
+    int delimiterIndex = 3755 + GB2312_FIRST_CHAR;
+    i = 0;
+    while (i < wordItem_charArrayTable[delimiterIndex].length) {
+      char c = wordItem_charArrayTable[delimiterIndex][i][0];
+      int j = getGB2312Id(c);// è¯¥æ??¹ç??·å?è¯¥æ??¨ç?index??
+      if (wordItem_charArrayTable[j] == null) {
+
+        int k = i;
+        // ä»?å¼?å§???°å???»¥jå¼?å¤´ç?ç¬????orditem??¸ª??
+        while (k < wordItem_charArrayTable[delimiterIndex].length
+            && wordItem_charArrayTable[delimiterIndex][k][0] == c) {
+          k++;
+        }
+        // æ­¤æ?k-iä¸?dä¸?????¹ç??·å?åº??wordItem??¸ª??
+        cnt = k - i;
+        if (cnt != 0) {
+          wordItem_charArrayTable[j] = new char[cnt][];
+          wordItem_frequencyTable[j] = new int[cnt];
+        }
+
+        // ä¸ºæ?ä¸?ä¸?ordItemèµ???
+        for (k = 0; k < cnt; k++, i++) {
+          // wordItemTable[j][k] = new WordItem();
+          wordItem_frequencyTable[j][k] = wordItem_frequencyTable[delimiterIndex][i];
+          wordItem_charArrayTable[j][k] = new char[wordItem_charArrayTable[delimiterIndex][i].length - 1];
+          System.arraycopy(wordItem_charArrayTable[delimiterIndex][i], 1,
+              wordItem_charArrayTable[j][k], 0,
+              wordItem_charArrayTable[j][k].length);
+        }
+        setTableIndex(c, j);
+      }
+    }
+    // å°??ç¬??å¯¹å????ç»????
+    wordItem_charArrayTable[delimiterIndex] = null;
+    wordItem_frequencyTable[delimiterIndex] = null;
+  }
+
+  /**
+   * ???åº??????§æ?æ³?????å°?????ä¸??è¯??§ç?é¢????¹¶?°å?ä¸?ä¸??ä¸??ä»¥å?å°???¨ç©º?´ï???¿«??´¢??º¦
+   */
+  private void mergeSameWords() {
+    int i;
+    for (i = 0; i < GB2312_FIRST_CHAR + CHAR_NUM_IN_FILE; i++) {
+      if (wordItem_charArrayTable[i] == null)
+        continue;
+      int len = 1;
+      for (int j = 1; j < wordItem_charArrayTable[i].length; j++) {
+        if (Utility.compareArray(wordItem_charArrayTable[i][j], 0,
+            wordItem_charArrayTable[i][j - 1], 0) != 0)
+          len++;
+
+      }
+      if (len < wordItem_charArrayTable[i].length) {
+        char[][] tempArray = new char[len][];
+        int[] tempFreq = new int[len];
+        int k = 0;
+        tempArray[0] = wordItem_charArrayTable[i][0];
+        tempFreq[0] = wordItem_frequencyTable[i][0];
+        for (int j = 1; j < wordItem_charArrayTable[i].length; j++) {
+          if (Utility.compareArray(wordItem_charArrayTable[i][j], 0,
+              tempArray[k], 0) != 0) {
+            k++;
+            // temp[k] = wordItemTable[i][j];
+            tempArray[k] = wordItem_charArrayTable[i][j];
+            tempFreq[k] = wordItem_frequencyTable[i][j];
+          } else {
+            // temp[k].frequency += wordItemTable[i][j].frequency;
+            tempFreq[k] += wordItem_frequencyTable[i][j];
+          }
+        }
+        // wordItemTable[i] = temp;
+        wordItem_charArrayTable[i] = tempArray;
+        wordItem_frequencyTable[i] = tempFreq;
+      }
+    }
+  }
+
+  private void sortEachItems() {
+    char[] tmpArray;
+    int tmpFreq;
+    for (int i = 0; i < wordItem_charArrayTable.length; i++) {
+      if (wordItem_charArrayTable[i] != null
+          && wordItem_charArrayTable[i].length > 1) {
+        for (int j = 0; j < wordItem_charArrayTable[i].length - 1; j++) {
+          for (int j2 = j + 1; j2 < wordItem_charArrayTable[i].length; j2++) {
+            if (Utility.compareArray(wordItem_charArrayTable[i][j], 0,
+                wordItem_charArrayTable[i][j2], 0) > 0) {
+              tmpArray = wordItem_charArrayTable[i][j];
+              tmpFreq = wordItem_frequencyTable[i][j];
+              wordItem_charArrayTable[i][j] = wordItem_charArrayTable[i][j2];
+              wordItem_frequencyTable[i][j] = wordItem_frequencyTable[i][j2];
+              wordItem_charArrayTable[i][j2] = tmpArray;
+              wordItem_frequencyTable[i][j2] = tmpFreq;
+            }
+          }
+        }
+      }
+    }
+  }
+
+  /**
+   * è®¡ç?å­??c?¨å?å¸?¡¨ä¸??è¯¥å????ç½???¶å?å°??????¡¨ä¸??ä½?½®???¼å?å§??
+   * 
+   * @param c
+   * @param j
+   * @return
+   */
+  private boolean setTableIndex(char c, int j) {
+    int index = getAvaliableTableIndex(c);
+    if (index != -1) {
+      charIndexTable[index] = c;
+      wordIndexTable[index] = (short) j;
+      return true;
+    } else
+      return false;
+  }
+
+  private short getAvaliableTableIndex(char c) {
+    int hash1 = (int) (hash1(c) % PRIME_INDEX_LENGTH);
+    int hash2 = hash2(c) % PRIME_INDEX_LENGTH;
+    if (hash1 < 0)
+      hash1 = PRIME_INDEX_LENGTH + hash1;
+    if (hash2 < 0)
+      hash2 = PRIME_INDEX_LENGTH + hash2;
+    int index = hash1;
+    int i = 1;
+    while (charIndexTable[index] != 0 && charIndexTable[index] != c
+        && i < PRIME_INDEX_LENGTH) {
+      index = (hash1 + i * hash2) % PRIME_INDEX_LENGTH;
+      i++;
+    }
+    // System.out.println(i - 1);
+
+    if (i < PRIME_INDEX_LENGTH
+        && (charIndexTable[index] == 0 || charIndexTable[index] == c)) {
+      return (short) index;
+    } else
+      return -1;
+  }
+
+  /**
+   * @param c
+   * @return
+   */
+  private short getWordItemTableIndex(char c) {
+    int hash1 = (int) (hash1(c) % PRIME_INDEX_LENGTH);
+    int hash2 = hash2(c) % PRIME_INDEX_LENGTH;
+    if (hash1 < 0)
+      hash1 = PRIME_INDEX_LENGTH + hash1;
+    if (hash2 < 0)
+      hash2 = PRIME_INDEX_LENGTH + hash2;
+    int index = hash1;
+    int i = 1;
+    while (charIndexTable[index] != 0 && charIndexTable[index] != c
+        && i < PRIME_INDEX_LENGTH) {
+      index = (hash1 + i * hash2) % PRIME_INDEX_LENGTH;
+      i++;
+    }
+
+    if (i < PRIME_INDEX_LENGTH && charIndexTable[index] == c) {
+      return (short) index;
+    } else
+      return -1;
+  }
+
+  /**
+   * ?¨å??¸å?ä¸???¾å?è¯??åº??char?°ç?ä¸?harArray???ç¬?¸²??????????¨å?è¯????¸­???ç½?
+   * 
+   * @param charArray ?¥æ????å¯¹å???har?°ç?
+   * @return ????¨å?è¯??ç»?¸­???ç½??å¦??æ²¡æ??°å?è¿??-1
+   */
+  private int findInTable(char[] charArray) {
+    if (charArray == null || charArray.length == 0)
+      return -1;
+    short index = getWordItemTableIndex(charArray[0]);
+    if (index == -1)
+      return -1;
+
+    return findInTable(index, charArray);
+
+  }
+
+  /**
+   * ?¨å??¸å?ä¸???¾å?è¯??åº??char?°ç?ä¸?harArray???ç¬?¸²??????????¨å?è¯????¸­???ç½?
+   * 
+   * @param knownHashIndex å·²ç????ç¬??ä¸??ç¬?harArray[0]??ashè¡¨ä¸­???ç½??å¦?????ç®????»¥?¨å???nt
+   *        findInTable(char[] charArray) ä»£æ?
+   * @param charArray ?¥æ????å¯¹å???har?°ç?
+   * @return ????¨å?è¯??ç»?¸­???ç½??å¦??æ²¡æ??°å?è¿??-1
+   */
+  private int findInTable(short knownHashIndex, char[] charArray) {
+    if (charArray == null || charArray.length == 0)
+      return -1;
+
+    char[][] items = wordItem_charArrayTable[wordIndexTable[knownHashIndex]];
+    int start = 0, end = items.length - 1;
+    int mid = (start + end) / 2, cmpResult;
+
+    // Binary search for the index of idArray
+    while (start <= end) {
+      cmpResult = Utility.compareArray(items[mid], 0, charArray, 1);
+
+      if (cmpResult == 0)
+        return mid;// find it
+      else if (cmpResult < 0)
+        start = mid + 1;
+      else if (cmpResult > 0)
+        end = mid - 1;
+
+      mid = (start + end) / 2;
+    }
+    return -1;
+  }
+
+  /**
+   * charArrayè¿?¸ª???å¯¹å????ç»??ä¸??WordDictionaryä¸????
+   * 
+   * @param charArray
+   * @return trueè¡¨ç¤ºå­??ï¼?alseè¡¨ç¤ºä¸????
+   */
+  public boolean isExist(char[] charArray) {
+    return findInTable(charArray) != -1;
+  }
+
+  /**
+   * @see{getPrefixMatch(char[] charArray, int knownStart)}
+   * @param charArray
+   * @return
+   */
+  public int getPrefixMatch(char[] charArray) {
+    return getPrefixMatch(charArray, 0);
+  }
+
+  /**
+   * ä»???¸ä¸­?¥æ?ä»?harArrayå¯¹å????è¯?¸º???(prefix)???è¯??ä½?½®, å¹¶è????ä¸?ä¸?»¡è¶³æ?ä»¶ç?ä½?½®??¸ºäº??å°??ç´?»£ä»?,
+   * ??»¥?¹æ?å·²æ??¥è?è®¾ç½®èµ·å???´¢ä½?½®, å¦??ä¸????µ·å§??ç½??é»????0
+   * 
+   * @see{getPrefixMatch(char[] charArray)}
+   * @param charArray ??????
+   * @param knownStart å·²ç???µ·å§??ç½?
+   * @return æ»¡è¶³????¡ä»¶???ä¸?ä¸??è¯??ä½?½®
+   */
+  public int getPrefixMatch(char[] charArray, int knownStart) {
+    short index = getWordItemTableIndex(charArray[0]);
+    if (index == -1)
+      return -1;
+    char[][] items = wordItem_charArrayTable[wordIndexTable[index]];
+    int start = knownStart, end = items.length - 1;
+
+    int mid = (start + end) / 2, cmpResult;
+
+    // Binary search for the index of idArray
+    while (start <= end) {
+      cmpResult = Utility.compareArrayByPrefix(charArray, 1, items[mid], 0);
+      if (cmpResult == 0) {
+        // Get the first item which match the current word
+        while (mid >= 0
+            && Utility.compareArrayByPrefix(charArray, 1, items[mid], 0) == 0)
+          mid--;
+        mid++;
+        return mid;// ?¾å?ç¬??ä¸?»¥charArrayä¸ºå?ç¼????è¯?
+      } else if (cmpResult < 0)
+        end = mid - 1;
+      else
+        start = mid + 1;
+      mid = (start + end) / 2;
+    }
+    return -1;
+  }
+
+  /**
+   * ?·å?idArrayå¯¹å???????é¢????osä¸?-1??????????§ç?è¯??
+   * 
+   * @param charArray è¾?????è¯??åº??charArray
+   * @param pos è¯??§ï?-1è¡¨ç¤ºè¦??æ±???????è¯??§ç?è¯??
+   * @return idArrayå¯¹å????é¢?
+   */
+  public int getFrequency(char[] charArray) {
+    short hashIndex = getWordItemTableIndex(charArray[0]);
+    if (hashIndex == -1)
+      return 0;
+    int itemIndex = findInTable(hashIndex, charArray);
+    if (itemIndex != -1)
+      return wordItem_frequencyTable[wordIndexTable[hashIndex]][itemIndex];
+    return 0;
+
+  }
+
+  /**
+   * ?¤æ?charArrayå¯¹å????ç¬?¸²???è·???¸ä¸­charArray[0]å¯¹å???ordIndex??harArray?¸ç?,
+   * ä¹?°±???charArray???ç½???¾ç????ä¸??å°±æ?wordIndex
+   * 
+   * @param charArray è¾????harArrayè¯??ï¼??ä¸?ä¸??è¡¨ç¤ºè¯??ä¸??ç´¢å???
+   * @param itemIndex ä½?½®ç¼??
+   * @return ????¸ç?
+   */
+  public boolean isEqual(char[] charArray, int itemIndex) {
+    short hashIndex = getWordItemTableIndex(charArray[0]);
+    return Utility.compareArray(charArray, 1,
+        wordItem_charArrayTable[wordIndexTable[hashIndex]][itemIndex], 0) == 0;
+  }
+
+  public static void main(String[] args) throws FileNotFoundException,
+      IOException {
+    CopyOfWordDictionary dic = new CopyOfWordDictionary();
+    dic.load("D:/analysis-data");
+    Utility.getCharType('??');
+    Utility.getCharType('æ±?');
+    Utility.getCharType(' ');// 0020
+    Utility.getCharType('??');// 3000
+    Utility.getCharType('??');// E095
+    Utility.getCharType(' ');// 3000
+    Utility.getCharType('\r');// 000D
+    Utility.getCharType('\n');// 000A
+    Utility.getCharType('\t');// 0009
+  }
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/HHMMSegmenter.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/HHMMSegmenter.java
new file mode 100644
index 0000000..d601343
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/HHMMSegmenter.java
@@ -0,0 +1,193 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+import java.util.List;
+
+import org.apache.lucene.analysis.cn.smart.CharType;
+import org.apache.lucene.analysis.cn.smart.Utility;
+import org.apache.lucene.analysis.cn.smart.WordType;
+
+public class HHMMSegmenter {
+
+  private static WordDictionary wordDict = WordDictionary.getInstance();
+
+  /**
+   * å¯»æ?sentenceä¸??????½ç?Tokenï¼?????æ·»å?ä¸¤ä¸ª?¹æ?Tokenï¼?"å§?##å§?",
+   * "??##??"ï¼?"å§?##å§?"Token??µ·å§??ç½??-1,"??##??"Token??µ·å§??ç½???¥å????åº?
+   * 
+   * @param sentence è¾?????å­??ä¸????"å§?##å§?","??##??"ç­?
+   * @param coreDict ?¸å?å­??
+   * @return ??????½ç?Token
+   * @see MultiTokenMap
+   */
+  private SegGraph createSegGraph(String sentence) {
+    int i = 0, j;
+    int length = sentence.length();
+    int foundIndex;
+    int[] charTypeArray = getCharTypes(sentence);
+    StringBuffer wordBuf = new StringBuffer();
+    SegToken token;
+    int frequency = 0; // word????°æ???
+    boolean hasFullWidth;
+    int wordType;
+    char[] charArray;
+
+    SegGraph segGraph = new SegGraph();
+    while (i < length) {
+      hasFullWidth = false;
+      switch (charTypeArray[i]) {
+        case CharType.SPACE_LIKE:
+          i++;
+          break;
+        case CharType.HANZI:
+          j = i + 1;
+          wordBuf.delete(0, wordBuf.length());
+          // ä¸????¸ªæ±???½ä??½æ????ï¼??å°??ä¸??å­????egGraphä¸??ï¼????????????¾æ?å­?
+          wordBuf.append(sentence.charAt(i));
+          charArray = new char[] { sentence.charAt(i) };
+          frequency = wordDict.getFrequency(charArray);
+          token = new SegToken(charArray, i, j, WordType.CHINESE_WORD,
+              frequency);
+          segGraph.addToken(token);
+
+          foundIndex = wordDict.getPrefixMatch(charArray);
+          while (j <= length && foundIndex != -1) {
+            if (wordDict.isEqual(charArray, foundIndex) && charArray.length > 1) {
+              // å°±æ???»¬è¦?????ï¼? ä¹?°±????¾å?äº??i?????ä¸??è¯?egTokenï¼?¹¶ä¸?????å­??
+              frequency = wordDict.getFrequency(charArray);
+              token = new SegToken(charArray, i, j, WordType.CHINESE_WORD,
+                  frequency);
+              segGraph.addToken(token);
+            }
+
+            while (j < length && charTypeArray[j] == CharType.SPACE_LIKE)
+              j++;
+
+            if (j < length && charTypeArray[j] == CharType.HANZI) {
+              wordBuf.append(sentence.charAt(j));
+              charArray = new char[wordBuf.length()];
+              wordBuf.getChars(0, charArray.length, charArray, 0);
+              // idArrayä½?¸º???å·²ç??¾å?è¿?(foundWordIndex!=-1),
+              // ??????è¿????dArray????½å??°å?foundWordIndexä»¥å?,
+              // ???foundWordIndexä¹??å¼?å§????
+              foundIndex = wordDict.getPrefixMatch(charArray, foundIndex);
+              j++;
+            } else {
+              break;
+            }
+          }
+          i++;
+          break;
+        case CharType.FULLWIDTH_LETTER:
+          hasFullWidth = true;
+        case CharType.LETTER:
+          j = i + 1;
+          while (j < length
+              && (charTypeArray[j] == CharType.LETTER || charTypeArray[j] == CharType.FULLWIDTH_LETTER)) {
+            if (charTypeArray[j] == CharType.FULLWIDTH_LETTER)
+              hasFullWidth = true;
+            j++;
+          }
+          // ?¾å?äº??i?????ä¸?okenï¼?±»??¸ºLETTER???ç¬?¸²
+          charArray = Utility.STRING_CHAR_ARRAY;
+          frequency = wordDict.getFrequency(charArray);
+          wordType = hasFullWidth ? WordType.FULLWIDTH_STRING : WordType.STRING;
+          token = new SegToken(charArray, i, j, wordType, frequency);
+          segGraph.addToken(token);
+          i = j;
+          break;
+        case CharType.FULLWIDTH_DIGIT:
+          hasFullWidth = true;
+        case CharType.DIGIT:
+          j = i + 1;
+          while (j < length
+              && (charTypeArray[j] == CharType.DIGIT || charTypeArray[j] == CharType.FULLWIDTH_DIGIT)) {
+            if (charTypeArray[j] == CharType.FULLWIDTH_DIGIT)
+              hasFullWidth = true;
+            j++;
+          }
+          // ?¾å?äº??i?????ä¸?okenï¼?±»??¸ºNUMBER???ç¬?¸²
+          charArray = Utility.NUMBER_CHAR_ARRAY;
+          frequency = wordDict.getFrequency(charArray);
+          wordType = hasFullWidth ? WordType.FULLWIDTH_NUMBER : WordType.NUMBER;
+          token = new SegToken(charArray, i, j, wordType, frequency);
+          segGraph.addToken(token);
+          i = j;
+          break;
+        case CharType.DELIMITER:
+          j = i + 1;
+          // ???ç¬????eightä¸???¥ä?ï¼???¸ª??å¤§ç?é¢???³å?
+          frequency = Utility.MAX_FREQUENCE;
+          charArray = new char[] { sentence.charAt(i) };
+          token = new SegToken(charArray, i, j, WordType.DELIMITER, frequency);
+          segGraph.addToken(token);
+          i = j;
+          break;
+        default:
+          j = i + 1;
+          // ???è®¤è????ç¬??ä½???¥ä¸²???ï¼??å¦?B2312ç¼??ä¹?????ç¬??æ¯?¸ªå­??å½??ä¸?ä¸?
+          charArray = Utility.STRING_CHAR_ARRAY;
+          frequency = wordDict.getFrequency(charArray);
+          token = new SegToken(charArray, i, j, WordType.STRING, frequency);
+          segGraph.addToken(token);
+          i = j;
+          break;
+      }
+    }
+
+    // ä¸?egGraphå¢??ä¸¤ä¸ª??okenï¼? "å§?##å§?","??##??"
+    charArray = Utility.START_CHAR_ARRAY;
+    frequency = wordDict.getFrequency(charArray);
+    token = new SegToken(charArray, -1, 0, WordType.SENTENCE_BEGIN, frequency);
+    segGraph.addToken(token);
+
+    // "??##??"
+    charArray = Utility.END_CHAR_ARRAY;
+    frequency = wordDict.getFrequency(charArray);
+    token = new SegToken(charArray, length, length + 1, WordType.SENTENCE_END,
+        frequency);
+    segGraph.addToken(token);
+
+    return segGraph;
+  }
+
+  /**
+   * ä¸?entenceä¸??æ¯?¸ªå­??ç¡????????ç¬?±»??
+   * 
+   * @see Utility.charType(char)
+   * @param sentence è¾????????å­?
+   * @return è¿?????ç¬?±»???ç»??å¦??è¾??ä¸?ullï¼???????ull
+   */
+  private static int[] getCharTypes(String sentence) {
+    int length = sentence.length();
+    int[] charTypeArray = new int[length];
+    // ???å¯¹å???¸ªæ±?????ç¬?±»???ç»?
+    for (int i = 0; i < length; i++) {
+      charTypeArray[i] = Utility.getCharType(sentence.charAt(i));
+    }
+
+    return charTypeArray;
+  }
+
+  public List process(String sentence) {
+    SegGraph segGraph = createSegGraph(sentence);
+    BiSegGraph biSegGraph = new BiSegGraph(segGraph);
+    List shortPath = biSegGraph.getShortPath();
+    return shortPath;
+  }
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java
new file mode 100644
index 0000000..612e6a5
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java
@@ -0,0 +1,33 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+public class PathNode implements Comparable {
+  public double weight;
+
+  public int preNode;
+
+  public int compareTo(Object p) {
+    PathNode pn = (PathNode) p;
+    if (weight < pn.weight)
+      return -1;
+    else if (weight == pn.weight)
+      return 0;
+    else
+      return 1;
+  }
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java
new file mode 100644
index 0000000..3960650
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java
@@ -0,0 +1,144 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+public class SegGraph {
+
+  /**
+   * ?¨ä?ä¸?rrayListè®°å?startOffset?¸å???okenï¼??ä¸?tartOffsetå°±æ?Token??ey
+   */
+  private Map tokenListTable = new HashMap();
+
+  private int maxStart = -1;
+
+  /**
+   * ?¥ç?startOffsetä¸???oken???å­??ï¼????²¡???è¯´æ?så¤?²¡??oken?????æ²¡æ?æ·»å?
+   * 
+   * @param s startOffset
+   * @return
+   */
+  public boolean isStartExist(int s) {
+    return tokenListTable.get(new Integer(s)) != null;
+  }
+
+  /**
+   * ???startOffsetä¸??????okensï¼????²¡???è¿??null
+   * 
+   * @param s
+   * @return ???????tartOffset??oken?????
+   */
+  public List getStartList(int s) {
+    return (List) tokenListTable.get(new Integer(s));
+  }
+
+  public int getMaxStart() {
+    return maxStart;
+  }
+
+  /**
+   * ä¸?egGraphä¸??????okens???ä¸?ä¸??ä¸???ndexï¼?ndexä»?0å¼?å§??
+   * ???startOffset?????¡ºåº??åº???¸å?startOffset??okens????¾ç½®???é¡ºå????
+   */
+  public List makeIndex() {
+    List result = new ArrayList();
+    int s = -1, count = 0, size = tokenListTable.size();
+    List tokenList;
+    short index = 0;
+    while (count < size) {
+      if (isStartExist(s)) {
+        tokenList = (List) tokenListTable.get(new Integer(s));
+        for (Iterator iter = tokenList.iterator(); iter.hasNext();) {
+          SegToken st = (SegToken) iter.next();
+          st.index = index;
+          result.add(st);
+          index++;
+        }
+        count++;
+      }
+      s++;
+    }
+    return result;
+  }
+
+  /**
+   * ??apä¸?????ä¸?okenï¼??äº?oken????¸å?startOffset?¾å????ä¸??è¡¨ä¸­ï¼?
+   * 
+   * @param token
+   */
+  public void addToken(SegToken token) {
+    int s = token.startOffset;
+    if (!isStartExist(s)) {
+      ArrayList newlist = new ArrayList();
+      newlist.add(token);
+      tokenListTable.put((Object) (new Integer(s)), newlist);
+    } else {
+      List tokenList = (List) tokenListTable.get((Object) (new Integer(s)));
+      tokenList.add(token);
+    }
+    if (s > maxStart)
+      maxStart = s;
+  }
+
+  /**
+   * ?·å?SegGraphä¸????µ·å§??Startï¼??ç½?okenç±»ç?ä¸??ï¼??ä¸??å§??ç½???½æ?å¤?¸ªTokenï¼??æ­¤ä?ç½??ä¸?oken?°å¹¶ä¸????
+   * 
+   * @return
+   */
+  public int getStartCount() {
+    return tokenListTable.size();
+  }
+
+  /**
+   * å°?apä¸???¨ç?????oken???èµ·å?ä½?½®ä»???°å¤§???å¼?????ä¸??è¡?
+   * 
+   * @return
+   */
+  public List toTokenList() {
+    List result = new ArrayList();
+    int s = -1, count = 0, size = tokenListTable.size();
+    List tokenList;
+
+    while (count < size) {
+      if (isStartExist(s)) {
+        tokenList = (List) tokenListTable.get(new Integer(s));
+        for (Iterator iter = tokenList.iterator(); iter.hasNext();) {
+          SegToken st = (SegToken) iter.next();
+          result.add(st);
+        }
+        count++;
+      }
+      s++;
+    }
+    return result;
+  }
+
+  public String toString() {
+    List tokenList = this.toTokenList();
+    StringBuffer sb = new StringBuffer();
+    for (Iterator iter = tokenList.iterator(); iter.hasNext();) {
+      SegToken t = (SegToken) iter.next();
+      sb.append(t + "\n");
+    }
+    return sb.toString();
+  }
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java
new file mode 100644
index 0000000..2e7cf80
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java
@@ -0,0 +1,64 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+public class SegToken {
+  public char[] charArray;
+
+  public int startOffset;
+
+  public int endOffset;
+
+  public int wordType;
+
+  public int weight;
+
+  public int index;
+
+  public SegToken(String word, int start, int end, int wordType, int weight) {
+    this.charArray = word.toCharArray();
+    this.startOffset = start;
+    this.endOffset = end;
+    this.wordType = wordType;
+    this.weight = weight;
+  }
+
+  public SegToken(char[] idArray, int start, int end, int wordType, int weight) {
+    this.charArray = idArray;
+    this.startOffset = start;
+    this.endOffset = end;
+    this.wordType = wordType;
+    this.weight = weight;
+  }
+
+  // public String toString() {
+  // return String.valueOf(charArray) + "/s(" + startOffset + ")e("
+  // + endOffset + ")/w(" + weight + ")t(" + wordType + ")";
+  // }
+
+  /**
+   * ?¤æ?ä¸¤ä¸ªToken?¸ç????è¦??ä»¶æ?ä»?»¬??µ·å§??ç½??ç­????¸ºè¿??ä»?»¬????¥ä¸­???å®¹ä??·ï?
+   * ??osä¸?eight?½å?ä»¥ä?è¯??ä¸???°å?ä¸????»¥?¨ä?å¯¹å????æ³?¡¨ç¤ºï???????è¦??ä¸?oken
+   * 
+   * @param t
+   * @return
+   */
+  // public boolean equals(RawToken t) {
+  // return this.startOffset == t.startOffset
+  // && this.endOffset == t.endOffset;
+  // }
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenFilter.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenFilter.java
new file mode 100644
index 0000000..e72553b
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenFilter.java
@@ -0,0 +1,50 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+import org.apache.lucene.analysis.cn.smart.Utility;
+import org.apache.lucene.analysis.cn.smart.WordType;
+
+public class SegTokenFilter {
+
+  public SegToken filter(SegToken token) {
+    switch (token.wordType) {
+      case WordType.FULLWIDTH_NUMBER:
+      case WordType.FULLWIDTH_STRING:
+        for (int i = 0; i < token.charArray.length; i++) {
+          if (token.charArray[i] >= 0xFF10)
+            token.charArray[i] -= 0xFEE0;
+
+          if (token.charArray[i] >= 0x0041 && token.charArray[i] <= 0x005A)
+            token.charArray[i] += 0x0020;
+        }
+        break;
+      case WordType.STRING:
+        for (int i = 0; i < token.charArray.length; i++) {
+          if (token.charArray[i] >= 0x0041 && token.charArray[i] <= 0x005A)
+            token.charArray[i] += 0x0020;
+        }
+        break;
+      case WordType.DELIMITER:
+        token.charArray = Utility.COMMON_DELIMITER;
+        break;
+      default:
+        break;
+    }
+    return token;
+  }
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java
new file mode 100644
index 0000000..10cfd03
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java
@@ -0,0 +1,48 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+public class SegTokenPair {
+
+  public char[] charArray;
+
+  /**
+   * from??o??okenå¯¹ç?index?·ï?è¡¨ç¤º??okenPair??¸¤ä¸?oken??egGraghä¸??ä½?½®??
+   */
+  public int from;
+
+  public int to;
+
+  public double weight;
+
+  public SegTokenPair(char[] idArray, int from, int to, double weight) {
+    this.charArray = idArray;
+    this.from = from;
+    this.to = to;
+    this.weight = weight;
+  }
+
+  // public String toString() {
+  // return String.valueOf(charArray) + ":f(" + from + ")t(" + to + "):"
+  // + weight;
+  // }
+
+  // public boolean equals(SegTokenPair tp) {
+  // return this.from == tp.from && this.to == tp.to;
+  // }
+
+}
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java
new file mode 100644
index 0000000..53cdfae
--- /dev/null
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java
@@ -0,0 +1,568 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn.smart.hhmm;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.io.RandomAccessFile;
+import java.io.UnsupportedEncodingException;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+
+import org.apache.lucene.analysis.cn.smart.AnalyzerProfile;
+import org.apache.lucene.analysis.cn.smart.Utility;
+
+public class WordDictionary extends AbstractDictionary {
+
+  private WordDictionary() {
+  }
+
+  private static WordDictionary singleInstance;
+
+  /**
+   * ä¸?ä¸??å¤§ç?ç´??ï¼??è¯?ash?¥æ??½å?????????ç½?
+   */
+  public static final int PRIME_INDEX_LENGTH = 12071;
+
+  /**
+   * wordIndexTableä¿??å°?nicodeä¸???????å­????ash??RIME_INDEX_LENGTH?¿åº¦???ç»?¸­ï¼?
+   * å½??ä¼???²ç?ï¼??å®??ä¸??ç¨???????B2312å­???¨å?ï¼?6768ä¸??ç¬??ä¸??äº?SCIIå­??ï¼?
+   * ???å¯¹è?äº??ç¬????????ä¸ºä?ä¿??æ¯?????ç¡??§ï?ä¿????????ç¬??charIndexTableä¸?»¥ç¡???¥æ????ç¡???
+   */
+  private short[] wordIndexTable;
+
+  private char[] charIndexTable;
+
+  /**
+   * å­???????åº??????°æ?ç»??ï¼?¸ºäº??????¨ç©º?´å¤ªå¤???¨ä?ä¸¤ä¸ª??????ç»´æ?ç»??å­??è¯?????????
+   * æ¯?¸ªè¯???¨ä?ä¸?har[]ä¸??æ¯?¸ªcharå¯¹å?ä¸?ä¸??å­???¶ä?å­??ï¼??ä¸??????¨ä?ä¸?ntä¸??
+   * è¿?¸¤ä¸??ç»????¸¤ä¸??è¡¨æ?ä¸?ä¸?å¯¹å??????æ­¤å?ä»¥å???ordItem_charArrayTable[i][j]?¥æ?è¯??
+   * ??ordItem_frequencyTable[i][j]?¥æ?è¯¢å?åº??é¢??
+   */
+  private char[][][] wordItem_charArrayTable;
+
+  private int[][] wordItem_frequencyTable;
+
+  // static Logger log = Logger.getLogger(WordDictionary.class);
+
+  public synchronized static WordDictionary getInstance() {
+    if (singleInstance == null) {
+      singleInstance = new WordDictionary();
+      try {
+        singleInstance.load();
+      } catch (IOException e) {
+        String wordDictRoot = AnalyzerProfile.ANALYSIS_DATA_DIR;
+        singleInstance.load(wordDictRoot);
+      } catch (ClassNotFoundException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    return singleInstance;
+  }
+
+  /**
+   * ä»???¨æ?ä»¶å¤¹dctFileRoot??½½è¯??åº??ä»¶ï?é¦??æµ???????oredict.mem??»¶ï¼? å¦??????´æ?ä½?¸ºåº?????è±¡å?è½½ï?
+   * å¦??æ²¡æ????è½½è??¸å?æº??ä»?oredict.dct
+   * 
+   * @param dctFileName è¯??åº??ä»¶ç?è·??
+   */
+  public void load(String dctFileRoot) {
+    String dctFilePath = dctFileRoot + "/coredict.dct";
+    File serialObj = new File(dctFileRoot + "/coredict.mem");
+
+    if (serialObj.exists() && loadFromObj(serialObj)) {
+
+    } else {
+      try {
+        wordIndexTable = new short[PRIME_INDEX_LENGTH];
+        charIndexTable = new char[PRIME_INDEX_LENGTH];
+        for (int i = 0; i < PRIME_INDEX_LENGTH; i++) {
+          charIndexTable[i] = 0;
+          wordIndexTable[i] = -1;
+        }
+        wordItem_charArrayTable = new char[GB2312_CHAR_NUM][][];
+        wordItem_frequencyTable = new int[GB2312_CHAR_NUM][];
+        // int total =
+        loadMainDataFromFile(dctFilePath);
+        expandDelimiterData();
+        mergeSameWords();
+        sortEachItems();
+        // log.info("load dictionary: " + dctFilePath + " total:" + total);
+      } catch (IOException e) {
+        throw new RuntimeException(e.getMessage());
+      }
+
+      saveToObj(serialObj);
+    }
+
+  }
+
+  /**
+   * ä»?ar?????½½è¯??åº??ä»¶ï?è¦??ä¿??WordDictionaryç±»å???·¯å¾?¸­??oredict.mem??»¶ï¼?»¥å°??ä½?¸ºåº?????è±¡å?è½?
+   * 
+   * @param dctFileName è¯??åº??ä»¶ç?è·??
+   * @throws ClassNotFoundException
+   * @throws IOException
+   */
+  public void load() throws IOException, ClassNotFoundException {
+    InputStream input = this.getClass().getResourceAsStream("coredict.mem");
+    loadFromObjectInputStream(input);
+  }
+
+  private boolean loadFromObj(File serialObj) {
+    try {
+      loadFromObjectInputStream(new FileInputStream(serialObj));
+      return true;
+    } catch (FileNotFoundException e) {
+      e.printStackTrace();
+    } catch (IOException e) {
+      e.printStackTrace();
+    } catch (ClassNotFoundException e) {
+      e.printStackTrace();
+    }
+    return false;
+  }
+
+  private void loadFromObjectInputStream(InputStream serialObjectInputStream)
+      throws IOException, ClassNotFoundException {
+    ObjectInputStream input = new ObjectInputStream(serialObjectInputStream);
+    wordIndexTable = (short[]) input.readObject();
+    charIndexTable = (char[]) input.readObject();
+    wordItem_charArrayTable = (char[][][]) input.readObject();
+    wordItem_frequencyTable = (int[][]) input.readObject();
+    // log.info("load core dict from serialization.");
+    input.close();
+  }
+
+  private void saveToObj(File serialObj) {
+    try {
+      ObjectOutputStream output = new ObjectOutputStream(new FileOutputStream(
+          serialObj));
+      output.writeObject(wordIndexTable);
+      output.writeObject(charIndexTable);
+      output.writeObject(wordItem_charArrayTable);
+      output.writeObject(wordItem_frequencyTable);
+      output.close();
+      // log.info("serialize core dict.");
+    } catch (Exception e) {
+      // log.warn(e.getMessage());
+    }
+  }
+
+  /**
+   * å°??åº??ä»¶å?è½½å?WordDictionary????³æ??????¸­ï¼?????è½½ï?æ²¡æ?è¿????¹¶??¿®?¹æ?ä½?
+   * 
+   * @param dctFilePath
+   * @return
+   * @throws FileNotFoundException
+   * @throws IOException
+   * @throws UnsupportedEncodingException
+   */
+  private int loadMainDataFromFile(String dctFilePath)
+      throws FileNotFoundException, IOException, UnsupportedEncodingException {
+    int i, cnt, length, total = 0;
+    // ??»¶ä¸??ç»??äº?6763ä¸??å­??5ä¸?©ºæ±??ç¬?3756~3760ï¼??ä¸??3756ä¸???¥å??¨ç??·ä¿¡????
+    int[] buffer = new int[3];
+    byte[] intBuffer = new byte[4];
+    String tmpword;
+    RandomAccessFile dctFile = new RandomAccessFile(dctFilePath, "r");
+
+    // å­????»¶ä¸??ä¸?ä¸??å­???°ç?ä½?½®??0ï¼?????ä¸??6768
+    for (i = GB2312_FIRST_CHAR; i < GB2312_FIRST_CHAR + CHAR_NUM_IN_FILE; i++) {
+      // if (i == 5231)
+      // System.out.println(i);
+
+      dctFile.read(intBuffer);// ???åº??ä»¶å?cä¸???????ä»¥å??¥ç???»¶ä¸?ittle
+      // endianç¼??ï¼???avaä¸?ig endianï¼??é¡»è½¬?¢è???
+      cnt = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN).getInt();
+      if (cnt <= 0) {
+        wordItem_charArrayTable[i] = null;
+        wordItem_frequencyTable[i] = null;
+        continue;
+      }
+      wordItem_charArrayTable[i] = new char[cnt][];
+      wordItem_frequencyTable[i] = new int[cnt];
+      total += cnt;
+      int j = 0;
+      while (j < cnt) {
+        // wordItemTable[i][j] = new WordItem();
+        dctFile.read(intBuffer);
+        buffer[0] = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN)
+            .getInt();// frequency
+        dctFile.read(intBuffer);
+        buffer[1] = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN)
+            .getInt();// length
+        dctFile.read(intBuffer);
+        buffer[2] = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN)
+            .getInt();// handle
+
+        // wordItemTable[i][j].frequency = buffer[0];
+        wordItem_frequencyTable[i][j] = buffer[0];
+
+        length = buffer[1];
+        if (length > 0) {
+          byte[] lchBuffer = new byte[length];
+          dctFile.read(lchBuffer);
+          tmpword = new String(lchBuffer, "GB2312");
+          // indexTable[i].wordItems[j].word = tmpword;
+          // wordItemTable[i][j].charArray = tmpword.toCharArray();
+          wordItem_charArrayTable[i][j] = tmpword.toCharArray();
+        } else {
+          // wordItemTable[i][j].charArray = null;
+          wordItem_charArrayTable[i][j] = null;
+        }
+        // System.out.println(indexTable[i].wordItems[j]);
+        j++;
+      }
+
+      String str = getCCByGB2312Id(i);
+      setTableIndex(str.charAt(0), i);
+    }
+    dctFile.close();
+    return total;
+  }
+
+  /**
+   * ???åº????????¹ç??·ç?ä¿¡æ???¹¶?°ä?ä¸??è¡¨é?(ä»?1å¼?å§??3755å¤?)???????¶å?å¼?ï¼??????°å?ä¸???·å?åº????¡¨ä¸?
+   */
+  private void expandDelimiterData() {
+    int i;
+    int cnt;
+    // ???ç¬???¨ä?1å¼?å§??3755å¤??å°??å§?????ç¬??å¯¹å?????¸å????å¯¹å?????¹ç??·ä¸­
+    int delimiterIndex = 3755 + GB2312_FIRST_CHAR;
+    i = 0;
+    while (i < wordItem_charArrayTable[delimiterIndex].length) {
+      char c = wordItem_charArrayTable[delimiterIndex][i][0];
+      int j = getGB2312Id(c);// è¯¥æ??¹ç??·å?è¯¥æ??¨ç?index??
+      if (wordItem_charArrayTable[j] == null) {
+
+        int k = i;
+        // ä»?å¼?å§???°å???»¥jå¼?å¤´ç?ç¬????orditem??¸ª??
+        while (k < wordItem_charArrayTable[delimiterIndex].length
+            && wordItem_charArrayTable[delimiterIndex][k][0] == c) {
+          k++;
+        }
+        // æ­¤æ?k-iä¸?dä¸?????¹ç??·å?åº??wordItem??¸ª??
+        cnt = k - i;
+        if (cnt != 0) {
+          wordItem_charArrayTable[j] = new char[cnt][];
+          wordItem_frequencyTable[j] = new int[cnt];
+        }
+
+        // ä¸ºæ?ä¸?ä¸?ordItemèµ???
+        for (k = 0; k < cnt; k++, i++) {
+          // wordItemTable[j][k] = new WordItem();
+          wordItem_frequencyTable[j][k] = wordItem_frequencyTable[delimiterIndex][i];
+          wordItem_charArrayTable[j][k] = new char[wordItem_charArrayTable[delimiterIndex][i].length - 1];
+          System.arraycopy(wordItem_charArrayTable[delimiterIndex][i], 1,
+              wordItem_charArrayTable[j][k], 0,
+              wordItem_charArrayTable[j][k].length);
+        }
+        setTableIndex(c, j);
+      }
+    }
+    // å°??ç¬??å¯¹å????ç»????
+    wordItem_charArrayTable[delimiterIndex] = null;
+    wordItem_frequencyTable[delimiterIndex] = null;
+  }
+
+  /**
+   * ???åº??????§æ?æ³?????å°?????ä¸??è¯??§ç?é¢????¹¶?°å?ä¸?ä¸??ä¸??ä»¥å?å°???¨ç©º?´ï???¿«??´¢??º¦
+   */
+  private void mergeSameWords() {
+    int i;
+    for (i = 0; i < GB2312_FIRST_CHAR + CHAR_NUM_IN_FILE; i++) {
+      if (wordItem_charArrayTable[i] == null)
+        continue;
+      int len = 1;
+      for (int j = 1; j < wordItem_charArrayTable[i].length; j++) {
+        if (Utility.compareArray(wordItem_charArrayTable[i][j], 0,
+            wordItem_charArrayTable[i][j - 1], 0) != 0)
+          len++;
+
+      }
+      if (len < wordItem_charArrayTable[i].length) {
+        char[][] tempArray = new char[len][];
+        int[] tempFreq = new int[len];
+        int k = 0;
+        tempArray[0] = wordItem_charArrayTable[i][0];
+        tempFreq[0] = wordItem_frequencyTable[i][0];
+        for (int j = 1; j < wordItem_charArrayTable[i].length; j++) {
+          if (Utility.compareArray(wordItem_charArrayTable[i][j], 0,
+              tempArray[k], 0) != 0) {
+            k++;
+            // temp[k] = wordItemTable[i][j];
+            tempArray[k] = wordItem_charArrayTable[i][j];
+            tempFreq[k] = wordItem_frequencyTable[i][j];
+          } else {
+            // temp[k].frequency += wordItemTable[i][j].frequency;
+            tempFreq[k] += wordItem_frequencyTable[i][j];
+          }
+        }
+        // wordItemTable[i] = temp;
+        wordItem_charArrayTable[i] = tempArray;
+        wordItem_frequencyTable[i] = tempFreq;
+      }
+    }
+  }
+
+  private void sortEachItems() {
+    char[] tmpArray;
+    int tmpFreq;
+    for (int i = 0; i < wordItem_charArrayTable.length; i++) {
+      if (wordItem_charArrayTable[i] != null
+          && wordItem_charArrayTable[i].length > 1) {
+        for (int j = 0; j < wordItem_charArrayTable[i].length - 1; j++) {
+          for (int j2 = j + 1; j2 < wordItem_charArrayTable[i].length; j2++) {
+            if (Utility.compareArray(wordItem_charArrayTable[i][j], 0,
+                wordItem_charArrayTable[i][j2], 0) > 0) {
+              tmpArray = wordItem_charArrayTable[i][j];
+              tmpFreq = wordItem_frequencyTable[i][j];
+              wordItem_charArrayTable[i][j] = wordItem_charArrayTable[i][j2];
+              wordItem_frequencyTable[i][j] = wordItem_frequencyTable[i][j2];
+              wordItem_charArrayTable[i][j2] = tmpArray;
+              wordItem_frequencyTable[i][j2] = tmpFreq;
+            }
+          }
+        }
+      }
+    }
+  }
+
+  /**
+   * è®¡ç?å­??c?¨å?å¸?¡¨ä¸??è¯¥å????ç½???¶å?å°??????¡¨ä¸??ä½?½®???¼å?å§??
+   * 
+   * @param c
+   * @param j
+   * @return
+   */
+  private boolean setTableIndex(char c, int j) {
+    int index = getAvaliableTableIndex(c);
+    if (index != -1) {
+      charIndexTable[index] = c;
+      wordIndexTable[index] = (short) j;
+      return true;
+    } else
+      return false;
+  }
+
+  private short getAvaliableTableIndex(char c) {
+    int hash1 = (int) (hash1(c) % PRIME_INDEX_LENGTH);
+    int hash2 = hash2(c) % PRIME_INDEX_LENGTH;
+    if (hash1 < 0)
+      hash1 = PRIME_INDEX_LENGTH + hash1;
+    if (hash2 < 0)
+      hash2 = PRIME_INDEX_LENGTH + hash2;
+    int index = hash1;
+    int i = 1;
+    while (charIndexTable[index] != 0 && charIndexTable[index] != c
+        && i < PRIME_INDEX_LENGTH) {
+      index = (hash1 + i * hash2) % PRIME_INDEX_LENGTH;
+      i++;
+    }
+    // System.out.println(i - 1);
+
+    if (i < PRIME_INDEX_LENGTH
+        && (charIndexTable[index] == 0 || charIndexTable[index] == c)) {
+      return (short) index;
+    } else
+      return -1;
+  }
+
+  /**
+   * @param c
+   * @return
+   */
+  private short getWordItemTableIndex(char c) {
+    int hash1 = (int) (hash1(c) % PRIME_INDEX_LENGTH);
+    int hash2 = hash2(c) % PRIME_INDEX_LENGTH;
+    if (hash1 < 0)
+      hash1 = PRIME_INDEX_LENGTH + hash1;
+    if (hash2 < 0)
+      hash2 = PRIME_INDEX_LENGTH + hash2;
+    int index = hash1;
+    int i = 1;
+    while (charIndexTable[index] != 0 && charIndexTable[index] != c
+        && i < PRIME_INDEX_LENGTH) {
+      index = (hash1 + i * hash2) % PRIME_INDEX_LENGTH;
+      i++;
+    }
+
+    if (i < PRIME_INDEX_LENGTH && charIndexTable[index] == c) {
+      return (short) index;
+    } else
+      return -1;
+  }
+
+  /**
+   * ?¨å??¸å?ä¸???¾å?è¯??åº??char?°ç?ä¸?harArray???ç¬?¸²??????????¨å?è¯????¸­???ç½?
+   * 
+   * @param charArray ?¥æ????å¯¹å???har?°ç?
+   * @return ????¨å?è¯??ç»?¸­???ç½??å¦??æ²¡æ??°å?è¿??-1
+   */
+  private int findInTable(char[] charArray) {
+    if (charArray == null || charArray.length == 0)
+      return -1;
+    short index = getWordItemTableIndex(charArray[0]);
+    if (index == -1)
+      return -1;
+
+    return findInTable(index, charArray);
+
+  }
+
+  /**
+   * ?¨å??¸å?ä¸???¾å?è¯??åº??char?°ç?ä¸?harArray???ç¬?¸²??????????¨å?è¯????¸­???ç½?
+   * 
+   * @param knownHashIndex å·²ç????ç¬??ä¸??ç¬?harArray[0]??ashè¡¨ä¸­???ç½??å¦?????ç®????»¥?¨å???nt
+   *        findInTable(char[] charArray) ä»£æ?
+   * @param charArray ?¥æ????å¯¹å???har?°ç?
+   * @return ????¨å?è¯??ç»?¸­???ç½??å¦??æ²¡æ??°å?è¿??-1
+   */
+  private int findInTable(short knownHashIndex, char[] charArray) {
+    if (charArray == null || charArray.length == 0)
+      return -1;
+
+    char[][] items = wordItem_charArrayTable[wordIndexTable[knownHashIndex]];
+    int start = 0, end = items.length - 1;
+    int mid = (start + end) / 2, cmpResult;
+
+    // Binary search for the index of idArray
+    while (start <= end) {
+      cmpResult = Utility.compareArray(items[mid], 0, charArray, 1);
+
+      if (cmpResult == 0)
+        return mid;// find it
+      else if (cmpResult < 0)
+        start = mid + 1;
+      else if (cmpResult > 0)
+        end = mid - 1;
+
+      mid = (start + end) / 2;
+    }
+    return -1;
+  }
+
+  /**
+   * charArrayè¿?¸ª???å¯¹å????ç»??ä¸??WordDictionaryä¸????
+   * 
+   * @param charArray
+   * @return trueè¡¨ç¤ºå­??ï¼?alseè¡¨ç¤ºä¸????
+   */
+  public boolean isExist(char[] charArray) {
+    return findInTable(charArray) != -1;
+  }
+
+  /**
+   * @see{getPrefixMatch(char[] charArray, int knownStart)}
+   * @param charArray
+   * @return
+   */
+  public int getPrefixMatch(char[] charArray) {
+    return getPrefixMatch(charArray, 0);
+  }
+
+  /**
+   * ä»???¸ä¸­?¥æ?ä»?harArrayå¯¹å????è¯?¸º???(prefix)???è¯??ä½?½®, å¹¶è????ä¸?ä¸?»¡è¶³æ?ä»¶ç?ä½?½®??¸ºäº??å°??ç´?»£ä»?,
+   * ??»¥?¹æ?å·²æ??¥è?è®¾ç½®èµ·å???´¢ä½?½®, å¦??ä¸????µ·å§??ç½??é»????0
+   * 
+   * @see{getPrefixMatch(char[] charArray)}
+   * @param charArray ??????
+   * @param knownStart å·²ç???µ·å§??ç½?
+   * @return æ»¡è¶³????¡ä»¶???ä¸?ä¸??è¯??ä½?½®
+   */
+  public int getPrefixMatch(char[] charArray, int knownStart) {
+    short index = getWordItemTableIndex(charArray[0]);
+    if (index == -1)
+      return -1;
+    char[][] items = wordItem_charArrayTable[wordIndexTable[index]];
+    int start = knownStart, end = items.length - 1;
+
+    int mid = (start + end) / 2, cmpResult;
+
+    // Binary search for the index of idArray
+    while (start <= end) {
+      cmpResult = Utility.compareArrayByPrefix(charArray, 1, items[mid], 0);
+      if (cmpResult == 0) {
+        // Get the first item which match the current word
+        while (mid >= 0
+            && Utility.compareArrayByPrefix(charArray, 1, items[mid], 0) == 0)
+          mid--;
+        mid++;
+        return mid;// ?¾å?ç¬??ä¸?»¥charArrayä¸ºå?ç¼????è¯?
+      } else if (cmpResult < 0)
+        end = mid - 1;
+      else
+        start = mid + 1;
+      mid = (start + end) / 2;
+    }
+    return -1;
+  }
+
+  /**
+   * ?·å?idArrayå¯¹å???????é¢????osä¸?-1??????????§ç?è¯??
+   * 
+   * @param charArray è¾?????è¯??åº??charArray
+   * @param pos è¯??§ï?-1è¡¨ç¤ºè¦??æ±???????è¯??§ç?è¯??
+   * @return idArrayå¯¹å????é¢?
+   */
+  public int getFrequency(char[] charArray) {
+    short hashIndex = getWordItemTableIndex(charArray[0]);
+    if (hashIndex == -1)
+      return 0;
+    int itemIndex = findInTable(hashIndex, charArray);
+    if (itemIndex != -1)
+      return wordItem_frequencyTable[wordIndexTable[hashIndex]][itemIndex];
+    return 0;
+
+  }
+
+  /**
+   * ?¤æ?charArrayå¯¹å????ç¬?¸²???è·???¸ä¸­charArray[0]å¯¹å???ordIndex??harArray?¸ç?,
+   * ä¹?°±???charArray???ç½???¾ç????ä¸??å°±æ?wordIndex
+   * 
+   * @param charArray è¾????harArrayè¯??ï¼??ä¸?ä¸??è¡¨ç¤ºè¯??ä¸??ç´¢å???
+   * @param itemIndex ä½?½®ç¼??
+   * @return ????¸ç?
+   */
+  public boolean isEqual(char[] charArray, int itemIndex) {
+    short hashIndex = getWordItemTableIndex(charArray[0]);
+    return Utility.compareArray(charArray, 1,
+        wordItem_charArrayTable[wordIndexTable[hashIndex]][itemIndex], 0) == 0;
+  }
+
+  public static void main(String[] args) throws FileNotFoundException,
+      IOException {
+    WordDictionary dic = new WordDictionary();
+    dic.load("D:/analysis-data");
+    Utility.getCharType('??');
+    Utility.getCharType('æ±?');
+    Utility.getCharType(' ');// 0020
+    Utility.getCharType('??');// 3000
+    Utility.getCharType('??');// E095
+    Utility.getCharType(' ');// 3000
+    Utility.getCharType('\r');// 000D
+    Utility.getCharType('\n');// 000A
+    Utility.getCharType('\t');// 0009
+  }
+}
diff --git a/contrib/analyzers/src/resources/org/apache/lucene/analysis/ar/stopwords.txt b/contrib/analyzers/src/resources/org/apache/lucene/analysis/ar/stopwords.txt
new file mode 100644
index 0000000..4bb557b
--- /dev/null
+++ b/contrib/analyzers/src/resources/org/apache/lucene/analysis/ar/stopwords.txt
@@ -0,0 +1,350 @@
+# This file was created by Jacques Savoy and is distributed under the BSD license.
+# See http://members.unine.ch/jacques.savoy/clef/index.html.
+# Also see http://www.opensource.org/licenses/bsd-license.html
+Ø¨
+Ø§
+Ø£
+?
+Ø¹Ø´Ø±
+Ø¹Ø¨Ø¯
+Ø¹Ø¯Ø¯
+Ø¹Ø¯Ø©
+Ø¹Ø´Ø±Ø©
+Ø¹Ø¯?
+Ø¹Ø§?
+Ø¹Ø§?Ø§
+Ø¹Ø±?Ø§Øª
+Ø¹?
+Ø¹?Ø¯
+Ø¹?Ø§?
+Ø¹?Ø¯?Ø§
+Ø¹??
+Ø¹??
+Ø¹???
+Ø¹???Ø§
+Ø¹???Ø©
+Ø²?Ø§Ø±Ø©
+Ø³Ø¨Øª?Ø¨Ø±
+Ø³Ø§Ø±Ø§????
+Ø³?Ø©
+Ø³?Ø±?Ø§
+Ø³??Ø§Øª
+ØªØ´Ø±??
+Øª?
+Øª??Ø²
+Ø¶Ø¯
+Ø¨Ø¹Ø¯
+Ø¨Ø¹Ø¶
+Ø§Ø¹Ø§Ø¯Ø©
+Ø§Ø¹??
+Ø§Ø¹??Øª
+Ø­Ø²Ø¨
+Ø­Ø²?Ø±Ø§?
+Ø¨Ø³Ø¨Ø¨
+Ø§Ø³Ø±Ø§Ø¦??
+Ø­Ø³??
+Ø­Øª?
+Ø§Øª?Ø§?
+ØµØ±Ø¨
+Ø§Ø°Ø§
+Ø§Ø­Ø¯
+Ø§Ø«Ø±
+ØºØ²Ø©
+Ø¨Ø±Ø³
+Ø¨Ø§Ø³?
+Ø§Ø¬Øª?Ø§Ø¹
+ØºØ¯Ø§
+Ø´Ø®ØµØ§
+ØµØ¨Ø§Ø­
+Ø§Ø·Ø§Ø±
+Ø§Ø±Ø¨Ø¹Ø©
+Ø¨ØºØ¯Ø§Ø¯
+Ø§Ø®Ø±?
+Ø¨Ø§Ø±?Ø³
+Ø±Ø§Ø¨??
+Ø´Ø±?
+Ø¨Ø§?
+Ø§Ø¨?
+Ø§Ø¬?
+Øº?Ø±
+Ø­Ø±?Ø©
+Ø±Ø¦?Ø³
+Ø¬Ø¯?Ø¯Ø©
+Ø§Ø·?Ø§?
+Ø¨Ø´??
+Ø¨Ø·??Ø©
+ØµØ­??Ø©
+Ø­Ø§??Ø§
+Ø¨?
+Ø¨?
+Ø«?
+Ø§?
+Ø§?
+Ø§?
+Ø§?
+Ø¨?Ø§
+Ø¬?Ø©
+Øµ?Ø±
+Ø­?Ø«
+Ø§?Ø¯
+Ø§?Ø§
+Ø§?Ø§
+Ø§?Ø¹Ø³?Ø±?Ø©
+Ø§?Ø¹Ø±Ø§?
+Ø§?Ø¹Ø§Øµ?Ø©
+Ø§?Ø¹Ø±Ø¨?Ø©
+Ø§?Ø¹Ø±Ø§??
+Ø§?Ø¹Ø±Ø§??Ø©
+Ø§?Ø¹Ø§?
+Ø§?Ø¹Ø§??
+Ø§?Ø¹?Ø§?Ø§Øª
+Ø§?Ø¹??
+Ø§?Ø³
+Ø§?Ø³Ø¹?Ø¯?Ø©
+Ø§?Ø³Ø§Ø¹Ø©
+Ø§?Ø³Ø¨Øª
+Ø§?Ø³Ø§Ø¨?
+Ø±?Ø³?Ø§
+Ø§?Ø³?Ø·Ø©
+Ø§?Ø³?Ø·Ø§Øª
+Ø§?Ø³?Ø§?
+Ø§?ØªØ¹Ø§??
+Ø§?ØªØ­Ø±?Ø±
+Ø§?Øª?
+Ø§?Øª?
+Ø§?Øª?Ø¨Ø±
+Ø¯?Ø±Ø©
+Ø§?Ø«Ø±
+Ø§?Ø§Ø±
+Ø§?Ø¶Ø§
+Ø§?Ø¬Ø²Ø§Ø¦Ø±
+Ø­?Ø§Ø³
+Ø§?Ø§Ø³Ø±Ø§Ø¦???
+Ø§?Ø§Ø³Ø±Ø§Ø¦???Ø©
+Ø§?Ø§Ø³Ø¨?Ø¹
+Ø§?Ø§Ø³?Ø­Ø©
+Ø§?Ø§Ø³?Ø§??Ø©
+Ø°?Ø±Øª
+Ø§?Ø§ØªØ­Ø§Ø¯
+Ø§?Ø§Øª?Ø§?
+Ø«?Ø§Ø«Ø©
+Ø§?Ø­Ø±Ø¨
+Ø§?Ø§Ø­Ø¯
+Ø§?Ø°Ø§Øª?
+Ø§?Ø´Ø±Ø·Ø©
+Ø§?Ø§Ø±Ø¨Ø¹Ø§Ø¡
+Ø§?ØºØ±Ø¨?Ø©
+Ø§?Ø®Ø§Ø±Ø¬?Ø©
+Ø§?Ø§Ø±Ø¯?
+Ø§?Ø´Ø±?
+Ø§?Ø±Ø§?
+Ø§?Ø­Ø¯?Ø¯
+Ø§?Ø±Ø¦?Ø³
+Ø§?Ø§Ø®?Ø±Ø©
+Ø§?Ø«Ø§??
+Ø§?Ø«Ø§??Ø©
+Ø§?Ø§Ø«???
+Ø´?Ø§?
+Ø¨?Ø§?
+Ø¯?Ø´?
+Ø§?Ø°?
+Ø§?Ø°?
+Ø§?Ø§?
+Ø§?Ø§?
+Ø§?Ø§?
+Ø®?Ø§?
+Ø§?Ø´?Ø®
+Ø§?Ø¬?Ø´
+Ø§?Ø¯?Ø±
+Ø§?Ø¶?Ø©
+Ø§?Ø¬?Ø¹Ø©
+Ø¨?Ø±?Ø²
+Ø§?Ø§?Ø³Ø·
+Ø§?Ø±?Ø³?
+Ø§?Ø¨?Ø³?Ø©
+Ø§?Ø±?Ø³?Ø©
+Ø¨?Ø±?Øª
+Ø§?Ø§?ØªØ®Ø§Ø¨Ø§Øª
+Ø§?Ø¨?Ø§Ø¯
+Ø§?Ø¯?Ø§Ø¹
+Ø§?Ø«?Ø«Ø§Ø¡
+Ø§?Ø§?Ø¨Ø§Ø¡
+Ø§?Ø«?Ø§Ø«Ø§Ø¡
+Ø§?Ø§?Ø±?Ø¨?
+Ø­?Ø§??
+Ø§?Ø°??
+Ø§?Ø¯??
+Ø§?Ø­??
+Ø§?Ø§??
+Ø§?Ø§??
+Ø§?Ø§??
+Ø§?Ø¯??Ø©
+Ø§?Ø®??Ø¬
+Ø§?Ø®??Ø³
+Ø§?Ø§??Ø±??
+Ø§?Ø§??Ø±??Ø©
+Ø§?Ø¯???
+Ø§?Ø§???
+Ø§?Ø¯???Ø©
+Ø§?Ø­???Ø©
+Ø¨??
+Ø°??
+Ø¯??
+Ø¯??
+Ø­??
+Ø­??
+Ø§??
+Ø§??
+Ø§??
+Ø§??
+Ø¶??
+Ø¬??Ø¨
+Ø¯??Ø©
+Ø§??Ø§
+Ø¬??Ø¹
+Ø§??Ø²Ø±Ø§Ø¡
+Ø§??ØªØ­Ø¯Ø«
+Ø§??ØªØ­Ø¯Ø©
+Ø¯??Ø§Ø±
+Ø§??Ø§Ø±
+Ø§??Ø¶Ø¹
+Ø§??Ø¯Ø³
+Ø§??Ø­Øª?Ø©
+Ø§??ØµØ¯Ø±
+Ø§??Ø¨Ø§Ø±Ø§Ø©
+Ø§??ØµØ±?
+Ø§??Ø§Ø¶?
+Ø§??ØµØ±?Ø©
+Ø§??Ø±Ø­?Ø©
+Ø§??Ø¯?
+Ø§??Ø¬?Ø©
+Ø§??Ø¬?Ø³
+Ø§??Ø±?Ø³?
+Ø§??Ø±?Ø³?Ø©
+Ø§??Ø§?Ø±Ø©
+Ø§??Ø¯??Ø©
+Ø§??Ø§??Ø§
+Ø§??Ø·??Ø©
+Ø§??Ø¬??Ø¹Ø©
+Ø§???
+Ø§???Ø³Ø·???
+Ø§???Ø³Ø·???Ø©
+Ø§???Ø³Ø·?????
+Ø§???Øª
+Ø§???Ø±Ø±
+Ø§???Ø§Øª
+Ø§???Ø§Ø¦?
+Ø§???Ø¨?
+Ø§???Ø·?Ø©
+Ø§???Ø§?Ø§Øª
+Ø§???Ø§?Ø¶Ø§Øª
+Ø§????
+Ø§????
+Ø§????
+Ø§????
+Ø§????Øª
+?
+?
+?
+?6
+?Ø¯
+?Ø§
+?Ø§
+?Ø¹
+?Ø²Ø§Ø±Ø©
+?Ø²?Ø±
+?Ø³Ø§Ø¡
+?Øª?
+?Ø±Ø©
+?ØµØ±
+?Ø°Ø§
+?Ø§Ø²
+?Ø£Ø³
+?Ø§Ø³Ø±
+?Ø±Ø§Ø±
+?ØµØ¯Ø±
+?Ø§Ø­Ø¯
+?Ø·Ø§Ø¹
+?ØµØ§Ø¯Ø±
+?Ø¨Ø§Ø±Ø§Ø©
+?Ø¨Ø§Ø±?
+?Ø§Ø¶Ø§?
+?Ø§Ø¶Ø§?Øª
+?Ø±Ø§?Ø³
+?Ø§Ø´?Ø·?
+?Ø§?
+?Ø¨?
+?Ø§?
+?Ø§?
+?Ø¯?
+?Ø­?
+?Ø°?
+?Ø§?
+?Ø­?Ø¯
+?Ø§?Ø¯
+?Ø°?Ø±
+?Ø¬?Ø³
+?Ø±?Ø³Ø§
+?Ø±?Ø³Øª??Ø±
+?Ø§?Øª
+?Ø§?Ø¶Ø­
+?Ø¨?Ø§?
+?Ø§??
+?Ø¯??Ø©
+?Ø¬??Ø¹Ø©
+?Ø§???
+??
+??
+??
+??
+??
+??
+??
+??
+??
+??Ø©
+??Ø§
+??Ø§
+??Ø°
+??Ø¯
+??Ø§
+??Ø³?
+??Ø³??
+??Øª?
+??Ø§Ø¡
+??Ø±Ø©
+??Ø·Ø©
+??Ø§Øª
+??Ø§Ø¨?
+??Ø¯?
+??Ø§?
+??Ø§?
+??Ø§?
+??Ø·?Ø©
+??Ø¸?Ø©
+??Ø§?Ø©
+??Ø§?Ø©
+??Ø§?Øª
+??Ø§?Øª
+??Ø§??
+???
+???
+???
+???
+???
+???
+???
+???
+???
+???
+???Ø§
+???Ø§
+???Ø§Ø±
+???Ø§?Ø©
+????
+????
+????Øª??
+?????
+?????
+?????
+?????Ø±?
diff --git a/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/smart/hhmm/bigramdict.mem b/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/smart/hhmm/bigramdict.mem
new file mode 100644
index 0000000..5eb8b2c
Binary files /dev/null and b/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/smart/hhmm/bigramdict.mem differ
diff --git a/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/smart/hhmm/coredict.mem b/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/smart/hhmm/coredict.mem
new file mode 100644
index 0000000..00314f3
Binary files /dev/null and b/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/smart/hhmm/coredict.mem differ
diff --git a/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/stopwords.txt b/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/stopwords.txt
new file mode 100644
index 0000000..c35f9fa
--- /dev/null
+++ b/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/stopwords.txt
@@ -0,0 +1,58 @@
+////////// å°???¹ç??·å??¨å??? ////////////////
+,
+.
+`
+-
+_
+=
+?
+'
+|
+"
+(
+)
+{
+}
+[
+]
+<
+>
+*
+#
+&
+^
+$
+@
+!
+~
+:
+;
++
+/
+\
+??
+??
+??
+ï¼?
+ï¼?
+??
+??
+ï¼?
+ï¼?
+ï¼?
+Â·
+ï¼?
+??
+??
+ï¼?
+ï¼?
+??
+??
+ï¼?
+ï¼?
+??
+??//ä¸??ç©ºæ?å­??
+
+//////////////// ?±æ????è¯? ////////////////
+
+//////////////// ä¸?????è¯? ////////////////
diff --git a/contrib/analyzers/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java b/contrib/analyzers/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java
new file mode 100644
index 0000000..66a6e44
--- /dev/null
+++ b/contrib/analyzers/src/test/org/apache/lucene/analysis/cn/TestSmartChineseAnalyzer.java
@@ -0,0 +1,86 @@
+/**
+ * Copyright 2009 www.imdict.net
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.cn;
+
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.Reader;
+import java.io.StringReader;
+import java.io.UnsupportedEncodingException;
+import java.util.Date;
+
+import junit.framework.TestCase;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+
+public class TestSmartChineseAnalyzer extends TestCase {
+
+  public void testChineseAnalyzer() throws IOException {
+    Token nt = new Token();
+    Analyzer ca = new SmartChineseAnalyzer(true);
+    Reader sentence = new StringReader("??´­ä¹°ä???????è£???");
+    String[] result = { "??", "è´?¹°", "äº?", "???", "??", "???" };
+    TokenStream ts = ca.tokenStream("sentence", sentence);
+    int i = 0;
+    nt = ts.next(nt);
+    while (nt != null) {
+      assertEquals(result[i], nt.term());
+      i++;
+      nt = ts.next(nt);
+    }
+    ts.close();
+  }
+
+  /**
+   * @param args
+   * @throws IOException
+   */
+  public static void main(String[] args) throws IOException {
+    new TestSmartChineseAnalyzer().sampleMethod();
+  }
+
+  /**
+   * @throws UnsupportedEncodingException
+   * @throws FileNotFoundException
+   * @throws IOException
+   */
+  private void sampleMethod() throws UnsupportedEncodingException,
+      FileNotFoundException, IOException {
+    Token nt = new Token();
+    Analyzer ca = new SmartChineseAnalyzer(true);
+    Reader sentence = new StringReader(
+        "???å°?°±ä¸????¸»?°è?ä¸ºè?å·±é?å¤§ä»¥???å®????¸ºä¸?ä¸?±¡???äº²ä??·ç??»å?, ??????æ¯??ç§»é????å½±å????å®???¹æ?ä¸?????ä¸ºç?å®¶æ??³ç?ä»?ä¹????????æ¬????????????????????????è¿?¸ª??????å®?ºº?°ä¸­å¹´ç???????ç¡????????ä»?ä¹?????³å????ä»?ä¹?????ä¿¡å?å¤?ºº???ä¸??·æ????????¼ã???ç«?????ä¸?ºº?½è???¸ºä½?????å®?????ç§??å®¶å?å¤§æ?????????å·±é??????ä¹????????ä¹???½å?å¥½ä?ä¹??å®??ä¸??å¸¸å??¾ç??????"
+            + "å¹¸è????ï¼???³æ????å­??ä¼?¸ºè¿?¸ªå¤????????????¤§ï¼???¢æ????ç¾??é«?¸­???ä¸??è¦???½å°±??¸®?©å???????ä»??ä¸?????è¶£ï?ä»???¸®?©ä?ä»????å¤§å????ä¸????????ä¸????è§??å¸??ä¸?ä¸????½¢???å­???°å¥¹????????????ä¸??å¸¸é?è¦??è¿????"
+            + "ç¾??é«?¸­?½æ?ä¸?????ä¸?¡¾???????¥è§¦ä¸?????ç¨?????ç§?????ä¸??§ï??´è¶£å¾???¹é????ç­??å¸??æ¯?¸ªå­???¾å??????è¶£ç?ä¸??????·ç????ä¸????è¦??é«?¹´çº§æ?å¼?å§?? ????¤§??¸ºä»?¹´ä¸??ç®?????ç¨?°±???ç©¶ä?ä¸??ä¸?µ°???è½?»¶é¡¹ç?ï¼??ä»¥å¥¹??????è¿????????è¯?????¥ä»¥????·ç????ä¼???¢ç??µè??¥æ?è¯??????¤§å¸??å®¶ä?ä¸?äº???·ï?????ºä?äº??å¤§å????????¨è?å¥¹è?äº?2ä¸????????ï¼????????å°??ä¸??¨å???"
+            + "?¨æ?è¯??????·ç?ä¸?äº??é¢??"
+            + "ä½??ä¸??æ¬¢å????äººå?ï¼? ä½??æ¬?¿®ä¸?¥¿???ä½??æ¬???²è??¨å?ï¼??????¨å?å¤?·¥ä½??ï¼????¸ª????????äººå?ï¼??????°å????å­?????ä½??æ¬??ä¸?ººå·¥ä????ä½????·±??????ä¿¡å?ï¼???????????å¼ºå?ï¼??????ºæ?ï¼??ä¹????????  ä½??æ¬¢è??±è??¨ç?å·¥ä???????ä½??æ¬¢å?è¯?????è¥¿å?ï¼? ä½??æ¬¢å¸®?©å?äººå?ï¼????????äººå?ï¼?????????¨å?å·¥å???º¤???ï¼?????å½??å¯¼å?ï¼?????ç»??æ´»å????ä½??ä¹???°å???º¤???ï¼?");
+    TokenStream ts = ca.tokenStream("sentence", sentence);
+
+    System.out.println("start: " + (new Date()));
+    long before = System.currentTimeMillis();
+    nt = ts.next(nt);
+    while (nt != null) {
+      System.out.println(nt.term());
+      nt = ts.next(nt);
+    }
+    ts.close();
+    long now = System.currentTimeMillis();
+    System.out.println("time: " + (now - before) / 1000.0 + " s");
+  }
+}

