GitDiffStart: 60b43961da0a16d15b852f2046b32daa5a95dfed | Tue Aug 5 17:17:42 2008 +0000
diff --git a/CHANGES.txt b/CHANGES.txt
index ff232ee..116627d 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -96,6 +96,10 @@ API Changes
 13. LUCENE-1324: Added TokenFilter.reset(). (Shai Erera via Mike
     McCandless)
 
+14. LUCENE-1340: Added Fieldable.omitTf() method to skip indexing term
+    frequency, positions and payloads.  This saves index space, and
+    indexing/searching time.  (Eks Dev via Mike McCandless)
+
 Bug fixes
     
  1. LUCENE-1134: Fixed BooleanQuery.rewrite to only optimize a single 
diff --git a/docs/fileformats.html b/docs/fileformats.html
index 5f860df..8f56687 100644
--- a/docs/fileformats.html
+++ b/docs/fileformats.html
@@ -575,7 +575,7 @@ document.write("Last Published: " + document.lastModified);
 <p>Term Frequency
                         data. For each term in the dictionary, the numbers of all the
                         documents that contain that term, and the frequency of the term in
-                        that document.
+                        that document if omitTf is false.
                     </p>
                 
 </li>
@@ -585,7 +585,9 @@ document.write("Last Published: " + document.lastModified);
                     
 <p>Term Proximity
                         data. For each term in the dictionary, the positions that the term
-                        occurs in each document.
+                        occurs in each document.  Note that this will
+                        not exist if all fields in all documents set
+                        omitTf to true.
                     </p>
                 
 </li>
@@ -1325,10 +1327,11 @@ document.write("Last Published: " + document.lastModified);
 <b>2.4 and above:</b>
                     Segments --&gt; Format, Version, NameCounter, SegCount, &lt;SegName, SegSize, DelGen, DocStoreOffset, [DocStoreSegment, DocStoreIsCompoundFile], HasSingleNormFile, NumField,
                     NormGen<sup>NumField</sup>,
-                    IsCompoundFile&gt;<sup>SegCount</sup>, Checksum
+                    IsCompoundFile, DeletionCount, HasProx&gt;<sup>SegCount</sup>, Checksum
                 </p>
 <p>
-                    Format, NameCounter, SegCount, SegSize, NumField, DocStoreOffset --&gt; Int32
+                    Format, NameCounter, SegCount, SegSize, NumField,
+                    DocStoreOffset, DeletionCount --&gt; Int32
                 </p>
 <p>
                     Version, DelGen, NormGen, Checksum --&gt; Int64
@@ -1337,7 +1340,8 @@ document.write("Last Published: " + document.lastModified);
                     SegName, DocStoreSegment --&gt; String
                 </p>
 <p>
-                    IsCompoundFile, HasSingleNormFile, DocStoreIsCompoundFile --&gt; Int8
+                    IsCompoundFile, HasSingleNormFile,
+                    DocStoreIsCompoundFile, HasProx --&gt; Int8
                 </p>
 <p>
                     Format is -1 as of Lucene 1.4, -3 (SegmentInfos.FORMAT_SINGLE_NORM_FILE) as of Lucene 2.1 and 2.2, -4 (SegmentInfos.FORMAT_SHARED_DOC_STORE) as of Lucene 2.3 and -5 (SegmentInfos.FORMAT_CHECKSUM) as of Lucene 2.4.
@@ -1419,7 +1423,15 @@ document.write("Last Published: " + document.lastModified);
 		    This is used to verify integrity of the file on
 		    opening the index.
 		</p>
-<a name="N104D8"></a><a name="Lock File"></a>
+<p>
+		    DeletionCount records the number of deleted
+		    documents in this segment.
+		</p>
+<p>
+		    HasProx is 1 if any fields in this segment have
+		    omitTf set to false; else, it's 0.
+		</p>
+<a name="N104DE"></a><a name="Lock File"></a>
 <h3 class="boxed">Lock File</h3>
 <p>
                     The write lock, which is stored in the index
@@ -1437,7 +1449,7 @@ document.write("Last Published: " + document.lastModified);
                     Note that prior to version 2.1, Lucene also used a
                     commit lock. This was removed in 2.1.
                 </p>
-<a name="N104E4"></a><a name="Deletable File"></a>
+<a name="N104EA"></a><a name="Deletable File"></a>
 <h3 class="boxed">Deletable File</h3>
 <p>
                     Prior to Lucene 2.1 there was a file "deletable"
@@ -1446,7 +1458,7 @@ document.write("Last Published: " + document.lastModified);
                     the files that are deletable, instead, so no file
                     is written.
                 </p>
-<a name="N104ED"></a><a name="Compound Files"></a>
+<a name="N104F3"></a><a name="Compound Files"></a>
 <h3 class="boxed">Compound Files</h3>
 <p>Starting with Lucene 1.4 the compound file format became default. This
                     is simply a container for all files described in the next section
@@ -1473,14 +1485,14 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N10515"></a><a name="Per-Segment Files"></a>
+<a name="N1051B"></a><a name="Per-Segment Files"></a>
 <h2 class="boxed">Per-Segment Files</h2>
 <div class="section">
 <p>
                 The remaining files are all per-segment, and are
                 thus defined by suffix.
             </p>
-<a name="N1051D"></a><a name="Fields"></a>
+<a name="N10523"></a><a name="Fields"></a>
 <h3 class="boxed">Fields</h3>
 <p>
                     
@@ -1699,7 +1711,7 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N105D8"></a><a name="Term Dictionary"></a>
+<a name="N105DE"></a><a name="Term Dictionary"></a>
 <h3 class="boxed">Term Dictionary</h3>
 <p>
                     The term dictionary is represented as two files:
@@ -1801,7 +1813,9 @@ document.write("Last Published: " + document.lastModified);
                             determines the position of this term's TermPositions within the .prx
                             file. In particular, it is the difference between the position of
                             this term's data in that file and the position of the previous
-                            term's data (or zero, for the first term in the file.
+                            term's data (or zero, for the first term in the file.  For fields
+			    with omitTf true, this will be 0 since
+                            prox information is not stored.
                         </p>
                         
 <p>SkipDelta determines the position of this
@@ -1887,12 +1901,12 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N10658"></a><a name="Frequencies"></a>
+<a name="N1065E"></a><a name="Frequencies"></a>
 <h3 class="boxed">Frequencies</h3>
 <p>
                     The .frq file contains the lists of documents
                     which contain each term, along with the frequency of the term in that
-                    document.
+                    document (if omitTf is false).
                 </p>
 <p>FreqFile (.frq) --&gt;
                     &lt;TermFreqs, SkipData&gt;
@@ -1905,7 +1919,7 @@ document.write("Last Published: " + document.lastModified);
                 
 </p>
 <p>TermFreq --&gt;
-                    DocDelta, Freq?
+                    DocDelta[, Freq?]
                 </p>
 <p>SkipData --&gt;
                     &lt;&lt;SkipLevelLength, SkipLevel&gt;
@@ -1932,21 +1946,31 @@ document.write("Last Published: " + document.lastModified);
 <p>TermFreq
                     entries are ordered by increasing document number.
                 </p>
-<p>DocDelta
-                    determines both the document number and the frequency. In
-                    particular, DocDelta/2 is the difference between this document number
-                    and the previous document number (or zero when this is the first
-                    document in a TermFreqs). When DocDelta is odd, the frequency is
-                    one. When DocDelta is even, the frequency is read as another VInt.
-                </p>
-<p>For
-                    example, the TermFreqs for a term which occurs once in document seven
-                    and three times in document eleven would be the following sequence of
-                    VInts:
-                </p>
-<p>15,
-                    8, 3
-                </p>
+<p>DocDelta: if omitTf is false, this determines both
+                    the document number and the frequency. In
+                    particular, DocDelta/2 is the difference between
+                    this document number and the previous document
+                    number (or zero when this is the first document in
+                    a TermFreqs). When DocDelta is odd, the frequency
+                    is one. When DocDelta is even, the frequency is
+                    read as another VInt.  If omitTf is true, DocDelta
+                    contains the gap (not multiplied by 2) between
+                    document numbers and no frequency information is
+                    stored.
+                </p>
+<p>For example, the TermFreqs for a term which occurs
+                    once in document seven and three times in document
+                    eleven, with omitTf false, would be the following
+                    sequence of VInts:
+                </p>
+<p>15, 8, 3
+                </p>
+<p> If omitTf were true it would be this sequence
+		of VInts instead:
+		  </p>
+<p>
+		   7,4
+                 </p>
 <p>DocSkip records the document number before every
                     SkipInterval
                     <sup>th</sup>
@@ -2005,11 +2029,15 @@ document.write("Last Published: " + document.lastModified);
                    entry in level-1. In the example has entry 15 on level 1 a pointer to entry 15 on level 0 and entry 31 on level 1 a pointer
                    to entry 31 on level 0.                   
                 </p>
-<a name="N106DA"></a><a name="Positions"></a>
+<a name="N106E6"></a><a name="Positions"></a>
 <h3 class="boxed">Positions</h3>
 <p>
                     The .prx file contains the lists of positions that
-                    each term occurs at within documents.
+                    each term occurs at within documents.  Note that
+                    fields with omitTf true do not store
+                    anything into this file, and if all fields in the
+                    index have omitTf true then the .prx file will not
+                    exist.
                 </p>
 <p>ProxFile (.prx) --&gt;
                     &lt;TermPositions&gt;
@@ -2071,7 +2099,7 @@ document.write("Last Published: " + document.lastModified);
                     Payload. If PayloadLength is not stored, then this Payload has the same
                     length as the Payload at the previous position.
                 </p>
-<a name="N10716"></a><a name="Normalization Factors"></a>
+<a name="N10722"></a><a name="Normalization Factors"></a>
 <h3 class="boxed">Normalization Factors</h3>
 <p>
                     
@@ -2175,7 +2203,7 @@ document.write("Last Published: " + document.lastModified);
 <b>2.1 and above:</b>
                     Separate norm files are created (when adequate) for both compound and non compound segments.
                 </p>
-<a name="N1077F"></a><a name="Term Vectors"></a>
+<a name="N1078B"></a><a name="Term Vectors"></a>
 <h3 class="boxed">Term Vectors</h3>
 <p>
 		  Term Vector support is an optional on a field by
@@ -2308,7 +2336,7 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N10815"></a><a name="Deleted Documents"></a>
+<a name="N10821"></a><a name="Deleted Documents"></a>
 <h3 class="boxed">Deleted Documents</h3>
 <p>The .del file is
                     optional, and only exists when a segment contains deletions.
@@ -2380,7 +2408,7 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N10858"></a><a name="Limitations"></a>
+<a name="N10864"></a><a name="Limitations"></a>
 <h2 class="boxed">Limitations</h2>
 <div class="section">
 <p>There
diff --git a/docs/fileformats.pdf b/docs/fileformats.pdf
index 63f066c..51341f8 100644
--- a/docs/fileformats.pdf
+++ b/docs/fileformats.pdf
@@ -11,7 +11,7 @@ Table of contents
     2.3 Segments........................................................................................................................4
     2.4 Document Numbers.......................................................................................................4
    3 Overview............................................................................................................................5
-   4 File Naming....................................................................................................................... 5
+   4 File Naming....................................................................................................................... 6
    5 Primitive Types.................................................................................................................. 6
     5.1 Byte................................................................................................................................6
     5.2 UInt32............................................................................................................................6
@@ -24,7 +24,7 @@ Table of contents
     6.2 Lock File......................................................................................................................10
     6.3 Deletable File...............................................................................................................10
     6.4 Compound Files...........................................................................................................10
-   7 Per-Segment Files............................................................................................................ 10
+   7 Per-Segment Files............................................................................................................ 11
     7.1 Fields........................................................................................................................... 11
 
                    Copyright © 2006 The Apache Software Foundation. All rights reserved.
@@ -32,9 +32,9 @@ Table of contents
 
  7.2 Term Dictionary.......................................................................................................... 12
  7.3 Frequencies..................................................................................................................14
- 7.4 Positions...................................................................................................................... 15
- 7.5 Normalization Factors................................................................................................. 16
- 7.6 Term Vectors............................................................................................................... 17
+ 7.4 Positions...................................................................................................................... 16
+ 7.5 Normalization Factors................................................................................................. 17
+ 7.6 Term Vectors............................................................................................................... 18
  7.7 Deleted Documents..................................................................................................... 19
 8 Limitations....................................................................................................................... 20
 
@@ -145,7 +145,6 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
     and the segment's base value is subtracted. For example two five document segments
     might be combined, so that the first segment has a base value of zero, and the second of
     five. Document three from the second segment would have an external value of eight.
-
 ?? When documents are deleted, gaps are created in the numbering. These are eventually
     removed as the index evolves through merging. Deleted documents are dropped when
     segments are merged. A freshly-merged segment thus has no gaps in its numbering.
@@ -153,7 +152,6 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
 3. Overview
 
 Each segment index maintains the following:
-
 ?? Field names. This contains the set of field names used in the index.
 ?? Stored Field values. This contains, for each document, a list of attribute-value pairs,
 
@@ -161,35 +159,30 @@ Each segment index maintains the following:
     the document, such as its title, url, or an identifier to access a database. The set of stored
     fields are what is returned for each hit when searching. This is keyed by document
     number.
-
 ?? Term dictionary. A dictionary containing all of the terms used in all of the indexed fields
     of all of the documents. The dictionary also contains the number of documents which
     contain the term, and pointers to the term's frequency and proximity data.
-
 ?? Term Frequency data. For each term in the dictionary, the numbers of all the documents
-    that contain that term, and the frequency of the term in that document.
-
+    that contain that term, and the frequency of the term in that document if omitTf is false.
 ?? Term Proximity data. For each term in the dictionary, the positions that the term occurs in
-    each document.
-
+    each document. Note that this will not exist if all fields in all documents set omitTf to
+    true.
 ?? Normalization factors. For each field in each document, a value is stored that is
     multiplied into the score for hits on that field.
-
 ?? Term Vectors. For each field in each document, the term vector (sometimes called
     document vector) may be stored. A term vector consists of term text and term frequency.
     To add Term Vectors to your index see the Field constructors
-
 ?? Deleted documents. An optional file indicating which documents are deleted.
 
 Details on each of these are provided in subsequent sections.
 
-4. File Naming
-
 Page 5
 
         Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
+4. File Naming
+
 All files belonging to a segment have the same name with varying extensions. The extensions
 correspond to the different file formats described below. When using the Compound File
 format (default in 1.4 and greater) these files are collapsed into a single .cfs file (see below
@@ -229,16 +222,16 @@ A variable-length format for positive integers is defined where the high-order b
 indicates whether more bytes remain to be read. The low-order seven bits are appended as
 increasingly more significant bits in the resulting integer value. Thus values from zero to 127
 may be stored in a single byte, values from 128 to 16,383 may be stored in two bytes, and so
-on.
-
-VInt Encoding Example
 
                                                                        Page 6
 
 Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
-Value   First byte                  Second byte                    Third byte
+on.
+VInt Encoding Example
+
+Value                  First byte   Second byte                    Third byte
 
 0 00000000
 
@@ -250,19 +243,19 @@ Value   First byte                  Second byte                    Third byte
 
 127 01111111
 
-128     10000000                    00000001
+128                    10000000     00000001
 
-129     10000001                    00000001
+129                    10000001     00000001
 
-130     10000010                    00000001
+130                    10000010     00000001
 
 ...
 
-16,383  11111111                    01111111
+16,383                 11111111     01111111
 
-16,384  10000000                    10000000                       00000001
+16,384                 10000000     10000000                       00000001
 
-16,385  10000001                    10000000                       00000001
+16,385                 10000001     10000000                       00000001
 
  ...
 
@@ -273,14 +266,14 @@ Lucene writes unicode character sequences as UTF-8 encoded bytes.
 
 5.6. String
 
-Lucene writes strings as UTF-8 encoded bytes. First the length, in bytes, is written as a VInt,
-followed by the bytes.
-
 Page 7
 
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
+             Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
+Lucene writes strings as UTF-8 encoded bytes. First the length, in bytes, is written as a VInt,
+followed by the bytes.
+
 String --> VInt, Chars
 
 6. Per-Index Files
@@ -316,21 +309,22 @@ NumField, NormGenNumField, IsCompoundFile>SegCount
 
 2.4 and above: Segments --> Format, Version, NameCounter, SegCount, <SegName,
 SegSize, DelGen, DocStoreOffset, [DocStoreSegment, DocStoreIsCompoundFile],
-HasSingleNormFile, NumField, NormGenNumField, IsCompoundFile>SegCount,
-Checksum
-
-Format, NameCounter, SegCount, SegSize, NumField, DocStoreOffset --> Int32
-
-Version, DelGen, NormGen, Checksum --> Int64
+HasSingleNormFile, NumField, NormGenNumField, IsCompoundFile, DeletionCount,
+HasProx>SegCount, Checksum
 
-SegName, DocStoreSegment --> String
+Format, NameCounter, SegCount, SegSize, NumField, DocStoreOffset, DeletionCount -->
+Int32
 
                                                                        Page 8
 
 Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
-IsCompoundFile, HasSingleNormFile, DocStoreIsCompoundFile --> Int8
+Version, DelGen, NormGen, Checksum --> Int64
+
+SegName, DocStoreSegment --> String
+
+IsCompoundFile, HasSingleNormFile, DocStoreIsCompoundFile, HasProx --> Int8
 
 Format is -1 as of Lucene 1.4, -3 (SegmentInfos.FORMAT_SINGLE_NORM_FILE) as of
 Lucene 2.1 and 2.2, -4 (SegmentInfos.FORMAT_SHARED_DOC_STORE) as of Lucene 2.3
@@ -371,18 +365,22 @@ stored field values (*.fdt and *.fdx) and term vectors (*.tvf, *.tvd and *.tvx)
 with this segment. Otherwise, DocStoreSegment is the name of the segment that has the
 shared doc store files; DocStoreIsCompoundFile is 1 if that segment is stored in compound
 file format (as a .cfx file); and DocStoreOffset is the starting document in the shared doc
-store files where this segment's documents begin. In this case, this segment does not store its
-own doc store files but instead shares a single set of these files with other segments.
-
-Checksum contains the CRC32 checksum of all bytes in the segments_N file up until the
 
 Page 9
 
         Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
+store files where this segment's documents begin. In this case, this segment does not store its
+own doc store files but instead shares a single set of these files with other segments.
+
+Checksum contains the CRC32 checksum of all bytes in the segments_N file up until the
 checksum. This is used to verify integrity of the file on opening the index.
 
+DeletionCount records the number of deleted documents in this segment.
+
+HasProx is 1 if any fields in this segment have omitTf set to false; else, it's 0.
+
 6.2. Lock File
 
 The write lock, which is stored in the index directory by default, is named "write.lock". If the
@@ -418,6 +416,12 @@ FileData --> raw file data
 The raw file data is the data from the individual files named above.
 
 Starting with Lucene 2.3, doc store files (stored field values and term vectors) can be shared
+
+                                                                       Page 10
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
 in a single set of files for more than one segment. When compound file is enabled, these
 shared files will be added into a single compound file (same format as above) but with the
 extension .cfx.
@@ -426,11 +430,6 @@ extension .cfx.
 
 The remaining files are all per-segment, and are thus defined by suffix.
 
-                                                                       Page 10
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 7.1. Fields
 Field Info
 Field names are stored in the field info file, with suffix .fnm.
@@ -454,17 +453,17 @@ Stored fields are represented by two files:
     accessed. The position of document n 's field data is the Uint64 at n*8 in this file.
 2. The field data, or .fdt file.
     This contains the stored fields of each document, as follows:
-    FieldData (.fdt) --> <DocFieldData> SegSize
-    DocFieldData --> FieldCount, <FieldNum, Bits, Value> FieldCount
-    FieldCount --> VInt
-    FieldNum --> VInt
-    Lucene <= 1.4:
 
 Page 11
 
          Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
+    FieldData (.fdt) --> <DocFieldData> SegSize
+    DocFieldData --> FieldCount, <FieldNum, Bits, Value> FieldCount
+    FieldCount --> VInt
+    FieldNum --> VInt
+    Lucene <= 1.4:
     Bits --> Byte
     Value --> String
     Only the low-order bit of Bits is used. It is one for tokenized fields, and zero for
@@ -490,17 +489,22 @@ The term dictionary is represented as two files:
     TIVersion --> UInt32
     TermCount --> UInt64
     IndexInterval --> UInt32
-    SkipInterval --> UInt32
-    MaxSkipLevels --> UInt32
-    TermInfos --> <TermInfo> TermCount
-    TermInfo --> <Term, DocFreq, FreqDelta, ProxDelta, SkipDelta>
-    Term --> <PrefixLength, Suffix, FieldNum>
 
                                                                        Page 12
 
 Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
+    SkipInterval --> UInt32
+
+    MaxSkipLevels --> UInt32
+
+    TermInfos --> <TermInfo> TermCount
+
+    TermInfo --> <Term, DocFreq, FreqDelta, ProxDelta, SkipDelta>
+
+    Term --> <PrefixLength, Suffix, FieldNum>
+
     Suffix --> String
 
     PrefixLength, DocFreq, FreqDelta, ProxDelta, SkipDelta
@@ -527,7 +531,8 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
 
     ProxDelta determines the position of this term's TermPositions within the .prx file. In
     particular, it is the difference between the position of this term's data in that file and the
-    position of the previous term's data (or zero, for the first term in the file.
+    position of the previous term's data (or zero, for the first term in the file. For fields with
+    omitTf true, this will be 0 since prox information is not stored.
 
     SkipDelta determines the position of this term's SkipData within the .frq file. In
     particular, it is the number of bytes after TermFreqs that the SkipData starts. In other
@@ -536,6 +541,12 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
 2. The term info index, or .tii file.
 
     This contains every IndexInterval th entry from the .tis file, along with its location in the
+
+Page 13
+
+         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
     "tis" file. This is designed to be read entirely into memory and used to provide random
     access to the "tis" file.
 
@@ -547,11 +558,6 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
 
     TIVersion --> UInt32
 
-Page 13
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
     IndexTermCount --> UInt64
 
     IndexInterval --> UInt32
@@ -579,13 +585,18 @@ Page 13
 7.3. Frequencies
 
 The .frq file contains the lists of documents which contain each term, along with the
-frequency of the term in that document.
+frequency of the term in that document (if omitTf is false).
 
 FreqFile (.frq) --> <TermFreqs, SkipData> TermCount
 
 TermFreqs --> <TermFreq> DocFreq
 
-TermFreq --> DocDelta, Freq?
+TermFreq --> DocDelta[, Freq?]
+
+                                                                       Page 14
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
 
 SkipData --> <<SkipLevelLength, SkipLevel> NumSkipLevels-1, SkipLevel> <SkipDatum>
 
@@ -599,23 +610,24 @@ SkipChildLevelPointer --> VLong
 
 TermFreqs are ordered by term (the term is implicit, from the .tis file).
 
-                                                                       Page 14
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 TermFreq entries are ordered by increasing document number.
 
-DocDelta determines both the document number and the frequency. In particular, DocDelta/2
-is the difference between this document number and the previous document number (or zero
-when this is the first document in a TermFreqs). When DocDelta is odd, the frequency is one.
-When DocDelta is even, the frequency is read as another VInt.
+DocDelta: if omitTf is false, this determines both the document number and the frequency. In
+particular, DocDelta/2 is the difference between this document number and the previous
+document number (or zero when this is the first document in a TermFreqs). When DocDelta
+is odd, the frequency is one. When DocDelta is even, the frequency is read as another VInt. If
+omitTf is true, DocDelta contains the gap (not multiplied by 2) between document numbers
+and no frequency information is stored.
 
 For example, the TermFreqs for a term which occurs once in document seven and three times
-in document eleven would be the following sequence of VInts:
+in document eleven, with omitTf false, would be the following sequence of VInts:
 
 15, 8, 3
 
+If omitTf were true it would be this sequence of VInts instead:
+
+7,4
+
 DocSkip records the document number before every SkipInterval th document in TermFreqs.
 If payloads are disabled for the term's field, then DocSkip represents the difference from the
 previous value in the sequence. If payloads are enabled for the term's field, then DocSkip/2
@@ -631,6 +643,12 @@ containing the 15 th and 31 st document numbers in TermFreqs. The first FreqSkip
 number of bytes after the beginning of TermFreqs that the 16 th SkipDatum starts, and the
 second the number of bytes after that that the 32 nd starts. The first ProxSkip names the
 number of bytes after the beginning of Positions that the 16 th SkipDatum starts, and the
+
+Page 15
+
+         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
 second the number of bytes after that that the 32 nd starts.
 
 Lucene 2.2 introduces the notion of skip levels. Each term can have multiple skip levels. The
@@ -647,12 +665,9 @@ entry 15 on level 0 and entry 31 on level 1 a pointer to entry 31 on level 0.
 
 7.4. Positions
 
-Page 15
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
-The .prx file contains the lists of positions that each term occurs at within documents.
+The .prx file contains the lists of positions that each term occurs at within documents. Note
+that fields with omitTf true do not store anything into this file, and if all fields in the index
+have omitTf true then the .prx file will not exist.
 
 ProxFile (.prx) --> <TermPositions> TermCount
 
@@ -678,6 +693,12 @@ position of the current occurrence in the document and the previous occurrence (
 this is the first occurrence in this document). If payloads are enabled for the term's field, then
 PositionDelta/2 is the difference between the current and the previous position. If payloads
 are enabled and PositionDelta is odd, then PayloadLength is stored, indicating the length of
+
+                                                                       Page 16
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
 the payload at the current term position.
 
 For example, the TermPositions for a term which occurs as the fourth term in one document,
@@ -698,11 +719,6 @@ the score for hits on that field:
 
 Norms (.f[0-9]*) --> <Byte> SegSize
 
-                                                                       Page 16
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 2.1 and above: There's a single .nrm file containing all norms:
 
 AllNorms (.nrm) --> NormsHeader,<Norms> NumFieldsWithNorms
@@ -727,35 +743,28 @@ These are converted to an IEEE single float value as follows:
 
 A separate norm file is created when the norm values of an existing segment are modified.
 When field N is modified, a separate norm file .sN is created, to maintain the norm values for
-that field.
 
-Pre-2.1: Separate norm files are created only for compound segments.
+Page 17
 
+         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
+that field.
+Pre-2.1: Separate norm files are created only for compound segments.
 2.1 and above: Separate norm files are created (when adequate) for both compound and non
 compound segments.
 
 7.6. Term Vectors
-
 Term Vector support is an optional on a field by field basis. It consists of 4 files.
 1. The Document Index or .tvx file.
 
     For each document, this stores the offset into the document data (.tvd) and field data (.tvf)
     files.
-
     DocumentIndex (.tvx) --> TVXVersion<DocumentPosition,FieldPosition> NumDocs
-
     TVXVersion --> Int (3 (TermVectorsReader.FORMAT_VERSION2) for Lucene 2.4)
-
     DocumentPosition --> UInt64 (offset in the .tvd file)
-
-Page 17
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
     FieldPosition --> UInt64 (offset in the .tvf file)
 2. The Document or .tvd file.
-
     This contains, for each document, the number of fields, a list of the fields with term
     vector info and finally a list of pointers to the field information in the .tvf (Term Vector
     Fields) file.
@@ -769,23 +778,34 @@ Page 17
     The .tvd file is used to map out the fields that have term vectors stored and where the
     field information is in the .tvf file.
 3. The Field or .tvf file.
+
+                                                                       Page 18
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
     This file contains, for each field that has a term vector stored, a list of the terms, their
     frequencies and, optionally, position and offest information.
+
     Field (.tvf) --> TVFVersion<NumTerms, Position/Offset, TermFreqs> NumFields
+
     TVFVersion --> Int (3 (TermVectorsReader.FORMAT_VERSION2) for Lucene 2.4)
+
     NumTerms --> VInt
+
     Position/Offset --> Byte
+
     TermFreqs --> <TermText, TermFreq, Positions?, Offsets?> NumTerms
+
     TermText --> <PrefixLength, Suffix>
+
     PrefixLength --> VInt
+
     Suffix --> String
-    TermFreq --> VInt
-    Positions --> <VInt>TermFreq
 
-                                                                       Page 18
+    TermFreq --> VInt
 
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
+    Positions --> <VInt>TermFreq
 
     Offsets --> <VInt, VInt>TermFreq
 
@@ -812,34 +832,25 @@ Although per-segment, this file is maintained exterior to compound segment files
 Pre-2.1: Deletions (.del) --> ByteCount,BitCount,Bits
 
 2.1 and above: Deletions (.del) --> [Format],ByteCount,BitCount, Bits | DGaps (depending
-on Format)
 
-Format,ByteSize,BitCount --> Uint32
+Page 19
 
-Bits --> <Byte> ByteCount
+         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                                                                                                Apache Lucene - Index File Formats
 
+on Format)
+Format,ByteSize,BitCount --> Uint32
+Bits --> <Byte> ByteCount
 DGaps --> <DGap,NonzeroByte> NonzeroBytesCount
-
 DGap --> VInt
-
 NonzeroByte --> Byte
-
 Format is Optional. -1 indicates DGaps. Non-negative value indicates Bits, and that Format is
 excluded.
-
 ByteCount indicates the number of bytes in Bits. It is typically (SegSize/8)+1.
-
 BitCount indicates the number of bits that are currently set in Bits.
-
 Bits contains one bit for each document indexed. When the bit corresponding to a document
 number is set, that document is marked as deleted. Bit ordering is from least to most
 significant. Thus, if Bits contains two bytes, 0x00 and 0x02, then document 9 is marked as
-
-Page 19
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-                                                                                                                Apache Lucene - Index File Formats
-
 deleted.
 DGaps represents sparse bit-vectors more efficiently than Bits. It is made of DGaps on
 indexes of nonzero bytes in Bits, and the nonzero bytes themselves. The number of nonzero
diff --git a/src/java/org/apache/lucene/document/AbstractField.java b/src/java/org/apache/lucene/document/AbstractField.java
index 77e158a..7eac52b 100755
--- a/src/java/org/apache/lucene/document/AbstractField.java
+++ b/src/java/org/apache/lucene/document/AbstractField.java
@@ -33,6 +33,7 @@ public abstract class AbstractField implements Fieldable {
   protected boolean isBinary = false;
   protected boolean isCompressed = false;
   protected boolean lazy = false;
+  protected boolean omitTf = false;
   protected float boost = 1.0f;
   // the one and only data object for all different kind of field values
   protected Object fieldsData = null;
@@ -203,6 +204,9 @@ public abstract class AbstractField implements Fieldable {
   /** True if norms are omitted for this indexed field */
   public boolean getOmitNorms() { return omitNorms; }
 
+  /** True if tf is omitted for this indexed field */
+  public boolean getOmitTf() { return omitTf; }
+  
   /** Expert:
    *
    * If set, omit normalization factors associated with this indexed field.
@@ -210,6 +214,12 @@ public abstract class AbstractField implements Fieldable {
    */
   public void setOmitNorms(boolean omitNorms) { this.omitNorms=omitNorms; }
 
+  /** Expert:
+  *
+  * If set, omit tf from postings of this indexed field.
+  */
+  public void setOmitTf(boolean omitTf) { this.omitTf=omitTf; }
+ 
   public boolean isLazy() {
     return lazy;
   }
@@ -257,6 +267,9 @@ public abstract class AbstractField implements Fieldable {
     if (omitNorms) {
       result.append(",omitNorms");
     }
+    if (omitTf) {
+      result.append(",omitTf");
+    }
     if (lazy){
       result.append(",lazy");
     }
diff --git a/src/java/org/apache/lucene/document/Fieldable.java b/src/java/org/apache/lucene/document/Fieldable.java
index 54a5165..919ca30 100755
--- a/src/java/org/apache/lucene/document/Fieldable.java
+++ b/src/java/org/apache/lucene/document/Fieldable.java
@@ -139,6 +139,15 @@ public interface Fieldable extends Serializable {
    */
   void setOmitNorms(boolean omitNorms);
 
+  /** Expert:
+   *
+   * If set, omit term freq, positions and payloads from postings for this field.
+   */
+  void setOmitTf(boolean omitTf);
+  
+  /** True if tf is omitted for this indexed field */
+  boolean getOmitTf();
+
   /**
    * Indicates whether a Field is Lazy or not.  The semantics of Lazy loading are such that if a Field is lazily loaded, retrieving
    * it's values via {@link #stringValue()} or {@link #binaryValue()} is only valid as long as the {@link org.apache.lucene.index.IndexReader} that
diff --git a/src/java/org/apache/lucene/index/CheckIndex.java b/src/java/org/apache/lucene/index/CheckIndex.java
index 4dea08f..65f3117 100644
--- a/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/src/java/org/apache/lucene/index/CheckIndex.java
@@ -114,13 +114,12 @@ public class CheckIndex {
     else if (format == SegmentInfos.FORMAT_SHARED_DOC_STORE)
       sFormat = "FORMAT_SHARED_DOC_STORE [Lucene 2.3]";
     else {
-      // LUCENE-1255: All versions before 2.3.2/2.4 were
-      // able to create position=-1 when the very first
-      // Token has positionIncrement 0
       if (format == SegmentInfos.FORMAT_CHECKSUM)
         sFormat = "FORMAT_CHECKSUM [Lucene 2.4]";
       else if (format == SegmentInfos.FORMAT_DEL_COUNT)
-          sFormat = "FORMAT_DEL_COUNT [Lucene 2.4]";
+        sFormat = "FORMAT_DEL_COUNT [Lucene 2.4]";
+      else if (format == SegmentInfos.FORMAT_HAS_PROX)
+        sFormat = "FORMAT_HAS_PROX [Lucene 2.4]";
       else if (format < SegmentInfos.CURRENT_FORMAT) {
         sFormat = "int=" + format + " [newer version of Lucene than this tool]";
         skip = true;
@@ -161,6 +160,7 @@ public class CheckIndex {
 
       try {
         out.println("    compound=" + info.getUseCompoundFile());
+        out.println("    hasProx=" + info.getHasProx());
         out.println("    numFiles=" + info.files().size());
         out.println("    size (MB)=" + nf.format(info.sizeInBytes()/(1024.*1024.)));
         final int docStoreOffset = info.getDocStoreOffset();
@@ -224,7 +224,7 @@ public class CheckIndex {
             final int doc = termPositions.doc();
             final int freq = termPositions.freq();
             if (doc <= lastDoc)
-              throw new RuntimeException("term " + term + ": doc " + doc + " < lastDoc " + lastDoc);
+              throw new RuntimeException("term " + term + ": doc " + doc + " <= lastDoc " + lastDoc);
             lastDoc = doc;
             if (freq <= 0)
               throw new RuntimeException("term " + term + ": doc " + doc + ": freq " + freq + " is out of bounds");
diff --git a/src/java/org/apache/lucene/index/DefaultSkipListWriter.java b/src/java/org/apache/lucene/index/DefaultSkipListWriter.java
index 73799ad..27d7c39 100644
--- a/src/java/org/apache/lucene/index/DefaultSkipListWriter.java
+++ b/src/java/org/apache/lucene/index/DefaultSkipListWriter.java
@@ -62,7 +62,8 @@ class DefaultSkipListWriter extends MultiLevelSkipListWriter {
     this.curStorePayloads = storePayloads;
     this.curPayloadLength = payloadLength;
     this.curFreqPointer = freqOutput.getFilePointer();
-    this.curProxPointer = proxOutput.getFilePointer();
+    if (proxOutput != null)
+      this.curProxPointer = proxOutput.getFilePointer();
   }
   
   protected void resetSkip() {
@@ -70,7 +71,8 @@ class DefaultSkipListWriter extends MultiLevelSkipListWriter {
     Arrays.fill(lastSkipDoc, 0);
     Arrays.fill(lastSkipPayloadLength, -1);  // we don't have to write the first length in the skip list
     Arrays.fill(lastSkipFreqPointer, freqOutput.getFilePointer());
-    Arrays.fill(lastSkipProxPointer, proxOutput.getFilePointer());
+    if (proxOutput != null)
+      Arrays.fill(lastSkipProxPointer, proxOutput.getFilePointer());
   }
   
   protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
diff --git a/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java b/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
index b160b48..e3e02d0 100644
--- a/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
+++ b/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
@@ -183,7 +183,7 @@ final class DocFieldProcessorPerThread extends DocConsumerPerThread {
         // easily add it
         FieldInfo fi = fieldInfos.add(fieldName, field.isIndexed(), field.isTermVectorStored(),
                                       field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),
-                                      field.getOmitNorms(), false);
+                                      field.getOmitNorms(), false, field.getOmitTf());
 
         fp = new DocFieldProcessorPerField(this, fi);
         fp.next = fieldHash[hashPos];
@@ -195,7 +195,7 @@ final class DocFieldProcessorPerThread extends DocConsumerPerThread {
       } else
         fp.fieldInfo.update(field.isIndexed(), field.isTermVectorStored(),
                             field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),
-                            field.getOmitNorms(), false);
+                            field.getOmitNorms(), false, field.getOmitTf());
 
       if (thisFieldGen != fp.lastGen) {
 
diff --git a/src/java/org/apache/lucene/index/DocumentsWriter.java b/src/java/org/apache/lucene/index/DocumentsWriter.java
index c444109..83af61f 100644
--- a/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -132,6 +132,8 @@ final class DocumentsWriter {
   boolean bufferIsFull;                   // True when it's time to write segment
   private boolean aborting;               // True if an abort is pending
 
+  private DocFieldProcessor docFieldProcessor;
+
   PrintStream infoStream;
   int maxFieldLength = IndexWriter.DEFAULT_MAX_FIELD_LENGTH;
   Similarity similarity;
@@ -261,7 +263,13 @@ final class DocumentsWriter {
     final DocInverter docInverter = new DocInverter(termsHash, normsWriter);
     final StoredFieldsWriter fieldsWriter = new StoredFieldsWriter(this);
     final DocFieldConsumers docFieldConsumers = new DocFieldConsumers(docInverter, fieldsWriter);
-    consumer = new DocFieldProcessor(this, docFieldConsumers);
+    consumer = docFieldProcessor = new DocFieldProcessor(this, docFieldConsumers);
+  }
+
+  /** Returns true if any of the fields in the current
+   *  buffered docs have omitTf==false */
+  boolean hasProx() {
+    return docFieldProcessor.fieldInfos.hasProx();
   }
 
   /** If non-null, various details of indexing are printed
diff --git a/src/java/org/apache/lucene/index/FieldInfo.java b/src/java/org/apache/lucene/index/FieldInfo.java
index 38ec654..0885118 100644
--- a/src/java/org/apache/lucene/index/FieldInfo.java
+++ b/src/java/org/apache/lucene/index/FieldInfo.java
@@ -27,13 +27,14 @@ final class FieldInfo {
   boolean storeOffsetWithTermVector;
   boolean storePositionWithTermVector;
 
-  boolean omitNorms; // omit norms associated with indexed fields
+  boolean omitNorms; // omit norms associated with indexed fields  
+  boolean omitTf; // omit tf
   
   boolean storePayloads; // whether this field stores payloads together with term positions
 
   FieldInfo(String na, boolean tk, int nu, boolean storeTermVector, 
             boolean storePositionWithTermVector,  boolean storeOffsetWithTermVector, 
-            boolean omitNorms, boolean storePayloads) {
+            boolean omitNorms, boolean storePayloads, boolean omitTf) {
     name = na;
     isIndexed = tk;
     number = nu;
@@ -42,15 +43,16 @@ final class FieldInfo {
     this.storePositionWithTermVector = storePositionWithTermVector;
     this.omitNorms = omitNorms;
     this.storePayloads = storePayloads;
+    this.omitTf = omitTf;
   }
 
   public Object clone() {
     return new FieldInfo(name, isIndexed, number, storeTermVector, storePositionWithTermVector,
-                         storeOffsetWithTermVector, omitNorms, storePayloads);
+                         storeOffsetWithTermVector, omitNorms, storePayloads, omitTf);
   }
 
   void update(boolean isIndexed, boolean storeTermVector, boolean storePositionWithTermVector, 
-              boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads) {
+              boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, boolean omitTf) {
     if (this.isIndexed != isIndexed) {
       this.isIndexed = true;                      // once indexed, always index
     }
@@ -66,6 +68,9 @@ final class FieldInfo {
     if (this.omitNorms != omitNorms) {
       this.omitNorms = false;                // once norms are stored, always store
     }
+    if (this.omitTf != omitTf) {
+      this.omitTf = true;                // if one require omitTf at least once, it remains off for life
+    }
     if (this.storePayloads != storePayloads) {
       this.storePayloads = true;
     }
@@ -87,6 +92,9 @@ final class FieldInfo {
     if (omitNorms != other.omitNorms) {
       omitNorms = false;                // once norms are stored, always store
     }
+    if (this.omitTf != omitTf) {
+      this.omitTf = true;                // if one require omitTf at least once, it remains off for life
+    }
     if (storePayloads != other.storePayloads) {
       storePayloads = true;
     }
diff --git a/src/java/org/apache/lucene/index/FieldInfos.java b/src/java/org/apache/lucene/index/FieldInfos.java
index 63270e2..234db7c 100644
--- a/src/java/org/apache/lucene/index/FieldInfos.java
+++ b/src/java/org/apache/lucene/index/FieldInfos.java
@@ -40,6 +40,7 @@ final class FieldInfos {
   static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x8;
   static final byte OMIT_NORMS = 0x10;
   static final byte STORE_PAYLOADS = 0x20;
+  static final byte OMIT_TF = 0x40;
   
   private ArrayList byNumber = new ArrayList();
   private HashMap byName = new HashMap();
@@ -86,6 +87,15 @@ final class FieldInfos {
               field.isStoreOffsetWithTermVector(), field.getOmitNorms());
     }
   }
+
+  /** Returns true if any fields do not omitTf */
+  boolean hasProx() {
+    final int numFields = byNumber.size();
+    for(int i=0;i<numFields;i++)
+      if (!fieldInfo(i).omitTf)
+        return true;
+    return false;
+  }
   
   /**
    * Add fields that are indexed. Whether they have termvectors has to be specified.
@@ -172,7 +182,7 @@ final class FieldInfos {
   synchronized public void add(String name, boolean isIndexed, boolean storeTermVector,
                   boolean storePositionWithTermVector, boolean storeOffsetWithTermVector, boolean omitNorms) {
     add(name, isIndexed, storeTermVector, storePositionWithTermVector,
-        storeOffsetWithTermVector, omitNorms, false);
+        storeOffsetWithTermVector, omitNorms, false, false);
   }
   
   /** If the field is not yet known, adds it. If it is known, checks to make
@@ -187,15 +197,16 @@ final class FieldInfos {
    * @param storeOffsetWithTermVector true if the term vector with offsets should be stored
    * @param omitNorms true if the norms for the indexed field should be omitted
    * @param storePayloads true if payloads should be stored for this field
+   * @param omitTf true if term freqs should be omitted for this field
    */
   synchronized public FieldInfo add(String name, boolean isIndexed, boolean storeTermVector,
                        boolean storePositionWithTermVector, boolean storeOffsetWithTermVector,
-                       boolean omitNorms, boolean storePayloads) {
+                       boolean omitNorms, boolean storePayloads, boolean omitTf) {
     FieldInfo fi = fieldInfo(name);
     if (fi == null) {
-      return addInternal(name, isIndexed, storeTermVector, storePositionWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads);
+      return addInternal(name, isIndexed, storeTermVector, storePositionWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads, omitTf);
     } else {
-      fi.update(isIndexed, storeTermVector, storePositionWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads);
+      fi.update(isIndexed, storeTermVector, storePositionWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads, omitTf);
     }
     return fi;
   }
@@ -205,7 +216,7 @@ final class FieldInfos {
     if (fi == null) {
       return addInternal(fieldInfo.name, fieldInfo.isIndexed, fieldInfo.storeTermVector,
                          fieldInfo.storePositionWithTermVector, fieldInfo.storeOffsetWithTermVector,
-                         fieldInfo.omitNorms, fieldInfo.storePayloads);
+                         fieldInfo.omitNorms, fieldInfo.storePayloads, fieldInfo.omitTf);
     } else {
       fi.update(fieldInfo);
     }
@@ -214,10 +225,10 @@ final class FieldInfos {
 
   private FieldInfo addInternal(String name, boolean isIndexed,
                                 boolean storeTermVector, boolean storePositionWithTermVector, 
-                                boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads) {
+                                boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, boolean omitTf) {
     FieldInfo fi =
       new FieldInfo(name, isIndexed, byNumber.size(), storeTermVector, storePositionWithTermVector,
-              storeOffsetWithTermVector, omitNorms, storePayloads);
+              storeOffsetWithTermVector, omitNorms, storePayloads, omitTf);
     byNumber.add(fi);
     byName.put(name, fi);
     return fi;
@@ -289,6 +300,8 @@ final class FieldInfos {
       if (fi.storeOffsetWithTermVector) bits |= STORE_OFFSET_WITH_TERMVECTOR;
       if (fi.omitNorms) bits |= OMIT_NORMS;
       if (fi.storePayloads) bits |= STORE_PAYLOADS;
+      if (fi.omitTf) bits |= OMIT_TF;
+      
       output.writeString(fi.name);
       output.writeByte(bits);
     }
@@ -305,8 +318,9 @@ final class FieldInfos {
       boolean storeOffsetWithTermVector = (bits & STORE_OFFSET_WITH_TERMVECTOR) != 0;
       boolean omitNorms = (bits & OMIT_NORMS) != 0;
       boolean storePayloads = (bits & STORE_PAYLOADS) != 0;
+      boolean omitTf = (bits & OMIT_TF) != 0;
       
-      addInternal(name, isIndexed, storeTermVector, storePositionsWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads);
+      addInternal(name, isIndexed, storeTermVector, storePositionsWithTermVector, storeOffsetWithTermVector, omitNorms, storePayloads, omitTf);
     }    
   }
 
diff --git a/src/java/org/apache/lucene/index/FreqProxFieldMergeState.java b/src/java/org/apache/lucene/index/FreqProxFieldMergeState.java
index ddfdff8..8e8b2ee 100644
--- a/src/java/org/apache/lucene/index/FreqProxFieldMergeState.java
+++ b/src/java/org/apache/lucene/index/FreqProxFieldMergeState.java
@@ -63,7 +63,8 @@ final class FreqProxFieldMergeState {
     textOffset = p.textStart & DocumentsWriter.CHAR_BLOCK_MASK;
 
     field.termsHashPerField.initReader(freq, p, 0);
-    field.termsHashPerField.initReader(prox, p, 1);
+    if (!field.fieldInfo.omitTf)
+      field.termsHashPerField.initReader(prox, p, 1);
 
     // Should always be true
     boolean result = nextDoc();
@@ -77,20 +78,27 @@ final class FreqProxFieldMergeState {
       if (p.lastDocCode != -1) {
         // Return last doc
         docID = p.lastDocID;
-        termFreq = p.docFreq;
+        if (!field.omitTf)
+          termFreq = p.docFreq;
         p.lastDocCode = -1;
         return true;
-      } else 
+      } else
         // EOF
         return false;
     }
 
     final int code = freq.readVInt();
-    docID += code >>> 1;
-    if ((code & 1) != 0)
-      termFreq = 1;
-    else
-      termFreq = freq.readVInt();
+    if (field.omitTf)
+      docID += code;
+    else {
+      docID += code >>> 1;
+      if ((code & 1) != 0)
+        termFreq = 1;
+      else
+        termFreq = freq.readVInt();
+    }
+
+    assert docID != p.lastDocID;
 
     return true;
   }
diff --git a/src/java/org/apache/lucene/index/FreqProxTermsWriter.java b/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
index cde789b..f17bdaa 100644
--- a/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
+++ b/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
@@ -31,10 +31,6 @@ import java.util.Iterator;
 
 final class FreqProxTermsWriter extends TermsHashConsumer {
 
-  FreqProxTermsWriter() {
-    streamCount = 2;
-  }
-
   public TermsHashConsumerPerThread addThread(TermsHashPerThread perThread) {
     return new FreqProxTermsWriterPerThread(perThread);
   }
@@ -102,7 +98,12 @@ final class FreqProxTermsWriter extends TermsHashConsumer {
                                                          state.docWriter.writer.getTermIndexInterval());
 
     final IndexOutput freqOut = state.directory.createOutput(state.segmentFileName(IndexFileNames.FREQ_EXTENSION));
-    final IndexOutput proxOut = state.directory.createOutput(state.segmentFileName(IndexFileNames.PROX_EXTENSION));
+    final IndexOutput proxOut;
+
+    if (fieldInfos.hasProx())
+      proxOut = state.directory.createOutput(state.segmentFileName(IndexFileNames.PROX_EXTENSION));
+    else
+      proxOut = null;
 
     final DefaultSkipListWriter skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,
                                                                            termsOut.maxSkipLevels,
@@ -135,6 +136,7 @@ final class FreqProxTermsWriter extends TermsHashConsumer {
         int numPostings = perField.numPostings;
         perField.reset();
         perField.shrinkHash(numPostings);
+        fields[i].reset();
       }
 
       start = end;
@@ -148,13 +150,15 @@ final class FreqProxTermsWriter extends TermsHashConsumer {
     }
 
     freqOut.close();
-    proxOut.close();
+    if (proxOut != null) {
+      state.flushedFiles.add(state.segmentFileName(IndexFileNames.PROX_EXTENSION));
+      proxOut.close();
+    }
     termsOut.close();
     
     // Record all files we have flushed
     state.flushedFiles.add(state.segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));
     state.flushedFiles.add(state.segmentFileName(IndexFileNames.FREQ_EXTENSION));
-    state.flushedFiles.add(state.segmentFileName(IndexFileNames.PROX_EXTENSION));
     state.flushedFiles.add(state.segmentFileName(IndexFileNames.TERMS_EXTENSION));
     state.flushedFiles.add(state.segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));
   }
@@ -205,8 +209,12 @@ final class FreqProxTermsWriter extends TermsHashConsumer {
     }
 
     final int skipInterval = termsOut.skipInterval;
-    final boolean currentFieldStorePayloads = fields[0].fieldInfo.storePayloads;
+    final boolean currentFieldOmitTf = fields[0].fieldInfo.omitTf;
 
+    // If current field omits tf then it cannot store
+    // payloads.  We silently drop the payloads in this case:
+    final boolean currentFieldStorePayloads = currentFieldOmitTf ? false : fields[0].fieldInfo.storePayloads;
+  
     FreqProxFieldMergeState[] termStates = new FreqProxFieldMergeState[numFields];
 
     while(numFields > 0) {
@@ -235,8 +243,12 @@ final class FreqProxTermsWriter extends TermsHashConsumer {
       final char[] text = termStates[0].text;
       final int start = termStates[0].textOffset;
 
-      long freqPointer = freqOut.getFilePointer();
-      long proxPointer = proxOut.getFilePointer();
+      final long freqPointer = freqOut.getFilePointer();
+      final long proxPointer;
+      if (proxOut != null)
+        proxPointer = proxOut.getFilePointer();
+      else
+        proxPointer = 0;
 
       skipListWriter.resetSkip();
 
@@ -261,45 +273,53 @@ final class FreqProxTermsWriter extends TermsHashConsumer {
         assert doc < flushState.numDocsInRAM;
         assert doc > lastDoc || df == 1;
 
-        final int newDocCode = (doc-lastDoc)<<1;
-
-        lastDoc = doc;
-
         final ByteSliceReader prox = minState.prox;
 
         // Carefully copy over the prox + payload info,
         // changing the format to match Lucene's segment
         // format.
-        for(int j=0;j<termDocFreq;j++) {
-          final int code = prox.readVInt();
-          if (currentFieldStorePayloads) {
-            final int payloadLength;
-            if ((code & 1) != 0) {
-              // This position has a payload
-              payloadLength = prox.readVInt();
-            } else
-              payloadLength = 0;
-            if (payloadLength != lastPayloadLength) {
-              proxOut.writeVInt(code|1);
-              proxOut.writeVInt(payloadLength);
-              lastPayloadLength = payloadLength;
-            } else
-              proxOut.writeVInt(code & (~1));
-            if (payloadLength > 0)
-              copyBytes(prox, proxOut, payloadLength);
-          } else {
-            assert 0 == (code & 1);
-            proxOut.writeVInt(code>>1);
+        if (!currentFieldOmitTf) {
+          // omitTf == false so we do write positions & payload          
+          assert proxOut != null;
+          for(int j=0;j<termDocFreq;j++) {
+            final int code = prox.readVInt();
+            if (currentFieldStorePayloads) {
+              final int payloadLength;
+              if ((code & 1) != 0) {
+                // This position has a payload
+                payloadLength = prox.readVInt();
+              } else
+                payloadLength = 0;
+              if (payloadLength != lastPayloadLength) {
+                proxOut.writeVInt(code|1);
+                proxOut.writeVInt(payloadLength);
+                lastPayloadLength = payloadLength;
+              } else
+                proxOut.writeVInt(code & (~1));
+              if (payloadLength > 0)
+                copyBytes(prox, proxOut, payloadLength);
+            } else {
+              assert 0 == (code & 1);
+              proxOut.writeVInt(code>>1);
+            }
+          } //End for
+          
+          final int newDocCode = (doc-lastDoc)<<1;
+
+          if (1 == termDocFreq) {
+            freqOut.writeVInt(newDocCode|1);
+           } else {
+            freqOut.writeVInt(newDocCode);
+            freqOut.writeVInt(termDocFreq);
           }
-        }
-
-        if (1 == termDocFreq) {
-          freqOut.writeVInt(newDocCode|1);
         } else {
-          freqOut.writeVInt(newDocCode);
-          freqOut.writeVInt(termDocFreq);
+          // omitTf==true: we store only the docs, without
+          // term freq, positions, payloads
+          freqOut.writeVInt(doc-lastDoc);
         }
 
+        lastDoc = doc;
+
         if (!minState.nextDoc()) {
 
           // Remove from termStates
diff --git a/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java b/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java
index 60714f1..2526194 100644
--- a/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java
+++ b/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java
@@ -31,6 +31,7 @@ final class FreqProxTermsWriterPerField extends TermsHashConsumerPerField implem
   final FieldInfo fieldInfo;
   final DocumentsWriter.DocState docState;
   final DocInverter.FieldInvertState fieldState;
+  boolean omitTf;
 
   public FreqProxTermsWriterPerField(TermsHashPerField termsHashPerField, FreqProxTermsWriterPerThread perThread, FieldInfo fieldInfo) {
     this.termsHashPerField = termsHashPerField;
@@ -38,11 +39,18 @@ final class FreqProxTermsWriterPerField extends TermsHashConsumerPerField implem
     this.fieldInfo = fieldInfo;
     docState = termsHashPerField.docState;
     fieldState = termsHashPerField.fieldState;
+    omitTf = fieldInfo.omitTf;
+  }
+
+  int getStreamCount() {
+    if (fieldInfo.omitTf)
+      return 1;
+    else
+      return 2;
   }
 
   void finish() {}
 
-  //boolean doNext;
   boolean hasPayloads;
 
   void skippingLongTerm(Token t) throws IOException {}
@@ -52,6 +60,12 @@ final class FreqProxTermsWriterPerField extends TermsHashConsumerPerField implem
     return fieldInfo.name.compareTo(other.fieldInfo.name);
   }
 
+  void reset() {
+    // Record, up front, whether our in-RAM format will be
+    // with or without term freqs:
+    omitTf = fieldInfo.omitTf;
+  }
+
   boolean start(Fieldable[] fields, int count) {
     for(int i=0;i<count;i++)
       if (fields[i].isIndexed())
@@ -76,10 +90,14 @@ final class FreqProxTermsWriterPerField extends TermsHashConsumerPerField implem
     // flush
     assert docState.testPoint("FreqProxTermsWriterPerField.newTerm start");
     FreqProxTermsWriter.PostingList p = (FreqProxTermsWriter.PostingList) p0;
-    p.lastDocCode = docState.docID << 1;
     p.lastDocID = docState.docID;
-    p.docFreq = 1;
-    writeProx(t, p, fieldState.position);
+    if (omitTf) {
+      p.lastDocCode = docState.docID;
+    } else {
+      p.lastDocCode = docState.docID << 1;
+      p.docFreq = 1;
+      writeProx(t, p, fieldState.position);
+    }
   }
 
   final void addTerm(Token t, RawPostingList p0) {
@@ -88,27 +106,37 @@ final class FreqProxTermsWriterPerField extends TermsHashConsumerPerField implem
 
     FreqProxTermsWriter.PostingList p = (FreqProxTermsWriter.PostingList) p0;
 
-    assert p.docFreq > 0;
-
-    if (docState.docID != p.lastDocID) {
-      // Term not yet seen in the current doc but previously
-      // seen in other doc(s) since the last flush
+    assert omitTf || p.docFreq > 0;
 
-      // Now that we know doc freq for previous doc,
-      // write it & lastDocCode
-      if (1 == p.docFreq)
-        termsHashPerField.writeVInt(0, p.lastDocCode|1);
-      else {
+    if (omitTf) {
+      if (docState.docID != p.lastDocID) {
+        assert docState.docID > p.lastDocID;
         termsHashPerField.writeVInt(0, p.lastDocCode);
-        termsHashPerField.writeVInt(0, p.docFreq);
+        p.lastDocCode = docState.docID - p.lastDocID;
+        p.lastDocID = docState.docID;
       }
-      p.docFreq = 1;
-      p.lastDocCode = (docState.docID - p.lastDocID) << 1;
-      p.lastDocID = docState.docID;
-      writeProx(t, p, fieldState.position);
     } else {
-      p.docFreq++;
-      writeProx(t, p, fieldState.position-p.lastPosition);
+      if (docState.docID != p.lastDocID) {
+        assert docState.docID > p.lastDocID;
+        // Term not yet seen in the current doc but previously
+        // seen in other doc(s) since the last flush
+
+        // Now that we know doc freq for previous doc,
+        // write it & lastDocCode
+        if (1 == p.docFreq)
+          termsHashPerField.writeVInt(0, p.lastDocCode|1);
+        else {
+          termsHashPerField.writeVInt(0, p.lastDocCode);
+          termsHashPerField.writeVInt(0, p.docFreq);
+        }
+        p.docFreq = 1;
+        p.lastDocCode = (docState.docID - p.lastDocID) << 1;
+        p.lastDocID = docState.docID;
+        writeProx(t, p, fieldState.position);
+      } else {
+        p.docFreq++;
+        writeProx(t, p, fieldState.position-p.lastPosition);
+      }
     }
   }
 
diff --git a/src/java/org/apache/lucene/index/IndexReader.java b/src/java/org/apache/lucene/index/IndexReader.java
index 1b4978d..3844089 100644
--- a/src/java/org/apache/lucene/index/IndexReader.java
+++ b/src/java/org/apache/lucene/index/IndexReader.java
@@ -75,6 +75,8 @@ public abstract class IndexReader {
     public static final FieldOption INDEXED = new FieldOption ("INDEXED");
     /** All fields that store payloads */
     public static final FieldOption STORES_PAYLOADS = new FieldOption ("STORES_PAYLOADS");
+    /** All fields that omit tf */
+    public static final FieldOption OMIT_TF = new FieldOption ("OMIT_TF");
     /** All fields which are not indexed */
     public static final FieldOption UNINDEXED = new FieldOption ("UNINDEXED");
     /** All fields which are indexed with termvectors enabled */
diff --git a/src/java/org/apache/lucene/index/IndexWriter.java b/src/java/org/apache/lucene/index/IndexWriter.java
index e79115e..4c71fdb 100644
--- a/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/src/java/org/apache/lucene/index/IndexWriter.java
@@ -3069,7 +3069,7 @@ public class IndexWriter {
           synchronized(this) {
             segmentInfos.setSize(0);                      // pop old infos & add new
             info = new SegmentInfo(mergedName, docCount, directory, false, true,
-                                   -1, null, false);
+                                   -1, null, false, merger.hasProx());
             segmentInfos.addElement(info);
           }
 
@@ -3377,7 +3377,8 @@ public class IndexWriter {
                                      flushedDocCount,
                                      directory, false, true,
                                      docStoreOffset, docStoreSegment,
-                                     docStoreIsCompoundFile);
+                                     docStoreIsCompoundFile,    
+                                     docWriter.hasProx());
       }
 
       docWriter.pushDeletes();
@@ -3615,6 +3616,8 @@ public class IndexWriter {
       }
     }
 
+    merge.info.setHasProx(merger.hasProx());
+
     segmentInfos.subList(start, start + merge.segments.size()).clear();
     segmentInfos.add(start, merge.info);
 
@@ -3905,7 +3908,8 @@ public class IndexWriter {
                                  directory, false, true,
                                  docStoreOffset,
                                  docStoreSegment,
-                                 docStoreIsCompoundFile);
+                                 docStoreIsCompoundFile,
+                                 false);
 
     // Also enroll the merged segment into mergingSegments;
     // this prevents it from getting selected for a merge
diff --git a/src/java/org/apache/lucene/index/SegmentInfo.java b/src/java/org/apache/lucene/index/SegmentInfo.java
index 78167f1..8bb87ca 100644
--- a/src/java/org/apache/lucene/index/SegmentInfo.java
+++ b/src/java/org/apache/lucene/index/SegmentInfo.java
@@ -77,6 +77,8 @@ final class SegmentInfo {
   private int delCount;                           // How many deleted docs in this segment, or -1 if not yet known
                                                   // (if it's an older index)
 
+  private boolean hasProx;                        // True if this segment has any fields with omitTf==false
+
   public SegmentInfo(String name, int docCount, Directory dir) {
     this.name = name;
     this.docCount = docCount;
@@ -89,14 +91,15 @@ final class SegmentInfo {
     docStoreSegment = name;
     docStoreIsCompoundFile = false;
     delCount = 0;
+    hasProx = true;
   }
 
   public SegmentInfo(String name, int docCount, Directory dir, boolean isCompoundFile, boolean hasSingleNormFile) { 
-    this(name, docCount, dir, isCompoundFile, hasSingleNormFile, -1, null, false);
+    this(name, docCount, dir, isCompoundFile, hasSingleNormFile, -1, null, false, true);
   }
 
   public SegmentInfo(String name, int docCount, Directory dir, boolean isCompoundFile, boolean hasSingleNormFile,
-                     int docStoreOffset, String docStoreSegment, boolean docStoreIsCompoundFile) { 
+                     int docStoreOffset, String docStoreSegment, boolean docStoreIsCompoundFile, boolean hasProx) { 
     this(name, docCount, dir);
     this.isCompoundFile = (byte) (isCompoundFile ? YES : NO);
     this.hasSingleNormFile = hasSingleNormFile;
@@ -104,6 +107,7 @@ final class SegmentInfo {
     this.docStoreOffset = docStoreOffset;
     this.docStoreSegment = docStoreSegment;
     this.docStoreIsCompoundFile = docStoreIsCompoundFile;
+    this.hasProx = hasProx;
     delCount = 0;
     assert docStoreOffset == -1 || docStoreSegment != null;
   }
@@ -180,6 +184,10 @@ final class SegmentInfo {
         assert delCount <= docCount;
       } else
         delCount = -1;
+      if (format <= SegmentInfos.FORMAT_HAS_PROX)
+        hasProx = input.readByte() == 1;
+      else
+        hasProx = true;
     } else {
       delGen = CHECK_DIR;
       normGen = null;
@@ -190,6 +198,7 @@ final class SegmentInfo {
       docStoreIsCompoundFile = false;
       docStoreSegment = null;
       delCount = -1;
+      hasProx = true;
     }
   }
   
@@ -507,6 +516,16 @@ final class SegmentInfo {
     }
     output.writeByte(isCompoundFile);
     output.writeInt(delCount);
+    output.writeByte((byte) (hasProx ? 1:0));
+  }
+
+  void setHasProx(boolean hasProx) {
+    this.hasProx = hasProx;
+    clearFiles();
+  }
+
+  boolean getHasProx() {
+    return hasProx;
   }
 
   private void addIfExists(List files, String fileName) throws IOException {
diff --git a/src/java/org/apache/lucene/index/SegmentInfos.java b/src/java/org/apache/lucene/index/SegmentInfos.java
index 90e79aa..159cfe3 100644
--- a/src/java/org/apache/lucene/index/SegmentInfos.java
+++ b/src/java/org/apache/lucene/index/SegmentInfos.java
@@ -65,8 +65,13 @@ final class SegmentInfos extends Vector {
    *  This way IndexWriter can efficiently report numDocs(). */
   public static final int FORMAT_DEL_COUNT = -6;
 
+  /** This format adds the boolean hasProx to record if any
+   *  fields in the segment store prox information (ie, have
+   *  omitTf==false) */
+  public static final int FORMAT_HAS_PROX = -7;
+
   /* This must always point to the most recent file format. */
-  static final int CURRENT_FORMAT = FORMAT_DEL_COUNT;
+  static final int CURRENT_FORMAT = FORMAT_HAS_PROX;
   
   public int counter = 0;    // used to name new segments
   /**
diff --git a/src/java/org/apache/lucene/index/SegmentMerger.java b/src/java/org/apache/lucene/index/SegmentMerger.java
index d2ca42a..b06cb9f 100644
--- a/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -83,6 +83,10 @@ final class SegmentMerger {
       checkAbort = new CheckAbort(merge, directory);
     termIndexInterval = writer.getTermIndexInterval();
   }
+  
+  boolean hasProx() {
+    return fieldInfos.hasProx();
+  }
 
   /**
    * Add an IndexReader to the collection of readers that are to be merged
@@ -164,6 +168,10 @@ final class SegmentMerger {
     // Basic files
     for (int i = 0; i < IndexFileNames.COMPOUND_EXTENSIONS.length; i++) {
       String ext = IndexFileNames.COMPOUND_EXTENSIONS[i];
+
+      if (ext.equals(IndexFileNames.PROX_EXTENSION) && !hasProx())
+        continue;
+
       if (mergeDocStores || (!ext.equals(IndexFileNames.FIELDS_EXTENSION) &&
                             !ext.equals(IndexFileNames.FIELDS_INDEX_EXTENSION)))
         files.add(segment + "." + ext);
@@ -198,11 +206,11 @@ final class SegmentMerger {
   }
 
   private void addIndexed(IndexReader reader, FieldInfos fieldInfos, Collection names, boolean storeTermVectors, boolean storePositionWithTermVector,
-                         boolean storeOffsetWithTermVector, boolean storePayloads) throws IOException {
+                         boolean storeOffsetWithTermVector, boolean storePayloads, boolean omitTf) throws IOException {
     Iterator i = names.iterator();
     while (i.hasNext()) {
       String field = (String)i.next();
-      fieldInfos.add(field, true, storeTermVectors, storePositionWithTermVector, storeOffsetWithTermVector, !reader.hasNorms(field), storePayloads);
+      fieldInfos.add(field, true, storeTermVectors, storePositionWithTermVector, storeOffsetWithTermVector, !reader.hasNorms(field), storePayloads, omitTf);
     }
   }
 
@@ -265,15 +273,16 @@ final class SegmentMerger {
         SegmentReader segmentReader = (SegmentReader) reader;
         for (int j = 0; j < segmentReader.getFieldInfos().size(); j++) {
           FieldInfo fi = segmentReader.getFieldInfos().fieldInfo(j);
-          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads);
+          fieldInfos.add(fi.name, fi.isIndexed, fi.storeTermVector, fi.storePositionWithTermVector, fi.storeOffsetWithTermVector, !reader.hasNorms(fi.name), fi.storePayloads, fi.omitTf);
         }
       } else {
-        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false);
-        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false);
-        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false);
-        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false);
-        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true);
-        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false);
+        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET), true, true, true, false, false);
+        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_POSITION), true, true, false, false, false);
+        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR_WITH_OFFSET), true, false, true, false, false);
+        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.TERMVECTOR), true, false, false, false, false);
+        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.OMIT_TF), false, false, false, false, true);
+        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.STORES_PAYLOADS), false, false, false, true, false);
+        addIndexed(reader, fieldInfos, reader.getFieldNames(IndexReader.FieldOption.INDEXED), false, false, false, false, false);
         fieldInfos.add(reader.getFieldNames(IndexReader.FieldOption.UNINDEXED), false);
       }
     }
@@ -477,7 +486,8 @@ final class SegmentMerger {
   private final void mergeTerms() throws CorruptIndexException, IOException {
     try {
       freqOutput = directory.createOutput(segment + ".frq");
-      proxOutput = directory.createOutput(segment + ".prx");
+      if (hasProx())
+        proxOutput = directory.createOutput(segment + ".prx");
       termInfosWriter =
               new TermInfosWriter(directory, segment, fieldInfos,
                                   termIndexInterval);
@@ -561,11 +571,20 @@ final class SegmentMerger {
    */
   private final int mergeTermInfo(SegmentMergeInfo[] smis, int n)
           throws CorruptIndexException, IOException {
-    long freqPointer = freqOutput.getFilePointer();
-    long proxPointer = proxOutput.getFilePointer();
-
-    int df = appendPostings(smis, n);		  // append posting data
-
+    final long freqPointer = freqOutput.getFilePointer();
+    final long proxPointer;
+    if (proxOutput != null)
+      proxPointer = proxOutput.getFilePointer();
+    else
+      proxPointer = 0;
+
+    int df;
+    if (fieldInfos.fieldInfo(smis[0].term.field).omitTf) { // append posting data
+      df = appendPostingsNoTf(smis, n);     
+    } else{
+      df = appendPostings(smis, n);      
+    }
+    
     long skipPointer = skipListWriter.writeSkip(freqOutput);
 
     if (df > 0) {
@@ -672,6 +691,53 @@ final class SegmentMerger {
     return df;
   }
 
+  /** Process postings from multiple segments without tf, all positioned on the
+   *  same term. Writes out merged entries only into freqOutput, proxOut is not written.
+   *
+   * @param smis array of segments
+   * @param n number of cells in the array actually occupied
+   * @return number of documents across all segments where this term was found
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   */
+  private final int appendPostingsNoTf(SegmentMergeInfo[] smis, int n)
+          throws CorruptIndexException, IOException {
+    int lastDoc = 0;
+    int df = 0;           // number of docs w/ term
+    skipListWriter.resetSkip();
+    int lastPayloadLength = -1;   // ensures that we write the first length
+    for (int i = 0; i < n; i++) {
+      SegmentMergeInfo smi = smis[i];
+      TermPositions postings = smi.getPositions();
+      assert postings != null;
+      int base = smi.base;
+      int[] docMap = smi.getDocMap();
+      postings.seek(smi.termEnum);
+      while (postings.next()) {
+        int doc = postings.doc();
+        if (docMap != null)
+          doc = docMap[doc];                      // map around deletions
+        doc += base;                              // convert to merged space
+
+        if (doc < 0 || (df > 0 && doc <= lastDoc))
+          throw new CorruptIndexException("docs out of order (" + doc +
+              " <= " + lastDoc + " )");
+
+        df++;
+
+        if ((df % skipInterval) == 0) {
+          skipListWriter.setSkipData(lastDoc, false, lastPayloadLength);
+          skipListWriter.bufferSkip(df);
+        }
+
+        int docCode = (doc - lastDoc);   
+        lastDoc = doc;
+        freqOutput.writeVInt(docCode);    // write doc & freq=1
+      }
+    }
+    return df;
+  }
+  
   private void mergeNorms() throws IOException {
     byte[] normBuffer = null;
     IndexOutput output = null;
diff --git a/src/java/org/apache/lucene/index/SegmentReader.java b/src/java/org/apache/lucene/index/SegmentReader.java
index caf9068..bd979df 100644
--- a/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/src/java/org/apache/lucene/index/SegmentReader.java
@@ -298,6 +298,12 @@ class SegmentReader extends DirectoryIndexReader {
 
       fieldInfos = new FieldInfos(cfsDir, segment + ".fnm");
 
+      boolean anyProx = false;
+      final int numFields = fieldInfos.size();
+      for(int i=0;!anyProx && i<numFields;i++)
+        if (!fieldInfos.fieldInfo(i).omitTf)
+          anyProx = true;
+
       final String fieldsSegment;
 
       if (si.getDocStoreOffset() != -1)
@@ -322,7 +328,8 @@ class SegmentReader extends DirectoryIndexReader {
       // make sure that all index files have been read or are kept open
       // so that if an index update removes them we'll still have them
       freqStream = cfsDir.openInput(segment + ".frq", readBufferSize);
-      proxStream = cfsDir.openInput(segment + ".prx", readBufferSize);
+      if (anyProx)
+        proxStream = cfsDir.openInput(segment + ".prx", readBufferSize);
       openNorms(cfsDir, readBufferSize);
 
       if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed
@@ -728,6 +735,9 @@ class SegmentReader extends DirectoryIndexReader {
       else if (!fi.isIndexed && fieldOption == IndexReader.FieldOption.UNINDEXED) {
         fieldSet.add(fi.name);
       }
+      else if (fi.omitTf && fieldOption == IndexReader.FieldOption.OMIT_TF) {
+        fieldSet.add(fi.name);
+      }
       else if (fi.storePayloads && fieldOption == IndexReader.FieldOption.STORES_PAYLOADS) {
         fieldSet.add(fi.name);
       }
diff --git a/src/java/org/apache/lucene/index/SegmentTermDocs.java b/src/java/org/apache/lucene/index/SegmentTermDocs.java
index 4c429d1..457b3a0 100644
--- a/src/java/org/apache/lucene/index/SegmentTermDocs.java
+++ b/src/java/org/apache/lucene/index/SegmentTermDocs.java
@@ -41,7 +41,8 @@ class SegmentTermDocs implements TermDocs {
   private boolean haveSkipped;
   
   protected boolean currentFieldStoresPayloads;
-
+  protected boolean currentFieldOmitTf;
+  
   protected SegmentTermDocs(SegmentReader parent) {
     this.parent = parent;
     this.freqStream = (IndexInput) parent.freqStream.clone();
@@ -75,6 +76,7 @@ class SegmentTermDocs implements TermDocs {
   void seek(TermInfo ti, Term term) throws IOException {
     count = 0;
     FieldInfo fi = parent.fieldInfos.fieldInfo(term.field);
+    currentFieldOmitTf = (fi != null) ? fi.omitTf : false;
     currentFieldStoresPayloads = (fi != null) ? fi.storePayloads : false;
     if (ti == null) {
       df = 0;
@@ -105,14 +107,19 @@ class SegmentTermDocs implements TermDocs {
     while (true) {
       if (count == df)
         return false;
-
-      int docCode = freqStream.readVInt();
-      doc += docCode >>> 1;       // shift off low bit
-      if ((docCode & 1) != 0)       // if low bit is set
-        freq = 1;         // freq is one
-      else
-        freq = freqStream.readVInt();     // else read freq
-
+      final int docCode = freqStream.readVInt();
+      
+      if (currentFieldOmitTf) {
+        doc += docCode;
+        freq = 1;
+      } else {
+        doc += docCode >>> 1;       // shift off low bit
+        if ((docCode & 1) != 0)       // if low bit is set
+          freq = 1;         // freq is one
+        else
+          freq = freqStream.readVInt();     // else read freq
+      }
+      
       count++;
 
       if (deletedDocs == null || !deletedDocs.get(doc))
@@ -126,27 +133,49 @@ class SegmentTermDocs implements TermDocs {
   public int read(final int[] docs, final int[] freqs)
           throws IOException {
     final int length = docs.length;
+    if (currentFieldOmitTf) {
+      return readNoTf(docs, freqs, length);
+    } else {
+      int i = 0;
+      while (i < length && count < df) {
+        // manually inlined call to next() for speed
+        final int docCode = freqStream.readVInt();
+        doc += docCode >>> 1;       // shift off low bit
+        if ((docCode & 1) != 0)       // if low bit is set
+          freq = 1;         // freq is one
+        else
+          freq = freqStream.readVInt();     // else read freq
+        count++;
+
+        if (deletedDocs == null || !deletedDocs.get(doc)) {
+          docs[i] = doc;
+          freqs[i] = freq;
+          ++i;
+        }
+      }
+      return i;
+    }
+  }
+
+  private final int readNoTf(final int[] docs, final int[] freqs, final int length) throws IOException {
     int i = 0;
     while (i < length && count < df) {
-
       // manually inlined call to next() for speed
-      final int docCode = freqStream.readVInt();
-      doc += docCode >>> 1;       // shift off low bit
-      if ((docCode & 1) != 0)       // if low bit is set
-        freq = 1;         // freq is one
-      else
-        freq = freqStream.readVInt();     // else read freq
+      doc += freqStream.readVInt();       
       count++;
 
       if (deletedDocs == null || !deletedDocs.get(doc)) {
         docs[i] = doc;
-        freqs[i] = freq;
+        // Hardware freq to 1 when term freqs were not
+        // stored in the index
+        freqs[i] = 1;
         ++i;
       }
     }
     return i;
   }
-
+ 
+  
   /** Overridden by SegmentTermPositions to skip in prox stream. */
   protected void skipProx(long proxPointer, int payloadLength) throws IOException {}
 
diff --git a/src/java/org/apache/lucene/index/SegmentTermPositions.java b/src/java/org/apache/lucene/index/SegmentTermPositions.java
index 2a125b8..0107d4c 100644
--- a/src/java/org/apache/lucene/index/SegmentTermPositions.java
+++ b/src/java/org/apache/lucene/index/SegmentTermPositions.java
@@ -60,6 +60,9 @@ extends SegmentTermDocs implements TermPositions {
   }
 
   public final int nextPosition() throws IOException {
+    if (currentFieldOmitTf)
+      // This field does not store term freq, positions, payloads
+      return 0;
     // perform lazy skips if neccessary
     lazySkip();
     proxCount--;
@@ -116,6 +119,7 @@ extends SegmentTermDocs implements TermPositions {
   }
 
   private void skipPositions(int n) throws IOException {
+    assert !currentFieldOmitTf;
     for (int f = n; f > 0; f--) {        // skip unread positions
       readDeltaPosition();
       skipPayload();
diff --git a/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java b/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
index 821ba49..1d229d9 100644
--- a/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
+++ b/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
@@ -39,7 +39,6 @@ final class TermVectorsTermsWriter extends TermsHashConsumer {
 
   public TermVectorsTermsWriter(DocumentsWriter docWriter) {
     this.docWriter = docWriter;
-    streamCount = 2;
   }
 
   public TermsHashConsumerPerThread addThread(TermsHashPerThread termsHashPerThread) {
diff --git a/src/java/org/apache/lucene/index/TermVectorsTermsWriterPerField.java b/src/java/org/apache/lucene/index/TermVectorsTermsWriterPerField.java
index dc421f0..39c77e5 100644
--- a/src/java/org/apache/lucene/index/TermVectorsTermsWriterPerField.java
+++ b/src/java/org/apache/lucene/index/TermVectorsTermsWriterPerField.java
@@ -47,6 +47,10 @@ final class TermVectorsTermsWriterPerField extends TermsHashConsumerPerField {
     fieldState = termsHashPerField.fieldState;
   }
 
+  int getStreamCount() {
+    return 2;
+  }
+
   boolean start(Fieldable[] fields, int count) {
     doVectors = false;
     doVectorPositions = false;
diff --git a/src/java/org/apache/lucene/index/TermsHash.java b/src/java/org/apache/lucene/index/TermsHash.java
index 9669667..068720d 100644
--- a/src/java/org/apache/lucene/index/TermsHash.java
+++ b/src/java/org/apache/lucene/index/TermsHash.java
@@ -42,7 +42,6 @@ final class TermsHash extends InvertedDocConsumer {
   final TermsHash nextTermsHash;
   final int bytesPerPosting;
   final int postingsFreeChunk;
-  final int streamCount;
   final DocumentsWriter docWriter;
   
   TermsHash primaryTermsHash;
@@ -55,7 +54,6 @@ final class TermsHash extends InvertedDocConsumer {
   public TermsHash(final DocumentsWriter docWriter, boolean trackAllocations, final TermsHashConsumer consumer, final TermsHash nextTermsHash) {
     this.docWriter = docWriter;
     this.consumer = consumer;
-    this.streamCount = consumer.streamCount;
     this.nextTermsHash = nextTermsHash;
     this.trackAllocations = trackAllocations;
 
diff --git a/src/java/org/apache/lucene/index/TermsHashConsumer.java b/src/java/org/apache/lucene/index/TermsHashConsumer.java
index 325655a..6b584ae 100644
--- a/src/java/org/apache/lucene/index/TermsHashConsumer.java
+++ b/src/java/org/apache/lucene/index/TermsHashConsumer.java
@@ -28,8 +28,6 @@ abstract class TermsHashConsumer {
   abstract void abort();
   abstract void closeDocStore(DocumentsWriter.FlushState state) throws IOException;
 
-  int streamCount;
-
   FieldInfos fieldInfos;
 
   void setFieldInfos(FieldInfos fieldInfos) {
diff --git a/src/java/org/apache/lucene/index/TermsHashConsumerPerField.java b/src/java/org/apache/lucene/index/TermsHashConsumerPerField.java
index 40af004..2c71641 100644
--- a/src/java/org/apache/lucene/index/TermsHashConsumerPerField.java
+++ b/src/java/org/apache/lucene/index/TermsHashConsumerPerField.java
@@ -32,4 +32,5 @@ abstract class TermsHashConsumerPerField {
   abstract void skippingLongTerm(Token t) throws IOException;
   abstract void newTerm(Token t, RawPostingList p) throws IOException;
   abstract void addTerm(Token t, RawPostingList p) throws IOException;
+  abstract int getStreamCount();
 }
diff --git a/src/java/org/apache/lucene/index/TermsHashPerField.java b/src/java/org/apache/lucene/index/TermsHashPerField.java
index dbecbbc..3a073a0 100644
--- a/src/java/org/apache/lucene/index/TermsHashPerField.java
+++ b/src/java/org/apache/lucene/index/TermsHashPerField.java
@@ -57,9 +57,9 @@ final class TermsHashPerField extends InvertedDocConsumerPerField {
     bytePool = perThread.bytePool;
     docState = perThread.docState;
     fieldState = docInverterPerField.fieldState;
-    streamCount = perThread.termsHash.streamCount;
-    numPostingInt = 2*streamCount;
     this.consumer = perThread.consumer.addField(this, fieldInfo);
+    streamCount = consumer.getStreamCount();
+    numPostingInt = 2*streamCount;
     this.fieldInfo = fieldInfo;
     if (nextPerThread != null)
       nextPerField = (TermsHashPerField) nextPerThread.addField(docInverterPerField, fieldInfo);
@@ -488,6 +488,7 @@ final class TermsHashPerField extends InvertedDocConsumerPerField {
   }
 
   void writeVInt(int stream, int i) {
+    assert stream < streamCount;
     while ((i & ~0x7F) != 0) {
       writeByte(stream, (byte)((i & 0x7f) | 0x80));
       i >>>= 7;
diff --git a/src/site/src/documentation/content/xdocs/fileformats.xml b/src/site/src/documentation/content/xdocs/fileformats.xml
index a1d7f96..a30cb74 100644
--- a/src/site/src/documentation/content/xdocs/fileformats.xml
+++ b/src/site/src/documentation/content/xdocs/fileformats.xml
@@ -246,14 +246,16 @@
                     <p>Term Frequency
                         data. For each term in the dictionary, the numbers of all the
                         documents that contain that term, and the frequency of the term in
-                        that document.
+                        that document if omitTf is false.
                     </p>
                 </li>
 
                 <li>
                     <p>Term Proximity
                         data. For each term in the dictionary, the positions that the term
-                        occurs in each document.
+                        occurs in each document.  Note that this will
+                        not exist if all fields in all documents set
+                        omitTf to true.
                     </p>
                 </li>
 
@@ -826,11 +828,12 @@
                     <b>2.4 and above:</b>
                     Segments --&gt; Format, Version, NameCounter, SegCount, &lt;SegName, SegSize, DelGen, DocStoreOffset, [DocStoreSegment, DocStoreIsCompoundFile], HasSingleNormFile, NumField,
                     NormGen<sup>NumField</sup>,
-                    IsCompoundFile&gt;<sup>SegCount</sup>, Checksum
+                    IsCompoundFile, DeletionCount, HasProx&gt;<sup>SegCount</sup>, Checksum
                 </p>
 
                 <p>
-                    Format, NameCounter, SegCount, SegSize, NumField, DocStoreOffset --&gt; Int32
+                    Format, NameCounter, SegCount, SegSize, NumField,
+                    DocStoreOffset, DeletionCount --&gt; Int32
                 </p>
 
                 <p>
@@ -842,7 +845,8 @@
                 </p>
 
                 <p>
-                    IsCompoundFile, HasSingleNormFile, DocStoreIsCompoundFile --&gt; Int8
+                    IsCompoundFile, HasSingleNormFile,
+                    DocStoreIsCompoundFile, HasProx --&gt; Int8
                 </p>
 
                 <p>
@@ -936,7 +940,16 @@
 		    This is used to verify integrity of the file on
 		    opening the index.
 		</p>
-		
+
+		<p>
+		    DeletionCount records the number of deleted
+		    documents in this segment.
+		</p>
+
+		<p>
+		    HasProx is 1 if any fields in this segment have
+		    omitTf set to false; else, it's 0.
+		</p>
 
             </section>
 
@@ -1264,7 +1277,9 @@
                             determines the position of this term's TermPositions within the .prx
                             file. In particular, it is the difference between the position of
                             this term's data in that file and the position of the previous
-                            term's data (or zero, for the first term in the file.
+                            term's data (or zero, for the first term in the file.  For fields
+			    with omitTf true, this will be 0 since
+                            prox information is not stored.
                         </p>
                         <p>SkipDelta determines the position of this
                             term's SkipData within the .frq file. In
@@ -1338,7 +1353,7 @@
                 <p>
                     The .frq file contains the lists of documents
                     which contain each term, along with the frequency of the term in that
-                    document.
+                    document (if omitTf is false).
                 </p>
                 <p>FreqFile (.frq) --&gt;
                     &lt;TermFreqs, SkipData&gt;
@@ -1349,7 +1364,7 @@
                     <sup>DocFreq</sup>
                 </p>
                 <p>TermFreq --&gt;
-                    DocDelta, Freq?
+                    DocDelta[, Freq?]
                 </p>
                 <p>SkipData --&gt;
                     &lt;&lt;SkipLevelLength, SkipLevel&gt;
@@ -1375,21 +1390,31 @@
                 <p>TermFreq
                     entries are ordered by increasing document number.
                 </p>
-                <p>DocDelta
-                    determines both the document number and the frequency. In
-                    particular, DocDelta/2 is the difference between this document number
-                    and the previous document number (or zero when this is the first
-                    document in a TermFreqs). When DocDelta is odd, the frequency is
-                    one. When DocDelta is even, the frequency is read as another VInt.
-                </p>
-                <p>For
-                    example, the TermFreqs for a term which occurs once in document seven
-                    and three times in document eleven would be the following sequence of
-                    VInts:
-                </p>
-                <p>15,
-                    8, 3
-                </p>
+                <p>DocDelta: if omitTf is false, this determines both
+                    the document number and the frequency. In
+                    particular, DocDelta/2 is the difference between
+                    this document number and the previous document
+                    number (or zero when this is the first document in
+                    a TermFreqs). When DocDelta is odd, the frequency
+                    is one. When DocDelta is even, the frequency is
+                    read as another VInt.  If omitTf is true, DocDelta
+                    contains the gap (not multiplied by 2) between
+                    document numbers and no frequency information is
+                    stored.
+                </p>
+                <p>For example, the TermFreqs for a term which occurs
+                    once in document seven and three times in document
+                    eleven, with omitTf false, would be the following
+                    sequence of VInts:
+                </p>
+                <p>15, 8, 3
+                </p>
+		<p> If omitTf were true it would be this sequence
+		of VInts instead:
+		  </p>
+		 <p>
+		   7,4
+                 </p>
                 <p>DocSkip records the document number before every
                     SkipInterval
                     <sup>th</sup>
@@ -1454,7 +1479,11 @@
 
                 <p>
                     The .prx file contains the lists of positions that
-                    each term occurs at within documents.
+                    each term occurs at within documents.  Note that
+                    fields with omitTf true do not store
+                    anything into this file, and if all fields in the
+                    index have omitTf true then the .prx file will not
+                    exist.
                 </p>
                 <p>ProxFile (.prx) --&gt;
                     &lt;TermPositions&gt;
diff --git a/src/test/org/apache/lucene/TestHitIterator.java b/src/test/org/apache/lucene/TestHitIterator.java
index 59fd120..56f8787 100644
--- a/src/test/org/apache/lucene/TestHitIterator.java
+++ b/src/test/org/apache/lucene/TestHitIterator.java
@@ -18,6 +18,7 @@ package org.apache.lucene;
  */
 
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.Term;
@@ -54,6 +55,8 @@ public class TestHitIterator extends LuceneTestCase {
 
     writer.close();
 
+    _TestUtil.checkIndex(directory);
+
     IndexSearcher searcher = new IndexSearcher(directory);
     Hits hits = searcher.search(new TermQuery(new Term("field", "iterator")));
 
diff --git a/src/test/org/apache/lucene/index/TestOmitTf.java b/src/test/org/apache/lucene/index/TestOmitTf.java
new file mode 100644
index 0000000..81a528b
--- /dev/null
+++ b/src/test/org/apache/lucene/index/TestOmitTf.java
@@ -0,0 +1,363 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Collection;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.HitCollector;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Searcher;
+import org.apache.lucene.search.Similarity;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockRAMDirectory;
+
+
+public class TestOmitTf extends LuceneTestCase {
+    
+  public static class SimpleSimilarity extends Similarity {
+    public float lengthNorm(String field, int numTerms) { return 1.0f; }
+    public float queryNorm(float sumOfSquaredWeights) { return 1.0f; }
+    
+    public float tf(float freq) { return freq; }
+    
+    public float sloppyFreq(int distance) { return 2.0f; }
+    public float idf(Collection terms, Searcher searcher) { return 1.0f; }
+    public float idf(int docFreq, int numDocs) { return 1.0f; }
+    public float coord(int overlap, int maxOverlap) { return 1.0f; }
+  }
+
+
+  // Tests whether the DocumentWriter correctly enable the
+  // omitTf bit in the FieldInfo
+  public void testOmitTf() throws Exception {
+    Directory ram = new MockRAMDirectory();
+    Analyzer analyzer = new StandardAnalyzer();
+    IndexWriter writer = new IndexWriter(ram, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
+    Document d = new Document();
+        
+    // this field will have Tf
+    Field f1 = new Field("f1", "This field has term freqs", Field.Store.NO, Field.Index.TOKENIZED);
+    d.add(f1);
+       
+    // this field will NOT have Tf
+    Field f2 = new Field("f2", "This field has NO Tf in all docs", Field.Store.NO, Field.Index.TOKENIZED);
+    f2.setOmitTf(true);
+    d.add(f2);
+        
+    writer.addDocument(d);
+    writer.optimize();
+    // now we add another document which has term freq for field f2 and not for f1 and verify if the SegmentMerger
+    // keep things constant
+    d = new Document();
+        
+    // Reverese
+    f1.setOmitTf(true);
+    d.add(f1);
+        
+    f2.setOmitTf(false);        
+    d.add(f2);
+        
+    writer.addDocument(d);
+    // force merge
+    writer.optimize();
+    // flush
+    writer.close();
+    _TestUtil.checkIndex(ram);
+
+    // only one segment in the index, so we can cast to SegmentReader
+    SegmentReader reader = (SegmentReader) IndexReader.open(ram);
+    FieldInfos fi = reader.fieldInfos();
+    assertTrue("OmitTf field bit should be set.", fi.fieldInfo("f1").omitTf);
+    assertTrue("OmitTf field bit should be set.", fi.fieldInfo("f2").omitTf);
+        
+    reader.close();
+    ram.close();
+  }
+ 
+  // Tests whether merging of docs that have different
+  // omitTf for the same field works
+  public void testMixedMerge() throws Exception {
+    Directory ram = new MockRAMDirectory();
+    Analyzer analyzer = new StandardAnalyzer();
+    IndexWriter writer = new IndexWriter(ram, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
+    writer.setMaxBufferedDocs(3);
+    writer.setMergeFactor(2);
+    Document d = new Document();
+        
+    // this field will have Tf
+    Field f1 = new Field("f1", "This field has term freqs", Field.Store.NO, Field.Index.TOKENIZED);
+    d.add(f1);
+       
+    // this field will NOT have Tf
+    Field f2 = new Field("f2", "This field has NO Tf in all docs", Field.Store.NO, Field.Index.TOKENIZED);
+    f2.setOmitTf(true);
+    d.add(f2);
+
+    for(int i=0;i<30;i++)
+      writer.addDocument(d);
+        
+    // now we add another document which has term freq for field f2 and not for f1 and verify if the SegmentMerger
+    // keep things constant
+    d = new Document();
+        
+    // Reverese
+    f1.setOmitTf(true);
+    d.add(f1);
+        
+    f2.setOmitTf(false);        
+    d.add(f2);
+        
+    for(int i=0;i<30;i++)
+      writer.addDocument(d);
+        
+    // force merge
+    writer.optimize();
+    // flush
+    writer.close();
+
+    _TestUtil.checkIndex(ram);
+
+    // only one segment in the index, so we can cast to SegmentReader
+    SegmentReader reader = (SegmentReader) IndexReader.open(ram);
+    FieldInfos fi = reader.fieldInfos();
+    assertTrue("OmitTf field bit should be set.", fi.fieldInfo("f1").omitTf);
+    assertTrue("OmitTf field bit should be set.", fi.fieldInfo("f2").omitTf);
+        
+    reader.close();
+    ram.close();
+  }
+
+  // Make sure first adding docs that do not omitTf for
+  // field X, then adding docs that do omitTf for that same
+  // field, 
+  public void testMixedRAM() throws Exception {
+    Directory ram = new MockRAMDirectory();
+    Analyzer analyzer = new StandardAnalyzer();
+    IndexWriter writer = new IndexWriter(ram, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
+    writer.setMaxBufferedDocs(10);
+    writer.setMergeFactor(2);
+    Document d = new Document();
+        
+    // this field will have Tf
+    Field f1 = new Field("f1", "This field has term freqs", Field.Store.NO, Field.Index.TOKENIZED);
+    d.add(f1);
+       
+    // this field will NOT have Tf
+    Field f2 = new Field("f2", "This field has NO Tf in all docs", Field.Store.NO, Field.Index.TOKENIZED);
+    d.add(f2);
+
+    for(int i=0;i<5;i++)
+      writer.addDocument(d);
+
+    f2.setOmitTf(true);
+        
+    for(int i=0;i<20;i++)
+      writer.addDocument(d);
+
+    // force merge
+    writer.optimize();
+
+    // flush
+    writer.close();
+
+    _TestUtil.checkIndex(ram);
+
+    // only one segment in the index, so we can cast to SegmentReader
+    SegmentReader reader = (SegmentReader) IndexReader.open(ram);
+    FieldInfos fi = reader.fieldInfos();
+    assertTrue("OmitTf field bit should not be set.", !fi.fieldInfo("f1").omitTf);
+    assertTrue("OmitTf field bit should be set.", fi.fieldInfo("f2").omitTf);
+        
+    reader.close();
+    ram.close();
+  }
+
+  private void assertNoPrx(Directory dir) throws Throwable {
+    final String[] files = dir.list();
+    for(int i=0;i<files.length;i++)
+      assertFalse(files[i].endsWith(".prx"));
+  }
+
+  // Verifies no *.prx exists when all fields omit term freq:
+  public void testNoPrxFile() throws Throwable {
+    Directory ram = new MockRAMDirectory();
+    Analyzer analyzer = new StandardAnalyzer();
+    IndexWriter writer = new IndexWriter(ram, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
+    writer.setMaxBufferedDocs(3);
+    writer.setMergeFactor(2);
+    writer.setUseCompoundFile(false);
+    Document d = new Document();
+        
+    Field f1 = new Field("f1", "This field has term freqs", Field.Store.NO, Field.Index.TOKENIZED);
+    f1.setOmitTf(true);
+    d.add(f1);
+
+    for(int i=0;i<30;i++)
+      writer.addDocument(d);
+
+    writer.commit();
+
+    assertNoPrx(ram);
+        
+    // force merge
+    writer.optimize();
+    // flush
+    writer.close();
+
+    assertNoPrx(ram);
+    _TestUtil.checkIndex(ram);
+    ram.close();
+  }
+ 
+  // Test scores with one field with Term Freqs and one without, otherwise with equal content 
+  public void testBasic() throws Exception {
+    Directory dir = new MockRAMDirectory();  
+    Analyzer analyzer = new StandardAnalyzer();
+    IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.LIMITED);
+    writer.setMergeFactor(2);
+    writer.setMaxBufferedDocs(2);
+    writer.setSimilarity(new SimpleSimilarity());
+        
+        
+    StringBuffer sb = new StringBuffer(265);
+    String term = "term";
+    for(int i = 0; i<30; i++){
+      Document d = new Document();
+      sb.append(term).append(" ");
+      String content  = sb.toString();
+      Field noTf = new Field("noTf", content + (i%2==0 ? "" : " notf"), Field.Store.NO, Field.Index.TOKENIZED);
+      noTf.setOmitTf(true);
+      d.add(noTf);
+          
+      Field tf = new Field("tf", content + (i%2==0 ? " tf" : ""), Field.Store.NO, Field.Index.TOKENIZED);
+      d.add(tf);
+          
+      writer.addDocument(d);
+      //System.out.println(d);
+    }
+        
+    writer.optimize();
+    // flush
+    writer.close();
+    _TestUtil.checkIndex(dir);
+
+    /*
+     * Verify the index
+     */         
+    Searcher searcher = new IndexSearcher(dir);
+    searcher.setSimilarity(new SimpleSimilarity());
+        
+    Term a = new Term("noTf", term);
+    Term b = new Term("tf", term);
+    Term c = new Term("noTf", "notf");
+    Term d = new Term("tf", "tf");
+    TermQuery q1 = new TermQuery(a);
+    TermQuery q2 = new TermQuery(b);
+    TermQuery q3 = new TermQuery(c);
+    TermQuery q4 = new TermQuery(d);
+
+        
+    searcher.search(q1,
+                    new CountingHitCollector() {
+                      public final void collect(int doc, float score) {
+                        //System.out.println("Q1: Doc=" + doc + " score=" + score);
+                        assertTrue(score==1.0f);
+                        super.collect(doc, score);
+                      }
+                    });
+    //System.out.println(CountingHitCollector.getCount());
+        
+        
+    searcher.search(q2,
+                    new CountingHitCollector() {
+                      public final void collect(int doc, float score) {
+                        //System.out.println("Q2: Doc=" + doc + " score=" + score);  
+                        assertTrue(score==1.0f+doc);
+                        super.collect(doc, score);
+                      }
+                    });
+    //System.out.println(CountingHitCollector.getCount());
+         
+        
+        
+        
+        
+    searcher.search(q3,
+                    new CountingHitCollector() {
+                      public final void collect(int doc, float score) {
+                        //System.out.println("Q1: Doc=" + doc + " score=" + score);
+                        assertTrue(score==1.0f);
+                        assertFalse(doc%2==0);
+                        super.collect(doc, score);
+                      }
+                    });
+    //System.out.println(CountingHitCollector.getCount());
+        
+        
+    searcher.search(q4,
+                    new CountingHitCollector() {
+                      public final void collect(int doc, float score) {
+                        //System.out.println("Q1: Doc=" + doc + " score=" + score);
+                        assertTrue(score==1.0f);
+                        assertTrue(doc%2==0);
+                        super.collect(doc, score);
+                      }
+                    });
+    //System.out.println(CountingHitCollector.getCount());
+        
+        
+        
+    BooleanQuery bq = new BooleanQuery();
+    bq.add(q1,Occur.MUST);
+    bq.add(q4,Occur.MUST);
+        
+    searcher.search(bq,
+                    new CountingHitCollector() {
+                      public final void collect(int doc, float score) {
+                        //System.out.println("BQ: Doc=" + doc + " score=" + score);
+                        super.collect(doc, score);
+                      }
+                    });
+    assertTrue(15 == CountingHitCollector.getCount());
+        
+    searcher.close();     
+    dir.close();
+  }
+     
+  public static class CountingHitCollector extends HitCollector {
+    static int count=0;
+    static int sum=0;
+    CountingHitCollector(){count=0;sum=0;}
+    public void collect(int doc, float score) {
+      count++;
+      sum += doc;  // use it to avoid any possibility of being optimized away
+    }
+
+    public static int getCount() { return count; }
+    public static int getSum() { return sum; }
+  }
+}

