GitDiffStart: f5c04ae17bf59933e2dd33578b9423a6259bb84c | Tue Dec 15 07:49:07 2009 +0000
diff --git a/build.xml b/build.xml
index 5a372a9..c81ade1 100644
--- a/build.xml
+++ b/build.xml
@@ -529,6 +529,8 @@
   	
     <solr-jar destfile="${dist}/apache-solr-dataimporthandler-src-${version}.jar"
               basedir="contrib/dataimporthandler/src/main/java" />
+    <solr-jar destfile="${dist}/apache-solr-dataimporthandler-extras-src-${version}.jar"
+              basedir="contrib/dataimporthandler/src/extras/main/java" />
 
     <solr-jar destfile="${dist}/apache-solr-cell-src-${version}.jar"
               basedir="contrib/extraction/src" />
@@ -731,6 +733,7 @@
     <sign-maven-dependency-artifacts artifact.id="solr-commons-csv" gpg.passphrase="${gpg.passphrase}"/>
     <sign-maven-artifacts artifact.id="solr-core" gpg.passphrase="${gpg.passphrase}"/>
     <sign-maven-artifacts artifact.id="solr-dataimporthandler" gpg.passphrase="${gpg.passphrase}"/>
+    <sign-maven-artifacts artifact.id="solr-dataimporthandler-extras" gpg.passphrase="${gpg.passphrase}"/>
     <sign-maven-artifacts artifact.id="solr-clustering" gpg.passphrase="${gpg.passphrase}"/>
 
     <sign-maven-artifacts artifact.id="solr-cell" gpg.passphrase="${gpg.passphrase}"/>
@@ -794,6 +797,15 @@
         </artifact-attachments>
       </m2-deploy>
 
+      <m2-deploy pom.xml="contrib/dataimporthandler/solr-dataimporthandler-extras-pom.xml.template"
+                 jar.file="${dist}/apache-solr-dataimporthandler-extras-${version}.jar">
+
+        <artifact-attachments>
+          <attach file="${dist}/apache-solr-dataimporthandler-extras-src-${version}.jar" classifier="sources"/>
+          <attach file="${dist}/apache-solr-dataimporthandler-docs-${version}.jar" classifier="javadoc"/>
+        </artifact-attachments>
+      </m2-deploy>
+
       <m2-deploy pom.xml="contrib/extraction/solr-cell-pom.xml.template"
                  jar.file="${dist}/apache-solr-cell-${version}.jar">
         <artifact-attachments>
diff --git a/contrib/dataimporthandler/CHANGES.txt b/contrib/dataimporthandler/CHANGES.txt
index d45f9da..102fdf0 100644
--- a/contrib/dataimporthandler/CHANGES.txt
+++ b/contrib/dataimporthandler/CHANGES.txt
@@ -50,7 +50,7 @@ Other Changes
 
 Build
 ----------------------
-* SOLR-1643: Moved dataimporthandler extras into dataimporthandler core (shalin)
+
 
 Documentation
 ----------------------
diff --git a/contrib/dataimporthandler/build.xml b/contrib/dataimporthandler/build.xml
index aea55c0..b2832a7 100644
--- a/contrib/dataimporthandler/build.xml
+++ b/contrib/dataimporthandler/build.xml
@@ -37,10 +37,17 @@
   	<pathelement location="${solr-path}/build/solr" />
   	<pathelement location="${solr-path}/build/solrj" />
   	<fileset dir="${solr-path}/lib" includes="*.jar"/>
-    <fileset dir="lib/" includes="*.jar"/>
-  	<fileset dir="${tikalibs-path}" includes="*.jar"/>
   </path>
 
+  <path id="extras.classpath">
+  	<pathelement location="${solr-path}/build/solr" />
+  	<pathelement location="${solr-path}/build/solrj" />
+  	<pathelement location="target/classes" />
+  	<fileset dir="${solr-path}/lib" includes="*.jar"/>
+  	<fileset dir="lib/" includes="*.jar"/>
+  	<fileset dir="${tikalibs-path}" includes="*.jar"/>
+  </path>
+	
   <path id="test.classpath">
   	<path refid="common.classpath" />
   	<path refid="classpath.jetty" />
@@ -49,6 +56,16 @@
     <pathelement path="${java.class.path}"/>
   </path>
 
+  <path id="test.extras.classpath">
+  	<path refid="extras.classpath" />
+  	<path refid="classpath.jetty" />
+	  <pathelement path="target/classes" />
+	  <pathelement path="target/extras/classes" />
+  	<pathelement path="target/test-classes" />
+  	<pathelement path="target/extras/test-classes" />
+    <pathelement path="${java.class.path}"/>
+  </path>
+	
   <target name="clean">
   	<delete failonerror="false" dir="target"/>
     <delete failonerror="false">
@@ -77,9 +94,18 @@
     </solr-javac>
   </target>
 
-  <target name="build" depends="compile">
+  <target name="compileExtras" depends="compile">
+    <solr-javac destdir="target/extras/classes"
+                classpathref="extras.classpath">
+      <src path="src/extras/main/java" />
+    </solr-javac>
+  </target>
+	
+  <target name="build" depends="compile,compileExtras">
     <solr-jar destfile="target/${fullnamever}.jar" basedir="target/classes"
               manifest="${common.dir}/${dest}/META-INF/MANIFEST.MF" />
+    <solr-jar destfile="target/apache-${ant.project.name}-extras-${version}.jar" basedir="target/extras/classes"
+              manifest="${common.dir}/${dest}/META-INF/MANIFEST.MF" />
   </target>
 	
   <target name="compileTests" depends="compile">
@@ -89,7 +115,16 @@
   	</solr-javac>
   </target>
 
-  <target name="test" depends="compileTests">
+  <target name="compileExtrasTests" depends="compileExtras">
+  	<solr-javac destdir="target/extras/test-classes"
+  	                classpathref="test.classpath">
+  	  <src path="src/extras/test/java" />
+  	</solr-javac>
+  </target>
+
+  <target  name="test" depends="testCore,testExtras"/>
+	
+  <target name="testCore" depends="compileTests">
   	<mkdir dir="${junit.output.dir}"/>
   	
   	<junit printsummary="on"
@@ -112,6 +147,29 @@
     <fail if="tests.failed">Tests failed!</fail>
   </target>
 
+  <target name="testExtras" depends="compileExtrasTests">
+  	<mkdir dir="${junit.output.dir}"/>
+
+  	<junit printsummary="on"
+           haltonfailure="no"
+           errorProperty="tests.failed"
+           failureProperty="tests.failed"
+           dir="src/extras/test/resources/"
+           >
+      <formatter type="brief" usefile="false" if="junit.details"/>
+      <classpath refid="test.extras.classpath"/>
+      <formatter type="xml"/>
+      <batchtest fork="yes" todir="${junit.output.dir}" unless="testcase">
+        <fileset dir="src/extras/test/java" includes="${junit.includes}"/>
+      </batchtest>
+      <batchtest fork="yes" todir="${junit.output.dir}" if="testcase">
+        <fileset dir="src/extras/test/java" includes="**/${testcase}.java"/>
+      </batchtest>
+    </junit>
+
+    <fail if="tests.failed">Tests failed!</fail>
+  </target>
+	
   <target name="dist" depends="build">
   	<copy todir="../../build/web">
   		<fileset dir="src/main/webapp" includes="**" />
@@ -119,6 +177,7 @@
   	<mkdir dir="../../build/web/WEB-INF/lib"/>
   	<copy file="target/${fullnamever}.jar" todir="${solr-path}/build/web/WEB-INF/lib"></copy>
   	<copy file="target/${fullnamever}.jar" todir="${solr-path}/dist"></copy>
+  	<copy file="target/apache-${ant.project.name}-extras-${version}.jar" todir="${solr-path}/dist"></copy>
   </target>
 	
   <target name="javadoc">
@@ -127,6 +186,7 @@
 
       <path id="javadoc.classpath">
         <path refid="common.classpath"/>
+        <path refid="extras.classpath"/>
       </path>
 
       <invoke-javadoc
@@ -134,6 +194,7 @@
       	title="${Name} ${version} contrib-${fullnamever} API">
         <sources>
           <packageset dir="src/main/java"/>
+          <packageset dir="src/extras/main/java"/>
         </sources>
       </invoke-javadoc>
     </sequential>
diff --git a/contrib/dataimporthandler/solr-dataimporthandler-extras-pom.xml.template b/contrib/dataimporthandler/solr-dataimporthandler-extras-pom.xml.template
new file mode 100644
index 0000000..491dd89
--- /dev/null
+++ b/contrib/dataimporthandler/solr-dataimporthandler-extras-pom.xml.template
@@ -0,0 +1,52 @@
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+
+  <!--
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+  -->
+
+  <modelVersion>4.0.0</modelVersion>
+
+  <parent>
+    <groupId>org.apache.solr</groupId>
+    <artifactId>solr-parent</artifactId>
+    <version>@maven_version@</version>
+  </parent>
+
+  <groupId>org.apache.solr</groupId>
+  <artifactId>solr-dataimporthandler-extras</artifactId>
+  <name>Apache Solr DataImportHandler Extras</name>
+  <version>@maven_version@</version>
+  <description>Apache Solr DataImportHandler Extras</description>
+  <packaging>jar</packaging>
+
+  <dependencies>
+    <dependency>
+      <groupId>javax.activation</groupId>
+      <artifactId>activation</artifactId>
+      <version>1.1</version>
+    </dependency>
+    <dependency>
+      <groupId>javax.mail</groupId>
+      <artifactId>mail</artifactId>
+      <version>1.4.1</version>
+    </dependency>
+  </dependencies>
+
+</project>
diff --git a/contrib/dataimporthandler/src/extras/main/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java b/contrib/dataimporthandler/src/extras/main/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java
new file mode 100644
index 0000000..7e464bf
--- /dev/null
+++ b/contrib/dataimporthandler/src/extras/main/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java
@@ -0,0 +1,599 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.solr.handler.dataimport;
+
+import com.sun.mail.imap.IMAPMessage;
+import org.apache.tika.config.TikaConfig;
+import org.apache.tika.utils.ParseUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import javax.mail.*;
+import javax.mail.internet.AddressException;
+import javax.mail.internet.ContentType;
+import javax.mail.internet.InternetAddress;
+import javax.mail.internet.MimeMessage;
+import javax.mail.search.AndTerm;
+import javax.mail.search.ComparisonTerm;
+import javax.mail.search.ReceivedDateTerm;
+import javax.mail.search.SearchTerm;
+import java.io.InputStream;
+import java.text.ParseException;
+import java.text.SimpleDateFormat;
+import java.util.*;
+
+/**
+ * An EntityProcessor instance which can index emails along with their attachments from POP3 or IMAP sources. Refer to
+ * <a href="http://wiki.apache.org/solr/DataImportHandler">http://wiki.apache.org/solr/DataImportHandler</a> for more
+ * details. <b>This API is experimental and subject to change</b>
+ *
+ * @version $Id$
+ * @since solr 1.4
+ */
+public class MailEntityProcessor extends EntityProcessorBase {
+
+  public static interface CustomFilter {
+    public SearchTerm getCustomSearch(Folder folder);
+  }
+
+  public void init(Context context) {
+    super.init(context);
+    // set attributes using  XXX getXXXFromContext(attribute, defualtValue);
+    // applies variable resolver and return default if value is not found or null
+    // REQUIRED : connection and folder info
+    user = getStringFromContext("user", null);
+    password = getStringFromContext("password", null);
+    host = getStringFromContext("host", null);
+    protocol = getStringFromContext("protocol", null);
+    folderNames = getStringFromContext("folders", null);
+    // validate
+    if (host == null || protocol == null || user == null || password == null
+            || folderNames == null)
+      throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
+              "'user|password|protocol|host|folders' are required attributes");
+
+    //OPTIONAL : have defaults and are optional
+    recurse = getBoolFromContext("recurse", true);
+    String excludes = getStringFromContext("exclude", "");
+    if (excludes != null && !excludes.trim().equals("")) {
+      exclude = Arrays.asList(excludes.split(","));
+    }
+    String includes = getStringFromContext("include", "");
+    if (includes != null && !includes.trim().equals("")) {
+      include = Arrays.asList(includes.split(","));
+    }
+    batchSize = getIntFromContext("batchSize", 20);
+    customFilter = getStringFromContext("customFilter", "");
+    String s = getStringFromContext("fetchMailsSince", "");
+    if (s != null)
+      try {
+        fetchMailsSince = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").parse(s);
+      } catch (ParseException e) {
+        throw new DataImportHandlerException(DataImportHandlerException.SEVERE, "Invalid value for fetchMailSince: " + s, e);
+      }
+
+    fetchSize = getIntFromContext("fetchSize", 32 * 1024);
+    cTimeout = getIntFromContext("connectTimeout", 30 * 1000);
+    rTimeout = getIntFromContext("readTimeout", 60 * 1000);
+    processAttachment = getBoolFromContext("processAttachement", true);
+
+    logConfig();
+  }
+
+  public Map<String, Object> nextRow() {
+    Message mail;
+    Map<String, Object> row = null;
+    do {
+      // try till there is a valid document or folders get exhausted.
+      // when mail == NULL, it means end of processing
+      mail = getNextMail();
+      if (mail != null)
+        row = getDocumentFromMail(mail);
+    } while (row == null && mail != null);    
+    return row;
+  }
+
+  private Message getNextMail() {
+    if (!connected) {
+      if (!connectToMailBox())
+        return null;
+      connected = true;
+    }
+    if (folderIter == null) {
+      createFilters();
+      folderIter = new FolderIterator(mailbox);
+    }
+    // get next message from the folder
+    // if folder is exhausted get next folder
+    // loop till a valid mail or all folders exhausted.
+    while (msgIter == null || !msgIter.hasNext()) {
+      Folder next = folderIter.hasNext() ? folderIter.next() : null;
+      if (next == null) {
+        return null;
+      }
+      msgIter = new MessageIterator(next, batchSize);
+    }
+    return msgIter.next();
+  }
+
+  private Map<String, Object> getDocumentFromMail(Message mail) {
+    Map<String, Object> row = new HashMap<String, Object>();
+    try {
+      addPartToDocument(mail, row, true);
+      return row;
+    } catch (Exception e) {
+      return null;
+    }
+  }
+
+  public void addPartToDocument(Part part, Map<String, Object> row, boolean outerMost) throws Exception {
+    if (part instanceof Message) {
+      addEnvelopToDocument(part, row);
+    }
+
+    String ct = part.getContentType();
+    ContentType ctype = new ContentType(ct);
+    if (part.isMimeType("multipart/*")) {
+      Multipart mp = (Multipart) part.getContent();
+      int count = mp.getCount();
+      if (part.isMimeType("multipart/alternative"))
+        count = 1;
+      for (int i = 0; i < count; i++)
+        addPartToDocument(mp.getBodyPart(i), row, false);
+    } else if (part.isMimeType("message/rfc822")) {
+      addPartToDocument((Part) part.getContent(), row, false);
+    } else {
+      String disp = part.getDisposition();
+      if (!processAttachment || (disp != null && disp.equalsIgnoreCase(Part.ATTACHMENT)))        return;
+      InputStream is = part.getInputStream();
+      String fileName = part.getFileName();
+      String content = ParseUtils.getStringContent(is, TikaConfig.getDefaultConfig(), ctype.getBaseType().toLowerCase());
+      if (disp != null && disp.equalsIgnoreCase(Part.ATTACHMENT)) {
+        if (row.get(ATTACHMENT) == null)
+          row.put(ATTACHMENT, new ArrayList<String>());
+        List<String> contents = (List<String>) row.get(ATTACHMENT);
+        contents.add(content);
+        row.put(ATTACHMENT, contents);
+        if (row.get(ATTACHMENT_NAMES) == null)
+          row.put(ATTACHMENT_NAMES, new ArrayList<String>());
+        List<String> names = (List<String>) row.get(ATTACHMENT_NAMES);
+        names.add(fileName);
+        row.put(ATTACHMENT_NAMES, names);
+      } else {
+        if (row.get(CONTENT) == null)
+          row.put(CONTENT, new ArrayList<String>());
+        List<String> contents = (List<String>) row.get(CONTENT);
+        contents.add(content);
+        row.put(CONTENT, contents);
+      }
+    }
+  }
+
+  private void addEnvelopToDocument(Part part, Map<String, Object> row) throws MessagingException {
+    MimeMessage mail = (MimeMessage) part;
+    Address[] adresses;
+    if ((adresses = mail.getFrom()) != null && adresses.length > 0)
+      row.put(FROM, adresses[0].toString());
+
+    List<String> to = new ArrayList<String>();
+    if ((adresses = mail.getRecipients(Message.RecipientType.TO)) != null)
+      addAddressToList(adresses, to);
+    if ((adresses = mail.getRecipients(Message.RecipientType.CC)) != null)
+      addAddressToList(adresses, to);
+    if ((adresses = mail.getRecipients(Message.RecipientType.BCC)) != null)
+      addAddressToList(adresses, to);
+    if (to.size() > 0)
+      row.put(TO_CC_BCC, to);
+
+    row.put(MESSAGE_ID, mail.getMessageID());
+    row.put(SUBJECT, mail.getSubject());
+
+    Date d = mail.getSentDate();
+    if (d != null) {
+      row.put(SENT_DATE, d);
+    }
+
+    List<String> flags = new ArrayList<String>();
+    for (Flags.Flag flag : mail.getFlags().getSystemFlags()) {
+      if (flag == Flags.Flag.ANSWERED)
+        flags.add(FLAG_ANSWERED);
+      else if (flag == Flags.Flag.DELETED)
+        flags.add(FLAG_DELETED);
+      else if (flag == Flags.Flag.DRAFT)
+        flags.add(FLAG_DRAFT);
+      else if (flag == Flags.Flag.FLAGGED)
+        flags.add(FLAG_FLAGGED);
+      else if (flag == Flags.Flag.RECENT)
+        flags.add(FLAG_RECENT);
+      else if (flag == Flags.Flag.SEEN)
+        flags.add(FLAG_SEEN);
+    }
+    flags.addAll(Arrays.asList(mail.getFlags().getUserFlags()));
+    row.put(FLAGS, flags);
+
+    String[] hdrs = mail.getHeader("X-Mailer");
+    if (hdrs != null)
+      row.put(XMAILER, hdrs[0]);
+  }
+
+
+  private void addAddressToList(Address[] adresses, List<String> to) throws AddressException {
+    for (Address address : adresses) {
+      to.add(address.toString());
+      InternetAddress ia = (InternetAddress) address;
+      if (ia.isGroup()) {
+        InternetAddress[] group = ia.getGroup(false);
+        for (InternetAddress member : group)
+          to.add(member.toString());
+      }
+    }
+  }
+
+  private boolean connectToMailBox() {
+    try {
+      Properties props = new Properties();
+      props.setProperty("mail.store.protocol", protocol);
+      props.setProperty("mail.imap.fetchsize", "" + fetchSize);
+      props.setProperty("mail.imap.timeout", "" + rTimeout);
+      props.setProperty("mail.imap.connectiontimeout", "" + cTimeout);
+      Session session = Session.getDefaultInstance(props, null);
+      mailbox = session.getStore(protocol);
+      mailbox.connect(host, user, password);
+      LOG.info("Connected to mailbox");
+      return true;
+    } catch (MessagingException e) {
+      throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
+              "Connection failed", e);
+    }
+  }
+
+  private void createFilters() {
+    if (fetchMailsSince != null) {
+      filters.add(new MailsSinceLastCheckFilter(fetchMailsSince));
+    }
+    if (customFilter != null && !customFilter.equals("")) {
+      try {
+        Class cf = Class.forName(customFilter);
+        Object obj = cf.newInstance();
+        if (obj instanceof CustomFilter) {
+          filters.add((CustomFilter) obj);
+        }
+      } catch (Exception e) {
+        throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
+                "Custom filter could not be created", e);
+      }
+    }
+  }
+
+  private void logConfig() {
+    if (!LOG.isInfoEnabled()) return;
+    StringBuffer config = new StringBuffer();
+    config.append("user : ").append(user).append(System.getProperty("line.separator"));
+    config.append("pwd : ").append(password).append(System.getProperty("line.separator"));
+    config.append("protocol : ").append(protocol).append(System.getProperty("line.separator"));
+    config.append("host : ").append(host).append(System.getProperty("line.separator"));
+    config.append("folders : ").append(folderNames).append(System.getProperty("line.separator"));
+    config.append("recurse : ").append(recurse).append(System.getProperty("line.separator"));
+    config.append("exclude : ").append(exclude.toString()).append(System.getProperty("line.separator"));
+    config.append("include : ").append(include.toString()).append(System.getProperty("line.separator"));
+    config.append("batchSize : ").append(batchSize).append(System.getProperty("line.separator"));
+    config.append("fetchSize : ").append(fetchSize).append(System.getProperty("line.separator"));
+    config.append("read timeout : ").append(rTimeout).append(System.getProperty("line.separator"));
+    config.append("conection timeout : ").append(cTimeout).append(System.getProperty("line.separator"));
+    config.append("custom filter : ").append(customFilter).append(System.getProperty("line.separator"));
+    config.append("fetch mail since : ").append(fetchMailsSince).append(System.getProperty("line.separator"));
+    LOG.info(config.toString());
+  }
+
+  class FolderIterator implements Iterator<Folder> {
+    private Store mailbox;
+    private List<String> topLevelFolders;
+    private List<Folder> folders = null;
+    private Folder lastFolder = null;
+
+    public FolderIterator(Store mailBox) {
+      this.mailbox = mailBox;
+      folders = new ArrayList<Folder>();
+      getTopLevelFolders(mailBox);
+    }
+
+    public boolean hasNext() {
+      return !folders.isEmpty();
+    }
+
+    public Folder next() {
+      try {
+        boolean hasMessages = false;
+        Folder next;
+        do {
+          if (lastFolder != null) {
+            lastFolder.close(false);
+            lastFolder = null;
+          }
+          if (folders.isEmpty()) {
+            mailbox.close();
+            return null;
+          }
+          next = folders.remove(0);
+          if (next != null) {
+            String fullName = next.getFullName();
+            if (!excludeFolder(fullName)) {
+              hasMessages = (next.getType() & Folder.HOLDS_MESSAGES) != 0;
+              next.open(Folder.READ_ONLY);
+              lastFolder = next;
+              LOG.info("Opened folder : " + fullName);
+            }
+            if (recurse && ((next.getType() & Folder.HOLDS_FOLDERS) != 0)) {
+              Folder[] children = next.list();
+              LOG.info("Added its children to list  : ");
+              for (int i = children.length - 1; i >= 0; i--) {
+                folders.add(0, children[i]);
+                LOG.info("child name : " + children[i].getFullName());
+              }
+              if (children.length == 0)
+                LOG.info("NO children : ");
+            }
+          }
+        }
+        while (!hasMessages);
+        return next;
+      } catch (MessagingException e) {
+        //throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
+        //        "Folder open failed", e);
+      }
+      return null;
+    }
+
+    public void remove() {
+      throw new UnsupportedOperationException("Its read only mode...");
+    }
+
+    private void getTopLevelFolders(Store mailBox) {
+      if (folderNames != null)
+        topLevelFolders = Arrays.asList(folderNames.split(","));
+      for (int i = 0; topLevelFolders != null && i < topLevelFolders.size(); i++) {
+        try {
+          folders.add(mailbox.getFolder(topLevelFolders.get(i)));
+        } catch (MessagingException e) {
+          // skip bad ones unless its the last one and still no good folder
+          if (folders.size() == 0 && i == topLevelFolders.size() - 1)
+            throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
+                    "Folder retreival failed");
+        }
+      }
+      if (topLevelFolders == null || topLevelFolders.size() == 0) {
+        try {
+          folders.add(mailBox.getDefaultFolder());
+        } catch (MessagingException e) {
+          throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
+                  "Folder retreival failed");
+        }
+      }
+    }
+
+    private boolean excludeFolder(String name) {
+      for (String s : exclude) {
+        if (name.matches(s))
+          return true;
+      }
+      for (String s : include) {
+        if (name.matches(s))
+          return false;
+      }
+      return include.size() > 0;
+    }
+  }
+
+  class MessageIterator implements Iterator<Message> {
+    private Folder folder;
+    private Message[] messagesInCurBatch;
+    private int current = 0;
+    private int currentBatch = 0;
+    private int batchSize = 0;
+    private int totalInFolder = 0;
+    private boolean doBatching = true;
+
+    public MessageIterator(Folder folder, int batchSize) {
+      try {
+        this.folder = folder;
+        this.batchSize = batchSize;
+        SearchTerm st = getSearchTerm();
+        if (st != null) {
+          doBatching = false;
+          messagesInCurBatch = folder.search(st);
+          totalInFolder = messagesInCurBatch.length;
+          folder.fetch(messagesInCurBatch, fp);
+          current = 0;
+          LOG.info("Total messages : " + totalInFolder);
+          LOG.info("Search criteria applied. Batching disabled");
+        } else {
+          totalInFolder = folder.getMessageCount();
+          LOG.info("Total messages : " + totalInFolder);
+          getNextBatch(batchSize, folder);
+        }
+      } catch (MessagingException e) {
+        throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
+                "Message retreival failed", e);
+      }
+    }
+
+    private void getNextBatch(int batchSize, Folder folder) throws MessagingException {
+      // after each batch invalidate cache
+      if (messagesInCurBatch != null) {
+        for (Message m : messagesInCurBatch) {
+          if (m instanceof IMAPMessage)
+            ((IMAPMessage) m).invalidateHeaders();
+        }
+      }
+      int lastMsg = (currentBatch + 1) * batchSize;
+      lastMsg = lastMsg > totalInFolder ? totalInFolder : lastMsg;
+      messagesInCurBatch = folder.getMessages(currentBatch * batchSize + 1, lastMsg);
+      folder.fetch(messagesInCurBatch, fp);
+      current = 0;
+      currentBatch++;
+      LOG.info("Current Batch  : " + currentBatch);
+      LOG.info("Messages in this batch  : " + messagesInCurBatch.length);
+    }
+
+    public boolean hasNext() {
+      boolean hasMore = current < messagesInCurBatch.length;
+      if (!hasMore && doBatching
+              && currentBatch * batchSize < totalInFolder) {
+        // try next batch
+        try {
+          getNextBatch(batchSize, folder);
+          hasMore = current < messagesInCurBatch.length;
+        } catch (MessagingException e) {
+          throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
+                  "Message retreival failed", e);
+        }
+      }
+      return hasMore;
+    }
+
+    public Message next() {
+      return hasNext() ? messagesInCurBatch[current++] : null;
+    }
+
+    public void remove() {
+      throw new UnsupportedOperationException("Its read only mode...");
+    }
+
+    private SearchTerm getSearchTerm() {
+      if (filters.size() == 0)
+        return null;
+      if (filters.size() == 1)
+        return filters.get(0).getCustomSearch(folder);
+      SearchTerm last = filters.get(0).getCustomSearch(folder);
+      for (int i = 1; i < filters.size(); i++) {
+        CustomFilter filter = filters.get(i);
+        SearchTerm st = filter.getCustomSearch(folder);
+        if (st != null) {
+          last = new AndTerm(last, st);
+        }
+      }
+      return last;
+    }
+  }
+
+  class MailsSinceLastCheckFilter implements CustomFilter {
+
+    private Date since;
+
+    public MailsSinceLastCheckFilter(Date date) {
+      since = date;
+    }
+
+    public SearchTerm getCustomSearch(Folder folder) {
+      return new ReceivedDateTerm(ComparisonTerm.GE, since);
+    }
+  }
+
+  // user settings stored in member variables
+  private String user;
+  private String password;
+  private String host;
+  private String protocol;
+
+  private String folderNames;
+  private List<String> exclude = new ArrayList<String>();
+  private List<String> include = new ArrayList<String>();
+  private boolean recurse;
+
+  private int batchSize;
+  private int fetchSize;
+  private int cTimeout;
+  private int rTimeout;
+
+  private Date fetchMailsSince;
+  private String customFilter;
+
+  private boolean processAttachment = true;
+
+  // holds the current state
+  private Store mailbox;
+  private boolean connected = false;
+  private FolderIterator folderIter;
+  private MessageIterator msgIter;
+  private List<CustomFilter> filters = new ArrayList<CustomFilter>();
+  private static FetchProfile fp = new FetchProfile();
+  private static final Logger LOG = LoggerFactory.getLogger(DataImporter.class);
+
+  // diagnostics
+  private int rowCount = 0;
+
+  static {
+    fp.add(FetchProfile.Item.ENVELOPE);
+    fp.add(FetchProfile.Item.FLAGS);
+    fp.add("X-Mailer");
+  }
+
+  // Fields To Index
+  // single valued
+  private static final String MESSAGE_ID = "messageId";
+  private static final String SUBJECT = "subject";
+  private static final String FROM = "from";
+  private static final String SENT_DATE = "sentDate";
+  private static final String XMAILER = "xMailer";
+  // multi valued
+  private static final String TO_CC_BCC = "allTo";
+  private static final String FLAGS = "flags";
+  private static final String CONTENT = "content";
+  private static final String ATTACHMENT = "attachment";
+  private static final String ATTACHMENT_NAMES = "attachmentNames";
+  // flag values
+  private static final String FLAG_ANSWERED = "answered";
+  private static final String FLAG_DELETED = "deleted";
+  private static final String FLAG_DRAFT = "draft";
+  private static final String FLAG_FLAGGED = "flagged";
+  private static final String FLAG_RECENT = "recent";
+  private static final String FLAG_SEEN = "seen";
+
+  private int getIntFromContext(String prop, int ifNull) {
+    int v = ifNull;
+    try {
+      String val = context.getEntityAttribute(prop);
+      if (val != null) {
+        val = context.replaceTokens(val);
+        v = Integer.valueOf(val);
+      }
+    } catch (NumberFormatException e) {
+      //do nothing
+    }
+    return v;
+  }
+
+  private boolean getBoolFromContext(String prop, boolean ifNull) {
+    boolean v = ifNull;
+    String val = context.getEntityAttribute(prop);
+    if (val != null) {
+      val = context.replaceTokens(val);
+      v = Boolean.valueOf(val);
+    }
+    return v;
+  }
+
+  private String getStringFromContext(String prop, String ifNull) {
+    String v = ifNull;
+    String val = context.getEntityAttribute(prop);
+    if (val != null) {
+      val = context.replaceTokens(val);
+      v = val;
+    }
+    return v;
+  }
+}
diff --git a/contrib/dataimporthandler/src/extras/main/java/org/apache/solr/handler/dataimport/TikaEntityProcessor.java b/contrib/dataimporthandler/src/extras/main/java/org/apache/solr/handler/dataimport/TikaEntityProcessor.java
new file mode 100644
index 0000000..cff0124
--- /dev/null
+++ b/contrib/dataimporthandler/src/extras/main/java/org/apache/solr/handler/dataimport/TikaEntityProcessor.java
@@ -0,0 +1,198 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.solr.handler.dataimport;
+
+import org.apache.commons.io.IOUtils;
+import static org.apache.solr.handler.dataimport.DataImportHandlerException.SEVERE;
+import static org.apache.solr.handler.dataimport.DataImportHandlerException.wrapAndThrow;
+import static org.apache.solr.handler.dataimport.DataImporter.COLUMN;
+import static org.apache.solr.handler.dataimport.XPathEntityProcessor.URL;
+import org.apache.tika.config.TikaConfig;
+import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.AutoDetectParser;
+import org.apache.tika.parser.Parser;
+import org.apache.tika.parser.ParseContext;
+import org.apache.tika.sax.BodyContentHandler;
+import org.apache.tika.sax.ContentHandlerDecorator;
+import org.apache.tika.sax.XHTMLContentHandler;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.xml.sax.Attributes;
+import org.xml.sax.ContentHandler;
+import org.xml.sax.SAXException;
+import org.xml.sax.helpers.DefaultHandler;
+
+import javax.xml.transform.OutputKeys;
+import javax.xml.transform.TransformerConfigurationException;
+import javax.xml.transform.sax.SAXTransformerFactory;
+import javax.xml.transform.sax.TransformerHandler;
+import javax.xml.transform.stream.StreamResult;
+import java.io.File;
+import java.io.InputStream;
+import java.io.StringWriter;
+import java.io.Writer;
+import java.util.HashMap;
+import java.util.Map;
+/**
+ * <p>An implementation of EntityProcessor which reads data from rich docs using Tika
+ *
+ * @version $Id$
+ * @since solr 1.5
+ */
+public class TikaEntityProcessor extends EntityProcessorBase {
+  private TikaConfig tikaConfig;
+  private static final Logger LOG = LoggerFactory.getLogger(TikaEntityProcessor.class);
+  private String format = "text";
+  private boolean done = false;
+  private String parser;
+  static final String AUTO_PARSER = "org.apache.tika.parser.AutoDetectParser";
+
+
+  @Override
+  protected void firstInit(Context context) {
+    String tikaConfigFile = context.getResolvedEntityAttribute("tikaConfig");
+    if (tikaConfigFile == null) {
+      tikaConfig = TikaConfig.getDefaultConfig();
+    } else {
+      File configFile = new File(tikaConfigFile);
+      if (!configFile.isAbsolute()) {
+        configFile = new File(context.getSolrCore().getResourceLoader().getConfigDir(), tikaConfigFile);
+      }
+      try {
+        tikaConfig = new TikaConfig(configFile);
+      } catch (Exception e) {
+        wrapAndThrow (SEVERE, e,"Unable to load Tika Config");
+      }
+    }
+
+    format = context.getResolvedEntityAttribute("format");
+    if(format == null)
+      format = "text";
+    if (!"html".equals(format) && !"xml".equals(format) && !"text".equals(format)&& !"none".equals(format) )
+      throw new DataImportHandlerException(SEVERE, "'format' can be one of text|html|xml|none");
+    parser = context.getResolvedEntityAttribute("parser");
+    if(parser == null) {
+      parser = AUTO_PARSER;
+    }
+    done = false;
+  }
+
+  public Map<String, Object> nextRow() {
+    if(done) return null;
+    Map<String, Object> row = new HashMap<String, Object>();
+    DataSource<InputStream> dataSource = context.getDataSource();
+    InputStream is = dataSource.getData(context.getResolvedEntityAttribute(URL));
+    ContentHandler contentHandler = null;
+    Metadata metadata = new Metadata();
+    StringWriter sw = new StringWriter();
+    try {
+      if ("html".equals(format)) {
+        contentHandler = getHtmlHandler(sw);
+      } else if ("xml".equals(format)) {
+        contentHandler = getXmlContentHandler(sw);
+      } else if ("text".equals(format)) {
+        contentHandler = getTextContentHandler(sw);
+      } else if("none".equals(format)){
+        contentHandler = new DefaultHandler();        
+      }
+    } catch (TransformerConfigurationException e) {
+      wrapAndThrow(SEVERE, e, "Unable to create content handler");
+    }
+    Parser tikaParser = null;
+    if(parser.equals(AUTO_PARSER)){
+      AutoDetectParser parser = new AutoDetectParser();
+      parser.setConfig(tikaConfig);
+      tikaParser = parser;
+    } else {
+      tikaParser = (Parser) context.getSolrCore().getResourceLoader().newInstance(parser);
+    }
+    try {
+      tikaParser.parse(is, contentHandler, metadata , new ParseContext());
+    } catch (Exception e) {
+      if(ABORT.equals(onError)){
+        wrapAndThrow(SEVERE, e, "Unable to read content");
+      } else {
+        LOG.warn("Unable to parse document "+ context.getResolvedEntityAttribute(URL) ,e);
+        return null;
+      }
+    }
+    IOUtils.closeQuietly(is);
+    for (Map<String, String> field : context.getAllEntityFields()) {
+      if (!"true".equals(field.get("meta"))) continue;
+      String col = field.get(COLUMN);
+      String s = metadata.get(col);
+      if (s != null) row.put(col, s);
+    }
+    if(!"none".equals(format) ) row.put("text", sw.toString());
+    done = true;
+    return row;
+  }
+
+  private static ContentHandler getHtmlHandler(Writer writer)
+          throws TransformerConfigurationException {
+    SAXTransformerFactory factory = (SAXTransformerFactory)
+            SAXTransformerFactory.newInstance();
+    TransformerHandler handler = factory.newTransformerHandler();
+    handler.getTransformer().setOutputProperty(OutputKeys.METHOD, "html");
+    handler.setResult(new StreamResult(writer));
+    return new ContentHandlerDecorator(handler) {
+      @Override
+      public void startElement(
+              String uri, String localName, String name, Attributes atts)
+              throws SAXException {
+        if (XHTMLContentHandler.XHTML.equals(uri)) {
+          uri = null;
+        }
+        if (!"head".equals(localName)) {
+          super.startElement(uri, localName, name, atts);
+        }
+      }
+
+      @Override
+      public void endElement(String uri, String localName, String name)
+              throws SAXException {
+        if (XHTMLContentHandler.XHTML.equals(uri)) {
+          uri = null;
+        }
+        if (!"head".equals(localName)) {
+          super.endElement(uri, localName, name);
+        }
+      }
+
+      @Override
+      public void startPrefixMapping(String prefix, String uri) {/*no op*/ }
+
+      @Override
+      public void endPrefixMapping(String prefix) {/*no op*/ }
+    };
+  }
+
+  private static ContentHandler getTextContentHandler(Writer writer) {
+    return new BodyContentHandler(writer);
+  }
+
+  private static ContentHandler getXmlContentHandler(Writer writer)
+          throws TransformerConfigurationException {
+    SAXTransformerFactory factory = (SAXTransformerFactory)
+            SAXTransformerFactory.newInstance();
+    TransformerHandler handler = factory.newTransformerHandler();
+    handler.getTransformer().setOutputProperty(OutputKeys.METHOD, "xml");
+    handler.setResult(new StreamResult(writer));
+    return handler;
+  }
+
+}
diff --git a/contrib/dataimporthandler/src/extras/test/java/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java b/contrib/dataimporthandler/src/extras/test/java/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java
new file mode 100644
index 0000000..5a5220e
--- /dev/null
+++ b/contrib/dataimporthandler/src/extras/test/java/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java
@@ -0,0 +1,211 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.solr.handler.dataimport;
+
+import junit.framework.Assert;
+import org.apache.solr.common.SolrInputDocument;
+import org.junit.Ignore;
+import org.junit.Test;
+
+import java.text.ParseException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+// Test mailbox is like this: foldername(mailcount)
+// top1(2) -> child11(6)
+//         -> child12(0)
+// top2(2) -> child21(1)
+//                 -> grandchild211(2)
+//                 -> grandchild212(1)
+//         -> child22(2)
+
+/**
+ * Test for MailEntityProcessor. The tests are marked as ignored because we'd need a mail server (real or mocked) for
+ * these to work.
+ *
+ * TODO: Find a way to make the tests actually test code
+ *
+ * @version $Id$
+ * @see org.apache.solr.handler.dataimport.MailEntityProcessor
+ * @since solr 1.4
+ */
+public class TestMailEntityProcessor {
+
+  // Credentials
+  private static final String user = "user";
+  private static final String password = "password";
+  private static final String host = "host";
+  private static final String protocol = "imaps";
+
+  private static Map<String, String> paramMap = new HashMap<String, String>();
+
+  @Test
+  @Ignore
+  public void testConnection() {
+    // also tests recurse = false and default settings
+    paramMap.put("folders", "top2");
+    paramMap.put("recurse", "false");
+    paramMap.put("processAttachement", "false");
+    DataImporter di = new DataImporter();
+    di.loadAndInit(getConfigFromMap(paramMap));
+    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
+    ent.isDocRoot = true;
+    DataImporter.RequestParams rp = new DataImporter.RequestParams();
+    rp.command = "full-import";
+    SolrWriterImpl swi = new SolrWriterImpl();
+    di.runCmd(rp, swi);
+    Assert.assertEquals("top1 did not return 2 messages", swi.docs.size(), 2);
+  }
+
+  @Test
+  @Ignore
+  public void testRecursion() {
+    paramMap.put("folders", "top2");
+    paramMap.put("recurse", "true");
+    paramMap.put("processAttachement", "false");
+    DataImporter di = new DataImporter();
+    di.loadAndInit(getConfigFromMap(paramMap));
+    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
+    ent.isDocRoot = true;
+    DataImporter.RequestParams rp = new DataImporter.RequestParams();
+    rp.command = "full-import";
+    SolrWriterImpl swi = new SolrWriterImpl();
+    di.runCmd(rp, swi);
+    Assert.assertEquals("top2 and its children did not return 8 messages", swi.docs.size(), 8);
+  }
+
+  @Test
+  @Ignore
+  public void testExclude() {
+    paramMap.put("folders", "top2");
+    paramMap.put("recurse", "true");
+    paramMap.put("processAttachement", "false");
+    paramMap.put("exclude", ".*grandchild.*");
+    DataImporter di = new DataImporter();
+    di.loadAndInit(getConfigFromMap(paramMap));
+    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
+    ent.isDocRoot = true;
+    DataImporter.RequestParams rp = new DataImporter.RequestParams();
+    rp.command = "full-import";
+    SolrWriterImpl swi = new SolrWriterImpl();
+    di.runCmd(rp, swi);
+    Assert.assertEquals("top2 and its direct children did not return 5 messages", swi.docs.size(), 5);
+  }
+
+  @Test
+  @Ignore
+  public void testInclude() {
+    paramMap.put("folders", "top2");
+    paramMap.put("recurse", "true");
+    paramMap.put("processAttachement", "false");
+    paramMap.put("include", ".*grandchild.*");
+    DataImporter di = new DataImporter();
+    di.loadAndInit(getConfigFromMap(paramMap));
+    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
+    ent.isDocRoot = true;
+    DataImporter.RequestParams rp = new DataImporter.RequestParams();
+    rp.command = "full-import";
+    SolrWriterImpl swi = new SolrWriterImpl();
+    di.runCmd(rp, swi);
+    Assert.assertEquals("top2 and its direct children did not return 3 messages", swi.docs.size(), 3);
+  }
+
+  @Test
+  @Ignore
+  public void testIncludeAndExclude() {
+    paramMap.put("folders", "top1,top2");
+    paramMap.put("recurse", "true");
+    paramMap.put("processAttachement", "false");
+    paramMap.put("exclude", ".*top1.*");
+    paramMap.put("include", ".*grandchild.*");
+    DataImporter di = new DataImporter();
+    di.loadAndInit(getConfigFromMap(paramMap));
+    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
+    ent.isDocRoot = true;
+    DataImporter.RequestParams rp = new DataImporter.RequestParams();
+    rp.command = "full-import";
+    SolrWriterImpl swi = new SolrWriterImpl();
+    di.runCmd(rp, swi);
+    Assert.assertEquals("top2 and its direct children did not return 3 messages", swi.docs.size(), 3);
+  }
+
+  @Test
+  @Ignore
+  public void testFetchTimeSince() throws ParseException {
+    paramMap.put("folders", "top1/child11");
+    paramMap.put("recurse", "true");
+    paramMap.put("processAttachement", "false");
+    paramMap.put("fetchMailsSince", "2008-12-26 00:00:00");
+    DataImporter di = new DataImporter();
+    di.loadAndInit(getConfigFromMap(paramMap));
+    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
+    ent.isDocRoot = true;
+    DataImporter.RequestParams rp = new DataImporter.RequestParams();
+    rp.command = "full-import";
+    SolrWriterImpl swi = new SolrWriterImpl();
+    di.runCmd(rp, swi);
+    Assert.assertEquals("top2 and its direct children did not return 3 messages", swi.docs.size(), 3);
+  }
+
+  private String getConfigFromMap(Map<String, String> params) {
+    String conf =
+            "<dataConfig>" +
+                    "<document>" +
+                    "<entity processor=\"org.apache.solr.handler.dataimport.MailEntityProcessor\" " +
+                    "someconfig" +
+                    "/>" +
+                    "</document>" +
+                    "</dataConfig>";
+    params.put("user", user);
+    params.put("password", password);
+    params.put("host", host);
+    params.put("protocol", protocol);
+    StringBuilder attribs = new StringBuilder("");
+    for (String key : params.keySet())
+      attribs.append(" ").append(key).append("=" + "\"").append(params.get(key)).append("\"");
+    attribs.append(" ");
+    return conf.replace("someconfig", attribs.toString());
+  }
+
+  static class SolrWriterImpl extends SolrWriter {
+    List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
+    Boolean deleteAllCalled;
+    Boolean commitCalled;
+
+    public SolrWriterImpl() {
+      super(null, ".");
+    }
+
+    public boolean upload(SolrInputDocument doc) {
+      return docs.add(doc);
+    }
+
+    public void log(int event, String name, Object row) {
+      // Do nothing
+    }
+
+    public void doDeleteAll() {
+      deleteAllCalled = Boolean.TRUE;
+    }
+
+    public void commit(boolean b) {
+      commitCalled = Boolean.TRUE;
+    }
+  }
+}
diff --git a/contrib/dataimporthandler/src/extras/test/java/org/apache/solr/handler/dataimport/TestTikaEntityProcessor.java b/contrib/dataimporthandler/src/extras/test/java/org/apache/solr/handler/dataimport/TestTikaEntityProcessor.java
new file mode 100644
index 0000000..17e245d
--- /dev/null
+++ b/contrib/dataimporthandler/src/extras/test/java/org/apache/solr/handler/dataimport/TestTikaEntityProcessor.java
@@ -0,0 +1,61 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.solr.handler.dataimport;
+
+import org.junit.After;
+import org.junit.Before;
+
+/**Testcase for TikaEntityProcessor
+ * @version $Id$
+ * @since solr 1.5 
+ */
+public class TestTikaEntityProcessor extends AbstractDataImportHandlerTest {
+
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+  }
+
+  @After
+  public void tearDown() throws Exception {
+    super.tearDown();
+  }
+
+  public String getSchemaFile() {
+    return "dataimport-schema-no-unique-key.xml";
+  }
+
+  public String getSolrConfigFile() {
+    return "dataimport-solrconfig.xml";
+  }
+
+  public void testIndexingWithTikaEntityProcessor() throws Exception {
+    String conf =
+            "<dataConfig>" +
+                    "  <dataSource name=\"binary\" type=\"BinFileDataSource\"/>" +
+                    "  <document>" +
+                    "    <entity processor=\"TikaEntityProcessor\" url=\"../../../../../extraction/src/test/resources/solr-word.pdf\" dataSource=\"binary\">" +
+                    "      <field column=\"Author\" meta=\"true\" name=\"author\"/>" +
+                    "      <field column=\"title\" meta=\"true\" name=\"docTitle\"/>" +
+                    "      <field column=\"text\"/>" +
+                    "     </entity>" +
+                    "  </document>" +
+                    "</dataConfig>";
+    super.runFullImport(conf);
+    assertQ(req("*:*"), "//*[@numFound='1']");
+  }
+}
diff --git a/contrib/dataimporthandler/src/extras/test/resources/solr/conf/dataimport-schema-no-unique-key.xml b/contrib/dataimporthandler/src/extras/test/resources/solr/conf/dataimport-schema-no-unique-key.xml
new file mode 100644
index 0000000..0be581f
--- /dev/null
+++ b/contrib/dataimporthandler/src/extras/test/resources/solr/conf/dataimport-schema-no-unique-key.xml
@@ -0,0 +1,203 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!--  
+ This is the Solr schema file. This file should be named "schema.xml" and
+ should be in the conf directory under the solr home
+ (i.e. ./solr/conf/schema.xml by default) 
+ or located where the classloader for the Solr webapp can find it.
+
+ This example schema is the recommended starting point for users.
+ It should be kept correct and concise, usable out-of-the-box.
+
+ For more information, on how to customize this file, please see
+ http://wiki.apache.org/solr/SchemaXml
+-->
+
+<schema name="test" version="1.2">
+  <!-- attribute "name" is the name of this schema and is only used for display purposes.
+       Applications should change this to reflect the nature of the search collection.
+       version="1.1" is Solr's version number for the schema syntax and semantics.  It should
+       not normally be changed by applications.
+       1.0: multiValued attribute did not exist, all fields are multiValued by nature
+       1.1: multiValued attribute introduced, false by default -->
+
+  <types>
+    <!-- field type definitions. The "name" attribute is
+       just a label to be used by field definitions.  The "class"
+       attribute and any other attributes determine the real
+       behavior of the fieldType.
+         Class names starting with "solr" refer to java classes in the
+       org.apache.solr.analysis package.
+    -->
+
+    <!-- The StrField type is not analyzed, but indexed/stored verbatim.  
+       - StrField and TextField support an optional compressThreshold which
+       limits compression (if enabled in the derived fields) to values which
+       exceed a certain size (in characters).
+    -->
+    <fieldType name="string" class="solr.StrField" sortMissingLast="true" omitNorms="true"/>
+
+    <!-- boolean type: "true" or "false" -->
+    <fieldType name="boolean" class="solr.BoolField" sortMissingLast="true" omitNorms="true"/>
+
+    <!-- The optional sortMissingLast and sortMissingFirst attributes are
+         currently supported on types that are sorted internally as strings.
+       - If sortMissingLast="true", then a sort on this field will cause documents
+         without the field to come after documents with the field,
+         regardless of the requested sort order (asc or desc).
+       - If sortMissingFirst="true", then a sort on this field will cause documents
+         without the field to come before documents with the field,
+         regardless of the requested sort order.
+       - If sortMissingLast="false" and sortMissingFirst="false" (the default),
+         then default lucene sorting will be used which places docs without the
+         field first in an ascending sort and last in a descending sort.
+    -->    
+
+
+    <!-- numeric field types that store and index the text
+         value verbatim (and hence don't support range queries, since the
+         lexicographic ordering isn't equal to the numeric ordering) -->
+    <fieldType name="integer" class="solr.IntField" omitNorms="true"/>
+    <fieldType name="long" class="solr.LongField" omitNorms="true"/>
+    <fieldType name="float" class="solr.FloatField" omitNorms="true"/>
+    <fieldType name="double" class="solr.DoubleField" omitNorms="true"/>
+
+
+    <!-- Numeric field types that manipulate the value into
+         a string value that isn't human-readable in its internal form,
+         but with a lexicographic ordering the same as the numeric ordering,
+         so that range queries work correctly. -->
+    <fieldType name="sint" class="solr.SortableIntField" sortMissingLast="true" omitNorms="true"/>
+    <fieldType name="slong" class="solr.SortableLongField" sortMissingLast="true" omitNorms="true"/>
+    <fieldType name="sfloat" class="solr.SortableFloatField" sortMissingLast="true" omitNorms="true"/>
+    <fieldType name="sdouble" class="solr.SortableDoubleField" sortMissingLast="true" omitNorms="true"/>
+
+
+    <!-- The format for this date field is of the form 1995-12-31T23:59:59Z, and
+         is a more restricted form of the canonical representation of dateTime
+         http://www.w3.org/TR/xmlschema-2/#dateTime    
+         The trailing "Z" designates UTC time and is mandatory.
+         Optional fractional seconds are allowed: 1995-12-31T23:59:59.999Z
+         All other components are mandatory.
+
+         Expressions can also be used to denote calculations that should be
+         performed relative to "NOW" to determine the value, ie...
+
+               NOW/HOUR
+                  ... Round to the start of the current hour
+               NOW-1DAY
+                  ... Exactly 1 day prior to now
+               NOW/DAY+6MONTHS+3DAYS
+                  ... 6 months and 3 days in the future from the start of
+                      the current day
+                      
+         Consult the DateField javadocs for more information.
+      -->
+    <fieldType name="date" class="solr.DateField" sortMissingLast="true" omitNorms="true"/>
+
+
+    <!-- The "RandomSortField" is not used to store or search any
+         data.  You can declare fields of this type it in your schema
+         to generate psuedo-random orderings of your docs for sorting 
+         purposes.  The ordering is generated based on the field name 
+         and the version of the index, As long as the index version
+         remains unchanged, and the same field name is reused,
+         the ordering of the docs will be consistent.  
+         If you want differend psuedo-random orderings of documents,
+         for the same version of the index, use a dynamicField and
+         change the name
+     -->
+    <fieldType name="random" class="solr.RandomSortField" indexed="true" />
+
+    <!-- solr.TextField allows the specification of custom text analyzers
+         specified as a tokenizer and a list of token filters. Different
+         analyzers may be specified for indexing and querying.
+
+         The optional positionIncrementGap puts space between multiple fields of
+         this type on the same document, with the purpose of preventing false phrase
+         matching across fields.
+
+         For more info on customizing your analyzer chain, please see
+         http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters
+     -->
+
+    <!-- One can also specify an existing Analyzer class that has a
+         default constructor via the class attribute on the analyzer element
+    <fieldType name="text_greek" class="solr.TextField">
+      <analyzer class="org.apache.lucene.analysis.el.GreekAnalyzer"/>
+    </fieldType>
+    -->
+
+    <!-- A text field that only splits on whitespace for exact matching of words -->
+    <fieldType name="text_ws" class="solr.TextField" positionIncrementGap="100">
+      <analyzer>
+        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
+      </analyzer>
+    </fieldType>
+
+    <!-- A text field that uses WordDelimiterFilter to enable splitting and matching of
+        words on case-change, alpha numeric boundaries, and non-alphanumeric chars,
+        so that a query of "wifi" or "wi fi" could match a document containing "Wi-Fi".
+        Synonyms and stopwords are customized by external files, and stemming is enabled.
+        Duplicate tokens at the same position (which may result from Stemmed Synonyms or
+        WordDelim parts) are removed.
+        -->
+    <fieldType name="text" class="solr.TextField" positionIncrementGap="100">
+      <analyzer type="index">
+        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
+        <!-- in this example, we will only use synonyms at query time
+        <filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/>
+        -->
+        <!--<filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/>-->
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+        <!--<filter class="solr.EnglishPorterFilterFactory" protected="protwords.txt"/>-->
+        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
+      </analyzer>
+      <analyzer type="query">
+        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
+        <!--<filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>-->
+        <!--<filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/>-->
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="1"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+        <!--<filter class="solr.EnglishPorterFilterFactory" protected="protwords.txt"/>-->
+        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
+      </analyzer>
+    </fieldType>
+    <!-- since fields of this type are by default not stored or indexed, any data added to 
+         them will be ignored outright 
+     --> 
+    <fieldtype name="ignored" stored="false" indexed="false" class="solr.StrField" /> 
+
+ </types>
+
+
+ <fields>
+   <field name="title" type="string" indexed="true" stored="true"/>
+   <field name="author" type="string" indexed="true" stored="true" />
+   <field name="text" type="text" indexed="true" stored="true" />
+   
+ </fields>
+ <!-- field for the QueryParser to use when an explicit fieldname is absent -->
+ <defaultSearchField>text</defaultSearchField>
+
+ <!-- SolrQueryParser configuration: defaultOperator="AND|OR" -->
+ <solrQueryParser defaultOperator="OR"/>
+
+</schema>
diff --git a/contrib/dataimporthandler/src/extras/test/resources/solr/conf/dataimport-solrconfig.xml b/contrib/dataimporthandler/src/extras/test/resources/solr/conf/dataimport-solrconfig.xml
new file mode 100644
index 0000000..ff7f3b8
--- /dev/null
+++ b/contrib/dataimporthandler/src/extras/test/resources/solr/conf/dataimport-solrconfig.xml
@@ -0,0 +1,409 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<config>
+  <!-- Set this to 'false' if you want solr to continue working after it has 
+       encountered an severe configuration error.  In a production environment, 
+       you may want solr to keep working even if one handler is mis-configured.
+
+       You may also set this to false using by setting the system property:
+         -Dsolr.abortOnConfigurationError=false
+     -->
+  <abortOnConfigurationError>${solr.abortOnConfigurationError:true}</abortOnConfigurationError>
+
+  <!-- Used to specify an alternate directory to hold all index data
+       other than the default ./data under the Solr home.
+       If replication is in use, this should match the replication configuration. -->
+       <dataDir>${solr.data.dir:./solr/data}</dataDir>
+
+
+  <indexDefaults>
+   <!-- Values here affect all index writers and act as a default unless overridden. -->
+    <useCompoundFile>false</useCompoundFile>
+
+    <mergeFactor>10</mergeFactor>
+    <!--
+     If both ramBufferSizeMB and maxBufferedDocs is set, then Lucene will flush based on whichever limit is hit first.
+
+     -->
+    <!--<maxBufferedDocs>1000</maxBufferedDocs>-->
+    <!-- Tell Lucene when to flush documents to disk.
+    Giving Lucene more memory for indexing means faster indexing at the cost of more RAM
+
+    If both ramBufferSizeMB and maxBufferedDocs is set, then Lucene will flush based on whichever limit is hit first.
+
+    -->
+    <ramBufferSizeMB>32</ramBufferSizeMB>
+    <maxMergeDocs>2147483647</maxMergeDocs>
+    <maxFieldLength>10000</maxFieldLength>
+    <writeLockTimeout>1000</writeLockTimeout>
+    <commitLockTimeout>10000</commitLockTimeout>
+
+    <!--
+     Expert: Turn on Lucene's auto commit capability.
+
+     TODO: Add recommendations on why you would want to do this.
+
+     NOTE: Despite the name, this value does not have any relation to Solr's autoCommit functionality
+
+     -->
+    <!--<luceneAutoCommit>false</luceneAutoCommit>-->
+    <!--
+     Expert:
+     The Merge Policy in Lucene controls how merging is handled by Lucene.  The default in 2.3 is the LogByteSizeMergePolicy, previous
+     versions used LogDocMergePolicy.
+
+     LogByteSizeMergePolicy chooses segments to merge based on their size.  The Lucene 2.2 default, LogDocMergePolicy chose when
+     to merge based on number of documents
+
+     Other implementations of MergePolicy must have a no-argument constructor
+     -->
+    <!--<mergePolicy>org.apache.lucene.index.LogByteSizeMergePolicy</mergePolicy>-->
+
+    <!--
+     Expert:
+     The Merge Scheduler in Lucene controls how merges are performed.  The ConcurrentMergeScheduler (Lucene 2.3 default)
+      can perform merges in the background using separate threads.  The SerialMergeScheduler (Lucene 2.2 default) does not.
+     -->
+    <!--<mergeScheduler>org.apache.lucene.index.ConcurrentMergeScheduler</mergeScheduler>-->
+
+    <!--
+      As long as Solr is the only process modifying your index, it is
+      safe to use Lucene's in process locking mechanism.  But you may
+      specify one of the other Lucene LockFactory implementations in
+      the event that you have a custom situation.
+      
+      none = NoLockFactory (typically only used with read only indexes)
+      single = SingleInstanceLockFactory (suggested)
+      native = NativeFSLockFactory
+      simple = SimpleFSLockFactory
+
+      ('simple' is the default for backwards compatibility with Solr 1.2)
+    -->
+    <lockType>single</lockType>
+  </indexDefaults>
+
+  <mainIndex>
+    <!-- options specific to the main on-disk lucene index -->
+    <useCompoundFile>false</useCompoundFile>
+    <ramBufferSizeMB>32</ramBufferSizeMB>
+    <mergeFactor>10</mergeFactor>
+    <!-- Deprecated -->
+    <!--<maxBufferedDocs>1000</maxBufferedDocs>-->
+    <maxMergeDocs>2147483647</maxMergeDocs>
+    <maxFieldLength>10000</maxFieldLength>
+
+    <!-- If true, unlock any held write or commit locks on startup. 
+         This defeats the locking mechanism that allows multiple
+         processes to safely access a lucene index, and should be
+         used with care.
+         This is not needed if lock type is 'none' or 'single'
+     -->
+    <unlockOnStartup>false</unlockOnStartup>
+  </mainIndex>
+
+  <!-- the default high-performance update handler -->
+  <updateHandler class="solr.DirectUpdateHandler2">
+
+    <!-- A prefix of "solr." for class names is an alias that
+         causes solr to search appropriate packages, including
+         org.apache.solr.(search|update|request|core|analysis)
+     -->
+
+    <!-- Limit the number of deletions Solr will buffer during doc updating.
+        
+        Setting this lower can help bound memory use during indexing.
+    -->
+    <maxPendingDeletes>100000</maxPendingDeletes>
+
+  </updateHandler>
+
+
+  <query>
+    <!-- Maximum number of clauses in a boolean query... can affect
+        range or prefix queries that expand to big boolean
+        queries.  An exception is thrown if exceeded.  -->
+    <maxBooleanClauses>1024</maxBooleanClauses>
+
+    
+    <!-- Cache used by SolrIndexSearcher for filters (DocSets),
+         unordered sets of *all* documents that match a query.
+         When a new searcher is opened, its caches may be prepopulated
+         or "autowarmed" using data from caches in the old searcher.
+         autowarmCount is the number of items to prepopulate.  For LRUCache,
+         the autowarmed items will be the most recently accessed items.
+       Parameters:
+         class - the SolrCache implementation (currently only LRUCache)
+         size - the maximum number of entries in the cache
+         initialSize - the initial capacity (number of entries) of
+           the cache.  (seel java.util.HashMap)
+         autowarmCount - the number of entries to prepopulate from
+           and old cache.
+         -->
+    <filterCache
+      class="solr.LRUCache"
+      size="512"
+      initialSize="512"
+      autowarmCount="256"/>
+
+   <!-- queryResultCache caches results of searches - ordered lists of
+         document ids (DocList) based on a query, a sort, and the range
+         of documents requested.  -->
+    <queryResultCache
+      class="solr.LRUCache"
+      size="512"
+      initialSize="512"
+      autowarmCount="256"/>
+
+  <!-- documentCache caches Lucene Document objects (the stored fields for each document).
+       Since Lucene internal document ids are transient, this cache will not be autowarmed.  -->
+    <documentCache
+      class="solr.LRUCache"
+      size="512"
+      initialSize="512"
+      autowarmCount="0"/>
+
+    <!-- If true, stored fields that are not requested will be loaded lazily.
+
+    This can result in a significant speed improvement if the usual case is to
+    not load all stored fields, especially if the skipped fields are large compressed
+    text fields.
+    -->
+    <enableLazyFieldLoading>true</enableLazyFieldLoading>
+
+    <!-- Example of a generic cache.  These caches may be accessed by name
+         through SolrIndexSearcher.getCache(),cacheLookup(), and cacheInsert().
+         The purpose is to enable easy caching of user/application level data.
+         The regenerator argument should be specified as an implementation
+         of solr.search.CacheRegenerator if autowarming is desired.  -->
+    <!--
+    <cache name="myUserCache"
+      class="solr.LRUCache"
+      size="4096"
+      initialSize="1024"
+      autowarmCount="1024"
+      regenerator="org.mycompany.mypackage.MyRegenerator"
+      />
+    -->
+
+   <!-- An optimization that attempts to use a filter to satisfy a search.
+         If the requested sort does not include score, then the filterCache
+         will be checked for a filter matching the query. If found, the filter
+         will be used as the source of document ids, and then the sort will be
+         applied to that.
+    <useFilterForSortedQuery>true</useFilterForSortedQuery>
+   -->
+
+   <!-- An optimization for use with the queryResultCache.  When a search
+         is requested, a superset of the requested number of document ids
+         are collected.  For example, if a search for a particular query
+         requests matching documents 10 through 19, and queryWindowSize is 50,
+         then documents 0 through 49 will be collected and cached.  Any further
+         requests in that range can be satisfied via the cache.  -->
+    <queryResultWindowSize>50</queryResultWindowSize>
+    
+    <!-- Maximum number of documents to cache for any entry in the
+         queryResultCache. -->
+    <queryResultMaxDocsCached>200</queryResultMaxDocsCached>
+
+    <!-- This entry enables an int hash representation for filters (DocSets)
+         when the number of items in the set is less than maxSize.  For smaller
+         sets, this representation is more memory efficient, more efficient to
+         iterate over, and faster to take intersections.  -->
+    <HashDocSet maxSize="3000" loadFactor="0.75"/>
+
+    <!-- a newSearcher event is fired whenever a new searcher is being prepared
+         and there is a current searcher handling requests (aka registered). -->
+    <!-- QuerySenderListener takes an array of NamedList and executes a
+         local query request for each NamedList in sequence. -->
+    <listener event="newSearcher" class="solr.QuerySenderListener">
+      <arr name="queries">
+        <lst> <str name="q">solr</str> <str name="start">0</str> <str name="rows">10</str> </lst>
+        <lst> <str name="q">rocks</str> <str name="start">0</str> <str name="rows">10</str> </lst>
+        <lst><str name="q">static newSearcher warming query from solrconfig.xml</str></lst>
+      </arr>
+    </listener>
+
+    <!-- a firstSearcher event is fired whenever a new searcher is being
+         prepared but there is no current registered searcher to handle
+         requests or to gain autowarming data from. -->
+    <listener event="firstSearcher" class="solr.QuerySenderListener">
+      <arr name="queries">
+      </arr>
+    </listener>
+
+    <!-- If a search request comes in and there is no current registered searcher,
+         then immediately register the still warming searcher and use it.  If
+         "false" then all requests will block until the first searcher is done
+         warming. -->
+    <useColdSearcher>false</useColdSearcher>
+
+    <!-- Maximum number of searchers that may be warming in the background
+      concurrently.  An error is returned if this limit is exceeded. Recommend
+      1-2 for read-only slaves, higher for masters w/o cache warming. -->
+    <maxWarmingSearchers>4</maxWarmingSearchers>
+
+  </query>
+
+  <!-- 
+    Let the dispatch filter handler /select?qt=XXX
+    handleSelect=true will use consistent error handling for /select and /update
+    handleSelect=false will use solr1.1 style error formatting
+    -->
+  <requestDispatcher handleSelect="true" >
+    <!--Make sure your system has some authentication before enabling remote streaming!  -->
+    <requestParsers enableRemoteStreaming="false" multipartUploadLimitInKB="2048" />
+        
+    <!-- Set HTTP caching related parameters (for proxy caches and clients).
+          
+         To get the behaviour of Solr 1.2 (ie: no caching related headers)
+         use the never304="true" option and do not specify a value for
+         <cacheControl>
+    -->
+    <httpCaching never304="true">
+    <!--httpCaching lastModifiedFrom="openTime"
+                 etagSeed="Solr"-->
+       <!-- lastModFrom="openTime" is the default, the Last-Modified value
+            (and validation against If-Modified-Since requests) will all be
+            relative to when the current Searcher was opened.
+            You can change it to lastModFrom="dirLastMod" if you want the
+            value to exactly corrispond to when the physical index was last
+            modified.
+               
+            etagSeed="..." is an option you can change to force the ETag
+            header (and validation against If-None-Match requests) to be
+            differnet even if the index has not changed (ie: when making
+            significant changes to your config file)
+
+            lastModifiedFrom and etagSeed are both ignored if you use the
+            never304="true" option.
+       -->
+       <!-- If you include a <cacheControl> directive, it will be used to
+            generate a Cache-Control header, as well as an Expires header
+            if the value contains "max-age="
+               
+            By default, no Cache-Control header is generated.
+
+            You can use the <cacheControl> option even if you have set
+            never304="true"
+       -->
+       <!-- <cacheControl>max-age=30, public</cacheControl> -->
+    </httpCaching>
+  </requestDispatcher>
+  
+      
+  <!-- requestHandler plugins... incoming queries will be dispatched to the
+     correct handler based on the path or the qt (query type) param.
+     Names starting with a '/' are accessed with the a path equal to the 
+     registered name.  Names without a leading '/' are accessed with:
+      http://host/app/select?qt=name
+     If no qt is defined, the requestHandler that declares default="true"
+     will be used.
+  -->
+  <requestHandler name="standard" class="solr.StandardRequestHandler" default="true">
+    <!-- default values for query parameters -->
+     <lst name="defaults">
+       <str name="echoParams">explicit</str>
+       <!-- 
+       <int name="rows">10</int>
+       <str name="fl">*</str>
+       <str name="version">2.1</str>
+        -->
+     </lst>
+  </requestHandler>
+  
+  <requestHandler name="/dataimport" class="org.apache.solr.handler.dataimport.DataImportHandler">
+    <lst name="defaults">
+      <lst name="datasource">
+         <str name="type">MockDataSource</str>
+      </lst>
+    </lst>
+  </requestHandler>
+    
+  <!--
+   
+   Search components are registered to SolrCore and used by Search Handlers
+   
+   By default, the following components are avaliable:
+    
+   <searchComponent name="query"     class="org.apache.solr.handler.component.QueryComponent" />
+   <searchComponent name="facet"     class="org.apache.solr.handler.component.FacetComponent" />
+   <searchComponent name="mlt"       class="org.apache.solr.handler.component.MoreLikeThisComponent" />
+   <searchComponent name="highlight" class="org.apache.solr.handler.component.HighlightComponent" />
+   <searchComponent name="debug"     class="org.apache.solr.handler.component.DebugComponent" />
+  
+   If you register a searchComponent to one of the standard names, that will be used instead.
+  
+   -->
+ 
+  <requestHandler name="/search" class="org.apache.solr.handler.component.SearchHandler">
+    <lst name="defaults">
+      <str name="echoParams">explicit</str>
+    </lst>
+    <!--
+    By default, this will register the following components:
+    
+    <arr name="components">
+      <str>query</str>
+      <str>facet</str>
+      <str>mlt</str>
+      <str>highlight</str>
+      <str>debug</str>
+    </arr>
+    
+    To insert handlers before or after the 'standard' components, use:
+    
+    <arr name="first-components">
+      <str>first</str>
+    </arr>
+    
+    <arr name="last-components">
+      <str>last</str>
+    </arr>
+    
+    -->
+  </requestHandler>
+  
+  <!-- Update request handler.  
+  
+       Note: Since solr1.1 requestHandlers requires a valid content type header if posted in 
+       the body. For example, curl now requires: -H 'Content-type:text/xml; charset=utf-8'
+       The response format differs from solr1.1 formatting and returns a standard error code.
+       
+       To enable solr1.1 behavior, remove the /update handler or change its path
+       
+       "update.processor.class" is the class name for the UpdateRequestProcessor.  It is initalized
+       only once.  This can not be changed for each request.
+    -->
+  <requestHandler name="/update" class="solr.XmlUpdateRequestHandler" >
+    <!--
+    <str name="update.processor.class">org.apache.solr.handler.UpdateRequestProcessor</str>
+    -->
+  </requestHandler>
+  
+  <!-- config for the admin interface --> 
+  <admin>
+    <defaultQuery>*:*</defaultQuery>
+    
+    <!-- configure a healthcheck file for servers behind a loadbalancer
+    <healthcheck type="file">server-enabled</healthcheck>
+    -->
+  </admin>
+
+</config>
+
diff --git a/contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java b/contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java
deleted file mode 100644
index 7e464bf..0000000
--- a/contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java
+++ /dev/null
@@ -1,599 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.solr.handler.dataimport;
-
-import com.sun.mail.imap.IMAPMessage;
-import org.apache.tika.config.TikaConfig;
-import org.apache.tika.utils.ParseUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import javax.mail.*;
-import javax.mail.internet.AddressException;
-import javax.mail.internet.ContentType;
-import javax.mail.internet.InternetAddress;
-import javax.mail.internet.MimeMessage;
-import javax.mail.search.AndTerm;
-import javax.mail.search.ComparisonTerm;
-import javax.mail.search.ReceivedDateTerm;
-import javax.mail.search.SearchTerm;
-import java.io.InputStream;
-import java.text.ParseException;
-import java.text.SimpleDateFormat;
-import java.util.*;
-
-/**
- * An EntityProcessor instance which can index emails along with their attachments from POP3 or IMAP sources. Refer to
- * <a href="http://wiki.apache.org/solr/DataImportHandler">http://wiki.apache.org/solr/DataImportHandler</a> for more
- * details. <b>This API is experimental and subject to change</b>
- *
- * @version $Id$
- * @since solr 1.4
- */
-public class MailEntityProcessor extends EntityProcessorBase {
-
-  public static interface CustomFilter {
-    public SearchTerm getCustomSearch(Folder folder);
-  }
-
-  public void init(Context context) {
-    super.init(context);
-    // set attributes using  XXX getXXXFromContext(attribute, defualtValue);
-    // applies variable resolver and return default if value is not found or null
-    // REQUIRED : connection and folder info
-    user = getStringFromContext("user", null);
-    password = getStringFromContext("password", null);
-    host = getStringFromContext("host", null);
-    protocol = getStringFromContext("protocol", null);
-    folderNames = getStringFromContext("folders", null);
-    // validate
-    if (host == null || protocol == null || user == null || password == null
-            || folderNames == null)
-      throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
-              "'user|password|protocol|host|folders' are required attributes");
-
-    //OPTIONAL : have defaults and are optional
-    recurse = getBoolFromContext("recurse", true);
-    String excludes = getStringFromContext("exclude", "");
-    if (excludes != null && !excludes.trim().equals("")) {
-      exclude = Arrays.asList(excludes.split(","));
-    }
-    String includes = getStringFromContext("include", "");
-    if (includes != null && !includes.trim().equals("")) {
-      include = Arrays.asList(includes.split(","));
-    }
-    batchSize = getIntFromContext("batchSize", 20);
-    customFilter = getStringFromContext("customFilter", "");
-    String s = getStringFromContext("fetchMailsSince", "");
-    if (s != null)
-      try {
-        fetchMailsSince = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").parse(s);
-      } catch (ParseException e) {
-        throw new DataImportHandlerException(DataImportHandlerException.SEVERE, "Invalid value for fetchMailSince: " + s, e);
-      }
-
-    fetchSize = getIntFromContext("fetchSize", 32 * 1024);
-    cTimeout = getIntFromContext("connectTimeout", 30 * 1000);
-    rTimeout = getIntFromContext("readTimeout", 60 * 1000);
-    processAttachment = getBoolFromContext("processAttachement", true);
-
-    logConfig();
-  }
-
-  public Map<String, Object> nextRow() {
-    Message mail;
-    Map<String, Object> row = null;
-    do {
-      // try till there is a valid document or folders get exhausted.
-      // when mail == NULL, it means end of processing
-      mail = getNextMail();
-      if (mail != null)
-        row = getDocumentFromMail(mail);
-    } while (row == null && mail != null);    
-    return row;
-  }
-
-  private Message getNextMail() {
-    if (!connected) {
-      if (!connectToMailBox())
-        return null;
-      connected = true;
-    }
-    if (folderIter == null) {
-      createFilters();
-      folderIter = new FolderIterator(mailbox);
-    }
-    // get next message from the folder
-    // if folder is exhausted get next folder
-    // loop till a valid mail or all folders exhausted.
-    while (msgIter == null || !msgIter.hasNext()) {
-      Folder next = folderIter.hasNext() ? folderIter.next() : null;
-      if (next == null) {
-        return null;
-      }
-      msgIter = new MessageIterator(next, batchSize);
-    }
-    return msgIter.next();
-  }
-
-  private Map<String, Object> getDocumentFromMail(Message mail) {
-    Map<String, Object> row = new HashMap<String, Object>();
-    try {
-      addPartToDocument(mail, row, true);
-      return row;
-    } catch (Exception e) {
-      return null;
-    }
-  }
-
-  public void addPartToDocument(Part part, Map<String, Object> row, boolean outerMost) throws Exception {
-    if (part instanceof Message) {
-      addEnvelopToDocument(part, row);
-    }
-
-    String ct = part.getContentType();
-    ContentType ctype = new ContentType(ct);
-    if (part.isMimeType("multipart/*")) {
-      Multipart mp = (Multipart) part.getContent();
-      int count = mp.getCount();
-      if (part.isMimeType("multipart/alternative"))
-        count = 1;
-      for (int i = 0; i < count; i++)
-        addPartToDocument(mp.getBodyPart(i), row, false);
-    } else if (part.isMimeType("message/rfc822")) {
-      addPartToDocument((Part) part.getContent(), row, false);
-    } else {
-      String disp = part.getDisposition();
-      if (!processAttachment || (disp != null && disp.equalsIgnoreCase(Part.ATTACHMENT)))        return;
-      InputStream is = part.getInputStream();
-      String fileName = part.getFileName();
-      String content = ParseUtils.getStringContent(is, TikaConfig.getDefaultConfig(), ctype.getBaseType().toLowerCase());
-      if (disp != null && disp.equalsIgnoreCase(Part.ATTACHMENT)) {
-        if (row.get(ATTACHMENT) == null)
-          row.put(ATTACHMENT, new ArrayList<String>());
-        List<String> contents = (List<String>) row.get(ATTACHMENT);
-        contents.add(content);
-        row.put(ATTACHMENT, contents);
-        if (row.get(ATTACHMENT_NAMES) == null)
-          row.put(ATTACHMENT_NAMES, new ArrayList<String>());
-        List<String> names = (List<String>) row.get(ATTACHMENT_NAMES);
-        names.add(fileName);
-        row.put(ATTACHMENT_NAMES, names);
-      } else {
-        if (row.get(CONTENT) == null)
-          row.put(CONTENT, new ArrayList<String>());
-        List<String> contents = (List<String>) row.get(CONTENT);
-        contents.add(content);
-        row.put(CONTENT, contents);
-      }
-    }
-  }
-
-  private void addEnvelopToDocument(Part part, Map<String, Object> row) throws MessagingException {
-    MimeMessage mail = (MimeMessage) part;
-    Address[] adresses;
-    if ((adresses = mail.getFrom()) != null && adresses.length > 0)
-      row.put(FROM, adresses[0].toString());
-
-    List<String> to = new ArrayList<String>();
-    if ((adresses = mail.getRecipients(Message.RecipientType.TO)) != null)
-      addAddressToList(adresses, to);
-    if ((adresses = mail.getRecipients(Message.RecipientType.CC)) != null)
-      addAddressToList(adresses, to);
-    if ((adresses = mail.getRecipients(Message.RecipientType.BCC)) != null)
-      addAddressToList(adresses, to);
-    if (to.size() > 0)
-      row.put(TO_CC_BCC, to);
-
-    row.put(MESSAGE_ID, mail.getMessageID());
-    row.put(SUBJECT, mail.getSubject());
-
-    Date d = mail.getSentDate();
-    if (d != null) {
-      row.put(SENT_DATE, d);
-    }
-
-    List<String> flags = new ArrayList<String>();
-    for (Flags.Flag flag : mail.getFlags().getSystemFlags()) {
-      if (flag == Flags.Flag.ANSWERED)
-        flags.add(FLAG_ANSWERED);
-      else if (flag == Flags.Flag.DELETED)
-        flags.add(FLAG_DELETED);
-      else if (flag == Flags.Flag.DRAFT)
-        flags.add(FLAG_DRAFT);
-      else if (flag == Flags.Flag.FLAGGED)
-        flags.add(FLAG_FLAGGED);
-      else if (flag == Flags.Flag.RECENT)
-        flags.add(FLAG_RECENT);
-      else if (flag == Flags.Flag.SEEN)
-        flags.add(FLAG_SEEN);
-    }
-    flags.addAll(Arrays.asList(mail.getFlags().getUserFlags()));
-    row.put(FLAGS, flags);
-
-    String[] hdrs = mail.getHeader("X-Mailer");
-    if (hdrs != null)
-      row.put(XMAILER, hdrs[0]);
-  }
-
-
-  private void addAddressToList(Address[] adresses, List<String> to) throws AddressException {
-    for (Address address : adresses) {
-      to.add(address.toString());
-      InternetAddress ia = (InternetAddress) address;
-      if (ia.isGroup()) {
-        InternetAddress[] group = ia.getGroup(false);
-        for (InternetAddress member : group)
-          to.add(member.toString());
-      }
-    }
-  }
-
-  private boolean connectToMailBox() {
-    try {
-      Properties props = new Properties();
-      props.setProperty("mail.store.protocol", protocol);
-      props.setProperty("mail.imap.fetchsize", "" + fetchSize);
-      props.setProperty("mail.imap.timeout", "" + rTimeout);
-      props.setProperty("mail.imap.connectiontimeout", "" + cTimeout);
-      Session session = Session.getDefaultInstance(props, null);
-      mailbox = session.getStore(protocol);
-      mailbox.connect(host, user, password);
-      LOG.info("Connected to mailbox");
-      return true;
-    } catch (MessagingException e) {
-      throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
-              "Connection failed", e);
-    }
-  }
-
-  private void createFilters() {
-    if (fetchMailsSince != null) {
-      filters.add(new MailsSinceLastCheckFilter(fetchMailsSince));
-    }
-    if (customFilter != null && !customFilter.equals("")) {
-      try {
-        Class cf = Class.forName(customFilter);
-        Object obj = cf.newInstance();
-        if (obj instanceof CustomFilter) {
-          filters.add((CustomFilter) obj);
-        }
-      } catch (Exception e) {
-        throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
-                "Custom filter could not be created", e);
-      }
-    }
-  }
-
-  private void logConfig() {
-    if (!LOG.isInfoEnabled()) return;
-    StringBuffer config = new StringBuffer();
-    config.append("user : ").append(user).append(System.getProperty("line.separator"));
-    config.append("pwd : ").append(password).append(System.getProperty("line.separator"));
-    config.append("protocol : ").append(protocol).append(System.getProperty("line.separator"));
-    config.append("host : ").append(host).append(System.getProperty("line.separator"));
-    config.append("folders : ").append(folderNames).append(System.getProperty("line.separator"));
-    config.append("recurse : ").append(recurse).append(System.getProperty("line.separator"));
-    config.append("exclude : ").append(exclude.toString()).append(System.getProperty("line.separator"));
-    config.append("include : ").append(include.toString()).append(System.getProperty("line.separator"));
-    config.append("batchSize : ").append(batchSize).append(System.getProperty("line.separator"));
-    config.append("fetchSize : ").append(fetchSize).append(System.getProperty("line.separator"));
-    config.append("read timeout : ").append(rTimeout).append(System.getProperty("line.separator"));
-    config.append("conection timeout : ").append(cTimeout).append(System.getProperty("line.separator"));
-    config.append("custom filter : ").append(customFilter).append(System.getProperty("line.separator"));
-    config.append("fetch mail since : ").append(fetchMailsSince).append(System.getProperty("line.separator"));
-    LOG.info(config.toString());
-  }
-
-  class FolderIterator implements Iterator<Folder> {
-    private Store mailbox;
-    private List<String> topLevelFolders;
-    private List<Folder> folders = null;
-    private Folder lastFolder = null;
-
-    public FolderIterator(Store mailBox) {
-      this.mailbox = mailBox;
-      folders = new ArrayList<Folder>();
-      getTopLevelFolders(mailBox);
-    }
-
-    public boolean hasNext() {
-      return !folders.isEmpty();
-    }
-
-    public Folder next() {
-      try {
-        boolean hasMessages = false;
-        Folder next;
-        do {
-          if (lastFolder != null) {
-            lastFolder.close(false);
-            lastFolder = null;
-          }
-          if (folders.isEmpty()) {
-            mailbox.close();
-            return null;
-          }
-          next = folders.remove(0);
-          if (next != null) {
-            String fullName = next.getFullName();
-            if (!excludeFolder(fullName)) {
-              hasMessages = (next.getType() & Folder.HOLDS_MESSAGES) != 0;
-              next.open(Folder.READ_ONLY);
-              lastFolder = next;
-              LOG.info("Opened folder : " + fullName);
-            }
-            if (recurse && ((next.getType() & Folder.HOLDS_FOLDERS) != 0)) {
-              Folder[] children = next.list();
-              LOG.info("Added its children to list  : ");
-              for (int i = children.length - 1; i >= 0; i--) {
-                folders.add(0, children[i]);
-                LOG.info("child name : " + children[i].getFullName());
-              }
-              if (children.length == 0)
-                LOG.info("NO children : ");
-            }
-          }
-        }
-        while (!hasMessages);
-        return next;
-      } catch (MessagingException e) {
-        //throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
-        //        "Folder open failed", e);
-      }
-      return null;
-    }
-
-    public void remove() {
-      throw new UnsupportedOperationException("Its read only mode...");
-    }
-
-    private void getTopLevelFolders(Store mailBox) {
-      if (folderNames != null)
-        topLevelFolders = Arrays.asList(folderNames.split(","));
-      for (int i = 0; topLevelFolders != null && i < topLevelFolders.size(); i++) {
-        try {
-          folders.add(mailbox.getFolder(topLevelFolders.get(i)));
-        } catch (MessagingException e) {
-          // skip bad ones unless its the last one and still no good folder
-          if (folders.size() == 0 && i == topLevelFolders.size() - 1)
-            throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
-                    "Folder retreival failed");
-        }
-      }
-      if (topLevelFolders == null || topLevelFolders.size() == 0) {
-        try {
-          folders.add(mailBox.getDefaultFolder());
-        } catch (MessagingException e) {
-          throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
-                  "Folder retreival failed");
-        }
-      }
-    }
-
-    private boolean excludeFolder(String name) {
-      for (String s : exclude) {
-        if (name.matches(s))
-          return true;
-      }
-      for (String s : include) {
-        if (name.matches(s))
-          return false;
-      }
-      return include.size() > 0;
-    }
-  }
-
-  class MessageIterator implements Iterator<Message> {
-    private Folder folder;
-    private Message[] messagesInCurBatch;
-    private int current = 0;
-    private int currentBatch = 0;
-    private int batchSize = 0;
-    private int totalInFolder = 0;
-    private boolean doBatching = true;
-
-    public MessageIterator(Folder folder, int batchSize) {
-      try {
-        this.folder = folder;
-        this.batchSize = batchSize;
-        SearchTerm st = getSearchTerm();
-        if (st != null) {
-          doBatching = false;
-          messagesInCurBatch = folder.search(st);
-          totalInFolder = messagesInCurBatch.length;
-          folder.fetch(messagesInCurBatch, fp);
-          current = 0;
-          LOG.info("Total messages : " + totalInFolder);
-          LOG.info("Search criteria applied. Batching disabled");
-        } else {
-          totalInFolder = folder.getMessageCount();
-          LOG.info("Total messages : " + totalInFolder);
-          getNextBatch(batchSize, folder);
-        }
-      } catch (MessagingException e) {
-        throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
-                "Message retreival failed", e);
-      }
-    }
-
-    private void getNextBatch(int batchSize, Folder folder) throws MessagingException {
-      // after each batch invalidate cache
-      if (messagesInCurBatch != null) {
-        for (Message m : messagesInCurBatch) {
-          if (m instanceof IMAPMessage)
-            ((IMAPMessage) m).invalidateHeaders();
-        }
-      }
-      int lastMsg = (currentBatch + 1) * batchSize;
-      lastMsg = lastMsg > totalInFolder ? totalInFolder : lastMsg;
-      messagesInCurBatch = folder.getMessages(currentBatch * batchSize + 1, lastMsg);
-      folder.fetch(messagesInCurBatch, fp);
-      current = 0;
-      currentBatch++;
-      LOG.info("Current Batch  : " + currentBatch);
-      LOG.info("Messages in this batch  : " + messagesInCurBatch.length);
-    }
-
-    public boolean hasNext() {
-      boolean hasMore = current < messagesInCurBatch.length;
-      if (!hasMore && doBatching
-              && currentBatch * batchSize < totalInFolder) {
-        // try next batch
-        try {
-          getNextBatch(batchSize, folder);
-          hasMore = current < messagesInCurBatch.length;
-        } catch (MessagingException e) {
-          throw new DataImportHandlerException(DataImportHandlerException.SEVERE,
-                  "Message retreival failed", e);
-        }
-      }
-      return hasMore;
-    }
-
-    public Message next() {
-      return hasNext() ? messagesInCurBatch[current++] : null;
-    }
-
-    public void remove() {
-      throw new UnsupportedOperationException("Its read only mode...");
-    }
-
-    private SearchTerm getSearchTerm() {
-      if (filters.size() == 0)
-        return null;
-      if (filters.size() == 1)
-        return filters.get(0).getCustomSearch(folder);
-      SearchTerm last = filters.get(0).getCustomSearch(folder);
-      for (int i = 1; i < filters.size(); i++) {
-        CustomFilter filter = filters.get(i);
-        SearchTerm st = filter.getCustomSearch(folder);
-        if (st != null) {
-          last = new AndTerm(last, st);
-        }
-      }
-      return last;
-    }
-  }
-
-  class MailsSinceLastCheckFilter implements CustomFilter {
-
-    private Date since;
-
-    public MailsSinceLastCheckFilter(Date date) {
-      since = date;
-    }
-
-    public SearchTerm getCustomSearch(Folder folder) {
-      return new ReceivedDateTerm(ComparisonTerm.GE, since);
-    }
-  }
-
-  // user settings stored in member variables
-  private String user;
-  private String password;
-  private String host;
-  private String protocol;
-
-  private String folderNames;
-  private List<String> exclude = new ArrayList<String>();
-  private List<String> include = new ArrayList<String>();
-  private boolean recurse;
-
-  private int batchSize;
-  private int fetchSize;
-  private int cTimeout;
-  private int rTimeout;
-
-  private Date fetchMailsSince;
-  private String customFilter;
-
-  private boolean processAttachment = true;
-
-  // holds the current state
-  private Store mailbox;
-  private boolean connected = false;
-  private FolderIterator folderIter;
-  private MessageIterator msgIter;
-  private List<CustomFilter> filters = new ArrayList<CustomFilter>();
-  private static FetchProfile fp = new FetchProfile();
-  private static final Logger LOG = LoggerFactory.getLogger(DataImporter.class);
-
-  // diagnostics
-  private int rowCount = 0;
-
-  static {
-    fp.add(FetchProfile.Item.ENVELOPE);
-    fp.add(FetchProfile.Item.FLAGS);
-    fp.add("X-Mailer");
-  }
-
-  // Fields To Index
-  // single valued
-  private static final String MESSAGE_ID = "messageId";
-  private static final String SUBJECT = "subject";
-  private static final String FROM = "from";
-  private static final String SENT_DATE = "sentDate";
-  private static final String XMAILER = "xMailer";
-  // multi valued
-  private static final String TO_CC_BCC = "allTo";
-  private static final String FLAGS = "flags";
-  private static final String CONTENT = "content";
-  private static final String ATTACHMENT = "attachment";
-  private static final String ATTACHMENT_NAMES = "attachmentNames";
-  // flag values
-  private static final String FLAG_ANSWERED = "answered";
-  private static final String FLAG_DELETED = "deleted";
-  private static final String FLAG_DRAFT = "draft";
-  private static final String FLAG_FLAGGED = "flagged";
-  private static final String FLAG_RECENT = "recent";
-  private static final String FLAG_SEEN = "seen";
-
-  private int getIntFromContext(String prop, int ifNull) {
-    int v = ifNull;
-    try {
-      String val = context.getEntityAttribute(prop);
-      if (val != null) {
-        val = context.replaceTokens(val);
-        v = Integer.valueOf(val);
-      }
-    } catch (NumberFormatException e) {
-      //do nothing
-    }
-    return v;
-  }
-
-  private boolean getBoolFromContext(String prop, boolean ifNull) {
-    boolean v = ifNull;
-    String val = context.getEntityAttribute(prop);
-    if (val != null) {
-      val = context.replaceTokens(val);
-      v = Boolean.valueOf(val);
-    }
-    return v;
-  }
-
-  private String getStringFromContext(String prop, String ifNull) {
-    String v = ifNull;
-    String val = context.getEntityAttribute(prop);
-    if (val != null) {
-      val = context.replaceTokens(val);
-      v = val;
-    }
-    return v;
-  }
-}
diff --git a/contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/TikaEntityProcessor.java b/contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/TikaEntityProcessor.java
deleted file mode 100644
index cff0124..0000000
--- a/contrib/dataimporthandler/src/main/java/org/apache/solr/handler/dataimport/TikaEntityProcessor.java
+++ /dev/null
@@ -1,198 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.solr.handler.dataimport;
-
-import org.apache.commons.io.IOUtils;
-import static org.apache.solr.handler.dataimport.DataImportHandlerException.SEVERE;
-import static org.apache.solr.handler.dataimport.DataImportHandlerException.wrapAndThrow;
-import static org.apache.solr.handler.dataimport.DataImporter.COLUMN;
-import static org.apache.solr.handler.dataimport.XPathEntityProcessor.URL;
-import org.apache.tika.config.TikaConfig;
-import org.apache.tika.metadata.Metadata;
-import org.apache.tika.parser.AutoDetectParser;
-import org.apache.tika.parser.Parser;
-import org.apache.tika.parser.ParseContext;
-import org.apache.tika.sax.BodyContentHandler;
-import org.apache.tika.sax.ContentHandlerDecorator;
-import org.apache.tika.sax.XHTMLContentHandler;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.xml.sax.Attributes;
-import org.xml.sax.ContentHandler;
-import org.xml.sax.SAXException;
-import org.xml.sax.helpers.DefaultHandler;
-
-import javax.xml.transform.OutputKeys;
-import javax.xml.transform.TransformerConfigurationException;
-import javax.xml.transform.sax.SAXTransformerFactory;
-import javax.xml.transform.sax.TransformerHandler;
-import javax.xml.transform.stream.StreamResult;
-import java.io.File;
-import java.io.InputStream;
-import java.io.StringWriter;
-import java.io.Writer;
-import java.util.HashMap;
-import java.util.Map;
-/**
- * <p>An implementation of EntityProcessor which reads data from rich docs using Tika
- *
- * @version $Id$
- * @since solr 1.5
- */
-public class TikaEntityProcessor extends EntityProcessorBase {
-  private TikaConfig tikaConfig;
-  private static final Logger LOG = LoggerFactory.getLogger(TikaEntityProcessor.class);
-  private String format = "text";
-  private boolean done = false;
-  private String parser;
-  static final String AUTO_PARSER = "org.apache.tika.parser.AutoDetectParser";
-
-
-  @Override
-  protected void firstInit(Context context) {
-    String tikaConfigFile = context.getResolvedEntityAttribute("tikaConfig");
-    if (tikaConfigFile == null) {
-      tikaConfig = TikaConfig.getDefaultConfig();
-    } else {
-      File configFile = new File(tikaConfigFile);
-      if (!configFile.isAbsolute()) {
-        configFile = new File(context.getSolrCore().getResourceLoader().getConfigDir(), tikaConfigFile);
-      }
-      try {
-        tikaConfig = new TikaConfig(configFile);
-      } catch (Exception e) {
-        wrapAndThrow (SEVERE, e,"Unable to load Tika Config");
-      }
-    }
-
-    format = context.getResolvedEntityAttribute("format");
-    if(format == null)
-      format = "text";
-    if (!"html".equals(format) && !"xml".equals(format) && !"text".equals(format)&& !"none".equals(format) )
-      throw new DataImportHandlerException(SEVERE, "'format' can be one of text|html|xml|none");
-    parser = context.getResolvedEntityAttribute("parser");
-    if(parser == null) {
-      parser = AUTO_PARSER;
-    }
-    done = false;
-  }
-
-  public Map<String, Object> nextRow() {
-    if(done) return null;
-    Map<String, Object> row = new HashMap<String, Object>();
-    DataSource<InputStream> dataSource = context.getDataSource();
-    InputStream is = dataSource.getData(context.getResolvedEntityAttribute(URL));
-    ContentHandler contentHandler = null;
-    Metadata metadata = new Metadata();
-    StringWriter sw = new StringWriter();
-    try {
-      if ("html".equals(format)) {
-        contentHandler = getHtmlHandler(sw);
-      } else if ("xml".equals(format)) {
-        contentHandler = getXmlContentHandler(sw);
-      } else if ("text".equals(format)) {
-        contentHandler = getTextContentHandler(sw);
-      } else if("none".equals(format)){
-        contentHandler = new DefaultHandler();        
-      }
-    } catch (TransformerConfigurationException e) {
-      wrapAndThrow(SEVERE, e, "Unable to create content handler");
-    }
-    Parser tikaParser = null;
-    if(parser.equals(AUTO_PARSER)){
-      AutoDetectParser parser = new AutoDetectParser();
-      parser.setConfig(tikaConfig);
-      tikaParser = parser;
-    } else {
-      tikaParser = (Parser) context.getSolrCore().getResourceLoader().newInstance(parser);
-    }
-    try {
-      tikaParser.parse(is, contentHandler, metadata , new ParseContext());
-    } catch (Exception e) {
-      if(ABORT.equals(onError)){
-        wrapAndThrow(SEVERE, e, "Unable to read content");
-      } else {
-        LOG.warn("Unable to parse document "+ context.getResolvedEntityAttribute(URL) ,e);
-        return null;
-      }
-    }
-    IOUtils.closeQuietly(is);
-    for (Map<String, String> field : context.getAllEntityFields()) {
-      if (!"true".equals(field.get("meta"))) continue;
-      String col = field.get(COLUMN);
-      String s = metadata.get(col);
-      if (s != null) row.put(col, s);
-    }
-    if(!"none".equals(format) ) row.put("text", sw.toString());
-    done = true;
-    return row;
-  }
-
-  private static ContentHandler getHtmlHandler(Writer writer)
-          throws TransformerConfigurationException {
-    SAXTransformerFactory factory = (SAXTransformerFactory)
-            SAXTransformerFactory.newInstance();
-    TransformerHandler handler = factory.newTransformerHandler();
-    handler.getTransformer().setOutputProperty(OutputKeys.METHOD, "html");
-    handler.setResult(new StreamResult(writer));
-    return new ContentHandlerDecorator(handler) {
-      @Override
-      public void startElement(
-              String uri, String localName, String name, Attributes atts)
-              throws SAXException {
-        if (XHTMLContentHandler.XHTML.equals(uri)) {
-          uri = null;
-        }
-        if (!"head".equals(localName)) {
-          super.startElement(uri, localName, name, atts);
-        }
-      }
-
-      @Override
-      public void endElement(String uri, String localName, String name)
-              throws SAXException {
-        if (XHTMLContentHandler.XHTML.equals(uri)) {
-          uri = null;
-        }
-        if (!"head".equals(localName)) {
-          super.endElement(uri, localName, name);
-        }
-      }
-
-      @Override
-      public void startPrefixMapping(String prefix, String uri) {/*no op*/ }
-
-      @Override
-      public void endPrefixMapping(String prefix) {/*no op*/ }
-    };
-  }
-
-  private static ContentHandler getTextContentHandler(Writer writer) {
-    return new BodyContentHandler(writer);
-  }
-
-  private static ContentHandler getXmlContentHandler(Writer writer)
-          throws TransformerConfigurationException {
-    SAXTransformerFactory factory = (SAXTransformerFactory)
-            SAXTransformerFactory.newInstance();
-    TransformerHandler handler = factory.newTransformerHandler();
-    handler.getTransformer().setOutputProperty(OutputKeys.METHOD, "xml");
-    handler.setResult(new StreamResult(writer));
-    return handler;
-  }
-
-}
diff --git a/contrib/dataimporthandler/src/test/java/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java b/contrib/dataimporthandler/src/test/java/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java
deleted file mode 100644
index 5a5220e..0000000
--- a/contrib/dataimporthandler/src/test/java/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java
+++ /dev/null
@@ -1,211 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.solr.handler.dataimport;
-
-import junit.framework.Assert;
-import org.apache.solr.common.SolrInputDocument;
-import org.junit.Ignore;
-import org.junit.Test;
-
-import java.text.ParseException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-// Test mailbox is like this: foldername(mailcount)
-// top1(2) -> child11(6)
-//         -> child12(0)
-// top2(2) -> child21(1)
-//                 -> grandchild211(2)
-//                 -> grandchild212(1)
-//         -> child22(2)
-
-/**
- * Test for MailEntityProcessor. The tests are marked as ignored because we'd need a mail server (real or mocked) for
- * these to work.
- *
- * TODO: Find a way to make the tests actually test code
- *
- * @version $Id$
- * @see org.apache.solr.handler.dataimport.MailEntityProcessor
- * @since solr 1.4
- */
-public class TestMailEntityProcessor {
-
-  // Credentials
-  private static final String user = "user";
-  private static final String password = "password";
-  private static final String host = "host";
-  private static final String protocol = "imaps";
-
-  private static Map<String, String> paramMap = new HashMap<String, String>();
-
-  @Test
-  @Ignore
-  public void testConnection() {
-    // also tests recurse = false and default settings
-    paramMap.put("folders", "top2");
-    paramMap.put("recurse", "false");
-    paramMap.put("processAttachement", "false");
-    DataImporter di = new DataImporter();
-    di.loadAndInit(getConfigFromMap(paramMap));
-    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
-    ent.isDocRoot = true;
-    DataImporter.RequestParams rp = new DataImporter.RequestParams();
-    rp.command = "full-import";
-    SolrWriterImpl swi = new SolrWriterImpl();
-    di.runCmd(rp, swi);
-    Assert.assertEquals("top1 did not return 2 messages", swi.docs.size(), 2);
-  }
-
-  @Test
-  @Ignore
-  public void testRecursion() {
-    paramMap.put("folders", "top2");
-    paramMap.put("recurse", "true");
-    paramMap.put("processAttachement", "false");
-    DataImporter di = new DataImporter();
-    di.loadAndInit(getConfigFromMap(paramMap));
-    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
-    ent.isDocRoot = true;
-    DataImporter.RequestParams rp = new DataImporter.RequestParams();
-    rp.command = "full-import";
-    SolrWriterImpl swi = new SolrWriterImpl();
-    di.runCmd(rp, swi);
-    Assert.assertEquals("top2 and its children did not return 8 messages", swi.docs.size(), 8);
-  }
-
-  @Test
-  @Ignore
-  public void testExclude() {
-    paramMap.put("folders", "top2");
-    paramMap.put("recurse", "true");
-    paramMap.put("processAttachement", "false");
-    paramMap.put("exclude", ".*grandchild.*");
-    DataImporter di = new DataImporter();
-    di.loadAndInit(getConfigFromMap(paramMap));
-    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
-    ent.isDocRoot = true;
-    DataImporter.RequestParams rp = new DataImporter.RequestParams();
-    rp.command = "full-import";
-    SolrWriterImpl swi = new SolrWriterImpl();
-    di.runCmd(rp, swi);
-    Assert.assertEquals("top2 and its direct children did not return 5 messages", swi.docs.size(), 5);
-  }
-
-  @Test
-  @Ignore
-  public void testInclude() {
-    paramMap.put("folders", "top2");
-    paramMap.put("recurse", "true");
-    paramMap.put("processAttachement", "false");
-    paramMap.put("include", ".*grandchild.*");
-    DataImporter di = new DataImporter();
-    di.loadAndInit(getConfigFromMap(paramMap));
-    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
-    ent.isDocRoot = true;
-    DataImporter.RequestParams rp = new DataImporter.RequestParams();
-    rp.command = "full-import";
-    SolrWriterImpl swi = new SolrWriterImpl();
-    di.runCmd(rp, swi);
-    Assert.assertEquals("top2 and its direct children did not return 3 messages", swi.docs.size(), 3);
-  }
-
-  @Test
-  @Ignore
-  public void testIncludeAndExclude() {
-    paramMap.put("folders", "top1,top2");
-    paramMap.put("recurse", "true");
-    paramMap.put("processAttachement", "false");
-    paramMap.put("exclude", ".*top1.*");
-    paramMap.put("include", ".*grandchild.*");
-    DataImporter di = new DataImporter();
-    di.loadAndInit(getConfigFromMap(paramMap));
-    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
-    ent.isDocRoot = true;
-    DataImporter.RequestParams rp = new DataImporter.RequestParams();
-    rp.command = "full-import";
-    SolrWriterImpl swi = new SolrWriterImpl();
-    di.runCmd(rp, swi);
-    Assert.assertEquals("top2 and its direct children did not return 3 messages", swi.docs.size(), 3);
-  }
-
-  @Test
-  @Ignore
-  public void testFetchTimeSince() throws ParseException {
-    paramMap.put("folders", "top1/child11");
-    paramMap.put("recurse", "true");
-    paramMap.put("processAttachement", "false");
-    paramMap.put("fetchMailsSince", "2008-12-26 00:00:00");
-    DataImporter di = new DataImporter();
-    di.loadAndInit(getConfigFromMap(paramMap));
-    DataConfig.Entity ent = di.getConfig().document.entities.get(0);
-    ent.isDocRoot = true;
-    DataImporter.RequestParams rp = new DataImporter.RequestParams();
-    rp.command = "full-import";
-    SolrWriterImpl swi = new SolrWriterImpl();
-    di.runCmd(rp, swi);
-    Assert.assertEquals("top2 and its direct children did not return 3 messages", swi.docs.size(), 3);
-  }
-
-  private String getConfigFromMap(Map<String, String> params) {
-    String conf =
-            "<dataConfig>" +
-                    "<document>" +
-                    "<entity processor=\"org.apache.solr.handler.dataimport.MailEntityProcessor\" " +
-                    "someconfig" +
-                    "/>" +
-                    "</document>" +
-                    "</dataConfig>";
-    params.put("user", user);
-    params.put("password", password);
-    params.put("host", host);
-    params.put("protocol", protocol);
-    StringBuilder attribs = new StringBuilder("");
-    for (String key : params.keySet())
-      attribs.append(" ").append(key).append("=" + "\"").append(params.get(key)).append("\"");
-    attribs.append(" ");
-    return conf.replace("someconfig", attribs.toString());
-  }
-
-  static class SolrWriterImpl extends SolrWriter {
-    List<SolrInputDocument> docs = new ArrayList<SolrInputDocument>();
-    Boolean deleteAllCalled;
-    Boolean commitCalled;
-
-    public SolrWriterImpl() {
-      super(null, ".");
-    }
-
-    public boolean upload(SolrInputDocument doc) {
-      return docs.add(doc);
-    }
-
-    public void log(int event, String name, Object row) {
-      // Do nothing
-    }
-
-    public void doDeleteAll() {
-      deleteAllCalled = Boolean.TRUE;
-    }
-
-    public void commit(boolean b) {
-      commitCalled = Boolean.TRUE;
-    }
-  }
-}
diff --git a/contrib/dataimporthandler/src/test/java/org/apache/solr/handler/dataimport/TestTikaEntityProcessor.java b/contrib/dataimporthandler/src/test/java/org/apache/solr/handler/dataimport/TestTikaEntityProcessor.java
deleted file mode 100644
index 059a2a4..0000000
--- a/contrib/dataimporthandler/src/test/java/org/apache/solr/handler/dataimport/TestTikaEntityProcessor.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.solr.handler.dataimport;
-
-import org.junit.After;
-import org.junit.Before;
-
-/**Testcase for TikaEntityProcessor
- * @version $Id$
- * @since solr 1.5 
- */
-public class TestTikaEntityProcessor extends AbstractDataImportHandlerTest {
-
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-  }
-
-  @After
-  public void tearDown() throws Exception {
-    super.tearDown();
-  }
-
-  public String getSchemaFile() {
-    return "dataimport-schema-no-unique-key.xml";
-  }
-
-  public String getSolrConfigFile() {
-    return "dataimport-solrconfig.xml";
-  }
-
-  public void testIndexingWithTikaEntityProcessor() throws Exception {
-    String conf =
-            "<dataConfig>" +
-                    "  <dataSource name=\"binary\" type=\"BinFileDataSource\"/>" +
-                    "  <document>" +
-                    "    <entity processor=\"TikaEntityProcessor\" url=\"../../../../extraction/src/test/resources/solr-word.pdf\" dataSource=\"binary\">" +
-                    "      <field column=\"Author\" meta=\"true\" name=\"author\"/>" +
-                    "      <field column=\"title\" meta=\"true\" name=\"docTitle\"/>" +
-                    "      <field column=\"text\"/>" +
-                    "     </entity>" +
-                    "  </document>" +
-                    "</dataConfig>";
-    super.runFullImport(conf);
-    assertQ(req("*:*"), "//*[@numFound='1']");
-  }
-}
diff --git a/contrib/dataimporthandler/src/test/resources/solr/conf/dataimport-schema-no-unique-key.xml b/contrib/dataimporthandler/src/test/resources/solr/conf/dataimport-schema-no-unique-key.xml
deleted file mode 100644
index 0be581f..0000000
--- a/contrib/dataimporthandler/src/test/resources/solr/conf/dataimport-schema-no-unique-key.xml
+++ /dev/null
@@ -1,203 +0,0 @@
-<?xml version="1.0" encoding="UTF-8" ?>
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-
-<!--  
- This is the Solr schema file. This file should be named "schema.xml" and
- should be in the conf directory under the solr home
- (i.e. ./solr/conf/schema.xml by default) 
- or located where the classloader for the Solr webapp can find it.
-
- This example schema is the recommended starting point for users.
- It should be kept correct and concise, usable out-of-the-box.
-
- For more information, on how to customize this file, please see
- http://wiki.apache.org/solr/SchemaXml
--->
-
-<schema name="test" version="1.2">
-  <!-- attribute "name" is the name of this schema and is only used for display purposes.
-       Applications should change this to reflect the nature of the search collection.
-       version="1.1" is Solr's version number for the schema syntax and semantics.  It should
-       not normally be changed by applications.
-       1.0: multiValued attribute did not exist, all fields are multiValued by nature
-       1.1: multiValued attribute introduced, false by default -->
-
-  <types>
-    <!-- field type definitions. The "name" attribute is
-       just a label to be used by field definitions.  The "class"
-       attribute and any other attributes determine the real
-       behavior of the fieldType.
-         Class names starting with "solr" refer to java classes in the
-       org.apache.solr.analysis package.
-    -->
-
-    <!-- The StrField type is not analyzed, but indexed/stored verbatim.  
-       - StrField and TextField support an optional compressThreshold which
-       limits compression (if enabled in the derived fields) to values which
-       exceed a certain size (in characters).
-    -->
-    <fieldType name="string" class="solr.StrField" sortMissingLast="true" omitNorms="true"/>
-
-    <!-- boolean type: "true" or "false" -->
-    <fieldType name="boolean" class="solr.BoolField" sortMissingLast="true" omitNorms="true"/>
-
-    <!-- The optional sortMissingLast and sortMissingFirst attributes are
-         currently supported on types that are sorted internally as strings.
-       - If sortMissingLast="true", then a sort on this field will cause documents
-         without the field to come after documents with the field,
-         regardless of the requested sort order (asc or desc).
-       - If sortMissingFirst="true", then a sort on this field will cause documents
-         without the field to come before documents with the field,
-         regardless of the requested sort order.
-       - If sortMissingLast="false" and sortMissingFirst="false" (the default),
-         then default lucene sorting will be used which places docs without the
-         field first in an ascending sort and last in a descending sort.
-    -->    
-
-
-    <!-- numeric field types that store and index the text
-         value verbatim (and hence don't support range queries, since the
-         lexicographic ordering isn't equal to the numeric ordering) -->
-    <fieldType name="integer" class="solr.IntField" omitNorms="true"/>
-    <fieldType name="long" class="solr.LongField" omitNorms="true"/>
-    <fieldType name="float" class="solr.FloatField" omitNorms="true"/>
-    <fieldType name="double" class="solr.DoubleField" omitNorms="true"/>
-
-
-    <!-- Numeric field types that manipulate the value into
-         a string value that isn't human-readable in its internal form,
-         but with a lexicographic ordering the same as the numeric ordering,
-         so that range queries work correctly. -->
-    <fieldType name="sint" class="solr.SortableIntField" sortMissingLast="true" omitNorms="true"/>
-    <fieldType name="slong" class="solr.SortableLongField" sortMissingLast="true" omitNorms="true"/>
-    <fieldType name="sfloat" class="solr.SortableFloatField" sortMissingLast="true" omitNorms="true"/>
-    <fieldType name="sdouble" class="solr.SortableDoubleField" sortMissingLast="true" omitNorms="true"/>
-
-
-    <!-- The format for this date field is of the form 1995-12-31T23:59:59Z, and
-         is a more restricted form of the canonical representation of dateTime
-         http://www.w3.org/TR/xmlschema-2/#dateTime    
-         The trailing "Z" designates UTC time and is mandatory.
-         Optional fractional seconds are allowed: 1995-12-31T23:59:59.999Z
-         All other components are mandatory.
-
-         Expressions can also be used to denote calculations that should be
-         performed relative to "NOW" to determine the value, ie...
-
-               NOW/HOUR
-                  ... Round to the start of the current hour
-               NOW-1DAY
-                  ... Exactly 1 day prior to now
-               NOW/DAY+6MONTHS+3DAYS
-                  ... 6 months and 3 days in the future from the start of
-                      the current day
-                      
-         Consult the DateField javadocs for more information.
-      -->
-    <fieldType name="date" class="solr.DateField" sortMissingLast="true" omitNorms="true"/>
-
-
-    <!-- The "RandomSortField" is not used to store or search any
-         data.  You can declare fields of this type it in your schema
-         to generate psuedo-random orderings of your docs for sorting 
-         purposes.  The ordering is generated based on the field name 
-         and the version of the index, As long as the index version
-         remains unchanged, and the same field name is reused,
-         the ordering of the docs will be consistent.  
-         If you want differend psuedo-random orderings of documents,
-         for the same version of the index, use a dynamicField and
-         change the name
-     -->
-    <fieldType name="random" class="solr.RandomSortField" indexed="true" />
-
-    <!-- solr.TextField allows the specification of custom text analyzers
-         specified as a tokenizer and a list of token filters. Different
-         analyzers may be specified for indexing and querying.
-
-         The optional positionIncrementGap puts space between multiple fields of
-         this type on the same document, with the purpose of preventing false phrase
-         matching across fields.
-
-         For more info on customizing your analyzer chain, please see
-         http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters
-     -->
-
-    <!-- One can also specify an existing Analyzer class that has a
-         default constructor via the class attribute on the analyzer element
-    <fieldType name="text_greek" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.el.GreekAnalyzer"/>
-    </fieldType>
-    -->
-
-    <!-- A text field that only splits on whitespace for exact matching of words -->
-    <fieldType name="text_ws" class="solr.TextField" positionIncrementGap="100">
-      <analyzer>
-        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
-      </analyzer>
-    </fieldType>
-
-    <!-- A text field that uses WordDelimiterFilter to enable splitting and matching of
-        words on case-change, alpha numeric boundaries, and non-alphanumeric chars,
-        so that a query of "wifi" or "wi fi" could match a document containing "Wi-Fi".
-        Synonyms and stopwords are customized by external files, and stemming is enabled.
-        Duplicate tokens at the same position (which may result from Stemmed Synonyms or
-        WordDelim parts) are removed.
-        -->
-    <fieldType name="text" class="solr.TextField" positionIncrementGap="100">
-      <analyzer type="index">
-        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
-        <!-- in this example, we will only use synonyms at query time
-        <filter class="solr.SynonymFilterFactory" synonyms="index_synonyms.txt" ignoreCase="true" expand="false"/>
-        -->
-        <!--<filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/>-->
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-        <!--<filter class="solr.EnglishPorterFilterFactory" protected="protwords.txt"/>-->
-        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
-      </analyzer>
-      <analyzer type="query">
-        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
-        <!--<filter class="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true"/>-->
-        <!--<filter class="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt"/>-->
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="1"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-        <!--<filter class="solr.EnglishPorterFilterFactory" protected="protwords.txt"/>-->
-        <filter class="solr.RemoveDuplicatesTokenFilterFactory"/>
-      </analyzer>
-    </fieldType>
-    <!-- since fields of this type are by default not stored or indexed, any data added to 
-         them will be ignored outright 
-     --> 
-    <fieldtype name="ignored" stored="false" indexed="false" class="solr.StrField" /> 
-
- </types>
-
-
- <fields>
-   <field name="title" type="string" indexed="true" stored="true"/>
-   <field name="author" type="string" indexed="true" stored="true" />
-   <field name="text" type="text" indexed="true" stored="true" />
-   
- </fields>
- <!-- field for the QueryParser to use when an explicit fieldname is absent -->
- <defaultSearchField>text</defaultSearchField>
-
- <!-- SolrQueryParser configuration: defaultOperator="AND|OR" -->
- <solrQueryParser defaultOperator="OR"/>
-
-</schema>

