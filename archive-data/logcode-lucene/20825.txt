GitDiffStart: 7ddfd04ae484cb28c18a409280e74625b284b6bc | Fri Dec 12 15:18:24 2008 +0000
diff --git a/build.xml b/build.xml
index 20ceb7e..c33daf2 100644
--- a/build.xml
+++ b/build.xml
@@ -198,11 +198,12 @@
           <packageset dir="${src}/java" />
           <packageset dir="${src}/webapp/src" />
           <packageset dir="contrib/dataimporthandler/src/main/java" />
-
+          <packageset dir="contrib/extraction/src/main/java" />
           <group title="Core" packages="org.apache.*" />
           <group title="Common" packages="org.apache.solr.common.*" />
           <group title="SolrJ" packages="org.apache.solr.client.solrj*" />
           <group title="contrib: DataImportHandler" packages="org.apache.solr.handler.dataimport*" />
+          <group title="contrib: Solr Cell" packages="org.apache.solr.handler.extraction*" />
         </sources>
       </invoke-javadoc>
     </sequential>
@@ -395,6 +396,7 @@
       <fileset dir="src/java"/>
       <fileset dir="src/webapp/src"/>
       <fileset dir="contrib/dataimporthandler/src/main/java" />
+      <fileset dir="contrib/extraction/src/main/java" />
     </clover-setup>
   </target>
 
@@ -485,6 +487,9 @@
   	
     <solr-jar destfile="${dist}/apache-solr-dataimporthandler-src-${version}.jar"
               basedir="contrib/dataimporthandler/src" />
+
+    <solr-jar destfile="${dist}/apache-solr-cell-src-${version}.jar"
+              basedir="contrib/extraction/src" />
   </target>
 
   <target name="dist-javadoc" description="Creates the Solr javadoc distribution files"
@@ -497,6 +502,8 @@
               basedir="${build.javadoc}/solrj" />
     <solr-jar destfile="${dist}/apache-solr-dataimporthandler-docs-${version}.jar"
               basedir="${build.javadoc}/contrib-solr-dataimporthandler" />
+    <solr-jar destfile="${dist}/apache-solr-cell-docs-${version}.jar"
+              basedir="${build.javadoc}/contrib-solr-cell" />
   </target>
 
   <!-- Creates the solr jar. -->
@@ -668,6 +675,7 @@
     <sign-maven-dependency-artifacts artifact.id="solr-commons-csv"/>
     <sign-maven-artifacts artifact.id="solr-core"/>
     <sign-maven-artifacts artifact.id="solr-dataimporthandler"/>
+    <sign-maven-artifacts artifact.id="solr-cell"/>
     <sign-maven-dependency-artifacts artifact.id="solr-lucene-analyzers"/>
     <sign-maven-dependency-artifacts artifact.id="solr-lucene-core"/>
     <sign-maven-dependency-artifacts artifact.id="solr-lucene-highlighter"/>
@@ -751,6 +759,16 @@
         </artifact-attachments>
       </m2-deploy>
 
+      <m2-deploy pom.xml="contrib/extraction/solr-cell-pom.xml.template"
+                 jar.file="${dist}/apache-solr-cell-${version}.jar">
+
+        <artifact-attachments>
+          <attach file="${dist}/apache-solr-cell-src-${version}.jar" classifier="sources"/>
+          <attach file="${dist}/apache-solr-cell-docs-${version}.jar" classifier="javadoc"/>
+        </artifact-attachments>
+      </m2-deploy>
+
+
       <m2-deploy pom.xml="${src}/maven/solr-core-pom.xml.template"
                  jar.file="${dist}/apache-solr-core-${version}.jar">
 
@@ -796,6 +814,8 @@
       </fileset>
       <fileset dir="contrib/dataimporthandler/src/main/java"/>
       <fileset dir="contrib/dataimporthandler/src/test/java"/>
+      <fileset dir="contrib/extraction/src/main/java"/>
+      <fileset dir="contrib/extraction/src/test/java"/>
     </rat:report>
   </target>
 
diff --git a/contrib/extraction/build.xml b/contrib/extraction/build.xml
index 2cdab0d..5aff808 100644
--- a/contrib/extraction/build.xml
+++ b/contrib/extraction/build.xml
@@ -110,7 +110,11 @@
   </target>
 
   <target name="dist" depends="build">
-
+    <mkdir dir="${solr-path}/dist/solr-cell-lib"/>
+    <copy file="build/${fullnamever}.jar" todir="${solr-path}/dist"/>
+    <copy todir="${solr-path}/dist/solr-cell-lib">
+      <fileset dir="lib"/>
+    </copy>
   </target>
 
   <target name="example" depends="build">
diff --git a/contrib/extraction/lib/tika-0.2-SNAPSHOT.jar b/contrib/extraction/lib/tika-0.2-SNAPSHOT.jar
deleted file mode 100644
index b20b524..0000000
--- a/contrib/extraction/lib/tika-0.2-SNAPSHOT.jar
+++ /dev/null
@@ -1,2 +0,0 @@
-AnyObjectId[16b9a3ed370d5a617d72f0b8935859bf0eac7678] was removed in git history.
-Apache SVN contains full history.
\ No newline at end of file
diff --git a/contrib/extraction/lib/tika-0.2.jar b/contrib/extraction/lib/tika-0.2.jar
new file mode 100644
index 0000000..7a52271
--- /dev/null
+++ b/contrib/extraction/lib/tika-0.2.jar
@@ -0,0 +1,2 @@
+AnyObjectId[65882f20fd59a46c577fbdfd3ddb63f4d49cb71c] was removed in git history.
+Apache SVN contains full history.
\ No newline at end of file
diff --git a/contrib/extraction/solr-cell-pom.xml.template b/contrib/extraction/solr-cell-pom.xml.template
new file mode 100644
index 0000000..c2d07e1
--- /dev/null
+++ b/contrib/extraction/solr-cell-pom.xml.template
@@ -0,0 +1,46 @@
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+
+  <!--
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+  -->
+
+  <modelVersion>4.0.0</modelVersion>
+
+  <parent>
+    <groupId>org.apache.solr</groupId>
+    <artifactId>solr-parent</artifactId>
+    <version>@maven_version@</version>
+  </parent>
+
+  <groupId>org.apache.solr</groupId>
+  <artifactId>solr-cell</artifactId>
+  <name>Apache Solr Content Extraction Library</name>
+  <version>@maven_version@</version>
+  <description>Apache Solr Content Extraction Library integrates Apache Tika content extraction framework into Solr</description>
+  <packaging>jar</packaging>
+
+  <dependencies>
+    <dependency>
+      <groupId>org.apache.tika</groupId>
+      <artifactId>tika</artifactId>
+      <version>0.2</version>
+    </dependency>
+  </dependencies>
+</project>
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingDocumentLoader.java b/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingDocumentLoader.java
deleted file mode 100644
index f58a2aa..0000000
--- a/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingDocumentLoader.java
+++ /dev/null
@@ -1,179 +0,0 @@
-package org.apache.solr.handler;
-
-import org.apache.commons.io.IOUtils;
-import org.apache.solr.common.SolrException;
-import org.apache.solr.common.params.SolrParams;
-import org.apache.solr.common.params.UpdateParams;
-import org.apache.solr.common.util.ContentStream;
-import org.apache.solr.request.SolrQueryRequest;
-import org.apache.solr.request.SolrQueryResponse;
-import org.apache.solr.schema.IndexSchema;
-import org.apache.solr.update.AddUpdateCommand;
-import org.apache.solr.update.processor.UpdateRequestProcessor;
-import org.apache.tika.config.TikaConfig;
-import org.apache.tika.metadata.Metadata;
-import org.apache.tika.parser.AutoDetectParser;
-import org.apache.tika.parser.Parser;
-import org.apache.tika.sax.XHTMLContentHandler;
-import org.apache.tika.sax.xpath.Matcher;
-import org.apache.tika.sax.xpath.MatchingContentHandler;
-import org.apache.tika.sax.xpath.XPathParser;
-import org.apache.xml.serialize.OutputFormat;
-import org.apache.xml.serialize.XMLSerializer;
-import org.xml.sax.ContentHandler;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.StringWriter;
-
-
-/**
- *
- *
- **/
-public class ExtractingDocumentLoader extends ContentStreamLoader {
-
-  /**
-   * XHTML XPath parser.
-   */
-  private static final XPathParser PARSER =
-          new XPathParser("xhtml", XHTMLContentHandler.XHTML);
-
-  final IndexSchema schema;
-  final SolrParams params;
-  final UpdateRequestProcessor processor;
-  protected AutoDetectParser autoDetectParser;
-
-  private final AddUpdateCommand templateAdd;
-
-  protected TikaConfig config;
-  protected SolrContentHandlerFactory factory;
-  //protected Collection<String> dateFormats = DateUtil.DEFAULT_DATE_FORMATS;
-
-  ExtractingDocumentLoader(SolrQueryRequest req, UpdateRequestProcessor processor,
-                           TikaConfig config, SolrContentHandlerFactory factory) {
-    this.params = req.getParams();
-    schema = req.getSchema();
-    this.config = config;
-    this.processor = processor;
-
-    templateAdd = new AddUpdateCommand();
-    templateAdd.allowDups = false;
-    templateAdd.overwriteCommitted = true;
-    templateAdd.overwritePending = true;
-
-    if (params.getBool(UpdateParams.OVERWRITE, true)) {
-      templateAdd.allowDups = false;
-      templateAdd.overwriteCommitted = true;
-      templateAdd.overwritePending = true;
-    } else {
-      templateAdd.allowDups = true;
-      templateAdd.overwriteCommitted = false;
-      templateAdd.overwritePending = false;
-    }
-    //this is lightweight
-    autoDetectParser = new AutoDetectParser(config);
-    this.factory = factory;
-  }
-
-
-  /**
-   * this must be MT safe... may be called concurrently from multiple threads.
-   *
-   * @param
-   * @param
-   */
-  void doAdd(SolrContentHandler handler, AddUpdateCommand template)
-          throws IOException {
-    template.solrDoc = handler.newDocument();
-    processor.processAdd(template);
-  }
-
-  void addDoc(SolrContentHandler handler) throws IOException {
-    templateAdd.indexedId = null;
-    doAdd(handler, templateAdd);
-  }
-
-  /**
-   * @param req
-   * @param stream
-   * @throws java.io.IOException
-   */
-  public void load(SolrQueryRequest req, SolrQueryResponse rsp, ContentStream stream) throws IOException {
-    errHeader = "ExtractingDocumentLoader: " + stream.getSourceInfo();
-    Parser parser = null;
-    String streamType = req.getParams().get(ExtractingParams.STREAM_TYPE, null);
-    if (streamType != null) {
-      //Cache?  Parsers are lightweight to construct and thread-safe, so I'm told
-      parser = config.getParser(streamType.trim().toLowerCase());
-    } else {
-      parser = autoDetectParser;
-    }
-    if (parser != null) {
-      Metadata metadata = new Metadata();
-      metadata.add(ExtractingMetadataConstants.STREAM_NAME, stream.getName());
-      metadata.add(ExtractingMetadataConstants.STREAM_SOURCE_INFO, stream.getSourceInfo());
-      metadata.add(ExtractingMetadataConstants.STREAM_SIZE, String.valueOf(stream.getSize()));
-      metadata.add(ExtractingMetadataConstants.STREAM_CONTENT_TYPE, stream.getContentType());
-
-      // If you specify the resource name (the filename, roughly) with this parameter,
-      // then Tika can make use of it in guessing the appropriate MIME type:
-      String resourceName = req.getParams().get(ExtractingParams.RESOURCE_NAME, null);
-      if (resourceName != null) {
-        metadata.add(Metadata.RESOURCE_NAME_KEY, resourceName);
-      }
-
-      SolrContentHandler handler = factory.createSolrContentHandler(metadata, params, schema);
-      InputStream inputStream = null;
-      try {
-        inputStream = stream.getStream();
-        String xpathExpr = params.get(ExtractingParams.XPATH_EXPRESSION);
-        boolean extractOnly = params.getBool(ExtractingParams.EXTRACT_ONLY, false);
-        ContentHandler parsingHandler = handler;
-
-        StringWriter writer = null;
-        XMLSerializer serializer = null;
-        if (extractOnly == true) {
-          writer = new StringWriter();
-          serializer = new XMLSerializer(writer, new OutputFormat("XML", "UTF-8", true));
-          if (xpathExpr != null) {
-            Matcher matcher =
-                    PARSER.parse(xpathExpr);
-            serializer.startDocument();//The MatchingContentHandler does not invoke startDocument.  See http://tika.markmail.org/message/kknu3hw7argwiqin
-            parsingHandler = new MatchingContentHandler(serializer, matcher);
-          } else {
-            parsingHandler = serializer;
-          }
-        } else if (xpathExpr != null) {
-          Matcher matcher =
-                  PARSER.parse(xpathExpr);
-          parsingHandler = new MatchingContentHandler(handler, matcher);
-        } //else leave it as is
-
-        //potentially use a wrapper handler for parsing, but we still need the SolrContentHandler for getting the document.
-        parser.parse(inputStream, parsingHandler, metadata);
-        if (extractOnly == false) {
-          addDoc(handler);
-        } else {
-          //serializer is not null, so we need to call endDoc on it if using xpath
-          if (xpathExpr != null){
-            serializer.endDocument();
-          }
-          rsp.add(stream.getName(), writer.toString());
-          writer.close();
-
-        }
-      } catch (Exception e) {
-        //TODO: handle here with an option to not fail and just log the exception
-        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);
-
-      } finally {
-        IOUtils.closeQuietly(inputStream);
-      }
-    } else {
-      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "Stream type of " + streamType + " didn't match any known parsers.  Please supply the " + ExtractingParams.STREAM_TYPE + " parameter.");
-    }
-  }
-
-
-}
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingMetadataConstants.java b/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingMetadataConstants.java
deleted file mode 100644
index f845a71..0000000
--- a/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingMetadataConstants.java
+++ /dev/null
@@ -1,13 +0,0 @@
-package org.apache.solr.handler;
-
-
-/**
- *
- *
- **/
-public interface ExtractingMetadataConstants {
-  String STREAM_NAME = "stream_name";
-  String STREAM_SOURCE_INFO = "stream_source_info";
-  String STREAM_SIZE = "stream_size";
-  String STREAM_CONTENT_TYPE = "stream_content_type";
-}
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingParams.java b/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingParams.java
deleted file mode 100644
index d88b12b..0000000
--- a/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingParams.java
+++ /dev/null
@@ -1,125 +0,0 @@
-package org.apache.solr.handler;
-
-
-/**
- * The various parameters to use when extracting content.
- *
- **/
-public interface ExtractingParams {
-
-  public static final String EXTRACTING_PREFIX = "ext.";
-
-  /**
-   * The param prefix for mapping Tika metadata to Solr fields.
-   * <p/>
-   * To map a field, add a name like:
-   * <pre>ext.map.title=solr.title</pre>
-   *
-   * In this example, the tika "title" metadata value will be added to a Solr field named "solr.title"
-   *
-   *
-   */
-  public static final String MAP_PREFIX = EXTRACTING_PREFIX + "map.";
-
-  /**
-   * The boost value for the name of the field.  The boost can be specified by a name mapping.
-   * <p/>
-   * For example
-   * <pre>
-   * ext.map.title=solr.title
-   * ext.boost.solr.title=2.5
-   * </pre>
-   * will boost the solr.title field for this document by 2.5
-   *
-   */
-  public static final String BOOST_PREFIX = EXTRACTING_PREFIX + "boost.";
-
-  /**
-   * Pass in literal values to be added to the document, as in
-   * <pre>
-   *  ext.literal.myField=Foo 
-   * </pre>
-   *
-   */
-  public static final String LITERALS_PREFIX = EXTRACTING_PREFIX + "literal.";
-
-
-  /**
-   * Restrict the extracted parts of a document to be indexed
-   *  by passing in an XPath expression.  All content that satisfies the XPath expr.
-   * will be passed to the {@link org.apache.solr.handler.SolrContentHandler}.
-   * <p/>
-   * See Tika's docs for what the extracted document looks like.
-   * <p/>
-   * @see #DEFAULT_FIELDNAME
-   * @see #CAPTURE_FIELDS
-   */
-  public static final String XPATH_EXPRESSION = EXTRACTING_PREFIX + "xpath";
-
-
-  /**
-   * Only extract and return the document, do not index it.
-   */
-  public static final String EXTRACT_ONLY = EXTRACTING_PREFIX + "extract.only";
-
-  /**
-    *  Don't throw an exception if a field doesn't exist, just ignore it
-   */
-  public static final String IGNORE_UNDECLARED_FIELDS = EXTRACTING_PREFIX + "ignore.und.fl";
-
-  /**
-   * Index attributes separately according to their name, instead of just adding them to the string buffer
-   */
-  public static final String INDEX_ATTRIBUTES = EXTRACTING_PREFIX + "idx.attr";
-
-  /**
-   * The field to index the contents to by default.  If you want to capture a specific piece
-   * of the Tika document separately, see {@link #CAPTURE_FIELDS}.
-   *
-   * @see #CAPTURE_FIELDS
-   */
-  public static final String DEFAULT_FIELDNAME = EXTRACTING_PREFIX + "def.fl";
-
-  /**
-   * Capture the specified fields (and everything included below it that isn't capture by some other capture field) separately from the default.  This is different
-   * then the case of passing in an XPath expression.
-   * <p/>
-   * The Capture field is based on the localName returned to the {@link org.apache.solr.handler.SolrContentHandler}
-   * by Tika, not to be confused by the mapped field.  The field name can then
-   * be mapped into the index schema.
-   * <p/>
-   * For instance, a Tika document may look like:
-   * <pre>
-   *  &lt;html&gt;
-   *    ...
-   *    &lt;body&gt;
-   *      &lt;p&gt;some text here.  &lt;div&gt;more text&lt;/div&gt;&lt;/p&gt;
-   *      Some more text
-   *    &lt;/body&gt;
-   * </pre>
-   * By passing in the p tag, you could capture all P tags separately from the rest of the text.
-   * Thus, in the example, the capture of the P tag would be: "some text here.  more text"
-   *
-   * @see #DEFAULT_FIELDNAME
-   */
-  public static final String CAPTURE_FIELDS = EXTRACTING_PREFIX + "capture";
-
-  /**
-   * The type of the stream.  If not specified, Tika will use mime type detection.
-   */
-  public static final String STREAM_TYPE = EXTRACTING_PREFIX + "stream.type";
-
-
-  /**
-   * Optional.  The file name. If specified, Tika can take this into account while
-   * guessing the MIME type.
-   */
-  public static final String RESOURCE_NAME = EXTRACTING_PREFIX + "resource.name";
-
-
-  /**
-   * Optional.  If specified, the prefix will be prepended to all Metadata, such that it would be possible
-   * to setup a dynamic field to automatically capture it
-   */
-  public static final String METADATA_PREFIX = EXTRACTING_PREFIX + "metadata.prefix";
-}
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingRequestHandler.java b/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingRequestHandler.java
deleted file mode 100644
index 2d00aeb..0000000
--- a/contrib/extraction/src/main/java/org/apache/solr/handler/ExtractingRequestHandler.java
+++ /dev/null
@@ -1,134 +0,0 @@
-package org.apache.solr.handler;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.solr.common.SolrException;
-import org.apache.solr.common.SolrException.ErrorCode;
-import org.apache.solr.common.util.DateUtil;
-import org.apache.solr.common.util.NamedList;
-import org.apache.solr.core.SolrCore;
-import org.apache.solr.request.SolrQueryRequest;
-import org.apache.solr.update.processor.UpdateRequestProcessor;
-import org.apache.solr.util.plugin.SolrCoreAware;
-import org.apache.tika.config.TikaConfig;
-import org.apache.tika.exception.TikaException;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.File;
-import java.util.Collection;
-import java.util.HashSet;
-
-
-/**
- * Handler for rich documents like PDF or Word or any other file format that Tika handles that need the text to be extracted
- * first from the document.
- * <p/>
- */
-
-public class ExtractingRequestHandler extends ContentStreamHandlerBase implements SolrCoreAware {
-
-  private transient static Logger log = LoggerFactory.getLogger(ExtractingRequestHandler.class);
-
-  public static final String CONFIG_LOCATION = "tika.config";
-  public static final String DATE_FORMATS = "date.formats";
-
-  protected TikaConfig config;
-
-
-  protected Collection<String> dateFormats = DateUtil.DEFAULT_DATE_FORMATS;
-  protected SolrContentHandlerFactory factory;
-
-
-  @Override
-  public void init(NamedList args) {
-    super.init(args);
-  }
-
-  public void inform(SolrCore core) {
-    if (initArgs != null) {
-      //if relative,then relative to config dir, otherwise, absolute path
-      String tikaConfigLoc = (String) initArgs.get(CONFIG_LOCATION);
-      if (tikaConfigLoc != null) {
-        File configFile = new File(tikaConfigLoc);
-        if (configFile.isAbsolute() == false) {
-          configFile = new File(core.getResourceLoader().getConfigDir(), configFile.getPath());
-        }
-        try {
-          config = new TikaConfig(configFile);
-        } catch (Exception e) {
-          throw new SolrException(ErrorCode.SERVER_ERROR, e);
-        }
-      } else {
-        try {
-          config = TikaConfig.getDefaultConfig();
-        } catch (TikaException e) {
-          throw new SolrException(ErrorCode.SERVER_ERROR, e);
-        }
-      }
-      NamedList configDateFormats = (NamedList) initArgs.get(DATE_FORMATS);
-      if (configDateFormats != null && configDateFormats.size() > 0) {
-        dateFormats = new HashSet<String>();
-        while (configDateFormats.iterator().hasNext()) {
-          String format = (String) configDateFormats.iterator().next();
-          log.info("Adding Date Format: " + format);
-          dateFormats.add(format);
-        }
-      }
-    } else {
-      try {
-        config = TikaConfig.getDefaultConfig();
-      } catch (TikaException e) {
-        throw new SolrException(ErrorCode.SERVER_ERROR, e);
-      }
-    }
-    factory = createFactory();
-  }
-
-  protected SolrContentHandlerFactory createFactory() {
-    return new SolrContentHandlerFactory(dateFormats);
-  }
-
-
-  protected ContentStreamLoader newLoader(SolrQueryRequest req, UpdateRequestProcessor processor) {
-    return new ExtractingDocumentLoader(req, processor, config, factory);
-  }
-
-  // ////////////////////// SolrInfoMBeans methods //////////////////////
-  @Override
-  public String getDescription() {
-    return "Add/Update Rich document";
-  }
-
-  @Override
-  public String getVersion() {
-    return "$Revision:$";
-  }
-
-  @Override
-  public String getSourceId() {
-    return "$Id:$";
-  }
-
-  @Override
-  public String getSource() {
-    return "$URL:$";
-  }
-}
-
-
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/SolrContentHandler.java b/contrib/extraction/src/main/java/org/apache/solr/handler/SolrContentHandler.java
deleted file mode 100644
index b25ee14..0000000
--- a/contrib/extraction/src/main/java/org/apache/solr/handler/SolrContentHandler.java
+++ /dev/null
@@ -1,353 +0,0 @@
-package org.apache.solr.handler;
-
-import org.apache.solr.common.SolrException;
-import org.apache.solr.common.SolrInputDocument;
-import org.apache.solr.common.SolrInputField;
-import org.apache.solr.common.params.SolrParams;
-import org.apache.solr.common.util.DateUtil;
-import org.apache.solr.schema.DateField;
-import org.apache.solr.schema.IndexSchema;
-import org.apache.solr.schema.SchemaField;
-import org.apache.solr.schema.StrField;
-import org.apache.solr.schema.TextField;
-import org.apache.solr.schema.FieldType;
-import org.apache.solr.schema.UUIDField;
-import org.apache.tika.metadata.Metadata;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.xml.sax.Attributes;
-import org.xml.sax.SAXException;
-import org.xml.sax.helpers.DefaultHandler;
-
-import java.text.DateFormat;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.Date;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Stack;
-import java.util.UUID;
-
-
-/**
- * This class is not thread-safe.  It is responsible for responding to Tika extraction events and producing a Solr document
- */
-public class SolrContentHandler extends DefaultHandler implements ExtractingParams {
-  private transient static Logger log = LoggerFactory.getLogger(SolrContentHandler.class);
-  protected SolrInputDocument document;
-
-  protected Collection<String> dateFormats = DateUtil.DEFAULT_DATE_FORMATS;
-
-  protected Metadata metadata;
-  protected SolrParams params;
-  protected StringBuilder catchAllBuilder = new StringBuilder(2048);
-  //private StringBuilder currentBuilder;
-  protected IndexSchema schema;
-  //create empty so we don't have to worry about null checks
-  protected Map<String, StringBuilder> fieldBuilders = Collections.emptyMap();
-  protected Stack<StringBuilder> bldrStack = new Stack<StringBuilder>();
-
-  protected boolean ignoreUndeclaredFields = false;
-  protected boolean indexAttribs = false;
-  protected String defaultFieldName;
-
-  protected String metadataPrefix = "";
-
-  /**
-   * Only access through getNextId();
-   */
-  private static long identifier = Long.MIN_VALUE;
-
-
-  public SolrContentHandler(Metadata metadata, SolrParams params, IndexSchema schema) {
-    this(metadata, params, schema, DateUtil.DEFAULT_DATE_FORMATS);
-  }
-
-
-  public SolrContentHandler(Metadata metadata, SolrParams params,
-                            IndexSchema schema, Collection<String> dateFormats) {
-    document = new SolrInputDocument();
-    this.metadata = metadata;
-    this.params = params;
-    this.schema = schema;
-    this.dateFormats = dateFormats;
-    this.ignoreUndeclaredFields = params.getBool(ExtractingParams.IGNORE_UNDECLARED_FIELDS, false);
-    this.indexAttribs = params.getBool(ExtractingParams.INDEX_ATTRIBUTES, false);
-    this.defaultFieldName = params.get(ExtractingParams.DEFAULT_FIELDNAME);
-    this.metadataPrefix = params.get(ExtractingParams.METADATA_PREFIX, "");
-    //if there's no default field and we are intending to index, then throw an exception
-    if (defaultFieldName == null && params.getBool(ExtractingParams.EXTRACT_ONLY, false) == false) {
-      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "No default field name specified");
-    }
-    String[] captureFields = params.getParams(ExtractingParams.CAPTURE_FIELDS);
-    if (captureFields != null && captureFields.length > 0) {
-      fieldBuilders = new HashMap<String, StringBuilder>();
-      for (int i = 0; i < captureFields.length; i++) {
-        fieldBuilders.put(captureFields[i], new StringBuilder());
-      }
-    }
-    bldrStack.push(catchAllBuilder);
-  }
-
-
-  /**
-   * This is called by a consumer when it is ready to deal with a new SolrInputDocument.  Overriding
-   * classes can use this hook to add in or change whatever they deem fit for the document at that time.
-   * The base implementation adds the metadata as fields, allowing for potential remapping.
-   *
-   * @return The {@link org.apache.solr.common.SolrInputDocument}.
-   */
-  public SolrInputDocument newDocument() {
-    float boost = 1.0f;
-    //handle the metadata extracted from the document
-    for (String name : metadata.names()) {
-      String[] vals = metadata.getValues(name);
-      name = findMappedMetadataName(name);
-      SchemaField schFld = schema.getFieldOrNull(name);
-      if (schFld != null) {
-        boost = getBoost(name);
-        if (schFld.multiValued()) {
-          for (int i = 0; i < vals.length; i++) {
-            String val = vals[i];
-            document.addField(name, transformValue(val, schFld), boost);
-          }
-        } else {
-          StringBuilder builder = new StringBuilder();
-          for (int i = 0; i < vals.length; i++) {
-            builder.append(vals[i]).append(' ');
-          }
-          document.addField(name, transformValue(builder.toString().trim(), schFld), boost);
-        }
-      } else {
-        //TODO: error or log?
-        if (ignoreUndeclaredFields == false) {
-          // Arguably we should handle this as a special case. Why? Because unlike basically
-          // all the other fields in metadata, this one was probably set not by Tika by in
-          // ExtractingDocumentLoader.load(). You shouldn't have to define a mapping for this
-          // field just because you specified a resource.name parameter to the handler, should
-          // you?
-          if (name != Metadata.RESOURCE_NAME_KEY) {
-            throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Invalid field: " + name);
-          }
-        }
-      }
-    }
-    //handle the literals from the params
-    Iterator<String> paramNames = params.getParameterNamesIterator();
-    while (paramNames.hasNext()) {
-      String name = paramNames.next();
-      if (name.startsWith(LITERALS_PREFIX)) {
-        String fieldName = name.substring(LITERALS_PREFIX.length());
-        //no need to map names here, since they are literals from the user
-        SchemaField schFld = schema.getFieldOrNull(fieldName);
-        if (schFld != null) {
-          String value = params.get(name);
-          boost = getBoost(fieldName);
-          //no need to transform here, b/c we can assume the user sent it in correctly
-          document.addField(fieldName, value, boost);
-        } else {
-          handleUndeclaredField(fieldName);
-        }
-      }
-    }
-    //add in the content
-    document.addField(defaultFieldName, catchAllBuilder.toString(), getBoost(defaultFieldName));
-
-    //add in the captured content
-    for (Map.Entry<String, StringBuilder> entry : fieldBuilders.entrySet()) {
-      if (entry.getValue().length() > 0) {
-        String fieldName = findMappedName(entry.getKey());
-        SchemaField schFld = schema.getFieldOrNull(fieldName);
-        if (schFld != null) {
-          document.addField(fieldName, transformValue(entry.getValue().toString(), schFld), getBoost(fieldName));
-        } else {
-          handleUndeclaredField(fieldName);
-        }
-      }
-    }
-    //make sure we have a unique id, if one is needed
-    SchemaField uniqueField = schema.getUniqueKeyField();
-    if (uniqueField != null) {
-      String uniqueFieldName = uniqueField.getName();
-      SolrInputField uniqFld = document.getField(uniqueFieldName);
-      if (uniqFld == null) {
-        String uniqId = generateId(uniqueField);
-        if (uniqId != null) {
-          document.addField(uniqueFieldName, uniqId);
-        }
-      }
-    }
-    if (log.isDebugEnabled()) {
-      log.debug("Doc: " + document);
-    }
-    return document;
-  }
-
-  /**
-   * Generate an ID for the document.  First try to get
-   * {@link org.apache.solr.handler.ExtractingMetadataConstants#STREAM_NAME} from the
-   * {@link org.apache.tika.metadata.Metadata}, then try {@link ExtractingMetadataConstants#STREAM_SOURCE_INFO}
-   * then try {@link org.apache.tika.metadata.Metadata#IDENTIFIER}.
-   * If those all are null, then generate a random UUID using {@link java.util.UUID#randomUUID()}.
-   *
-   * @param uniqueField The SchemaField representing the unique field.
-   * @return The id as a string
-   */
-  protected String generateId(SchemaField uniqueField) {
-    //we don't have a unique field specified, so let's add one
-    String uniqId = null;
-    FieldType type = uniqueField.getType();
-    if (type instanceof StrField || type instanceof TextField) {
-      uniqId = metadata.get(ExtractingMetadataConstants.STREAM_NAME);
-      if (uniqId == null) {
-        uniqId = metadata.get(ExtractingMetadataConstants.STREAM_SOURCE_INFO);
-      }
-      if (uniqId == null) {
-        uniqId = metadata.get(Metadata.IDENTIFIER);
-      }
-      if (uniqId == null) {
-        //last chance, just create one
-        uniqId = UUID.randomUUID().toString();
-      }
-    } else if (type instanceof UUIDField){
-      uniqId = UUID.randomUUID().toString();
-    }
-    else {
-      uniqId = String.valueOf(getNextId());
-    }
-    return uniqId;
-  }
-
-
-  @Override
-  public void startDocument() throws SAXException {
-    document.clear();
-    catchAllBuilder.setLength(0);
-    for (StringBuilder builder : fieldBuilders.values()) {
-      builder.setLength(0);
-    }
-    bldrStack.clear();
-    bldrStack.push(catchAllBuilder);
-  }
-
-
-  @Override
-  public void startElement(String uri, String localName, String qName, Attributes attributes) throws SAXException {
-    StringBuilder theBldr = fieldBuilders.get(localName);
-    if (theBldr != null) {
-      //we need to switch the currentBuilder
-      bldrStack.push(theBldr);
-    }
-    if (indexAttribs == true) {
-      for (int i = 0; i < attributes.getLength(); i++) {
-        String fieldName = findMappedName(localName);
-        SchemaField schFld = schema.getFieldOrNull(fieldName);
-        if (schFld != null) {
-          document.addField(fieldName, transformValue(attributes.getValue(i), schFld), getBoost(fieldName));
-        } else {
-          handleUndeclaredField(fieldName);
-        }
-      }
-    } else {
-      for (int i = 0; i < attributes.getLength(); i++) {
-        bldrStack.peek().append(attributes.getValue(i)).append(' ');
-      }
-    }
-  }
-
-  protected void handleUndeclaredField(String fieldName) {
-    if (ignoreUndeclaredFields == false) {
-      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Invalid field: " + fieldName);
-    } else {
-      if (log.isInfoEnabled()) {
-        log.info("Ignoring Field: " + fieldName);
-      }
-    }
-  }
-
-  @Override
-  public void endElement(String uri, String localName, String qName) throws SAXException {
-    StringBuilder theBldr = fieldBuilders.get(localName);
-    if (theBldr != null) {
-      //pop the stack
-      bldrStack.pop();
-      assert (bldrStack.size() >= 1);
-    }
-
-
-  }
-
-
-  @Override
-  public void characters(char[] chars, int offset, int length) throws SAXException {
-    bldrStack.peek().append(chars, offset, length);
-  }
-
-
-  
-
-  /**
-   * Can be used to transform input values based on their {@link org.apache.solr.schema.SchemaField}
-   * <p/>
-   * This implementation only formats dates using the {@link org.apache.solr.common.util.DateUtil}.
-   *
-   * @param val    The value to transform
-   * @param schFld The {@link org.apache.solr.schema.SchemaField}
-   * @return The potentially new value.
-   */
-  protected String transformValue(String val, SchemaField schFld) {
-    String result = val;
-    if (schFld.getType() instanceof DateField) {
-      //try to transform the date
-      try {
-        Date date = DateUtil.parseDate(val, dateFormats);
-        DateFormat df = DateUtil.getThreadLocalDateFormat();
-        result = df.format(date);
-
-      } catch (Exception e) {
-        //TODO: error or log?
-        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Invalid value: " + val + " for field: " + schFld, e);
-      }
-    }
-    return result;
-  }
-
-
-  /**
-   * Get the value of any boost factor for the mapped name.
-   *
-   * @param name The name of the field to see if there is a boost specified
-   * @return The boost value
-   */
-  protected float getBoost(String name) {
-    return params.getFloat(BOOST_PREFIX + name, 1.0f);
-  }
-
-  /**
-   * Get the name mapping
-   *
-   * @param name The name to check to see if there is a mapping
-   * @return The new name, if there is one, else <code>name</code>
-   */
-  protected String findMappedName(String name) {
-    return params.get(ExtractingParams.MAP_PREFIX + name, name);
-  }
-
-  /**
-   * Get the name mapping for the metadata field.  Prepends metadataPrefix onto the returned result.
-   *
-   * @param name The name to check to see if there is a mapping
-   * @return The new name, else <code>name</code>
-   */
-  protected String findMappedMetadataName(String name) {
-    return metadataPrefix + params.get(ExtractingParams.MAP_PREFIX + name, name);
-  }
-
-
-  protected synchronized long getNextId(){
-    return identifier++;
-  }
-
-
-}
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/SolrContentHandlerFactory.java b/contrib/extraction/src/main/java/org/apache/solr/handler/SolrContentHandlerFactory.java
deleted file mode 100644
index 68ef434..0000000
--- a/contrib/extraction/src/main/java/org/apache/solr/handler/SolrContentHandlerFactory.java
+++ /dev/null
@@ -1,25 +0,0 @@
-package org.apache.solr.handler;
-
-import org.apache.tika.metadata.Metadata;
-import org.apache.solr.common.params.SolrParams;
-import org.apache.solr.schema.IndexSchema;
-
-import java.util.Collection;
-
-
-/**
- *
- *
- **/
-public class SolrContentHandlerFactory {
-  protected Collection<String> dateFormats;
-
-  public SolrContentHandlerFactory(Collection<String> dateFormats) {
-    this.dateFormats = dateFormats;
-  }
-
-  public SolrContentHandler createSolrContentHandler(Metadata metadata, SolrParams params, IndexSchema schema) {
-    return new SolrContentHandler(metadata, params, schema,
-            dateFormats);
-  }
-}
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingDocumentLoader.java b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingDocumentLoader.java
new file mode 100644
index 0000000..506c706
--- /dev/null
+++ b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingDocumentLoader.java
@@ -0,0 +1,180 @@
+package org.apache.solr.handler.extraction;
+
+import org.apache.commons.io.IOUtils;
+import org.apache.solr.common.SolrException;
+import org.apache.solr.common.params.SolrParams;
+import org.apache.solr.common.params.UpdateParams;
+import org.apache.solr.common.util.ContentStream;
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.solr.request.SolrQueryResponse;
+import org.apache.solr.schema.IndexSchema;
+import org.apache.solr.update.AddUpdateCommand;
+import org.apache.solr.update.processor.UpdateRequestProcessor;
+import org.apache.solr.handler.ContentStreamLoader;
+import org.apache.tika.config.TikaConfig;
+import org.apache.tika.metadata.Metadata;
+import org.apache.tika.parser.AutoDetectParser;
+import org.apache.tika.parser.Parser;
+import org.apache.tika.sax.XHTMLContentHandler;
+import org.apache.tika.sax.xpath.Matcher;
+import org.apache.tika.sax.xpath.MatchingContentHandler;
+import org.apache.tika.sax.xpath.XPathParser;
+import org.apache.xml.serialize.OutputFormat;
+import org.apache.xml.serialize.XMLSerializer;
+import org.xml.sax.ContentHandler;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.StringWriter;
+
+
+/**
+ * The class responsible for loading extracted content into Solr.
+ *
+ **/
+public class ExtractingDocumentLoader extends ContentStreamLoader {
+
+  /**
+   * XHTML XPath parser.
+   */
+  private static final XPathParser PARSER =
+          new XPathParser("xhtml", XHTMLContentHandler.XHTML);
+
+  final IndexSchema schema;
+  final SolrParams params;
+  final UpdateRequestProcessor processor;
+  protected AutoDetectParser autoDetectParser;
+
+  private final AddUpdateCommand templateAdd;
+
+  protected TikaConfig config;
+  protected SolrContentHandlerFactory factory;
+  //protected Collection<String> dateFormats = DateUtil.DEFAULT_DATE_FORMATS;
+
+  ExtractingDocumentLoader(SolrQueryRequest req, UpdateRequestProcessor processor,
+                           TikaConfig config, SolrContentHandlerFactory factory) {
+    this.params = req.getParams();
+    schema = req.getSchema();
+    this.config = config;
+    this.processor = processor;
+
+    templateAdd = new AddUpdateCommand();
+    templateAdd.allowDups = false;
+    templateAdd.overwriteCommitted = true;
+    templateAdd.overwritePending = true;
+
+    if (params.getBool(UpdateParams.OVERWRITE, true)) {
+      templateAdd.allowDups = false;
+      templateAdd.overwriteCommitted = true;
+      templateAdd.overwritePending = true;
+    } else {
+      templateAdd.allowDups = true;
+      templateAdd.overwriteCommitted = false;
+      templateAdd.overwritePending = false;
+    }
+    //this is lightweight
+    autoDetectParser = new AutoDetectParser(config);
+    this.factory = factory;
+  }
+
+
+  /**
+   * this must be MT safe... may be called concurrently from multiple threads.
+   *
+   * @param
+   * @param
+   */
+  void doAdd(SolrContentHandler handler, AddUpdateCommand template)
+          throws IOException {
+    template.solrDoc = handler.newDocument();
+    processor.processAdd(template);
+  }
+
+  void addDoc(SolrContentHandler handler) throws IOException {
+    templateAdd.indexedId = null;
+    doAdd(handler, templateAdd);
+  }
+
+  /**
+   * @param req
+   * @param stream
+   * @throws java.io.IOException
+   */
+  public void load(SolrQueryRequest req, SolrQueryResponse rsp, ContentStream stream) throws IOException {
+    errHeader = "ExtractingDocumentLoader: " + stream.getSourceInfo();
+    Parser parser = null;
+    String streamType = req.getParams().get(ExtractingParams.STREAM_TYPE, null);
+    if (streamType != null) {
+      //Cache?  Parsers are lightweight to construct and thread-safe, so I'm told
+      parser = config.getParser(streamType.trim().toLowerCase());
+    } else {
+      parser = autoDetectParser;
+    }
+    if (parser != null) {
+      Metadata metadata = new Metadata();
+      metadata.add(ExtractingMetadataConstants.STREAM_NAME, stream.getName());
+      metadata.add(ExtractingMetadataConstants.STREAM_SOURCE_INFO, stream.getSourceInfo());
+      metadata.add(ExtractingMetadataConstants.STREAM_SIZE, String.valueOf(stream.getSize()));
+      metadata.add(ExtractingMetadataConstants.STREAM_CONTENT_TYPE, stream.getContentType());
+
+      // If you specify the resource name (the filename, roughly) with this parameter,
+      // then Tika can make use of it in guessing the appropriate MIME type:
+      String resourceName = req.getParams().get(ExtractingParams.RESOURCE_NAME, null);
+      if (resourceName != null) {
+        metadata.add(Metadata.RESOURCE_NAME_KEY, resourceName);
+      }
+
+      SolrContentHandler handler = factory.createSolrContentHandler(metadata, params, schema);
+      InputStream inputStream = null;
+      try {
+        inputStream = stream.getStream();
+        String xpathExpr = params.get(ExtractingParams.XPATH_EXPRESSION);
+        boolean extractOnly = params.getBool(ExtractingParams.EXTRACT_ONLY, false);
+        ContentHandler parsingHandler = handler;
+
+        StringWriter writer = null;
+        XMLSerializer serializer = null;
+        if (extractOnly == true) {
+          writer = new StringWriter();
+          serializer = new XMLSerializer(writer, new OutputFormat("XML", "UTF-8", true));
+          if (xpathExpr != null) {
+            Matcher matcher =
+                    PARSER.parse(xpathExpr);
+            serializer.startDocument();//The MatchingContentHandler does not invoke startDocument.  See http://tika.markmail.org/message/kknu3hw7argwiqin
+            parsingHandler = new MatchingContentHandler(serializer, matcher);
+          } else {
+            parsingHandler = serializer;
+          }
+        } else if (xpathExpr != null) {
+          Matcher matcher =
+                  PARSER.parse(xpathExpr);
+          parsingHandler = new MatchingContentHandler(handler, matcher);
+        } //else leave it as is
+
+        //potentially use a wrapper handler for parsing, but we still need the SolrContentHandler for getting the document.
+        parser.parse(inputStream, parsingHandler, metadata);
+        if (extractOnly == false) {
+          addDoc(handler);
+        } else {
+          //serializer is not null, so we need to call endDoc on it if using xpath
+          if (xpathExpr != null){
+            serializer.endDocument();
+          }
+          rsp.add(stream.getName(), writer.toString());
+          writer.close();
+
+        }
+      } catch (Exception e) {
+        //TODO: handle here with an option to not fail and just log the exception
+        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, e);
+
+      } finally {
+        IOUtils.closeQuietly(inputStream);
+      }
+    } else {
+      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, "Stream type of " + streamType + " didn't match any known parsers.  Please supply the " + ExtractingParams.STREAM_TYPE + " parameter.");
+    }
+  }
+
+
+}
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingMetadataConstants.java b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingMetadataConstants.java
new file mode 100644
index 0000000..6f78024
--- /dev/null
+++ b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingMetadataConstants.java
@@ -0,0 +1,13 @@
+package org.apache.solr.handler.extraction;
+
+
+/**
+ * Constants used internally by the {@link ExtractingRequestHandler}.
+ *
+ **/
+public interface ExtractingMetadataConstants {
+  String STREAM_NAME = "stream_name";
+  String STREAM_SOURCE_INFO = "stream_source_info";
+  String STREAM_SIZE = "stream_size";
+  String STREAM_CONTENT_TYPE = "stream_content_type";
+}
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingParams.java b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingParams.java
new file mode 100644
index 0000000..bd25915
--- /dev/null
+++ b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingParams.java
@@ -0,0 +1,125 @@
+package org.apache.solr.handler.extraction;
+
+
+/**
+ * The various Solr Parameters names to use when extracting content.
+ *
+ **/
+public interface ExtractingParams {
+
+  public static final String EXTRACTING_PREFIX = "ext.";
+
+  /**
+   * The param prefix for mapping Tika metadata to Solr fields.
+   * <p/>
+   * To map a field, add a name like:
+   * <pre>ext.map.title=solr.title</pre>
+   *
+   * In this example, the tika "title" metadata value will be added to a Solr field named "solr.title"
+   *
+   *
+   */
+  public static final String MAP_PREFIX = EXTRACTING_PREFIX + "map.";
+
+  /**
+   * The boost value for the name of the field.  The boost can be specified by a name mapping.
+   * <p/>
+   * For example
+   * <pre>
+   * ext.map.title=solr.title
+   * ext.boost.solr.title=2.5
+   * </pre>
+   * will boost the solr.title field for this document by 2.5
+   *
+   */
+  public static final String BOOST_PREFIX = EXTRACTING_PREFIX + "boost.";
+
+  /**
+   * Pass in literal values to be added to the document, as in
+   * <pre>
+   *  ext.literal.myField=Foo 
+   * </pre>
+   *
+   */
+  public static final String LITERALS_PREFIX = EXTRACTING_PREFIX + "literal.";
+
+
+  /**
+   * Restrict the extracted parts of a document to be indexed
+   *  by passing in an XPath expression.  All content that satisfies the XPath expr.
+   * will be passed to the {@link SolrContentHandler}.
+   * <p/>
+   * See Tika's docs for what the extracted document looks like.
+   * <p/>
+   * @see #DEFAULT_FIELDNAME
+   * @see #CAPTURE_FIELDS
+   */
+  public static final String XPATH_EXPRESSION = EXTRACTING_PREFIX + "xpath";
+
+
+  /**
+   * Only extract and return the document, do not index it.
+   */
+  public static final String EXTRACT_ONLY = EXTRACTING_PREFIX + "extract.only";
+
+  /**
+    *  Don't throw an exception if a field doesn't exist, just ignore it
+   */
+  public static final String IGNORE_UNDECLARED_FIELDS = EXTRACTING_PREFIX + "ignore.und.fl";
+
+  /**
+   * Index attributes separately according to their name, instead of just adding them to the string buffer
+   */
+  public static final String INDEX_ATTRIBUTES = EXTRACTING_PREFIX + "idx.attr";
+
+  /**
+   * The field to index the contents to by default.  If you want to capture a specific piece
+   * of the Tika document separately, see {@link #CAPTURE_FIELDS}.
+   *
+   * @see #CAPTURE_FIELDS
+   */
+  public static final String DEFAULT_FIELDNAME = EXTRACTING_PREFIX + "def.fl";
+
+  /**
+   * Capture the specified fields (and everything included below it that isn't capture by some other capture field) separately from the default.  This is different
+   * then the case of passing in an XPath expression.
+   * <p/>
+   * The Capture field is based on the localName returned to the {@link SolrContentHandler}
+   * by Tika, not to be confused by the mapped field.  The field name can then
+   * be mapped into the index schema.
+   * <p/>
+   * For instance, a Tika document may look like:
+   * <pre>
+   *  &lt;html&gt;
+   *    ...
+   *    &lt;body&gt;
+   *      &lt;p&gt;some text here.  &lt;div&gt;more text&lt;/div&gt;&lt;/p&gt;
+   *      Some more text
+   *    &lt;/body&gt;
+   * </pre>
+   * By passing in the p tag, you could capture all P tags separately from the rest of the text.
+   * Thus, in the example, the capture of the P tag would be: "some text here.  more text"
+   *
+   * @see #DEFAULT_FIELDNAME
+   */
+  public static final String CAPTURE_FIELDS = EXTRACTING_PREFIX + "capture";
+
+  /**
+   * The type of the stream.  If not specified, Tika will use mime type detection.
+   */
+  public static final String STREAM_TYPE = EXTRACTING_PREFIX + "stream.type";
+
+
+  /**
+   * Optional.  The file name. If specified, Tika can take this into account while
+   * guessing the MIME type.
+   */
+  public static final String RESOURCE_NAME = EXTRACTING_PREFIX + "resource.name";
+
+
+  /**
+   * Optional.  If specified, the prefix will be prepended to all Metadata, such that it would be possible
+   * to setup a dynamic field to automatically capture it
+   */
+  public static final String METADATA_PREFIX = EXTRACTING_PREFIX + "metadata.prefix";
+}
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java
new file mode 100644
index 0000000..b5d6215
--- /dev/null
+++ b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/ExtractingRequestHandler.java
@@ -0,0 +1,136 @@
+package org.apache.solr.handler.extraction;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.solr.common.SolrException;
+import org.apache.solr.common.SolrException.ErrorCode;
+import org.apache.solr.common.util.DateUtil;
+import org.apache.solr.common.util.NamedList;
+import org.apache.solr.core.SolrCore;
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.solr.update.processor.UpdateRequestProcessor;
+import org.apache.solr.util.plugin.SolrCoreAware;
+import org.apache.solr.handler.ContentStreamHandlerBase;
+import org.apache.solr.handler.ContentStreamLoader;
+import org.apache.tika.config.TikaConfig;
+import org.apache.tika.exception.TikaException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.File;
+import java.util.Collection;
+import java.util.HashSet;
+
+
+/**
+ * Handler for rich documents like PDF or Word or any other file format that Tika handles that need the text to be extracted
+ * first from the document.
+ * <p/>
+ */
+
+public class ExtractingRequestHandler extends ContentStreamHandlerBase implements SolrCoreAware {
+
+  private transient static Logger log = LoggerFactory.getLogger(ExtractingRequestHandler.class);
+
+  public static final String CONFIG_LOCATION = "tika.config";
+  public static final String DATE_FORMATS = "date.formats";
+
+  protected TikaConfig config;
+
+
+  protected Collection<String> dateFormats = DateUtil.DEFAULT_DATE_FORMATS;
+  protected SolrContentHandlerFactory factory;
+
+
+  @Override
+  public void init(NamedList args) {
+    super.init(args);
+  }
+
+  public void inform(SolrCore core) {
+    if (initArgs != null) {
+      //if relative,then relative to config dir, otherwise, absolute path
+      String tikaConfigLoc = (String) initArgs.get(CONFIG_LOCATION);
+      if (tikaConfigLoc != null) {
+        File configFile = new File(tikaConfigLoc);
+        if (configFile.isAbsolute() == false) {
+          configFile = new File(core.getResourceLoader().getConfigDir(), configFile.getPath());
+        }
+        try {
+          config = new TikaConfig(configFile);
+        } catch (Exception e) {
+          throw new SolrException(ErrorCode.SERVER_ERROR, e);
+        }
+      } else {
+        try {
+          config = TikaConfig.getDefaultConfig();
+        } catch (TikaException e) {
+          throw new SolrException(ErrorCode.SERVER_ERROR, e);
+        }
+      }
+      NamedList configDateFormats = (NamedList) initArgs.get(DATE_FORMATS);
+      if (configDateFormats != null && configDateFormats.size() > 0) {
+        dateFormats = new HashSet<String>();
+        while (configDateFormats.iterator().hasNext()) {
+          String format = (String) configDateFormats.iterator().next();
+          log.info("Adding Date Format: " + format);
+          dateFormats.add(format);
+        }
+      }
+    } else {
+      try {
+        config = TikaConfig.getDefaultConfig();
+      } catch (TikaException e) {
+        throw new SolrException(ErrorCode.SERVER_ERROR, e);
+      }
+    }
+    factory = createFactory();
+  }
+
+  protected SolrContentHandlerFactory createFactory() {
+    return new SolrContentHandlerFactory(dateFormats);
+  }
+
+
+  protected ContentStreamLoader newLoader(SolrQueryRequest req, UpdateRequestProcessor processor) {
+    return new ExtractingDocumentLoader(req, processor, config, factory);
+  }
+
+  // ////////////////////// SolrInfoMBeans methods //////////////////////
+  @Override
+  public String getDescription() {
+    return "Add/Update Rich document";
+  }
+
+  @Override
+  public String getVersion() {
+    return "$Revision:$";
+  }
+
+  @Override
+  public String getSourceId() {
+    return "$Id:$";
+  }
+
+  @Override
+  public String getSource() {
+    return "$URL:$";
+  }
+}
+
+
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/SolrContentHandler.java b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/SolrContentHandler.java
new file mode 100644
index 0000000..acaa20f
--- /dev/null
+++ b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/SolrContentHandler.java
@@ -0,0 +1,362 @@
+package org.apache.solr.handler.extraction;
+
+import org.apache.solr.common.SolrException;
+import org.apache.solr.common.SolrInputDocument;
+import org.apache.solr.common.SolrInputField;
+import org.apache.solr.common.params.SolrParams;
+import org.apache.solr.common.util.DateUtil;
+import org.apache.solr.schema.DateField;
+import org.apache.solr.schema.IndexSchema;
+import org.apache.solr.schema.SchemaField;
+import org.apache.solr.schema.StrField;
+import org.apache.solr.schema.TextField;
+import org.apache.solr.schema.FieldType;
+import org.apache.solr.schema.UUIDField;
+import org.apache.tika.metadata.Metadata;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.xml.sax.Attributes;
+import org.xml.sax.SAXException;
+import org.xml.sax.helpers.DefaultHandler;
+
+import java.text.DateFormat;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Stack;
+import java.util.UUID;
+
+
+/**
+ * The class responsible for handling Tika events and translating them into {@link org.apache.solr.common.SolrInputDocument}s.
+ * <B>This class is not thread-safe.</B>
+ * <p/>
+ *
+ * User's may wish to override this class to provide their own functionality.
+ *
+ * @see org.apache.solr.handler.extraction.SolrContentHandlerFactory
+ * @see org.apache.solr.handler.extraction.ExtractingRequestHandler
+ * @see org.apache.solr.handler.extraction.ExtractingDocumentLoader
+ *
+ */
+public class SolrContentHandler extends DefaultHandler implements ExtractingParams {
+  private transient static Logger log = LoggerFactory.getLogger(SolrContentHandler.class);
+  protected SolrInputDocument document;
+
+  protected Collection<String> dateFormats = DateUtil.DEFAULT_DATE_FORMATS;
+
+  protected Metadata metadata;
+  protected SolrParams params;
+  protected StringBuilder catchAllBuilder = new StringBuilder(2048);
+  //private StringBuilder currentBuilder;
+  protected IndexSchema schema;
+  //create empty so we don't have to worry about null checks
+  protected Map<String, StringBuilder> fieldBuilders = Collections.emptyMap();
+  protected Stack<StringBuilder> bldrStack = new Stack<StringBuilder>();
+
+  protected boolean ignoreUndeclaredFields = false;
+  protected boolean indexAttribs = false;
+  protected String defaultFieldName;
+
+  protected String metadataPrefix = "";
+
+  /**
+   * Only access through getNextId();
+   */
+  private static long identifier = Long.MIN_VALUE;
+
+
+  public SolrContentHandler(Metadata metadata, SolrParams params, IndexSchema schema) {
+    this(metadata, params, schema, DateUtil.DEFAULT_DATE_FORMATS);
+  }
+
+
+  public SolrContentHandler(Metadata metadata, SolrParams params,
+                            IndexSchema schema, Collection<String> dateFormats) {
+    document = new SolrInputDocument();
+    this.metadata = metadata;
+    this.params = params;
+    this.schema = schema;
+    this.dateFormats = dateFormats;
+    this.ignoreUndeclaredFields = params.getBool(IGNORE_UNDECLARED_FIELDS, false);
+    this.indexAttribs = params.getBool(INDEX_ATTRIBUTES, false);
+    this.defaultFieldName = params.get(DEFAULT_FIELDNAME);
+    this.metadataPrefix = params.get(METADATA_PREFIX, "");
+    //if there's no default field and we are intending to index, then throw an exception
+    if (defaultFieldName == null && params.getBool(EXTRACT_ONLY, false) == false) {
+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "No default field name specified");
+    }
+    String[] captureFields = params.getParams(CAPTURE_FIELDS);
+    if (captureFields != null && captureFields.length > 0) {
+      fieldBuilders = new HashMap<String, StringBuilder>();
+      for (int i = 0; i < captureFields.length; i++) {
+        fieldBuilders.put(captureFields[i], new StringBuilder());
+      }
+    }
+    bldrStack.push(catchAllBuilder);
+  }
+
+
+  /**
+   * This is called by a consumer when it is ready to deal with a new SolrInputDocument.  Overriding
+   * classes can use this hook to add in or change whatever they deem fit for the document at that time.
+   * The base implementation adds the metadata as fields, allowing for potential remapping.
+   *
+   * @return The {@link org.apache.solr.common.SolrInputDocument}.
+   */
+  public SolrInputDocument newDocument() {
+    float boost = 1.0f;
+    //handle the metadata extracted from the document
+    for (String name : metadata.names()) {
+      String[] vals = metadata.getValues(name);
+      name = findMappedMetadataName(name);
+      SchemaField schFld = schema.getFieldOrNull(name);
+      if (schFld != null) {
+        boost = getBoost(name);
+        if (schFld.multiValued()) {
+          for (int i = 0; i < vals.length; i++) {
+            String val = vals[i];
+            document.addField(name, transformValue(val, schFld), boost);
+          }
+        } else {
+          StringBuilder builder = new StringBuilder();
+          for (int i = 0; i < vals.length; i++) {
+            builder.append(vals[i]).append(' ');
+          }
+          document.addField(name, transformValue(builder.toString().trim(), schFld), boost);
+        }
+      } else {
+        //TODO: error or log?
+        if (ignoreUndeclaredFields == false) {
+          // Arguably we should handle this as a special case. Why? Because unlike basically
+          // all the other fields in metadata, this one was probably set not by Tika by in
+          // ExtractingDocumentLoader.load(). You shouldn't have to define a mapping for this
+          // field just because you specified a resource.name parameter to the handler, should
+          // you?
+          if (name != Metadata.RESOURCE_NAME_KEY) {
+            throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Invalid field: " + name);
+          }
+        }
+      }
+    }
+    //handle the literals from the params
+    Iterator<String> paramNames = params.getParameterNamesIterator();
+    while (paramNames.hasNext()) {
+      String name = paramNames.next();
+      if (name.startsWith(LITERALS_PREFIX)) {
+        String fieldName = name.substring(LITERALS_PREFIX.length());
+        //no need to map names here, since they are literals from the user
+        SchemaField schFld = schema.getFieldOrNull(fieldName);
+        if (schFld != null) {
+          String value = params.get(name);
+          boost = getBoost(fieldName);
+          //no need to transform here, b/c we can assume the user sent it in correctly
+          document.addField(fieldName, value, boost);
+        } else {
+          handleUndeclaredField(fieldName);
+        }
+      }
+    }
+    //add in the content
+    document.addField(defaultFieldName, catchAllBuilder.toString(), getBoost(defaultFieldName));
+
+    //add in the captured content
+    for (Map.Entry<String, StringBuilder> entry : fieldBuilders.entrySet()) {
+      if (entry.getValue().length() > 0) {
+        String fieldName = findMappedName(entry.getKey());
+        SchemaField schFld = schema.getFieldOrNull(fieldName);
+        if (schFld != null) {
+          document.addField(fieldName, transformValue(entry.getValue().toString(), schFld), getBoost(fieldName));
+        } else {
+          handleUndeclaredField(fieldName);
+        }
+      }
+    }
+    //make sure we have a unique id, if one is needed
+    SchemaField uniqueField = schema.getUniqueKeyField();
+    if (uniqueField != null) {
+      String uniqueFieldName = uniqueField.getName();
+      SolrInputField uniqFld = document.getField(uniqueFieldName);
+      if (uniqFld == null) {
+        String uniqId = generateId(uniqueField);
+        if (uniqId != null) {
+          document.addField(uniqueFieldName, uniqId);
+        }
+      }
+    }
+    if (log.isDebugEnabled()) {
+      log.debug("Doc: " + document);
+    }
+    return document;
+  }
+
+  /**
+   * Generate an ID for the document.  First try to get
+   * {@link ExtractingMetadataConstants#STREAM_NAME} from the
+   * {@link org.apache.tika.metadata.Metadata}, then try {@link ExtractingMetadataConstants#STREAM_SOURCE_INFO}
+   * then try {@link org.apache.tika.metadata.Metadata#IDENTIFIER}.
+   * If those all are null, then generate a random UUID using {@link java.util.UUID#randomUUID()}.
+   *
+   * @param uniqueField The SchemaField representing the unique field.
+   * @return The id as a string
+   */
+  protected String generateId(SchemaField uniqueField) {
+    //we don't have a unique field specified, so let's add one
+    String uniqId = null;
+    FieldType type = uniqueField.getType();
+    if (type instanceof StrField || type instanceof TextField) {
+      uniqId = metadata.get(ExtractingMetadataConstants.STREAM_NAME);
+      if (uniqId == null) {
+        uniqId = metadata.get(ExtractingMetadataConstants.STREAM_SOURCE_INFO);
+      }
+      if (uniqId == null) {
+        uniqId = metadata.get(Metadata.IDENTIFIER);
+      }
+      if (uniqId == null) {
+        //last chance, just create one
+        uniqId = UUID.randomUUID().toString();
+      }
+    } else if (type instanceof UUIDField){
+      uniqId = UUID.randomUUID().toString();
+    }
+    else {
+      uniqId = String.valueOf(getNextId());
+    }
+    return uniqId;
+  }
+
+
+  @Override
+  public void startDocument() throws SAXException {
+    document.clear();
+    catchAllBuilder.setLength(0);
+    for (StringBuilder builder : fieldBuilders.values()) {
+      builder.setLength(0);
+    }
+    bldrStack.clear();
+    bldrStack.push(catchAllBuilder);
+  }
+
+
+  @Override
+  public void startElement(String uri, String localName, String qName, Attributes attributes) throws SAXException {
+    StringBuilder theBldr = fieldBuilders.get(localName);
+    if (theBldr != null) {
+      //we need to switch the currentBuilder
+      bldrStack.push(theBldr);
+    }
+    if (indexAttribs == true) {
+      for (int i = 0; i < attributes.getLength(); i++) {
+        String fieldName = findMappedName(localName);
+        SchemaField schFld = schema.getFieldOrNull(fieldName);
+        if (schFld != null) {
+          document.addField(fieldName, transformValue(attributes.getValue(i), schFld), getBoost(fieldName));
+        } else {
+          handleUndeclaredField(fieldName);
+        }
+      }
+    } else {
+      for (int i = 0; i < attributes.getLength(); i++) {
+        bldrStack.peek().append(attributes.getValue(i)).append(' ');
+      }
+    }
+  }
+
+  protected void handleUndeclaredField(String fieldName) {
+    if (ignoreUndeclaredFields == false) {
+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Invalid field: " + fieldName);
+    } else {
+      if (log.isInfoEnabled()) {
+        log.info("Ignoring Field: " + fieldName);
+      }
+    }
+  }
+
+  @Override
+  public void endElement(String uri, String localName, String qName) throws SAXException {
+    StringBuilder theBldr = fieldBuilders.get(localName);
+    if (theBldr != null) {
+      //pop the stack
+      bldrStack.pop();
+      assert (bldrStack.size() >= 1);
+    }
+
+
+  }
+
+
+  @Override
+  public void characters(char[] chars, int offset, int length) throws SAXException {
+    bldrStack.peek().append(chars, offset, length);
+  }
+
+
+  
+
+  /**
+   * Can be used to transform input values based on their {@link org.apache.solr.schema.SchemaField}
+   * <p/>
+   * This implementation only formats dates using the {@link org.apache.solr.common.util.DateUtil}.
+   *
+   * @param val    The value to transform
+   * @param schFld The {@link org.apache.solr.schema.SchemaField}
+   * @return The potentially new value.
+   */
+  protected String transformValue(String val, SchemaField schFld) {
+    String result = val;
+    if (schFld.getType() instanceof DateField) {
+      //try to transform the date
+      try {
+        Date date = DateUtil.parseDate(val, dateFormats);
+        DateFormat df = DateUtil.getThreadLocalDateFormat();
+        result = df.format(date);
+
+      } catch (Exception e) {
+        //TODO: error or log?
+        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Invalid value: " + val + " for field: " + schFld, e);
+      }
+    }
+    return result;
+  }
+
+
+  /**
+   * Get the value of any boost factor for the mapped name.
+   *
+   * @param name The name of the field to see if there is a boost specified
+   * @return The boost value
+   */
+  protected float getBoost(String name) {
+    return params.getFloat(BOOST_PREFIX + name, 1.0f);
+  }
+
+  /**
+   * Get the name mapping
+   *
+   * @param name The name to check to see if there is a mapping
+   * @return The new name, if there is one, else <code>name</code>
+   */
+  protected String findMappedName(String name) {
+    return params.get(MAP_PREFIX + name, name);
+  }
+
+  /**
+   * Get the name mapping for the metadata field.  Prepends metadataPrefix onto the returned result.
+   *
+   * @param name The name to check to see if there is a mapping
+   * @return The new name, else <code>name</code>
+   */
+  protected String findMappedMetadataName(String name) {
+    return metadataPrefix + params.get(MAP_PREFIX + name, name);
+  }
+
+
+  protected synchronized long getNextId(){
+    return identifier++;
+  }
+
+
+}
diff --git a/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/SolrContentHandlerFactory.java b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/SolrContentHandlerFactory.java
new file mode 100644
index 0000000..36b5ebd
--- /dev/null
+++ b/contrib/extraction/src/main/java/org/apache/solr/handler/extraction/SolrContentHandlerFactory.java
@@ -0,0 +1,25 @@
+package org.apache.solr.handler.extraction;
+
+import org.apache.tika.metadata.Metadata;
+import org.apache.solr.common.params.SolrParams;
+import org.apache.solr.schema.IndexSchema;
+
+import java.util.Collection;
+
+
+/**
+ *
+ *
+ **/
+public class SolrContentHandlerFactory {
+  protected Collection<String> dateFormats;
+
+  public SolrContentHandlerFactory(Collection<String> dateFormats) {
+    this.dateFormats = dateFormats;
+  }
+
+  public SolrContentHandler createSolrContentHandler(Metadata metadata, SolrParams params, IndexSchema schema) {
+    return new SolrContentHandler(metadata, params, schema,
+            dateFormats);
+  }
+}
diff --git a/contrib/extraction/src/test/java/org/apache/solr/handler/ExtractingRequestHandlerTest.java b/contrib/extraction/src/test/java/org/apache/solr/handler/ExtractingRequestHandlerTest.java
index 8dea409..81266a0 100644
--- a/contrib/extraction/src/test/java/org/apache/solr/handler/ExtractingRequestHandlerTest.java
+++ b/contrib/extraction/src/test/java/org/apache/solr/handler/ExtractingRequestHandlerTest.java
@@ -6,6 +6,8 @@ import org.apache.solr.request.SolrQueryResponse;
 import org.apache.solr.common.util.ContentStream;
 import org.apache.solr.common.util.ContentStreamBase;
 import org.apache.solr.common.util.NamedList;
+import org.apache.solr.handler.extraction.ExtractingParams;
+import org.apache.solr.handler.extraction.ExtractingRequestHandler;
 
 import java.util.List;
 import java.util.ArrayList;
diff --git a/contrib/extraction/src/test/resources/solr/conf/solrconfig.xml b/contrib/extraction/src/test/resources/solr/conf/solrconfig.xml
index 7842824..f7495d6 100644
--- a/contrib/extraction/src/test/resources/solr/conf/solrconfig.xml
+++ b/contrib/extraction/src/test/resources/solr/conf/solrconfig.xml
@@ -308,7 +308,7 @@
   	<bool name="httpCaching">false</bool>
   </requestHandler>
   
-  <requestHandler name="/update/extract" class="org.apache.solr.handler.ExtractingRequestHandler"/>
+  <requestHandler name="/update/extract" class="org.apache.solr.handler.extraction.ExtractingRequestHandler"/>
 
 
   <highlighting>
diff --git a/example/solr/conf/solrconfig.xml b/example/solr/conf/solrconfig.xml
index 095cb99..297f828 100755
--- a/example/solr/conf/solrconfig.xml
+++ b/example/solr/conf/solrconfig.xml
@@ -627,6 +627,16 @@
     </arr>
   </requestHandler>
 
+<!--
+<requestHandler name="/update/extract" class="solr.ExtractingRequestHandler">
+    <lst name="defaults">
+      <str name="ext.map.Last-Modified">last_modified</str>
+      <bool name="ext.ignore.und.fl">true</bool>
+    </lst>
+  </requestHandler>
+-->
+
+
 
   <searchComponent name="termsComp" class="org.apache.solr.handler.component.TermsComponent"/>
 

