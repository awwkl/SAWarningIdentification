GitDiffStart: 09c2c4d0aa3e5f621602a258dd552ad66a3384e1 | Mon Sep 1 11:37:03 2014 +0000
diff --git a/build.xml b/build.xml
index 22b908a..e537ef1 100644
--- a/build.xml
+++ b/build.xml
@@ -260,8 +260,8 @@
     </copy>
     
     <pathconvert property="eclipse.fileset.sourcefolders" pathsep="|" dirsep="/">
-      <dirset dir="${basedir}/lucene" includes="**/src/java, **/src/resources, **/src/test, **/src/test-files, **/src/examples" excludes="tools/**, build/**, backwards/**" />
-      <dirset dir="${basedir}/solr" includes="**/src/java, **/src/resources, **/src/test, **/src/test-files, **/src/examples" excludes="build/**" />
+      <dirset dir="${basedir}/lucene" includes="**/src/java, **/src/resources, **/src/test, **/src/test-resources, **/src/test-files, **/src/examples" excludes="tools/**, build/**, backwards/**" />
+      <dirset dir="${basedir}/solr" includes="**/src/java, **/src/resources, **/src/test, **/src/test-resources, **/src/test-files, **/src/examples" excludes="build/**" />
       <map from="${basedir}/" to=""/>
     </pathconvert>
     <!-- TODO: find a better way to exclude duplicate JAR files & fix the servlet-api mess! -->
diff --git a/lucene/backward-codecs/build.xml b/lucene/backward-codecs/build.xml
new file mode 100644
index 0000000..3de2979
--- /dev/null
+++ b/lucene/backward-codecs/build.xml
@@ -0,0 +1,26 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+  -->
+
+<project name="backward-codecs" default="default">
+
+  <description>
+    Codecs for older versions of Lucene.
+  </description>
+
+  <import file="../module-build.xml"/>
+
+</project>
diff --git a/lucene/backward-codecs/ivy.xml b/lucene/backward-codecs/ivy.xml
new file mode 100644
index 0000000..6d86d6a
--- /dev/null
+++ b/lucene/backward-codecs/ivy.xml
@@ -0,0 +1,21 @@
+<!--
+   Licensed to the Apache Software Foundation (ASF) under one
+   or more contributor license agreements.  See the NOTICE file
+   distributed with this work for additional information
+   regarding copyright ownership.  The ASF licenses this file
+   to you under the Apache License, Version 2.0 (the
+   "License"); you may not use this file except in compliance
+   with the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing,
+   software distributed under the License is distributed on an
+   "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+   KIND, either express or implied.  See the License for the
+   specific language governing permissions and limitations
+   under the License.    
+-->
+<ivy-module version="2.0">
+    <info organisation="org.apache.lucene" module="backward-codecs"/>
+</ivy-module>
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
new file mode 100644
index 0000000..bb7f75f
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
@@ -0,0 +1,118 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
+
+/**
+ * Implements the Lucene 4.0 index format, with configurable per-field postings formats.
+ * <p>
+ * If you want to reuse functionality of this codec in another codec, extend
+ * {@link FilterCodec}.
+ *
+ * @see org.apache.lucene.codecs.lucene40 package documentation for file format details.
+ * @deprecated Only for reading old 4.0 segments
+ */
+// NOTE: if we make largish changes in a minor release, easier to just make Lucene42Codec or whatever
+// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
+// (it writes a minor version, etc).
+@Deprecated
+public class Lucene40Codec extends Codec {
+  private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat();
+  private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
+  private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
+  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
+  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  
+  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
+    @Override
+    public PostingsFormat getPostingsFormatForField(String field) {
+      return Lucene40Codec.this.getPostingsFormatForField(field);
+    }
+  };
+
+  /** Sole constructor. */
+  public Lucene40Codec() {
+    super("Lucene40");
+  }
+  
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return fieldsFormat;
+  }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+
+  @Override
+  public final PostingsFormat postingsFormat() {
+    return postingsFormat;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+  
+  @Override
+  public SegmentInfoFormat segmentInfoFormat() {
+    return infosFormat;
+  }
+  
+  private final DocValuesFormat defaultDVFormat = new Lucene40DocValuesFormat();
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return defaultDVFormat;
+  }
+
+  private final NormsFormat normsFormat = new Lucene40NormsFormat();
+
+  @Override
+  public NormsFormat normsFormat() {
+    return normsFormat;
+  }
+
+  @Override
+  public final LiveDocsFormat liveDocsFormat() {
+    return liveDocsFormat;
+  }
+
+  /** Returns the postings format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene40"
+   */
+  public PostingsFormat getPostingsFormatForField(String field) {
+    return defaultFormat;
+  }
+  
+  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene40");
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java
new file mode 100644
index 0000000..4f350ef
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java
@@ -0,0 +1,206 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Lucene 4.0 DocValues format.
+ * <p>
+ * Files:
+ * <ul>
+ *   <li><tt>.dv.cfs</tt>: {@link CompoundFileDirectory compound container}</li>
+ *   <li><tt>.dv.cfe</tt>: {@link CompoundFileDirectory compound entries}</li>
+ * </ul>
+ * Entries within the compound file:
+ * <ul>
+ *   <li><tt>&lt;segment&gt;_&lt;fieldNumber&gt;.dat</tt>: data values</li>
+ *   <li><tt>&lt;segment&gt;_&lt;fieldNumber&gt;.idx</tt>: index into the .dat for DEREF types</li>
+ * </ul>
+ * <p>
+ * There are several many types of {@code DocValues} with different encodings.
+ * From the perspective of filenames, all types store their values in <tt>.dat</tt>
+ * entries within the compound file. In the case of dereferenced/sorted types, the <tt>.dat</tt>
+ * actually contains only the unique values, and an additional <tt>.idx</tt> file contains
+ * pointers to these unique values.
+ * </p>
+ * Formats:
+ * <ul>
+ *    <li>{@code VAR_INTS} .dat --&gt; Header, PackedType, MinValue, 
+ *        DefaultValue, PackedStream</li>
+ *    <li>{@code FIXED_INTS_8} .dat --&gt; Header, ValueSize, 
+ *        {@link DataOutput#writeByte Byte}<sup>maxdoc</sup></li>
+ *    <li>{@code FIXED_INTS_16} .dat --&gt; Header, ValueSize,
+ *        {@link DataOutput#writeShort Short}<sup>maxdoc</sup></li>
+ *    <li>{@code FIXED_INTS_32} .dat --&gt; Header, ValueSize,
+ *        {@link DataOutput#writeInt Int32}<sup>maxdoc</sup></li>
+ *    <li>{@code FIXED_INTS_64} .dat --&gt; Header, ValueSize,
+ *        {@link DataOutput#writeLong Int64}<sup>maxdoc</sup></li>
+ *    <li>{@code FLOAT_32} .dat --&gt; Header, ValueSize, Float32<sup>maxdoc</sup></li>
+ *    <li>{@code FLOAT_64} .dat --&gt; Header, ValueSize, Float64<sup>maxdoc</sup></li>
+ *    <li>{@code BYTES_FIXED_STRAIGHT} .dat --&gt; Header, ValueSize,
+ *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>maxdoc</sup></li>
+ *    <li>{@code BYTES_VAR_STRAIGHT} .idx --&gt; Header, TotalBytes, Addresses</li>
+ *    <li>{@code BYTES_VAR_STRAIGHT} .dat --&gt; Header,
+          ({@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>maxdoc</sup></li>
+ *    <li>{@code BYTES_FIXED_DEREF} .idx --&gt; Header, NumValues, Addresses</li>
+ *    <li>{@code BYTES_FIXED_DEREF} .dat --&gt; Header, ValueSize,
+ *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>NumValues</sup></li>
+ *    <li>{@code BYTES_VAR_DEREF} .idx --&gt; Header, TotalVarBytes, Addresses</li>
+ *    <li>{@code BYTES_VAR_DEREF} .dat --&gt; Header,
+ *        (LengthPrefix + {@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>NumValues</sup></li>
+ *    <li>{@code BYTES_FIXED_SORTED} .idx --&gt; Header, NumValues, Ordinals</li>
+ *    <li>{@code BYTES_FIXED_SORTED} .dat --&gt; Header, ValueSize,
+ *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>NumValues</sup></li>
+ *    <li>{@code BYTES_VAR_SORTED} .idx --&gt; Header, TotalVarBytes, Addresses, Ordinals</li>
+ *    <li>{@code BYTES_VAR_SORTED} .dat --&gt; Header,
+ *        ({@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>NumValues</sup></li>
+ * </ul>
+ * Data Types:
+ * <ul>
+ *    <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *    <li>PackedType --&gt; {@link DataOutput#writeByte Byte}</li>
+ *    <li>MaxAddress, MinValue, DefaultValue --&gt; {@link DataOutput#writeLong Int64}</li>
+ *    <li>PackedStream, Addresses, Ordinals --&gt; {@link PackedInts}</li>
+ *    <li>ValueSize, NumValues --&gt; {@link DataOutput#writeInt Int32}</li>
+ *    <li>Float32 --&gt; 32-bit float encoded with {@link Float#floatToRawIntBits(float)}
+ *                       then written as {@link DataOutput#writeInt Int32}</li>
+ *    <li>Float64 --&gt; 64-bit float encoded with {@link Double#doubleToRawLongBits(double)}
+ *                       then written as {@link DataOutput#writeLong Int64}</li>
+ *    <li>TotalBytes --&gt; {@link DataOutput#writeVLong VLong}</li>
+ *    <li>TotalVarBytes --&gt; {@link DataOutput#writeLong Int64}</li>
+ *    <li>LengthPrefix --&gt; Length of the data value as {@link DataOutput#writeVInt VInt} (maximum
+ *                       of 2 bytes)</li>
+ * </ul>
+ * Notes:
+ * <ul>
+ *    <li>PackedType is a 0 when compressed, 1 when the stream is written as 64-bit integers.</li>
+ *    <li>Addresses stores pointers to the actual byte location (indexed by docid). In the VAR_STRAIGHT
+ *        case, each entry can have a different length, so to determine the length, docid+1 is 
+ *        retrieved. A sentinel address is written at the end for the VAR_STRAIGHT case, so the Addresses 
+ *        stream contains maxdoc+1 indices. For the deduplicated VAR_DEREF case, each length
+ *        is encoded as a prefix to the data itself as a {@link DataOutput#writeVInt VInt} 
+ *        (maximum of 2 bytes).</li>
+ *    <li>Ordinals stores the term ID in sorted order (indexed by docid). In the FIXED_SORTED case,
+ *        the address into the .dat can be computed from the ordinal as 
+ *        <code>Header+ValueSize+(ordinal*ValueSize)</code> because the byte length is fixed.
+ *        In the VAR_SORTED case, there is double indirection (docid -> ordinal -> address), but
+ *        an additional sentinel ordinal+address is always written (so there are NumValues+1 ordinals). To
+ *        determine the length, ord+1's address is looked up as well.</li>
+ *    <li>{@code BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT} in contrast to other straight 
+ *        variants uses a <tt>.idx</tt> file to improve lookup perfromance. In contrast to 
+ *        {@code BYTES_VAR_DEREF BYTES_VAR_DEREF} it doesn't apply deduplication of the document values.
+ *    </li>
+ * </ul>
+ * <p>
+ * Limitations:
+ * <ul>
+ *   <li> Binary doc values can be at most {@link #MAX_BINARY_FIELD_LENGTH} in length.
+ * </ul>
+ * @deprecated Only for reading old 4.0 and 4.1 segments
+ */
+@Deprecated
+// NOTE: not registered in SPI, doesnt respect segment suffix, etc
+// for back compat only!
+public class Lucene40DocValuesFormat extends DocValuesFormat {
+  
+  /** Maximum length for each binary doc values field. */
+  public static final int MAX_BINARY_FIELD_LENGTH = (1 << 15) - 2;
+  
+  /** Sole constructor. */
+  public Lucene40DocValuesFormat() {
+    super("Lucene40");
+  }
+  
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+  
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                     "dv", 
+                                                     IndexFileNames.COMPOUND_FILE_EXTENSION);
+    return new Lucene40DocValuesReader(state, filename, Lucene40FieldInfosReader.LEGACY_DV_TYPE_KEY);
+  }
+  
+  // constants for VAR_INTS
+  static final String VAR_INTS_CODEC_NAME = "PackedInts";
+  static final int VAR_INTS_VERSION_START = 0;
+  static final int VAR_INTS_VERSION_CURRENT = VAR_INTS_VERSION_START;
+  static final byte VAR_INTS_PACKED = 0x00;
+  static final byte VAR_INTS_FIXED_64 = 0x01;
+  
+  // constants for FIXED_INTS_8, FIXED_INTS_16, FIXED_INTS_32, FIXED_INTS_64
+  static final String INTS_CODEC_NAME = "Ints";
+  static final int INTS_VERSION_START = 0;
+  static final int INTS_VERSION_CURRENT = INTS_VERSION_START;
+  
+  // constants for FLOAT_32, FLOAT_64
+  static final String FLOATS_CODEC_NAME = "Floats";
+  static final int FLOATS_VERSION_START = 0;
+  static final int FLOATS_VERSION_CURRENT = FLOATS_VERSION_START;
+  
+  // constants for BYTES_FIXED_STRAIGHT
+  static final String BYTES_FIXED_STRAIGHT_CODEC_NAME = "FixedStraightBytes";
+  static final int BYTES_FIXED_STRAIGHT_VERSION_START = 0;
+  static final int BYTES_FIXED_STRAIGHT_VERSION_CURRENT = BYTES_FIXED_STRAIGHT_VERSION_START;
+  
+  // constants for BYTES_VAR_STRAIGHT
+  static final String BYTES_VAR_STRAIGHT_CODEC_NAME_IDX = "VarStraightBytesIdx";
+  static final String BYTES_VAR_STRAIGHT_CODEC_NAME_DAT = "VarStraightBytesDat";
+  static final int BYTES_VAR_STRAIGHT_VERSION_START = 0;
+  static final int BYTES_VAR_STRAIGHT_VERSION_CURRENT = BYTES_VAR_STRAIGHT_VERSION_START;
+  
+  // constants for BYTES_FIXED_DEREF
+  static final String BYTES_FIXED_DEREF_CODEC_NAME_IDX = "FixedDerefBytesIdx";
+  static final String BYTES_FIXED_DEREF_CODEC_NAME_DAT = "FixedDerefBytesDat";
+  static final int BYTES_FIXED_DEREF_VERSION_START = 0;
+  static final int BYTES_FIXED_DEREF_VERSION_CURRENT = BYTES_FIXED_DEREF_VERSION_START;
+  
+  // constants for BYTES_VAR_DEREF
+  static final String BYTES_VAR_DEREF_CODEC_NAME_IDX = "VarDerefBytesIdx";
+  static final String BYTES_VAR_DEREF_CODEC_NAME_DAT = "VarDerefBytesDat";
+  static final int BYTES_VAR_DEREF_VERSION_START = 0;
+  static final int BYTES_VAR_DEREF_VERSION_CURRENT = BYTES_VAR_DEREF_VERSION_START;
+  
+  // constants for BYTES_FIXED_SORTED
+  static final String BYTES_FIXED_SORTED_CODEC_NAME_IDX = "FixedSortedBytesIdx";
+  static final String BYTES_FIXED_SORTED_CODEC_NAME_DAT = "FixedSortedBytesDat";
+  static final int BYTES_FIXED_SORTED_VERSION_START = 0;
+  static final int BYTES_FIXED_SORTED_VERSION_CURRENT = BYTES_FIXED_SORTED_VERSION_START;
+  
+  // constants for BYTES_VAR_SORTED
+  // NOTE THIS IS NOT A BUG! 4.0 actually screwed this up (VAR_SORTED and VAR_DEREF have same codec header)
+  static final String BYTES_VAR_SORTED_CODEC_NAME_IDX = "VarDerefBytesIdx";
+  static final String BYTES_VAR_SORTED_CODEC_NAME_DAT = "VarDerefBytesDat";
+  static final int BYTES_VAR_SORTED_VERSION_START = 0;
+  static final int BYTES_VAR_SORTED_VERSION_CURRENT = BYTES_VAR_SORTED_VERSION_START;
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
new file mode 100644
index 0000000..f70d0fb
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
@@ -0,0 +1,661 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosReader.LegacyDocValuesType;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedNumericDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Reads the 4.0 format of norms/docvalues
+ * @lucene.experimental
+ * @deprecated Only for reading old 4.0 and 4.1 segments
+ */
+@Deprecated
+final class Lucene40DocValuesReader extends DocValuesProducer {
+  private final Directory dir;
+  private final SegmentReadState state;
+  private final String legacyKey;
+  private static final String segmentSuffix = "dv";
+
+  // ram instances we have already loaded
+  private final Map<Integer,NumericDocValues> numericInstances =
+      new HashMap<>();
+  private final Map<Integer,BinaryDocValues> binaryInstances =
+      new HashMap<>();
+  private final Map<Integer,SortedDocValues> sortedInstances =
+      new HashMap<>();
+
+  private final AtomicLong ramBytesUsed;
+
+  Lucene40DocValuesReader(SegmentReadState state, String filename, String legacyKey) throws IOException {
+    this.state = state;
+    this.legacyKey = legacyKey;
+    this.dir = new CompoundFileDirectory(state.directory, filename, state.context, false);
+    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOf(getClass()));
+  }
+
+  @Override
+  public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOException {
+    NumericDocValues instance = numericInstances.get(field.number);
+    if (instance == null) {
+      String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+      IndexInput input = dir.openInput(fileName, state.context);
+      boolean success = false;
+      try {
+        switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
+          case VAR_INTS:
+            instance = loadVarIntsField(field, input);
+            break;
+          case FIXED_INTS_8:
+            instance = loadByteField(field, input);
+            break;
+          case FIXED_INTS_16:
+            instance = loadShortField(field, input);
+            break;
+          case FIXED_INTS_32:
+            instance = loadIntField(field, input);
+            break;
+          case FIXED_INTS_64:
+            instance = loadLongField(field, input);
+            break;
+          case FLOAT_32:
+            instance = loadFloatField(field, input);
+            break;
+          case FLOAT_64:
+            instance = loadDoubleField(field, input);
+            break;
+          default:
+            throw new AssertionError();
+        }
+        CodecUtil.checkEOF(input);
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(input);
+        } else {
+          IOUtils.closeWhileHandlingException(input);
+        }
+      }
+      numericInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+
+  private NumericDocValues loadVarIntsField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.VAR_INTS_CODEC_NAME,
+                                 Lucene40DocValuesFormat.VAR_INTS_VERSION_START,
+                                 Lucene40DocValuesFormat.VAR_INTS_VERSION_CURRENT);
+    byte header = input.readByte();
+    if (header == Lucene40DocValuesFormat.VAR_INTS_FIXED_64) {
+      int maxDoc = state.segmentInfo.getDocCount();
+      final long values[] = new long[maxDoc];
+      for (int i = 0; i < values.length; i++) {
+        values[i] = input.readLong();
+      }
+      ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          return values[docID];
+        }
+      };
+    } else if (header == Lucene40DocValuesFormat.VAR_INTS_PACKED) {
+      final long minValue = input.readLong();
+      final long defaultValue = input.readLong();
+      final PackedInts.Reader reader = PackedInts.getReader(input);
+      ramBytesUsed.addAndGet(reader.ramBytesUsed());
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          final long value = reader.get(docID);
+          if (value == defaultValue) {
+            return 0;
+          } else {
+            return minValue + value;
+          }
+        }
+      };
+    } else {
+      throw new CorruptIndexException("invalid VAR_INTS header byte: " + header + " (resource=" + input + ")");
+    }
+  }
+
+  private NumericDocValues loadByteField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
+                                 Lucene40DocValuesFormat.INTS_VERSION_START,
+                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 1) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final byte values[] = new byte[maxDoc];
+    input.readBytes(values, 0, values.length);
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+
+  private NumericDocValues loadShortField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
+                                 Lucene40DocValuesFormat.INTS_VERSION_START,
+                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 2) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final short values[] = new short[maxDoc];
+    for (int i = 0; i < values.length; i++) {
+      values[i] = input.readShort();
+    }
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+
+  private NumericDocValues loadIntField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
+                                 Lucene40DocValuesFormat.INTS_VERSION_START,
+                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 4) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final int values[] = new int[maxDoc];
+    for (int i = 0; i < values.length; i++) {
+      values[i] = input.readInt();
+    }
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+
+  private NumericDocValues loadLongField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
+                                 Lucene40DocValuesFormat.INTS_VERSION_START,
+                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 8) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final long values[] = new long[maxDoc];
+    for (int i = 0; i < values.length; i++) {
+      values[i] = input.readLong();
+    }
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+
+  private NumericDocValues loadFloatField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.FLOATS_CODEC_NAME,
+                                 Lucene40DocValuesFormat.FLOATS_VERSION_START,
+                                 Lucene40DocValuesFormat.FLOATS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 4) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final int values[] = new int[maxDoc];
+    for (int i = 0; i < values.length; i++) {
+      values[i] = input.readInt();
+    }
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+
+  private NumericDocValues loadDoubleField(FieldInfo field, IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.FLOATS_CODEC_NAME,
+                                 Lucene40DocValuesFormat.FLOATS_VERSION_START,
+                                 Lucene40DocValuesFormat.FLOATS_VERSION_CURRENT);
+    int valueSize = input.readInt();
+    if (valueSize != 8) {
+      throw new CorruptIndexException("invalid valueSize: " + valueSize);
+    }
+    int maxDoc = state.segmentInfo.getDocCount();
+    final long values[] = new long[maxDoc];
+    for (int i = 0; i < values.length; i++) {
+      values[i] = input.readLong();
+    }
+    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
+    return new NumericDocValues() {
+      @Override
+      public long get(int docID) {
+        return values[docID];
+      }
+    };
+  }
+
+  @Override
+  public synchronized BinaryDocValues getBinary(FieldInfo field) throws IOException {
+    BinaryDocValues instance = binaryInstances.get(field.number);
+    if (instance == null) {
+      switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
+        case BYTES_FIXED_STRAIGHT:
+          instance = loadBytesFixedStraight(field);
+          break;
+        case BYTES_VAR_STRAIGHT:
+          instance = loadBytesVarStraight(field);
+          break;
+        case BYTES_FIXED_DEREF:
+          instance = loadBytesFixedDeref(field);
+          break;
+        case BYTES_VAR_DEREF:
+          instance = loadBytesVarDeref(field);
+          break;
+        default:
+          throw new AssertionError();
+      }
+      binaryInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+
+  private BinaryDocValues loadBytesFixedStraight(FieldInfo field) throws IOException {
+    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    IndexInput input = dir.openInput(fileName, state.context);
+    boolean success = false;
+    try {
+      CodecUtil.checkHeader(input, Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_CODEC_NAME,
+                                   Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_START,
+                                   Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_CURRENT);
+      final int fixedLength = input.readInt();
+      PagedBytes bytes = new PagedBytes(16);
+      bytes.copy(input, fixedLength * (long)state.segmentInfo.getDocCount());
+      final PagedBytes.Reader bytesReader = bytes.freeze(true);
+      CodecUtil.checkEOF(input);
+      success = true;
+      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed());
+      return new BinaryDocValues() {
+
+        @Override
+        public BytesRef get(int docID) {
+          final BytesRef term = new BytesRef();
+          bytesReader.fillSlice(term, fixedLength * (long)docID, fixedLength);
+          return term;
+        }
+      };
+    } finally {
+      if (success) {
+        IOUtils.close(input);
+      } else {
+        IOUtils.closeWhileHandlingException(input);
+      }
+    }
+  }
+
+  private BinaryDocValues loadBytesVarStraight(FieldInfo field) throws IOException {
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+    IndexInput data = null;
+    IndexInput index = null;
+    boolean success = false;
+    try {
+      data = dir.openInput(dataName, state.context);
+      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_DAT,
+                                  Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_START,
+                                  Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
+      index = dir.openInput(indexName, state.context);
+      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_IDX,
+                                   Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_START,
+                                   Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
+      long totalBytes = index.readVLong();
+      PagedBytes bytes = new PagedBytes(16);
+      bytes.copy(data, totalBytes);
+      final PagedBytes.Reader bytesReader = bytes.freeze(true);
+      final PackedInts.Reader reader = PackedInts.getReader(index);
+      CodecUtil.checkEOF(data);
+      CodecUtil.checkEOF(index);
+      success = true;
+      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + reader.ramBytesUsed());
+      return new BinaryDocValues() {
+        @Override
+        public BytesRef get(int docID) {
+          final BytesRef term = new BytesRef();
+          long startAddress = reader.get(docID);
+          long endAddress = reader.get(docID+1);
+          bytesReader.fillSlice(term, startAddress, (int)(endAddress - startAddress));
+          return term;
+        }
+      };
+    } finally {
+      if (success) {
+        IOUtils.close(data, index);
+      } else {
+        IOUtils.closeWhileHandlingException(data, index);
+      }
+    }
+  }
+
+  private BinaryDocValues loadBytesFixedDeref(FieldInfo field) throws IOException {
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+    IndexInput data = null;
+    IndexInput index = null;
+    boolean success = false;
+    try {
+      data = dir.openInput(dataName, state.context);
+      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_DAT,
+                                  Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_START,
+                                  Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
+      index = dir.openInput(indexName, state.context);
+      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_IDX,
+                                   Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_START,
+                                   Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
+
+      final int fixedLength = data.readInt();
+      final int valueCount = index.readInt();
+      PagedBytes bytes = new PagedBytes(16);
+      bytes.copy(data, fixedLength * (long) valueCount);
+      final PagedBytes.Reader bytesReader = bytes.freeze(true);
+      final PackedInts.Reader reader = PackedInts.getReader(index);
+      CodecUtil.checkEOF(data);
+      CodecUtil.checkEOF(index);
+      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + reader.ramBytesUsed());
+      success = true;
+      return new BinaryDocValues() {
+        @Override
+        public BytesRef get(int docID) {
+          final BytesRef term = new BytesRef();
+          final long offset = fixedLength * reader.get(docID);
+          bytesReader.fillSlice(term, offset, fixedLength);
+          return term;
+        }
+      };
+    } finally {
+      if (success) {
+        IOUtils.close(data, index);
+      } else {
+        IOUtils.closeWhileHandlingException(data, index);
+      }
+    }
+  }
+
+  private BinaryDocValues loadBytesVarDeref(FieldInfo field) throws IOException {
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+    IndexInput data = null;
+    IndexInput index = null;
+    boolean success = false;
+    try {
+      data = dir.openInput(dataName, state.context);
+      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_DAT,
+                                  Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_START,
+                                  Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
+      index = dir.openInput(indexName, state.context);
+      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_IDX,
+                                   Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_START,
+                                   Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
+
+      final long totalBytes = index.readLong();
+      final PagedBytes bytes = new PagedBytes(16);
+      bytes.copy(data, totalBytes);
+      final PagedBytes.Reader bytesReader = bytes.freeze(true);
+      final PackedInts.Reader reader = PackedInts.getReader(index);
+      CodecUtil.checkEOF(data);
+      CodecUtil.checkEOF(index);
+      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + reader.ramBytesUsed());
+      success = true;
+      return new BinaryDocValues() {
+        
+        @Override
+        public BytesRef get(int docID) {
+          final BytesRef term = new BytesRef();
+          long startAddress = reader.get(docID);
+          BytesRef lengthBytes = new BytesRef();
+          bytesReader.fillSlice(lengthBytes, startAddress, 1);
+          byte code = lengthBytes.bytes[lengthBytes.offset];
+          if ((code & 128) == 0) {
+            // length is 1 byte
+            bytesReader.fillSlice(term, startAddress + 1, (int) code);
+          } else {
+            bytesReader.fillSlice(lengthBytes, startAddress + 1, 1);
+            int length = ((code & 0x7f) << 8) | (lengthBytes.bytes[lengthBytes.offset] & 0xff);
+            bytesReader.fillSlice(term, startAddress + 2, length);
+          }
+          return term;
+        }
+      };
+    } finally {
+      if (success) {
+        IOUtils.close(data, index);
+      } else {
+        IOUtils.closeWhileHandlingException(data, index);
+      }
+    }
+  }
+
+  @Override
+  public synchronized SortedDocValues getSorted(FieldInfo field) throws IOException {
+    SortedDocValues instance = sortedInstances.get(field.number);
+    if (instance == null) {
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+      String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+      IndexInput data = null;
+      IndexInput index = null;
+      boolean success = false;
+      try {
+        data = dir.openInput(dataName, state.context);
+        index = dir.openInput(indexName, state.context);
+        switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
+          case BYTES_FIXED_SORTED:
+            instance = loadBytesFixedSorted(field, data, index);
+            break;
+          case BYTES_VAR_SORTED:
+            instance = loadBytesVarSorted(field, data, index);
+            break;
+          default:
+            throw new AssertionError();
+        }
+        CodecUtil.checkEOF(data);
+        CodecUtil.checkEOF(index);
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(data, index);
+        } else {
+          IOUtils.closeWhileHandlingException(data, index);
+        }
+      }
+      sortedInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+
+  private SortedDocValues loadBytesFixedSorted(FieldInfo field, IndexInput data, IndexInput index) throws IOException {
+    CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_DAT,
+                                Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_START,
+                                Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
+    CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_IDX,
+                                 Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_START,
+                                 Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
+
+    final int fixedLength = data.readInt();
+    final int valueCount = index.readInt();
+
+    PagedBytes bytes = new PagedBytes(16);
+    bytes.copy(data, fixedLength * (long) valueCount);
+    final PagedBytes.Reader bytesReader = bytes.freeze(true);
+    final PackedInts.Reader reader = PackedInts.getReader(index);
+    ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + reader.ramBytesUsed());
+
+    return correctBuggyOrds(new SortedDocValues() {
+      @Override
+      public int getOrd(int docID) {
+        return (int) reader.get(docID);
+      }
+
+      @Override
+      public BytesRef lookupOrd(int ord) {
+        final BytesRef term = new BytesRef();
+        bytesReader.fillSlice(term, fixedLength * (long) ord, fixedLength);
+        return term;
+      }
+
+      @Override
+      public int getValueCount() {
+        return valueCount;
+      }
+    });
+  }
+
+  private SortedDocValues loadBytesVarSorted(FieldInfo field, IndexInput data, IndexInput index) throws IOException {
+    CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_DAT,
+                                Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_START,
+                                Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
+    CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_IDX,
+                                 Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_START,
+                                 Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
+
+    long maxAddress = index.readLong();
+    PagedBytes bytes = new PagedBytes(16);
+    bytes.copy(data, maxAddress);
+    final PagedBytes.Reader bytesReader = bytes.freeze(true);
+    final PackedInts.Reader addressReader = PackedInts.getReader(index);
+    final PackedInts.Reader ordsReader = PackedInts.getReader(index);
+
+    final int valueCount = addressReader.size() - 1;
+    ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + addressReader.ramBytesUsed() + ordsReader.ramBytesUsed());
+
+    return correctBuggyOrds(new SortedDocValues() {
+      @Override
+      public int getOrd(int docID) {
+        return (int)ordsReader.get(docID);
+      }
+
+      @Override
+      public BytesRef lookupOrd(int ord) {
+        final BytesRef term = new BytesRef();
+        long startAddress = addressReader.get(ord);
+        long endAddress = addressReader.get(ord+1);
+        bytesReader.fillSlice(term, startAddress, (int)(endAddress - startAddress));
+        return term;
+      }
+
+      @Override
+      public int getValueCount() {
+        return valueCount;
+      }
+    });
+  }
+
+  // detects and corrects LUCENE-4717 in old indexes
+  private SortedDocValues correctBuggyOrds(final SortedDocValues in) {
+    final int maxDoc = state.segmentInfo.getDocCount();
+    for (int i = 0; i < maxDoc; i++) {
+      if (in.getOrd(i) == 0) {
+        return in; // ok
+      }
+    }
+
+    // we had ord holes, return an ord-shifting-impl that corrects the bug
+    return new SortedDocValues() {
+      @Override
+      public int getOrd(int docID) {
+        return in.getOrd(docID) - 1;
+      }
+
+      @Override
+      public BytesRef lookupOrd(int ord) {
+        return in.lookupOrd(ord+1);
+      }
+
+      @Override
+      public int getValueCount() {
+        return in.getValueCount() - 1;
+      }
+    };
+  }
+  
+  @Override
+  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
+    throw new IllegalStateException("Lucene 4.0 does not support SortedNumeric: how did you pull this off?");
+  }
+
+  @Override
+  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
+    throw new IllegalStateException("Lucene 4.0 does not support SortedSet: how did you pull this off?");
+  }
+
+  @Override
+  public Bits getDocsWithField(FieldInfo field) throws IOException {
+    return new Bits.MatchAllBits(state.segmentInfo.getDocCount());
+  }
+
+  @Override
+  public void close() throws IOException {
+    dir.close();
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return ramBytesUsed.get();
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java
new file mode 100644
index 0000000..bf51c65
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java
@@ -0,0 +1,129 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.store.DataOutput; // javadoc
+
+/**
+ * Lucene 4.0 Field Infos format.
+ * <p>
+ * <p>Field names are stored in the field info file, with suffix <tt>.fnm</tt>.</p>
+ * <p>FieldInfos (.fnm) --&gt; Header,FieldsCount, &lt;FieldName,FieldNumber,
+ * FieldBits,DocValuesBits,Attributes&gt; <sup>FieldsCount</sup></p>
+ * <p>Data types:
+ * <ul>
+ *   <li>Header --&gt; {@link CodecUtil#checkHeader CodecHeader}</li>
+ *   <li>FieldsCount --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>FieldName --&gt; {@link DataOutput#writeString String}</li>
+ *   <li>FieldBits, DocValuesBits --&gt; {@link DataOutput#writeByte Byte}</li>
+ *   <li>FieldNumber --&gt; {@link DataOutput#writeInt VInt}</li>
+ *   <li>Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
+ * </ul>
+ * </p>
+ * Field Descriptions:
+ * <ul>
+ *   <li>FieldsCount: the number of fields in this file.</li>
+ *   <li>FieldName: name of the field as a UTF-8 String.</li>
+ *   <li>FieldNumber: the field's number. Note that unlike previous versions of
+ *       Lucene, the fields are not numbered implicitly by their order in the
+ *       file, instead explicitly.</li>
+ *   <li>FieldBits: a byte containing field options.
+ *       <ul>
+ *         <li>The low-order bit is one for indexed fields, and zero for non-indexed
+ *             fields.</li>
+ *         <li>The second lowest-order bit is one for fields that have term vectors
+ *             stored, and zero for fields without term vectors.</li>
+ *         <li>If the third lowest order-bit is set (0x4), offsets are stored into
+ *             the postings list in addition to positions.</li>
+ *         <li>Fourth bit is unused.</li>
+ *         <li>If the fifth lowest-order bit is set (0x10), norms are omitted for the
+ *             indexed field.</li>
+ *         <li>If the sixth lowest-order bit is set (0x20), payloads are stored for the
+ *             indexed field.</li>
+ *         <li>If the seventh lowest-order bit is set (0x40), term frequencies and
+ *             positions omitted for the indexed field.</li>
+ *         <li>If the eighth lowest-order bit is set (0x80), positions are omitted for the
+ *             indexed field.</li>
+ *       </ul>
+ *    </li>
+ *    <li>DocValuesBits: a byte containing per-document value types. The type
+ *        recorded as two four-bit integers, with the high-order bits representing
+ *        <code>norms</code> options, and the low-order bits representing 
+ *        {@code DocValues} options. Each four-bit integer can be decoded as such:
+ *        <ul>
+ *          <li>0: no DocValues for this field.</li>
+ *          <li>1: variable-width signed integers. ({@code Type#VAR_INTS VAR_INTS})</li>
+ *          <li>2: 32-bit floating point values. ({@code Type#FLOAT_32 FLOAT_32})</li>
+ *          <li>3: 64-bit floating point values. ({@code Type#FLOAT_64 FLOAT_64})</li>
+ *          <li>4: fixed-length byte array values. ({@code Type#BYTES_FIXED_STRAIGHT BYTES_FIXED_STRAIGHT})</li>
+ *          <li>5: fixed-length dereferenced byte array values. ({@code Type#BYTES_FIXED_DEREF BYTES_FIXED_DEREF})</li>
+ *          <li>6: variable-length byte array values. ({@code Type#BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT})</li>
+ *          <li>7: variable-length dereferenced byte array values. ({@code Type#BYTES_VAR_DEREF BYTES_VAR_DEREF})</li>
+ *          <li>8: 16-bit signed integers. ({@code Type#FIXED_INTS_16 FIXED_INTS_16})</li>
+ *          <li>9: 32-bit signed integers. ({@code Type#FIXED_INTS_32 FIXED_INTS_32})</li>
+ *          <li>10: 64-bit signed integers. ({@code Type#FIXED_INTS_64 FIXED_INTS_64})</li>
+ *          <li>11: 8-bit signed integers. ({@code Type#FIXED_INTS_8 FIXED_INTS_8})</li>
+ *          <li>12: fixed-length sorted byte array values. ({@code Type#BYTES_FIXED_SORTED BYTES_FIXED_SORTED})</li>
+ *          <li>13: variable-length sorted byte array values. ({@code Type#BYTES_VAR_SORTED BYTES_VAR_SORTED})</li>
+ *        </ul>
+ *    </li>
+ *    <li>Attributes: a key-value map of codec-private attributes.</li>
+ * </ul>
+ *
+ * @lucene.experimental
+ * @deprecated Only for reading old 4.0 and 4.1 segments
+ */
+@Deprecated
+public class Lucene40FieldInfosFormat extends FieldInfosFormat {
+  private final FieldInfosReader reader = new Lucene40FieldInfosReader();
+  
+  /** Sole constructor. */
+  public Lucene40FieldInfosFormat() {
+  }
+
+  @Override
+  public FieldInfosReader getFieldInfosReader() throws IOException {
+    return reader;
+  }
+
+  @Override
+  public FieldInfosWriter getFieldInfosWriter() throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+  
+  /** Extension of field infos */
+  static final String FIELD_INFOS_EXTENSION = "fnm";
+  
+  static final String CODEC_NAME = "Lucene40FieldInfos";
+  static final int FORMAT_START = 0;
+  static final int FORMAT_CURRENT = FORMAT_START;
+  
+  static final byte IS_INDEXED = 0x1;
+  static final byte STORE_TERMVECTOR = 0x2;
+  static final byte STORE_OFFSETS_IN_POSTINGS = 0x4;
+  static final byte OMIT_NORMS = 0x10;
+  static final byte STORE_PAYLOADS = 0x20;
+  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
+  static final byte OMIT_POSITIONS = -128;
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java
new file mode 100644
index 0000000..6fc195c
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java
@@ -0,0 +1,153 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 4.0 FieldInfos reader.
+ * 
+ * @lucene.experimental
+ * @see Lucene40FieldInfosFormat
+ * @deprecated Only for reading old 4.0 and 4.1 segments
+ */
+@Deprecated
+class Lucene40FieldInfosReader extends FieldInfosReader {
+
+  /** Sole constructor. */
+  public Lucene40FieldInfosReader() {
+  }
+
+  @Override
+  public FieldInfos read(Directory directory, String segmentName, String segmentSuffix, IOContext iocontext) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene40FieldInfosFormat.FIELD_INFOS_EXTENSION);
+    IndexInput input = directory.openInput(fileName, iocontext);
+    
+    boolean success = false;
+    try {
+      CodecUtil.checkHeader(input, Lucene40FieldInfosFormat.CODEC_NAME, 
+                                   Lucene40FieldInfosFormat.FORMAT_START, 
+                                   Lucene40FieldInfosFormat.FORMAT_CURRENT);
+
+      final int size = input.readVInt(); //read in the size
+      FieldInfo infos[] = new FieldInfo[size];
+
+      for (int i = 0; i < size; i++) {
+        String name = input.readString();
+        final int fieldNumber = input.readVInt();
+        byte bits = input.readByte();
+        boolean isIndexed = (bits & Lucene40FieldInfosFormat.IS_INDEXED) != 0;
+        boolean storeTermVector = (bits & Lucene40FieldInfosFormat.STORE_TERMVECTOR) != 0;
+        boolean omitNorms = (bits & Lucene40FieldInfosFormat.OMIT_NORMS) != 0;
+        boolean storePayloads = (bits & Lucene40FieldInfosFormat.STORE_PAYLOADS) != 0;
+        final IndexOptions indexOptions;
+        if (!isIndexed) {
+          indexOptions = null;
+        } else if ((bits & Lucene40FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
+          indexOptions = IndexOptions.DOCS_ONLY;
+        } else if ((bits & Lucene40FieldInfosFormat.OMIT_POSITIONS) != 0) {
+          indexOptions = IndexOptions.DOCS_AND_FREQS;
+        } else if ((bits & Lucene40FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS) != 0) {
+          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
+        } else {
+          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+        }
+
+        // LUCENE-3027: past indices were able to write
+        // storePayloads=true when omitTFAP is also true,
+        // which is invalid.  We correct that, here:
+        if (isIndexed && indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+          storePayloads = false;
+        }
+        // DV Types are packed in one byte
+        byte val = input.readByte();
+        final LegacyDocValuesType oldValuesType = getDocValuesType((byte) (val & 0x0F));
+        final LegacyDocValuesType oldNormsType = getDocValuesType((byte) ((val >>> 4) & 0x0F));
+        final Map<String,String> attributes = input.readStringStringMap();;
+        if (oldValuesType.mapping != null) {
+          attributes.put(LEGACY_DV_TYPE_KEY, oldValuesType.name());
+        }
+        if (oldNormsType.mapping != null) {
+          if (oldNormsType.mapping != DocValuesType.NUMERIC) {
+            throw new CorruptIndexException("invalid norm type: " + oldNormsType + " (resource=" + input + ")");
+          }
+          attributes.put(LEGACY_NORM_TYPE_KEY, oldNormsType.name());
+        }
+        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
+          omitNorms, storePayloads, indexOptions, oldValuesType.mapping, oldNormsType.mapping, -1, Collections.unmodifiableMap(attributes));
+      }
+
+      CodecUtil.checkEOF(input);
+      FieldInfos fieldInfos = new FieldInfos(infos);
+      success = true;
+      return fieldInfos;
+    } finally {
+      if (success) {
+        input.close();
+      } else {
+        IOUtils.closeWhileHandlingException(input);
+      }
+    }
+  }
+  
+  static final String LEGACY_DV_TYPE_KEY = Lucene40FieldInfosReader.class.getSimpleName() + ".dvtype";
+  static final String LEGACY_NORM_TYPE_KEY = Lucene40FieldInfosReader.class.getSimpleName() + ".normtype";
+  
+  // mapping of 4.0 types -> 4.2 types
+  static enum LegacyDocValuesType {
+    NONE(null),
+    VAR_INTS(DocValuesType.NUMERIC),
+    FLOAT_32(DocValuesType.NUMERIC),
+    FLOAT_64(DocValuesType.NUMERIC),
+    BYTES_FIXED_STRAIGHT(DocValuesType.BINARY),
+    BYTES_FIXED_DEREF(DocValuesType.BINARY),
+    BYTES_VAR_STRAIGHT(DocValuesType.BINARY),
+    BYTES_VAR_DEREF(DocValuesType.BINARY),
+    FIXED_INTS_16(DocValuesType.NUMERIC),
+    FIXED_INTS_32(DocValuesType.NUMERIC),
+    FIXED_INTS_64(DocValuesType.NUMERIC),
+    FIXED_INTS_8(DocValuesType.NUMERIC),
+    BYTES_FIXED_SORTED(DocValuesType.SORTED),
+    BYTES_VAR_SORTED(DocValuesType.SORTED);
+    
+    final DocValuesType mapping;
+    LegacyDocValuesType(DocValuesType mapping) {
+      this.mapping = mapping;
+    }
+  }
+  
+  // decodes a 4.0 type
+  private static LegacyDocValuesType getDocValuesType(byte b) {
+    return LegacyDocValuesType.values()[b];
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
new file mode 100644
index 0000000..ed2f507
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
@@ -0,0 +1,63 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.CompoundFileDirectory;
+
+/**
+ * Lucene 4.0 Norms Format.
+ * <p>
+ * Files:
+ * <ul>
+ *   <li><tt>.nrm.cfs</tt>: {@link CompoundFileDirectory compound container}</li>
+ *   <li><tt>.nrm.cfe</tt>: {@link CompoundFileDirectory compound entries}</li>
+ * </ul>
+ * Norms are implemented as DocValues, so other than file extension, norms are 
+ * written exactly the same way as {@link Lucene40DocValuesFormat DocValues}.
+ * 
+ * @see Lucene40DocValuesFormat
+ * @lucene.experimental
+ * @deprecated Only for reading old 4.0 and 4.1 segments
+ */
+@Deprecated
+public class Lucene40NormsFormat extends NormsFormat {
+
+  /** Sole constructor. */
+  public Lucene40NormsFormat() {}
+  
+  @Override
+  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+
+  @Override
+  public NormsProducer normsProducer(SegmentReadState state) throws IOException {
+    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+                                                     "nrm", 
+                                                     IndexFileNames.COMPOUND_FILE_EXTENSION);
+    return new Lucene40NormsReader(state, filename);
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java
new file mode 100644
index 0000000..6ca7bc7
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java
@@ -0,0 +1,59 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+
+/**
+ * Reads 4.0/4.1 norms.
+ * Implemented the same as docvalues, but with a different filename.
+ * @deprecated Only for reading old 4.0 and 4.1 segments
+ */
+@Deprecated
+class Lucene40NormsReader extends NormsProducer {
+  private final Lucene40DocValuesReader impl;
+  
+  public Lucene40NormsReader(SegmentReadState state, String filename) throws IOException {
+    impl = new Lucene40DocValuesReader(state, filename, Lucene40FieldInfosReader.LEGACY_NORM_TYPE_KEY);
+  }
+  
+  @Override
+  public NumericDocValues getNorms(FieldInfo field) throws IOException {
+    return impl.getNumeric(field);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    impl.close();
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return impl.ramBytesUsed();
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    impl.checkIntegrity();
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java
new file mode 100644
index 0000000..435a5ee
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java
@@ -0,0 +1,52 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.PostingsBaseFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/** 
+ * Provides a {@link PostingsReaderBase} and {@link
+ * PostingsWriterBase}.
+ *
+ * @deprecated Only for reading old 4.0 segments */
+
+// TODO: should these also be named / looked up via SPI?
+@Deprecated
+public final class Lucene40PostingsBaseFormat extends PostingsBaseFormat {
+
+  /** Sole constructor. */
+  public Lucene40PostingsBaseFormat() {
+    super("Lucene40");
+  }
+
+  @Override
+  public PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException {
+    return new Lucene40PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+  }
+
+  @Override
+  public PostingsWriterBase postingsWriterBase(SegmentWriteState state) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
new file mode 100644
index 0000000..93b21a7
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
@@ -0,0 +1,281 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase; // javadocs
+import org.apache.lucene.codecs.blocktree.BlockTreeTermsReader;
+import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
+import org.apache.lucene.index.DocsEnum; // javadocs
+import org.apache.lucene.index.FieldInfo.IndexOptions; // javadocs
+import org.apache.lucene.index.FieldInfos; // javadocs
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.DataOutput; // javadocs
+import org.apache.lucene.util.fst.FST; // javadocs
+
+/** 
+ * Lucene 4.0 Postings format.
+ * <p>
+ * Files:
+ * <ul>
+ *   <li><tt>.tim</tt>: <a href="#Termdictionary">Term Dictionary</a></li>
+ *   <li><tt>.tip</tt>: <a href="#Termindex">Term Index</a></li>
+ *   <li><tt>.frq</tt>: <a href="#Frequencies">Frequencies</a></li>
+ *   <li><tt>.prx</tt>: <a href="#Positions">Positions</a></li>
+ * </ul>
+ * </p>
+ * <p>
+ * <a name="Termdictionary" id="Termdictionary"></a>
+ * <h3>Term Dictionary</h3>
+ *
+ * <p>The .tim file contains the list of terms in each
+ * field along with per-term statistics (such as docfreq)
+ * and pointers to the frequencies, positions and
+ * skip data in the .frq and .prx files.
+ * See {@link BlockTreeTermsWriter} for more details on the format.
+ * </p>
+ *
+ * <p>NOTE: The term dictionary can plug into different postings implementations:
+ * the postings writer/reader are actually responsible for encoding 
+ * and decoding the Postings Metadata and Term Metadata sections described here:</p>
+ * <ul>
+ *    <li>Postings Metadata --&gt; Header, SkipInterval, MaxSkipLevels, SkipMinimum</li>
+ *    <li>Term Metadata --&gt; FreqDelta, SkipDelta?, ProxDelta?
+ *    <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *    <li>SkipInterval,MaxSkipLevels,SkipMinimum --&gt; {@link DataOutput#writeInt Uint32}</li>
+ *    <li>SkipDelta,FreqDelta,ProxDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
+ * </ul>
+ * <p>Notes:</p>
+ * <ul>
+ *    <li>Header is a {@link CodecUtil#writeHeader CodecHeader} storing the version information
+ *        for the postings.</li>
+ *    <li>SkipInterval is the fraction of TermDocs stored in skip tables. It is used to accelerate 
+ *        {@link DocsEnum#advance(int)}. Larger values result in smaller indexes, greater 
+ *        acceleration, but fewer accelerable cases, while smaller values result in bigger indexes, 
+ *        less acceleration (in case of a small value for MaxSkipLevels) and more accelerable cases.
+ *        </li>
+ *    <li>MaxSkipLevels is the max. number of skip levels stored for each term in the .frq file. A 
+ *        low value results in smaller indexes but less acceleration, a larger value results in 
+ *        slightly larger indexes but greater acceleration. See format of .frq file for more 
+ *        information about skip levels.</li>
+ *    <li>SkipMinimum is the minimum document frequency a term must have in order to write any 
+ *        skip data at all.</li>
+ *    <li>FreqDelta determines the position of this term's TermFreqs within the .frq
+ *        file. In particular, it is the difference between the position of this term's
+ *        data in that file and the position of the previous term's data (or zero, for
+ *        the first term in the block).</li>
+ *    <li>ProxDelta determines the position of this term's TermPositions within the
+ *        .prx file. In particular, it is the difference between the position of this
+ *        term's data in that file and the position of the previous term's data (or zero,
+ *        for the first term in the block. For fields that omit position data, this will
+ *        be 0 since prox information is not stored.</li>
+ *    <li>SkipDelta determines the position of this term's SkipData within the .frq
+ *        file. In particular, it is the number of bytes after TermFreqs that the
+ *        SkipData starts. In other words, it is the length of the TermFreq data.
+ *        SkipDelta is only stored if DocFreq is not smaller than SkipMinimum.</li>
+ * </ul>
+ * <a name="Termindex" id="Termindex"></a>
+ * <h3>Term Index</h3>
+ * <p>The .tip file contains an index into the term dictionary, so that it can be 
+ * accessed randomly.  See {@link BlockTreeTermsWriter} for more details on the format.</p>
+ * <a name="Frequencies" id="Frequencies"></a>
+ * <h3>Frequencies</h3>
+ * <p>The .frq file contains the lists of documents which contain each term, along
+ * with the frequency of the term in that document (except when frequencies are
+ * omitted: {@link IndexOptions#DOCS_ONLY}).</p>
+ * <ul>
+ *   <li>FreqFile (.frq) --&gt; Header, &lt;TermFreqs, SkipData?&gt; <sup>TermCount</sup></li>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>TermFreqs --&gt; &lt;TermFreq&gt; <sup>DocFreq</sup></li>
+ *   <li>TermFreq --&gt; DocDelta[, Freq?]</li>
+ *   <li>SkipData --&gt; &lt;&lt;SkipLevelLength, SkipLevel&gt;
+ *       <sup>NumSkipLevels-1</sup>, SkipLevel&gt; &lt;SkipDatum&gt;</li>
+ *   <li>SkipLevel --&gt; &lt;SkipDatum&gt; <sup>DocFreq/(SkipInterval^(Level +
+ *       1))</sup></li>
+ *   <li>SkipDatum --&gt;
+ *       DocSkip,PayloadLength?,OffsetLength?,FreqSkip,ProxSkip,SkipChildLevelPointer?</li>
+ *   <li>DocDelta,Freq,DocSkip,PayloadLength,OffsetLength,FreqSkip,ProxSkip --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>SkipChildLevelPointer --&gt; {@link DataOutput#writeVLong VLong}</li>
+ * </ul>
+ * <p>TermFreqs are ordered by term (the term is implicit, from the term dictionary).</p>
+ * <p>TermFreq entries are ordered by increasing document number.</p>
+ * <p>DocDelta: if frequencies are indexed, this determines both the document
+ * number and the frequency. In particular, DocDelta/2 is the difference between
+ * this document number and the previous document number (or zero when this is the
+ * first document in a TermFreqs). When DocDelta is odd, the frequency is one.
+ * When DocDelta is even, the frequency is read as another VInt. If frequencies
+ * are omitted, DocDelta contains the gap (not multiplied by 2) between document
+ * numbers and no frequency information is stored.</p>
+ * <p>For example, the TermFreqs for a term which occurs once in document seven
+ * and three times in document eleven, with frequencies indexed, would be the
+ * following sequence of VInts:</p>
+ * <p>15, 8, 3</p>
+ * <p>If frequencies were omitted ({@link IndexOptions#DOCS_ONLY}) it would be this
+ * sequence of VInts instead:</p>
+ * <p>7,4</p>
+ * <p>DocSkip records the document number before every SkipInterval <sup>th</sup>
+ * document in TermFreqs. If payloads and offsets are disabled for the term's field, then
+ * DocSkip represents the difference from the previous value in the sequence. If
+ * payloads and/or offsets are enabled for the term's field, then DocSkip/2 represents the
+ * difference from the previous value in the sequence. In this case when
+ * DocSkip is odd, then PayloadLength and/or OffsetLength are stored indicating the length of 
+ * the last payload/offset before the SkipInterval<sup>th</sup> document in TermPositions.</p>
+ * <p>PayloadLength indicates the length of the last payload.</p>
+ * <p>OffsetLength indicates the length of the last offset (endOffset-startOffset).</p>
+ * <p>
+ * FreqSkip and ProxSkip record the position of every SkipInterval <sup>th</sup>
+ * entry in FreqFile and ProxFile, respectively. File positions are relative to
+ * the start of TermFreqs and Positions, to the previous SkipDatum in the
+ * sequence.</p>
+ * <p>For example, if DocFreq=35 and SkipInterval=16, then there are two SkipData
+ * entries, containing the 15 <sup>th</sup> and 31 <sup>st</sup> document numbers
+ * in TermFreqs. The first FreqSkip names the number of bytes after the beginning
+ * of TermFreqs that the 16 <sup>th</sup> SkipDatum starts, and the second the
+ * number of bytes after that that the 32 <sup>nd</sup> starts. The first ProxSkip
+ * names the number of bytes after the beginning of Positions that the 16
+ * <sup>th</sup> SkipDatum starts, and the second the number of bytes after that
+ * that the 32 <sup>nd</sup> starts.</p>
+ * <p>Each term can have multiple skip levels. The amount of skip levels for a
+ * term is NumSkipLevels = Min(MaxSkipLevels,
+ * floor(log(DocFreq/log(SkipInterval)))). The number of SkipData entries for a
+ * skip level is DocFreq/(SkipInterval^(Level + 1)), whereas the lowest skip level
+ * is Level=0.<br>
+ * Example: SkipInterval = 4, MaxSkipLevels = 2, DocFreq = 35. Then skip level 0
+ * has 8 SkipData entries, containing the 3<sup>rd</sup>, 7<sup>th</sup>,
+ * 11<sup>th</sup>, 15<sup>th</sup>, 19<sup>th</sup>, 23<sup>rd</sup>,
+ * 27<sup>th</sup>, and 31<sup>st</sup> document numbers in TermFreqs. Skip level
+ * 1 has 2 SkipData entries, containing the 15<sup>th</sup> and 31<sup>st</sup>
+ * document numbers in TermFreqs.<br>
+ * The SkipData entries on all upper levels &gt; 0 contain a SkipChildLevelPointer
+ * referencing the corresponding SkipData entry in level-1. In the example has
+ * entry 15 on level 1 a pointer to entry 15 on level 0 and entry 31 on level 1 a
+ * pointer to entry 31 on level 0.
+ * </p>
+ * <a name="Positions" id="Positions"></a>
+ * <h3>Positions</h3>
+ * <p>The .prx file contains the lists of positions that each term occurs at
+ * within documents. Note that fields omitting positional data do not store
+ * anything into this file, and if all fields in the index omit positional data
+ * then the .prx file will not exist.</p>
+ * <ul>
+ *   <li>ProxFile (.prx) --&gt; Header, &lt;TermPositions&gt; <sup>TermCount</sup></li>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>TermPositions --&gt; &lt;Positions&gt; <sup>DocFreq</sup></li>
+ *   <li>Positions --&gt; &lt;PositionDelta,PayloadLength?,OffsetDelta?,OffsetLength?,PayloadData?&gt; <sup>Freq</sup></li>
+ *   <li>PositionDelta,OffsetDelta,OffsetLength,PayloadLength --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>PayloadData --&gt; {@link DataOutput#writeByte byte}<sup>PayloadLength</sup></li>
+ * </ul>
+ * <p>TermPositions are ordered by term (the term is implicit, from the term dictionary).</p>
+ * <p>Positions entries are ordered by increasing document number (the document
+ * number is implicit from the .frq file).</p>
+ * <p>PositionDelta is, if payloads are disabled for the term's field, the
+ * difference between the position of the current occurrence in the document and
+ * the previous occurrence (or zero, if this is the first occurrence in this
+ * document). If payloads are enabled for the term's field, then PositionDelta/2
+ * is the difference between the current and the previous position. If payloads
+ * are enabled and PositionDelta is odd, then PayloadLength is stored, indicating
+ * the length of the payload at the current term position.</p>
+ * <p>For example, the TermPositions for a term which occurs as the fourth term in
+ * one document, and as the fifth and ninth term in a subsequent document, would
+ * be the following sequence of VInts (payloads disabled):</p>
+ * <p>4, 5, 4</p>
+ * <p>PayloadData is metadata associated with the current term position. If
+ * PayloadLength is stored at the current position, then it indicates the length
+ * of this payload. If PayloadLength is not stored, then this payload has the same
+ * length as the payload at the previous position.</p>
+ * <p>OffsetDelta/2 is the difference between this position's startOffset from the
+ * previous occurrence (or zero, if this is the first occurrence in this document).
+ * If OffsetDelta is odd, then the length (endOffset-startOffset) differs from the
+ * previous occurrence and an OffsetLength follows. Offset data is only written for
+ * {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}.</p>
+ * 
+ *  @deprecated Only for reading old 4.0 segments */
+
+// TODO: this class could be created by wrapping
+// BlockTreeTermsDict around Lucene40PostingsBaseFormat; ie
+// we should not duplicate the code from that class here:
+@Deprecated
+public class Lucene40PostingsFormat extends PostingsFormat {
+
+  /** minimum items (terms or sub-blocks) per block for BlockTree */
+  protected final int minBlockSize;
+  /** maximum items (terms or sub-blocks) per block for BlockTree */
+  protected final int maxBlockSize;
+
+  /** Creates {@code Lucene40PostingsFormat} with default
+   *  settings. */
+  public Lucene40PostingsFormat() {
+    this(BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+  }
+
+  /** Creates {@code Lucene40PostingsFormat} with custom
+   *  values for {@code minBlockSize} and {@code
+   *  maxBlockSize} passed to block terms dictionary.
+   *  @see BlockTreeTermsWriter#BlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int) */
+  private Lucene40PostingsFormat(int minBlockSize, int maxBlockSize) {
+    super("Lucene40");
+    this.minBlockSize = minBlockSize;
+    assert minBlockSize > 1;
+    this.maxBlockSize = maxBlockSize;
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postings = new Lucene40PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+
+    boolean success = false;
+    try {
+      FieldsProducer ret = new BlockTreeTermsReader(
+                                                    state.directory,
+                                                    state.fieldInfos,
+                                                    state.segmentInfo,
+                                                    postings,
+                                                    state.context,
+                                                    state.segmentSuffix);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        postings.close();
+      }
+    }
+  }
+
+  /** Extension of freq postings file */
+  static final String FREQ_EXTENSION = "frq";
+
+  /** Extension of prox postings file */
+  static final String PROX_EXTENSION = "prx";
+
+  @Override
+  public String toString() {
+    return getName() + "(minBlockSize=" + minBlockSize + " maxBlockSize=" + maxBlockSize + ")";
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
new file mode 100644
index 0000000..0730d7b
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
@@ -0,0 +1,1170 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.IOUtils;
+
+/** 
+ * Concrete class that reads the 4.0 frq/prox
+ * postings format. 
+ *  
+ *  @see Lucene40PostingsFormat
+ *  @deprecated Only for reading old 4.0 segments */
+@Deprecated
+public class Lucene40PostingsReader extends PostingsReaderBase {
+
+  final static String TERMS_CODEC = "Lucene40PostingsWriterTerms";
+  final static String FRQ_CODEC = "Lucene40PostingsWriterFrq";
+  final static String PRX_CODEC = "Lucene40PostingsWriterPrx";
+
+  //private static boolean DEBUG = BlockTreeTermsWriter.DEBUG;
+  
+  // Increment version to change it:
+  final static int VERSION_START = 0;
+  final static int VERSION_LONG_SKIP = 1;
+  final static int VERSION_CURRENT = VERSION_LONG_SKIP;
+
+  private final IndexInput freqIn;
+  private final IndexInput proxIn;
+  // public static boolean DEBUG = BlockTreeTermsWriter.DEBUG;
+
+  int skipInterval;
+  int maxSkipLevels;
+  int skipMinimum;
+
+  // private String segment;
+
+  /** Sole constructor. */
+  public Lucene40PostingsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo segmentInfo, IOContext ioContext, String segmentSuffix) throws IOException {
+    boolean success = false;
+    IndexInput freqIn = null;
+    IndexInput proxIn = null;
+    try {
+      freqIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION),
+                           ioContext);
+      CodecUtil.checkHeader(freqIn, FRQ_CODEC, VERSION_START, VERSION_CURRENT);
+      // TODO: hasProx should (somehow!) become codec private,
+      // but it's tricky because 1) FIS.hasProx is global (it
+      // could be all fields that have prox are written by a
+      // different codec), 2) the field may have had prox in
+      // the past but all docs w/ that field were deleted.
+      // Really we'd need to init prxOut lazily on write, and
+      // then somewhere record that we actually wrote it so we
+      // know whether to open on read:
+      if (fieldInfos.hasProx()) {
+        proxIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION),
+                             ioContext);
+        CodecUtil.checkHeader(proxIn, PRX_CODEC, VERSION_START, VERSION_CURRENT);
+      } else {
+        proxIn = null;
+      }
+      this.freqIn = freqIn;
+      this.proxIn = proxIn;
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(freqIn, proxIn);
+      }
+    }
+  }
+
+  @Override
+  public void init(IndexInput termsIn) throws IOException {
+
+    // Make sure we are talking to the matching past writer
+    CodecUtil.checkHeader(termsIn, TERMS_CODEC, VERSION_START, VERSION_CURRENT);
+
+    skipInterval = termsIn.readInt();
+    maxSkipLevels = termsIn.readInt();
+    skipMinimum = termsIn.readInt();
+  }
+
+  // Must keep final because we do non-standard clone
+  private final static class StandardTermState extends BlockTermState {
+    long freqOffset;
+    long proxOffset;
+    long skipOffset;
+
+    @Override
+    public StandardTermState clone() {
+      StandardTermState other = new StandardTermState();
+      other.copyFrom(this);
+      return other;
+    }
+
+    @Override
+    public void copyFrom(TermState _other) {
+      super.copyFrom(_other);
+      StandardTermState other = (StandardTermState) _other;
+      freqOffset = other.freqOffset;
+      proxOffset = other.proxOffset;
+      skipOffset = other.skipOffset;
+    }
+
+    @Override
+    public String toString() {
+      return super.toString() + " freqFP=" + freqOffset + " proxFP=" + proxOffset + " skipOffset=" + skipOffset;
+    }
+  }
+
+  @Override
+  public BlockTermState newTermState() {
+    return new StandardTermState();
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      if (freqIn != null) {
+        freqIn.close();
+      }
+    } finally {
+      if (proxIn != null) {
+        proxIn.close();
+      }
+    }
+  }
+
+  @Override
+  public void decodeTerm(long[] longs, DataInput in, FieldInfo fieldInfo, BlockTermState _termState, boolean absolute)
+    throws IOException {
+    final StandardTermState termState = (StandardTermState) _termState;
+    // if (DEBUG) System.out.println("SPR: nextTerm seg=" + segment + " tbOrd=" + termState.termBlockOrd + " bytesReader.fp=" + termState.bytesReader.getPosition());
+    final boolean isFirstTerm = termState.termBlockOrd == 0;
+    if (absolute) {
+      termState.freqOffset = 0;
+      termState.proxOffset = 0;
+    }
+
+    termState.freqOffset += in.readVLong();
+    /*
+    if (DEBUG) {
+      System.out.println("  dF=" + termState.docFreq);
+      System.out.println("  freqFP=" + termState.freqOffset);
+    }
+    */
+    assert termState.freqOffset < freqIn.length();
+
+    if (termState.docFreq >= skipMinimum) {
+      termState.skipOffset = in.readVLong();
+      // if (DEBUG) System.out.println("  skipOffset=" + termState.skipOffset + " vs freqIn.length=" + freqIn.length());
+      assert termState.freqOffset + termState.skipOffset < freqIn.length();
+    } else {
+      // undefined
+    }
+
+    if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
+      termState.proxOffset += in.readVLong();
+      // if (DEBUG) System.out.println("  proxFP=" + termState.proxOffset);
+    }
+  }
+    
+  @Override
+  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+    if (canReuse(reuse, liveDocs)) {
+      // if (DEBUG) System.out.println("SPR.docs ts=" + termState);
+      return ((SegmentDocsEnumBase) reuse).reset(fieldInfo, (StandardTermState)termState);
+    }
+    return newDocsEnum(liveDocs, fieldInfo, (StandardTermState)termState);
+  }
+  
+  private boolean canReuse(DocsEnum reuse, Bits liveDocs) {
+    if (reuse != null && (reuse instanceof SegmentDocsEnumBase)) {
+      SegmentDocsEnumBase docsEnum = (SegmentDocsEnumBase) reuse;
+      // If you are using ParellelReader, and pass in a
+      // reused DocsEnum, it could have come from another
+      // reader also using standard codec
+      if (docsEnum.startFreqIn == freqIn) {
+        // we only reuse if the the actual the incoming enum has the same liveDocs as the given liveDocs
+        return liveDocs == docsEnum.liveDocs;
+      }
+    }
+    return false;
+  }
+  
+  private DocsEnum newDocsEnum(Bits liveDocs, FieldInfo fieldInfo, StandardTermState termState) throws IOException {
+    if (liveDocs == null) {
+      return new AllDocsSegmentDocsEnum(freqIn).reset(fieldInfo, termState);
+    } else {
+      return new LiveDocsSegmentDocsEnum(freqIn, liveDocs).reset(fieldInfo, termState);
+    }
+  }
+
+  @Override
+  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs,
+                                               DocsAndPositionsEnum reuse, int flags)
+    throws IOException {
+
+    boolean hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+
+    // TODO: can we optimize if FLAG_PAYLOADS / FLAG_OFFSETS
+    // isn't passed?
+
+    // TODO: refactor
+    if (fieldInfo.hasPayloads() || hasOffsets) {
+      SegmentFullPositionsEnum docsEnum;
+      if (reuse == null || !(reuse instanceof SegmentFullPositionsEnum)) {
+        docsEnum = new SegmentFullPositionsEnum(freqIn, proxIn);
+      } else {
+        docsEnum = (SegmentFullPositionsEnum) reuse;
+        if (docsEnum.startFreqIn != freqIn) {
+          // If you are using ParellelReader, and pass in a
+          // reused DocsEnum, it could have come from another
+          // reader also using standard codec
+          docsEnum = new SegmentFullPositionsEnum(freqIn, proxIn);
+        }
+      }
+      return docsEnum.reset(fieldInfo, (StandardTermState) termState, liveDocs);
+    } else {
+      SegmentDocsAndPositionsEnum docsEnum;
+      if (reuse == null || !(reuse instanceof SegmentDocsAndPositionsEnum)) {
+        docsEnum = new SegmentDocsAndPositionsEnum(freqIn, proxIn);
+      } else {
+        docsEnum = (SegmentDocsAndPositionsEnum) reuse;
+        if (docsEnum.startFreqIn != freqIn) {
+          // If you are using ParellelReader, and pass in a
+          // reused DocsEnum, it could have come from another
+          // reader also using standard codec
+          docsEnum = new SegmentDocsAndPositionsEnum(freqIn, proxIn);
+        }
+      }
+      return docsEnum.reset(fieldInfo, (StandardTermState) termState, liveDocs);
+    }
+  }
+
+  static final int BUFFERSIZE = 64;
+  
+  private abstract class SegmentDocsEnumBase extends DocsEnum {
+    
+    protected final int[] docs = new int[BUFFERSIZE];
+    protected final int[] freqs = new int[BUFFERSIZE];
+    
+    final IndexInput freqIn; // reuse
+    final IndexInput startFreqIn; // reuse
+    Lucene40SkipListReader skipper; // reuse - lazy loaded
+    
+    protected boolean indexOmitsTF;                               // does current field omit term freq?
+    protected boolean storePayloads;                        // does current field store payloads?
+    protected boolean storeOffsets;                         // does current field store offsets?
+
+    protected int limit;                                    // number of docs in this posting
+    protected int ord;                                      // how many docs we've read
+    protected int doc;                                 // doc we last read
+    protected int accum;                                    // accumulator for doc deltas
+    protected int freq;                                     // freq we last read
+    protected int maxBufferedDocId;
+    
+    protected int start;
+    protected int count;
+
+
+    protected long freqOffset;
+    protected long skipOffset;
+
+    protected boolean skipped;
+    protected final Bits liveDocs;
+    
+    SegmentDocsEnumBase(IndexInput startFreqIn, Bits liveDocs) {
+      this.startFreqIn = startFreqIn;
+      this.freqIn = startFreqIn.clone();
+      this.liveDocs = liveDocs;
+      
+    }
+    
+    
+    DocsEnum reset(FieldInfo fieldInfo, StandardTermState termState) throws IOException {
+      indexOmitsTF = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY;
+      storePayloads = fieldInfo.hasPayloads();
+      storeOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      freqOffset = termState.freqOffset;
+      skipOffset = termState.skipOffset;
+
+      // TODO: for full enum case (eg segment merging) this
+      // seek is unnecessary; maybe we can avoid in such
+      // cases
+      freqIn.seek(termState.freqOffset);
+      limit = termState.docFreq;
+      assert limit > 0;
+      ord = 0;
+      doc = -1;
+      accum = 0;
+      // if (DEBUG) System.out.println("  sde limit=" + limit + " freqFP=" + freqOffset);
+      skipped = false;
+
+      start = -1;
+      count = 0;
+      freq = 1;
+      if (indexOmitsTF) {
+        Arrays.fill(freqs, 1);
+      }
+      maxBufferedDocId = -1;
+      return this;
+    }
+    
+    @Override
+    public final int freq() {
+      return freq;
+    }
+
+    @Override
+    public final int docID() {
+      return doc;
+    }
+    
+    @Override
+    public final int advance(int target) throws IOException {
+      // last doc in our buffer is >= target, binary search + next()
+      if (++start < count && maxBufferedDocId >= target) {
+        if ((count-start) > 32) { // 32 seemed to be a sweetspot here so use binsearch if the pending results are a lot
+          start = binarySearch(count - 1, start, target, docs);
+          return nextDoc();
+        } else {
+          return linearScan(target);
+        }
+      }
+      
+      start = count; // buffer is consumed
+      
+      return doc = skipTo(target);
+    }
+    
+    private final int binarySearch(int hi, int low, int target, int[] docs) {
+      while (low <= hi) {
+        int mid = (hi + low) >>> 1;
+        int doc = docs[mid];
+        if (doc < target) {
+          low = mid + 1;
+        } else if (doc > target) {
+          hi = mid - 1;
+        } else {
+          low = mid;
+          break;
+        }
+      }
+      return low-1;
+    }
+    
+    final int readFreq(final IndexInput freqIn, final int code)
+        throws IOException {
+      if ((code & 1) != 0) { // if low bit is set
+        return 1; // freq is one
+      } else {
+        return freqIn.readVInt(); // else read freq
+      }
+    }
+    
+    protected abstract int linearScan(int scanTo) throws IOException;
+    
+    protected abstract int scanTo(int target) throws IOException;
+
+    protected final int refill() throws IOException {
+      final int doc = nextUnreadDoc();
+      count = 0;
+      start = -1;
+      if (doc == NO_MORE_DOCS) {
+        return NO_MORE_DOCS;
+      }
+      final int numDocs = Math.min(docs.length, limit - ord);
+      ord += numDocs;
+      if (indexOmitsTF) {
+        count = fillDocs(numDocs);
+      } else {
+        count = fillDocsAndFreqs(numDocs);
+      }
+      maxBufferedDocId = count > 0 ? docs[count-1] : NO_MORE_DOCS;
+      return doc;
+    }
+    
+
+    protected abstract int nextUnreadDoc() throws IOException;
+
+
+    private final int fillDocs(int size) throws IOException {
+      final IndexInput freqIn = this.freqIn;
+      final int docs[] = this.docs;
+      int docAc = accum;
+      for (int i = 0; i < size; i++) {
+        docAc += freqIn.readVInt();
+        docs[i] = docAc;
+      }
+      accum = docAc;
+      return size;
+    }
+    
+    private final int fillDocsAndFreqs(int size) throws IOException {
+      final IndexInput freqIn = this.freqIn;
+      final int docs[] = this.docs;
+      final int freqs[] = this.freqs;
+      int docAc = accum;
+      for (int i = 0; i < size; i++) {
+        final int code = freqIn.readVInt();
+        docAc += code >>> 1; // shift off low bit
+        freqs[i] = readFreq(freqIn, code);
+        docs[i] = docAc;
+      }
+      accum = docAc;
+      return size;
+     
+    }
+
+    private final int skipTo(int target) throws IOException {
+      if ((target - skipInterval) >= accum && limit >= skipMinimum) {
+
+        // There are enough docs in the posting to have
+        // skip data, and it isn't too close.
+
+        if (skipper == null) {
+          // This is the first time this enum has ever been used for skipping -- do lazy init
+          skipper = new Lucene40SkipListReader(freqIn.clone(), maxSkipLevels, skipInterval);
+        }
+
+        if (!skipped) {
+
+          // This is the first time this posting has
+          // skipped since reset() was called, so now we
+          // load the skip data for this posting
+
+          skipper.init(freqOffset + skipOffset,
+                       freqOffset, 0,
+                       limit, storePayloads, storeOffsets);
+
+          skipped = true;
+        }
+
+        final int newOrd = skipper.skipTo(target); 
+
+        if (newOrd > ord) {
+          // Skipper moved
+
+          ord = newOrd;
+          accum = skipper.getDoc();
+          freqIn.seek(skipper.getFreqPointer());
+        }
+      }
+      return scanTo(target);
+    }
+    
+    @Override
+    public long cost() {
+      return limit;
+    }
+  }
+  
+  private final class AllDocsSegmentDocsEnum extends SegmentDocsEnumBase {
+
+    AllDocsSegmentDocsEnum(IndexInput startFreqIn) {
+      super(startFreqIn, null);
+      assert liveDocs == null;
+    }
+    
+    @Override
+    public final int nextDoc() throws IOException {
+      if (++start < count) {
+        freq = freqs[start];
+        return doc = docs[start];
+      }
+      return doc = refill();
+    }
+    
+
+    @Override
+    protected final int linearScan(int scanTo) throws IOException {
+      final int[] docs = this.docs;
+      final int upTo = count;
+      for (int i = start; i < upTo; i++) {
+        final int d = docs[i];
+        if (scanTo <= d) {
+          start = i;
+          freq = freqs[i];
+          return doc = docs[i];
+        }
+      }
+      return doc = refill();
+    }
+
+    @Override
+    protected int scanTo(int target) throws IOException { 
+      int docAcc = accum;
+      int frq = 1;
+      final IndexInput freqIn = this.freqIn;
+      final boolean omitTF = indexOmitsTF;
+      final int loopLimit = limit;
+      for (int i = ord; i < loopLimit; i++) {
+        int code = freqIn.readVInt();
+        if (omitTF) {
+          docAcc += code;
+        } else {
+          docAcc += code >>> 1; // shift off low bit
+          frq = readFreq(freqIn, code);
+        }
+        if (docAcc >= target) {
+          freq = frq;
+          ord = i + 1;
+          return accum = docAcc;
+        }
+      }
+      ord = limit;
+      freq = frq;
+      accum = docAcc;
+      return NO_MORE_DOCS;
+    }
+
+    @Override
+    protected final int nextUnreadDoc() throws IOException {
+      if (ord++ < limit) {
+        int code = freqIn.readVInt();
+        if (indexOmitsTF) {
+          accum += code;
+        } else {
+          accum += code >>> 1; // shift off low bit
+          freq = readFreq(freqIn, code);
+        }
+        return accum;
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+    
+  }
+  
+  private final class LiveDocsSegmentDocsEnum extends SegmentDocsEnumBase {
+
+    LiveDocsSegmentDocsEnum(IndexInput startFreqIn, Bits liveDocs) {
+      super(startFreqIn, liveDocs);
+      assert liveDocs != null;
+    }
+    
+    @Override
+    public final int nextDoc() throws IOException {
+      final Bits liveDocs = this.liveDocs;
+      for (int i = start+1; i < count; i++) {
+        int d = docs[i];
+        if (liveDocs.get(d)) {
+          start = i;
+          freq = freqs[i];
+          return doc = d;
+        }
+      }
+      start = count;
+      return doc = refill();
+    }
+
+    @Override
+    protected final int linearScan(int scanTo) throws IOException {
+      final int[] docs = this.docs;
+      final int upTo = count;
+      final Bits liveDocs = this.liveDocs;
+      for (int i = start; i < upTo; i++) {
+        int d = docs[i];
+        if (scanTo <= d && liveDocs.get(d)) {
+          start = i;
+          freq = freqs[i];
+          return doc = docs[i];
+        }
+      }
+      return doc = refill();
+    }
+    
+    @Override
+    protected int scanTo(int target) throws IOException { 
+      int docAcc = accum;
+      int frq = 1;
+      final IndexInput freqIn = this.freqIn;
+      final boolean omitTF = indexOmitsTF;
+      final int loopLimit = limit;
+      final Bits liveDocs = this.liveDocs;
+      for (int i = ord; i < loopLimit; i++) {
+        int code = freqIn.readVInt();
+        if (omitTF) {
+          docAcc += code;
+        } else {
+          docAcc += code >>> 1; // shift off low bit
+          frq = readFreq(freqIn, code);
+        }
+        if (docAcc >= target && liveDocs.get(docAcc)) {
+          freq = frq;
+          ord = i + 1;
+          return accum = docAcc;
+        }
+      }
+      ord = limit;
+      freq = frq;
+      accum = docAcc;
+      return NO_MORE_DOCS;
+    }
+
+    @Override
+    protected final int nextUnreadDoc() throws IOException {
+      int docAcc = accum;
+      int frq = 1;
+      final IndexInput freqIn = this.freqIn;
+      final boolean omitTF = indexOmitsTF;
+      final int loopLimit = limit;
+      final Bits liveDocs = this.liveDocs;
+      for (int i = ord; i < loopLimit; i++) {
+        int code = freqIn.readVInt();
+        if (omitTF) {
+          docAcc += code;
+        } else {
+          docAcc += code >>> 1; // shift off low bit
+          frq = readFreq(freqIn, code);
+        }
+        if (liveDocs.get(docAcc)) {
+          freq = frq;
+          ord = i + 1;
+          return accum = docAcc;
+        }
+      }
+      ord = limit;
+      freq = frq;
+      accum = docAcc;
+      return NO_MORE_DOCS;
+      
+    }
+  }
+  
+  // TODO specialize DocsAndPosEnum too
+  
+  // Decodes docs & positions. payloads nor offsets are present.
+  private final class SegmentDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    final IndexInput startFreqIn;
+    private final IndexInput freqIn;
+    private final IndexInput proxIn;
+    int limit;                                    // number of docs in this posting
+    int ord;                                      // how many docs we've read
+    int doc = -1;                                 // doc we last read
+    int accum;                                    // accumulator for doc deltas
+    int freq;                                     // freq we last read
+    int position;
+
+    Bits liveDocs;
+
+    long freqOffset;
+    long skipOffset;
+    long proxOffset;
+
+    int posPendingCount;
+
+    boolean skipped;
+    Lucene40SkipListReader skipper;
+    private long lazyProxPointer;
+
+    public SegmentDocsAndPositionsEnum(IndexInput freqIn, IndexInput proxIn) {
+      startFreqIn = freqIn;
+      this.freqIn = freqIn.clone();
+      this.proxIn = proxIn.clone();
+    }
+
+    public SegmentDocsAndPositionsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
+      assert fieldInfo.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+      assert !fieldInfo.hasPayloads();
+
+      this.liveDocs = liveDocs;
+
+      // TODO: for full enum case (eg segment merging) this
+      // seek is unnecessary; maybe we can avoid in such
+      // cases
+      freqIn.seek(termState.freqOffset);
+      lazyProxPointer = termState.proxOffset;
+
+      limit = termState.docFreq;
+      assert limit > 0;
+
+      ord = 0;
+      doc = -1;
+      accum = 0;
+      position = 0;
+
+      skipped = false;
+      posPendingCount = 0;
+
+      freqOffset = termState.freqOffset;
+      proxOffset = termState.proxOffset;
+      skipOffset = termState.skipOffset;
+      // if (DEBUG) System.out.println("StandardR.D&PE reset seg=" + segment + " limit=" + limit + " freqFP=" + freqOffset + " proxFP=" + proxOffset);
+
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      // if (DEBUG) System.out.println("SPR.nextDoc seg=" + segment + " freqIn.fp=" + freqIn.getFilePointer());
+      while(true) {
+        if (ord == limit) {
+          // if (DEBUG) System.out.println("  return END");
+          return doc = NO_MORE_DOCS;
+        }
+
+        ord++;
+
+        // Decode next doc/freq pair
+        final int code = freqIn.readVInt();
+
+        accum += code >>> 1;              // shift off low bit
+        if ((code & 1) != 0) {          // if low bit is set
+          freq = 1;                     // freq is one
+        } else {
+          freq = freqIn.readVInt();     // else read freq
+        }
+        posPendingCount += freq;
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          break;
+        }
+      }
+
+      position = 0;
+
+      // if (DEBUG) System.out.println("  return doc=" + doc);
+      return (doc = accum);
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+
+      //System.out.println("StandardR.D&PE advance target=" + target);
+
+      if ((target - skipInterval) >= doc && limit >= skipMinimum) {
+
+        // There are enough docs in the posting to have
+        // skip data, and it isn't too close
+
+        if (skipper == null) {
+          // This is the first time this enum has ever been used for skipping -- do lazy init
+          skipper = new Lucene40SkipListReader(freqIn.clone(), maxSkipLevels, skipInterval);
+        }
+
+        if (!skipped) {
+
+          // This is the first time this posting has
+          // skipped, since reset() was called, so now we
+          // load the skip data for this posting
+
+          skipper.init(freqOffset+skipOffset,
+                       freqOffset, proxOffset,
+                       limit, false, false);
+
+          skipped = true;
+        }
+
+        final int newOrd = skipper.skipTo(target); 
+
+        if (newOrd > ord) {
+          // Skipper moved
+          ord = newOrd;
+          doc = accum = skipper.getDoc();
+          freqIn.seek(skipper.getFreqPointer());
+          lazyProxPointer = skipper.getProxPointer();
+          posPendingCount = 0;
+          position = 0;
+        }
+      }
+        
+      // Now, linear scan for the rest:
+      do {
+        nextDoc();
+      } while (target > doc);
+
+      return doc;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+
+      if (lazyProxPointer != -1) {
+        proxIn.seek(lazyProxPointer);
+        lazyProxPointer = -1;
+      }
+
+      // scan over any docs that were iterated without their positions
+      if (posPendingCount > freq) {
+        position = 0;
+        while(posPendingCount != freq) {
+          if ((proxIn.readByte() & 0x80) == 0) {
+            posPendingCount--;
+          }
+        }
+      }
+
+      position += proxIn.readVInt();
+
+      posPendingCount--;
+
+      assert posPendingCount >= 0: "nextPosition() was called too many times (more than freq() times) posPendingCount=" + posPendingCount;
+
+      return position;
+    }
+
+    @Override
+    public int startOffset() {
+      return -1;
+    }
+
+    @Override
+    public int endOffset() {
+      return -1;
+    }
+
+    /** Returns the payload at this position, or null if no
+     *  payload was indexed. */
+    @Override
+    public BytesRef getPayload() throws IOException {
+      return null;
+    }
+    
+    @Override
+    public long cost() {
+      return limit;
+    }
+  }
+  
+  // Decodes docs & positions & (payloads and/or offsets)
+  private class SegmentFullPositionsEnum extends DocsAndPositionsEnum {
+    final IndexInput startFreqIn;
+    private final IndexInput freqIn;
+    private final IndexInput proxIn;
+
+    int limit;                                    // number of docs in this posting
+    int ord;                                      // how many docs we've read
+    int doc = -1;                                 // doc we last read
+    int accum;                                    // accumulator for doc deltas
+    int freq;                                     // freq we last read
+    int position;
+
+    Bits liveDocs;
+
+    long freqOffset;
+    long skipOffset;
+    long proxOffset;
+
+    int posPendingCount;
+    int payloadLength;
+    boolean payloadPending;
+
+    boolean skipped;
+    Lucene40SkipListReader skipper;
+    private BytesRefBuilder payload;
+    private long lazyProxPointer;
+    
+    boolean storePayloads;
+    boolean storeOffsets;
+    
+    int offsetLength;
+    int startOffset;
+
+    public SegmentFullPositionsEnum(IndexInput freqIn, IndexInput proxIn) {
+      startFreqIn = freqIn;
+      this.freqIn = freqIn.clone();
+      this.proxIn = proxIn.clone();
+    }
+
+    public SegmentFullPositionsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
+      storeOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      storePayloads = fieldInfo.hasPayloads();
+      assert fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+      assert storePayloads || storeOffsets;
+      if (payload == null) {
+        payload = new BytesRefBuilder();
+      }
+
+      this.liveDocs = liveDocs;
+
+      // TODO: for full enum case (eg segment merging) this
+      // seek is unnecessary; maybe we can avoid in such
+      // cases
+      freqIn.seek(termState.freqOffset);
+      lazyProxPointer = termState.proxOffset;
+
+      limit = termState.docFreq;
+      ord = 0;
+      doc = -1;
+      accum = 0;
+      position = 0;
+      startOffset = 0;
+
+      skipped = false;
+      posPendingCount = 0;
+      payloadPending = false;
+
+      freqOffset = termState.freqOffset;
+      proxOffset = termState.proxOffset;
+      skipOffset = termState.skipOffset;
+      //System.out.println("StandardR.D&PE reset seg=" + segment + " limit=" + limit + " freqFP=" + freqOffset + " proxFP=" + proxOffset + " this=" + this);
+
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      while(true) {
+        if (ord == limit) {
+          //System.out.println("StandardR.D&PE seg=" + segment + " nextDoc return doc=END");
+          return doc = NO_MORE_DOCS;
+        }
+
+        ord++;
+
+        // Decode next doc/freq pair
+        final int code = freqIn.readVInt();
+
+        accum += code >>> 1; // shift off low bit
+        if ((code & 1) != 0) { // if low bit is set
+          freq = 1; // freq is one
+        } else {
+          freq = freqIn.readVInt(); // else read freq
+        }
+        posPendingCount += freq;
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          break;
+        }
+      }
+
+      position = 0;
+      startOffset = 0;
+
+      //System.out.println("StandardR.D&PE nextDoc seg=" + segment + " return doc=" + doc);
+      return (doc = accum);
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int freq() throws IOException {
+      return freq;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+
+      //System.out.println("StandardR.D&PE advance seg=" + segment + " target=" + target + " this=" + this);
+
+      if ((target - skipInterval) >= doc && limit >= skipMinimum) {
+
+        // There are enough docs in the posting to have
+        // skip data, and it isn't too close
+
+        if (skipper == null) {
+          // This is the first time this enum has ever been used for skipping -- do lazy init
+          skipper = new Lucene40SkipListReader(freqIn.clone(), maxSkipLevels, skipInterval);
+        }
+
+        if (!skipped) {
+
+          // This is the first time this posting has
+          // skipped, since reset() was called, so now we
+          // load the skip data for this posting
+          //System.out.println("  init skipper freqOffset=" + freqOffset + " skipOffset=" + skipOffset + " vs len=" + freqIn.length());
+          skipper.init(freqOffset+skipOffset,
+                       freqOffset, proxOffset,
+                       limit, storePayloads, storeOffsets);
+
+          skipped = true;
+        }
+
+        final int newOrd = skipper.skipTo(target); 
+
+        if (newOrd > ord) {
+          // Skipper moved
+          ord = newOrd;
+          doc = accum = skipper.getDoc();
+          freqIn.seek(skipper.getFreqPointer());
+          lazyProxPointer = skipper.getProxPointer();
+          posPendingCount = 0;
+          position = 0;
+          startOffset = 0;
+          payloadPending = false;
+          payloadLength = skipper.getPayloadLength();
+          offsetLength = skipper.getOffsetLength();
+        }
+      }
+        
+      // Now, linear scan for the rest:
+      do {
+        nextDoc();
+      } while (target > doc);
+
+      return doc;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+
+      if (lazyProxPointer != -1) {
+        proxIn.seek(lazyProxPointer);
+        lazyProxPointer = -1;
+      }
+      
+      if (payloadPending && payloadLength > 0) {
+        // payload of last position was never retrieved -- skip it
+        proxIn.seek(proxIn.getFilePointer() + payloadLength);
+        payloadPending = false;
+      }
+
+      // scan over any docs that were iterated without their positions
+      while(posPendingCount > freq) {
+        final int code = proxIn.readVInt();
+
+        if (storePayloads) {
+          if ((code & 1) != 0) {
+            // new payload length
+            payloadLength = proxIn.readVInt();
+            assert payloadLength >= 0;
+          }
+          assert payloadLength != -1;
+        }
+        
+        if (storeOffsets) {
+          if ((proxIn.readVInt() & 1) != 0) {
+            // new offset length
+            offsetLength = proxIn.readVInt();
+          }
+        }
+        
+        if (storePayloads) {
+          proxIn.seek(proxIn.getFilePointer() + payloadLength);
+        }
+
+        posPendingCount--;
+        position = 0;
+        startOffset = 0;
+        payloadPending = false;
+        //System.out.println("StandardR.D&PE skipPos");
+      }
+
+      // read next position
+      if (payloadPending && payloadLength > 0) {
+        // payload wasn't retrieved for last position
+        proxIn.seek(proxIn.getFilePointer()+payloadLength);
+      }
+
+      int code = proxIn.readVInt();
+      if (storePayloads) {
+        if ((code & 1) != 0) {
+          // new payload length
+          payloadLength = proxIn.readVInt();
+          assert payloadLength >= 0;
+        }
+        assert payloadLength != -1;
+          
+        payloadPending = true;
+        code >>>= 1;
+      }
+      position += code;
+      
+      if (storeOffsets) {
+        int offsetCode = proxIn.readVInt();
+        if ((offsetCode & 1) != 0) {
+          // new offset length
+          offsetLength = proxIn.readVInt();
+        }
+        startOffset += offsetCode >>> 1;
+      }
+
+      posPendingCount--;
+
+      assert posPendingCount >= 0: "nextPosition() was called too many times (more than freq() times) posPendingCount=" + posPendingCount;
+
+      //System.out.println("StandardR.D&PE nextPos   return pos=" + position);
+      return position;
+    }
+
+    @Override
+    public int startOffset() throws IOException {
+      return storeOffsets ? startOffset : -1;
+    }
+
+    @Override
+    public int endOffset() throws IOException {
+      return storeOffsets ? startOffset + offsetLength : -1;
+    }
+
+    /** Returns the payload at this position, or null if no
+     *  payload was indexed. */
+    @Override
+    public BytesRef getPayload() throws IOException {
+      if (storePayloads) {
+        if (payloadLength <= 0) {
+          return null;
+        }
+        assert lazyProxPointer == -1;
+        assert posPendingCount < freq;
+        
+        if (payloadPending) {
+          payload.grow(payloadLength);
+
+          proxIn.readBytes(payload.bytes(), 0, payloadLength);
+          payload.setLength(payloadLength);
+          payloadPending = false;
+        }
+
+        return payload.get();
+      } else {
+        return null;
+      }
+    }
+    
+    @Override
+    public long cost() {
+      return limit;
+    }
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return 0;
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {}
+
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java
new file mode 100644
index 0000000..a5cb465
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java
@@ -0,0 +1,98 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.SegmentInfoReader;
+import org.apache.lucene.codecs.SegmentInfoWriter;
+import org.apache.lucene.index.IndexWriter; // javadocs
+import org.apache.lucene.index.SegmentInfo; // javadocs
+import org.apache.lucene.index.SegmentInfos; // javadocs
+import org.apache.lucene.store.DataOutput; // javadocs
+
+/**
+ * Lucene 4.0 Segment info format.
+ * <p>
+ * Files:
+ * <ul>
+ *   <li><tt>.si</tt>: Header, SegVersion, SegSize, IsCompoundFile, Diagnostics, Attributes, Files
+ * </ul>
+ * </p>
+ * Data types:
+ * <p>
+ * <ul>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>SegSize --&gt; {@link DataOutput#writeInt Int32}</li>
+ *   <li>SegVersion --&gt; {@link DataOutput#writeString String}</li>
+ *   <li>Files --&gt; {@link DataOutput#writeStringSet Set&lt;String&gt;}</li>
+ *   <li>Diagnostics, Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
+ *   <li>IsCompoundFile --&gt; {@link DataOutput#writeByte Int8}</li>
+ * </ul>
+ * </p>
+ * Field Descriptions:
+ * <p>
+ * <ul>
+ *   <li>SegVersion is the code version that created the segment.</li>
+ *   <li>SegSize is the number of documents contained in the segment index.</li>
+ *   <li>IsCompoundFile records whether the segment is written as a compound file or
+ *       not. If this is -1, the segment is not a compound file. If it is 1, the segment
+ *       is a compound file.</li>
+ *   <li>Checksum contains the CRC32 checksum of all bytes in the segments_N file up
+ *       until the checksum. This is used to verify integrity of the file on opening the
+ *       index.</li>
+ *   <li>The Diagnostics Map is privately written by {@link IndexWriter}, as a debugging aid,
+ *       for each segment it creates. It includes metadata like the current Lucene
+ *       version, OS, Java version, why the segment was created (merge, flush,
+ *       addIndexes), etc.</li>
+ *   <li>Attributes: a key-value map of codec-private attributes.</li>
+ *   <li>Files is a list of files referred to by this segment.</li>
+ * </ul>
+ * </p>
+ * 
+ * @see SegmentInfos
+ * @lucene.experimental
+ * @deprecated Only for reading old 4.0-4.5 segments, and supporting IndexWriter.addIndexes
+ */
+@Deprecated
+public class Lucene40SegmentInfoFormat extends SegmentInfoFormat {
+  private final SegmentInfoReader reader = new Lucene40SegmentInfoReader();
+  private final SegmentInfoWriter writer = new Lucene40SegmentInfoWriter();
+
+  /** Sole constructor. */
+  public Lucene40SegmentInfoFormat() {
+  }
+  
+  @Override
+  public SegmentInfoReader getSegmentInfoReader() {
+    return reader;
+  }
+
+  // we must unfortunately support write, to allow addIndexes to write a new .si with rewritten filenames:
+  // see LUCENE-5377
+  @Override
+  public SegmentInfoWriter getSegmentInfoWriter() {
+    return writer;
+  }
+
+  /** File extension used to store {@link SegmentInfo}. */
+  public final static String SI_EXTENSION = "si";
+  static final String CODEC_NAME = "Lucene40SegmentInfo";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java
new file mode 100644
index 0000000..078b1ae
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java
@@ -0,0 +1,85 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.SegmentInfoReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.Version;
+
+/**
+ * Lucene 4.0 implementation of {@link SegmentInfoReader}.
+ * 
+ * @see Lucene40SegmentInfoFormat
+ * @lucene.experimental
+ * @deprecated Only for reading old 4.0-4.5 segments
+ */
+@Deprecated
+public class Lucene40SegmentInfoReader extends SegmentInfoReader {
+
+  /** Sole constructor. */
+  public Lucene40SegmentInfoReader() {
+  }
+
+  @Override
+  public SegmentInfo read(Directory dir, String segment, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segment, "", Lucene40SegmentInfoFormat.SI_EXTENSION);
+    final IndexInput input = dir.openInput(fileName, context);
+    boolean success = false;
+    try {
+      CodecUtil.checkHeader(input, Lucene40SegmentInfoFormat.CODEC_NAME,
+                                   Lucene40SegmentInfoFormat.VERSION_START,
+                                   Lucene40SegmentInfoFormat.VERSION_CURRENT);
+      final Version version = Version.parse(input.readString());
+      final int docCount = input.readInt();
+      if (docCount < 0) {
+        throw new CorruptIndexException("invalid docCount: " + docCount + " (resource=" + input + ")");
+      }
+      final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
+      final Map<String,String> diagnostics = input.readStringStringMap();
+      input.readStringStringMap(); // read deprecated attributes
+      final Set<String> files = input.readStringSet();
+      
+      CodecUtil.checkEOF(input);
+
+      final SegmentInfo si = new SegmentInfo(dir, version, segment, docCount, isCompoundFile, null, diagnostics);
+      si.setFiles(files);
+
+      success = true;
+
+      return si;
+
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(input);
+      } else {
+        input.close();
+      }
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java
new file mode 100644
index 0000000..61bb7ad
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java
@@ -0,0 +1,77 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.SegmentInfoWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 4.0 implementation of {@link SegmentInfoWriter}.
+ * 
+ * @see Lucene40SegmentInfoFormat
+ * @lucene.experimental
+ */
+@Deprecated
+public class Lucene40SegmentInfoWriter extends SegmentInfoWriter {
+
+  /** Sole constructor. */
+  public Lucene40SegmentInfoWriter() {
+  }
+
+  /** Save a single segment's info. */
+  @Override
+  public void write(Directory dir, SegmentInfo si, FieldInfos fis, IOContext ioContext) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(si.name, "", Lucene40SegmentInfoFormat.SI_EXTENSION);
+    si.addFile(fileName);
+
+    final IndexOutput output = dir.createOutput(fileName, ioContext);
+
+    boolean success = false;
+    try {
+      CodecUtil.writeHeader(output, Lucene40SegmentInfoFormat.CODEC_NAME, Lucene40SegmentInfoFormat.VERSION_CURRENT);
+      // Write the Lucene version that created this segment, since 3.1
+      output.writeString(si.getVersion().toString());
+      output.writeInt(si.getDocCount());
+
+      output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
+      output.writeStringStringMap(si.getDiagnostics());
+      output.writeStringStringMap(Collections.<String,String>emptyMap());
+      output.writeStringSet(si.files());
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(output);
+        // TODO: why must we do this? do we not get tracking dir wrapper?
+        IOUtils.deleteFilesIgnoringExceptions(si.dir, fileName);
+      } else {
+        output.close();
+      }
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java
new file mode 100644
index 0000000..1580a39
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java
@@ -0,0 +1,143 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.codecs.MultiLevelSkipListReader;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * Implements the skip list reader for the 4.0 posting list format
+ * that stores positions and payloads.
+ * 
+ * @see Lucene40PostingsFormat
+ * @deprecated Only for reading old 4.0 segments
+ */
+@Deprecated
+public class Lucene40SkipListReader extends MultiLevelSkipListReader {
+  private boolean currentFieldStoresPayloads;
+  private boolean currentFieldStoresOffsets;
+  private long freqPointer[];
+  private long proxPointer[];
+  private int payloadLength[];
+  private int offsetLength[];
+  
+  private long lastFreqPointer;
+  private long lastProxPointer;
+  private int lastPayloadLength;
+  private int lastOffsetLength;
+                           
+  /** Sole constructor. */
+  public Lucene40SkipListReader(IndexInput skipStream, int maxSkipLevels, int skipInterval) {
+    super(skipStream, maxSkipLevels, skipInterval);
+    freqPointer = new long[maxSkipLevels];
+    proxPointer = new long[maxSkipLevels];
+    payloadLength = new int[maxSkipLevels];
+    offsetLength = new int[maxSkipLevels];
+  }
+
+  /** Per-term initialization. */
+  public void init(long skipPointer, long freqBasePointer, long proxBasePointer, int df, boolean storesPayloads, boolean storesOffsets) {
+    super.init(skipPointer, df);
+    this.currentFieldStoresPayloads = storesPayloads;
+    this.currentFieldStoresOffsets = storesOffsets;
+    lastFreqPointer = freqBasePointer;
+    lastProxPointer = proxBasePointer;
+
+    Arrays.fill(freqPointer, freqBasePointer);
+    Arrays.fill(proxPointer, proxBasePointer);
+    Arrays.fill(payloadLength, 0);
+    Arrays.fill(offsetLength, 0);
+  }
+
+  /** Returns the freq pointer of the doc to which the last call of 
+   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
+  public long getFreqPointer() {
+    return lastFreqPointer;
+  }
+
+  /** Returns the prox pointer of the doc to which the last call of 
+   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
+  public long getProxPointer() {
+    return lastProxPointer;
+  }
+  
+  /** Returns the payload length of the payload stored just before 
+   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
+   * has skipped.  */
+  public int getPayloadLength() {
+    return lastPayloadLength;
+  }
+  
+  /** Returns the offset length (endOffset-startOffset) of the position stored just before 
+   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
+   * has skipped.  */
+  public int getOffsetLength() {
+    return lastOffsetLength;
+  }
+  
+  @Override
+  protected void seekChild(int level) throws IOException {
+    super.seekChild(level);
+    freqPointer[level] = lastFreqPointer;
+    proxPointer[level] = lastProxPointer;
+    payloadLength[level] = lastPayloadLength;
+    offsetLength[level] = lastOffsetLength;
+  }
+  
+  @Override
+  protected void setLastSkipData(int level) {
+    super.setLastSkipData(level);
+    lastFreqPointer = freqPointer[level];
+    lastProxPointer = proxPointer[level];
+    lastPayloadLength = payloadLength[level];
+    lastOffsetLength = offsetLength[level];
+  }
+
+
+  @Override
+  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
+    int delta;
+    if (currentFieldStoresPayloads || currentFieldStoresOffsets) {
+      // the current field stores payloads and/or offsets.
+      // if the doc delta is odd then we have
+      // to read the current payload/offset lengths
+      // because it differs from the lengths of the
+      // previous payload/offset
+      delta = skipStream.readVInt();
+      if ((delta & 1) != 0) {
+        if (currentFieldStoresPayloads) {
+          payloadLength[level] = skipStream.readVInt();
+        }
+        if (currentFieldStoresOffsets) {
+          offsetLength[level] = skipStream.readVInt();
+        }
+      }
+      delta >>>= 1;
+    } else {
+      delta = skipStream.readVInt();
+    }
+
+    freqPointer[level] += skipStream.readVInt();
+    proxPointer[level] += skipStream.readVInt();
+    
+    return delta;
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java
new file mode 100644
index 0000000..dcdf360
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java
@@ -0,0 +1,99 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.DataOutput; // javadocs
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/** 
+ * Lucene 4.0 Stored Fields Format.
+ * <p>Stored fields are represented by two files:</p>
+ * <ol>
+ * <li><a name="field_index" id="field_index"></a>
+ * <p>The field index, or <tt>.fdx</tt> file.</p>
+ * <p>This is used to find the location within the field data file of the fields
+ * of a particular document. Because it contains fixed-length data, this file may
+ * be easily randomly accessed. The position of document <i>n</i> 's field data is
+ * the {@link DataOutput#writeLong Uint64} at <i>n*8</i> in this file.</p>
+ * <p>This contains, for each document, a pointer to its field data, as
+ * follows:</p>
+ * <ul>
+ * <li>FieldIndex (.fdx) --&gt; &lt;Header&gt;, &lt;FieldValuesPosition&gt; <sup>SegSize</sup></li>
+ * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ * <li>FieldValuesPosition --&gt; {@link DataOutput#writeLong Uint64}</li>
+ * </ul>
+ * </li>
+ * <li>
+ * <p><a name="field_data" id="field_data"></a>The field data, or <tt>.fdt</tt> file.</p>
+ * <p>This contains the stored fields of each document, as follows:</p>
+ * <ul>
+ * <li>FieldData (.fdt) --&gt; &lt;Header&gt;, &lt;DocFieldData&gt; <sup>SegSize</sup></li>
+ * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ * <li>DocFieldData --&gt; FieldCount, &lt;FieldNum, Bits, Value&gt;
+ * <sup>FieldCount</sup></li>
+ * <li>FieldCount --&gt; {@link DataOutput#writeVInt VInt}</li>
+ * <li>FieldNum --&gt; {@link DataOutput#writeVInt VInt}</li>
+ * <li>Bits --&gt; {@link DataOutput#writeByte Byte}</li>
+ * <ul>
+ * <li>low order bit reserved.</li>
+ * <li>second bit is one for fields containing binary data</li>
+ * <li>third bit reserved.</li>
+ * <li>4th to 6th bit (mask: 0x7&lt;&lt;3) define the type of a numeric field:
+ * <ul>
+ * <li>all bits in mask are cleared if no numeric field at all</li>
+ * <li>1&lt;&lt;3: Value is Int</li>
+ * <li>2&lt;&lt;3: Value is Long</li>
+ * <li>3&lt;&lt;3: Value is Int as Float (as of {@link Float#intBitsToFloat(int)}</li>
+ * <li>4&lt;&lt;3: Value is Long as Double (as of {@link Double#longBitsToDouble(long)}</li>
+ * </ul>
+ * </li>
+ * </ul>
+ * <li>Value --&gt; String | BinaryValue | Int | Long (depending on Bits)</li>
+ * <li>BinaryValue --&gt; ValueSize, &lt;{@link DataOutput#writeByte Byte}&gt;^ValueSize</li>
+ * <li>ValueSize --&gt; {@link DataOutput#writeVInt VInt}</li>
+ * </li>
+ * </ul>
+ * </ol>
+ * @lucene.experimental */
+public class Lucene40StoredFieldsFormat extends StoredFieldsFormat {
+
+  /** Sole constructor. */
+  public Lucene40StoredFieldsFormat() {
+  }
+
+  @Override
+  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si,
+      FieldInfos fn, IOContext context) throws IOException {
+    return new Lucene40StoredFieldsReader(directory, si, fn, context);
+  }
+
+  @Override
+  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si,
+      IOContext context) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java
new file mode 100644
index 0000000..a2848dc
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java
@@ -0,0 +1,264 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.StoredFieldVisitor;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+import java.io.Closeable;
+import java.nio.charset.StandardCharsets;
+
+/**
+ * Class responsible for access to stored document fields.
+ * <p/>
+ * It uses &lt;segment&gt;.fdt and &lt;segment&gt;.fdx; files.
+ * 
+ * @see Lucene40StoredFieldsFormat
+ * @lucene.internal
+ */
+public final class Lucene40StoredFieldsReader extends StoredFieldsReader implements Cloneable, Closeable {
+
+  // NOTE: bit 0 is free here!  You can steal it!
+  static final int FIELD_IS_BINARY = 1 << 1;
+
+  // the old bit 1 << 2 was compressed, is now left out
+
+  private static final int _NUMERIC_BIT_SHIFT = 3;
+  static final int FIELD_IS_NUMERIC_MASK = 0x07 << _NUMERIC_BIT_SHIFT;
+
+  static final int FIELD_IS_NUMERIC_INT = 1 << _NUMERIC_BIT_SHIFT;
+  static final int FIELD_IS_NUMERIC_LONG = 2 << _NUMERIC_BIT_SHIFT;
+  static final int FIELD_IS_NUMERIC_FLOAT = 3 << _NUMERIC_BIT_SHIFT;
+  static final int FIELD_IS_NUMERIC_DOUBLE = 4 << _NUMERIC_BIT_SHIFT;
+
+  // the next possible bits are: 1 << 6; 1 << 7
+  // currently unused: static final int FIELD_IS_NUMERIC_SHORT = 5 << _NUMERIC_BIT_SHIFT;
+  // currently unused: static final int FIELD_IS_NUMERIC_BYTE = 6 << _NUMERIC_BIT_SHIFT;
+
+  static final String CODEC_NAME_IDX = "Lucene40StoredFieldsIndex";
+  static final String CODEC_NAME_DAT = "Lucene40StoredFieldsData";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+  static final long HEADER_LENGTH_IDX = CodecUtil.headerLength(CODEC_NAME_IDX);
+  static final long HEADER_LENGTH_DAT = CodecUtil.headerLength(CODEC_NAME_DAT);
+
+
+
+  /** Extension of stored fields file */
+  public static final String FIELDS_EXTENSION = "fdt";
+  
+  /** Extension of stored fields index file */
+  public static final String FIELDS_INDEX_EXTENSION = "fdx";
+  
+  private static final long RAM_BYTES_USED = RamUsageEstimator.shallowSizeOfInstance(Lucene40StoredFieldsReader.class);
+
+  private final FieldInfos fieldInfos;
+  private final IndexInput fieldsStream;
+  private final IndexInput indexStream;
+  private int numTotalDocs;
+  private int size;
+  private boolean closed;
+
+  /** Returns a cloned FieldsReader that shares open
+   *  IndexInputs with the original one.  It is the caller's
+   *  job not to close the original FieldsReader until all
+   *  clones are called (eg, currently SegmentReader manages
+   *  this logic). */
+  @Override
+  public Lucene40StoredFieldsReader clone() {
+    ensureOpen();
+    return new Lucene40StoredFieldsReader(fieldInfos, numTotalDocs, size, fieldsStream.clone(), indexStream.clone());
+  }
+  
+  /** Used only by clone. */
+  private Lucene40StoredFieldsReader(FieldInfos fieldInfos, int numTotalDocs, int size, IndexInput fieldsStream, IndexInput indexStream) {
+    this.fieldInfos = fieldInfos;
+    this.numTotalDocs = numTotalDocs;
+    this.size = size;
+    this.fieldsStream = fieldsStream;
+    this.indexStream = indexStream;
+  }
+
+  /** Sole constructor. */
+  public Lucene40StoredFieldsReader(Directory d, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
+    final String segment = si.name;
+    boolean success = false;
+    fieldInfos = fn;
+    try {
+      fieldsStream = d.openInput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
+      final String indexStreamFN = IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION);
+      indexStream = d.openInput(indexStreamFN, context);
+      
+      CodecUtil.checkHeader(indexStream, CODEC_NAME_IDX, VERSION_START, VERSION_CURRENT);
+      CodecUtil.checkHeader(fieldsStream, CODEC_NAME_DAT, VERSION_START, VERSION_CURRENT);
+      assert HEADER_LENGTH_DAT == fieldsStream.getFilePointer();
+      assert HEADER_LENGTH_IDX == indexStream.getFilePointer();
+      final long indexSize = indexStream.length() - HEADER_LENGTH_IDX;
+      this.size = (int) (indexSize >> 3);
+      // Verify two sources of "maxDoc" agree:
+      if (this.size != si.getDocCount()) {
+        throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + this.size + " but segmentInfo shows " + si.getDocCount());
+      }
+      numTotalDocs = (int) (indexSize >> 3);
+      success = true;
+    } finally {
+      // With lock-less commits, it's entirely possible (and
+      // fine) to hit a FileNotFound exception above. In
+      // this case, we want to explicitly close any subset
+      // of things that were opened so that we don't have to
+      // wait for a GC to do so.
+      if (!success) {
+        try {
+          close();
+        } catch (Throwable t) {} // ensure we throw our original exception
+      }
+    }
+  }
+
+  /**
+   * @throws AlreadyClosedException if this FieldsReader is closed
+   */
+  private void ensureOpen() throws AlreadyClosedException {
+    if (closed) {
+      throw new AlreadyClosedException("this FieldsReader is closed");
+    }
+  }
+
+  /**
+   * Closes the underlying {@link org.apache.lucene.store.IndexInput} streams.
+   * This means that the Fields values will not be accessible.
+   *
+   * @throws IOException If an I/O error occurs
+   */
+  @Override
+  public final void close() throws IOException {
+    if (!closed) {
+      IOUtils.close(fieldsStream, indexStream);
+      closed = true;
+    }
+  }
+
+  /** Returns number of documents. */
+  public final int size() {
+    return size;
+  }
+
+  private void seekIndex(int docID) throws IOException {
+    indexStream.seek(HEADER_LENGTH_IDX + docID * 8L);
+  }
+
+  @Override
+  public final void visitDocument(int n, StoredFieldVisitor visitor) throws IOException {
+    seekIndex(n);
+    fieldsStream.seek(indexStream.readLong());
+
+    final int numFields = fieldsStream.readVInt();
+    for (int fieldIDX = 0; fieldIDX < numFields; fieldIDX++) {
+      int fieldNumber = fieldsStream.readVInt();
+      FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
+      
+      int bits = fieldsStream.readByte() & 0xFF;
+      assert bits <= (FIELD_IS_NUMERIC_MASK | FIELD_IS_BINARY): "bits=" + Integer.toHexString(bits);
+
+      switch(visitor.needsField(fieldInfo)) {
+        case YES:
+          readField(visitor, fieldInfo, bits);
+          break;
+        case NO: 
+          skipField(bits);
+          break;
+        case STOP: 
+          return;
+      }
+    }
+  }
+
+  private void readField(StoredFieldVisitor visitor, FieldInfo info, int bits) throws IOException {
+    final int numeric = bits & FIELD_IS_NUMERIC_MASK;
+    if (numeric != 0) {
+      switch(numeric) {
+        case FIELD_IS_NUMERIC_INT:
+          visitor.intField(info, fieldsStream.readInt());
+          return;
+        case FIELD_IS_NUMERIC_LONG:
+          visitor.longField(info, fieldsStream.readLong());
+          return;
+        case FIELD_IS_NUMERIC_FLOAT:
+          visitor.floatField(info, Float.intBitsToFloat(fieldsStream.readInt()));
+          return;
+        case FIELD_IS_NUMERIC_DOUBLE:
+          visitor.doubleField(info, Double.longBitsToDouble(fieldsStream.readLong()));
+          return;
+        default:
+          throw new CorruptIndexException("Invalid numeric type: " + Integer.toHexString(numeric));
+      }
+    } else { 
+      final int length = fieldsStream.readVInt();
+      byte bytes[] = new byte[length];
+      fieldsStream.readBytes(bytes, 0, length);
+      if ((bits & FIELD_IS_BINARY) != 0) {
+        visitor.binaryField(info, bytes);
+      } else {
+        visitor.stringField(info, new String(bytes, 0, bytes.length, StandardCharsets.UTF_8));
+      }
+    }
+  }
+  
+  private void skipField(int bits) throws IOException {
+    final int numeric = bits & FIELD_IS_NUMERIC_MASK;
+    if (numeric != 0) {
+      switch(numeric) {
+        case FIELD_IS_NUMERIC_INT:
+        case FIELD_IS_NUMERIC_FLOAT:
+          fieldsStream.readInt();
+          return;
+        case FIELD_IS_NUMERIC_LONG:
+        case FIELD_IS_NUMERIC_DOUBLE:
+          fieldsStream.readLong();
+          return;
+        default: 
+          throw new CorruptIndexException("Invalid numeric type: " + Integer.toHexString(numeric));
+      }
+    } else {
+      final int length = fieldsStream.readVInt();
+      fieldsStream.seek(fieldsStream.getFilePointer() + length);
+    }
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return RAM_BYTES_USED;
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {}
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java
new file mode 100644
index 0000000..efea3bb
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java
@@ -0,0 +1,131 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.DataOutput; // javadocs
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Lucene 4.0 Term Vectors format.
+ * <p>Term Vector support is an optional on a field by field basis. It consists of
+ * 3 files.</p>
+ * <ol>
+ * <li><a name="tvx" id="tvx"></a>
+ * <p>The Document Index or .tvx file.</p>
+ * <p>For each document, this stores the offset into the document data (.tvd) and
+ * field data (.tvf) files.</p>
+ * <p>DocumentIndex (.tvx) --&gt; Header,&lt;DocumentPosition,FieldPosition&gt;
+ * <sup>NumDocs</sup></p>
+ * <ul>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>DocumentPosition --&gt; {@link DataOutput#writeLong UInt64} (offset in the .tvd file)</li>
+ *   <li>FieldPosition --&gt; {@link DataOutput#writeLong UInt64} (offset in the .tvf file)</li>
+ * </ul>
+ * </li>
+ * <li><a name="tvd" id="tvd"></a>
+ * <p>The Document or .tvd file.</p>
+ * <p>This contains, for each document, the number of fields, a list of the fields
+ * with term vector info and finally a list of pointers to the field information
+ * in the .tvf (Term Vector Fields) file.</p>
+ * <p>The .tvd file is used to map out the fields that have term vectors stored
+ * and where the field information is in the .tvf file.</p>
+ * <p>Document (.tvd) --&gt; Header,&lt;NumFields, FieldNums,
+ * FieldPositions&gt; <sup>NumDocs</sup></p>
+ * <ul>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>NumFields --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>FieldNums --&gt; &lt;FieldNumDelta&gt; <sup>NumFields</sup></li>
+ *   <li>FieldNumDelta --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>FieldPositions --&gt; &lt;FieldPositionDelta&gt; <sup>NumFields-1</sup></li>
+ *   <li>FieldPositionDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
+ * </ul>
+ * </li>
+ * <li><a name="tvf" id="tvf"></a>
+ * <p>The Field or .tvf file.</p>
+ * <p>This file contains, for each field that has a term vector stored, a list of
+ * the terms, their frequencies and, optionally, position, offset, and payload
+ * information.</p>
+ * <p>Field (.tvf) --&gt; Header,&lt;NumTerms, Flags, TermFreqs&gt;
+ * <sup>NumFields</sup></p>
+ * <ul>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>NumTerms --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>Flags --&gt; {@link DataOutput#writeByte Byte}</li>
+ *   <li>TermFreqs --&gt; &lt;TermText, TermFreq, Positions?, PayloadData?, Offsets?&gt;
+ *       <sup>NumTerms</sup></li>
+ *   <li>TermText --&gt; &lt;PrefixLength, Suffix&gt;</li>
+ *   <li>PrefixLength --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>Suffix --&gt; {@link DataOutput#writeString String}</li>
+ *   <li>TermFreq --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>Positions --&gt; &lt;PositionDelta PayloadLength?&gt;<sup>TermFreq</sup></li>
+ *   <li>PositionDelta --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>PayloadLength --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>PayloadData --&gt; {@link DataOutput#writeByte Byte}<sup>NumPayloadBytes</sup></li>
+ *   <li>Offsets --&gt; &lt;{@link DataOutput#writeVInt VInt}, {@link DataOutput#writeVInt VInt}&gt;<sup>TermFreq</sup></li>
+ * </ul>
+ * <p>Notes:</p>
+ * <ul>
+ * <li>Flags byte stores whether this term vector has position, offset, payload.
+ * information stored.</li>
+ * <li>Term byte prefixes are shared. The PrefixLength is the number of initial
+ * bytes from the previous term which must be pre-pended to a term's suffix
+ * in order to form the term's bytes. Thus, if the previous term's text was "bone"
+ * and the term is "boy", the PrefixLength is two and the suffix is "y".</li>
+ * <li>PositionDelta is, if payloads are disabled for the term's field, the
+ * difference between the position of the current occurrence in the document and
+ * the previous occurrence (or zero, if this is the first occurrence in this
+ * document). If payloads are enabled for the term's field, then PositionDelta/2
+ * is the difference between the current and the previous position. If payloads
+ * are enabled and PositionDelta is odd, then PayloadLength is stored, indicating
+ * the length of the payload at the current term position.</li>
+ * <li>PayloadData is metadata associated with a term position. If
+ * PayloadLength is stored at the current position, then it indicates the length
+ * of this payload. If PayloadLength is not stored, then this payload has the same
+ * length as the payload at the previous position. PayloadData encodes the 
+ * concatenated bytes for all of a terms occurrences.</li>
+ * <li>Offsets are stored as delta encoded VInts. The first VInt is the
+ * startOffset, the second is the endOffset.</li>
+ * </ul>
+ * </li>
+ * </ol>
+ */
+public class Lucene40TermVectorsFormat extends TermVectorsFormat {
+
+  /** Sole constructor. */
+  public Lucene40TermVectorsFormat() {
+  }
+  
+  @Override
+  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
+    return new Lucene40TermVectorsReader(directory, segmentInfo, fieldInfos, context);
+  }
+
+  @Override
+  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
new file mode 100644
index 0000000..857d653
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
@@ -0,0 +1,712 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.NoSuchElementException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 4.0 Term Vectors reader.
+ * <p>
+ * It reads .tvd, .tvf, and .tvx files.
+ * 
+ * @see Lucene40TermVectorsFormat
+ */
+public class Lucene40TermVectorsReader extends TermVectorsReader implements Closeable {
+
+  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1;
+
+  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2;
+  
+  static final byte STORE_PAYLOAD_WITH_TERMVECTOR = 0x4;
+  
+  /** Extension of vectors fields file */
+  static final String VECTORS_FIELDS_EXTENSION = "tvf";
+
+  /** Extension of vectors documents file */
+  static final String VECTORS_DOCUMENTS_EXTENSION = "tvd";
+
+  /** Extension of vectors index file */
+  static final String VECTORS_INDEX_EXTENSION = "tvx";
+  
+  static final String CODEC_NAME_FIELDS = "Lucene40TermVectorsFields";
+  static final String CODEC_NAME_DOCS = "Lucene40TermVectorsDocs";
+  static final String CODEC_NAME_INDEX = "Lucene40TermVectorsIndex";
+
+  static final int VERSION_NO_PAYLOADS = 0;
+  static final int VERSION_PAYLOADS = 1;
+  static final int VERSION_START = VERSION_NO_PAYLOADS;
+  static final int VERSION_CURRENT = VERSION_PAYLOADS;
+  
+  static final long HEADER_LENGTH_FIELDS = CodecUtil.headerLength(CODEC_NAME_FIELDS);
+  static final long HEADER_LENGTH_DOCS = CodecUtil.headerLength(CODEC_NAME_DOCS);
+  static final long HEADER_LENGTH_INDEX = CodecUtil.headerLength(CODEC_NAME_INDEX);
+
+  private FieldInfos fieldInfos;
+
+  private IndexInput tvx;
+  private IndexInput tvd;
+  private IndexInput tvf;
+  private int size;
+  private int numTotalDocs;
+  
+
+  /** Used by clone. */
+  Lucene40TermVectorsReader(FieldInfos fieldInfos, IndexInput tvx, IndexInput tvd, IndexInput tvf, int size, int numTotalDocs) {
+    this.fieldInfos = fieldInfos;
+    this.tvx = tvx;
+    this.tvd = tvd;
+    this.tvf = tvf;
+    this.size = size;
+    this.numTotalDocs = numTotalDocs;
+  }
+    
+  /** Sole constructor. */
+  public Lucene40TermVectorsReader(Directory d, SegmentInfo si, FieldInfos fieldInfos, IOContext context)
+    throws IOException {
+    final String segment = si.name;
+    final int size = si.getDocCount();
+    
+    boolean success = false;
+
+    try {
+      String idxName = IndexFileNames.segmentFileName(segment, "", VECTORS_INDEX_EXTENSION);
+      tvx = d.openInput(idxName, context);
+      final int tvxVersion = CodecUtil.checkHeader(tvx, CODEC_NAME_INDEX, VERSION_START, VERSION_CURRENT);
+      
+      String fn = IndexFileNames.segmentFileName(segment, "", VECTORS_DOCUMENTS_EXTENSION);
+      tvd = d.openInput(fn, context);
+      final int tvdVersion = CodecUtil.checkHeader(tvd, CODEC_NAME_DOCS, VERSION_START, VERSION_CURRENT);
+      fn = IndexFileNames.segmentFileName(segment, "", VECTORS_FIELDS_EXTENSION);
+      tvf = d.openInput(fn, context);
+      final int tvfVersion = CodecUtil.checkHeader(tvf, CODEC_NAME_FIELDS, VERSION_START, VERSION_CURRENT);
+      assert HEADER_LENGTH_INDEX == tvx.getFilePointer();
+      assert HEADER_LENGTH_DOCS == tvd.getFilePointer();
+      assert HEADER_LENGTH_FIELDS == tvf.getFilePointer();
+      assert tvxVersion == tvdVersion;
+      assert tvxVersion == tvfVersion;
+
+      numTotalDocs = (int) (tvx.length()-HEADER_LENGTH_INDEX >> 4);
+
+      this.size = numTotalDocs;
+      assert size == 0 || numTotalDocs == size;
+
+      this.fieldInfos = fieldInfos;
+      success = true;
+    } finally {
+      // With lock-less commits, it's entirely possible (and
+      // fine) to hit a FileNotFound exception above. In
+      // this case, we want to explicitly close any subset
+      // of things that were opened so that we don't have to
+      // wait for a GC to do so.
+      if (!success) {
+        try {
+          close();
+        } catch (Throwable t) {} // ensure we throw our original exception
+      }
+    }
+  }
+
+  // Not private to avoid synthetic access$NNN methods
+  void seekTvx(final int docNum) throws IOException {
+    tvx.seek(docNum * 16L + HEADER_LENGTH_INDEX);
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.close(tvx, tvd, tvf);
+  }
+
+  /**
+   * 
+   * @return The number of documents in the reader
+   */
+  int size() {
+    return size;
+  }
+
+  private class TVFields extends Fields {
+    private final int[] fieldNumbers;
+    private final long[] fieldFPs;
+    private final Map<Integer,Integer> fieldNumberToIndex = new HashMap<>();
+
+    public TVFields(int docID) throws IOException {
+      seekTvx(docID);
+      tvd.seek(tvx.readLong());
+      
+      final int fieldCount = tvd.readVInt();
+      assert fieldCount >= 0;
+      if (fieldCount != 0) {
+        fieldNumbers = new int[fieldCount];
+        fieldFPs = new long[fieldCount];
+        for(int fieldUpto=0;fieldUpto<fieldCount;fieldUpto++) {
+          final int fieldNumber = tvd.readVInt();
+          fieldNumbers[fieldUpto] = fieldNumber;
+          fieldNumberToIndex.put(fieldNumber, fieldUpto);
+        }
+
+        long position = tvx.readLong();
+        fieldFPs[0] = position;
+        for(int fieldUpto=1;fieldUpto<fieldCount;fieldUpto++) {
+          position += tvd.readVLong();
+          fieldFPs[fieldUpto] = position;
+        }
+      } else {
+        // TODO: we can improve writer here, eg write 0 into
+        // tvx file, so we know on first read from tvx that
+        // this doc has no TVs
+        fieldNumbers = null;
+        fieldFPs = null;
+      }
+    }
+    
+    @Override
+    public Iterator<String> iterator() {
+      return new Iterator<String>() {
+        private int fieldUpto;
+
+        @Override
+        public String next() {
+          if (fieldNumbers != null && fieldUpto < fieldNumbers.length) {
+            return fieldInfos.fieldInfo(fieldNumbers[fieldUpto++]).name;
+          } else {
+            throw new NoSuchElementException();
+          }
+        }
+
+        @Override
+        public boolean hasNext() {
+          return fieldNumbers != null && fieldUpto < fieldNumbers.length;
+        }
+
+        @Override
+        public void remove() {
+          throw new UnsupportedOperationException();
+        }
+      };
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+      if (fieldInfo == null) {
+        // No such field
+        return null;
+      }
+
+      final Integer fieldIndex = fieldNumberToIndex.get(fieldInfo.number);
+      if (fieldIndex == null) {
+        // Term vectors were not indexed for this field
+        return null;
+      }
+
+      return new TVTerms(fieldFPs[fieldIndex]);
+    }
+
+    @Override
+    public int size() {
+      if (fieldNumbers == null) {
+        return 0;
+      } else {
+        return fieldNumbers.length;
+      }
+    }
+  }
+
+  private class TVTerms extends Terms {
+    private final int numTerms;
+    private final long tvfFPStart;
+    private final boolean storePositions;
+    private final boolean storeOffsets;
+    private final boolean storePayloads;
+
+    public TVTerms(long tvfFP) throws IOException {
+      tvf.seek(tvfFP);
+      numTerms = tvf.readVInt();
+      final byte bits = tvf.readByte();
+      storePositions = (bits & STORE_POSITIONS_WITH_TERMVECTOR) != 0;
+      storeOffsets = (bits & STORE_OFFSET_WITH_TERMVECTOR) != 0;
+      storePayloads = (bits & STORE_PAYLOAD_WITH_TERMVECTOR) != 0;
+      tvfFPStart = tvf.getFilePointer();
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      TVTermsEnum termsEnum;
+      if (reuse instanceof TVTermsEnum) {
+        termsEnum = (TVTermsEnum) reuse;
+        if (!termsEnum.canReuse(tvf)) {
+          termsEnum = new TVTermsEnum();
+        }
+      } else {
+        termsEnum = new TVTermsEnum();
+      }
+      termsEnum.reset(numTerms, tvfFPStart, storePositions, storeOffsets, storePayloads);
+      return termsEnum;
+    }
+
+    @Override
+    public long size() {
+      return numTerms;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return -1;
+    }
+
+    @Override
+    public long getSumDocFreq() {
+      // Every term occurs in just one doc:
+      return numTerms;
+    }
+
+    @Override
+    public int getDocCount() {
+      return 1;
+    }
+
+    @Override
+    public boolean hasFreqs() {
+      return true;
+    }
+
+    @Override
+    public boolean hasOffsets() {
+      return storeOffsets;
+    }
+
+    @Override
+    public boolean hasPositions() {
+      return storePositions;
+    }
+    
+    @Override
+    public boolean hasPayloads() {
+      return storePayloads;
+    }
+  }
+
+  private class TVTermsEnum extends TermsEnum {
+    private final IndexInput origTVF;
+    private final IndexInput tvf;
+    private int numTerms;
+    private int nextTerm;
+    private int freq;
+    private BytesRefBuilder lastTerm = new BytesRefBuilder();
+    private BytesRefBuilder term = new BytesRefBuilder();
+    private boolean storePositions;
+    private boolean storeOffsets;
+    private boolean storePayloads;
+    private long tvfFP;
+
+    private int[] positions;
+    private int[] startOffsets;
+    private int[] endOffsets;
+    
+    // one shared byte[] for any term's payloads
+    private int[] payloadOffsets;
+    private int lastPayloadLength;
+    private byte[] payloadData;
+
+    // NOTE: tvf is pre-positioned by caller
+    public TVTermsEnum() {
+      this.origTVF = Lucene40TermVectorsReader.this.tvf;
+      tvf = origTVF.clone();
+    }
+
+    public boolean canReuse(IndexInput tvf) {
+      return tvf == origTVF;
+    }
+
+    public void reset(int numTerms, long tvfFPStart, boolean storePositions, boolean storeOffsets, boolean storePayloads) throws IOException {
+      this.numTerms = numTerms;
+      this.storePositions = storePositions;
+      this.storeOffsets = storeOffsets;
+      this.storePayloads = storePayloads;
+      nextTerm = 0;
+      tvf.seek(tvfFPStart);
+      tvfFP = tvfFPStart;
+      positions = null;
+      startOffsets = null;
+      endOffsets = null;
+      payloadOffsets = null;
+      payloadData = null;
+      lastPayloadLength = -1;
+    }
+
+    // NOTE: slow!  (linear scan)
+    @Override
+    public SeekStatus seekCeil(BytesRef text)
+      throws IOException {
+      if (nextTerm != 0) {
+        final int cmp = text.compareTo(term.get());
+        if (cmp < 0) {
+          nextTerm = 0;
+          tvf.seek(tvfFP);
+        } else if (cmp == 0) {
+          return SeekStatus.FOUND;
+        }
+      }
+
+      while (next() != null) {
+        final int cmp = text.compareTo(term.get());
+        if (cmp < 0) {
+          return SeekStatus.NOT_FOUND;
+        } else if (cmp == 0) {
+          return SeekStatus.FOUND;
+        }
+      }
+
+      return SeekStatus.END;
+    }
+
+    @Override
+    public void seekExact(long ord) {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      if (nextTerm >= numTerms) {
+        return null;
+      }
+      term.copyBytes(lastTerm.get());
+      final int start = tvf.readVInt();
+      final int deltaLen = tvf.readVInt();
+      term.setLength(start + deltaLen);
+      term.grow(term.length());
+      tvf.readBytes(term.bytes(), start, deltaLen);
+      freq = tvf.readVInt();
+
+      if (storePayloads) {
+        positions = new int[freq];
+        payloadOffsets = new int[freq];
+        int totalPayloadLength = 0;
+        int pos = 0;
+        for(int posUpto=0;posUpto<freq;posUpto++) {
+          int code = tvf.readVInt();
+          pos += code >>> 1;
+          positions[posUpto] = pos;
+          if ((code & 1) != 0) {
+            // length change
+            lastPayloadLength = tvf.readVInt();
+          }
+          payloadOffsets[posUpto] = totalPayloadLength;
+          totalPayloadLength += lastPayloadLength;
+          assert totalPayloadLength >= 0;
+        }
+        payloadData = new byte[totalPayloadLength];
+        tvf.readBytes(payloadData, 0, payloadData.length);
+      } else if (storePositions /* no payloads */) {
+        // TODO: we could maybe reuse last array, if we can
+        // somehow be careful about consumer never using two
+        // D&PEnums at once...
+        positions = new int[freq];
+        int pos = 0;
+        for(int posUpto=0;posUpto<freq;posUpto++) {
+          pos += tvf.readVInt();
+          positions[posUpto] = pos;
+        }
+      }
+
+      if (storeOffsets) {
+        startOffsets = new int[freq];
+        endOffsets = new int[freq];
+        int offset = 0;
+        for(int posUpto=0;posUpto<freq;posUpto++) {
+          startOffsets[posUpto] = offset + tvf.readVInt();
+          offset = endOffsets[posUpto] = startOffsets[posUpto] + tvf.readVInt();
+        }
+      }
+
+      lastTerm.copyBytes(term.get());
+      nextTerm++;
+      return term.get();
+    }
+
+    @Override
+    public BytesRef term() {
+      return term.get();
+    }
+
+    @Override
+    public long ord() {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public int docFreq() {
+      return 1;
+    }
+
+    @Override
+    public long totalTermFreq() {
+      return freq;
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags /* ignored */) throws IOException {
+      TVDocsEnum docsEnum;
+      if (reuse != null && reuse instanceof TVDocsEnum) {
+        docsEnum = (TVDocsEnum) reuse;
+      } else {
+        docsEnum = new TVDocsEnum();
+      }
+      docsEnum.reset(liveDocs, freq);
+      return docsEnum;
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+
+      if (!storePositions && !storeOffsets) {
+        return null;
+      }
+      
+      TVDocsAndPositionsEnum docsAndPositionsEnum;
+      if (reuse != null && reuse instanceof TVDocsAndPositionsEnum) {
+        docsAndPositionsEnum = (TVDocsAndPositionsEnum) reuse;
+      } else {
+        docsAndPositionsEnum = new TVDocsAndPositionsEnum();
+      }
+      docsAndPositionsEnum.reset(liveDocs, positions, startOffsets, endOffsets, payloadOffsets, payloadData);
+      return docsAndPositionsEnum;
+    }
+  }
+
+  // NOTE: sort of a silly class, since you can get the
+  // freq() already by TermsEnum.totalTermFreq
+  private static class TVDocsEnum extends DocsEnum {
+    private boolean didNext;
+    private int doc = -1;
+    private int freq;
+    private Bits liveDocs;
+
+    @Override
+    public int freq() throws IOException {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return (doc = 0);
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      return slowAdvance(target);
+    }
+
+    public void reset(Bits liveDocs, int freq) {
+      this.liveDocs = liveDocs;
+      this.freq = freq;
+      this.doc = -1;
+      didNext = false;
+    }
+    
+    @Override
+    public long cost() {
+      return 1;
+    }
+  }
+
+  private static class TVDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private boolean didNext;
+    private int doc = -1;
+    private int nextPos;
+    private Bits liveDocs;
+    private int[] positions;
+    private int[] startOffsets;
+    private int[] endOffsets;
+    private int[] payloadOffsets;
+    private BytesRef payload = new BytesRef();
+    private byte[] payloadBytes;
+
+    @Override
+    public int freq() throws IOException {
+      if (positions != null) {
+        return positions.length;
+      } else {
+        assert startOffsets != null;
+        return startOffsets.length;
+      }
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return (doc = 0);
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      return slowAdvance(target);
+    }
+
+    public void reset(Bits liveDocs, int[] positions, int[] startOffsets, int[] endOffsets, int[] payloadLengths, byte[] payloadBytes) {
+      this.liveDocs = liveDocs;
+      this.positions = positions;
+      this.startOffsets = startOffsets;
+      this.endOffsets = endOffsets;
+      this.payloadOffsets = payloadLengths;
+      this.payloadBytes = payloadBytes;
+      this.doc = -1;
+      didNext = false;
+      nextPos = 0;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      if (payloadOffsets == null) {
+        return null;
+      } else {
+        int off = payloadOffsets[nextPos-1];
+        int end = nextPos == payloadOffsets.length ? payloadBytes.length : payloadOffsets[nextPos];
+        if (end - off == 0) {
+          return null;
+        }
+        payload.bytes = payloadBytes;
+        payload.offset = off;
+        payload.length = end - off;
+        return payload;
+      }
+    }
+
+    @Override
+    public int nextPosition() {
+      assert (positions != null && nextPos < positions.length) ||
+        startOffsets != null && nextPos < startOffsets.length;
+
+      if (positions != null) {
+        return positions[nextPos++];
+      } else {
+        nextPos++;
+        return -1;
+      }
+    }
+
+    @Override
+    public int startOffset() {
+      if (startOffsets == null) {
+        return -1;
+      } else {
+        return startOffsets[nextPos-1];
+      }
+    }
+
+    @Override
+    public int endOffset() {
+      if (endOffsets == null) {
+        return -1;
+      } else {
+        return endOffsets[nextPos-1];
+      }
+    }
+    
+    @Override
+    public long cost() {
+      return 1;
+    }
+  }
+
+  @Override
+  public Fields get(int docID) throws IOException {
+    if (tvx != null) {
+      Fields fields = new TVFields(docID);
+      if (fields.size() == 0) {
+        // TODO: we can improve writer here, eg write 0 into
+        // tvx file, so we know on first read from tvx that
+        // this doc has no TVs
+        return null;
+      } else {
+        return fields;
+      }
+    } else {
+      return null;
+    }
+  }
+
+  @Override
+  public TermVectorsReader clone() {
+    IndexInput cloneTvx = null;
+    IndexInput cloneTvd = null;
+    IndexInput cloneTvf = null;
+
+    // These are null when a TermVectorsReader was created
+    // on a segment that did not have term vectors saved
+    if (tvx != null && tvd != null && tvf != null) {
+      cloneTvx = tvx.clone();
+      cloneTvd = tvd.clone();
+      cloneTvf = tvf.clone();
+    }
+    
+    return new Lucene40TermVectorsReader(fieldInfos, cloneTvx, cloneTvd, cloneTvf, size, numTotalDocs);
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return 0;
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {}
+}
+
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
new file mode 100644
index 0000000..5e4ee03
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
@@ -0,0 +1,135 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat;
+import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Implements the Lucene 4.1 index format, with configurable per-field postings formats.
+ * <p>
+ * If you want to reuse functionality of this codec in another codec, extend
+ * {@link FilterCodec}.
+ *
+ * @see org.apache.lucene.codecs.lucene41 package documentation for file format details.
+ * @deprecated Only for reading old 4.0 segments
+ * @lucene.experimental
+ */
+@Deprecated
+public class Lucene41Codec extends Codec {
+  // TODO: slightly evil
+  private final StoredFieldsFormat fieldsFormat = new CompressingStoredFieldsFormat("Lucene41StoredFields", CompressionMode.FAST, 1 << 14) {
+    @Override
+    public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
+      throw new UnsupportedOperationException("this codec can only be used for reading");
+    }
+  };
+  private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
+  private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
+  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
+  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  
+  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
+    @Override
+    public PostingsFormat getPostingsFormatForField(String field) {
+      return Lucene41Codec.this.getPostingsFormatForField(field);
+    }
+  };
+
+  /** Sole constructor. */
+  public Lucene41Codec() {
+    super("Lucene41");
+  }
+  
+  // TODO: slightly evil
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return fieldsFormat;
+  }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+
+  @Override
+  public final PostingsFormat postingsFormat() {
+    return postingsFormat;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+  
+  @Override
+  public SegmentInfoFormat segmentInfoFormat() {
+    return infosFormat;
+  }
+  
+  @Override
+  public final LiveDocsFormat liveDocsFormat() {
+    return liveDocsFormat;
+  }
+
+  /** Returns the postings format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene41"
+   */
+  public PostingsFormat getPostingsFormatForField(String field) {
+    return defaultFormat;
+  }
+  
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return dvFormat;
+  }
+
+  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
+  private final DocValuesFormat dvFormat = new Lucene40DocValuesFormat();
+  private final NormsFormat normsFormat = new Lucene40NormsFormat();
+
+  @Override
+  public NormsFormat normsFormat() {
+    return normsFormat;
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java
new file mode 100644
index 0000000..3e522a4
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java
@@ -0,0 +1,149 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
+import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
+import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * Implements the Lucene 4.2 index format, with configurable per-field postings
+ * and docvalues formats.
+ * <p>
+ * If you want to reuse functionality of this codec in another codec, extend
+ * {@link FilterCodec}.
+ *
+ * @see org.apache.lucene.codecs.lucene42 package documentation for file format details.
+ * @lucene.experimental
+ * @deprecated Only for reading old 4.2 segments
+ */
+// NOTE: if we make largish changes in a minor release, easier to just make Lucene43Codec or whatever
+// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
+// (it writes a minor version, etc).
+@Deprecated
+public class Lucene42Codec extends Codec {
+  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
+  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
+  private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat();
+  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
+  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  
+  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
+    @Override
+    public PostingsFormat getPostingsFormatForField(String field) {
+      return Lucene42Codec.this.getPostingsFormatForField(field);
+    }
+  };
+  
+  
+  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
+    @Override
+    public DocValuesFormat getDocValuesFormatForField(String field) {
+      return Lucene42Codec.this.getDocValuesFormatForField(field);
+    }
+  };
+
+  /** Sole constructor. */
+  public Lucene42Codec() {
+    super("Lucene42");
+  }
+  
+  @Override
+  public final StoredFieldsFormat storedFieldsFormat() {
+    return fieldsFormat;
+  }
+  
+  @Override
+  public final TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+
+  @Override
+  public final PostingsFormat postingsFormat() {
+    return postingsFormat;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+  
+  @Override
+  public SegmentInfoFormat segmentInfoFormat() {
+    return infosFormat;
+  }
+  
+  @Override
+  public final LiveDocsFormat liveDocsFormat() {
+    return liveDocsFormat;
+  }
+
+  /** Returns the postings format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene41"
+   */
+  public PostingsFormat getPostingsFormatForField(String field) {
+    return defaultFormat;
+  }
+  
+  /** Returns the docvalues format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene42"
+   */
+  public DocValuesFormat getDocValuesFormatForField(String field) {
+    return defaultDVFormat;
+  }
+  
+  @Override
+  public final DocValuesFormat docValuesFormat() {
+    return docValuesFormat;
+  }
+
+  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
+  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene42");
+
+  private final NormsFormat normsFormat = new Lucene42NormsFormat() {
+    @Override
+    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
+      throw new UnsupportedOperationException("this codec can only be used for reading");
+    }
+  };
+
+  @Override
+  public NormsFormat normsFormat() {
+    return normsFormat;
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java
new file mode 100644
index 0000000..02e6001
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java
@@ -0,0 +1,173 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+
+/**
+ * Lucene 4.2 DocValues format.
+ * <p>
+ * Encodes the four per-document value types (Numeric,Binary,Sorted,SortedSet) with seven basic strategies.
+ * <p>
+ * <ul>
+ *    <li>Delta-compressed Numerics: per-document integers written in blocks of 4096. For each block
+ *        the minimum value is encoded, and each entry is a delta from that minimum value.
+ *    <li>Table-compressed Numerics: when the number of unique values is very small, a lookup table
+ *        is written instead. Each per-document entry is instead the ordinal to this table.
+ *    <li>Uncompressed Numerics: when all values would fit into a single byte, and the 
+ *        <code>acceptableOverheadRatio</code> would pack values into 8 bits per value anyway, they
+ *        are written as absolute values (with no indirection or packing) for performance.
+ *    <li>GCD-compressed Numerics: when all numbers share a common divisor, such as dates, the greatest
+ *        common denominator (GCD) is computed, and quotients are stored using Delta-compressed Numerics.
+ *    <li>Fixed-width Binary: one large concatenated byte[] is written, along with the fixed length.
+ *        Each document's value can be addressed by maxDoc*length. 
+ *    <li>Variable-width Binary: one large concatenated byte[] is written, along with end addresses 
+ *        for each document. The addresses are written in blocks of 4096, with the current absolute
+ *        start for the block, and the average (expected) delta per entry. For each document the 
+ *        deviation from the delta (actual - expected) is written.
+ *    <li>Sorted: an FST mapping deduplicated terms to ordinals is written, along with the per-document
+ *        ordinals written using one of the numeric strategies above.
+ *    <li>SortedSet: an FST mapping deduplicated terms to ordinals is written, along with the per-document
+ *        ordinal list written using one of the binary strategies above.  
+ * </ul>
+ * <p>
+ * Files:
+ * <ol>
+ *   <li><tt>.dvd</tt>: DocValues data</li>
+ *   <li><tt>.dvm</tt>: DocValues metadata</li>
+ * </ol>
+ * <ol>
+ *   <li><a name="dvm" id="dvm"></a>
+ *   <p>The DocValues metadata or .dvm file.</p>
+ *   <p>For DocValues field, this stores metadata, such as the offset into the 
+ *      DocValues data (.dvd)</p>
+ *   <p>DocValues metadata (.dvm) --&gt; Header,&lt;FieldNumber,EntryType,Entry&gt;<sup>NumFields</sup>,Footer</p>
+ *   <ul>
+ *     <li>Entry --&gt; NumericEntry | BinaryEntry | SortedEntry</li>
+ *     <li>NumericEntry --&gt; DataOffset,CompressionType,PackedVersion</li>
+ *     <li>BinaryEntry --&gt; DataOffset,DataLength,MinLength,MaxLength,PackedVersion?,BlockSize?</li>
+ *     <li>SortedEntry --&gt; DataOffset,ValueCount</li>
+ *     <li>FieldNumber,PackedVersion,MinLength,MaxLength,BlockSize,ValueCount --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *     <li>DataOffset,DataLength --&gt; {@link DataOutput#writeLong Int64}</li>
+ *     <li>EntryType,CompressionType --&gt; {@link DataOutput#writeByte Byte}</li>
+ *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ *   </ul>
+ *   <p>Sorted fields have two entries: a SortedEntry with the FST metadata,
+ *      and an ordinary NumericEntry for the document-to-ord metadata.</p>
+ *   <p>SortedSet fields have two entries: a SortedEntry with the FST metadata,
+ *      and an ordinary BinaryEntry for the document-to-ord-list metadata.</p>
+ *   <p>FieldNumber of -1 indicates the end of metadata.</p>
+ *   <p>EntryType is a 0 (NumericEntry), 1 (BinaryEntry, or 2 (SortedEntry)</p>
+ *   <p>DataOffset is the pointer to the start of the data in the DocValues data (.dvd)</p>
+ *   <p>CompressionType indicates how Numeric values will be compressed:
+ *      <ul>
+ *         <li>0 --&gt; delta-compressed. For each block of 4096 integers, every integer is delta-encoded
+ *             from the minimum value within the block. 
+ *         <li>1 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
+ *             a lookup table of unique values is written, followed by the ordinal for each document.
+ *         <li>2 --&gt; uncompressed. When the <code>acceptableOverheadRatio</code> parameter would upgrade the number
+ *             of bits required to 8, and all values fit in a byte, these are written as absolute binary values
+ *             for performance.
+ *         <li>3 --&gt, gcd-compressed. When all integers share a common divisor, only quotients are stored
+ *             using blocks of delta-encoded ints.
+ *      </ul>
+ *   <p>MinLength and MaxLength represent the min and max byte[] value lengths for Binary values.
+ *      If they are equal, then all values are of a fixed size, and can be addressed as DataOffset + (docID * length).
+ *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
+ *      is written for the addresses.
+ *   <li><a name="dvd" id="dvd"></a>
+ *   <p>The DocValues data or .dvd file.</p>
+ *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
+ *   <p>DocValues data (.dvd) --&gt; Header,&lt;NumericData | BinaryData | SortedData&gt;<sup>NumFields</sup>,Footer</p>
+ *   <ul>
+ *     <li>NumericData --&gt; DeltaCompressedNumerics | TableCompressedNumerics | UncompressedNumerics | GCDCompressedNumerics</li>
+ *     <li>BinaryData --&gt;  {@link DataOutput#writeByte Byte}<sup>DataLength</sup>,Addresses</li>
+ *     <li>SortedData --&gt; {@link FST FST&lt;Int64&gt;}</li>
+ *     <li>DeltaCompressedNumerics --&gt; {@link BlockPackedWriter BlockPackedInts(blockSize=4096)}</li>
+ *     <li>TableCompressedNumerics --&gt; TableSize,{@link DataOutput#writeLong Int64}<sup>TableSize</sup>,{@link PackedInts PackedInts}</li>
+ *     <li>UncompressedNumerics --&gt; {@link DataOutput#writeByte Byte}<sup>maxdoc</sup></li>
+ *     <li>Addresses --&gt; {@link MonotonicBlockPackedWriter MonotonicBlockPackedInts(blockSize=4096)}</li>
+ *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ *   </ul>
+ *   <p>SortedSet entries store the list of ordinals in their BinaryData as a
+ *      sequences of increasing {@link DataOutput#writeVLong vLong}s, delta-encoded.</p>       
+ * </ol>
+ * <p>
+ * Limitations:
+ * <ul>
+ *   <li> Binary doc values can be at most {@link #MAX_BINARY_FIELD_LENGTH} in length.
+ * </ul>
+ * @deprecated Only for reading old 4.2 segments
+ */
+@Deprecated
+public class Lucene42DocValuesFormat extends DocValuesFormat {
+
+  /** Maximum length for each binary doc values field. */
+  public static final int MAX_BINARY_FIELD_LENGTH = (1 << 15) - 2;
+  
+  final float acceptableOverheadRatio;
+  
+  /** 
+   * Calls {@link #Lucene42DocValuesFormat(float) 
+   * Lucene42DocValuesFormat(PackedInts.DEFAULT)} 
+   */
+  public Lucene42DocValuesFormat() {
+    this(PackedInts.DEFAULT);
+  }
+  
+  /**
+   * Creates a new Lucene42DocValuesFormat with the specified
+   * <code>acceptableOverheadRatio</code> for NumericDocValues.
+   * @param acceptableOverheadRatio compression parameter for numerics. 
+   *        Currently this is only used when the number of unique values is small.
+   *        
+   * @lucene.experimental
+   */
+  public Lucene42DocValuesFormat(float acceptableOverheadRatio) {
+    super("Lucene42");
+    this.acceptableOverheadRatio = acceptableOverheadRatio;
+  }
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+  
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new Lucene42DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+  
+  static final String DATA_CODEC = "Lucene42DocValuesData";
+  static final String DATA_EXTENSION = "dvd";
+  static final String METADATA_CODEC = "Lucene42DocValuesMetadata";
+  static final String METADATA_EXTENSION = "dvm";
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
new file mode 100644
index 0000000..ef7ed68
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
@@ -0,0 +1,627 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedNumericDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.IntsRefBuilder;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.BytesRefFSTEnum.InputOutput;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.FST.Arc;
+import org.apache.lucene.util.fst.FST.BytesReader;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util;
+import org.apache.lucene.util.packed.BlockPackedReader;
+import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Reader for {@link Lucene42DocValuesFormat}
+ */
+class Lucene42DocValuesProducer extends DocValuesProducer {
+  // metadata maps (just file pointers and minimal stuff)
+  private final Map<Integer,NumericEntry> numerics;
+  private final Map<Integer,BinaryEntry> binaries;
+  private final Map<Integer,FSTEntry> fsts;
+  private final IndexInput data;
+  private final int version;
+  
+  // ram instances we have already loaded
+  private final Map<Integer,NumericDocValues> numericInstances = 
+      new HashMap<>();
+  private final Map<Integer,BinaryDocValues> binaryInstances =
+      new HashMap<>();
+  private final Map<Integer,FST<Long>> fstInstances =
+      new HashMap<>();
+  
+  private final int maxDoc;
+  private final AtomicLong ramBytesUsed;
+  
+  static final byte NUMBER = 0;
+  static final byte BYTES = 1;
+  static final byte FST = 2;
+
+  static final int BLOCK_SIZE = 4096;
+  
+  static final byte DELTA_COMPRESSED = 0;
+  static final byte TABLE_COMPRESSED = 1;
+  static final byte UNCOMPRESSED = 2;
+  static final byte GCD_COMPRESSED = 3;
+  
+  static final int VERSION_START = 0;
+  static final int VERSION_GCD_COMPRESSION = 1;
+  static final int VERSION_CHECKSUM = 2;
+  static final int VERSION_CURRENT = VERSION_CHECKSUM;
+    
+  Lucene42DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    maxDoc = state.segmentInfo.getDocCount();
+    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+    // read in the entries from the metadata file.
+    ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
+    boolean success = false;
+    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
+    try {
+      version = CodecUtil.checkHeader(in, metaCodec, 
+                                      VERSION_START,
+                                      VERSION_CURRENT);
+      numerics = new HashMap<>();
+      binaries = new HashMap<>();
+      fsts = new HashMap<>();
+      readFields(in, state.fieldInfos);
+
+      if (version >= VERSION_CHECKSUM) {
+        CodecUtil.checkFooter(in);
+      } else {
+        CodecUtil.checkEOF(in);
+      }
+      
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(in);
+      } else {
+        IOUtils.closeWhileHandlingException(in);
+      }
+    }
+
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+    this.data = state.directory.openInput(dataName, state.context);
+    success = false;
+    try {
+      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
+                                                 VERSION_START,
+                                                 VERSION_CURRENT);
+      if (version != version2) {
+        throw new CorruptIndexException("Format versions mismatch");
+      }
+      
+      if (version >= VERSION_CHECKSUM) {
+        // NOTE: data file is too costly to verify checksum against all the bytes on open,
+        // but for now we at least verify proper structure of the checksum footer: which looks
+        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+        // such as file truncation.
+        CodecUtil.retrieveChecksum(data);
+      }
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this.data);
+      }
+    }
+  }
+  
+  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
+    int fieldNumber = meta.readVInt();
+    while (fieldNumber != -1) {
+      if (infos.fieldInfo(fieldNumber) == null) {
+        // trickier to validate more: because we re-use for norms, because we use multiple entries
+        // for "composite" types like sortedset, etc.
+        throw new CorruptIndexException("Invalid field number: " + fieldNumber + " (resource=" + meta + ")");
+      }
+      int fieldType = meta.readByte();
+      if (fieldType == NUMBER) {
+        NumericEntry entry = new NumericEntry();
+        entry.offset = meta.readLong();
+        entry.format = meta.readByte();
+        switch(entry.format) {
+          case DELTA_COMPRESSED:
+          case TABLE_COMPRESSED:
+          case GCD_COMPRESSED:
+          case UNCOMPRESSED:
+               break;
+          default:
+               throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
+        }
+        if (entry.format != UNCOMPRESSED) {
+          entry.packedIntsVersion = meta.readVInt();
+        }
+        numerics.put(fieldNumber, entry);
+      } else if (fieldType == BYTES) {
+        BinaryEntry entry = new BinaryEntry();
+        entry.offset = meta.readLong();
+        entry.numBytes = meta.readLong();
+        entry.minLength = meta.readVInt();
+        entry.maxLength = meta.readVInt();
+        if (entry.minLength != entry.maxLength) {
+          entry.packedIntsVersion = meta.readVInt();
+          entry.blockSize = meta.readVInt();
+        }
+        binaries.put(fieldNumber, entry);
+      } else if (fieldType == FST) {
+        FSTEntry entry = new FSTEntry();
+        entry.offset = meta.readLong();
+        entry.numOrds = meta.readVLong();
+        fsts.put(fieldNumber, entry);
+      } else {
+        throw new CorruptIndexException("invalid entry type: " + fieldType + ", input=" + meta);
+      }
+      fieldNumber = meta.readVInt();
+    }
+  }
+
+  @Override
+  public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOException {
+    NumericDocValues instance = numericInstances.get(field.number);
+    if (instance == null) {
+      instance = loadNumeric(field);
+      numericInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+  
+  @Override
+  public long ramBytesUsed() {
+    return ramBytesUsed.get();
+  }
+  
+  @Override
+  public void checkIntegrity() throws IOException {
+    if (version >= VERSION_CHECKSUM) {
+      CodecUtil.checksumEntireFile(data);
+    }
+  }
+
+  private NumericDocValues loadNumeric(FieldInfo field) throws IOException {
+    NumericEntry entry = numerics.get(field.number);
+    data.seek(entry.offset);
+    switch (entry.format) {
+      case TABLE_COMPRESSED:
+        int size = data.readVInt();
+        if (size > 256) {
+          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, input=" + data);
+        }
+        final long decode[] = new long[size];
+        for (int i = 0; i < decode.length; i++) {
+          decode[i] = data.readLong();
+        }
+        final int formatID = data.readVInt();
+        final int bitsPerValue = data.readVInt();
+        final PackedInts.Reader ordsReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), entry.packedIntsVersion, maxDoc, bitsPerValue);
+        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
+        return new NumericDocValues() {
+          @Override
+          public long get(int docID) {
+            return decode[(int)ordsReader.get(docID)];
+          }
+        };
+      case DELTA_COMPRESSED:
+        final int blockSize = data.readVInt();
+        final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, blockSize, maxDoc, false);
+        ramBytesUsed.addAndGet(reader.ramBytesUsed());
+        return reader;
+      case UNCOMPRESSED:
+        final byte bytes[] = new byte[maxDoc];
+        data.readBytes(bytes, 0, bytes.length);
+        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes));
+        return new NumericDocValues() {
+          @Override
+          public long get(int docID) {
+            return bytes[docID];
+          }
+        };
+      case GCD_COMPRESSED:
+        final long min = data.readLong();
+        final long mult = data.readLong();
+        final int quotientBlockSize = data.readVInt();
+        final BlockPackedReader quotientReader = new BlockPackedReader(data, entry.packedIntsVersion, quotientBlockSize, maxDoc, false);
+        ramBytesUsed.addAndGet(quotientReader.ramBytesUsed());
+        return new NumericDocValues() {
+          @Override
+          public long get(int docID) {
+            return min + mult * quotientReader.get(docID);
+          }
+        };
+      default:
+        throw new AssertionError();
+    }
+  }
+
+  @Override
+  public synchronized BinaryDocValues getBinary(FieldInfo field) throws IOException {
+    BinaryDocValues instance = binaryInstances.get(field.number);
+    if (instance == null) {
+      instance = loadBinary(field);
+      binaryInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+  
+  private BinaryDocValues loadBinary(FieldInfo field) throws IOException {
+    BinaryEntry entry = binaries.get(field.number);
+    data.seek(entry.offset);
+    PagedBytes bytes = new PagedBytes(16);
+    bytes.copy(data, entry.numBytes);
+    final PagedBytes.Reader bytesReader = bytes.freeze(true);
+    if (entry.minLength == entry.maxLength) {
+      final int fixedLength = entry.minLength;
+      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed());
+      return new BinaryDocValues() {
+        @Override
+        public BytesRef get(int docID) {
+          final BytesRef term = new BytesRef();
+          bytesReader.fillSlice(term, fixedLength * (long)docID, fixedLength);
+          return term;
+        }
+      };
+    } else {
+      final MonotonicBlockPackedReader addresses = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, maxDoc, false);
+      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + addresses.ramBytesUsed());
+      return new BinaryDocValues() {
+
+        @Override
+        public BytesRef get(int docID) {
+          long startAddress = docID == 0 ? 0 : addresses.get(docID-1);
+          long endAddress = addresses.get(docID); 
+          final BytesRef term = new BytesRef();
+          bytesReader.fillSlice(term, startAddress, (int) (endAddress - startAddress));
+          return term;
+        }
+      };
+    }
+  }
+  
+  @Override
+  public SortedDocValues getSorted(FieldInfo field) throws IOException {
+    final FSTEntry entry = fsts.get(field.number);
+    FST<Long> instance;
+    synchronized(this) {
+      instance = fstInstances.get(field.number);
+      if (instance == null) {
+        data.seek(entry.offset);
+        instance = new FST<>(data, PositiveIntOutputs.getSingleton());
+        ramBytesUsed.addAndGet(instance.ramBytesUsed());
+        fstInstances.put(field.number, instance);
+      }
+    }
+    final NumericDocValues docToOrd = getNumeric(field);
+    final FST<Long> fst = instance;
+    
+    // per-thread resources
+    final BytesReader in = fst.getBytesReader();
+    final Arc<Long> firstArc = new Arc<>();
+    final Arc<Long> scratchArc = new Arc<>();
+    final IntsRefBuilder scratchInts = new IntsRefBuilder();
+    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
+    
+    return new SortedDocValues() {
+
+      final BytesRefBuilder term = new BytesRefBuilder();
+
+      @Override
+      public int getOrd(int docID) {
+        return (int) docToOrd.get(docID);
+      }
+
+      @Override
+      public BytesRef lookupOrd(int ord) {
+        try {
+          in.setPosition(0);
+          fst.getFirstArc(firstArc);
+          IntsRef output = Util.getByOutput(fst, ord, in, firstArc, scratchArc, scratchInts);
+          term.grow(output.length);
+          term.clear();
+          return Util.toBytesRef(output, term);
+        } catch (IOException bogus) {
+          throw new RuntimeException(bogus);
+        }
+      }
+
+      @Override
+      public int lookupTerm(BytesRef key) {
+        try {
+          InputOutput<Long> o = fstEnum.seekCeil(key);
+          if (o == null) {
+            return -getValueCount()-1;
+          } else if (o.input.equals(key)) {
+            return o.output.intValue();
+          } else {
+            return (int) -o.output-1;
+          }
+        } catch (IOException bogus) {
+          throw new RuntimeException(bogus);
+        }
+      }
+
+      @Override
+      public int getValueCount() {
+        return (int)entry.numOrds;
+      }
+
+      @Override
+      public TermsEnum termsEnum() {
+        return new FSTTermsEnum(fst);
+      }
+    };
+  }
+  
+  @Override
+  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
+    final FSTEntry entry = fsts.get(field.number);
+    if (entry.numOrds == 0) {
+      return DocValues.emptySortedSet(); // empty FST!
+    }
+    FST<Long> instance;
+    synchronized(this) {
+      instance = fstInstances.get(field.number);
+      if (instance == null) {
+        data.seek(entry.offset);
+        instance = new FST<>(data, PositiveIntOutputs.getSingleton());
+        ramBytesUsed.addAndGet(instance.ramBytesUsed());
+        fstInstances.put(field.number, instance);
+      }
+    }
+    final BinaryDocValues docToOrds = getBinary(field);
+    final FST<Long> fst = instance;
+    
+    // per-thread resources
+    final BytesReader in = fst.getBytesReader();
+    final Arc<Long> firstArc = new Arc<>();
+    final Arc<Long> scratchArc = new Arc<>();
+    final IntsRefBuilder scratchInts = new IntsRefBuilder();
+    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
+    final ByteArrayDataInput input = new ByteArrayDataInput();
+    return new SortedSetDocValues() {
+      final BytesRefBuilder term = new BytesRefBuilder();
+      BytesRef ordsRef;
+      long currentOrd;
+
+      @Override
+      public long nextOrd() {
+        if (input.eof()) {
+          return NO_MORE_ORDS;
+        } else {
+          currentOrd += input.readVLong();
+          return currentOrd;
+        }
+      }
+      
+      @Override
+      public void setDocument(int docID) {
+        ordsRef = docToOrds.get(docID);
+        input.reset(ordsRef.bytes, ordsRef.offset, ordsRef.length);
+        currentOrd = 0;
+      }
+
+      @Override
+      public BytesRef lookupOrd(long ord) {
+        try {
+          in.setPosition(0);
+          fst.getFirstArc(firstArc);
+          IntsRef output = Util.getByOutput(fst, ord, in, firstArc, scratchArc, scratchInts);
+          term.grow(output.length);
+          term.clear();
+          return Util.toBytesRef(output, term);
+        } catch (IOException bogus) {
+          throw new RuntimeException(bogus);
+        }
+      }
+
+      @Override
+      public long lookupTerm(BytesRef key) {
+        try {
+          InputOutput<Long> o = fstEnum.seekCeil(key);
+          if (o == null) {
+            return -getValueCount()-1;
+          } else if (o.input.equals(key)) {
+            return o.output.intValue();
+          } else {
+            return -o.output-1;
+          }
+        } catch (IOException bogus) {
+          throw new RuntimeException(bogus);
+        }
+      }
+
+      @Override
+      public long getValueCount() {
+        return entry.numOrds;
+      }
+
+      @Override
+      public TermsEnum termsEnum() {
+        return new FSTTermsEnum(fst);
+      }
+    };
+  }
+  
+  @Override
+  public Bits getDocsWithField(FieldInfo field) throws IOException {
+    if (field.getDocValuesType() == FieldInfo.DocValuesType.SORTED_SET) {
+      return DocValues.docsWithValue(getSortedSet(field), maxDoc);
+    } else {
+      return new Bits.MatchAllBits(maxDoc);
+    }
+  }
+  
+  @Override
+  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
+    throw new IllegalStateException("Lucene 4.2 does not support SortedNumeric: how did you pull this off?");
+  }
+
+  @Override
+  public void close() throws IOException {
+    data.close();
+  }
+  
+  static class NumericEntry {
+    long offset;
+    byte format;
+    int packedIntsVersion;
+  }
+  
+  static class BinaryEntry {
+    long offset;
+    long numBytes;
+    int minLength;
+    int maxLength;
+    int packedIntsVersion;
+    int blockSize;
+  }
+  
+  static class FSTEntry {
+    long offset;
+    long numOrds;
+  }
+  
+  // exposes FSTEnum directly as a TermsEnum: avoids binary-search next()
+  static class FSTTermsEnum extends TermsEnum {
+    final BytesRefFSTEnum<Long> in;
+    
+    // this is all for the complicated seek(ord)...
+    // maybe we should add a FSTEnum that supports this operation?
+    final FST<Long> fst;
+    final FST.BytesReader bytesReader;
+    final Arc<Long> firstArc = new Arc<>();
+    final Arc<Long> scratchArc = new Arc<>();
+    final IntsRefBuilder scratchInts = new IntsRefBuilder();
+    final BytesRefBuilder scratchBytes = new BytesRefBuilder();
+    
+    FSTTermsEnum(FST<Long> fst) {
+      this.fst = fst;
+      in = new BytesRefFSTEnum<>(fst);
+      bytesReader = fst.getBytesReader();
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      InputOutput<Long> io = in.next();
+      if (io == null) {
+        return null;
+      } else {
+        return io.input;
+      }
+    }
+
+    @Override
+    public SeekStatus seekCeil(BytesRef text) throws IOException {
+      if (in.seekCeil(text) == null) {
+        return SeekStatus.END;
+      } else if (term().equals(text)) {
+        // TODO: add SeekStatus to FSTEnum like in https://issues.apache.org/jira/browse/LUCENE-3729
+        // to remove this comparision?
+        return SeekStatus.FOUND;
+      } else {
+        return SeekStatus.NOT_FOUND;
+      }
+    }
+
+    @Override
+    public boolean seekExact(BytesRef text) throws IOException {
+      if (in.seekExact(text) == null) {
+        return false;
+      } else {
+        return true;
+      }
+    }
+
+    @Override
+    public void seekExact(long ord) throws IOException {
+      // TODO: would be better to make this simpler and faster.
+      // but we dont want to introduce a bug that corrupts our enum state!
+      bytesReader.setPosition(0);
+      fst.getFirstArc(firstArc);
+      IntsRef output = Util.getByOutput(fst, ord, bytesReader, firstArc, scratchArc, scratchInts);
+      BytesRefBuilder scratchBytes = new BytesRefBuilder();
+      scratchBytes.clear();
+      Util.toBytesRef(output, scratchBytes);
+      // TODO: we could do this lazily, better to try to push into FSTEnum though?
+      in.seekExact(scratchBytes.get());
+    }
+
+    @Override
+    public BytesRef term() throws IOException {
+      return in.current().input;
+    }
+
+    @Override
+    public long ord() throws IOException {
+      return in.current().output;
+    }
+
+    @Override
+    public int docFreq() throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public long totalTermFreq() throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java
new file mode 100644
index 0000000..5873bc0
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java
@@ -0,0 +1,122 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.index.FieldInfo.DocValuesType; // javadoc
+import org.apache.lucene.store.DataOutput; // javadoc
+
+/**
+ * Lucene 4.2 Field Infos format.
+ * <p>
+ * <p>Field names are stored in the field info file, with suffix <tt>.fnm</tt>.</p>
+ * <p>FieldInfos (.fnm) --&gt; Header,FieldsCount, &lt;FieldName,FieldNumber,
+ * FieldBits,DocValuesBits,Attributes&gt; <sup>FieldsCount</sup></p>
+ * <p>Data types:
+ * <ul>
+ *   <li>Header --&gt; {@link CodecUtil#checkHeader CodecHeader}</li>
+ *   <li>FieldsCount --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>FieldName --&gt; {@link DataOutput#writeString String}</li>
+ *   <li>FieldBits, DocValuesBits --&gt; {@link DataOutput#writeByte Byte}</li>
+ *   <li>FieldNumber --&gt; {@link DataOutput#writeInt VInt}</li>
+ *   <li>Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
+ * </ul>
+ * </p>
+ * Field Descriptions:
+ * <ul>
+ *   <li>FieldsCount: the number of fields in this file.</li>
+ *   <li>FieldName: name of the field as a UTF-8 String.</li>
+ *   <li>FieldNumber: the field's number. Note that unlike previous versions of
+ *       Lucene, the fields are not numbered implicitly by their order in the
+ *       file, instead explicitly.</li>
+ *   <li>FieldBits: a byte containing field options.
+ *       <ul>
+ *         <li>The low-order bit is one for indexed fields, and zero for non-indexed
+ *             fields.</li>
+ *         <li>The second lowest-order bit is one for fields that have term vectors
+ *             stored, and zero for fields without term vectors.</li>
+ *         <li>If the third lowest order-bit is set (0x4), offsets are stored into
+ *             the postings list in addition to positions.</li>
+ *         <li>Fourth bit is unused.</li>
+ *         <li>If the fifth lowest-order bit is set (0x10), norms are omitted for the
+ *             indexed field.</li>
+ *         <li>If the sixth lowest-order bit is set (0x20), payloads are stored for the
+ *             indexed field.</li>
+ *         <li>If the seventh lowest-order bit is set (0x40), term frequencies and
+ *             positions omitted for the indexed field.</li>
+ *         <li>If the eighth lowest-order bit is set (0x80), positions are omitted for the
+ *             indexed field.</li>
+ *       </ul>
+ *    </li>
+ *    <li>DocValuesBits: a byte containing per-document value types. The type
+ *        recorded as two four-bit integers, with the high-order bits representing
+ *        <code>norms</code> options, and the low-order bits representing 
+ *        {@code DocValues} options. Each four-bit integer can be decoded as such:
+ *        <ul>
+ *          <li>0: no DocValues for this field.</li>
+ *          <li>1: NumericDocValues. ({@link DocValuesType#NUMERIC})</li>
+ *          <li>2: BinaryDocValues. ({@code DocValuesType#BINARY})</li>
+ *          <li>3: SortedDocValues. ({@code DocValuesType#SORTED})</li>
+ *        </ul>
+ *    </li>
+ *    <li>Attributes: a key-value map of codec-private attributes.</li>
+ * </ul>
+ *
+ * @lucene.experimental
+ * @deprecated Only for reading old 4.2-4.5 segments
+ */
+@Deprecated
+public class Lucene42FieldInfosFormat extends FieldInfosFormat {
+  private final FieldInfosReader reader = new Lucene42FieldInfosReader();
+  
+  /** Sole constructor. */
+  public Lucene42FieldInfosFormat() {
+  }
+
+  @Override
+  public FieldInfosReader getFieldInfosReader() throws IOException {
+    return reader;
+  }
+
+  @Override
+  public FieldInfosWriter getFieldInfosWriter() throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+  
+  /** Extension of field infos */
+  static final String EXTENSION = "fnm";
+  
+  // Codec header
+  static final String CODEC_NAME = "Lucene42FieldInfos";
+  static final int FORMAT_START = 0;
+  static final int FORMAT_CURRENT = FORMAT_START;
+  
+  // Field flags
+  static final byte IS_INDEXED = 0x1;
+  static final byte STORE_TERMVECTOR = 0x2;
+  static final byte STORE_OFFSETS_IN_POSTINGS = 0x4;
+  static final byte OMIT_NORMS = 0x10;
+  static final byte STORE_PAYLOADS = 0x20;
+  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
+  static final byte OMIT_POSITIONS = -128;
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java
new file mode 100644
index 0000000..1919b79
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java
@@ -0,0 +1,123 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 4.2 FieldInfos reader.
+ * 
+ * @lucene.experimental
+ * @deprecated Only for reading old 4.2-4.5 segments
+ * @see Lucene42FieldInfosFormat
+ */
+@Deprecated
+final class Lucene42FieldInfosReader extends FieldInfosReader {
+
+  /** Sole constructor. */
+  public Lucene42FieldInfosReader() {
+  }
+
+  @Override
+  public FieldInfos read(Directory directory, String segmentName, String segmentSuffix, IOContext iocontext) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene42FieldInfosFormat.EXTENSION);
+    IndexInput input = directory.openInput(fileName, iocontext);
+    
+    boolean success = false;
+    try {
+      CodecUtil.checkHeader(input, Lucene42FieldInfosFormat.CODEC_NAME, 
+                                   Lucene42FieldInfosFormat.FORMAT_START, 
+                                   Lucene42FieldInfosFormat.FORMAT_CURRENT);
+
+      final int size = input.readVInt(); //read in the size
+      FieldInfo infos[] = new FieldInfo[size];
+
+      for (int i = 0; i < size; i++) {
+        String name = input.readString();
+        final int fieldNumber = input.readVInt();
+        byte bits = input.readByte();
+        boolean isIndexed = (bits & Lucene42FieldInfosFormat.IS_INDEXED) != 0;
+        boolean storeTermVector = (bits & Lucene42FieldInfosFormat.STORE_TERMVECTOR) != 0;
+        boolean omitNorms = (bits & Lucene42FieldInfosFormat.OMIT_NORMS) != 0;
+        boolean storePayloads = (bits & Lucene42FieldInfosFormat.STORE_PAYLOADS) != 0;
+        final IndexOptions indexOptions;
+        if (!isIndexed) {
+          indexOptions = null;
+        } else if ((bits & Lucene42FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
+          indexOptions = IndexOptions.DOCS_ONLY;
+        } else if ((bits & Lucene42FieldInfosFormat.OMIT_POSITIONS) != 0) {
+          indexOptions = IndexOptions.DOCS_AND_FREQS;
+        } else if ((bits & Lucene42FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS) != 0) {
+          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
+        } else {
+          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+        }
+
+        // DV Types are packed in one byte
+        byte val = input.readByte();
+        final DocValuesType docValuesType = getDocValuesType(input, (byte) (val & 0x0F));
+        final DocValuesType normsType = getDocValuesType(input, (byte) ((val >>> 4) & 0x0F));
+        final Map<String,String> attributes = input.readStringStringMap();
+        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
+          omitNorms, storePayloads, indexOptions, docValuesType, normsType, -1, Collections.unmodifiableMap(attributes));
+      }
+
+      CodecUtil.checkEOF(input);
+      FieldInfos fieldInfos = new FieldInfos(infos);
+      success = true;
+      return fieldInfos;
+    } finally {
+      if (success) {
+        input.close();
+      } else {
+        IOUtils.closeWhileHandlingException(input);
+      }
+    }
+  }
+  
+  private static DocValuesType getDocValuesType(IndexInput input, byte b) throws IOException {
+    if (b == 0) {
+      return null;
+    } else if (b == 1) {
+      return DocValuesType.NUMERIC;
+    } else if (b == 2) {
+      return DocValuesType.BINARY;
+    } else if (b == 3) {
+      return DocValuesType.SORTED;
+    } else if (b == 4) {
+      return DocValuesType.SORTED_SET;
+    } else {
+      throw new CorruptIndexException("invalid docvalues byte: " + b + " (resource=" + input + ")");
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java
new file mode 100644
index 0000000..2c66075
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java
@@ -0,0 +1,84 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Lucene 4.2 score normalization format.
+ * <p>
+ * NOTE: this uses the same format as {@link Lucene42DocValuesFormat}
+ * Numeric DocValues, but with different file extensions, and passing
+ * {@link PackedInts#FASTEST} for uncompressed encoding: trading off
+ * space for performance.
+ * <p>
+ * Files:
+ * <ul>
+ *   <li><tt>.nvd</tt>: DocValues data</li>
+ *   <li><tt>.nvm</tt>: DocValues metadata</li>
+ * </ul>
+ * @see Lucene42DocValuesFormat
+ */
+public class Lucene42NormsFormat extends NormsFormat {
+  final float acceptableOverheadRatio;
+
+  /** 
+   * Calls {@link #Lucene42NormsFormat(float) 
+   * Lucene42DocValuesFormat(PackedInts.FASTEST)} 
+   */
+  public Lucene42NormsFormat() {
+    // note: we choose FASTEST here (otherwise our norms are half as big but 15% slower than previous lucene)
+    this(PackedInts.FASTEST);
+  }
+  
+  /**
+   * Creates a new Lucene42DocValuesFormat with the specified
+   * <code>acceptableOverheadRatio</code> for NumericDocValues.
+   * @param acceptableOverheadRatio compression parameter for numerics. 
+   *        Currently this is only used when the number of unique values is small.
+   *        
+   * @lucene.experimental
+   */
+  public Lucene42NormsFormat(float acceptableOverheadRatio) {
+    this.acceptableOverheadRatio = acceptableOverheadRatio;
+  }
+  
+  @Override
+  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    throw new UnsupportedOperationException("this codec can only be used for reading");
+  }
+  
+  @Override
+  public NormsProducer normsProducer(SegmentReadState state) throws IOException {
+    return new Lucene42NormsProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+  
+  static final String DATA_CODEC = "Lucene41NormsData";
+  static final String DATA_EXTENSION = "nvd";
+  static final String METADATA_CODEC = "Lucene41NormsMetadata";
+  static final String METADATA_EXTENSION = "nvm";
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java
new file mode 100644
index 0000000..a8b2073
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java
@@ -0,0 +1,59 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.NormsProducer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+
+/**
+ * Reads 4.2-4.8 norms.
+ * Implemented the same as docvalues, but with a different filename.
+ * @deprecated Only for reading old segments
+ */
+@Deprecated
+class Lucene42NormsProducer extends NormsProducer {
+  private final Lucene42DocValuesProducer impl;
+  
+  Lucene42NormsProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    impl = new Lucene42DocValuesProducer(state, dataCodec, dataExtension, metaCodec, metaExtension);
+  }
+
+  @Override
+  public NumericDocValues getNorms(FieldInfo field) throws IOException {
+    return impl.getNumeric(field);
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    impl.checkIntegrity();
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return impl.ramBytesUsed();
+  }
+
+  @Override
+  public void close() throws IOException {
+    impl.close();
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java
new file mode 100644
index 0000000..ced5584
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java
@@ -0,0 +1,152 @@
+package org.apache.lucene.codecs.lucene45;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
+import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42FieldInfosFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42NormsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
+import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * Implements the Lucene 4.5 index format, with configurable per-field postings
+ * and docvalues formats.
+ * <p>
+ * If you want to reuse functionality of this codec in another codec, extend
+ * {@link FilterCodec}.
+ *
+ * @see org.apache.lucene.codecs.lucene45 package documentation for file format details.
+ * @lucene.experimental
+ * @deprecated Only for reading old 4.3-4.5 segments
+ */
+// NOTE: if we make largish changes in a minor release, easier to just make Lucene46Codec or whatever
+// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
+// (it writes a minor version, etc).
+@Deprecated
+public class Lucene45Codec extends Codec {
+  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
+  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
+  private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat();
+  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
+  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  
+  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
+    @Override
+    public PostingsFormat getPostingsFormatForField(String field) {
+      return Lucene45Codec.this.getPostingsFormatForField(field);
+    }
+  };
+  
+  
+  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
+    @Override
+    public DocValuesFormat getDocValuesFormatForField(String field) {
+      return Lucene45Codec.this.getDocValuesFormatForField(field);
+    }
+  };
+
+  /** Sole constructor. */
+  public Lucene45Codec() {
+    super("Lucene45");
+  }
+  
+  @Override
+  public final StoredFieldsFormat storedFieldsFormat() {
+    return fieldsFormat;
+  }
+  
+  @Override
+  public final TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+
+  @Override
+  public final PostingsFormat postingsFormat() {
+    return postingsFormat;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+  
+  @Override
+  public SegmentInfoFormat segmentInfoFormat() {
+    return infosFormat;
+  }
+  
+  @Override
+  public final LiveDocsFormat liveDocsFormat() {
+    return liveDocsFormat;
+  }
+
+  /** Returns the postings format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene41"
+   */
+  public PostingsFormat getPostingsFormatForField(String field) {
+    return defaultFormat;
+  }
+  
+  /** Returns the docvalues format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene45"
+   */
+  public DocValuesFormat getDocValuesFormatForField(String field) {
+    return defaultDVFormat;
+  }
+  
+  @Override
+  public final DocValuesFormat docValuesFormat() {
+    return docValuesFormat;
+  }
+
+  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
+  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene45");
+
+  private final NormsFormat normsFormat = new Lucene42NormsFormat() {
+    @Override
+    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
+      throw new UnsupportedOperationException("this codec can only be used for reading");
+    }
+  };
+
+  @Override
+  public NormsFormat normsFormat() {
+    return normsFormat;
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java
new file mode 100644
index 0000000..54e0af8
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java
@@ -0,0 +1,433 @@
+package org.apache.lucene.codecs.lucene45;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable; // javadocs
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.MathUtil;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+
+/** writer for {@link Lucene45DocValuesFormat} */
+class Lucene45DocValuesConsumer extends DocValuesConsumer implements Closeable {
+
+  static final int BLOCK_SIZE = 16384;
+  static final int ADDRESS_INTERVAL = 16;
+
+  /** Compressed using packed blocks of ints. */
+  public static final int DELTA_COMPRESSED = 0;
+  /** Compressed by computing the GCD. */
+  public static final int GCD_COMPRESSED = 1;
+  /** Compressed by giving IDs to unique values. */
+  public static final int TABLE_COMPRESSED = 2;
+  
+  /** Uncompressed binary, written directly (fixed length). */
+  public static final int BINARY_FIXED_UNCOMPRESSED = 0;
+  /** Uncompressed binary, written directly (variable length). */
+  public static final int BINARY_VARIABLE_UNCOMPRESSED = 1;
+  /** Compressed binary with shared prefixes */
+  public static final int BINARY_PREFIX_COMPRESSED = 2;
+
+  /** Standard storage for sorted set values with 1 level of indirection:
+   *  docId -> address -> ord. */
+  public static final int SORTED_SET_WITH_ADDRESSES = 0;
+  /** Single-valued sorted set values, encoded as sorted values, so no level
+   *  of indirection: docId -> ord. */
+  public static final int SORTED_SET_SINGLE_VALUED_SORTED = 1;
+
+  IndexOutput data, meta;
+  final int maxDoc;
+  
+  /** expert: Creates a new writer */
+  public Lucene45DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    boolean success = false;
+    try {
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+      data = state.directory.createOutput(dataName, state.context);
+      CodecUtil.writeHeader(data, dataCodec, Lucene45DocValuesFormat.VERSION_CURRENT);
+      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+      meta = state.directory.createOutput(metaName, state.context);
+      CodecUtil.writeHeader(meta, metaCodec, Lucene45DocValuesFormat.VERSION_CURRENT);
+      maxDoc = state.segmentInfo.getDocCount();
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this);
+      }
+    }
+  }
+  
+  @Override
+  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+    checkCanWrite(field);
+    addNumericField(field, values, true);
+  }
+
+  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {
+    long count = 0;
+    long minValue = Long.MAX_VALUE;
+    long maxValue = Long.MIN_VALUE;
+    long gcd = 0;
+    boolean missing = false;
+    // TODO: more efficient?
+    HashSet<Long> uniqueValues = null;
+    if (optimizeStorage) {
+      uniqueValues = new HashSet<>();
+
+      for (Number nv : values) {
+        final long v;
+        if (nv == null) {
+          v = 0;
+          missing = true;
+        } else {
+          v = nv.longValue();
+        }
+
+        if (gcd != 1) {
+          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
+            // in that case v - minValue might overflow and make the GCD computation return
+            // wrong results. Since these extreme values are unlikely, we just discard
+            // GCD computation for them
+            gcd = 1;
+          } else if (count != 0) { // minValue needs to be set first
+            gcd = MathUtil.gcd(gcd, v - minValue);
+          }
+        }
+
+        minValue = Math.min(minValue, v);
+        maxValue = Math.max(maxValue, v);
+
+        if (uniqueValues != null) {
+          if (uniqueValues.add(v)) {
+            if (uniqueValues.size() > 256) {
+              uniqueValues = null;
+            }
+          }
+        }
+
+        ++count;
+      }
+    } else {
+      for (@SuppressWarnings("unused") Number nv : values) {
+        ++count;
+      }
+    }
+    
+    final long delta = maxValue - minValue;
+
+    final int format;
+    if (uniqueValues != null
+        && (PackedInts.bitsRequired(uniqueValues.size() - 1) < PackedInts.unsignedBitsRequired(delta))
+        && count <= Integer.MAX_VALUE) {
+      format = TABLE_COMPRESSED;
+    } else if (gcd != 0 && gcd != 1) {
+      format = GCD_COMPRESSED;
+    } else {
+      format = DELTA_COMPRESSED;
+    }
+    meta.writeVInt(field.number);
+    meta.writeByte(Lucene45DocValuesFormat.NUMERIC);
+    meta.writeVInt(format);
+    if (missing) {
+      meta.writeLong(data.getFilePointer());
+      writeMissingBitset(values);
+    } else {
+      meta.writeLong(-1L);
+    }
+    meta.writeVInt(PackedInts.VERSION_CURRENT);
+    meta.writeLong(data.getFilePointer());
+    meta.writeVLong(count);
+    meta.writeVInt(BLOCK_SIZE);
+
+    switch (format) {
+      case GCD_COMPRESSED:
+        meta.writeLong(minValue);
+        meta.writeLong(gcd);
+        final BlockPackedWriter quotientWriter = new BlockPackedWriter(data, BLOCK_SIZE);
+        for (Number nv : values) {
+          long value = nv == null ? 0 : nv.longValue();
+          quotientWriter.add((value - minValue) / gcd);
+        }
+        quotientWriter.finish();
+        break;
+      case DELTA_COMPRESSED:
+        final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
+        for (Number nv : values) {
+          writer.add(nv == null ? 0 : nv.longValue());
+        }
+        writer.finish();
+        break;
+      case TABLE_COMPRESSED:
+        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
+        final HashMap<Long,Integer> encode = new HashMap<>();
+        meta.writeVInt(decode.length);
+        for (int i = 0; i < decode.length; i++) {
+          meta.writeLong(decode[i]);
+          encode.put(decode[i], i);
+        }
+        final int bitsRequired = PackedInts.bitsRequired(uniqueValues.size() - 1);
+        final PackedInts.Writer ordsWriter = PackedInts.getWriterNoHeader(data, PackedInts.Format.PACKED, (int) count, bitsRequired, PackedInts.DEFAULT_BUFFER_SIZE);
+        for (Number nv : values) {
+          ordsWriter.add(encode.get(nv == null ? 0 : nv.longValue()));
+        }
+        ordsWriter.finish();
+        break;
+      default:
+        throw new AssertionError();
+    }
+  }
+  
+  // TODO: in some cases representing missing with minValue-1 wouldn't take up additional space and so on,
+  // but this is very simple, and algorithms only check this for values of 0 anyway (doesnt slow down normal decode)
+  void writeMissingBitset(Iterable<?> values) throws IOException {
+    byte bits = 0;
+    int count = 0;
+    for (Object v : values) {
+      if (count == 8) {
+        data.writeByte(bits);
+        count = 0;
+        bits = 0;
+      }
+      if (v != null) {
+        bits |= 1 << (count & 7);
+      }
+      count++;
+    }
+    if (count > 0) {
+      data.writeByte(bits);
+    }
+  }
+
+  @Override
+  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
+    checkCanWrite(field);
+    // write the byte[] data
+    meta.writeVInt(field.number);
+    meta.writeByte(Lucene45DocValuesFormat.BINARY);
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    final long startFP = data.getFilePointer();
+    long count = 0;
+    boolean missing = false;
+    for(BytesRef v : values) {
+      final int length;
+      if (v == null) {
+        length = 0;
+        missing = true;
+      } else {
+        length = v.length;
+      }
+      minLength = Math.min(minLength, length);
+      maxLength = Math.max(maxLength, length);
+      if (v != null) {
+        data.writeBytes(v.bytes, v.offset, v.length);
+      }
+      count++;
+    }
+    meta.writeVInt(minLength == maxLength ? BINARY_FIXED_UNCOMPRESSED : BINARY_VARIABLE_UNCOMPRESSED);
+    if (missing) {
+      meta.writeLong(data.getFilePointer());
+      writeMissingBitset(values);
+    } else {
+      meta.writeLong(-1L);
+    }
+    meta.writeVInt(minLength);
+    meta.writeVInt(maxLength);
+    meta.writeVLong(count);
+    meta.writeLong(startFP);
+    
+    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
+    // otherwise, we need to record the length fields...
+    if (minLength != maxLength) {
+      meta.writeLong(data.getFilePointer());
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      meta.writeVInt(BLOCK_SIZE);
+
+      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
+      long addr = 0;
+      for (BytesRef v : values) {
+        if (v != null) {
+          addr += v.length;
+        }
+        writer.add(addr);
+      }
+      writer.finish();
+    }
+  }
+  
+  /** expert: writes a value dictionary for a sorted/sortedset field */
+  protected void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
+    // first check if its a "fixed-length" terms dict
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    for (BytesRef v : values) {
+      minLength = Math.min(minLength, v.length);
+      maxLength = Math.max(maxLength, v.length);
+    }
+    if (minLength == maxLength) {
+      // no index needed: direct addressing by mult
+      addBinaryField(field, values);
+    } else {
+      // header
+      meta.writeVInt(field.number);
+      meta.writeByte(Lucene45DocValuesFormat.BINARY);
+      meta.writeVInt(BINARY_PREFIX_COMPRESSED);
+      meta.writeLong(-1L);
+      // now write the bytes: sharing prefixes within a block
+      final long startFP = data.getFilePointer();
+      // currently, we have to store the delta from expected for every 1/nth term
+      // we could avoid this, but its not much and less overall RAM than the previous approach!
+      RAMOutputStream addressBuffer = new RAMOutputStream();
+      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);
+      BytesRefBuilder lastTerm = new BytesRefBuilder();
+      long count = 0;
+      for (BytesRef v : values) {
+        if (count % ADDRESS_INTERVAL == 0) {
+          termAddresses.add(data.getFilePointer() - startFP);
+          // force the first term in a block to be abs-encoded
+          lastTerm.clear();
+        }
+        
+        // prefix-code
+        int sharedPrefix = StringHelper.bytesDifference(lastTerm.get(), v);
+        data.writeVInt(sharedPrefix);
+        data.writeVInt(v.length - sharedPrefix);
+        data.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);
+        lastTerm.copyBytes(v);
+        count++;
+      }
+      final long indexStartFP = data.getFilePointer();
+      // write addresses of indexed terms
+      termAddresses.finish();
+      addressBuffer.writeTo(data);
+      addressBuffer = null;
+      termAddresses = null;
+      meta.writeVInt(minLength);
+      meta.writeVInt(maxLength);
+      meta.writeVLong(count);
+      meta.writeLong(startFP);
+      meta.writeVInt(ADDRESS_INTERVAL);
+      meta.writeLong(indexStartFP);
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      meta.writeVInt(BLOCK_SIZE);
+    }
+  }
+
+  @Override
+  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+    checkCanWrite(field);
+    meta.writeVInt(field.number);
+    meta.writeByte(Lucene45DocValuesFormat.SORTED);
+    addTermsDict(field, values);
+    addNumericField(field, docToOrd, false);
+  }
+  
+  @Override
+  public void addSortedNumericField(FieldInfo field, Iterable<Number> docToValueCount, Iterable<Number> values) throws IOException {
+    throw new UnsupportedOperationException("Lucene 4.5 does not support SORTED_NUMERIC");
+  }
+
+  @Override
+  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
+    checkCanWrite(field);
+    meta.writeVInt(field.number);
+    meta.writeByte(Lucene45DocValuesFormat.SORTED_SET);
+
+    if (isSingleValued(docToOrdCount)) {
+      meta.writeVInt(SORTED_SET_SINGLE_VALUED_SORTED);
+      // The field is single-valued, we can encode it as SORTED
+      addSortedField(field, values, singletonView(docToOrdCount, ords, -1L));
+      return;
+    }
+
+    meta.writeVInt(SORTED_SET_WITH_ADDRESSES);
+
+    // write the ord -> byte[] as a binary field
+    addTermsDict(field, values);
+
+    // write the stream of ords as a numeric field
+    // NOTE: we could return an iterator that delta-encodes these within a doc
+    addNumericField(field, ords, false);
+
+    // write the doc -> ord count as a absolute index to the stream
+    meta.writeVInt(field.number);
+    meta.writeByte(Lucene45DocValuesFormat.NUMERIC);
+    meta.writeVInt(DELTA_COMPRESSED);
+    meta.writeLong(-1L);
+    meta.writeVInt(PackedInts.VERSION_CURRENT);
+    meta.writeLong(data.getFilePointer());
+    meta.writeVLong(maxDoc);
+    meta.writeVInt(BLOCK_SIZE);
+
+    final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
+    long addr = 0;
+    for (Number v : docToOrdCount) {
+      addr += v.longValue();
+      writer.add(addr);
+    }
+    writer.finish();
+  }
+
+  @Override
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      if (meta != null) {
+        meta.writeVInt(-1); // write EOF marker
+        CodecUtil.writeFooter(meta); // write checksum
+      }
+      if (data != null) {
+        CodecUtil.writeFooter(data); // write checksum
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data, meta);
+      } else {
+        IOUtils.closeWhileHandlingException(data, meta);
+      }
+      meta = data = null;
+    }
+  }
+  
+  void checkCanWrite(FieldInfo field) {
+    if ((field.getDocValuesType() == DocValuesType.NUMERIC || 
+        field.getDocValuesType() == DocValuesType.BINARY) && 
+        field.getDocValuesGen() != -1) {
+      // ok
+    } else {
+      throw new UnsupportedOperationException("this codec can only be used for reading");
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java
new file mode 100644
index 0000000..04b4821
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java
@@ -0,0 +1,196 @@
+package org.apache.lucene.codecs.lucene45;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.SmallFloat;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Lucene 4.5 DocValues format.
+ * <p>
+ * Encodes the four per-document value types (Numeric,Binary,Sorted,SortedSet) with these strategies:
+ * <p>
+ * {@link DocValuesType#NUMERIC NUMERIC}:
+ * <ul>
+ *    <li>Delta-compressed: per-document integers written in blocks of 16k. For each block
+ *        the minimum value in that block is encoded, and each entry is a delta from that 
+ *        minimum value. Each block of deltas is compressed with bitpacking. For more 
+ *        information, see {@link BlockPackedWriter}.
+ *    <li>Table-compressed: when the number of unique values is very small (&lt; 256), and
+ *        when there are unused "gaps" in the range of values used (such as {@link SmallFloat}), 
+ *        a lookup table is written instead. Each per-document entry is instead the ordinal 
+ *        to this table, and those ordinals are compressed with bitpacking ({@link PackedInts}). 
+ *    <li>GCD-compressed: when all numbers share a common divisor, such as dates, the greatest
+ *        common denominator (GCD) is computed, and quotients are stored using Delta-compressed Numerics.
+ * </ul>
+ * <p>
+ * {@link DocValuesType#BINARY BINARY}:
+ * <ul>
+ *    <li>Fixed-width Binary: one large concatenated byte[] is written, along with the fixed length.
+ *        Each document's value can be addressed directly with multiplication ({@code docID * length}). 
+ *    <li>Variable-width Binary: one large concatenated byte[] is written, along with end addresses 
+ *        for each document. The addresses are written in blocks of 16k, with the current absolute
+ *        start for the block, and the average (expected) delta per entry. For each document the 
+ *        deviation from the delta (actual - expected) is written.
+ *    <li>Prefix-compressed Binary: values are written in chunks of 16, with the first value written
+ *        completely and other values sharing prefixes. chunk addresses are written in blocks of 16k,
+ *        with the current absolute start for the block, and the average (expected) delta per entry. 
+ *        For each chunk the deviation from the delta (actual - expected) is written.
+ * </ul>
+ * <p>
+ * {@link DocValuesType#SORTED SORTED}:
+ * <ul>
+ *    <li>Sorted: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
+ *        along with the per-document ordinals written using one of the numeric strategies above.
+ * </ul>
+ * <p>
+ * {@link DocValuesType#SORTED_SET SORTED_SET}:
+ * <ul>
+ *    <li>SortedSet: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
+ *        an ordinal list and per-document index into this list are written using the numeric strategies 
+ *        above. 
+ * </ul>
+ * <p>
+ * Files:
+ * <ol>
+ *   <li><tt>.dvd</tt>: DocValues data</li>
+ *   <li><tt>.dvm</tt>: DocValues metadata</li>
+ * </ol>
+ * <ol>
+ *   <li><a name="dvm" id="dvm"></a>
+ *   <p>The DocValues metadata or .dvm file.</p>
+ *   <p>For DocValues field, this stores metadata, such as the offset into the 
+ *      DocValues data (.dvd)</p>
+ *   <p>DocValues metadata (.dvm) --&gt; Header,&lt;Entry&gt;<sup>NumFields</sup>,Footer</p>
+ *   <ul>
+ *     <li>Entry --&gt; NumericEntry | BinaryEntry | SortedEntry | SortedSetEntry</li>
+ *     <li>NumericEntry --&gt; GCDNumericEntry | TableNumericEntry | DeltaNumericEntry</li>
+ *     <li>GCDNumericEntry --&gt; NumericHeader,MinValue,GCD</li>
+ *     <li>TableNumericEntry --&gt; NumericHeader,TableSize,{@link DataOutput#writeLong Int64}<sup>TableSize</sup></li>
+ *     <li>DeltaNumericEntry --&gt; NumericHeader</li>
+ *     <li>NumericHeader --&gt; FieldNumber,EntryType,NumericType,MissingOffset,PackedVersion,DataOffset,Count,BlockSize</li>
+ *     <li>BinaryEntry --&gt; FixedBinaryEntry | VariableBinaryEntry | PrefixBinaryEntry</li>
+ *     <li>FixedBinaryEntry --&gt; BinaryHeader</li>
+ *     <li>VariableBinaryEntry --&gt; BinaryHeader,AddressOffset,PackedVersion,BlockSize</li>
+ *     <li>PrefixBinaryEntry --&gt; BinaryHeader,AddressInterval,AddressOffset,PackedVersion,BlockSize</li>
+ *     <li>BinaryHeader --&gt; FieldNumber,EntryType,BinaryType,MissingOffset,MinLength,MaxLength,DataOffset</li>
+ *     <li>SortedEntry --&gt; FieldNumber,EntryType,BinaryEntry,NumericEntry</li>
+ *     <li>SortedSetEntry --&gt; EntryType,BinaryEntry,NumericEntry,NumericEntry</li>
+ *     <li>FieldNumber,PackedVersion,MinLength,MaxLength,BlockSize,ValueCount --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *     <li>EntryType,CompressionType --&gt; {@link DataOutput#writeByte Byte}</li>
+ *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *     <li>MinValue,GCD,MissingOffset,AddressOffset,DataOffset --&gt; {@link DataOutput#writeLong Int64}</li>
+ *     <li>TableSize --&gt; {@link DataOutput#writeVInt vInt}</li>
+ *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ *   </ul>
+ *   <p>Sorted fields have two entries: a BinaryEntry with the value metadata,
+ *      and an ordinary NumericEntry for the document-to-ord metadata.</p>
+ *   <p>SortedSet fields have three entries: a BinaryEntry with the value metadata,
+ *      and two NumericEntries for the document-to-ord-index and ordinal list metadata.</p>
+ *   <p>FieldNumber of -1 indicates the end of metadata.</p>
+ *   <p>EntryType is a 0 (NumericEntry) or 1 (BinaryEntry)</p>
+ *   <p>DataOffset is the pointer to the start of the data in the DocValues data (.dvd)</p>
+ *   <p>NumericType indicates how Numeric values will be compressed:
+ *      <ul>
+ *         <li>0 --&gt; delta-compressed. For each block of 16k integers, every integer is delta-encoded
+ *             from the minimum value within the block. 
+ *         <li>1 --&gt, gcd-compressed. When all integers share a common divisor, only quotients are stored
+ *             using blocks of delta-encoded ints.
+ *         <li>2 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
+ *             a lookup table of unique values is written, followed by the ordinal for each document.
+ *      </ul>
+ *   <p>BinaryType indicates how Binary values will be stored:
+ *      <ul>
+ *         <li>0 --&gt; fixed-width. All values have the same length, addressing by multiplication. 
+ *         <li>1 --&gt, variable-width. An address for each value is stored.
+ *         <li>2 --&gt; prefix-compressed. An address to the start of every interval'th value is stored.
+ *      </ul>
+ *   <p>MinLength and MaxLength represent the min and max byte[] value lengths for Binary values.
+ *      If they are equal, then all values are of a fixed size, and can be addressed as DataOffset + (docID * length).
+ *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
+ *      is written for the addresses.
+ *   <p>MissingOffset points to a byte[] containing a bitset of all documents that had a value for the field.
+ *      If its -1, then there are no missing values.
+ *   <p>Checksum contains the CRC32 checksum of all bytes in the .dvm file up
+ *      until the checksum. This is used to verify integrity of the file on opening the
+ *      index.
+ *   <li><a name="dvd" id="dvd"></a>
+ *   <p>The DocValues data or .dvd file.</p>
+ *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
+ *   <p>DocValues data (.dvd) --&gt; Header,&lt;NumericData | BinaryData | SortedData&gt;<sup>NumFields</sup>,Footer</p>
+ *   <ul>
+ *     <li>NumericData --&gt; DeltaCompressedNumerics | TableCompressedNumerics | GCDCompressedNumerics</li>
+ *     <li>BinaryData --&gt;  {@link DataOutput#writeByte Byte}<sup>DataLength</sup>,Addresses</li>
+ *     <li>SortedData --&gt; {@link FST FST&lt;Int64&gt;}</li>
+ *     <li>DeltaCompressedNumerics --&gt; {@link BlockPackedWriter BlockPackedInts(blockSize=16k)}</li>
+ *     <li>TableCompressedNumerics --&gt; {@link PackedInts PackedInts}</li>
+ *     <li>GCDCompressedNumerics --&gt; {@link BlockPackedWriter BlockPackedInts(blockSize=16k)}</li>
+ *     <li>Addresses --&gt; {@link MonotonicBlockPackedWriter MonotonicBlockPackedInts(blockSize=16k)}</li>
+ *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ *   </ul>
+ *   <p>SortedSet entries store the list of ordinals in their BinaryData as a
+ *      sequences of increasing {@link DataOutput#writeVLong vLong}s, delta-encoded.</p>
+ * </ol>
+ * @deprecated Only for reading old 4.3-4.5 segments
+ * @lucene.experimental
+ */
+@Deprecated
+public class Lucene45DocValuesFormat extends DocValuesFormat {
+
+  /** Sole Constructor */
+  public Lucene45DocValuesFormat() {
+    super("Lucene45");
+  }
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    // really we should be read-only, but to allow posting of dv updates against old segments...
+    return new Lucene45DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
+  }
+
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new Lucene45DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
+  }
+  
+  static final String DATA_CODEC = "Lucene45DocValuesData";
+  static final String DATA_EXTENSION = "dvd";
+  static final String META_CODEC = "Lucene45ValuesMetadata";
+  static final String META_EXTENSION = "dvm";
+  static final int VERSION_START = 0;
+  static final int VERSION_SORTED_SET_SINGLE_VALUE_OPTIMIZED = 1;
+  static final int VERSION_CHECKSUM = 2;
+  static final int VERSION_CURRENT = VERSION_CHECKSUM;
+  static final byte NUMERIC = 0;
+  static final byte BINARY = 1;
+  static final byte SORTED = 2;
+  static final byte SORTED_SET = 3;
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java
new file mode 100644
index 0000000..7dedb33
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java
@@ -0,0 +1,909 @@
+package org.apache.lucene.codecs.lucene45;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.BINARY_FIXED_UNCOMPRESSED;
+import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.BINARY_PREFIX_COMPRESSED;
+import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.BINARY_VARIABLE_UNCOMPRESSED;
+import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.DELTA_COMPRESSED;
+import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.GCD_COMPRESSED;
+import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.SORTED_SET_SINGLE_VALUED_SORTED;
+import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.SORTED_SET_WITH_ADDRESSES;
+import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.TABLE_COMPRESSED;
+import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat.VERSION_SORTED_SET_SINGLE_VALUE_OPTIMIZED;
+
+import java.io.Closeable; // javadocs
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.RandomAccessOrds;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedNumericDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LongValues;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.Version;
+import org.apache.lucene.util.packed.BlockPackedReader;
+import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
+import org.apache.lucene.util.packed.PackedInts;
+
+/** reader for {@link Lucene45DocValuesFormat} */
+class Lucene45DocValuesProducer extends DocValuesProducer implements Closeable {
+  private final Map<Integer,NumericEntry> numerics;
+  private final Map<Integer,BinaryEntry> binaries;
+  private final Map<Integer,SortedSetEntry> sortedSets;
+  private final Map<Integer,NumericEntry> ords;
+  private final Map<Integer,NumericEntry> ordIndexes;
+  private final AtomicLong ramBytesUsed;
+  private final IndexInput data;
+  private final int maxDoc;
+  private final int version;
+  
+  // We need this for pre-4.9 indexes which recorded multiple fields' DocValues
+  // updates under the same generation, and therefore the passed FieldInfos may
+  // not include all the fields that are encoded in this generation. In that
+  // case, we are more lenient about the fields we read and the passed-in
+  // FieldInfos.
+  @Deprecated
+  private final boolean lenientFieldInfoCheck;
+
+  // memory-resident structures
+  private final Map<Integer,MonotonicBlockPackedReader> addressInstances = new HashMap<>();
+  private final Map<Integer,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<>();
+  
+  /** expert: instantiates a new reader */
+  @SuppressWarnings("deprecation")
+  protected Lucene45DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    Version ver = state.segmentInfo.getVersion();
+    lenientFieldInfoCheck = Version.LUCENE_4_9_0.onOrAfter(ver);
+    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+    // read in the entries from the metadata file.
+    ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
+    this.maxDoc = state.segmentInfo.getDocCount();
+    boolean success = false;
+    try {
+      version = CodecUtil.checkHeader(in, metaCodec, 
+                                      Lucene45DocValuesFormat.VERSION_START,
+                                      Lucene45DocValuesFormat.VERSION_CURRENT);
+      numerics = new HashMap<>();
+      ords = new HashMap<>();
+      ordIndexes = new HashMap<>();
+      binaries = new HashMap<>();
+      sortedSets = new HashMap<>();
+      readFields(in, state.fieldInfos);
+
+      if (version >= Lucene45DocValuesFormat.VERSION_CHECKSUM) {
+        CodecUtil.checkFooter(in);
+      } else {
+        CodecUtil.checkEOF(in);
+      }
+
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(in);
+      } else {
+        IOUtils.closeWhileHandlingException(in);
+      }
+    }
+
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+    this.data = state.directory.openInput(dataName, state.context);
+    success = false;
+    try {
+      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
+                                                 Lucene45DocValuesFormat.VERSION_START,
+                                                 Lucene45DocValuesFormat.VERSION_CURRENT);
+      if (version != version2) {
+        throw new CorruptIndexException("Format versions mismatch");
+      }
+      
+      if (version >= Lucene45DocValuesFormat.VERSION_CHECKSUM) {
+        // NOTE: data file is too costly to verify checksum against all the bytes on open,
+        // but for now we at least verify proper structure of the checksum footer: which looks
+        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+        // such as file truncation.
+        CodecUtil.retrieveChecksum(data);
+      }
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this.data);
+      }
+    }
+    
+    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
+  }
+
+  private void readSortedField(int fieldNumber, IndexInput meta, FieldInfos infos) throws IOException {
+    // sorted = binary + numeric
+    if (meta.readVInt() != fieldNumber) {
+      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    if (meta.readByte() != Lucene45DocValuesFormat.BINARY) {
+      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    BinaryEntry b = readBinaryEntry(meta);
+    binaries.put(fieldNumber, b);
+    
+    if (meta.readVInt() != fieldNumber) {
+      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    if (meta.readByte() != Lucene45DocValuesFormat.NUMERIC) {
+      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    NumericEntry n = readNumericEntry(meta);
+    ords.put(fieldNumber, n);
+  }
+
+  private void readSortedSetFieldWithAddresses(int fieldNumber, IndexInput meta, FieldInfos infos) throws IOException {
+    // sortedset = binary + numeric (addresses) + ordIndex
+    if (meta.readVInt() != fieldNumber) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    if (meta.readByte() != Lucene45DocValuesFormat.BINARY) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    BinaryEntry b = readBinaryEntry(meta);
+    binaries.put(fieldNumber, b);
+
+    if (meta.readVInt() != fieldNumber) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    if (meta.readByte() != Lucene45DocValuesFormat.NUMERIC) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    NumericEntry n1 = readNumericEntry(meta);
+    ords.put(fieldNumber, n1);
+
+    if (meta.readVInt() != fieldNumber) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    if (meta.readByte() != Lucene45DocValuesFormat.NUMERIC) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    NumericEntry n2 = readNumericEntry(meta);
+    ordIndexes.put(fieldNumber, n2);
+  }
+
+  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
+    int fieldNumber = meta.readVInt();
+    while (fieldNumber != -1) {
+      if ((lenientFieldInfoCheck && fieldNumber < 0) || (!lenientFieldInfoCheck && infos.fieldInfo(fieldNumber) == null)) {
+        // trickier to validate more: because we re-use for norms, because we use multiple entries
+        // for "composite" types like sortedset, etc.
+        throw new CorruptIndexException("Invalid field number: " + fieldNumber + " (resource=" + meta + ")");
+      }
+      byte type = meta.readByte();
+      if (type == Lucene45DocValuesFormat.NUMERIC) {
+        numerics.put(fieldNumber, readNumericEntry(meta));
+      } else if (type == Lucene45DocValuesFormat.BINARY) {
+        BinaryEntry b = readBinaryEntry(meta);
+        binaries.put(fieldNumber, b);
+      } else if (type == Lucene45DocValuesFormat.SORTED) {
+        readSortedField(fieldNumber, meta, infos);
+      } else if (type == Lucene45DocValuesFormat.SORTED_SET) {
+        SortedSetEntry ss = readSortedSetEntry(meta);
+        sortedSets.put(fieldNumber, ss);
+        if (ss.format == SORTED_SET_WITH_ADDRESSES) {
+          readSortedSetFieldWithAddresses(fieldNumber, meta, infos);
+        } else if (ss.format == SORTED_SET_SINGLE_VALUED_SORTED) {
+          if (meta.readVInt() != fieldNumber) {
+            throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+          }
+          if (meta.readByte() != Lucene45DocValuesFormat.SORTED) {
+            throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+          }
+          readSortedField(fieldNumber, meta, infos);
+        } else {
+          throw new AssertionError();
+        }
+      } else {
+        throw new CorruptIndexException("invalid type: " + type + ", resource=" + meta);
+      }
+      fieldNumber = meta.readVInt();
+    }
+  }
+  
+  static NumericEntry readNumericEntry(IndexInput meta) throws IOException {
+    NumericEntry entry = new NumericEntry();
+    entry.format = meta.readVInt();
+    entry.missingOffset = meta.readLong();
+    entry.packedIntsVersion = meta.readVInt();
+    entry.offset = meta.readLong();
+    entry.count = meta.readVLong();
+    entry.blockSize = meta.readVInt();
+    switch(entry.format) {
+      case GCD_COMPRESSED:
+        entry.minValue = meta.readLong();
+        entry.gcd = meta.readLong();
+        break;
+      case TABLE_COMPRESSED:
+        if (entry.count > Integer.MAX_VALUE) {
+          throw new CorruptIndexException("Cannot use TABLE_COMPRESSED with more than MAX_VALUE values, input=" + meta);
+        }
+        final int uniqueValues = meta.readVInt();
+        if (uniqueValues > 256) {
+          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, input=" + meta);
+        }
+        entry.table = new long[uniqueValues];
+        for (int i = 0; i < uniqueValues; ++i) {
+          entry.table[i] = meta.readLong();
+        }
+        break;
+      case DELTA_COMPRESSED:
+        break;
+      default:
+        throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
+    }
+    return entry;
+  }
+  
+  static BinaryEntry readBinaryEntry(IndexInput meta) throws IOException {
+    BinaryEntry entry = new BinaryEntry();
+    entry.format = meta.readVInt();
+    entry.missingOffset = meta.readLong();
+    entry.minLength = meta.readVInt();
+    entry.maxLength = meta.readVInt();
+    entry.count = meta.readVLong();
+    entry.offset = meta.readLong();
+    switch(entry.format) {
+      case BINARY_FIXED_UNCOMPRESSED:
+        break;
+      case BINARY_PREFIX_COMPRESSED:
+        entry.addressInterval = meta.readVInt();
+        entry.addressesOffset = meta.readLong();
+        entry.packedIntsVersion = meta.readVInt();
+        entry.blockSize = meta.readVInt();
+        break;
+      case BINARY_VARIABLE_UNCOMPRESSED:
+        entry.addressesOffset = meta.readLong();
+        entry.packedIntsVersion = meta.readVInt();
+        entry.blockSize = meta.readVInt();
+        break;
+      default:
+        throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
+    }
+    return entry;
+  }
+
+  SortedSetEntry readSortedSetEntry(IndexInput meta) throws IOException {
+    SortedSetEntry entry = new SortedSetEntry();
+    if (version >= VERSION_SORTED_SET_SINGLE_VALUE_OPTIMIZED) {
+      entry.format = meta.readVInt();
+    } else {
+      entry.format = SORTED_SET_WITH_ADDRESSES;
+    }
+    if (entry.format != SORTED_SET_SINGLE_VALUED_SORTED && entry.format != SORTED_SET_WITH_ADDRESSES) {
+      throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
+    }
+    return entry;
+  }
+
+  @Override
+  public NumericDocValues getNumeric(FieldInfo field) throws IOException {
+    NumericEntry entry = numerics.get(field.number);
+    return getNumeric(entry);
+  }
+  
+  @Override
+  public long ramBytesUsed() {
+    return ramBytesUsed.get();
+  }
+  
+  @Override
+  public void checkIntegrity() throws IOException {
+    if (version >= Lucene45DocValuesFormat.VERSION_CHECKSUM) {
+      CodecUtil.checksumEntireFile(data);
+    }
+  }
+
+  LongValues getNumeric(NumericEntry entry) throws IOException {
+    final IndexInput data = this.data.clone();
+    data.seek(entry.offset);
+
+    switch (entry.format) {
+      case DELTA_COMPRESSED:
+        final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
+        return reader;
+      case GCD_COMPRESSED:
+        final long min = entry.minValue;
+        final long mult = entry.gcd;
+        final BlockPackedReader quotientReader = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
+        return new LongValues() {
+          @Override
+          public long get(long id) {
+            return min + mult * quotientReader.get(id);
+          }
+        };
+      case TABLE_COMPRESSED:
+        final long table[] = entry.table;
+        final int bitsRequired = PackedInts.bitsRequired(table.length - 1);
+        final PackedInts.Reader ords = PackedInts.getDirectReaderNoHeader(data, PackedInts.Format.PACKED, entry.packedIntsVersion, (int) entry.count, bitsRequired);
+        return new LongValues() {
+          @Override
+          public long get(long id) {
+            return table[(int) ords.get((int) id)];
+          }
+        };
+      default:
+        throw new AssertionError();
+    }
+  }
+
+  @Override
+  public BinaryDocValues getBinary(FieldInfo field) throws IOException {
+    BinaryEntry bytes = binaries.get(field.number);
+    switch(bytes.format) {
+      case BINARY_FIXED_UNCOMPRESSED:
+        return getFixedBinary(field, bytes);
+      case BINARY_VARIABLE_UNCOMPRESSED:
+        return getVariableBinary(field, bytes);
+      case BINARY_PREFIX_COMPRESSED:
+        return getCompressedBinary(field, bytes);
+      default:
+        throw new AssertionError();
+    }
+  }
+  
+  private BinaryDocValues getFixedBinary(FieldInfo field, final BinaryEntry bytes) {
+    final IndexInput data = this.data.clone();
+
+    return new LongBinaryDocValues() {
+      final BytesRef term;
+      {
+        term = new BytesRef(bytes.maxLength);
+        term.offset = 0;
+        term.length = bytes.maxLength;
+      }
+
+      @Override
+      public BytesRef get(long id) {
+        long address = bytes.offset + id * bytes.maxLength;
+        try {
+          data.seek(address);
+          data.readBytes(term.bytes, 0, term.length);
+          return term;
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    };
+  }
+  
+  /** returns an address instance for variable-length binary values.
+   *  @lucene.internal */
+  protected MonotonicBlockPackedReader getAddressInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
+    final MonotonicBlockPackedReader addresses;
+    synchronized (addressInstances) {
+      MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
+      if (addrInstance == null) {
+        data.seek(bytes.addressesOffset);
+        addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count, false);
+        addressInstances.put(field.number, addrInstance);
+        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
+      addresses = addrInstance;
+    }
+    return addresses;
+  }
+  
+  private BinaryDocValues getVariableBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
+    final IndexInput data = this.data.clone();
+    
+    final MonotonicBlockPackedReader addresses = getAddressInstance(data, field, bytes);
+
+    return new LongBinaryDocValues() {
+      final BytesRef term = new BytesRef(Math.max(0, bytes.maxLength));
+
+      @Override
+      public BytesRef get(long id) {
+        long startAddress = bytes.offset + (id == 0 ? 0 : addresses.get(id-1));
+        long endAddress = bytes.offset + addresses.get(id);
+        int length = (int) (endAddress - startAddress);
+        try {
+          data.seek(startAddress);
+          data.readBytes(term.bytes, 0, length);
+          term.length = length;
+          return term;
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    };
+  }
+  
+  /** returns an address instance for prefix-compressed binary values. 
+   * @lucene.internal */
+  protected MonotonicBlockPackedReader getIntervalInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
+    final MonotonicBlockPackedReader addresses;
+    final long interval = bytes.addressInterval;
+    synchronized (addressInstances) {
+      MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
+      if (addrInstance == null) {
+        data.seek(bytes.addressesOffset);
+        final long size;
+        if (bytes.count % interval == 0) {
+          size = bytes.count / interval;
+        } else {
+          size = 1L + bytes.count / interval;
+        }
+        addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
+        addressInstances.put(field.number, addrInstance);
+        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
+      addresses = addrInstance;
+    }
+    return addresses;
+  }
+
+
+  private BinaryDocValues getCompressedBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
+    final IndexInput data = this.data.clone();
+
+    final MonotonicBlockPackedReader addresses = getIntervalInstance(data, field, bytes);
+    
+    return new CompressedBinaryDocValues(bytes, addresses, data);
+  }
+
+  @Override
+  public SortedDocValues getSorted(FieldInfo field) throws IOException {
+    final int valueCount = (int) binaries.get(field.number).count;
+    final BinaryDocValues binary = getBinary(field);
+    NumericEntry entry = ords.get(field.number);
+    IndexInput data = this.data.clone();
+    data.seek(entry.offset);
+    final BlockPackedReader ordinals = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
+    
+    return new SortedDocValues() {
+
+      @Override
+      public int getOrd(int docID) {
+        return (int) ordinals.get(docID);
+      }
+
+      @Override
+      public BytesRef lookupOrd(int ord) {
+        return binary.get(ord);
+      }
+
+      @Override
+      public int getValueCount() {
+        return valueCount;
+      }
+
+      @Override
+      public int lookupTerm(BytesRef key) {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return (int) ((CompressedBinaryDocValues)binary).lookupTerm(key);
+        } else {
+        return super.lookupTerm(key);
+        }
+      }
+
+      @Override
+      public TermsEnum termsEnum() {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return ((CompressedBinaryDocValues)binary).getTermsEnum();
+        } else {
+          return super.termsEnum();
+        }
+      }
+    };
+  }
+  
+  /** returns an address instance for sortedset ordinal lists
+   * @lucene.internal */
+  protected MonotonicBlockPackedReader getOrdIndexInstance(IndexInput data, FieldInfo field, NumericEntry entry) throws IOException {
+    final MonotonicBlockPackedReader ordIndex;
+    synchronized (ordIndexInstances) {
+      MonotonicBlockPackedReader ordIndexInstance = ordIndexInstances.get(field.number);
+      if (ordIndexInstance == null) {
+        data.seek(entry.offset);
+        ordIndexInstance = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, entry.count, false);
+        ordIndexInstances.put(field.number, ordIndexInstance);
+        ramBytesUsed.addAndGet(ordIndexInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
+      ordIndex = ordIndexInstance;
+    }
+    return ordIndex;
+  }
+  
+  @Override
+  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
+    throw new IllegalStateException("Lucene 4.5 does not support SortedNumeric: how did you pull this off?");
+  }
+
+  @Override
+  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
+    SortedSetEntry ss = sortedSets.get(field.number);
+    if (ss.format == SORTED_SET_SINGLE_VALUED_SORTED) {
+      final SortedDocValues values = getSorted(field);
+      return DocValues.singleton(values);
+    } else if (ss.format != SORTED_SET_WITH_ADDRESSES) {
+      throw new AssertionError();
+    }
+
+    final IndexInput data = this.data.clone();
+    final long valueCount = binaries.get(field.number).count;
+    // we keep the byte[]s and list of ords on disk, these could be large
+    final LongBinaryDocValues binary = (LongBinaryDocValues) getBinary(field);
+    final LongValues ordinals = getNumeric(ords.get(field.number));
+    // but the addresses to the ord stream are in RAM
+    final MonotonicBlockPackedReader ordIndex = getOrdIndexInstance(data, field, ordIndexes.get(field.number));
+    
+    return new RandomAccessOrds() {
+      long startOffset;
+      long offset;
+      long endOffset;
+      
+      @Override
+      public long nextOrd() {
+        if (offset == endOffset) {
+          return NO_MORE_ORDS;
+        } else {
+          long ord = ordinals.get(offset);
+          offset++;
+          return ord;
+        }
+      }
+
+      @Override
+      public void setDocument(int docID) {
+        startOffset = offset = (docID == 0 ? 0 : ordIndex.get(docID-1));
+        endOffset = ordIndex.get(docID);
+      }
+
+      @Override
+      public BytesRef lookupOrd(long ord) {
+        return binary.get(ord);
+      }
+
+      @Override
+      public long getValueCount() {
+        return valueCount;
+      }
+      
+      @Override
+      public long lookupTerm(BytesRef key) {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return ((CompressedBinaryDocValues)binary).lookupTerm(key);
+        } else {
+          return super.lookupTerm(key);
+        }
+      }
+
+      @Override
+      public TermsEnum termsEnum() {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return ((CompressedBinaryDocValues)binary).getTermsEnum();
+        } else {
+          return super.termsEnum();
+        }
+      }
+
+      @Override
+      public long ordAt(int index) {
+        return ordinals.get(startOffset + index);
+      }
+
+      @Override
+      public int cardinality() {
+        return (int) (endOffset - startOffset);
+      }
+    };
+  }
+  
+  private Bits getMissingBits(final long offset) throws IOException {
+    if (offset == -1) {
+      return new Bits.MatchAllBits(maxDoc);
+    } else {
+      final IndexInput in = data.clone();
+      return new Bits() {
+
+        @Override
+        public boolean get(int index) {
+          try {
+            in.seek(offset + (index >> 3));
+            return (in.readByte() & (1 << (index & 7))) != 0;
+          } catch (IOException e) {
+            throw new RuntimeException(e);
+          }
+        }
+
+        @Override
+        public int length() {
+          return maxDoc;
+        }
+      };
+    }
+  }
+
+  @Override
+  public Bits getDocsWithField(FieldInfo field) throws IOException {
+    switch(field.getDocValuesType()) {
+      case SORTED_SET:
+        return DocValues.docsWithValue(getSortedSet(field), maxDoc);
+      case SORTED:
+        return DocValues.docsWithValue(getSorted(field), maxDoc);
+      case BINARY:
+        BinaryEntry be = binaries.get(field.number);
+        return getMissingBits(be.missingOffset);
+      case NUMERIC:
+        NumericEntry ne = numerics.get(field.number);
+        return getMissingBits(ne.missingOffset);
+      default:
+        throw new AssertionError();
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    data.close();
+  }
+  
+  /** metadata entry for a numeric docvalues field */
+  protected static class NumericEntry {
+    private NumericEntry() {}
+    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
+    long missingOffset;
+    /** offset to the actual numeric values */
+    public long offset;
+
+    int format;
+    /** packed ints version used to encode these numerics */
+    public int packedIntsVersion;
+    /** count of values written */
+    public long count;
+    /** packed ints blocksize */
+    public int blockSize;
+    
+    long minValue;
+    long gcd;
+    long table[];
+  }
+  
+  /** metadata entry for a binary docvalues field */
+  protected static class BinaryEntry {
+    private BinaryEntry() {}
+    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
+    long missingOffset;
+    /** offset to the actual binary values */
+    long offset;
+
+    int format;
+    /** count of values written */
+    public long count;
+    int minLength;
+    int maxLength;
+    /** offset to the addressing data that maps a value to its slice of the byte[] */
+    public long addressesOffset;
+    /** interval of shared prefix chunks (when using prefix-compressed binary) */
+    public long addressInterval;
+    /** packed ints version used to encode addressing information */
+    public int packedIntsVersion;
+    /** packed ints blocksize */
+    public int blockSize;
+  }
+
+  /** metadata entry for a sorted-set docvalues field */
+  protected static class SortedSetEntry {
+    private SortedSetEntry() {}
+    int format;
+  }
+
+  // internally we compose complex dv (sorted/sortedset) from other ones
+  static abstract class LongBinaryDocValues extends BinaryDocValues {
+    @Override
+    public final BytesRef get(int docID) {
+      return get((long) docID);
+    }
+    
+    abstract BytesRef get(long id);
+  }
+  
+  // in the compressed case, we add a few additional operations for
+  // more efficient reverse lookup and enumeration
+  static class CompressedBinaryDocValues extends LongBinaryDocValues {
+    final BinaryEntry bytes;
+    final long interval;
+    final long numValues;
+    final long numIndexValues;
+    final MonotonicBlockPackedReader addresses;
+    final IndexInput data;
+    final TermsEnum termsEnum;
+    
+    public CompressedBinaryDocValues(BinaryEntry bytes, MonotonicBlockPackedReader addresses, IndexInput data) throws IOException {
+      this.bytes = bytes;
+      this.interval = bytes.addressInterval;
+      this.addresses = addresses;
+      this.data = data;
+      this.numValues = bytes.count;
+      this.numIndexValues = addresses.size();
+      this.termsEnum = getTermsEnum(data);
+    }
+    
+    @Override
+    public BytesRef get(long id) {
+      try {
+        termsEnum.seekExact(id);
+        return termsEnum.term();
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    
+    long lookupTerm(BytesRef key) {
+      try {
+        SeekStatus status = termsEnum.seekCeil(key);
+        if (status == SeekStatus.END) {
+          return -numValues-1;
+        } else if (status == SeekStatus.FOUND) {
+          return termsEnum.ord();
+        } else {
+          return -termsEnum.ord()-1;
+        }
+      } catch (IOException bogus) {
+        throw new RuntimeException(bogus);
+      }
+    }
+    
+    TermsEnum getTermsEnum() {
+      try {
+        return getTermsEnum(data.clone());
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    
+    private TermsEnum getTermsEnum(final IndexInput input) throws IOException {
+      input.seek(bytes.offset);
+      
+      return new TermsEnum() {
+        private long currentOrd = -1;
+        // TODO: maxLength is negative when all terms are merged away...
+        private final BytesRef term = new BytesRef(bytes.maxLength < 0 ? 0 : bytes.maxLength);
+
+        @Override
+        public BytesRef next() throws IOException {
+          if (++currentOrd >= numValues) {
+            return null;
+          } else {
+            int start = input.readVInt();
+            int suffix = input.readVInt();
+            input.readBytes(term.bytes, start, suffix);
+            term.length = start + suffix;
+            return term;
+          }
+        }
+
+        @Override
+        public SeekStatus seekCeil(BytesRef text) throws IOException {
+          // binary-search just the index values to find the block,
+          // then scan within the block
+          long low = 0;
+          long high = numIndexValues-1;
+
+          while (low <= high) {
+            long mid = (low + high) >>> 1;
+            seekExact(mid * interval);
+            int cmp = term.compareTo(text);
+
+            if (cmp < 0) {
+              low = mid + 1;
+            } else if (cmp > 0) {
+              high = mid - 1;
+            } else {
+              // we got lucky, found an indexed term
+              return SeekStatus.FOUND;
+            }
+          }
+          
+          if (numIndexValues == 0) {
+            return SeekStatus.END;
+          }
+          
+          // block before insertion point
+          long block = low-1;
+          seekExact(block < 0 ? -1 : block * interval);
+          
+          while (next() != null) {
+            int cmp = term.compareTo(text);
+            if (cmp == 0) {
+              return SeekStatus.FOUND;
+            } else if (cmp > 0) {
+              return SeekStatus.NOT_FOUND;
+            }
+          }
+          
+          return SeekStatus.END;
+        }
+
+        @Override
+        public void seekExact(long ord) throws IOException {
+          long block = ord / interval;
+
+          if (ord >= currentOrd && block == currentOrd / interval) {
+            // seek within current block
+          } else {
+            // position before start of block
+            currentOrd = ord - ord % interval - 1;
+            input.seek(bytes.offset + addresses.get(block));
+          }
+          
+          while (currentOrd < ord) {
+            next();
+          }
+        }
+
+        @Override
+        public BytesRef term() throws IOException {
+          return term;
+        }
+
+        @Override
+        public long ord() throws IOException {
+          return currentOrd;
+        }
+
+        @Override
+        public int docFreq() throws IOException {
+          throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public long totalTermFreq() throws IOException {
+          return -1;
+        }
+
+        @Override
+        public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+          throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+          throw new UnsupportedOperationException();
+        }
+      };
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/package.html b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/package.html
new file mode 100644
index 0000000..890ca6c
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene45/package.html
@@ -0,0 +1,396 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Lucene 4.5 file format.
+
+<h1>Apache Lucene - Index File Formats</h1>
+<div>
+<ul>
+<li><a href="#Introduction">Introduction</a></li>
+<li><a href="#Definitions">Definitions</a>
+  <ul>
+  <li><a href="#Inverted_Indexing">Inverted Indexing</a></li>
+  <li><a href="#Types_of_Fields">Types of Fields</a></li>
+  <li><a href="#Segments">Segments</a></li>
+  <li><a href="#Document_Numbers">Document Numbers</a></li>
+  </ul>
+</li>
+<li><a href="#Overview">Index Structure Overview</a></li>
+<li><a href="#File_Naming">File Naming</a></li>
+<li><a href="#file-names">Summary of File Extensions</a></li>
+  <ul>
+  <li><a href="#Lock_File">Lock File</a></li>
+  <li><a href="#History">History</a></li>
+  <li><a href="#Limitations">Limitations</a></li>
+  </ul>
+</ul>
+</div>
+<a name="Introduction"></a>
+<h2>Introduction</h2>
+<div>
+<p>This document defines the index file formats used in this version of Lucene.
+If you are using a different version of Lucene, please consult the copy of
+<code>docs/</code> that was distributed with
+the version you are using.</p>
+<p>Apache Lucene is written in Java, but several efforts are underway to write
+<a href="http://wiki.apache.org/lucene-java/LuceneImplementations">versions of
+Lucene in other programming languages</a>. If these versions are to remain
+compatible with Apache Lucene, then a language-independent definition of the
+Lucene index format is required. This document thus attempts to provide a
+complete and independent definition of the Apache Lucene file formats.</p>
+<p>As Lucene evolves, this document should evolve. Versions of Lucene in
+different programming languages should endeavor to agree on file formats, and
+generate new versions of this document.</p>
+</div>
+<a name="Definitions" id="Definitions"></a>
+<h2>Definitions</h2>
+<div>
+<p>The fundamental concepts in Lucene are index, document, field and term.</p>
+<p>An index contains a sequence of documents.</p>
+<ul>
+<li>A document is a sequence of fields.</li>
+<li>A field is a named sequence of terms.</li>
+<li>A term is a sequence of bytes.</li>
+</ul>
+<p>The same sequence of bytes in two different fields is considered a different 
+term. Thus terms are represented as a pair: the string naming the field, and the
+bytes within the field.</p>
+<a name="Inverted_Indexing"></a>
+<h3>Inverted Indexing</h3>
+<p>The index stores statistics about terms in order to make term-based search
+more efficient. Lucene's index falls into the family of indexes known as an
+<i>inverted index.</i> This is because it can list, for a term, the documents
+that contain it. This is the inverse of the natural relationship, in which
+documents list terms.</p>
+<a name="Types_of_Fields"></a>
+<h3>Types of Fields</h3>
+<p>In Lucene, fields may be <i>stored</i>, in which case their text is stored
+in the index literally, in a non-inverted manner. Fields that are inverted are
+called <i>indexed</i>. A field may be both stored and indexed.</p>
+<p>The text of a field may be <i>tokenized</i> into terms to be indexed, or the
+text of a field may be used literally as a term to be indexed. Most fields are
+tokenized, but sometimes it is useful for certain identifier fields to be
+indexed literally.</p>
+<p>See the {@link org.apache.lucene.document.Field Field}
+java docs for more information on Fields.</p>
+<a name="Segments" id="Segments"></a>
+<h3>Segments</h3>
+<p>Lucene indexes may be composed of multiple sub-indexes, or <i>segments</i>.
+Each segment is a fully independent index, which could be searched separately.
+Indexes evolve by:</p>
+<ol>
+<li>Creating new segments for newly added documents.</li>
+<li>Merging existing segments.</li>
+</ol>
+<p>Searches may involve multiple segments and/or multiple indexes, each index
+potentially composed of a set of segments.</p>
+<a name="Document_Numbers"></a>
+<h3>Document Numbers</h3>
+<p>Internally, Lucene refers to documents by an integer <i>document number</i>.
+The first document added to an index is numbered zero, and each subsequent
+document added gets a number one greater than the previous.</p>
+<p>Note that a document's number may change, so caution should be taken when
+storing these numbers outside of Lucene. In particular, numbers may change in
+the following situations:</p>
+<ul>
+<li>
+<p>The numbers stored in each segment are unique only within the segment, and
+must be converted before they can be used in a larger context. The standard
+technique is to allocate each segment a range of values, based on the range of
+numbers used in that segment. To convert a document number from a segment to an
+external value, the segment's <i>base</i> document number is added. To convert
+an external value back to a segment-specific value, the segment is identified
+by the range that the external value is in, and the segment's base value is
+subtracted. For example two five document segments might be combined, so that
+the first segment has a base value of zero, and the second of five. Document
+three from the second segment would have an external value of eight.</p>
+</li>
+<li>
+<p>When documents are deleted, gaps are created in the numbering. These are
+eventually removed as the index evolves through merging. Deleted documents are
+dropped when segments are merged. A freshly-merged segment thus has no gaps in
+its numbering.</p>
+</li>
+</ul>
+</div>
+<a name="Overview" id="Overview"></a>
+<h2>Index Structure Overview</h2>
+<div>
+<p>Each segment index maintains the following:</p>
+<ul>
+<li>
+{@link org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat Segment info}.
+   This contains metadata about a segment, such as the number of documents,
+   what files it uses, 
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene42.Lucene42FieldInfosFormat Field names}. 
+   This contains the set of field names used in the index.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Stored Field values}. 
+This contains, for each document, a list of attribute-value pairs, where the attributes 
+are field names. These are used to store auxiliary information about the document, such as 
+its title, url, or an identifier to access a database. The set of stored fields are what is 
+returned for each hit when searching. This is keyed by document number.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Term dictionary}. 
+A dictionary containing all of the terms used in all of the
+indexed fields of all of the documents. The dictionary also contains the number
+of documents which contain the term, and pointers to the term's frequency and
+proximity data.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Term Frequency data}. 
+For each term in the dictionary, the numbers of all the
+documents that contain that term, and the frequency of the term in that
+document, unless frequencies are omitted (IndexOptions.DOCS_ONLY)
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Term Proximity data}. 
+For each term in the dictionary, the positions that the
+term occurs in each document. Note that this will not exist if all fields in
+all documents omit position data.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene42.Lucene42NormsFormat Normalization factors}. 
+For each field in each document, a value is stored
+that is multiplied into the score for hits on that field.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vectors}. 
+For each field in each document, the term vector (sometimes
+called document vector) may be stored. A term vector consists of term text and
+term frequency. To add Term Vectors to your index see the 
+{@link org.apache.lucene.document.Field Field} constructors
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat Per-document values}. 
+Like stored values, these are also keyed by document
+number, but are generally intended to be loaded into main memory for fast
+access. Whereas stored values are generally intended for summary results from
+searches, per-document values are useful for things like scoring factors.
+</li>
+<li>
+{@link org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat Deleted documents}. 
+An optional file indicating which documents are deleted.
+</li>
+</ul>
+<p>Details on each of these are provided in their linked pages.</p>
+</div>
+<a name="File_Naming"></a>
+<h2>File Naming</h2>
+<div>
+<p>All files belonging to a segment have the same name with varying extensions.
+The extensions correspond to the different file formats described below. When
+using the Compound File format (default in 1.4 and greater) these files (except
+for the Segment info file, the Lock file, and Deleted documents file) are collapsed 
+into a single .cfs file (see below for details)</p>
+<p>Typically, all segments in an index are stored in a single directory,
+although this is not required.</p>
+<p>As of version 2.1 (lock-less commits), file names are never re-used (there
+is one exception, "segments.gen", see below). That is, when any file is saved
+to the Directory it is given a never before used filename. This is achieved
+using a simple generations approach. For example, the first segments file is
+segments_1, then segments_2, etc. The generation is a sequential long integer
+represented in alpha-numeric (base 36) form.</p>
+</div>
+<a name="file-names" id="file-names"></a>
+<h2>Summary of File Extensions</h2>
+<div>
+<p>The following table summarizes the names and extensions of the files in
+Lucene:</p>
+<table cellspacing="1" cellpadding="4">
+<tr>
+<th>Name</th>
+<th>Extension</th>
+<th>Brief Description</th>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.index.SegmentInfos Segments File}</td>
+<td>segments.gen, segments_N</td>
+<td>Stores information about a commit point</td>
+</tr>
+<tr>
+<td><a href="#Lock_File">Lock File</a></td>
+<td>write.lock</td>
+<td>The Write lock prevents multiple IndexWriters from writing to the same
+file.</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat Segment Info}</td>
+<td>.si</td>
+<td>Stores metadata about a segment</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.store.CompoundFileDirectory Compound File}</td>
+<td>.cfs, .cfe</td>
+<td>An optional "virtual" file consisting of all the other index files for
+systems that frequently run out of file handles.</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene42.Lucene42FieldInfosFormat Fields}</td>
+<td>.fnm</td>
+<td>Stores information about the fields</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Field Index}</td>
+<td>.fdx</td>
+<td>Contains pointers to field data</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Field Data}</td>
+<td>.fdt</td>
+<td>The stored fields for documents</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Term Dictionary}</td>
+<td>.tim</td>
+<td>The term dictionary, stores term info</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Term Index}</td>
+<td>.tip</td>
+<td>The index into the Term Dictionary</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Frequencies}</td>
+<td>.doc</td>
+<td>Contains the list of docs which contain each term along with frequency</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Positions}</td>
+<td>.pos</td>
+<td>Stores position information about where a term occurs in the index</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Payloads}</td>
+<td>.pay</td>
+<td>Stores additional per-position metadata information such as character offsets and user payloads</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene42.Lucene42NormsFormat Norms}</td>
+<td>.nvd, .nvm</td>
+<td>Encodes length and boost factors for docs and fields</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat Per-Document Values}</td>
+<td>.dvd, .dvm</td>
+<td>Encodes additional scoring factors or other per-document information.</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Index}</td>
+<td>.tvx</td>
+<td>Stores offset into the document data file</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Documents}</td>
+<td>.tvd</td>
+<td>Contains information about each document that has term vectors</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Fields}</td>
+<td>.tvf</td>
+<td>The field level info about term vectors</td>
+</tr>
+<tr>
+<td>{@link org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat Deleted Documents}</td>
+<td>.del</td>
+<td>Info about what files are deleted</td>
+</tr>
+</table>
+</div>
+<a name="Lock_File" id="Lock_File"></a>
+<h2>Lock File</h2>
+The write lock, which is stored in the index directory by default, is named
+"write.lock". If the lock directory is different from the index directory then
+the write lock will be named "XXXX-write.lock" where XXXX is a unique prefix
+derived from the full path to the index directory. When this file is present, a
+writer is currently modifying the index (adding or removing documents). This
+lock file ensures that only one writer is modifying the index at a time.</p>
+<a name="History"></a>
+<h2>History</h2>
+<p>Compatibility notes are provided in this document, describing how file
+formats have changed from prior versions:</p>
+<ul>
+<li>In version 2.1, the file format was changed to allow lock-less commits (ie,
+no more commit lock). The change is fully backwards compatible: you can open a
+pre-2.1 index for searching or adding/deleting of docs. When the new segments
+file is saved (committed), it will be written in the new file format (meaning
+no specific "upgrade" process is needed). But note that once a commit has
+occurred, pre-2.1 Lucene will not be able to read the index.</li>
+<li>In version 2.3, the file format was changed to allow segments to share a
+single set of doc store (vectors &amp; stored fields) files. This allows for
+faster indexing in certain cases. The change is fully backwards compatible (in
+the same way as the lock-less commits change in 2.1).</li>
+<li>In version 2.4, Strings are now written as true UTF-8 byte sequence, not
+Java's modified UTF-8. See <a href="http://issues.apache.org/jira/browse/LUCENE-510">
+LUCENE-510</a> for details.</li>
+<li>In version 2.9, an optional opaque Map&lt;String,String&gt; CommitUserData
+may be passed to IndexWriter's commit methods (and later retrieved), which is
+recorded in the segments_N file. See <a href="http://issues.apache.org/jira/browse/LUCENE-1382">
+LUCENE-1382</a> for details. Also,
+diagnostics were added to each segment written recording details about why it
+was written (due to flush, merge; which OS/JRE was used; etc.). See issue
+<a href="http://issues.apache.org/jira/browse/LUCENE-1654">LUCENE-1654</a> for details.</li>
+<li>In version 3.0, compressed fields are no longer written to the index (they
+can still be read, but on merge the new segment will write them, uncompressed).
+See issue <a href="http://issues.apache.org/jira/browse/LUCENE-1960">LUCENE-1960</a> 
+for details.</li>
+<li>In version 3.1, segments records the code version that created them. See
+<a href="http://issues.apache.org/jira/browse/LUCENE-2720">LUCENE-2720</a> for details. 
+Additionally segments track explicitly whether or not they have term vectors. 
+See <a href="http://issues.apache.org/jira/browse/LUCENE-2811">LUCENE-2811</a> 
+for details.</li>
+<li>In version 3.2, numeric fields are written as natively to stored fields
+file, previously they were stored in text format only.</li>
+<li>In version 3.4, fields can omit position data while still indexing term
+frequencies.</li>
+<li>In version 4.0, the format of the inverted index became extensible via
+the {@link org.apache.lucene.codecs.Codec Codec} api. Fast per-document storage
+({@code DocValues}) was introduced. Normalization factors need no longer be a 
+single byte, they can be any {@link org.apache.lucene.index.NumericDocValues NumericDocValues}. 
+Terms need not be unicode strings, they can be any byte sequence. Term offsets 
+can optionally be indexed into the postings lists. Payloads can be stored in the 
+term vectors.</li>
+<li>In version 4.1, the format of the postings list changed to use either
+of FOR compression or variable-byte encoding, depending upon the frequency
+of the term. Terms appearing only once were changed to inline directly into
+the term dictionary. Stored fields are compressed by default. </li>
+<li>In version 4.2, term vectors are compressed by default. DocValues has 
+a new multi-valued type (SortedSet), that can be used for faceting/grouping/joining
+on multi-valued fields.</li>
+<li>In version 4.5, DocValues were extended to explicitly represent missing values.</li>
+</ul>
+<a name="Limitations" id="Limitations"></a>
+<h2>Limitations</h2>
+<div>
+<p>Lucene uses a Java <code>int</code> to refer to
+document numbers, and the index file format uses an <code>Int32</code>
+on-disk to store document numbers. This is a limitation
+of both the index file format and the current implementation. Eventually these
+should be replaced with either <code>UInt64</code> values, or
+better yet, {@link org.apache.lucene.store.DataOutput#writeVInt VInt} values which have no limit.</p>
+</div>
+</body>
+</html>
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java
new file mode 100755
index 0000000..833c16b
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java
@@ -0,0 +1,149 @@
+package org.apache.lucene.codecs.lucene46;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
+import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42NormsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
+import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * Implements the Lucene 4.6 index format, with configurable per-field postings
+ * and docvalues formats.
+ * <p>
+ * If you want to reuse functionality of this codec in another codec, extend
+ * {@link FilterCodec}.
+ *
+ * @see org.apache.lucene.codecs.lucene46 package documentation for file format details.
+ * @lucene.experimental
+ * @deprecated Only for reading old 4.6-4.8 segments
+ */
+// NOTE: if we make largish changes in a minor release, easier to just make Lucene46Codec or whatever
+// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
+// (it writes a minor version, etc).
+@Deprecated
+public class Lucene46Codec extends Codec {
+  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
+  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
+  private final FieldInfosFormat fieldInfosFormat = new Lucene46FieldInfosFormat();
+  private final SegmentInfoFormat segmentInfosFormat = new Lucene46SegmentInfoFormat();
+  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  
+  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
+    @Override
+    public PostingsFormat getPostingsFormatForField(String field) {
+      return Lucene46Codec.this.getPostingsFormatForField(field);
+    }
+  };
+  
+  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
+    @Override
+    public DocValuesFormat getDocValuesFormatForField(String field) {
+      return Lucene46Codec.this.getDocValuesFormatForField(field);
+    }
+  };
+
+  /** Sole constructor. */
+  public Lucene46Codec() {
+    super("Lucene46");
+  }
+  
+  @Override
+  public final StoredFieldsFormat storedFieldsFormat() {
+    return fieldsFormat;
+  }
+  
+  @Override
+  public final TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+
+  @Override
+  public final PostingsFormat postingsFormat() {
+    return postingsFormat;
+  }
+  
+  @Override
+  public final FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+  
+  @Override
+  public final SegmentInfoFormat segmentInfoFormat() {
+    return segmentInfosFormat;
+  }
+  
+  @Override
+  public final LiveDocsFormat liveDocsFormat() {
+    return liveDocsFormat;
+  }
+
+  /** Returns the postings format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene41"
+   */
+  public PostingsFormat getPostingsFormatForField(String field) {
+    return defaultFormat;
+  }
+  
+  /** Returns the docvalues format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene45"
+   */
+  public DocValuesFormat getDocValuesFormatForField(String field) {
+    return defaultDVFormat;
+  }
+  
+  @Override
+  public final DocValuesFormat docValuesFormat() {
+    return docValuesFormat;
+  }
+
+  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
+  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene45");
+
+  private final NormsFormat normsFormat = new Lucene42NormsFormat() {
+    @Override
+    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
+      throw new UnsupportedOperationException("this codec can only be used for reading");
+    }
+  };
+
+  @Override
+  public NormsFormat normsFormat() {
+    return normsFormat;
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java
new file mode 100644
index 0000000..cf18c203
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java
@@ -0,0 +1,148 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FilterCodec;
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
+import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
+import org.apache.lucene.codecs.lucene46.Lucene46FieldInfosFormat;
+import org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoFormat;
+import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * Implements the Lucene 4.9 index format, with configurable per-field postings
+ * and docvalues formats.
+ * <p>
+ * If you want to reuse functionality of this codec in another codec, extend
+ * {@link FilterCodec}.
+ *
+ * @see org.apache.lucene.codecs.lucene49 package documentation for file format details.
+ * @lucene.experimental
+ */
+// NOTE: if we make largish changes in a minor release, easier to just make Lucene410Codec or whatever
+// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
+// (it writes a minor version, etc).
+public class Lucene49Codec extends Codec {
+  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
+  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
+  private final FieldInfosFormat fieldInfosFormat = new Lucene46FieldInfosFormat();
+  private final SegmentInfoFormat segmentInfosFormat = new Lucene46SegmentInfoFormat();
+  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
+  
+  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
+    @Override
+    public PostingsFormat getPostingsFormatForField(String field) {
+      return Lucene49Codec.this.getPostingsFormatForField(field);
+    }
+  };
+  
+  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
+    @Override
+    public DocValuesFormat getDocValuesFormatForField(String field) {
+      return Lucene49Codec.this.getDocValuesFormatForField(field);
+    }
+  };
+
+  /** Sole constructor. */
+  public Lucene49Codec() {
+    super("Lucene49");
+  }
+  
+  @Override
+  public final StoredFieldsFormat storedFieldsFormat() {
+    return fieldsFormat;
+  }
+  
+  @Override
+  public final TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+
+  @Override
+  public final PostingsFormat postingsFormat() {
+    return postingsFormat;
+  }
+  
+  @Override
+  public final FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+  
+  @Override
+  public final SegmentInfoFormat segmentInfoFormat() {
+    return segmentInfosFormat;
+  }
+  
+  @Override
+  public final LiveDocsFormat liveDocsFormat() {
+    return liveDocsFormat;
+  }
+
+  /** Returns the postings format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene41"
+   */
+  public PostingsFormat getPostingsFormatForField(String field) {
+    return defaultFormat;
+  }
+  
+  /** Returns the docvalues format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene49"
+   */
+  public DocValuesFormat getDocValuesFormatForField(String field) {
+    return defaultDVFormat;
+  }
+  
+  @Override
+  public final DocValuesFormat docValuesFormat() {
+    return docValuesFormat;
+  }
+
+  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
+  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene49");
+
+  private final NormsFormat normsFormat = new Lucene49NormsFormat() {
+    @Override
+    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
+      throw new UnsupportedOperationException("this codec can only be used for reading");
+    }
+  };
+
+  @Override
+  public NormsFormat normsFormat() {
+   return normsFormat;
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java
new file mode 100644
index 0000000..416f070
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java
@@ -0,0 +1,471 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable; // javadocs
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.MathUtil;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.packed.DirectWriter;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts;
+
+/** writer for {@link Lucene49DocValuesFormat} */
+class Lucene49DocValuesConsumer extends DocValuesConsumer implements Closeable {
+
+  static final int BLOCK_SIZE = 16384;
+  static final int ADDRESS_INTERVAL = 16;
+
+  /** Compressed using packed blocks of ints. */
+  public static final int DELTA_COMPRESSED = 0;
+  /** Compressed by computing the GCD. */
+  public static final int GCD_COMPRESSED = 1;
+  /** Compressed by giving IDs to unique values. */
+  public static final int TABLE_COMPRESSED = 2;
+  /** Compressed with monotonically increasing values */
+  public static final int MONOTONIC_COMPRESSED = 3;
+  
+  /** Uncompressed binary, written directly (fixed length). */
+  public static final int BINARY_FIXED_UNCOMPRESSED = 0;
+  /** Uncompressed binary, written directly (variable length). */
+  public static final int BINARY_VARIABLE_UNCOMPRESSED = 1;
+  /** Compressed binary with shared prefixes */
+  public static final int BINARY_PREFIX_COMPRESSED = 2;
+
+  /** Standard storage for sorted set values with 1 level of indirection:
+   *  docId -> address -> ord. */
+  public static final int SORTED_WITH_ADDRESSES = 0;
+  /** Single-valued sorted set values, encoded as sorted values, so no level
+   *  of indirection: docId -> ord. */
+  public static final int SORTED_SINGLE_VALUED = 1;
+
+  IndexOutput data, meta;
+  final int maxDoc;
+  
+  /** expert: Creates a new writer */
+  public Lucene49DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    boolean success = false;
+    try {
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+      data = state.directory.createOutput(dataName, state.context);
+      CodecUtil.writeHeader(data, dataCodec, Lucene49DocValuesFormat.VERSION_CURRENT);
+      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+      meta = state.directory.createOutput(metaName, state.context);
+      CodecUtil.writeHeader(meta, metaCodec, Lucene49DocValuesFormat.VERSION_CURRENT);
+      maxDoc = state.segmentInfo.getDocCount();
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this);
+      }
+    }
+  }
+  
+  @Override
+  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+    checkCanWrite(field);
+    addNumericField(field, values, true);
+  }
+
+  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {
+    long count = 0;
+    long minValue = Long.MAX_VALUE;
+    long maxValue = Long.MIN_VALUE;
+    long gcd = 0;
+    boolean missing = false;
+    // TODO: more efficient?
+    HashSet<Long> uniqueValues = null;
+    if (optimizeStorage) {
+      uniqueValues = new HashSet<>();
+
+      for (Number nv : values) {
+        final long v;
+        if (nv == null) {
+          v = 0;
+          missing = true;
+        } else {
+          v = nv.longValue();
+        }
+
+        if (gcd != 1) {
+          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
+            // in that case v - minValue might overflow and make the GCD computation return
+            // wrong results. Since these extreme values are unlikely, we just discard
+            // GCD computation for them
+            gcd = 1;
+          } else if (count != 0) { // minValue needs to be set first
+            gcd = MathUtil.gcd(gcd, v - minValue);
+          }
+        }
+
+        minValue = Math.min(minValue, v);
+        maxValue = Math.max(maxValue, v);
+
+        if (uniqueValues != null) {
+          if (uniqueValues.add(v)) {
+            if (uniqueValues.size() > 256) {
+              uniqueValues = null;
+            }
+          }
+        }
+
+        ++count;
+      }
+    } else {
+      for (Number nv : values) {
+        long v = nv.longValue();
+        minValue = Math.min(minValue, v);
+        maxValue = Math.max(maxValue, v);
+        ++count;
+      }
+    }
+    
+    final long delta = maxValue - minValue;
+    final int deltaBitsRequired = DirectWriter.unsignedBitsRequired(delta);
+    final int tableBitsRequired = uniqueValues == null
+        ? Integer.MAX_VALUE
+        : DirectWriter.bitsRequired(uniqueValues.size() - 1);
+
+    final int format;
+    if (uniqueValues != null && tableBitsRequired < deltaBitsRequired) {
+      format = TABLE_COMPRESSED;
+    } else if (gcd != 0 && gcd != 1) {
+      final long gcdDelta = (maxValue - minValue) / gcd;
+      final long gcdBitsRequired = DirectWriter.unsignedBitsRequired(gcdDelta);
+      format = gcdBitsRequired < deltaBitsRequired ? GCD_COMPRESSED : DELTA_COMPRESSED;
+    } else {
+      format = DELTA_COMPRESSED;
+    }
+    meta.writeVInt(field.number);
+    meta.writeByte(Lucene49DocValuesFormat.NUMERIC);
+    meta.writeVInt(format);
+    if (missing) {
+      meta.writeLong(data.getFilePointer());
+      writeMissingBitset(values);
+    } else {
+      meta.writeLong(-1L);
+    }
+    meta.writeLong(data.getFilePointer());
+    meta.writeVLong(count);
+
+    switch (format) {
+      case GCD_COMPRESSED:
+        meta.writeLong(minValue);
+        meta.writeLong(gcd);
+        final long maxDelta = (maxValue - minValue) / gcd;
+        final int bits = DirectWriter.unsignedBitsRequired(maxDelta);
+        meta.writeVInt(bits);
+        final DirectWriter quotientWriter = DirectWriter.getInstance(data, count, bits);
+        for (Number nv : values) {
+          long value = nv == null ? 0 : nv.longValue();
+          quotientWriter.add((value - minValue) / gcd);
+        }
+        quotientWriter.finish();
+        break;
+      case DELTA_COMPRESSED:
+        final long minDelta = delta < 0 ? 0 : minValue;
+        meta.writeLong(minDelta);
+        meta.writeVInt(deltaBitsRequired);
+        final DirectWriter writer = DirectWriter.getInstance(data, count, deltaBitsRequired);
+        for (Number nv : values) {
+          long v = nv == null ? 0 : nv.longValue();
+          writer.add(v - minDelta);
+        }
+        writer.finish();
+        break;
+      case TABLE_COMPRESSED:
+        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
+        Arrays.sort(decode);
+        final HashMap<Long,Integer> encode = new HashMap<>();
+        meta.writeVInt(decode.length);
+        for (int i = 0; i < decode.length; i++) {
+          meta.writeLong(decode[i]);
+          encode.put(decode[i], i);
+        }
+        meta.writeVInt(tableBitsRequired);
+        final DirectWriter ordsWriter = DirectWriter.getInstance(data, count, tableBitsRequired);
+        for (Number nv : values) {
+          ordsWriter.add(encode.get(nv == null ? 0 : nv.longValue()));
+        }
+        ordsWriter.finish();
+        break;
+      default:
+        throw new AssertionError();
+    }
+    meta.writeLong(data.getFilePointer());
+  }
+  
+  // TODO: in some cases representing missing with minValue-1 wouldn't take up additional space and so on,
+  // but this is very simple, and algorithms only check this for values of 0 anyway (doesnt slow down normal decode)
+  void writeMissingBitset(Iterable<?> values) throws IOException {
+    byte bits = 0;
+    int count = 0;
+    for (Object v : values) {
+      if (count == 8) {
+        data.writeByte(bits);
+        count = 0;
+        bits = 0;
+      }
+      if (v != null) {
+        bits |= 1 << (count & 7);
+      }
+      count++;
+    }
+    if (count > 0) {
+      data.writeByte(bits);
+    }
+  }
+
+  @Override
+  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
+    checkCanWrite(field);
+    // write the byte[] data
+    meta.writeVInt(field.number);
+    meta.writeByte(Lucene49DocValuesFormat.BINARY);
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    final long startFP = data.getFilePointer();
+    long count = 0;
+    boolean missing = false;
+    for(BytesRef v : values) {
+      final int length;
+      if (v == null) {
+        length = 0;
+        missing = true;
+      } else {
+        length = v.length;
+      }
+      minLength = Math.min(minLength, length);
+      maxLength = Math.max(maxLength, length);
+      if (v != null) {
+        data.writeBytes(v.bytes, v.offset, v.length);
+      }
+      count++;
+    }
+    meta.writeVInt(minLength == maxLength ? BINARY_FIXED_UNCOMPRESSED : BINARY_VARIABLE_UNCOMPRESSED);
+    if (missing) {
+      meta.writeLong(data.getFilePointer());
+      writeMissingBitset(values);
+    } else {
+      meta.writeLong(-1L);
+    }
+    meta.writeVInt(minLength);
+    meta.writeVInt(maxLength);
+    meta.writeVLong(count);
+    meta.writeLong(startFP);
+    
+    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
+    // otherwise, we need to record the length fields...
+    if (minLength != maxLength) {
+      meta.writeLong(data.getFilePointer());
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      meta.writeVInt(BLOCK_SIZE);
+
+      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
+      long addr = 0;
+      writer.add(addr);
+      for (BytesRef v : values) {
+        if (v != null) {
+          addr += v.length;
+        }
+        writer.add(addr);
+      }
+      writer.finish();
+    }
+  }
+  
+  /** expert: writes a value dictionary for a sorted/sortedset field */
+  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
+    // first check if its a "fixed-length" terms dict
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    for (BytesRef v : values) {
+      minLength = Math.min(minLength, v.length);
+      maxLength = Math.max(maxLength, v.length);
+    }
+    if (minLength == maxLength) {
+      // no index needed: direct addressing by mult
+      addBinaryField(field, values);
+    } else {
+      // header
+      meta.writeVInt(field.number);
+      meta.writeByte(Lucene49DocValuesFormat.BINARY);
+      meta.writeVInt(BINARY_PREFIX_COMPRESSED);
+      meta.writeLong(-1L);
+      // now write the bytes: sharing prefixes within a block
+      final long startFP = data.getFilePointer();
+      // currently, we have to store the delta from expected for every 1/nth term
+      // we could avoid this, but its not much and less overall RAM than the previous approach!
+      RAMOutputStream addressBuffer = new RAMOutputStream();
+      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);
+      BytesRefBuilder lastTerm = new BytesRefBuilder();
+      lastTerm.grow(Math.max(0, maxLength));
+      long count = 0;
+      for (BytesRef v : values) {
+        if (count % ADDRESS_INTERVAL == 0) {
+          termAddresses.add(data.getFilePointer() - startFP);
+          // force the first term in a block to be abs-encoded
+          lastTerm.clear();
+        }
+        
+        // prefix-code
+        int sharedPrefix = StringHelper.bytesDifference(lastTerm.get(), v);
+        data.writeVInt(sharedPrefix);
+        data.writeVInt(v.length - sharedPrefix);
+        data.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);
+        lastTerm.copyBytes(v);
+        count++;
+      }
+      final long indexStartFP = data.getFilePointer();
+      // write addresses of indexed terms
+      termAddresses.finish();
+      addressBuffer.writeTo(data);
+      addressBuffer = null;
+      termAddresses = null;
+      meta.writeVInt(minLength);
+      meta.writeVInt(maxLength);
+      meta.writeVLong(count);
+      meta.writeLong(startFP);
+      meta.writeVInt(ADDRESS_INTERVAL);
+      meta.writeLong(indexStartFP);
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      meta.writeVInt(BLOCK_SIZE);
+    }
+  }
+
+  @Override
+  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+    checkCanWrite(field);
+    meta.writeVInt(field.number);
+    meta.writeByte(Lucene49DocValuesFormat.SORTED);
+    addTermsDict(field, values);
+    addNumericField(field, docToOrd, false);
+  }
+
+  @Override
+  public void addSortedNumericField(FieldInfo field, final Iterable<Number> docToValueCount, final Iterable<Number> values) throws IOException {
+    checkCanWrite(field);
+    meta.writeVInt(field.number);
+    meta.writeByte(Lucene49DocValuesFormat.SORTED_NUMERIC);
+    if (isSingleValued(docToValueCount)) {
+      meta.writeVInt(SORTED_SINGLE_VALUED);
+      // The field is single-valued, we can encode it as NUMERIC
+      addNumericField(field, singletonView(docToValueCount, values, null));
+    } else {
+      meta.writeVInt(SORTED_WITH_ADDRESSES);
+      // write the stream of values as a numeric field
+      addNumericField(field, values, true);
+      // write the doc -> ord count as a absolute index to the stream
+      addAddresses(field, docToValueCount);
+    }
+  }
+
+  @Override
+  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
+    checkCanWrite(field);
+    meta.writeVInt(field.number);
+    meta.writeByte(Lucene49DocValuesFormat.SORTED_SET);
+
+    if (isSingleValued(docToOrdCount)) {
+      meta.writeVInt(SORTED_SINGLE_VALUED);
+      // The field is single-valued, we can encode it as SORTED
+      addSortedField(field, values, singletonView(docToOrdCount, ords, -1L));
+    } else {
+      meta.writeVInt(SORTED_WITH_ADDRESSES);
+
+      // write the ord -> byte[] as a binary field
+      addTermsDict(field, values);
+
+      // write the stream of ords as a numeric field
+      // NOTE: we could return an iterator that delta-encodes these within a doc
+      addNumericField(field, ords, false);
+
+      // write the doc -> ord count as a absolute index to the stream
+      addAddresses(field, docToOrdCount);
+    }
+  }
+  
+  // writes addressing information as MONOTONIC_COMPRESSED integer
+  private void addAddresses(FieldInfo field, Iterable<Number> values) throws IOException {
+    meta.writeVInt(field.number);
+    meta.writeByte(Lucene49DocValuesFormat.NUMERIC);
+    meta.writeVInt(MONOTONIC_COMPRESSED);
+    meta.writeLong(-1L);
+    meta.writeLong(data.getFilePointer());
+    meta.writeVLong(maxDoc);
+    meta.writeVInt(PackedInts.VERSION_CURRENT);
+    meta.writeVInt(BLOCK_SIZE);
+
+    final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
+    long addr = 0;
+    writer.add(addr);
+    for (Number v : values) {
+      addr += v.longValue();
+      writer.add(addr);
+    }
+    writer.finish();
+    meta.writeLong(data.getFilePointer());
+  }
+
+  @Override
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      if (meta != null) {
+        meta.writeVInt(-1); // write EOF marker
+        CodecUtil.writeFooter(meta); // write checksum
+      }
+      if (data != null) {
+        CodecUtil.writeFooter(data); // write checksum
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data, meta);
+      } else {
+        IOUtils.closeWhileHandlingException(data, meta);
+      }
+      meta = data = null;
+    }
+  }
+  
+  void checkCanWrite(FieldInfo field) {
+    if ((field.getDocValuesType() == DocValuesType.NUMERIC || 
+        field.getDocValuesType() == DocValuesType.BINARY) && 
+        field.getDocValuesGen() != -1) {
+      // ok
+    } else {
+      throw new UnsupportedOperationException("this codec can only be used for reading");
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java
new file mode 100644
index 0000000..281c0f2
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java
@@ -0,0 +1,195 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.SmallFloat;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.packed.DirectWriter;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
+
+/**
+ * Lucene 4.9 DocValues format.
+ * <p>
+ * Encodes the five per-document value types (Numeric,Binary,Sorted,SortedSet,SortedNumeric) with these strategies:
+ * <p>
+ * {@link DocValuesType#NUMERIC NUMERIC}:
+ * <ul>
+ *    <li>Delta-compressed: per-document integers written as deltas from the minimum value,
+ *        compressed with bitpacking. For more information, see {@link DirectWriter}.
+ *    <li>Table-compressed: when the number of unique values is very small (&lt; 256), and
+ *        when there are unused "gaps" in the range of values used (such as {@link SmallFloat}), 
+ *        a lookup table is written instead. Each per-document entry is instead the ordinal 
+ *        to this table, and those ordinals are compressed with bitpacking ({@link DirectWriter}). 
+ *    <li>GCD-compressed: when all numbers share a common divisor, such as dates, the greatest
+ *        common denominator (GCD) is computed, and quotients are stored using Delta-compressed Numerics.
+ *    <li>Monotonic-compressed: when all numbers are monotonically increasing offsets, they are written
+ *        as blocks of bitpacked integers, encoding the deviation from the expected delta.
+ * </ul>
+ * <p>
+ * {@link DocValuesType#BINARY BINARY}:
+ * <ul>
+ *    <li>Fixed-width Binary: one large concatenated byte[] is written, along with the fixed length.
+ *        Each document's value can be addressed directly with multiplication ({@code docID * length}). 
+ *    <li>Variable-width Binary: one large concatenated byte[] is written, along with end addresses 
+ *        for each document. The addresses are written as Monotonic-compressed numerics.
+ *    <li>Prefix-compressed Binary: values are written in chunks of 16, with the first value written
+ *        completely and other values sharing prefixes. chunk addresses are written as Monotonic-compressed
+ *        numerics.
+ * </ul>
+ * <p>
+ * {@link DocValuesType#SORTED SORTED}:
+ * <ul>
+ *    <li>Sorted: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
+ *        along with the per-document ordinals written using one of the numeric strategies above.
+ * </ul>
+ * <p>
+ * {@link DocValuesType#SORTED_SET SORTED_SET}:
+ * <ul>
+ *    <li>SortedSet: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
+ *        an ordinal list and per-document index into this list are written using the numeric strategies 
+ *        above. 
+ * </ul>
+ * <p>
+ * {@link DocValuesType#SORTED_NUMERIC SORTED_NUMERIC}:
+ * <ul>
+ *    <li>SortedNumeric: a value list and per-document index into this list are written using the numeric
+ *        strategies above.
+ * </ul>
+ * <p>
+ * Files:
+ * <ol>
+ *   <li><tt>.dvd</tt>: DocValues data</li>
+ *   <li><tt>.dvm</tt>: DocValues metadata</li>
+ * </ol>
+ * <ol>
+ *   <li><a name="dvm" id="dvm"></a>
+ *   <p>The DocValues metadata or .dvm file.</p>
+ *   <p>For DocValues field, this stores metadata, such as the offset into the 
+ *      DocValues data (.dvd)</p>
+ *   <p>DocValues metadata (.dvm) --&gt; Header,&lt;Entry&gt;<sup>NumFields</sup>,Footer</p>
+ *   <ul>
+ *     <li>Entry --&gt; NumericEntry | BinaryEntry | SortedEntry | SortedSetEntry | SortedNumericEntry</li>
+ *     <li>NumericEntry --&gt; GCDNumericEntry | TableNumericEntry | DeltaNumericEntry</li>
+ *     <li>GCDNumericEntry --&gt; NumericHeader,MinValue,GCD,BitsPerValue</li>
+ *     <li>TableNumericEntry --&gt; NumericHeader,TableSize,{@link DataOutput#writeLong Int64}<sup>TableSize</sup>,BitsPerValue</li>
+ *     <li>DeltaNumericEntry --&gt; NumericHeader,MinValue,BitsPerValue</li>
+ *     <li>MonotonicNumericEntry --&gt; NumericHeader,PackedVersion,BlockSize</li>
+ *     <li>NumericHeader --&gt; FieldNumber,EntryType,NumericType,MissingOffset,DataOffset,Count,EndOffset</li>
+ *     <li>BinaryEntry --&gt; FixedBinaryEntry | VariableBinaryEntry | PrefixBinaryEntry</li>
+ *     <li>FixedBinaryEntry --&gt; BinaryHeader</li>
+ *     <li>VariableBinaryEntry --&gt; BinaryHeader,AddressOffset,PackedVersion,BlockSize</li>
+ *     <li>PrefixBinaryEntry --&gt; BinaryHeader,AddressInterval,AddressOffset,PackedVersion,BlockSize</li>
+ *     <li>BinaryHeader --&gt; FieldNumber,EntryType,BinaryType,MissingOffset,MinLength,MaxLength,DataOffset</li>
+ *     <li>SortedEntry --&gt; FieldNumber,EntryType,BinaryEntry,NumericEntry</li>
+ *     <li>SortedSetEntry --&gt; EntryType,BinaryEntry,NumericEntry,NumericEntry</li>
+ *     <li>SortedNumericEntry --&gt; EntryType,NumericEntry,NumericEntry</li>
+ *     <li>FieldNumber,PackedVersion,MinLength,MaxLength,BlockSize,ValueCount --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *     <li>EntryType,CompressionType --&gt; {@link DataOutput#writeByte Byte}</li>
+ *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *     <li>MinValue,GCD,MissingOffset,AddressOffset,DataOffset,EndOffset --&gt; {@link DataOutput#writeLong Int64}</li>
+ *     <li>TableSize,BitsPerValue --&gt; {@link DataOutput#writeVInt vInt}</li>
+ *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ *   </ul>
+ *   <p>Sorted fields have two entries: a BinaryEntry with the value metadata,
+ *      and an ordinary NumericEntry for the document-to-ord metadata.</p>
+ *   <p>SortedSet fields have three entries: a BinaryEntry with the value metadata,
+ *      and two NumericEntries for the document-to-ord-index and ordinal list metadata.</p>
+ *   <p>SortedNumeric fields have two entries: A NumericEntry with the value metadata,
+ *      and a numeric entry with the document-to-value index.</p>
+ *   <p>FieldNumber of -1 indicates the end of metadata.</p>
+ *   <p>EntryType is a 0 (NumericEntry) or 1 (BinaryEntry)</p>
+ *   <p>DataOffset is the pointer to the start of the data in the DocValues data (.dvd)</p>
+ *   <p>EndOffset is the pointer to the end of the data in the DocValues data (.dvd)</p>
+ *   <p>NumericType indicates how Numeric values will be compressed:
+ *      <ul>
+ *         <li>0 --&gt; delta-compressed. For each block of 16k integers, every integer is delta-encoded
+ *             from the minimum value within the block. 
+ *         <li>1 --&gt, gcd-compressed. When all integers share a common divisor, only quotients are stored
+ *             using blocks of delta-encoded ints.
+ *         <li>2 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
+ *             a lookup table of unique values is written, followed by the ordinal for each document.
+ *      </ul>
+ *   <p>BinaryType indicates how Binary values will be stored:
+ *      <ul>
+ *         <li>0 --&gt; fixed-width. All values have the same length, addressing by multiplication. 
+ *         <li>1 --&gt, variable-width. An address for each value is stored.
+ *         <li>2 --&gt; prefix-compressed. An address to the start of every interval'th value is stored.
+ *      </ul>
+ *   <p>MinLength and MaxLength represent the min and max byte[] value lengths for Binary values.
+ *      If they are equal, then all values are of a fixed size, and can be addressed as DataOffset + (docID * length).
+ *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
+ *      is written for the addresses.
+ *   <p>MissingOffset points to a byte[] containing a bitset of all documents that had a value for the field.
+ *      If its -1, then there are no missing values.
+ *   <p>Checksum contains the CRC32 checksum of all bytes in the .dvm file up
+ *      until the checksum. This is used to verify integrity of the file on opening the
+ *      index.
+ *   <li><a name="dvd" id="dvd"></a>
+ *   <p>The DocValues data or .dvd file.</p>
+ *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
+ *   <p>DocValues data (.dvd) --&gt; Header,&lt;NumericData | BinaryData | SortedData&gt;<sup>NumFields</sup>,Footer</p>
+ *   <ul>
+ *     <li>NumericData --&gt; DeltaCompressedNumerics | TableCompressedNumerics | GCDCompressedNumerics</li>
+ *     <li>BinaryData --&gt;  {@link DataOutput#writeByte Byte}<sup>DataLength</sup>,Addresses</li>
+ *     <li>SortedData --&gt; {@link FST FST&lt;Int64&gt;}</li>
+ *     <li>DeltaCompressedNumerics,TableCompressedNumerics,GCDCompressedNumerics --&gt; {@link DirectWriter PackedInts}</li>
+ *     <li>Addresses --&gt; {@link MonotonicBlockPackedWriter MonotonicBlockPackedInts(blockSize=16k)}</li>
+ *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
+ *   </ul>
+ * </ol>
+ * @lucene.experimental
+ */
+public class Lucene49DocValuesFormat extends DocValuesFormat {
+
+  /** Sole Constructor */
+  public Lucene49DocValuesFormat() {
+    super("Lucene49");
+  }
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return new Lucene49DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
+  }
+
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new Lucene49DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
+  }
+  
+  static final String DATA_CODEC = "Lucene49DocValuesData";
+  static final String DATA_EXTENSION = "dvd";
+  static final String META_CODEC = "Lucene49ValuesMetadata";
+  static final String META_EXTENSION = "dvm";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+  static final byte NUMERIC = 0;
+  static final byte BINARY = 1;
+  static final byte SORTED = 2;
+  static final byte SORTED_SET = 3;
+  static final byte SORTED_NUMERIC = 4;
+}
diff --git a/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java
new file mode 100644
index 0000000..70b8dd8
--- /dev/null
+++ b/lucene/backward-codecs/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java
@@ -0,0 +1,946 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.BINARY_FIXED_UNCOMPRESSED;
+import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.BINARY_PREFIX_COMPRESSED;
+import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.BINARY_VARIABLE_UNCOMPRESSED;
+import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.DELTA_COMPRESSED;
+import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.GCD_COMPRESSED;
+import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.MONOTONIC_COMPRESSED;
+import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.SORTED_SINGLE_VALUED;
+import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.SORTED_WITH_ADDRESSES;
+import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.TABLE_COMPRESSED;
+
+import java.io.Closeable; // javadocs
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.RandomAccessOrds;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.SortedNumericDocValues;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.RandomAccessInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LongValues;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.packed.DirectReader;
+import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
+
+/** reader for {@link Lucene49DocValuesFormat} */
+class Lucene49DocValuesProducer extends DocValuesProducer implements Closeable {
+  private final Map<Integer,NumericEntry> numerics;
+  private final Map<Integer,BinaryEntry> binaries;
+  private final Map<Integer,SortedSetEntry> sortedSets;
+  private final Map<Integer,SortedSetEntry> sortedNumerics;
+  private final Map<Integer,NumericEntry> ords;
+  private final Map<Integer,NumericEntry> ordIndexes;
+  private final AtomicLong ramBytesUsed;
+  private final IndexInput data;
+  private final int maxDoc;
+  private final int version;
+
+  // memory-resident structures
+  private final Map<Integer,MonotonicBlockPackedReader> addressInstances = new HashMap<>();
+  private final Map<Integer,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<>();
+  
+  /** expert: instantiates a new reader */
+  Lucene49DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+    // read in the entries from the metadata file.
+    ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
+    this.maxDoc = state.segmentInfo.getDocCount();
+    boolean success = false;
+    try {
+      version = CodecUtil.checkHeader(in, metaCodec, 
+                                      Lucene49DocValuesFormat.VERSION_START,
+                                      Lucene49DocValuesFormat.VERSION_CURRENT);
+      numerics = new HashMap<>();
+      ords = new HashMap<>();
+      ordIndexes = new HashMap<>();
+      binaries = new HashMap<>();
+      sortedSets = new HashMap<>();
+      sortedNumerics = new HashMap<>();
+      readFields(in, state.fieldInfos);
+
+      CodecUtil.checkFooter(in);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(in);
+      } else {
+        IOUtils.closeWhileHandlingException(in);
+      }
+    }
+
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+    this.data = state.directory.openInput(dataName, state.context);
+    success = false;
+    try {
+      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
+                                                 Lucene49DocValuesFormat.VERSION_START,
+                                                 Lucene49DocValuesFormat.VERSION_CURRENT);
+      if (version != version2) {
+        throw new CorruptIndexException("Format versions mismatch");
+      }
+      
+      // NOTE: data file is too costly to verify checksum against all the bytes on open,
+      // but for now we at least verify proper structure of the checksum footer: which looks
+      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
+      // such as file truncation.
+      CodecUtil.retrieveChecksum(data);
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this.data);
+      }
+    }
+    
+    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
+  }
+
+  private void readSortedField(int fieldNumber, IndexInput meta, FieldInfos infos) throws IOException {
+    // sorted = binary + numeric
+    if (meta.readVInt() != fieldNumber) {
+      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    if (meta.readByte() != Lucene49DocValuesFormat.BINARY) {
+      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    BinaryEntry b = readBinaryEntry(meta);
+    binaries.put(fieldNumber, b);
+    
+    if (meta.readVInt() != fieldNumber) {
+      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
+      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    NumericEntry n = readNumericEntry(meta);
+    ords.put(fieldNumber, n);
+  }
+
+  private void readSortedSetFieldWithAddresses(int fieldNumber, IndexInput meta, FieldInfos infos) throws IOException {
+    // sortedset = binary + numeric (addresses) + ordIndex
+    if (meta.readVInt() != fieldNumber) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    if (meta.readByte() != Lucene49DocValuesFormat.BINARY) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    BinaryEntry b = readBinaryEntry(meta);
+    binaries.put(fieldNumber, b);
+
+    if (meta.readVInt() != fieldNumber) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    NumericEntry n1 = readNumericEntry(meta);
+    ords.put(fieldNumber, n1);
+
+    if (meta.readVInt() != fieldNumber) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
+      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+    }
+    NumericEntry n2 = readNumericEntry(meta);
+    ordIndexes.put(fieldNumber, n2);
+  }
+
+  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
+    int fieldNumber = meta.readVInt();
+    while (fieldNumber != -1) {
+      if (infos.fieldInfo(fieldNumber) == null) {
+        // trickier to validate more: because we re-use for norms, because we use multiple entries
+        // for "composite" types like sortedset, etc.
+        throw new CorruptIndexException("Invalid field number: " + fieldNumber + " (resource=" + meta + ")");
+      }
+      byte type = meta.readByte();
+      if (type == Lucene49DocValuesFormat.NUMERIC) {
+        numerics.put(fieldNumber, readNumericEntry(meta));
+      } else if (type == Lucene49DocValuesFormat.BINARY) {
+        BinaryEntry b = readBinaryEntry(meta);
+        binaries.put(fieldNumber, b);
+      } else if (type == Lucene49DocValuesFormat.SORTED) {
+        readSortedField(fieldNumber, meta, infos);
+      } else if (type == Lucene49DocValuesFormat.SORTED_SET) {
+        SortedSetEntry ss = readSortedSetEntry(meta);
+        sortedSets.put(fieldNumber, ss);
+        if (ss.format == SORTED_WITH_ADDRESSES) {
+          readSortedSetFieldWithAddresses(fieldNumber, meta, infos);
+        } else if (ss.format == SORTED_SINGLE_VALUED) {
+          if (meta.readVInt() != fieldNumber) {
+            throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+          }
+          if (meta.readByte() != Lucene49DocValuesFormat.SORTED) {
+            throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+          }
+          readSortedField(fieldNumber, meta, infos);
+        } else {
+          throw new AssertionError();
+        }
+      } else if (type == Lucene49DocValuesFormat.SORTED_NUMERIC) {
+        SortedSetEntry ss = readSortedSetEntry(meta);
+        sortedNumerics.put(fieldNumber, ss);
+        if (meta.readVInt() != fieldNumber) {
+          throw new CorruptIndexException("sortednumeric entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+        }
+        if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
+          throw new CorruptIndexException("sortednumeric entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+        }
+        numerics.put(fieldNumber, readNumericEntry(meta));
+        if (ss.format == SORTED_WITH_ADDRESSES) {
+          if (meta.readVInt() != fieldNumber) {
+            throw new CorruptIndexException("sortednumeric entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+          }
+          if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
+            throw new CorruptIndexException("sortednumeric entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
+          }
+          NumericEntry ordIndex = readNumericEntry(meta);
+          ordIndexes.put(fieldNumber, ordIndex);
+        } else if (ss.format != SORTED_SINGLE_VALUED) {
+          throw new AssertionError();
+        }
+      } else {
+        throw new CorruptIndexException("invalid type: " + type + ", resource=" + meta);
+      }
+      fieldNumber = meta.readVInt();
+    }
+  }
+  
+  static NumericEntry readNumericEntry(IndexInput meta) throws IOException {
+    NumericEntry entry = new NumericEntry();
+    entry.format = meta.readVInt();
+    entry.missingOffset = meta.readLong();
+    entry.offset = meta.readLong();
+    entry.count = meta.readVLong();
+    switch(entry.format) {
+      case GCD_COMPRESSED:
+        entry.minValue = meta.readLong();
+        entry.gcd = meta.readLong();
+        entry.bitsPerValue = meta.readVInt();
+        break;
+      case TABLE_COMPRESSED:
+        final int uniqueValues = meta.readVInt();
+        if (uniqueValues > 256) {
+          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, input=" + meta);
+        }
+        entry.table = new long[uniqueValues];
+        for (int i = 0; i < uniqueValues; ++i) {
+          entry.table[i] = meta.readLong();
+        }
+        entry.bitsPerValue = meta.readVInt();
+        break;
+      case DELTA_COMPRESSED:
+        entry.minValue = meta.readLong();
+        entry.bitsPerValue = meta.readVInt();
+        break;
+      case MONOTONIC_COMPRESSED:
+        entry.packedIntsVersion = meta.readVInt();
+        entry.blockSize = meta.readVInt();
+        break;
+      default:
+        throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
+    }
+    entry.endOffset = meta.readLong();
+    return entry;
+  }
+  
+  static BinaryEntry readBinaryEntry(IndexInput meta) throws IOException {
+    BinaryEntry entry = new BinaryEntry();
+    entry.format = meta.readVInt();
+    entry.missingOffset = meta.readLong();
+    entry.minLength = meta.readVInt();
+    entry.maxLength = meta.readVInt();
+    entry.count = meta.readVLong();
+    entry.offset = meta.readLong();
+    switch(entry.format) {
+      case BINARY_FIXED_UNCOMPRESSED:
+        break;
+      case BINARY_PREFIX_COMPRESSED:
+        entry.addressInterval = meta.readVInt();
+        entry.addressesOffset = meta.readLong();
+        entry.packedIntsVersion = meta.readVInt();
+        entry.blockSize = meta.readVInt();
+        break;
+      case BINARY_VARIABLE_UNCOMPRESSED:
+        entry.addressesOffset = meta.readLong();
+        entry.packedIntsVersion = meta.readVInt();
+        entry.blockSize = meta.readVInt();
+        break;
+      default:
+        throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
+    }
+    return entry;
+  }
+
+  SortedSetEntry readSortedSetEntry(IndexInput meta) throws IOException {
+    SortedSetEntry entry = new SortedSetEntry();
+    entry.format = meta.readVInt();
+    if (entry.format != SORTED_SINGLE_VALUED && entry.format != SORTED_WITH_ADDRESSES) {
+      throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
+    }
+    return entry;
+  }
+
+  @Override
+  public NumericDocValues getNumeric(FieldInfo field) throws IOException {
+    NumericEntry entry = numerics.get(field.number);
+    return getNumeric(entry);
+  }
+  
+  @Override
+  public long ramBytesUsed() {
+    return ramBytesUsed.get();
+  }
+  
+  @Override
+  public void checkIntegrity() throws IOException {
+    CodecUtil.checksumEntireFile(data);
+  }
+
+  LongValues getNumeric(NumericEntry entry) throws IOException {
+    RandomAccessInput slice = this.data.randomAccessSlice(entry.offset, entry.endOffset - entry.offset);
+    switch (entry.format) {
+      case DELTA_COMPRESSED:
+        final long delta = entry.minValue;
+        final LongValues values = DirectReader.getInstance(slice, entry.bitsPerValue);
+        return new LongValues() {
+          @Override
+          public long get(long id) {
+            return delta + values.get(id);
+          }
+        };
+      case GCD_COMPRESSED:
+        final long min = entry.minValue;
+        final long mult = entry.gcd;
+        final LongValues quotientReader = DirectReader.getInstance(slice, entry.bitsPerValue);
+        return new LongValues() {
+          @Override
+          public long get(long id) {
+            return min + mult * quotientReader.get(id);
+          }
+        };
+      case TABLE_COMPRESSED:
+        final long table[] = entry.table;
+        final LongValues ords = DirectReader.getInstance(slice, entry.bitsPerValue);
+        return new LongValues() {
+          @Override
+          public long get(long id) {
+            return table[(int) ords.get(id)];
+          }
+        };
+      default:
+        throw new AssertionError();
+    }
+  }
+
+  @Override
+  public BinaryDocValues getBinary(FieldInfo field) throws IOException {
+    BinaryEntry bytes = binaries.get(field.number);
+    switch(bytes.format) {
+      case BINARY_FIXED_UNCOMPRESSED:
+        return getFixedBinary(field, bytes);
+      case BINARY_VARIABLE_UNCOMPRESSED:
+        return getVariableBinary(field, bytes);
+      case BINARY_PREFIX_COMPRESSED:
+        return getCompressedBinary(field, bytes);
+      default:
+        throw new AssertionError();
+    }
+  }
+  
+  private BinaryDocValues getFixedBinary(FieldInfo field, final BinaryEntry bytes) {
+    final IndexInput data = this.data.clone();
+
+    return new LongBinaryDocValues() {
+      final BytesRef term;
+      {
+        term = new BytesRef(bytes.maxLength);
+        term.offset = 0;
+        term.length = bytes.maxLength;
+      }
+      
+      @Override
+      public BytesRef get(long id) {
+        long address = bytes.offset + id * bytes.maxLength;
+        try {
+          data.seek(address);
+          data.readBytes(term.bytes, 0, term.length);
+          return term;
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    };
+  }
+  
+  /** returns an address instance for variable-length binary values. */
+  private MonotonicBlockPackedReader getAddressInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
+    final MonotonicBlockPackedReader addresses;
+    synchronized (addressInstances) {
+      MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
+      if (addrInstance == null) {
+        data.seek(bytes.addressesOffset);
+        addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count+1, false);
+        addressInstances.put(field.number, addrInstance);
+        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
+      addresses = addrInstance;
+    }
+    return addresses;
+  }
+  
+  private BinaryDocValues getVariableBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
+    final IndexInput data = this.data.clone();
+    
+    final MonotonicBlockPackedReader addresses = getAddressInstance(data, field, bytes);
+
+    return new LongBinaryDocValues() {
+      final BytesRef term = new BytesRef(Math.max(0, bytes.maxLength));
+      
+      @Override
+      public BytesRef get(long id) {
+        long startAddress = bytes.offset + addresses.get(id);
+        long endAddress = bytes.offset + addresses.get(id+1);
+        int length = (int) (endAddress - startAddress);
+        try {
+          data.seek(startAddress);
+          data.readBytes(term.bytes, 0, length);
+          term.length = length;
+          return term;
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    };
+  }
+  
+  /** returns an address instance for prefix-compressed binary values. */
+  private MonotonicBlockPackedReader getIntervalInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
+    final MonotonicBlockPackedReader addresses;
+    final long interval = bytes.addressInterval;
+    synchronized (addressInstances) {
+      MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
+      if (addrInstance == null) {
+        data.seek(bytes.addressesOffset);
+        final long size;
+        if (bytes.count % interval == 0) {
+          size = bytes.count / interval;
+        } else {
+          size = 1L + bytes.count / interval;
+        }
+        addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
+        addressInstances.put(field.number, addrInstance);
+        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
+      addresses = addrInstance;
+    }
+    return addresses;
+  }
+
+
+  private BinaryDocValues getCompressedBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
+    final IndexInput data = this.data.clone();
+
+    final MonotonicBlockPackedReader addresses = getIntervalInstance(data, field, bytes);
+    
+    return new CompressedBinaryDocValues(bytes, addresses, data);
+  }
+
+  @Override
+  public SortedDocValues getSorted(FieldInfo field) throws IOException {
+    final int valueCount = (int) binaries.get(field.number).count;
+    final BinaryDocValues binary = getBinary(field);
+    NumericEntry entry = ords.get(field.number);
+    final LongValues ordinals = getNumeric(entry);
+    
+    return new SortedDocValues() {
+
+      @Override
+      public int getOrd(int docID) {
+        return (int) ordinals.get(docID);
+      }
+
+      @Override
+      public BytesRef lookupOrd(int ord) {
+        return binary.get(ord);
+      }
+
+      @Override
+      public int getValueCount() {
+        return valueCount;
+      }
+
+      @Override
+      public int lookupTerm(BytesRef key) {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return (int) ((CompressedBinaryDocValues)binary).lookupTerm(key);
+        } else {
+        return super.lookupTerm(key);
+        }
+      }
+
+      @Override
+      public TermsEnum termsEnum() {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return ((CompressedBinaryDocValues)binary).getTermsEnum();
+        } else {
+          return super.termsEnum();
+        }
+      }
+    };
+  }
+  
+  /** returns an address instance for sortedset ordinal lists */
+  private MonotonicBlockPackedReader getOrdIndexInstance(IndexInput data, FieldInfo field, NumericEntry entry) throws IOException {
+    final MonotonicBlockPackedReader ordIndex;
+    synchronized (ordIndexInstances) {
+      MonotonicBlockPackedReader ordIndexInstance = ordIndexInstances.get(field.number);
+      if (ordIndexInstance == null) {
+        data.seek(entry.offset);
+        ordIndexInstance = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, entry.count+1, false);
+        ordIndexInstances.put(field.number, ordIndexInstance);
+        ramBytesUsed.addAndGet(ordIndexInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
+      }
+      ordIndex = ordIndexInstance;
+    }
+    return ordIndex;
+  }
+  
+  @Override
+  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
+    SortedSetEntry ss = sortedNumerics.get(field.number);
+    NumericEntry numericEntry = numerics.get(field.number);
+    final LongValues values = getNumeric(numericEntry);
+    if (ss.format == SORTED_SINGLE_VALUED) {
+      final Bits docsWithField = getMissingBits(numericEntry.missingOffset);
+      return DocValues.singleton(values, docsWithField);
+    } else if (ss.format == SORTED_WITH_ADDRESSES) {
+      final IndexInput data = this.data.clone();
+      final MonotonicBlockPackedReader ordIndex = getOrdIndexInstance(data, field, ordIndexes.get(field.number));
+      
+      return new SortedNumericDocValues() {
+        long startOffset;
+        long endOffset;
+        
+        @Override
+        public void setDocument(int doc) {
+          startOffset = ordIndex.get(doc);
+          endOffset = ordIndex.get(doc+1L);
+        }
+
+        @Override
+        public long valueAt(int index) {
+          return values.get(startOffset + index);
+        }
+
+        @Override
+        public int count() {
+          return (int) (endOffset - startOffset);
+        }
+      };
+    } else {
+      throw new AssertionError();
+    }
+  }
+
+  @Override
+  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
+    SortedSetEntry ss = sortedSets.get(field.number);
+    if (ss.format == SORTED_SINGLE_VALUED) {
+      final SortedDocValues values = getSorted(field);
+      return DocValues.singleton(values);
+    } else if (ss.format != SORTED_WITH_ADDRESSES) {
+      throw new AssertionError();
+    }
+
+    final IndexInput data = this.data.clone();
+    final long valueCount = binaries.get(field.number).count;
+    // we keep the byte[]s and list of ords on disk, these could be large
+    final LongBinaryDocValues binary = (LongBinaryDocValues) getBinary(field);
+    final LongValues ordinals = getNumeric(ords.get(field.number));
+    // but the addresses to the ord stream are in RAM
+    final MonotonicBlockPackedReader ordIndex = getOrdIndexInstance(data, field, ordIndexes.get(field.number));
+    
+    return new RandomAccessOrds() {
+      long startOffset;
+      long offset;
+      long endOffset;
+      
+      @Override
+      public long nextOrd() {
+        if (offset == endOffset) {
+          return NO_MORE_ORDS;
+        } else {
+          long ord = ordinals.get(offset);
+          offset++;
+          return ord;
+        }
+      }
+
+      @Override
+      public void setDocument(int docID) {
+        startOffset = offset = ordIndex.get(docID);
+        endOffset = ordIndex.get(docID+1L);
+      }
+
+      @Override
+      public BytesRef lookupOrd(long ord) {
+        return binary.get(ord);
+      }
+
+      @Override
+      public long getValueCount() {
+        return valueCount;
+      }
+      
+      @Override
+      public long lookupTerm(BytesRef key) {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return ((CompressedBinaryDocValues)binary).lookupTerm(key);
+        } else {
+          return super.lookupTerm(key);
+        }
+      }
+
+      @Override
+      public TermsEnum termsEnum() {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return ((CompressedBinaryDocValues)binary).getTermsEnum();
+        } else {
+          return super.termsEnum();
+        }
+      }
+
+      @Override
+      public long ordAt(int index) {
+        return ordinals.get(startOffset + index);
+      }
+
+      @Override
+      public int cardinality() {
+        return (int) (endOffset - startOffset);
+      }
+    };
+  }
+  
+  private Bits getMissingBits(final long offset) throws IOException {
+    if (offset == -1) {
+      return new Bits.MatchAllBits(maxDoc);
+    } else {
+      int length = (int) ((maxDoc + 7L) >>> 3);
+      final RandomAccessInput in = data.randomAccessSlice(offset, length);
+      return new Bits() {
+        @Override
+        public boolean get(int index) {
+          try {
+            return (in.readByte(index >> 3) & (1 << (index & 7))) != 0;
+          } catch (IOException e) {
+            throw new RuntimeException(e);
+          }
+        }
+
+        @Override
+        public int length() {
+          return maxDoc;
+        }
+      };
+    }
+  }
+
+  @Override
+  public Bits getDocsWithField(FieldInfo field) throws IOException {
+    switch(field.getDocValuesType()) {
+      case SORTED_SET:
+        return DocValues.docsWithValue(getSortedSet(field), maxDoc);
+      case SORTED_NUMERIC:
+        return DocValues.docsWithValue(getSortedNumeric(field), maxDoc);
+      case SORTED:
+        return DocValues.docsWithValue(getSorted(field), maxDoc);
+      case BINARY:
+        BinaryEntry be = binaries.get(field.number);
+        return getMissingBits(be.missingOffset);
+      case NUMERIC:
+        NumericEntry ne = numerics.get(field.number);
+        return getMissingBits(ne.missingOffset);
+      default:
+        throw new AssertionError();
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    data.close();
+  }
+  
+  /** metadata entry for a numeric docvalues field */
+  static class NumericEntry {
+    private NumericEntry() {}
+    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
+    long missingOffset;
+    /** offset to the actual numeric values */
+    public long offset;
+    /** end offset to the actual numeric values */
+    public long endOffset;
+    /** bits per value used to pack the numeric values */
+    public int bitsPerValue;
+
+    int format;
+    /** packed ints version used to encode these numerics */
+    public int packedIntsVersion;
+    /** count of values written */
+    public long count;
+    /** packed ints blocksize */
+    public int blockSize;
+    
+    long minValue;
+    long gcd;
+    long table[];
+  }
+  
+  /** metadata entry for a binary docvalues field */
+  static class BinaryEntry {
+    private BinaryEntry() {}
+    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
+    long missingOffset;
+    /** offset to the actual binary values */
+    long offset;
+
+    int format;
+    /** count of values written */
+    public long count;
+    int minLength;
+    int maxLength;
+    /** offset to the addressing data that maps a value to its slice of the byte[] */
+    public long addressesOffset;
+    /** interval of shared prefix chunks (when using prefix-compressed binary) */
+    public long addressInterval;
+    /** packed ints version used to encode addressing information */
+    public int packedIntsVersion;
+    /** packed ints blocksize */
+    public int blockSize;
+  }
+
+  /** metadata entry for a sorted-set docvalues field */
+  static class SortedSetEntry {
+    private SortedSetEntry() {}
+    int format;
+  }
+
+  // internally we compose complex dv (sorted/sortedset) from other ones
+  static abstract class LongBinaryDocValues extends BinaryDocValues {
+    @Override
+    public final BytesRef get(int docID) {
+      return get((long)docID);
+    }
+    
+    abstract BytesRef get(long id);
+  }
+  
+  // in the compressed case, we add a few additional operations for
+  // more efficient reverse lookup and enumeration
+  static class CompressedBinaryDocValues extends LongBinaryDocValues {
+    final BinaryEntry bytes;
+    final long interval;
+    final long numValues;
+    final long numIndexValues;
+    final MonotonicBlockPackedReader addresses;
+    final IndexInput data;
+    final TermsEnum termsEnum;
+    
+    public CompressedBinaryDocValues(BinaryEntry bytes, MonotonicBlockPackedReader addresses, IndexInput data) throws IOException {
+      this.bytes = bytes;
+      this.interval = bytes.addressInterval;
+      this.addresses = addresses;
+      this.data = data;
+      this.numValues = bytes.count;
+      this.numIndexValues = addresses.size();
+      this.termsEnum = getTermsEnum(data);
+    }
+    
+    @Override
+    public BytesRef get(long id) {
+      try {
+        termsEnum.seekExact(id);
+        return termsEnum.term();
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    
+    long lookupTerm(BytesRef key) {
+      try {
+        SeekStatus status = termsEnum.seekCeil(key);
+        if (status == SeekStatus.END) {
+          return -numValues-1;
+        } else if (status == SeekStatus.FOUND) {
+          return termsEnum.ord();
+        } else {
+          return -termsEnum.ord()-1;
+        }
+      } catch (IOException bogus) {
+        throw new RuntimeException(bogus);
+      }
+    }
+    
+    TermsEnum getTermsEnum() {
+      try {
+        return getTermsEnum(data.clone());
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    
+    private TermsEnum getTermsEnum(final IndexInput input) throws IOException {
+      input.seek(bytes.offset);
+      
+      return new TermsEnum() {
+        private long currentOrd = -1;
+        // TODO: maxLength is negative when all terms are merged away...
+        private final BytesRef term = new BytesRef(bytes.maxLength < 0 ? 0 : bytes.maxLength);
+
+        @Override
+        public BytesRef next() throws IOException {
+          if (++currentOrd >= numValues) {
+            return null;
+          } else {
+            int start = input.readVInt();
+            int suffix = input.readVInt();
+            input.readBytes(term.bytes, start, suffix);
+            term.length = start + suffix;
+            return term;
+          }
+        }
+
+        @Override
+        public SeekStatus seekCeil(BytesRef text) throws IOException {
+          // binary-search just the index values to find the block,
+          // then scan within the block
+          long low = 0;
+          long high = numIndexValues-1;
+
+          while (low <= high) {
+            long mid = (low + high) >>> 1;
+            seekExact(mid * interval);
+            int cmp = term.compareTo(text);
+
+            if (cmp < 0) {
+              low = mid + 1;
+            } else if (cmp > 0) {
+              high = mid - 1;
+            } else {
+              // we got lucky, found an indexed term
+              return SeekStatus.FOUND;
+            }
+          }
+          
+          if (numIndexValues == 0) {
+            return SeekStatus.END;
+          }
+          
+          // block before insertion point
+          long block = low-1;
+          seekExact(block < 0 ? -1 : block * interval);
+          
+          while (next() != null) {
+            int cmp = term.compareTo(text);
+            if (cmp == 0) {
+              return SeekStatus.FOUND;
+            } else if (cmp > 0) {
+              return SeekStatus.NOT_FOUND;
+            }
+          }
+          
+          return SeekStatus.END;
+        }
+
+        @Override
+        public void seekExact(long ord) throws IOException {
+          long block = ord / interval;
+
+          if (ord >= currentOrd && block == currentOrd / interval) {
+            // seek within current block
+          } else {
+            // position before start of block
+            currentOrd = ord - ord % interval - 1;
+            input.seek(bytes.offset + addresses.get(block));
+          }
+          
+          while (currentOrd < ord) {
+            next();
+          }
+        }
+
+        @Override
+        public BytesRef term() throws IOException {
+          return term;
+        }
+
+        @Override
+        public long ord() throws IOException {
+          return currentOrd;
+        }
+
+        @Override
+        public int docFreq() throws IOException {
+          throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public long totalTermFreq() throws IOException {
+          return -1;
+        }
+
+        @Override
+        public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+          throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+          throw new UnsupportedOperationException();
+        }
+      };
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.Codec b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
new file mode 100644
index 0000000..ccb80e4
--- /dev/null
+++ b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
@@ -0,0 +1,21 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.lucene40.Lucene40Codec
+org.apache.lucene.codecs.lucene41.Lucene41Codec
+org.apache.lucene.codecs.lucene42.Lucene42Codec
+org.apache.lucene.codecs.lucene45.Lucene45Codec
+org.apache.lucene.codecs.lucene46.Lucene46Codec
+org.apache.lucene.codecs.lucene49.Lucene49Codec
diff --git a/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
new file mode 100644
index 0000000..01ce305
--- /dev/null
+++ b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
@@ -0,0 +1,18 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.lucene42.Lucene42DocValuesFormat
+org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat
+org.apache.lucene.codecs.lucene49.Lucene49DocValuesFormat
diff --git a/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
new file mode 100644
index 0000000..112a169
--- /dev/null
+++ b/lucene/backward-codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -0,0 +1,16 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat
diff --git a/lucene/backward-codecs/src/test-resources/META-INF/services/org.apache.lucene.codecs.Codec b/lucene/backward-codecs/src/test-resources/META-INF/services/org.apache.lucene.codecs.Codec
new file mode 100644
index 0000000..4868c99
--- /dev/null
+++ b/lucene/backward-codecs/src/test-resources/META-INF/services/org.apache.lucene.codecs.Codec
@@ -0,0 +1,21 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.lucene40.Lucene40RWCodec
+org.apache.lucene.codecs.lucene41.Lucene41RWCodec
+org.apache.lucene.codecs.lucene42.Lucene42RWCodec
+org.apache.lucene.codecs.lucene45.Lucene45RWCodec
+org.apache.lucene.codecs.lucene46.Lucene46RWCodec
+org.apache.lucene.codecs.lucene49.Lucene49RWCodec
diff --git a/lucene/backward-codecs/src/test-resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat b/lucene/backward-codecs/src/test-resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
new file mode 100644
index 0000000..ef5b87e
--- /dev/null
+++ b/lucene/backward-codecs/src/test-resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
@@ -0,0 +1,16 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.lucene42.Lucene42RWDocValuesFormat
diff --git a/lucene/backward-codecs/src/test-resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/backward-codecs/src/test-resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
new file mode 100644
index 0000000..c36d889
--- /dev/null
+++ b/lucene/backward-codecs/src/test-resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -0,0 +1,16 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.lucene40.Lucene40RWPostingsFormat
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
new file mode 100644
index 0000000..2f63685
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
@@ -0,0 +1,547 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.TreeSet;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.MissingOrdRemapper;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosReader.LegacyDocValuesType;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+class Lucene40DocValuesWriter extends DocValuesConsumer {
+  private final Directory dir;
+  private final SegmentWriteState state;
+  private final String legacyKey;
+  private final static String segmentSuffix = "dv";
+
+  // note: intentionally ignores seg suffix
+  Lucene40DocValuesWriter(SegmentWriteState state, String filename, String legacyKey) throws IOException {
+    this.state = state;
+    this.legacyKey = legacyKey;
+    this.dir = new CompoundFileDirectory(state.directory, filename, state.context, true);
+  }
+  
+  @Override
+  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+    // examine the values to determine best type to use
+    long minValue = Long.MAX_VALUE;
+    long maxValue = Long.MIN_VALUE;
+    for (Number n : values) {
+      long v = n == null ? 0 : n.longValue();
+      minValue = Math.min(minValue, v);
+      maxValue = Math.max(maxValue, v);
+    }
+    
+    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    IndexOutput data = dir.createOutput(fileName, state.context);
+    boolean success = false;
+    try {
+      if (minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 4) {
+        // fits in a byte[], would be more than 4bpv, just write byte[]
+        addBytesField(field, data, values);
+      } else if (minValue >= Short.MIN_VALUE && maxValue <= Short.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 8) {
+        // fits in a short[], would be more than 8bpv, just write short[]
+        addShortsField(field, data, values);
+      } else if (minValue >= Integer.MIN_VALUE && maxValue <= Integer.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 16) {
+        // fits in a int[], would be more than 16bpv, just write int[]
+        addIntsField(field, data, values);
+      } else {
+        addVarIntsField(field, data, values, minValue, maxValue);
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data);
+      } else {
+        IOUtils.closeWhileHandlingException(data);
+      }
+    }
+  }
+
+  private void addBytesField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_8.name());
+    CodecUtil.writeHeader(output, 
+                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
+                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    output.writeInt(1); // size
+    for (Number n : values) {
+      output.writeByte(n == null ? 0 : n.byteValue());
+    }
+  }
+  
+  private void addShortsField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_16.name());
+    CodecUtil.writeHeader(output, 
+                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
+                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    output.writeInt(2); // size
+    for (Number n : values) {
+      output.writeShort(n == null ? 0 : n.shortValue());
+    }
+  }
+  
+  private void addIntsField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_32.name());
+    CodecUtil.writeHeader(output, 
+                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
+                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
+    output.writeInt(4); // size
+    for (Number n : values) {
+      output.writeInt(n == null ? 0 : n.intValue());
+    }
+  }
+  
+  private void addVarIntsField(FieldInfo field, IndexOutput output, Iterable<Number> values, long minValue, long maxValue) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.VAR_INTS.name());
+    
+    CodecUtil.writeHeader(output, 
+                          Lucene40DocValuesFormat.VAR_INTS_CODEC_NAME, 
+                          Lucene40DocValuesFormat.VAR_INTS_VERSION_CURRENT);
+    
+    final long delta = maxValue - minValue;
+    
+    if (delta < 0) {
+      // writes longs
+      output.writeByte(Lucene40DocValuesFormat.VAR_INTS_FIXED_64);
+      for (Number n : values) {
+        output.writeLong(n == null ? 0 : n.longValue());
+      }
+    } else {
+      // writes packed ints
+      output.writeByte(Lucene40DocValuesFormat.VAR_INTS_PACKED);
+      output.writeLong(minValue);
+      output.writeLong(0 - minValue); // default value (representation of 0)
+      PackedInts.Writer writer = PackedInts.getWriter(output, 
+                                                      state.segmentInfo.getDocCount(),
+                                                      PackedInts.bitsRequired(delta), 
+                                                      PackedInts.DEFAULT);
+      for (Number n : values) {
+        long v = n == null ? 0 : n.longValue();
+        writer.add(v - minValue);
+      }
+      writer.finish();
+    }
+  }
+
+  @Override
+  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
+    // examine the values to determine best type to use
+    HashSet<BytesRef> uniqueValues = new HashSet<>();
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    for (BytesRef b : values) {
+      if (b == null) {
+        b = new BytesRef(); // 4.0 doesnt distinguish
+      }
+      if (b.length > Lucene40DocValuesFormat.MAX_BINARY_FIELD_LENGTH) {
+        throw new IllegalArgumentException("DocValuesField \"" + field.name + "\" is too large, must be <= " + Lucene40DocValuesFormat.MAX_BINARY_FIELD_LENGTH);
+      }
+      minLength = Math.min(minLength, b.length);
+      maxLength = Math.max(maxLength, b.length);
+      if (uniqueValues != null) {
+        if (uniqueValues.add(BytesRef.deepCopyOf(b))) {
+          if (uniqueValues.size() > 256) {
+            uniqueValues = null;
+          }
+        }
+      }
+    }
+    
+    int maxDoc = state.segmentInfo.getDocCount();
+    final boolean fixed = minLength == maxLength;
+    final boolean dedup = uniqueValues != null && uniqueValues.size() * 2 < maxDoc;
+    
+    if (dedup) {
+      // we will deduplicate and deref values
+      boolean success = false;
+      IndexOutput data = null;
+      IndexOutput index = null;
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+      String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+      try {
+        data = dir.createOutput(dataName, state.context);
+        index = dir.createOutput(indexName, state.context);
+        if (fixed) {
+          addFixedDerefBytesField(field, data, index, values, minLength);
+        } else {
+          addVarDerefBytesField(field, data, index, values);
+        }
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(data, index);
+        } else {
+          IOUtils.closeWhileHandlingException(data, index);
+        }
+      }
+    } else {
+      // we dont deduplicate, just write values straight
+      if (fixed) {
+        // fixed byte[]
+        String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+        IndexOutput data = dir.createOutput(fileName, state.context);
+        boolean success = false;
+        try {
+          addFixedStraightBytesField(field, data, values, minLength);
+          success = true;
+        } finally {
+          if (success) {
+            IOUtils.close(data);
+          } else {
+            IOUtils.closeWhileHandlingException(data);
+          }
+        }
+      } else {
+        // variable byte[]
+        boolean success = false;
+        IndexOutput data = null;
+        IndexOutput index = null;
+        String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+        String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+        try {
+          data = dir.createOutput(dataName, state.context);
+          index = dir.createOutput(indexName, state.context);
+          addVarStraightBytesField(field, data, index, values);
+          success = true;
+        } finally {
+          if (success) {
+            IOUtils.close(data, index);
+          } else {
+            IOUtils.closeWhileHandlingException(data, index);
+          }
+        }
+      }
+    }
+  }
+  
+  private void addFixedStraightBytesField(FieldInfo field, IndexOutput output, Iterable<BytesRef> values, int length) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_STRAIGHT.name());
+
+    CodecUtil.writeHeader(output, 
+                          Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_CODEC_NAME,
+                          Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_CURRENT);
+    
+    output.writeInt(length);
+    for (BytesRef v : values) {
+      if (v != null) {
+        output.writeBytes(v.bytes, v.offset, v.length);
+      }
+    }
+  }
+  
+  // NOTE: 4.0 file format docs are crazy/wrong here...
+  private void addVarStraightBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_STRAIGHT.name());
+    
+    CodecUtil.writeHeader(data, 
+                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_DAT,
+                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
+    
+    CodecUtil.writeHeader(index, 
+                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_IDX,
+                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
+    
+    /* values */
+    
+    final long startPos = data.getFilePointer();
+    
+    for (BytesRef v : values) {
+      if (v != null) {
+        data.writeBytes(v.bytes, v.offset, v.length);
+      }
+    }
+    
+    /* addresses */
+    
+    final long maxAddress = data.getFilePointer() - startPos;
+    index.writeVLong(maxAddress);
+    
+    final int maxDoc = state.segmentInfo.getDocCount();
+    assert maxDoc != Integer.MAX_VALUE; // unsupported by the 4.0 impl
+    
+    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc+1, PackedInts.bitsRequired(maxAddress), PackedInts.DEFAULT);
+    long currentPosition = 0;
+    for (BytesRef v : values) {
+      w.add(currentPosition);
+      if (v != null) {
+        currentPosition += v.length;
+      }
+    }
+    // write sentinel
+    assert currentPosition == maxAddress;
+    w.add(currentPosition);
+    w.finish();
+  }
+  
+  private void addFixedDerefBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, int length) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_DEREF.name());
+
+    CodecUtil.writeHeader(data, 
+                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_DAT,
+                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
+    
+    CodecUtil.writeHeader(index, 
+                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_IDX,
+                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
+    
+    // deduplicate
+    TreeSet<BytesRef> dictionary = new TreeSet<>();
+    for (BytesRef v : values) {
+      dictionary.add(v == null ? new BytesRef() : BytesRef.deepCopyOf(v));
+    }
+    
+    /* values */
+    data.writeInt(length);
+    for (BytesRef v : dictionary) {
+      data.writeBytes(v.bytes, v.offset, v.length);
+    }
+    
+    /* ordinals */
+    int valueCount = dictionary.size();
+    assert valueCount > 0;
+    index.writeInt(valueCount);
+    final int maxDoc = state.segmentInfo.getDocCount();
+    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
+
+    for (BytesRef v : values) {
+      if (v == null) {
+        v = new BytesRef();
+      }
+      int ord = dictionary.headSet(v).size();
+      w.add(ord);
+    }
+    w.finish();
+  }
+  
+  private void addVarDerefBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_DEREF.name());
+
+    CodecUtil.writeHeader(data, 
+                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_DAT,
+                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
+    
+    CodecUtil.writeHeader(index, 
+                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_IDX,
+                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
+    
+    // deduplicate
+    TreeSet<BytesRef> dictionary = new TreeSet<>();
+    for (BytesRef v : values) {
+      dictionary.add(v == null ? new BytesRef() : BytesRef.deepCopyOf(v));
+    }
+    
+    /* values */
+    long startPosition = data.getFilePointer();
+    long currentAddress = 0;
+    HashMap<BytesRef,Long> valueToAddress = new HashMap<>();
+    for (BytesRef v : dictionary) {
+      currentAddress = data.getFilePointer() - startPosition;
+      valueToAddress.put(v, currentAddress);
+      writeVShort(data, v.length);
+      data.writeBytes(v.bytes, v.offset, v.length);
+    }
+    
+    /* ordinals */
+    long totalBytes = data.getFilePointer() - startPosition;
+    index.writeLong(totalBytes);
+    final int maxDoc = state.segmentInfo.getDocCount();
+    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(currentAddress), PackedInts.DEFAULT);
+
+    for (BytesRef v : values) {
+      w.add(valueToAddress.get(v == null ? new BytesRef() : v));
+    }
+    w.finish();
+  }
+  
+  // the little vint encoding used for var-deref
+  private static void writeVShort(IndexOutput o, int i) throws IOException {
+    assert i >= 0 && i <= Short.MAX_VALUE;
+    if (i < 128) {
+      o.writeByte((byte)i);
+    } else {
+      o.writeByte((byte) (0x80 | (i >> 8)));
+      o.writeByte((byte) (i & 0xff));
+    }
+  }
+
+  @Override
+  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+    // examine the values to determine best type to use
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    for (BytesRef b : values) {
+      minLength = Math.min(minLength, b.length);
+      maxLength = Math.max(maxLength, b.length);
+    }
+    
+    // but dont use fixed if there are missing values (we are simulating how lucene40 wrote dv...)
+    boolean anyMissing = false;
+    for (Number n : docToOrd) {
+      if (n.longValue() == -1) {
+        anyMissing = true;
+        break;
+      }
+    }
+    
+    boolean success = false;
+    IndexOutput data = null;
+    IndexOutput index = null;
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
+    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
+    
+    try {
+      data = dir.createOutput(dataName, state.context);
+      index = dir.createOutput(indexName, state.context);
+      if (minLength == maxLength && !anyMissing) {
+        // fixed byte[]
+        addFixedSortedBytesField(field, data, index, values, docToOrd, minLength);
+      } else {
+        // var byte[]
+        // three cases for simulating the old writer:
+        // 1. no missing
+        // 2. missing (and empty string in use): remap ord=-1 -> ord=0
+        // 3. missing (and empty string not in use): remap all ords +1, insert empty string into values
+        if (!anyMissing) {
+          addVarSortedBytesField(field, data, index, values, docToOrd);
+        } else if (minLength == 0) {
+          addVarSortedBytesField(field, data, index, values, MissingOrdRemapper.mapMissingToOrd0(docToOrd));
+        } else {
+          addVarSortedBytesField(field, data, index, MissingOrdRemapper.insertEmptyValue(values), MissingOrdRemapper.mapAllOrds(docToOrd));
+        }
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data, index);
+      } else {
+        IOUtils.closeWhileHandlingException(data, index);
+      }
+    }
+  }
+  
+  private void addFixedSortedBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, Iterable<Number> docToOrd, int length) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_SORTED.name());
+
+    CodecUtil.writeHeader(data, 
+                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_DAT,
+                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
+    
+    CodecUtil.writeHeader(index, 
+                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_IDX,
+                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
+    
+    /* values */
+    
+    data.writeInt(length);
+    int valueCount = 0;
+    for (BytesRef v : values) {
+      data.writeBytes(v.bytes, v.offset, v.length);
+      valueCount++;
+    }
+    
+    /* ordinals */
+    
+    index.writeInt(valueCount);
+    int maxDoc = state.segmentInfo.getDocCount();
+    assert valueCount > 0;
+    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
+    for (Number n : docToOrd) {
+      w.add(n.longValue());
+    }
+    w.finish();
+  }
+  
+  private void addVarSortedBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_SORTED.name());
+    
+    CodecUtil.writeHeader(data, 
+                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_DAT,
+                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
+
+    CodecUtil.writeHeader(index, 
+                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_IDX,
+                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
+
+    /* values */
+    
+    final long startPos = data.getFilePointer();
+    
+    int valueCount = 0;
+    for (BytesRef v : values) {
+      data.writeBytes(v.bytes, v.offset, v.length);
+      valueCount++;
+    }
+    
+    /* addresses */
+    
+    final long maxAddress = data.getFilePointer() - startPos;
+    index.writeLong(maxAddress);
+    
+    assert valueCount != Integer.MAX_VALUE; // unsupported by the 4.0 impl
+    
+    final PackedInts.Writer w = PackedInts.getWriter(index, valueCount+1, PackedInts.bitsRequired(maxAddress), PackedInts.DEFAULT);
+    long currentPosition = 0;
+    for (BytesRef v : values) {
+      w.add(currentPosition);
+      currentPosition += v.length;
+    }
+    // write sentinel
+    assert currentPosition == maxAddress;
+    w.add(currentPosition);
+    w.finish();
+    
+    /* ordinals */
+    
+    final int maxDoc = state.segmentInfo.getDocCount();
+    assert valueCount > 0;
+    final PackedInts.Writer ords = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
+    for (Number n : docToOrd) {
+      ords.add(n.longValue());
+    }
+    ords.finish();
+  }
+  
+  @Override
+  public void addSortedNumericField(FieldInfo field, Iterable<Number> docToValueCount, Iterable<Number> values) throws IOException {
+    throw new UnsupportedOperationException("Lucene 4.0 does not support SortedNumeric docvalues");
+  }
+  
+  @Override
+  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrdCount, Iterable<Number> ords) throws IOException {
+    throw new UnsupportedOperationException("Lucene 4.0 does not support SortedSet docvalues");
+  }
+
+  @Override
+  public void close() throws IOException {
+    dir.close();
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java
new file mode 100644
index 0000000..125e0b9
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java
@@ -0,0 +1,104 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosReader.LegacyDocValuesType;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 4.0 FieldInfos writer.
+ * 
+ * @see Lucene40FieldInfosFormat
+ * @lucene.experimental
+ */
+@Deprecated
+public class Lucene40FieldInfosWriter extends FieldInfosWriter {
+
+  /** Sole constructor. */
+  public Lucene40FieldInfosWriter() {
+  }
+  
+  @Override
+  public void write(Directory directory, String segmentName, String segmentSuffix, FieldInfos infos, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene40FieldInfosFormat.FIELD_INFOS_EXTENSION);
+    IndexOutput output = directory.createOutput(fileName, context);
+    boolean success = false;
+    try {
+      CodecUtil.writeHeader(output, Lucene40FieldInfosFormat.CODEC_NAME, Lucene40FieldInfosFormat.FORMAT_CURRENT);
+      output.writeVInt(infos.size());
+      for (FieldInfo fi : infos) {
+        IndexOptions indexOptions = fi.getIndexOptions();
+        byte bits = 0x0;
+        if (fi.hasVectors()) bits |= Lucene40FieldInfosFormat.STORE_TERMVECTOR;
+        if (fi.omitsNorms()) bits |= Lucene40FieldInfosFormat.OMIT_NORMS;
+        if (fi.hasPayloads()) bits |= Lucene40FieldInfosFormat.STORE_PAYLOADS;
+        if (fi.isIndexed()) {
+          bits |= Lucene40FieldInfosFormat.IS_INDEXED;
+          assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
+          if (indexOptions == IndexOptions.DOCS_ONLY) {
+            bits |= Lucene40FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS;
+          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
+            bits |= Lucene40FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS;
+          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
+            bits |= Lucene40FieldInfosFormat.OMIT_POSITIONS;
+          }
+        }
+        output.writeString(fi.name);
+        output.writeVInt(fi.number);
+        output.writeByte(bits);
+
+        // pack the DV types in one byte
+        final byte dv = docValuesByte(fi.getDocValuesType(), fi.getAttribute(Lucene40FieldInfosReader.LEGACY_DV_TYPE_KEY));
+        final byte nrm = docValuesByte(fi.getNormType(), fi.getAttribute(Lucene40FieldInfosReader.LEGACY_NORM_TYPE_KEY));
+        assert (dv & (~0xF)) == 0 && (nrm & (~0x0F)) == 0;
+        byte val = (byte) (0xff & ((nrm << 4) | dv));
+        output.writeByte(val);
+        output.writeStringStringMap(fi.attributes());
+      }
+      success = true;
+    } finally {
+      if (success) {
+        output.close();
+      } else {
+        IOUtils.closeWhileHandlingException(output);
+      }
+    }
+  }
+  
+  /** 4.0-style docvalues byte */
+  public byte docValuesByte(DocValuesType type, String legacyTypeAtt) {
+    if (type == null) {
+      assert legacyTypeAtt == null;
+      return 0;
+    } else {
+      assert legacyTypeAtt != null;
+      return (byte) LegacyDocValuesType.valueOf(legacyTypeAtt).ordinal();
+    }
+  }  
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
new file mode 100644
index 0000000..2c489a4
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
@@ -0,0 +1,321 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Consumes doc & freq, writing them using the current
+ *  index file format */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PushPostingsWriterBase;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsEnum;  // javadocs
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Concrete class that writes the 4.0 frq/prx postings format.
+ * 
+ * @see Lucene40PostingsFormat
+ * @lucene.experimental 
+ */
+public final class Lucene40PostingsWriter extends PushPostingsWriterBase {
+
+  final IndexOutput freqOut;
+  final IndexOutput proxOut;
+  final Lucene40SkipListWriter skipListWriter;
+  /** Expert: The fraction of TermDocs entries stored in skip tables,
+   * used to accelerate {@link DocsEnum#advance(int)}.  Larger values result in
+   * smaller indexes, greater acceleration, but fewer accelerable cases, while
+   * smaller values result in bigger indexes, less acceleration and more
+   * accelerable cases. More detailed experiments would be useful here. */
+  static final int DEFAULT_SKIP_INTERVAL = 16;
+  final int skipInterval;
+  
+  /**
+   * Expert: minimum docFreq to write any skip data at all
+   */
+  final int skipMinimum;
+
+  /** Expert: The maximum number of skip levels. Smaller values result in 
+   * slightly smaller indexes, but slower skipping in big posting lists.
+   */
+  final int maxSkipLevels = 10;
+  final int totalNumDocs;
+
+  // Starts a new term
+  long freqStart;
+  long proxStart;
+  int lastPayloadLength;
+  int lastOffsetLength;
+  int lastPosition;
+  int lastOffset;
+
+  final static StandardTermState emptyState = new StandardTermState();
+  StandardTermState lastState;
+
+  // private String segment;
+
+  /** Creates a {@link Lucene40PostingsWriter}, with the
+   *  {@link #DEFAULT_SKIP_INTERVAL}. */
+  public Lucene40PostingsWriter(SegmentWriteState state) throws IOException {
+    this(state, DEFAULT_SKIP_INTERVAL);
+  }
+  
+  /** Creates a {@link Lucene40PostingsWriter}, with the
+   *  specified {@code skipInterval}. */
+  public Lucene40PostingsWriter(SegmentWriteState state, int skipInterval) throws IOException {
+    super();
+    this.skipInterval = skipInterval;
+    this.skipMinimum = skipInterval; /* set to the same for now */
+    // this.segment = state.segmentName;
+    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION);
+    freqOut = state.directory.createOutput(fileName, state.context);
+    boolean success = false;
+    IndexOutput proxOut = null;
+    try {
+      CodecUtil.writeHeader(freqOut, Lucene40PostingsReader.FRQ_CODEC, Lucene40PostingsReader.VERSION_CURRENT);
+      // TODO: this is a best effort, if one of these fields has no postings
+      // then we make an empty prx file, same as if we are wrapped in 
+      // per-field postingsformat. maybe... we shouldn't
+      // bother w/ this opto?  just create empty prx file...?
+      if (state.fieldInfos.hasProx()) {
+        // At least one field does not omit TF, so create the
+        // prox file
+        fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION);
+        proxOut = state.directory.createOutput(fileName, state.context);
+        CodecUtil.writeHeader(proxOut, Lucene40PostingsReader.PRX_CODEC, Lucene40PostingsReader.VERSION_CURRENT);
+      } else {
+        // Every field omits TF so we will write no prox file
+        proxOut = null;
+      }
+      this.proxOut = proxOut;
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(freqOut, proxOut);
+      }
+    }
+
+    totalNumDocs = state.segmentInfo.getDocCount();
+
+    skipListWriter = new Lucene40SkipListWriter(skipInterval,
+                                               maxSkipLevels,
+                                               totalNumDocs,
+                                               freqOut,
+                                               proxOut);
+  }
+
+  @Override
+  public void init(IndexOutput termsOut) throws IOException {
+    CodecUtil.writeHeader(termsOut, Lucene40PostingsReader.TERMS_CODEC, Lucene40PostingsReader.VERSION_CURRENT);
+    termsOut.writeInt(skipInterval);                // write skipInterval
+    termsOut.writeInt(maxSkipLevels);               // write maxSkipLevels
+    termsOut.writeInt(skipMinimum);                 // write skipMinimum
+  }
+
+  @Override
+  public BlockTermState newTermState() {
+    return new StandardTermState();
+  }
+
+  @Override
+  public void startTerm() {
+    freqStart = freqOut.getFilePointer();
+    //if (DEBUG) System.out.println("SPW: startTerm freqOut.fp=" + freqStart);
+    if (proxOut != null) {
+      proxStart = proxOut.getFilePointer();
+    }
+    // force first payload to write its length
+    lastPayloadLength = -1;
+    // force first offset to write its length
+    lastOffsetLength = -1;
+    skipListWriter.resetSkip();
+  }
+
+  // Currently, this instance is re-used across fields, so
+  // our parent calls setField whenever the field changes
+  @Override
+  public int setField(FieldInfo fieldInfo) {
+    super.setField(fieldInfo);
+    //System.out.println("SPW: setField");
+    /*
+    if (BlockTreeTermsWriter.DEBUG && fieldInfo.name.equals("id")) {
+      DEBUG = true;
+    } else {
+      DEBUG = false;
+    }
+    */
+
+    lastState = emptyState;
+    //System.out.println("  set init blockFreqStart=" + freqStart);
+    //System.out.println("  set init blockProxStart=" + proxStart);
+    return 0;
+  }
+
+  int lastDocID;
+  int df;
+
+  @Override
+  public void startDoc(int docID, int termDocFreq) throws IOException {
+    // if (DEBUG) System.out.println("SPW:   startDoc seg=" + segment + " docID=" + docID + " tf=" + termDocFreq + " freqOut.fp=" + freqOut.getFilePointer());
+
+    final int delta = docID - lastDocID;
+    
+    if (docID < 0 || (df > 0 && delta <= 0)) {
+      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " ) (freqOut: " + freqOut + ")");
+    }
+
+    if ((++df % skipInterval) == 0) {
+      skipListWriter.setSkipData(lastDocID, writePayloads, lastPayloadLength, writeOffsets, lastOffsetLength);
+      skipListWriter.bufferSkip(df);
+    }
+
+    assert docID < totalNumDocs: "docID=" + docID + " totalNumDocs=" + totalNumDocs;
+
+    lastDocID = docID;
+    if (indexOptions == IndexOptions.DOCS_ONLY) {
+      freqOut.writeVInt(delta);
+    } else if (1 == termDocFreq) {
+      freqOut.writeVInt((delta<<1) | 1);
+    } else {
+      freqOut.writeVInt(delta<<1);
+      freqOut.writeVInt(termDocFreq);
+    }
+
+    lastPosition = 0;
+    lastOffset = 0;
+  }
+
+  /** Add a new position & payload */
+  @Override
+  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
+    //if (DEBUG) System.out.println("SPW:     addPos pos=" + position + " payload=" + (payload == null ? "null" : (payload.length + " bytes")) + " proxFP=" + proxOut.getFilePointer());
+    assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 : "invalid indexOptions: " + indexOptions;
+    assert proxOut != null;
+
+    final int delta = position - lastPosition;
+    
+    assert delta >= 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
+
+    lastPosition = position;
+
+    int payloadLength = 0;
+
+    if (writePayloads) {
+      payloadLength = payload == null ? 0 : payload.length;
+
+      if (payloadLength != lastPayloadLength) {
+        lastPayloadLength = payloadLength;
+        proxOut.writeVInt((delta<<1)|1);
+        proxOut.writeVInt(payloadLength);
+      } else {
+        proxOut.writeVInt(delta << 1);
+      }
+    } else {
+      proxOut.writeVInt(delta);
+    }
+    
+    if (writeOffsets) {
+      // don't use startOffset - lastEndOffset, because this creates lots of negative vints for synonyms,
+      // and the numbers aren't that much smaller anyways.
+      int offsetDelta = startOffset - lastOffset;
+      int offsetLength = endOffset - startOffset;
+      assert offsetDelta >= 0 && offsetLength >= 0 : "startOffset=" + startOffset + ",lastOffset=" + lastOffset + ",endOffset=" + endOffset;
+      if (offsetLength != lastOffsetLength) {
+        proxOut.writeVInt(offsetDelta << 1 | 1);
+        proxOut.writeVInt(offsetLength);
+      } else {
+        proxOut.writeVInt(offsetDelta << 1);
+      }
+      lastOffset = startOffset;
+      lastOffsetLength = offsetLength;
+    }
+    
+    if (payloadLength > 0) {
+      proxOut.writeBytes(payload.bytes, payload.offset, payloadLength);
+    }
+  }
+
+  @Override
+  public void finishDoc() {
+  }
+
+  private static class StandardTermState extends BlockTermState {
+    public long freqStart;
+    public long proxStart;
+    public long skipOffset;
+  }
+
+  /** Called when we are done adding docs to this term */
+  @Override
+  public void finishTerm(BlockTermState _state) throws IOException {
+    StandardTermState state = (StandardTermState) _state;
+    // if (DEBUG) System.out.println("SPW: finishTerm seg=" + segment + " freqStart=" + freqStart);
+    assert state.docFreq > 0;
+
+    // TODO: wasteful we are counting this (counting # docs
+    // for this term) in two places?
+    assert state.docFreq == df;
+    state.freqStart = freqStart;
+    state.proxStart = proxStart;
+    if (df >= skipMinimum) {
+      state.skipOffset = skipListWriter.writeSkip(freqOut)-freqStart;
+    } else {
+      state.skipOffset = -1;
+    }
+    lastDocID = 0;
+    df = 0;
+  }
+
+  @Override
+  public void encodeTerm(long[] empty, DataOutput out, FieldInfo fieldInfo, BlockTermState _state, boolean absolute) throws IOException {
+    StandardTermState state = (StandardTermState)_state;
+    if (absolute) {
+      lastState = emptyState;
+    }
+    out.writeVLong(state.freqStart - lastState.freqStart);
+    if (state.skipOffset != -1) {
+      assert state.skipOffset > 0;
+      out.writeVLong(state.skipOffset);
+    }
+    if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
+      out.writeVLong(state.proxStart - lastState.proxStart);
+    }
+    lastState = state;
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      freqOut.close();
+    } finally {
+      if (proxOut != null) {
+        proxOut.close();
+      }
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java
new file mode 100644
index 0000000..2052ae2
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java
@@ -0,0 +1,74 @@
+package org.apache.lucene.codecs.lucene40;
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.util.LuceneTestCase;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Read-write version of Lucene40Codec for testing */
+@SuppressWarnings("deprecation")
+public final class Lucene40RWCodec extends Lucene40Codec {
+  
+  private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat() {
+    @Override
+    public FieldInfosWriter getFieldInfosWriter() throws IOException {
+      if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+        return super.getFieldInfosWriter();
+      } else {
+        return new Lucene40FieldInfosWriter();
+      }
+    }
+  };
+  
+  private final DocValuesFormat docValues = new Lucene40RWDocValuesFormat();
+  private final NormsFormat norms = new Lucene40RWNormsFormat();
+  private final StoredFieldsFormat stored = new Lucene40RWStoredFieldsFormat();
+  private final TermVectorsFormat vectors = new Lucene40RWTermVectorsFormat();
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfos;
+  }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValues;
+  }
+
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return stored;
+  }
+
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectors;
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java
new file mode 100644
index 0000000..64b7f02
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java
@@ -0,0 +1,42 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.LuceneTestCase;
+
+/** Read-write version of {@link Lucene40DocValuesFormat} for testing */
+@SuppressWarnings("deprecation")
+public class Lucene40RWDocValuesFormat extends Lucene40DocValuesFormat {
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+      return super.fieldsConsumer(state);
+    } else {
+      String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+          "dv", 
+          IndexFileNames.COMPOUND_FILE_EXTENSION);
+      return new Lucene40DocValuesWriter(state, filename, Lucene40FieldInfosReader.LEGACY_DV_TYPE_KEY);
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java
new file mode 100644
index 0000000..c7ea1da
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java
@@ -0,0 +1,54 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.LuceneTestCase;
+
+/** Read-write version of {@link Lucene40NormsFormat} for testing */
+@SuppressWarnings("deprecation")
+public class Lucene40RWNormsFormat extends Lucene40NormsFormat {
+
+  @Override
+  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+      return super.normsConsumer(state);
+    } else {
+      String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
+          "nrm", 
+          IndexFileNames.COMPOUND_FILE_EXTENSION);
+      final Lucene40DocValuesWriter impl = new Lucene40DocValuesWriter(state, filename, Lucene40FieldInfosReader.LEGACY_NORM_TYPE_KEY);
+      return new NormsConsumer() {
+        @Override
+        public void addNormsField(FieldInfo field, Iterable<Number> values) throws IOException {
+          impl.addNumericField(field, values);
+        }
+        
+        @Override
+        public void close() throws IOException {
+          impl.close();
+        }
+      };
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java
new file mode 100644
index 0000000..3243744
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java
@@ -0,0 +1,57 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Read-write version of {@link Lucene40PostingsFormat} for testing.
+ */
+@SuppressWarnings("deprecation")
+public class Lucene40RWPostingsFormat extends Lucene40PostingsFormat {
+  
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+      return super.fieldsConsumer(state);
+    } else {
+      PostingsWriterBase docs = new Lucene40PostingsWriter(state);
+      
+      // TODO: should we make the terms index more easily
+      // pluggable?  Ie so that this codec would record which
+      // index impl was used, and switch on loading?
+      // Or... you must make a new Codec for this?
+      boolean success = false;
+      try {
+        FieldsConsumer ret = new BlockTreeTermsWriter(state, docs, minBlockSize, maxBlockSize);
+        success = true;
+        return ret;
+      } finally {
+        if (!success) {
+          docs.close();
+        }
+      }
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java
new file mode 100644
index 0000000..f0d430f
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java
@@ -0,0 +1,41 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.LuceneTestCase;
+
+/** 
+ * Simulates writing Lucene 4.0 Stored Fields Format.
+ */ 
+public class Lucene40RWStoredFieldsFormat extends Lucene40StoredFieldsFormat {
+
+  @Override
+  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
+    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+      throw new UnsupportedOperationException("this codec can only be used for reading");
+    } else {
+      return new Lucene40StoredFieldsWriter(directory, si.name, context);
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java
new file mode 100644
index 0000000..81c2ac3
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java
@@ -0,0 +1,41 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.LuceneTestCase;
+
+/** 
+ * Simulates writing Lucene 4.0 Stored Fields Format.
+ */ 
+public class Lucene40RWTermVectorsFormat extends Lucene40TermVectorsFormat {
+
+  @Override
+  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
+    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+      throw new UnsupportedOperationException("this codec can only be used for reading");
+    } else {
+      return new Lucene40TermVectorsWriter(directory, segmentInfo.name, context);
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java
new file mode 100644
index 0000000..62bd304
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java
@@ -0,0 +1,153 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.codecs.MultiLevelSkipListWriter;
+
+
+/**
+ * Implements the skip list writer for the 4.0 posting list format
+ * that stores positions and payloads.
+ * 
+ * @see Lucene40PostingsFormat
+ * @deprecated Only for reading old 4.0 segments
+ */
+@Deprecated
+public class Lucene40SkipListWriter extends MultiLevelSkipListWriter {
+  private int[] lastSkipDoc;
+  private int[] lastSkipPayloadLength;
+  private int[] lastSkipOffsetLength;
+  private long[] lastSkipFreqPointer;
+  private long[] lastSkipProxPointer;
+  
+  private IndexOutput freqOutput;
+  private IndexOutput proxOutput;
+
+  private int curDoc;
+  private boolean curStorePayloads;
+  private boolean curStoreOffsets;
+  private int curPayloadLength;
+  private int curOffsetLength;
+  private long curFreqPointer;
+  private long curProxPointer;
+
+  /** Sole constructor. */
+  public Lucene40SkipListWriter(int skipInterval, int numberOfSkipLevels, int docCount, IndexOutput freqOutput, IndexOutput proxOutput) {
+    super(skipInterval, numberOfSkipLevels, docCount);
+    this.freqOutput = freqOutput;
+    this.proxOutput = proxOutput;
+    
+    lastSkipDoc = new int[numberOfSkipLevels];
+    lastSkipPayloadLength = new int[numberOfSkipLevels];
+    lastSkipOffsetLength = new int[numberOfSkipLevels];
+    lastSkipFreqPointer = new long[numberOfSkipLevels];
+    lastSkipProxPointer = new long[numberOfSkipLevels];
+  }
+
+  /**
+   * Sets the values for the current skip data. 
+   */
+  public void setSkipData(int doc, boolean storePayloads, int payloadLength, boolean storeOffsets, int offsetLength) {
+    assert storePayloads || payloadLength == -1;
+    assert storeOffsets  || offsetLength == -1;
+    this.curDoc = doc;
+    this.curStorePayloads = storePayloads;
+    this.curPayloadLength = payloadLength;
+    this.curStoreOffsets = storeOffsets;
+    this.curOffsetLength = offsetLength;
+    this.curFreqPointer = freqOutput.getFilePointer();
+    if (proxOutput != null)
+      this.curProxPointer = proxOutput.getFilePointer();
+  }
+
+  @Override
+  public void resetSkip() {
+    super.resetSkip();
+    Arrays.fill(lastSkipDoc, 0);
+    Arrays.fill(lastSkipPayloadLength, -1);  // we don't have to write the first length in the skip list
+    Arrays.fill(lastSkipOffsetLength, -1);  // we don't have to write the first length in the skip list
+    Arrays.fill(lastSkipFreqPointer, freqOutput.getFilePointer());
+    if (proxOutput != null)
+      Arrays.fill(lastSkipProxPointer, proxOutput.getFilePointer());
+  }
+  
+  @Override
+  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
+    // To efficiently store payloads/offsets in the posting lists we do not store the length of
+    // every payload/offset. Instead we omit the length if the previous lengths were the same
+    //
+    // However, in order to support skipping, the length at every skip point must be known.
+    // So we use the same length encoding that we use for the posting lists for the skip data as well:
+    // Case 1: current field does not store payloads/offsets
+    //           SkipDatum                 --> DocSkip, FreqSkip, ProxSkip
+    //           DocSkip,FreqSkip,ProxSkip --> VInt
+    //           DocSkip records the document number before every SkipInterval th  document in TermFreqs. 
+    //           Document numbers are represented as differences from the previous value in the sequence.
+    // Case 2: current field stores payloads/offsets
+    //           SkipDatum                 --> DocSkip, PayloadLength?,OffsetLength?,FreqSkip,ProxSkip
+    //           DocSkip,FreqSkip,ProxSkip --> VInt
+    //           PayloadLength,OffsetLength--> VInt    
+    //         In this case DocSkip/2 is the difference between
+    //         the current and the previous value. If DocSkip
+    //         is odd, then a PayloadLength encoded as VInt follows,
+    //         if DocSkip is even, then it is assumed that the
+    //         current payload/offset lengths equals the lengths at the previous
+    //         skip point
+    int delta = curDoc - lastSkipDoc[level];
+    
+    if (curStorePayloads || curStoreOffsets) {
+      assert curStorePayloads || curPayloadLength == lastSkipPayloadLength[level];
+      assert curStoreOffsets  || curOffsetLength == lastSkipOffsetLength[level];
+
+      if (curPayloadLength == lastSkipPayloadLength[level] && curOffsetLength == lastSkipOffsetLength[level]) {
+        // the current payload/offset lengths equals the lengths at the previous skip point,
+        // so we don't store the lengths again
+        skipBuffer.writeVInt(delta << 1);
+      } else {
+        // the payload and/or offset length is different from the previous one. We shift the DocSkip, 
+        // set the lowest bit and store the current payload and/or offset lengths as VInts.
+        skipBuffer.writeVInt(delta << 1 | 1);
+
+        if (curStorePayloads) {
+          skipBuffer.writeVInt(curPayloadLength);
+          lastSkipPayloadLength[level] = curPayloadLength;
+        }
+        if (curStoreOffsets) {
+          skipBuffer.writeVInt(curOffsetLength);
+          lastSkipOffsetLength[level] = curOffsetLength;
+        }
+      }
+    } else {
+      // current field does not store payloads or offsets
+      skipBuffer.writeVInt(delta);
+    }
+
+    skipBuffer.writeVInt((int) (curFreqPointer - lastSkipFreqPointer[level]));
+    skipBuffer.writeVInt((int) (curProxPointer - lastSkipProxPointer[level]));
+
+    lastSkipDoc[level] = curDoc;
+    
+    lastSkipFreqPointer[level] = curFreqPointer;
+    lastSkipProxPointer[level] = curProxPointer;
+  }
+
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java
new file mode 100644
index 0000000..0b8b350
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java
@@ -0,0 +1,188 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you may not
+ * use this file except in compliance with the License. You may obtain a copy of
+ * the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.StorableField;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsReader.*;
+
+
+/** 
+ * Class responsible for writing stored document fields.
+ * <p/>
+ * It uses &lt;segment&gt;.fdt and &lt;segment&gt;.fdx; files.
+ * 
+ * @see Lucene40StoredFieldsFormat
+ * @lucene.experimental 
+ */
+public final class Lucene40StoredFieldsWriter extends StoredFieldsWriter {
+
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput fieldsStream;
+  private IndexOutput indexStream;
+  private final RAMOutputStream fieldsBuffer = new RAMOutputStream();
+
+  /** Sole constructor. */
+  public Lucene40StoredFieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    assert directory != null;
+    this.directory = directory;
+    this.segment = segment;
+
+    boolean success = false;
+    try {
+      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
+      indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION), context);
+
+      CodecUtil.writeHeader(fieldsStream, CODEC_NAME_DAT, VERSION_CURRENT);
+      CodecUtil.writeHeader(indexStream, CODEC_NAME_IDX, VERSION_CURRENT);
+      assert HEADER_LENGTH_DAT == fieldsStream.getFilePointer();
+      assert HEADER_LENGTH_IDX == indexStream.getFilePointer();
+      success = true;
+    } finally {
+      if (!success) {
+        abort();
+      }
+    }
+  }
+
+  int numStoredFields;
+
+  // Writes the contents of buffer into the fields stream
+  // and adds a new entry for this document into the index
+  // stream.  This assumes the buffer was already written
+  // in the correct fields format.
+  @Override
+  public void startDocument() throws IOException {
+    indexStream.writeLong(fieldsStream.getFilePointer());
+  }
+
+  @Override
+  public void finishDocument() throws IOException {
+    fieldsStream.writeVInt(numStoredFields);
+    fieldsBuffer.writeTo(fieldsStream);
+    fieldsBuffer.reset();
+    numStoredFields = 0;
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(fieldsStream, indexStream);
+    } finally {
+      fieldsStream = indexStream = null;
+    }
+  }
+
+  @Override
+  public void abort() {
+    try {
+      close();
+    } catch (Throwable ignored) {}
+    IOUtils.deleteFilesIgnoringExceptions(directory,
+        IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION),
+        IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION));
+  }
+
+  @Override
+  public void writeField(FieldInfo info, StorableField field) throws IOException {
+    numStoredFields++;
+
+    fieldsBuffer.writeVInt(info.number);
+    int bits = 0;
+    final BytesRef bytes;
+    final String string;
+    // TODO: maybe a field should serialize itself?
+    // this way we don't bake into indexer all these
+    // specific encodings for different fields?  and apps
+    // can customize...
+
+    Number number = field.numericValue();
+    if (number != null) {
+      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
+        bits |= FIELD_IS_NUMERIC_INT;
+      } else if (number instanceof Long) {
+        bits |= FIELD_IS_NUMERIC_LONG;
+      } else if (number instanceof Float) {
+        bits |= FIELD_IS_NUMERIC_FLOAT;
+      } else if (number instanceof Double) {
+        bits |= FIELD_IS_NUMERIC_DOUBLE;
+      } else {
+        throw new IllegalArgumentException("cannot store numeric type " + number.getClass());
+      }
+      string = null;
+      bytes = null;
+    } else {
+      bytes = field.binaryValue();
+      if (bytes != null) {
+        bits |= FIELD_IS_BINARY;
+        string = null;
+      } else {
+        string = field.stringValue();
+        if (string == null) {
+          throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
+        }
+      }
+    }
+
+    fieldsBuffer.writeByte((byte) bits);
+
+    if (bytes != null) {
+      fieldsBuffer.writeVInt(bytes.length);
+      fieldsBuffer.writeBytes(bytes.bytes, bytes.offset, bytes.length);
+    } else if (string != null) {
+      fieldsBuffer.writeString(field.stringValue());
+    } else {
+      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
+        fieldsBuffer.writeInt(number.intValue());
+      } else if (number instanceof Long) {
+        fieldsBuffer.writeLong(number.longValue());
+      } else if (number instanceof Float) {
+        fieldsBuffer.writeInt(Float.floatToIntBits(number.floatValue()));
+      } else if (number instanceof Double) {
+        fieldsBuffer.writeLong(Double.doubleToLongBits(number.doubleValue()));
+      } else {
+        throw new AssertionError("Cannot get here");
+      }
+    }
+  }
+
+  @Override
+  public void finish(FieldInfos fis, int numDocs) {
+    long indexFP = indexStream.getFilePointer();
+    if (HEADER_LENGTH_IDX+((long) numDocs)*8 != indexFP)
+      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
+      // we detect that the bug has struck, here, and
+      // throw an exception to prevent the corruption from
+      // entering the index.  See LUCENE-1282 for
+      // details.
+      throw new RuntimeException("fdx size mismatch: docCount is " + numDocs + " but fdx file size is " + indexFP + " (wrote numDocs=" + ((indexFP-HEADER_LENGTH_IDX)/8.0) + " file=" + indexStream.toString() + "; now aborting this merge to prevent index corruption");
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java
new file mode 100644
index 0000000..5b5b301
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java
@@ -0,0 +1,303 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefBuilder;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+
+import static org.apache.lucene.codecs.lucene40.Lucene40TermVectorsReader.*;
+
+
+// TODO: make a new 4.0 TV format that encodes better
+//   - use startOffset (not endOffset) as base for delta on
+//     next startOffset because today for syns or ngrams or
+//     WDF or shingles etc. we are encoding negative vints
+//     (= slow, 5 bytes per)
+//   - if doc has no term vectors, write 0 into the tvx
+//     file; saves a seek to tvd only to read a 0 vint (and
+//     saves a byte in tvd)
+
+/**
+ * Lucene 4.0 Term Vectors writer.
+ * <p>
+ * It writes .tvd, .tvf, and .tvx files.
+ * 
+ * @see Lucene40TermVectorsFormat
+ */
+public final class Lucene40TermVectorsWriter extends TermVectorsWriter {
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput tvx = null, tvd = null, tvf = null;
+  
+  /** Sole constructor. */
+  public Lucene40TermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    this.directory = directory;
+    this.segment = segment;
+    boolean success = false;
+    try {
+      // Open files for TermVector storage
+      tvx = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION), context);
+      CodecUtil.writeHeader(tvx, CODEC_NAME_INDEX, VERSION_CURRENT);
+      tvd = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_DOCUMENTS_EXTENSION), context);
+      CodecUtil.writeHeader(tvd, CODEC_NAME_DOCS, VERSION_CURRENT);
+      tvf = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_FIELDS_EXTENSION), context);
+      CodecUtil.writeHeader(tvf, CODEC_NAME_FIELDS, VERSION_CURRENT);
+      assert HEADER_LENGTH_INDEX == tvx.getFilePointer();
+      assert HEADER_LENGTH_DOCS == tvd.getFilePointer();
+      assert HEADER_LENGTH_FIELDS == tvf.getFilePointer();
+      success = true;
+    } finally {
+      if (!success) {
+        abort();
+      }
+    }
+  }
+ 
+  @Override
+  public void startDocument(int numVectorFields) throws IOException {
+    lastFieldName = null;
+    this.numVectorFields = numVectorFields;
+    tvx.writeLong(tvd.getFilePointer());
+    tvx.writeLong(tvf.getFilePointer());
+    tvd.writeVInt(numVectorFields);
+    fieldCount = 0;
+    fps = ArrayUtil.grow(fps, numVectorFields);
+  }
+  
+  private long fps[] = new long[10]; // pointers to the tvf before writing each field 
+  private int fieldCount = 0;        // number of fields we have written so far for this document
+  private int numVectorFields = 0;   // total number of fields we will write for this document
+  private String lastFieldName;
+
+  @Override
+  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets, boolean payloads) throws IOException {
+    assert lastFieldName == null || info.name.compareTo(lastFieldName) > 0: "fieldName=" + info.name + " lastFieldName=" + lastFieldName;
+    lastFieldName = info.name;
+    this.positions = positions;
+    this.offsets = offsets;
+    this.payloads = payloads;
+    lastTerm.clear();
+    lastPayloadLength = -1; // force first payload to write its length
+    fps[fieldCount++] = tvf.getFilePointer();
+    tvd.writeVInt(info.number);
+    tvf.writeVInt(numTerms);
+    byte bits = 0x0;
+    if (positions)
+      bits |= Lucene40TermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR;
+    if (offsets)
+      bits |= Lucene40TermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR;
+    if (payloads)
+      bits |= Lucene40TermVectorsReader.STORE_PAYLOAD_WITH_TERMVECTOR;
+    tvf.writeByte(bits);
+  }
+  
+  @Override
+  public void finishDocument() throws IOException {
+    assert fieldCount == numVectorFields;
+    for (int i = 1; i < fieldCount; i++) {
+      tvd.writeVLong(fps[i] - fps[i-1]);
+    }
+  }
+
+  private final BytesRefBuilder lastTerm = new BytesRefBuilder();
+
+  // NOTE: we override addProx, so we don't need to buffer when indexing.
+  // we also don't buffer during bulk merges.
+  private int offsetStartBuffer[] = new int[10];
+  private int offsetEndBuffer[] = new int[10];
+  private BytesRefBuilder payloadData = new BytesRefBuilder();
+  private int bufferedIndex = 0;
+  private int bufferedFreq = 0;
+  private boolean positions = false;
+  private boolean offsets = false;
+  private boolean payloads = false;
+
+  @Override
+  public void startTerm(BytesRef term, int freq) throws IOException {
+    final int prefix = StringHelper.bytesDifference(lastTerm.get(), term);
+    final int suffix = term.length - prefix;
+    tvf.writeVInt(prefix);
+    tvf.writeVInt(suffix);
+    tvf.writeBytes(term.bytes, term.offset + prefix, suffix);
+    tvf.writeVInt(freq);
+    lastTerm.copyBytes(term);
+    lastPosition = lastOffset = 0;
+    
+    if (offsets && positions) {
+      // we might need to buffer if its a non-bulk merge
+      offsetStartBuffer = ArrayUtil.grow(offsetStartBuffer, freq);
+      offsetEndBuffer = ArrayUtil.grow(offsetEndBuffer, freq);
+    }
+    bufferedIndex = 0;
+    bufferedFreq = freq;
+    payloadData.clear();
+  }
+
+  int lastPosition = 0;
+  int lastOffset = 0;
+  int lastPayloadLength = -1; // force first payload to write its length
+
+  BytesRefBuilder scratch = new BytesRefBuilder(); // used only by this optimized flush below
+
+  @Override
+  public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
+    if (payloads) {
+      // TODO, maybe overkill and just call super.addProx() in this case?
+      // we do avoid buffering the offsets in RAM though.
+      for (int i = 0; i < numProx; i++) {
+        int code = positions.readVInt();
+        if ((code & 1) == 1) {
+          int length = positions.readVInt();
+          scratch.grow(length);
+          scratch.setLength(length);
+          positions.readBytes(scratch.bytes(), 0, scratch.length());
+          writePosition(code >>> 1, scratch.get());
+        } else {
+          writePosition(code >>> 1, null);
+        }
+      }
+      tvf.writeBytes(payloadData.bytes(), 0, payloadData.length());
+    } else if (positions != null) {
+      // pure positions, no payloads
+      for (int i = 0; i < numProx; i++) {
+        tvf.writeVInt(positions.readVInt() >>> 1);
+      }
+    }
+    
+    if (offsets != null) {
+      for (int i = 0; i < numProx; i++) {
+        tvf.writeVInt(offsets.readVInt());
+        tvf.writeVInt(offsets.readVInt());
+      }
+    }
+  }
+
+  @Override
+  public void addPosition(int position, int startOffset, int endOffset, BytesRef payload) throws IOException {
+    if (positions && (offsets || payloads)) {
+      // write position delta
+      writePosition(position - lastPosition, payload);
+      lastPosition = position;
+      
+      // buffer offsets
+      if (offsets) {
+        offsetStartBuffer[bufferedIndex] = startOffset;
+        offsetEndBuffer[bufferedIndex] = endOffset;
+      }
+      
+      bufferedIndex++;
+    } else if (positions) {
+      // write position delta
+      writePosition(position - lastPosition, payload);
+      lastPosition = position;
+    } else if (offsets) {
+      // write offset deltas
+      tvf.writeVInt(startOffset - lastOffset);
+      tvf.writeVInt(endOffset - startOffset);
+      lastOffset = endOffset;
+    }
+  }
+  
+  @Override
+  public void finishTerm() throws IOException {
+    if (bufferedIndex > 0) {
+      // dump buffer
+      assert positions && (offsets || payloads);
+      assert bufferedIndex == bufferedFreq;
+      if (payloads) {
+        tvf.writeBytes(payloadData.bytes(), 0, payloadData.length());
+      }
+      if (offsets) {
+        for (int i = 0; i < bufferedIndex; i++) {
+          tvf.writeVInt(offsetStartBuffer[i] - lastOffset);
+          tvf.writeVInt(offsetEndBuffer[i] - offsetStartBuffer[i]);
+          lastOffset = offsetEndBuffer[i];
+        }
+      }
+    }
+  }
+
+  private void writePosition(int delta, BytesRef payload) throws IOException {
+    if (payloads) {
+      int payloadLength = payload == null ? 0 : payload.length;
+
+      if (payloadLength != lastPayloadLength) {
+        lastPayloadLength = payloadLength;
+        tvf.writeVInt((delta<<1)|1);
+        tvf.writeVInt(payloadLength);
+      } else {
+        tvf.writeVInt(delta << 1);
+      }
+      if (payloadLength > 0) {
+        if (payloadLength + payloadData.length() < 0) {
+          // we overflowed the payload buffer, just throw UOE
+          // having > Integer.MAX_VALUE bytes of payload for a single term in a single doc is nuts.
+          throw new UnsupportedOperationException("A term cannot have more than Integer.MAX_VALUE bytes of payload data in a single document");
+        }
+        payloadData.append(payload);
+      }
+    } else {
+      tvf.writeVInt(delta);
+    }
+  }
+
+  @Override
+  public void abort() {
+    try {
+      close();
+    } catch (Throwable ignored) {}
+    IOUtils.deleteFilesIgnoringExceptions(directory, IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION),
+        IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_DOCUMENTS_EXTENSION),
+        IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_FIELDS_EXTENSION));
+  }
+  
+  @Override
+  public void finish(FieldInfos fis, int numDocs) {
+    long indexFP = tvx.getFilePointer();
+    if (HEADER_LENGTH_INDEX+((long) numDocs)*16 != indexFP)
+      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
+      // we detect that the bug has struck, here, and
+      // throw an exception to prevent the corruption from
+      // entering the index.  See LUCENE-1282 for
+      // details.
+      throw new RuntimeException("tvx size mismatch: mergedDocs is " + numDocs + " but tvx size is " + indexFP + " (wrote numDocs=" + ((indexFP - HEADER_LENGTH_INDEX)/16.0) + " file=" + tvx.toString() + "; now aborting this merge to prevent index corruption");
+  }
+
+  /** Close all streams. */
+  @Override
+  public void close() throws IOException {
+    // make an effort to close all streams we can but remember and re-throw
+    // the first exception encountered in this process
+    IOUtils.close(tvx, tvd, tvf);
+    tvx = tvd = tvf = null;
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java
new file mode 100644
index 0000000..7e09e86
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java
@@ -0,0 +1,283 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * <code>TestBitVector</code> tests the <code>BitVector</code>, obviously.
+ */
+public class TestBitVector extends LuceneTestCase
+{
+
+    /**
+     * Test the default constructor on BitVectors of various sizes.
+     */
+    public void testConstructSize() throws Exception {
+        doTestConstructOfSize(8);
+        doTestConstructOfSize(20);
+        doTestConstructOfSize(100);
+        doTestConstructOfSize(1000);
+    }
+
+    private void doTestConstructOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        assertEquals(n,bv.size());
+    }
+
+    /**
+     * Test the get() and set() methods on BitVectors of various sizes.
+     */
+    public void testGetSet() throws Exception {
+        doTestGetSetVectorOfSize(8);
+        doTestGetSetVectorOfSize(20);
+        doTestGetSetVectorOfSize(100);
+        doTestGetSetVectorOfSize(1000);
+    }
+
+    private void doTestGetSetVectorOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        for(int i=0;i<bv.size();i++) {
+            // ensure a set bit can be git'
+            assertFalse(bv.get(i));
+            bv.set(i);
+            assertTrue(bv.get(i));
+        }
+    }
+
+    /**
+     * Test the clear() method on BitVectors of various sizes.
+     */
+    public void testClear() throws Exception {
+        doTestClearVectorOfSize(8);
+        doTestClearVectorOfSize(20);
+        doTestClearVectorOfSize(100);
+        doTestClearVectorOfSize(1000);
+    }
+
+    private void doTestClearVectorOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        for(int i=0;i<bv.size();i++) {
+            // ensure a set bit is cleared
+            assertFalse(bv.get(i));
+            bv.set(i);
+            assertTrue(bv.get(i));
+            bv.clear(i);
+            assertFalse(bv.get(i));
+        }
+    }
+
+    /**
+     * Test the count() method on BitVectors of various sizes.
+     */
+    public void testCount() throws Exception {
+        doTestCountVectorOfSize(8);
+        doTestCountVectorOfSize(20);
+        doTestCountVectorOfSize(100);
+        doTestCountVectorOfSize(1000);
+    }
+
+    private void doTestCountVectorOfSize(int n) {
+        BitVector bv = new BitVector(n);
+        // test count when incrementally setting bits
+        for(int i=0;i<bv.size();i++) {
+            assertFalse(bv.get(i));
+            assertEquals(i,bv.count());
+            bv.set(i);
+            assertTrue(bv.get(i));
+            assertEquals(i+1,bv.count());
+        }
+
+        bv = new BitVector(n);
+        // test count when setting then clearing bits
+        for(int i=0;i<bv.size();i++) {
+            assertFalse(bv.get(i));
+            assertEquals(0,bv.count());
+            bv.set(i);
+            assertTrue(bv.get(i));
+            assertEquals(1,bv.count());
+            bv.clear(i);
+            assertFalse(bv.get(i));
+            assertEquals(0,bv.count());
+        }
+    }
+
+    /**
+     * Test writing and construction to/from Directory.
+     */
+    public void testWriteRead() throws Exception {
+        doTestWriteRead(8);
+        doTestWriteRead(20);
+        doTestWriteRead(100);
+        doTestWriteRead(1000);
+    }
+
+    private void doTestWriteRead(int n) throws Exception {
+        MockDirectoryWrapper d = new  MockDirectoryWrapper(random(), new RAMDirectory());
+        d.setPreventDoubleWrite(false);
+        BitVector bv = new BitVector(n);
+        // test count when incrementally setting bits
+        for(int i=0;i<bv.size();i++) {
+            assertFalse(bv.get(i));
+            assertEquals(i,bv.count());
+            bv.set(i);
+            assertTrue(bv.get(i));
+            assertEquals(i+1,bv.count());
+            bv.write(d, "TESTBV", newIOContext(random()));
+            BitVector compare = new BitVector(d, "TESTBV", newIOContext(random()));
+            // compare bit vectors with bits set incrementally
+            assertTrue(doCompare(bv,compare));
+        }
+    }
+    
+    /**
+     * Test r/w when size/count cause switching between bit-set and d-gaps file formats.  
+     */
+    public void testDgaps() throws IOException {
+      doTestDgaps(1,0,1);
+      doTestDgaps(10,0,1);
+      doTestDgaps(100,0,1);
+      doTestDgaps(1000,4,7);
+      doTestDgaps(10000,40,43);
+      doTestDgaps(100000,415,418);
+      doTestDgaps(1000000,3123,3126);
+      // now exercise skipping of fully populated byte in the bitset (they are omitted if bitset is sparse)
+      MockDirectoryWrapper d = new  MockDirectoryWrapper(random(), new RAMDirectory());
+      d.setPreventDoubleWrite(false);
+      BitVector bv = new BitVector(10000);
+      bv.set(0);
+      for (int i = 8; i < 16; i++) {
+        bv.set(i);
+      } // make sure we have once byte full of set bits
+      for (int i = 32; i < 40; i++) {
+        bv.set(i);
+      } // get a second byte full of set bits
+      // add some more bits here 
+      for (int i = 40; i < 10000; i++) {
+        if (random().nextInt(1000) == 0) {
+          bv.set(i);
+        }
+      }
+      bv.write(d, "TESTBV", newIOContext(random()));
+      BitVector compare = new BitVector(d, "TESTBV", newIOContext(random()));
+      assertTrue(doCompare(bv,compare));
+    }
+    
+    private void doTestDgaps(int size, int count1, int count2) throws IOException {
+      MockDirectoryWrapper d = new  MockDirectoryWrapper(random(), new RAMDirectory());
+      d.setPreventDoubleWrite(false);
+      BitVector bv = new BitVector(size);
+      bv.invertAll();
+      for (int i=0; i<count1; i++) {
+        bv.clear(i);
+        assertEquals(i+1,size-bv.count());
+      }
+      bv.write(d, "TESTBV", newIOContext(random()));
+      // gradually increase number of set bits
+      for (int i=count1; i<count2; i++) {
+        BitVector bv2 = new BitVector(d, "TESTBV", newIOContext(random()));
+        assertTrue(doCompare(bv,bv2));
+        bv = bv2;
+        bv.clear(i);
+        assertEquals(i+1, size-bv.count());
+        bv.write(d, "TESTBV", newIOContext(random()));
+      }
+      // now start decreasing number of set bits
+      for (int i=count2-1; i>=count1; i--) {
+        BitVector bv2 = new BitVector(d, "TESTBV", newIOContext(random()));
+        assertTrue(doCompare(bv,bv2));
+        bv = bv2;
+        bv.set(i);
+        assertEquals(i,size-bv.count());
+        bv.write(d, "TESTBV", newIOContext(random()));
+      }
+    }
+
+    public void testSparseWrite() throws IOException {
+      Directory d = newDirectory();
+      final int numBits = 10240;
+      BitVector bv = new BitVector(numBits);
+      bv.invertAll();
+      int numToClear = random().nextInt(5);
+      for(int i=0;i<numToClear;i++) {
+        bv.clear(random().nextInt(numBits));
+      }
+      bv.write(d, "test", newIOContext(random()));
+      final long size = d.fileLength("test");
+      assertTrue("size=" + size, size < 100);
+      d.close();
+    }
+
+    public void testClearedBitNearEnd() throws IOException {
+      Directory d = newDirectory();
+      final int numBits = TestUtil.nextInt(random(), 7, 1000);
+      BitVector bv = new BitVector(numBits);
+      bv.invertAll();
+      bv.clear(numBits- TestUtil.nextInt(random(), 1, 7));
+      bv.write(d, "test", newIOContext(random()));
+      assertEquals(numBits-1, bv.count());
+      d.close();
+    }
+
+    public void testMostlySet() throws IOException {
+      Directory d = newDirectory();
+      final int numBits = TestUtil.nextInt(random(), 30, 1000);
+      for(int numClear=0;numClear<20;numClear++) {
+        BitVector bv = new BitVector(numBits);
+        bv.invertAll();
+        int count = 0;
+        while(count < numClear) {
+          final int bit = random().nextInt(numBits);
+          // Don't use getAndClear, so that count is recomputed
+          if (bv.get(bit)) {
+            bv.clear(bit);
+            count++;
+            assertEquals(numBits-count, bv.count());
+          }
+        }
+      }
+
+      d.close();
+    }
+
+    /**
+     * Compare two BitVectors.
+     * This should really be an equals method on the BitVector itself.
+     * @param bv One bit vector
+     * @param compare The second to compare
+     */
+    private boolean doCompare(BitVector bv, BitVector compare) {
+        boolean equal = true;
+        for(int i=0;i<bv.size();i++) {
+            // bits must be equal
+            if(bv.get(i)!=compare.get(i)) {
+                equal = false;
+                break;
+            }
+        }
+        assertEquals(bv.count(), compare.count());
+        return equal;
+    }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java
new file mode 100644
index 0000000..048f05a
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java
@@ -0,0 +1,46 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseDocValuesFormatTestCase;
+import org.junit.BeforeClass;
+
+/**
+ * Tests Lucene40DocValuesFormat
+ */
+public class TestLucene40DocValuesFormat extends BaseDocValuesFormatTestCase {
+  private final Codec codec = new Lucene40RWCodec();
+  
+  @BeforeClass
+  public static void beforeClass() {
+    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
+  }
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+
+  // LUCENE-4583: This codec should throw IAE on huge binary values:
+  @Override
+  protected boolean codecAcceptsHugeBinaryValues(String field) {
+    return false;
+  }
+
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40NormsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40NormsFormat.java
new file mode 100644
index 0000000..f3fb65a
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40NormsFormat.java
@@ -0,0 +1,38 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseNormsFormatTestCase;
+import org.junit.BeforeClass;
+
+
+/** Tests Lucene40's norms format */
+public class TestLucene40NormsFormat extends BaseNormsFormatTestCase {
+  final Codec codec = new Lucene40RWCodec();
+  
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+  
+  @BeforeClass
+  public static void beforeClass() {
+    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java
new file mode 100644
index 0000000..a3deacc
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java
@@ -0,0 +1,40 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BasePostingsFormatTestCase;
+import org.junit.BeforeClass;
+
+/**
+ * Tests Lucene40PostingsFormat
+ */
+public class TestLucene40PostingsFormat extends BasePostingsFormatTestCase {
+  private final Codec codec = new Lucene40RWCodec();
+
+  @BeforeClass
+  public static void beforeClass() {
+    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
+  }
+  
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+  
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java
new file mode 100644
index 0000000..a3f6bc9
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java
@@ -0,0 +1,132 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.Collections;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.junit.BeforeClass;
+
+public class TestLucene40PostingsReader extends LuceneTestCase {
+  static final String terms[] = new String[100];
+  static {
+    for (int i = 0; i < terms.length; i++) {
+      terms[i] = Integer.toString(i+1);
+    }
+  }
+  
+  @BeforeClass
+  public static void beforeClass() {
+    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
+  }
+
+  /** tests terms with different probabilities of being in the document.
+   *  depends heavily on term vectors cross-check at checkIndex
+   */
+  public void testPostings() throws Exception {
+    Directory dir = newFSDirectory(createTempDir("postings"));
+    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
+    iwc.setCodec(Codec.forName("Lucene40"));
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    
+    Document doc = new Document();
+    
+    // id field
+    FieldType idType = new FieldType(StringField.TYPE_NOT_STORED);
+    idType.setStoreTermVectors(true);
+    Field idField = new Field("id", "", idType);
+    doc.add(idField);
+    
+    // title field: short text field
+    FieldType titleType = new FieldType(TextField.TYPE_NOT_STORED);
+    titleType.setStoreTermVectors(true);
+    titleType.setStoreTermVectorPositions(true);
+    titleType.setStoreTermVectorOffsets(true);
+    titleType.setIndexOptions(indexOptions());
+    Field titleField = new Field("title", "", titleType);
+    doc.add(titleField);
+    
+    // body field: long text field
+    FieldType bodyType = new FieldType(TextField.TYPE_NOT_STORED);
+    bodyType.setStoreTermVectors(true);
+    bodyType.setStoreTermVectorPositions(true);
+    bodyType.setStoreTermVectorOffsets(true);
+    bodyType.setIndexOptions(indexOptions());
+    Field bodyField = new Field("body", "", bodyType);
+    doc.add(bodyField);
+    
+    int numDocs = atLeast(1000);
+    for (int i = 0; i < numDocs; i++) {
+      idField.setStringValue(Integer.toString(i));
+      titleField.setStringValue(fieldValue(1));
+      bodyField.setStringValue(fieldValue(3));
+      iw.addDocument(doc);
+      if (random().nextInt(20) == 0) {
+        iw.deleteDocuments(new Term("id", Integer.toString(i)));
+      }
+    }
+    if (random().nextBoolean()) {
+      // delete 1-100% of docs
+      iw.deleteDocuments(new Term("title", terms[random().nextInt(terms.length)]));
+    }
+    iw.close();
+    dir.close(); // checkindex
+  }
+  
+  IndexOptions indexOptions() {
+    switch(random().nextInt(4)) {
+      case 0: return IndexOptions.DOCS_ONLY;
+      case 1: return IndexOptions.DOCS_AND_FREQS;
+      case 2: return IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+      default: return IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
+    }
+  }
+  
+  String fieldValue(int maxTF) {
+    ArrayList<String> shuffled = new ArrayList<>();
+    StringBuilder sb = new StringBuilder();
+    int i = random().nextInt(terms.length);
+    while (i < terms.length) {
+      int tf =  TestUtil.nextInt(random(), 1, maxTF);
+      for (int j = 0; j < tf; j++) {
+        shuffled.add(terms[i]);
+      }
+      i++;
+    }
+    Collections.shuffle(shuffled, random());
+    for (String term : shuffled) {
+      sb.append(term);
+      sb.append(' ');
+    }
+    return sb.toString();
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java
new file mode 100644
index 0000000..2502d89
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java
@@ -0,0 +1,35 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseStoredFieldsFormatTestCase;
+import org.junit.BeforeClass;
+
+public class TestLucene40StoredFieldsFormat extends BaseStoredFieldsFormatTestCase {
+  
+  @BeforeClass
+  public static void beforeClass() {
+    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
+  }
+  
+  @Override
+  protected Codec getCodec() {
+    return new Lucene40RWCodec();
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java
new file mode 100644
index 0000000..e97b3b3
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java
@@ -0,0 +1,36 @@
+package org.apache.lucene.codecs.lucene40;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseTermVectorsFormatTestCase;
+import org.junit.BeforeClass;
+
+public class TestLucene40TermVectorsFormat extends BaseTermVectorsFormatTestCase {
+
+  @BeforeClass
+  public static void beforeClass() {
+    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
+  }
+  
+  @Override
+  protected Codec getCodec() {
+    return new Lucene40RWCodec();
+  }
+  
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
new file mode 100644
index 0000000..9ac520d
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
@@ -0,0 +1,199 @@
+package org.apache.lucene.codecs.lucene40;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.IdentityHashMap;
+import java.util.List;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits.MatchNoBits;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LineFileDocs;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+import org.junit.BeforeClass;
+
+// TODO: really this should be in BaseTestPF or somewhere else? useful test!
+public class TestReuseDocsEnum extends LuceneTestCase {
+
+  @BeforeClass
+  public static void beforeClass() {
+    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
+  }
+  
+  public void testReuseDocsEnumNoReuse() throws IOException {
+    Directory dir = newDirectory();
+    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,
+        newIndexWriterConfig(new MockAnalyzer(random())).setCodec(cp));
+    int numdocs = atLeast(20);
+    createRandomIndex(numdocs, writer, random());
+    writer.commit();
+
+    DirectoryReader open = DirectoryReader.open(dir);
+    for (AtomicReaderContext ctx : open.leaves()) {
+      AtomicReader indexReader = ctx.reader();
+      Terms terms = indexReader.terms("body");
+      TermsEnum iterator = terms.iterator(null);
+      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
+      MatchNoBits bits = new Bits.MatchNoBits(indexReader.maxDoc());
+      while ((iterator.next()) != null) {
+        DocsEnum docs = iterator.docs(random().nextBoolean() ? bits : new Bits.MatchNoBits(indexReader.maxDoc()), null, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
+        enums.put(docs, true);
+      }
+      
+      assertEquals(terms.size(), enums.size());
+    }
+    writer.commit();
+    IOUtils.close(writer, open, dir);
+  }
+  
+  // tests for reuse only if bits are the same either null or the same instance
+  public void testReuseDocsEnumSameBitsOrNull() throws IOException {
+    Directory dir = newDirectory();
+    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,
+        newIndexWriterConfig(new MockAnalyzer(random())).setCodec(cp));
+    int numdocs = atLeast(20);
+    createRandomIndex(numdocs, writer, random());
+    writer.commit();
+
+    DirectoryReader open = DirectoryReader.open(dir);
+    for (AtomicReaderContext ctx : open.leaves()) {
+      Terms terms = ctx.reader().terms("body");
+      TermsEnum iterator = terms.iterator(null);
+      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
+      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());
+      DocsEnum docs = null;
+      while ((iterator.next()) != null) {
+        docs = iterator.docs(bits, docs, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
+        enums.put(docs, true);
+      }
+      
+      assertEquals(1, enums.size());
+      enums.clear();
+      iterator = terms.iterator(null);
+      docs = null;
+      while ((iterator.next()) != null) {
+        docs = iterator.docs(new Bits.MatchNoBits(open.maxDoc()), docs, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
+        enums.put(docs, true);
+      }
+      assertEquals(terms.size(), enums.size());
+      
+      enums.clear();
+      iterator = terms.iterator(null);
+      docs = null;
+      while ((iterator.next()) != null) {
+        docs = iterator.docs(null, docs, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
+        enums.put(docs, true);
+      }
+      assertEquals(1, enums.size());  
+    }
+    writer.close();
+    IOUtils.close(open, dir);
+  }
+  
+  // make sure we never reuse from another reader even if it is the same field & codec etc
+  public void testReuseDocsEnumDifferentReader() throws IOException {
+    Directory dir = newDirectory();
+    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());
+    MockAnalyzer analyzer = new MockAnalyzer(random());
+    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,
+        newIndexWriterConfig(analyzer).setCodec(cp));
+    int numdocs = atLeast(20);
+    createRandomIndex(numdocs, writer, random());
+    writer.commit();
+
+    DirectoryReader firstReader = DirectoryReader.open(dir);
+    DirectoryReader secondReader = DirectoryReader.open(dir);
+    List<AtomicReaderContext> leaves = firstReader.leaves();
+    List<AtomicReaderContext> leaves2 = secondReader.leaves();
+    
+    for (AtomicReaderContext ctx : leaves) {
+      Terms terms = ctx.reader().terms("body");
+      TermsEnum iterator = terms.iterator(null);
+      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
+      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());
+      iterator = terms.iterator(null);
+      DocsEnum docs = null;
+      BytesRef term = null;
+      while ((term = iterator.next()) != null) {
+        docs = iterator.docs(null, randomDocsEnum("body", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
+        enums.put(docs, true);
+      }
+      assertEquals(terms.size(), enums.size());
+      
+      iterator = terms.iterator(null);
+      enums.clear();
+      docs = null;
+      while ((term = iterator.next()) != null) {
+        docs = iterator.docs(bits, randomDocsEnum("body", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
+        enums.put(docs, true);
+      }
+      assertEquals(terms.size(), enums.size());
+    }
+    writer.close();
+    IOUtils.close(firstReader, secondReader, dir);
+  }
+  
+  public DocsEnum randomDocsEnum(String field, BytesRef term, List<AtomicReaderContext> readers, Bits bits) throws IOException {
+    if (random().nextInt(10) == 0) {
+      return null;
+    }
+    AtomicReader indexReader = readers.get(random().nextInt(readers.size())).reader();
+    Terms terms = indexReader.terms(field);
+    if (terms == null) {
+      return null;
+    }
+    TermsEnum iterator = terms.iterator(null);
+    if (iterator.seekExact(term)) {
+      return iterator.docs(bits, null, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
+    }
+    return null;
+  }
+
+  /**
+   * populates a writer with random stuff. this must be fully reproducable with
+   * the seed!
+   */
+  public static void createRandomIndex(int numdocs, RandomIndexWriter writer,
+      Random random) throws IOException {
+    LineFileDocs lineFileDocs = new LineFileDocs(random);
+
+    for (int i = 0; i < numdocs; i++) {
+      writer.addDocument(lineFileDocs.nextDoc());
+    }
+    
+    lineFileDocs.close();
+  }
+
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java
new file mode 100644
index 0000000..c5a1cc9
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java
@@ -0,0 +1,80 @@
+package org.apache.lucene.codecs.lucene41;
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosWriter;
+import org.apache.lucene.codecs.lucene40.Lucene40RWDocValuesFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40RWNormsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40RWTermVectorsFormat;
+import org.apache.lucene.util.LuceneTestCase;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Read-write version of {@link Lucene41Codec} for testing.
+ */
+@SuppressWarnings("deprecation")
+public class Lucene41RWCodec extends Lucene41Codec {
+  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
+  private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat() {
+    @Override
+    public FieldInfosWriter getFieldInfosWriter() throws IOException {
+      if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+        return super.getFieldInfosWriter();
+      } else {
+        return new Lucene40FieldInfosWriter();
+      }
+    }
+  };
+  
+  private final DocValuesFormat docValues = new Lucene40RWDocValuesFormat();
+  private final NormsFormat norms = new Lucene40RWNormsFormat();
+  private final TermVectorsFormat vectors = new Lucene40RWTermVectorsFormat();
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfos;
+  }
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return fieldsFormat;
+  }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValues;
+  }
+
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectors;
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
new file mode 100644
index 0000000..48762c5
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
@@ -0,0 +1,387 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.MissingOrdRemapper;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.ByteArrayDataOutput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.IntsRefBuilder;
+import org.apache.lucene.util.MathUtil;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.FST.INPUT_TYPE;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
+import org.apache.lucene.util.packed.PackedInts;
+
+import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.VERSION_GCD_COMPRESSION;
+import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.BLOCK_SIZE;
+import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.BYTES;
+import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.NUMBER;
+import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.FST;
+import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.DELTA_COMPRESSED;
+import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.GCD_COMPRESSED;
+import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.TABLE_COMPRESSED;
+import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.UNCOMPRESSED;
+
+/**
+ * Writer for {@link Lucene42DocValuesFormat}
+ */
+class Lucene42DocValuesConsumer extends DocValuesConsumer {
+  final IndexOutput data, meta;
+  final int maxDoc;
+  final float acceptableOverheadRatio;
+  
+  Lucene42DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension, float acceptableOverheadRatio) throws IOException {
+    this.acceptableOverheadRatio = acceptableOverheadRatio;
+    maxDoc = state.segmentInfo.getDocCount();
+    boolean success = false;
+    try {
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+      data = state.directory.createOutput(dataName, state.context);
+      // this writer writes the format 4.2 did!
+      CodecUtil.writeHeader(data, dataCodec, VERSION_GCD_COMPRESSION);
+      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+      meta = state.directory.createOutput(metaName, state.context);
+      CodecUtil.writeHeader(meta, metaCodec, VERSION_GCD_COMPRESSION);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this);
+      }
+    }
+  }
+
+  @Override
+  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+    addNumericField(field, values, true);
+  }
+
+  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {
+    meta.writeVInt(field.number);
+    meta.writeByte(NUMBER);
+    meta.writeLong(data.getFilePointer());
+    long minValue = Long.MAX_VALUE;
+    long maxValue = Long.MIN_VALUE;
+    long gcd = 0;
+    // TODO: more efficient?
+    HashSet<Long> uniqueValues = null;
+    if (optimizeStorage) {
+      uniqueValues = new HashSet<>();
+
+      long count = 0;
+      for (Number nv : values) {
+        // TODO: support this as MemoryDVFormat (and be smart about missing maybe)
+        final long v = nv == null ? 0 : nv.longValue();
+
+        if (gcd != 1) {
+          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
+            // in that case v - minValue might overflow and make the GCD computation return
+            // wrong results. Since these extreme values are unlikely, we just discard
+            // GCD computation for them
+            gcd = 1;
+          } else if (count != 0) { // minValue needs to be set first
+            gcd = MathUtil.gcd(gcd, v - minValue);
+          }
+        }
+
+        minValue = Math.min(minValue, v);
+        maxValue = Math.max(maxValue, v);
+
+        if (uniqueValues != null) {
+          if (uniqueValues.add(v)) {
+            if (uniqueValues.size() > 256) {
+              uniqueValues = null;
+            }
+          }
+        }
+
+        ++count;
+      }
+      assert count == maxDoc;
+    }
+
+    if (uniqueValues != null) {
+      // small number of unique values
+      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);
+      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);
+      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {
+        meta.writeByte(UNCOMPRESSED); // uncompressed
+        for (Number nv : values) {
+          data.writeByte(nv == null ? 0 : (byte) nv.longValue());
+        }
+      } else {
+        meta.writeByte(TABLE_COMPRESSED); // table-compressed
+        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
+        final HashMap<Long,Integer> encode = new HashMap<>();
+        data.writeVInt(decode.length);
+        for (int i = 0; i < decode.length; i++) {
+          data.writeLong(decode[i]);
+          encode.put(decode[i], i);
+        }
+
+        meta.writeVInt(PackedInts.VERSION_CURRENT);
+        data.writeVInt(formatAndBits.format.getId());
+        data.writeVInt(formatAndBits.bitsPerValue);
+
+        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);
+        for(Number nv : values) {
+          writer.add(encode.get(nv == null ? 0 : nv.longValue()));
+        }
+        writer.finish();
+      }
+    } else if (gcd != 0 && gcd != 1) {
+      meta.writeByte(GCD_COMPRESSED);
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      data.writeLong(minValue);
+      data.writeLong(gcd);
+      data.writeVInt(BLOCK_SIZE);
+
+      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
+      for (Number nv : values) {
+        long value = nv == null ? 0 : nv.longValue();
+        writer.add((value - minValue) / gcd);
+      }
+      writer.finish();
+    } else {
+      meta.writeByte(DELTA_COMPRESSED); // delta-compressed
+
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      data.writeVInt(BLOCK_SIZE);
+
+      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
+      for (Number nv : values) {
+        writer.add(nv == null ? 0 : nv.longValue());
+      }
+      writer.finish();
+    }
+  }
+  
+  @Override
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      if (meta != null) {
+        meta.writeVInt(-1); // write EOF marker
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data, meta);
+      } else {
+        IOUtils.closeWhileHandlingException(data, meta);
+      }
+    }
+  }
+
+  @Override
+  public void addBinaryField(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
+    // write the byte[] data
+    meta.writeVInt(field.number);
+    meta.writeByte(BYTES);
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    final long startFP = data.getFilePointer();
+    for(BytesRef v : values) {
+      final int length = v == null ? 0 : v.length;
+      if (length > Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH) {
+        throw new IllegalArgumentException("DocValuesField \"" + field.name + "\" is too large, must be <= " + Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);
+      }
+      minLength = Math.min(minLength, length);
+      maxLength = Math.max(maxLength, length);
+      if (v != null) {
+        data.writeBytes(v.bytes, v.offset, v.length);
+      }
+    }
+    meta.writeLong(startFP);
+    meta.writeLong(data.getFilePointer() - startFP);
+    meta.writeVInt(minLength);
+    meta.writeVInt(maxLength);
+    
+    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
+    // otherwise, we need to record the length fields...
+    if (minLength != maxLength) {
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      meta.writeVInt(BLOCK_SIZE);
+
+      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
+      long addr = 0;
+      for (BytesRef v : values) {
+        if (v != null) {
+          addr += v.length;
+        }
+        writer.add(addr);
+      }
+      writer.finish();
+    }
+  }
+  
+  private void writeFST(FieldInfo field, Iterable<BytesRef> values) throws IOException {
+    meta.writeVInt(field.number);
+    meta.writeByte(FST);
+    meta.writeLong(data.getFilePointer());
+    PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
+    Builder<Long> builder = new Builder<>(INPUT_TYPE.BYTE1, outputs);
+    IntsRefBuilder scratch = new IntsRefBuilder();
+    long ord = 0;
+    for (BytesRef v : values) {
+      builder.add(Util.toIntsRef(v, scratch), ord);
+      ord++;
+    }
+    FST<Long> fst = builder.finish();
+    if (fst != null) {
+      fst.save(data);
+    }
+    meta.writeVLong(ord);
+  }
+
+  @Override
+  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+    // three cases for simulating the old writer:
+    // 1. no missing
+    // 2. missing (and empty string in use): remap ord=-1 -> ord=0
+    // 3. missing (and empty string not in use): remap all ords +1, insert empty string into values
+    boolean anyMissing = false;
+    for (Number n : docToOrd) {
+      if (n.longValue() == -1) {
+        anyMissing = true;
+        break;
+      }
+    }
+    
+    boolean hasEmptyString = false;
+    for (BytesRef b : values) {
+      hasEmptyString = b.length == 0;
+      break;
+    }
+    
+    if (!anyMissing) {
+      // nothing to do
+    } else if (hasEmptyString) {
+      docToOrd = MissingOrdRemapper.mapMissingToOrd0(docToOrd);
+    } else {
+      docToOrd = MissingOrdRemapper.mapAllOrds(docToOrd);
+      values = MissingOrdRemapper.insertEmptyValue(values);
+    }
+    
+    // write the ordinals as numerics
+    addNumericField(field, docToOrd, false);
+    
+    // write the values as FST
+    writeFST(field, values);
+  }
+
+  // note: this might not be the most efficient... but its fairly simple
+  @Override
+  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
+    // write the ordinals as a binary field
+    addBinaryField(field, new Iterable<BytesRef>() {
+      @Override
+      public Iterator<BytesRef> iterator() {
+        return new SortedSetIterator(docToOrdCount.iterator(), ords.iterator());
+      }
+    });
+      
+    // write the values as FST
+    writeFST(field, values);
+  }
+  
+  // per-document vint-encoded byte[]
+  static class SortedSetIterator implements Iterator<BytesRef> {
+    byte[] buffer = new byte[10];
+    ByteArrayDataOutput out = new ByteArrayDataOutput();
+    BytesRef ref = new BytesRef();
+    
+    final Iterator<Number> counts;
+    final Iterator<Number> ords;
+    
+    SortedSetIterator(Iterator<Number> counts, Iterator<Number> ords) {
+      this.counts = counts;
+      this.ords = ords;
+    }
+    
+    @Override
+    public boolean hasNext() {
+      return counts.hasNext();
+    }
+
+    @Override
+    public BytesRef next() {
+      if (!hasNext()) {
+        throw new NoSuchElementException();
+      }
+      
+      int count = counts.next().intValue();
+      int maxSize = count*9; // worst case
+      if (maxSize > buffer.length) {
+        buffer = ArrayUtil.grow(buffer, maxSize);
+      }
+      
+      try {
+        encodeValues(count);
+      } catch (IOException bogus) {
+        throw new RuntimeException(bogus);
+      }
+      
+      ref.bytes = buffer;
+      ref.offset = 0;
+      ref.length = out.getPosition();
+
+      return ref;
+    }
+    
+    // encodes count values to buffer
+    private void encodeValues(int count) throws IOException {
+      out.reset(buffer);
+      long lastOrd = 0;
+      for (int i = 0; i < count; i++) {
+        long ord = ords.next().longValue();
+        out.writeVLong(ord - lastOrd);
+        lastOrd = ord;
+      }
+    }
+
+    @Override
+    public void remove() {
+      throw new UnsupportedOperationException();
+    }
+  }
+
+  @Override
+  public void addSortedNumericField(FieldInfo field, Iterable<Number> docToValueCount, Iterable<Number> values) throws IOException {
+    throw new UnsupportedOperationException("Lucene 4.2 does not support SORTED_NUMERIC");
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java
new file mode 100644
index 0000000..e3f7b6c
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java
@@ -0,0 +1,109 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Lucene 4.2 FieldInfos writer.
+ * 
+ * @see Lucene42FieldInfosFormat
+ * @lucene.experimental
+ */
+@Deprecated
+public final class Lucene42FieldInfosWriter extends FieldInfosWriter {
+  
+  /** Sole constructor. */
+  public Lucene42FieldInfosWriter() {
+  }
+  
+  @Override
+  public void write(Directory directory, String segmentName, String segmentSuffix, FieldInfos infos, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene42FieldInfosFormat.EXTENSION);
+    IndexOutput output = directory.createOutput(fileName, context);
+    boolean success = false;
+    try {
+      CodecUtil.writeHeader(output, Lucene42FieldInfosFormat.CODEC_NAME, Lucene42FieldInfosFormat.FORMAT_CURRENT);
+      output.writeVInt(infos.size());
+      for (FieldInfo fi : infos) {
+        IndexOptions indexOptions = fi.getIndexOptions();
+        byte bits = 0x0;
+        if (fi.hasVectors()) bits |= Lucene42FieldInfosFormat.STORE_TERMVECTOR;
+        if (fi.omitsNorms()) bits |= Lucene42FieldInfosFormat.OMIT_NORMS;
+        if (fi.hasPayloads()) bits |= Lucene42FieldInfosFormat.STORE_PAYLOADS;
+        if (fi.isIndexed()) {
+          bits |= Lucene42FieldInfosFormat.IS_INDEXED;
+          assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
+          if (indexOptions == IndexOptions.DOCS_ONLY) {
+            bits |= Lucene42FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS;
+          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
+            bits |= Lucene42FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS;
+          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
+            bits |= Lucene42FieldInfosFormat.OMIT_POSITIONS;
+          }
+        }
+        output.writeString(fi.name);
+        output.writeVInt(fi.number);
+        output.writeByte(bits);
+
+        // pack the DV types in one byte
+        final byte dv = docValuesByte(fi.getDocValuesType());
+        final byte nrm = docValuesByte(fi.getNormType());
+        assert (dv & (~0xF)) == 0 && (nrm & (~0x0F)) == 0;
+        byte val = (byte) (0xff & ((nrm << 4) | dv));
+        output.writeByte(val);
+        output.writeStringStringMap(fi.attributes());
+      }
+      success = true;
+    } finally {
+      if (success) {
+        output.close();
+      } else {
+        IOUtils.closeWhileHandlingException(output);
+      }
+    }
+  }
+  
+  private static byte docValuesByte(DocValuesType type) {
+    if (type == null) {
+      return 0;
+    } else if (type == DocValuesType.NUMERIC) {
+      return 1;
+    } else if (type == DocValuesType.BINARY) {
+      return 2;
+    } else if (type == DocValuesType.SORTED) {
+      return 3;
+    } else if (type == DocValuesType.SORTED_SET) {
+      return 4;
+    } else {
+      throw new AssertionError();
+    }
+  }  
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java
new file mode 100644
index 0000000..dc1825d
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java
@@ -0,0 +1,196 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.MathUtil;
+import org.apache.lucene.util.packed.BlockPackedWriter;
+import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
+import org.apache.lucene.util.packed.PackedInts;
+
+import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.VERSION_CURRENT;
+
+/**
+ * Writer for {@link Lucene42NormsFormat}
+ */
+class Lucene42NormsConsumer extends NormsConsumer { 
+  static final byte NUMBER = 0;
+
+  static final int BLOCK_SIZE = 4096;
+  
+  static final byte DELTA_COMPRESSED = 0;
+  static final byte TABLE_COMPRESSED = 1;
+  static final byte UNCOMPRESSED = 2;
+  static final byte GCD_COMPRESSED = 3;
+
+  IndexOutput data, meta;
+  final int maxDoc;
+  final float acceptableOverheadRatio;
+  
+  Lucene42NormsConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension, float acceptableOverheadRatio) throws IOException {
+    this.acceptableOverheadRatio = acceptableOverheadRatio;
+    maxDoc = state.segmentInfo.getDocCount();
+    boolean success = false;
+    try {
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+      data = state.directory.createOutput(dataName, state.context);
+      CodecUtil.writeHeader(data, dataCodec, VERSION_CURRENT);
+      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+      meta = state.directory.createOutput(metaName, state.context);
+      CodecUtil.writeHeader(meta, metaCodec, VERSION_CURRENT);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this);
+      }
+    }
+  }
+
+  @Override
+  public void addNormsField(FieldInfo field, Iterable<Number> values) throws IOException {
+    meta.writeVInt(field.number);
+    meta.writeByte(NUMBER);
+    meta.writeLong(data.getFilePointer());
+    long minValue = Long.MAX_VALUE;
+    long maxValue = Long.MIN_VALUE;
+    long gcd = 0;
+    // TODO: more efficient?
+    HashSet<Long> uniqueValues = null;
+    if (true) {
+      uniqueValues = new HashSet<>();
+
+      long count = 0;
+      for (Number nv : values) {
+        assert nv != null;
+        final long v = nv.longValue();
+
+        if (gcd != 1) {
+          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
+            // in that case v - minValue might overflow and make the GCD computation return
+            // wrong results. Since these extreme values are unlikely, we just discard
+            // GCD computation for them
+            gcd = 1;
+          } else if (count != 0) { // minValue needs to be set first
+            gcd = MathUtil.gcd(gcd, v - minValue);
+          }
+        }
+
+        minValue = Math.min(minValue, v);
+        maxValue = Math.max(maxValue, v);
+
+        if (uniqueValues != null) {
+          if (uniqueValues.add(v)) {
+            if (uniqueValues.size() > 256) {
+              uniqueValues = null;
+            }
+          }
+        }
+
+        ++count;
+      }
+      assert count == maxDoc;
+    }
+
+    if (uniqueValues != null) {
+      // small number of unique values
+      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);
+      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);
+      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {
+        meta.writeByte(UNCOMPRESSED); // uncompressed
+        for (Number nv : values) {
+          data.writeByte(nv == null ? 0 : (byte) nv.longValue());
+        }
+      } else {
+        meta.writeByte(TABLE_COMPRESSED); // table-compressed
+        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
+        final HashMap<Long,Integer> encode = new HashMap<>();
+        data.writeVInt(decode.length);
+        for (int i = 0; i < decode.length; i++) {
+          data.writeLong(decode[i]);
+          encode.put(decode[i], i);
+        }
+
+        meta.writeVInt(PackedInts.VERSION_CURRENT);
+        data.writeVInt(formatAndBits.format.getId());
+        data.writeVInt(formatAndBits.bitsPerValue);
+
+        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);
+        for(Number nv : values) {
+          writer.add(encode.get(nv == null ? 0 : nv.longValue()));
+        }
+        writer.finish();
+      }
+    } else if (gcd != 0 && gcd != 1) {
+      meta.writeByte(GCD_COMPRESSED);
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      data.writeLong(minValue);
+      data.writeLong(gcd);
+      data.writeVInt(BLOCK_SIZE);
+
+      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
+      for (Number nv : values) {
+        long value = nv == null ? 0 : nv.longValue();
+        writer.add((value - minValue) / gcd);
+      }
+      writer.finish();
+    } else {
+      meta.writeByte(DELTA_COMPRESSED); // delta-compressed
+
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      data.writeVInt(BLOCK_SIZE);
+
+      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
+      for (Number nv : values) {
+        writer.add(nv == null ? 0 : nv.longValue());
+      }
+      writer.finish();
+    }
+  }
+  
+  @Override
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      if (meta != null) {
+        meta.writeVInt(-1); // write EOF marker
+        CodecUtil.writeFooter(meta); // write checksum
+      }
+      if (data != null) {
+        CodecUtil.writeFooter(data); // write checksum
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data, meta);
+      } else {
+        IOUtils.closeWhileHandlingException(data, meta);
+      }
+      meta = data = null;
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java
new file mode 100644
index 0000000..68f3859
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java
@@ -0,0 +1,62 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Read-write version of {@link Lucene42Codec} for testing.
+ */
+@SuppressWarnings("deprecation")
+public class Lucene42RWCodec extends Lucene42Codec {
+
+  private static final DocValuesFormat dv = new Lucene42RWDocValuesFormat();
+  private static final NormsFormat norms = new Lucene42RWNormsFormat();
+
+  private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat() {
+    @Override
+    public FieldInfosWriter getFieldInfosWriter() throws IOException {
+      if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+        return super.getFieldInfosWriter();
+      } else {
+        return new Lucene42FieldInfosWriter();
+      }
+    }
+  };
+
+  @Override
+  public DocValuesFormat getDocValuesFormatForField(String field) {
+    return dv;
+  }
+
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }  
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java
new file mode 100644
index 0000000..569ba9e
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java
@@ -0,0 +1,41 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Read-write version of {@link Lucene42DocValuesFormat} for testing.
+ */
+@SuppressWarnings("deprecation")
+public class Lucene42RWDocValuesFormat extends Lucene42DocValuesFormat {
+  
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+      return super.fieldsConsumer(state);
+    } else {
+      // note: we choose DEFAULT here (its reasonably fast, and for small bpv has tiny waste)
+      return new Lucene42DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION, acceptableOverheadRatio);
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java
new file mode 100644
index 0000000..fbca416
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java
@@ -0,0 +1,39 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.NormsConsumer;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Read-write version of {@link Lucene42NormsFormat}
+ */
+public class Lucene42RWNormsFormat extends Lucene42NormsFormat {
+
+  @Override
+  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    if (LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+      return new Lucene42NormsConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION, acceptableOverheadRatio);
+    } else {
+      return super.normsConsumer(state);
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java
new file mode 100644
index 0000000..a7796c4
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java
@@ -0,0 +1,44 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
+import org.junit.BeforeClass;
+
+/**
+ * Tests Lucene42DocValuesFormat
+ */
+public class TestLucene42DocValuesFormat extends BaseCompressingDocValuesFormatTestCase {
+  private final Codec codec = new Lucene42RWCodec();
+
+  @BeforeClass
+  public static void beforeClass() {
+    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
+  }
+  
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+
+  @Override
+  protected boolean codecAcceptsHugeBinaryValues(String field) {
+    return false;
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42NormsFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42NormsFormat.java
new file mode 100644
index 0000000..9ee7827
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene42/TestLucene42NormsFormat.java
@@ -0,0 +1,38 @@
+package org.apache.lucene.codecs.lucene42;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseNormsFormatTestCase;
+import org.junit.BeforeClass;
+
+
+/** Tests Lucene42's norms format */
+public class TestLucene42NormsFormat extends BaseNormsFormatTestCase {
+  final Codec codec = new Lucene42RWCodec();
+  
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+  
+  @BeforeClass
+  public static void beforeClass() {
+    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java
new file mode 100644
index 0000000..0263b39
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java
@@ -0,0 +1,66 @@
+package org.apache.lucene.codecs.lucene45;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42FieldInfosFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42FieldInfosWriter;
+import org.apache.lucene.codecs.lucene42.Lucene42RWNormsFormat;
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Read-write version of {@link Lucene45Codec} for testing.
+ */
+@SuppressWarnings("deprecation")
+public class Lucene45RWCodec extends Lucene45Codec {
+  
+  private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat() {
+    @Override
+    public FieldInfosWriter getFieldInfosWriter() throws IOException {
+      if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+        return super.getFieldInfosWriter();
+      } else {
+        return new Lucene42FieldInfosWriter();
+      }
+    }
+  };
+
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+  
+  private static final DocValuesFormat docValues = new Lucene45RWDocValuesFormat();
+  
+  @Override
+  public DocValuesFormat getDocValuesFormatForField(String field) {
+    return docValues;
+  }
+
+  private static final NormsFormat norms = new Lucene42RWNormsFormat();
+
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java
new file mode 100644
index 0000000..87a6dd3
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java
@@ -0,0 +1,45 @@
+package org.apache.lucene.codecs.lucene45;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Read-write version of {@link Lucene45DocValuesFormat} for testing.
+ */
+public class Lucene45RWDocValuesFormat extends Lucene45DocValuesFormat {
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    if (LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+      return new Lucene45DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION) {
+        @Override
+        void checkCanWrite(FieldInfo field) {
+           // allow writing all fields 
+        }
+      };
+    } else {
+      return super.fieldsConsumer(state);
+    }
+  }
+}
\ No newline at end of file
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java
new file mode 100644
index 0000000..67654da
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java
@@ -0,0 +1,39 @@
+package org.apache.lucene.codecs.lucene45;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
+import org.junit.BeforeClass;
+
+/**
+ * Tests Lucene45DocValuesFormat
+ */
+public class TestLucene45DocValuesFormat extends BaseCompressingDocValuesFormatTestCase {
+  private final Codec codec = new Lucene45RWCodec();
+  
+  @BeforeClass
+  public static void beforeClass() {
+    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
+  }
+  
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java
new file mode 100644
index 0000000..1d27f49
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java
@@ -0,0 +1,44 @@
+package org.apache.lucene.codecs.lucene46;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.lucene42.Lucene42RWNormsFormat;
+import org.apache.lucene.codecs.lucene45.Lucene45RWDocValuesFormat;
+
+/**
+ * Read-write version of {@link Lucene46Codec} for testing.
+ */
+@SuppressWarnings("deprecation")
+public class Lucene46RWCodec extends Lucene46Codec {
+  
+  private static final DocValuesFormat docValues = new Lucene45RWDocValuesFormat();
+  
+  @Override
+  public DocValuesFormat getDocValuesFormatForField(String field) {
+    return docValues;
+  }
+  
+  private static final NormsFormat norms = new Lucene42RWNormsFormat();
+
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java
new file mode 100644
index 0000000..4884d83
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java
@@ -0,0 +1,42 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.NormsFormat;
+
+/**
+ * Read-write version of {@link Lucene49Codec} for testing.
+ */
+@SuppressWarnings("deprecation")
+public class Lucene49RWCodec extends Lucene49Codec {
+  
+  private static final DocValuesFormat docValues = new Lucene49RWDocValuesFormat();
+  
+  @Override
+  public DocValuesFormat getDocValuesFormatForField(String field) {
+    return docValues;
+  }
+  
+  private static final NormsFormat norms = new Lucene49NormsFormat();
+
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java
new file mode 100644
index 0000000..53c1fe5
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java
@@ -0,0 +1,44 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.LuceneTestCase;
+
+/** Read-write version of {@link Lucene49DocValuesFormat} for testing */
+public class Lucene49RWDocValuesFormat extends Lucene49DocValuesFormat {
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    if (LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
+      return new Lucene49DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION) {
+        @Override
+        void checkCanWrite(FieldInfo field) {
+          // allow writing all fields 
+        }
+      };
+    } else {
+      return super.fieldsConsumer(state);
+    }
+  }
+  
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java
new file mode 100644
index 0000000..c6bb58c
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java
@@ -0,0 +1,39 @@
+package org.apache.lucene.codecs.lucene49;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
+import org.junit.BeforeClass;
+
+/**
+ * Tests Lucene49DocValuesFormat
+ */
+public class TestLucene49DocValuesFormat extends BaseCompressingDocValuesFormatTestCase {
+  private final Codec codec = new Lucene49RWCodec();
+  
+  @BeforeClass
+  public static void beforeClass() {
+    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
+  }
+  
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java b/lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
new file mode 100644
index 0000000..4b73f43
--- /dev/null
+++ b/lucene/backward-codecs/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
@@ -0,0 +1,1164 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.DoubleDocValuesField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.FloatDocValuesField;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.LongField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.SortedDocValuesField;
+import org.apache.lucene.document.SortedNumericDocValuesField;
+import org.apache.lucene.document.SortedSetDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.NumericRangeQuery;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.store.NIOFSDirectory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.store.SimpleFSDirectory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.InfoStream;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+import org.apache.lucene.util.NumericUtils;
+import org.apache.lucene.util.TestUtil;
+import org.apache.lucene.util.Version;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+
+/*
+  Verify we can read the pre-5.0 file format, do searches
+  against it, and add documents to it.
+*/
+// note: add this if we make a 4.x impersonator
+// TODO: don't use 4.x codec, its unrealistic since it means
+// we won't even be running the actual code, only the impostor
+// @SuppressCodecs("Lucene4x")
+@SuppressCodecs({"Lucene40", "Lucene41", "Lucene42", "Lucene45", "Lucene46", "Lucene49"})
+public class TestBackwardsCompatibility extends LuceneTestCase {
+
+  // Uncomment these cases & run them on an older Lucene version,
+  // to generate indexes to test backwards compatibility.  These
+  // indexes will be created under directory /tmp/idx/.
+  //
+  // However, you must first disable the Lucene TestSecurityManager,
+  // which will otherwise disallow writing outside of the build/
+  // directory - to do this, comment out the "java.security.manager"
+  // <sysproperty> under the "test-macro" <macrodef>.
+  //
+  // Be sure to create the indexes with the actual format:
+  //  ant test -Dtestcase=TestBackwardsCompatibility -Dversion=x.y.z
+  //      -Dtests.codec=LuceneXY -Dtests.postingsformat=LuceneXY -Dtests.docvaluesformat=LuceneXY
+  //
+  // Zip up the generated indexes:
+  //
+  //    cd /tmp/idx/index.cfs   ; zip index.<VERSION>.cfs.zip *
+  //    cd /tmp/idx/index.nocfs ; zip index.<VERSION>.nocfs.zip *
+  //
+  // Then move those 2 zip files to your trunk checkout and add them
+  // to the oldNames array.
+
+  /*
+  public void testCreateCFS() throws IOException {
+    createIndex("index.cfs", true, false);
+  }
+
+  public void testCreateNoCFS() throws IOException {
+    createIndex("index.nocfs", false, false);
+  }
+  */
+
+/*
+  // These are only needed for the special upgrade test to verify
+  // that also single-segment indexes are correctly upgraded by IndexUpgrader.
+  // You don't need them to be build for non-4.0 (the test is happy with just one
+  // "old" segment format, version is unimportant:
+  
+  public void testCreateSingleSegmentCFS() throws IOException {
+    createIndex("index.singlesegment.cfs", true, true);
+  }
+
+  public void testCreateSingleSegmentNoCFS() throws IOException {
+    createIndex("index.singlesegment.nocfs", false, true);
+  }
+
+*/  
+
+  /*
+  public void testCreateMoreTermsIndex() throws Exception {
+    // we use a real directory name that is not cleaned up,
+    // because this method is only used to create backwards
+    // indexes:
+    File indexDir = new File("moreterms");
+    _TestUtil.rmDir(indexDir);
+    Directory dir = newFSDirectory(indexDir);
+
+    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();
+    mp.setUseCompoundFile(false);
+    mp.setNoCFSRatio(1.0);
+    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);
+    MockAnalyzer analyzer = new MockAnalyzer(random());
+    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));
+
+    // TODO: remove randomness
+    IndexWriterConfig conf = new IndexWriterConfig(analyzer)
+      .setMergePolicy(mp);
+    conf.setCodec(Codec.forName("Lucene40"));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    LineFileDocs docs = new LineFileDocs(null, true);
+    for(int i=0;i<50;i++) {
+      writer.addDocument(docs.nextDoc());
+    }
+    writer.close();
+    dir.close();
+
+    // Gives you time to copy the index out!: (there is also
+    // a test option to not remove temp dir...):
+    Thread.sleep(100000);
+  }
+  */
+  
+  private void updateNumeric(IndexWriter writer, String id, String f, String cf, long value) throws IOException {
+    writer.updateNumericDocValue(new Term("id", id), f, value);
+    writer.updateNumericDocValue(new Term("id", id), cf, value*2);
+  }
+  
+  private void updateBinary(IndexWriter writer, String id, String f, String cf, long value) throws IOException {
+    writer.updateBinaryDocValue(new Term("id", id), f, TestDocValuesUpdatesOnOldSegments.toBytes(value));
+    writer.updateBinaryDocValue(new Term("id", id), cf, TestDocValuesUpdatesOnOldSegments.toBytes(value*2));
+  }
+
+/*  // Creates an index with DocValues updates
+  public void testCreateIndexWithDocValuesUpdates() throws Exception {
+    // we use a real directory name that is not cleaned up,
+    // because this method is only used to create backwards
+    // indexes:
+    File indexDir = new File("/tmp/idx/dvupdates");
+    TestUtil.rm(indexDir);
+    Directory dir = newFSDirectory(indexDir);
+    
+    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))
+      .setUseCompoundFile(false).setMergePolicy(NoMergePolicy.INSTANCE);
+    IndexWriter writer = new IndexWriter(dir, conf);
+    // create an index w/ few doc-values fields, some with updates and some without
+    for (int i = 0; i < 30; i++) {
+      Document doc = new Document();
+      doc.add(new StringField("id", "" + i, Store.NO));
+      doc.add(new NumericDocValuesField("ndv1", i));
+      doc.add(new NumericDocValuesField("ndv1_c", i*2));
+      doc.add(new NumericDocValuesField("ndv2", i*3));
+      doc.add(new NumericDocValuesField("ndv2_c", i*6));
+      doc.add(new BinaryDocValuesField("bdv1", TestDocValuesUpdatesOnOldSegments.toBytes(i)));
+      doc.add(new BinaryDocValuesField("bdv1_c", TestDocValuesUpdatesOnOldSegments.toBytes(i*2)));
+      doc.add(new BinaryDocValuesField("bdv2", TestDocValuesUpdatesOnOldSegments.toBytes(i*3)));
+      doc.add(new BinaryDocValuesField("bdv2_c", TestDocValuesUpdatesOnOldSegments.toBytes(i*6)));
+      writer.addDocument(doc);
+      if ((i+1) % 10 == 0) {
+        writer.commit(); // flush every 10 docs
+      }
+    }
+    
+    // first segment: no updates
+    
+    // second segment: update two fields, same gen
+    updateNumeric(writer, "10", "ndv1", "ndv1_c", 100L);
+    updateBinary(writer, "11", "bdv1", "bdv1_c", 100L);
+    writer.commit();
+    
+    // third segment: update few fields, different gens, few docs
+    updateNumeric(writer, "20", "ndv1", "ndv1_c", 100L);
+    updateBinary(writer, "21", "bdv1", "bdv1_c", 100L);
+    writer.commit();
+    updateNumeric(writer, "22", "ndv1", "ndv1_c", 200L); // update the field again
+    writer.commit();
+    
+    writer.close();
+    dir.close();
+  }*/
+
+  final static String[] oldNames = {"40.cfs",
+                                    "40.nocfs",
+                                    "41.cfs",
+                                    "41.nocfs",
+                                    "42.cfs",
+                                    "42.nocfs",
+                                    // TODO: these are on 4x, but something is wrong (they seem to be a too old DV format):
+                                    "45.cfs",
+                                    "45.nocfs",
+                                    "461.cfs",
+                                    "461.nocfs",
+                                    "49.cfs",
+                                    "49.nocfs"
+  };
+  
+  final String[] unsupportedNames = {"19.cfs",
+                                     "19.nocfs",
+                                     "20.cfs",
+                                     "20.nocfs",
+                                     "21.cfs",
+                                     "21.nocfs",
+                                     "22.cfs",
+                                     "22.nocfs",
+                                     "23.cfs",
+                                     "23.nocfs",
+                                     "24.cfs",
+                                     "24.nocfs",
+                                     "29.cfs",
+                                     "29.nocfs",
+                                     "30.cfs",
+                                     "30.nocfs",
+                                     "31.cfs",
+                                     "31.nocfs",
+                                     "32.cfs",
+                                     "32.nocfs",
+                                     "34.cfs",
+                                     "34.nocfs"
+  };
+  
+  final static String[] oldSingleSegmentNames = {"40.optimized.cfs",
+                                                 "40.optimized.nocfs",
+  };
+  
+  static Map<String,Directory> oldIndexDirs;
+
+  /**
+   * Randomizes the use of some of hte constructor variations
+   */
+  private static IndexUpgrader newIndexUpgrader(Directory dir) {
+    final boolean streamType = random().nextBoolean();
+    final int choice = TestUtil.nextInt(random(), 0, 2);
+    switch (choice) {
+      case 0: return new IndexUpgrader(dir);
+      case 1: return new IndexUpgrader(dir, streamType ? null : InfoStream.NO_OUTPUT, false);
+      case 2: return new IndexUpgrader(dir, newIndexWriterConfig(null), false);
+      default: fail("case statement didn't get updated when random bounds changed");
+    }
+    return null; // never get here
+  }
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    assertFalse("test infra is broken!", LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE);
+    List<String> names = new ArrayList<>(oldNames.length + oldSingleSegmentNames.length);
+    names.addAll(Arrays.asList(oldNames));
+    names.addAll(Arrays.asList(oldSingleSegmentNames));
+    oldIndexDirs = new HashMap<>();
+    for (String name : names) {
+      File dir = createTempDir(name);
+      File dataFile = new File(TestBackwardsCompatibility.class.getResource("index." + name + ".zip").toURI());
+      TestUtil.unzip(dataFile, dir);
+      oldIndexDirs.put(name, newFSDirectory(dir));
+    }
+  }
+  
+  @AfterClass
+  public static void afterClass() throws Exception {
+    for (Directory d : oldIndexDirs.values()) {
+      d.close();
+    }
+    oldIndexDirs = null;
+  }
+  
+  /** This test checks that *only* IndexFormatTooOldExceptions are thrown when you open and operate on too old indexes! */
+  public void testUnsupportedOldIndexes() throws Exception {
+    for(int i=0;i<unsupportedNames.length;i++) {
+      if (VERBOSE) {
+        System.out.println("TEST: index " + unsupportedNames[i]);
+      }
+      File oldIndxeDir = createTempDir(unsupportedNames[i]);
+      TestUtil.unzip(getDataFile("unsupported." + unsupportedNames[i] + ".zip"), oldIndxeDir);
+      BaseDirectoryWrapper dir = newFSDirectory(oldIndxeDir);
+      // don't checkindex, these are intentionally not supported
+      dir.setCheckIndexOnClose(false);
+
+      IndexReader reader = null;
+      IndexWriter writer = null;
+      try {
+        reader = DirectoryReader.open(dir);
+        fail("DirectoryReader.open should not pass for "+unsupportedNames[i]);
+      } catch (IndexFormatTooOldException e) {
+        // pass
+      } finally {
+        if (reader != null) reader.close();
+        reader = null;
+      }
+
+      try {
+        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())).setCommitOnClose(false));
+        fail("IndexWriter creation should not pass for "+unsupportedNames[i]);
+      } catch (IndexFormatTooOldException e) {
+        // pass
+        if (VERBOSE) {
+          System.out.println("TEST: got expected exc:");
+          e.printStackTrace(System.out);
+        }
+        // Make sure exc message includes a path=
+        assertTrue("got exc message: " + e.getMessage(), e.getMessage().indexOf("path=\"") != -1);
+      } finally {
+        // we should fail to open IW, and so it should be null when we get here.
+        // However, if the test fails (i.e., IW did not fail on open), we need
+        // to close IW. However, if merges are run, IW may throw
+        // IndexFormatTooOldException, and we don't want to mask the fail()
+        // above, so close without waiting for merges.
+        if (writer != null) {
+          try {
+            writer.commit();
+          } finally {
+            writer.close();
+          }
+        }
+        writer = null;
+      }
+      
+      ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
+      CheckIndex checker = new CheckIndex(dir);
+      checker.setInfoStream(new PrintStream(bos, false, IOUtils.UTF_8));
+      CheckIndex.Status indexStatus = checker.checkIndex();
+      assertFalse(indexStatus.clean);
+      assertTrue(bos.toString(IOUtils.UTF_8).contains(IndexFormatTooOldException.class.getName()));
+
+      dir.close();
+      TestUtil.rm(oldIndxeDir);
+    }
+  }
+  
+  public void testFullyMergeOldIndex() throws Exception {
+    for (String name : oldNames) {
+      if (VERBOSE) {
+        System.out.println("\nTEST: index=" + name);
+      }
+      Directory dir = newDirectory(oldIndexDirs.get(name));
+      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(new MockAnalyzer(random())));
+      w.forceMerge(1);
+      w.close();
+      
+      dir.close();
+    }
+  }
+
+  public void testAddOldIndexes() throws IOException {
+    for (String name : oldNames) {
+      if (VERBOSE) {
+        System.out.println("\nTEST: old index " + name);
+      }
+      Directory targetDir = newDirectory();
+      IndexWriter w = new IndexWriter(targetDir, newIndexWriterConfig(new MockAnalyzer(random())));
+      w.addIndexes(oldIndexDirs.get(name));
+      if (VERBOSE) {
+        System.out.println("\nTEST: done adding indices; now close");
+      }
+      w.close();
+      
+      targetDir.close();
+    }
+  }
+
+  public void testAddOldIndexesReader() throws IOException {
+    for (String name : oldNames) {
+      IndexReader reader = DirectoryReader.open(oldIndexDirs.get(name));
+      
+      Directory targetDir = newDirectory();
+      IndexWriter w = new IndexWriter(targetDir, newIndexWriterConfig(new MockAnalyzer(random())));
+      w.addIndexes(reader);
+      w.close();
+      reader.close();
+            
+      targetDir.close();
+    }
+  }
+
+  public void testSearchOldIndex() throws IOException {
+    for (String name : oldNames) {
+      searchIndex(oldIndexDirs.get(name), name);
+    }
+  }
+
+  public void testIndexOldIndexNoAdds() throws IOException {
+    for (String name : oldNames) {
+      Directory dir = newDirectory(oldIndexDirs.get(name));
+      changeIndexNoAdds(random(), dir);
+      dir.close();
+    }
+  }
+
+  public void testIndexOldIndex() throws IOException {
+    for (String name : oldNames) {
+      if (VERBOSE) {
+        System.out.println("TEST: oldName=" + name);
+      }
+      Directory dir = newDirectory(oldIndexDirs.get(name));
+      changeIndexWithAdds(random(), dir, name);
+      dir.close();
+    }
+  }
+
+  private void doTestHits(ScoreDoc[] hits, int expectedCount, IndexReader reader) throws IOException {
+    final int hitCount = hits.length;
+    assertEquals("wrong number of hits", expectedCount, hitCount);
+    for(int i=0;i<hitCount;i++) {
+      reader.document(hits[i].doc);
+      reader.getTermVectors(hits[i].doc);
+    }
+  }
+
+  public void searchIndex(Directory dir, String oldName) throws IOException {
+    //QueryParser parser = new QueryParser("contents", new MockAnalyzer(random));
+    //Query query = parser.parse("handle:1");
+
+    IndexReader reader = DirectoryReader.open(dir);
+    IndexSearcher searcher = newSearcher(reader);
+
+    TestUtil.checkIndex(dir);
+    
+    // true if this is a 4.0+ index
+    final boolean is40Index = MultiFields.getMergedFieldInfos(reader).fieldInfo("content5") != null;
+    // true if this is a 4.2+ index
+    final boolean is42Index = MultiFields.getMergedFieldInfos(reader).fieldInfo("dvSortedSet") != null;
+    // true if this is a 4.9+ index
+    final boolean is49Index = MultiFields.getMergedFieldInfos(reader).fieldInfo("dvSortedNumeric") != null;
+
+    assert is40Index; // NOTE: currently we can only do this on trunk!
+
+    final Bits liveDocs = MultiFields.getLiveDocs(reader);
+
+    for(int i=0;i<35;i++) {
+      if (liveDocs.get(i)) {
+        StoredDocument d = reader.document(i);
+        List<StorableField> fields = d.getFields();
+        boolean isProxDoc = d.getField("content3") == null;
+        if (isProxDoc) {
+          final int numFields = is40Index ? 7 : 5;
+          assertEquals(numFields, fields.size());
+          StorableField f =  d.getField("id");
+          assertEquals(""+i, f.stringValue());
+
+          f = d.getField("utf8");
+          assertEquals("Lu\uD834\uDD1Ece\uD834\uDD60ne \u0000 \u2620 ab\ud917\udc17cd", f.stringValue());
+
+          f =  d.getField("autf8");
+          assertEquals("Lu\uD834\uDD1Ece\uD834\uDD60ne \u0000 \u2620 ab\ud917\udc17cd", f.stringValue());
+      
+          f = d.getField("content2");
+          assertEquals("here is more content with aaa aaa aaa", f.stringValue());
+
+          f = d.getField("fie\u2C77ld");
+          assertEquals("field with non-ascii name", f.stringValue());
+        }
+
+        Fields tfvFields = reader.getTermVectors(i);
+        assertNotNull("i=" + i, tfvFields);
+        Terms tfv = tfvFields.terms("utf8");
+        assertNotNull("docID=" + i + " index=" + oldName, tfv);
+      } else {
+        // Only ID 7 is deleted
+        assertEquals(7, i);
+      }
+    }
+
+    if (is40Index) {
+      // check docvalues fields
+      NumericDocValues dvByte = MultiDocValues.getNumericValues(reader, "dvByte");
+      BinaryDocValues dvBytesDerefFixed = MultiDocValues.getBinaryValues(reader, "dvBytesDerefFixed");
+      BinaryDocValues dvBytesDerefVar = MultiDocValues.getBinaryValues(reader, "dvBytesDerefVar");
+      SortedDocValues dvBytesSortedFixed = MultiDocValues.getSortedValues(reader, "dvBytesSortedFixed");
+      SortedDocValues dvBytesSortedVar = MultiDocValues.getSortedValues(reader, "dvBytesSortedVar");
+      BinaryDocValues dvBytesStraightFixed = MultiDocValues.getBinaryValues(reader, "dvBytesStraightFixed");
+      BinaryDocValues dvBytesStraightVar = MultiDocValues.getBinaryValues(reader, "dvBytesStraightVar");
+      NumericDocValues dvDouble = MultiDocValues.getNumericValues(reader, "dvDouble");
+      NumericDocValues dvFloat = MultiDocValues.getNumericValues(reader, "dvFloat");
+      NumericDocValues dvInt = MultiDocValues.getNumericValues(reader, "dvInt");
+      NumericDocValues dvLong = MultiDocValues.getNumericValues(reader, "dvLong");
+      NumericDocValues dvPacked = MultiDocValues.getNumericValues(reader, "dvPacked");
+      NumericDocValues dvShort = MultiDocValues.getNumericValues(reader, "dvShort");
+      SortedSetDocValues dvSortedSet = null;
+      if (is42Index) {
+        dvSortedSet = MultiDocValues.getSortedSetValues(reader, "dvSortedSet");
+      }
+      SortedNumericDocValues dvSortedNumeric = null;
+      if (is49Index) {
+        dvSortedNumeric = MultiDocValues.getSortedNumericValues(reader, "dvSortedNumeric");
+      }
+      
+      for (int i=0;i<35;i++) {
+        int id = Integer.parseInt(reader.document(i).get("id"));
+        assertEquals(id, dvByte.get(i));
+        
+        byte bytes[] = new byte[] {
+            (byte)(id >>> 24), (byte)(id >>> 16),(byte)(id >>> 8),(byte)id
+        };
+        BytesRef expectedRef = new BytesRef(bytes);
+        
+        BytesRef term = dvBytesDerefFixed.get(i);
+        assertEquals(expectedRef, term);
+        term = dvBytesDerefVar.get(i);
+        assertEquals(expectedRef, term);
+        term = dvBytesSortedFixed.get(i);
+        assertEquals(expectedRef, term);
+        term = dvBytesSortedVar.get(i);
+        assertEquals(expectedRef, term);
+        term = dvBytesStraightFixed.get(i);
+        assertEquals(expectedRef, term);
+        term = dvBytesStraightVar.get(i);
+        assertEquals(expectedRef, term);
+        
+        assertEquals((double)id, Double.longBitsToDouble(dvDouble.get(i)), 0D);
+        assertEquals((float)id, Float.intBitsToFloat((int)dvFloat.get(i)), 0F);
+        assertEquals(id, dvInt.get(i));
+        assertEquals(id, dvLong.get(i));
+        assertEquals(id, dvPacked.get(i));
+        assertEquals(id, dvShort.get(i));
+        if (is42Index) {
+          dvSortedSet.setDocument(i);
+          long ord = dvSortedSet.nextOrd();
+          assertEquals(SortedSetDocValues.NO_MORE_ORDS, dvSortedSet.nextOrd());
+          term = dvSortedSet.lookupOrd(ord);
+          assertEquals(expectedRef, term);
+        }
+        if (is49Index) {
+          dvSortedNumeric.setDocument(i);
+          assertEquals(1, dvSortedNumeric.count());
+          assertEquals(id, dvSortedNumeric.valueAt(0));
+        }
+      }
+    }
+    
+    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(new String("content"), "aaa")), null, 1000).scoreDocs;
+
+    // First document should be #0
+    StoredDocument d = searcher.getIndexReader().document(hits[0].doc);
+    assertEquals("didn't get the right document first", "0", d.get("id"));
+
+    doTestHits(hits, 34, searcher.getIndexReader());
+    
+    if (is40Index) {
+      hits = searcher.search(new TermQuery(new Term(new String("content5"), "aaa")), null, 1000).scoreDocs;
+
+      doTestHits(hits, 34, searcher.getIndexReader());
+    
+      hits = searcher.search(new TermQuery(new Term(new String("content6"), "aaa")), null, 1000).scoreDocs;
+
+      doTestHits(hits, 34, searcher.getIndexReader());
+    }
+
+    hits = searcher.search(new TermQuery(new Term("utf8", "\u0000")), null, 1000).scoreDocs;
+    assertEquals(34, hits.length);
+    hits = searcher.search(new TermQuery(new Term(new String("utf8"), "lu\uD834\uDD1Ece\uD834\uDD60ne")), null, 1000).scoreDocs;
+    assertEquals(34, hits.length);
+    hits = searcher.search(new TermQuery(new Term("utf8", "ab\ud917\udc17cd")), null, 1000).scoreDocs;
+    assertEquals(34, hits.length);
+
+    reader.close();
+  }
+
+  private int compare(String name, String v) {
+    int v0 = Integer.parseInt(name.substring(0, 2));
+    int v1 = Integer.parseInt(v);
+    return v0 - v1;
+  }
+
+  public void changeIndexWithAdds(Random random, Directory dir, String origOldName) throws IOException {
+    // open writer
+    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random))
+                                                 .setOpenMode(OpenMode.APPEND)
+                                                 .setMergePolicy(newLogMergePolicy()));
+    // add 10 docs
+    for(int i=0;i<10;i++) {
+      addDoc(writer, 35+i);
+    }
+
+    // make sure writer sees right total -- writer seems not to know about deletes in .del?
+    final int expected;
+    if (compare(origOldName, "24") < 0) {
+      expected = 44;
+    } else {
+      expected = 45;
+    }
+    assertEquals("wrong doc count", expected, writer.numDocs());
+    writer.close();
+
+    // make sure searching sees right # hits
+    IndexReader reader = DirectoryReader.open(dir);
+    IndexSearcher searcher = newSearcher(reader);
+    ScoreDoc[] hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
+    StoredDocument d = searcher.getIndexReader().document(hits[0].doc);
+    assertEquals("wrong first document", "0", d.get("id"));
+    doTestHits(hits, 44, searcher.getIndexReader());
+    reader.close();
+
+    // fully merge
+    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random))
+                                    .setOpenMode(OpenMode.APPEND)
+                                    .setMergePolicy(newLogMergePolicy()));
+    writer.forceMerge(1);
+    writer.close();
+
+    reader = DirectoryReader.open(dir);
+    searcher = newSearcher(reader);
+    hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
+    assertEquals("wrong number of hits", 44, hits.length);
+    d = searcher.doc(hits[0].doc);
+    doTestHits(hits, 44, searcher.getIndexReader());
+    assertEquals("wrong first document", "0", d.get("id"));
+    reader.close();
+  }
+
+  public void changeIndexNoAdds(Random random, Directory dir) throws IOException {
+    // make sure searching sees right # hits
+    DirectoryReader reader = DirectoryReader.open(dir);
+    IndexSearcher searcher = newSearcher(reader);
+    ScoreDoc[] hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
+    assertEquals("wrong number of hits", 34, hits.length);
+    StoredDocument d = searcher.doc(hits[0].doc);
+    assertEquals("wrong first document", "0", d.get("id"));
+    reader.close();
+
+    // fully merge
+    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random))
+                                                .setOpenMode(OpenMode.APPEND));
+    writer.forceMerge(1);
+    writer.close();
+
+    reader = DirectoryReader.open(dir);
+    searcher = newSearcher(reader);
+    hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
+    assertEquals("wrong number of hits", 34, hits.length);
+    doTestHits(hits, 34, searcher.getIndexReader());
+    reader.close();
+  }
+
+  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {
+    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:
+    File indexDir = new File("/tmp/idx", dirName);
+    TestUtil.rm(indexDir);
+    Directory dir = newFSDirectory(indexDir);
+    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();
+    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);
+    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);
+    // TODO: remove randomness
+    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))
+      .setMaxBufferedDocs(10).setMergePolicy(mp);
+    IndexWriter writer = new IndexWriter(dir, conf);
+    
+    for(int i=0;i<35;i++) {
+      addDoc(writer, i);
+    }
+    assertEquals("wrong doc count", 35, writer.maxDoc());
+    if (fullyMerged) {
+      writer.forceMerge(1);
+    }
+    writer.close();
+
+    if (!fullyMerged) {
+      // open fresh writer so we get no prx file in the added segment
+      mp = new LogByteSizeMergePolicy();
+      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);
+      // TODO: remove randomness
+      conf = new IndexWriterConfig(new MockAnalyzer(random()))
+        .setMaxBufferedDocs(10).setMergePolicy(mp);
+      writer = new IndexWriter(dir, conf);
+      addNoProxDoc(writer);
+      writer.close();
+
+      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));
+      Term searchTerm = new Term("id", "7");
+      writer.deleteDocuments(searchTerm);
+      writer.close();
+    }
+    
+    dir.close();
+    
+    return indexDir;
+  }
+
+  private void addDoc(IndexWriter writer, int id) throws IOException
+  {
+    Document doc = new Document();
+    doc.add(new TextField("content", "aaa", Field.Store.NO));
+    doc.add(new StringField("id", Integer.toString(id), Field.Store.YES));
+    FieldType customType2 = new FieldType(TextField.TYPE_STORED);
+    customType2.setStoreTermVectors(true);
+    customType2.setStoreTermVectorPositions(true);
+    customType2.setStoreTermVectorOffsets(true);
+    doc.add(new Field("autf8", "Lu\uD834\uDD1Ece\uD834\uDD60ne \u0000 \u2620 ab\ud917\udc17cd", customType2));
+    doc.add(new Field("utf8", "Lu\uD834\uDD1Ece\uD834\uDD60ne \u0000 \u2620 ab\ud917\udc17cd", customType2));
+    doc.add(new Field("content2", "here is more content with aaa aaa aaa", customType2));
+    doc.add(new Field("fie\u2C77ld", "field with non-ascii name", customType2));
+    // add numeric fields, to test if flex preserves encoding
+    doc.add(new IntField("trieInt", id, Field.Store.NO));
+    doc.add(new LongField("trieLong", (long) id, Field.Store.NO));
+    // add docvalues fields
+    doc.add(new NumericDocValuesField("dvByte", (byte) id));
+    byte bytes[] = new byte[] {
+      (byte)(id >>> 24), (byte)(id >>> 16),(byte)(id >>> 8),(byte)id
+    };
+    BytesRef ref = new BytesRef(bytes);
+    doc.add(new BinaryDocValuesField("dvBytesDerefFixed", ref));
+    doc.add(new BinaryDocValuesField("dvBytesDerefVar", ref));
+    doc.add(new SortedDocValuesField("dvBytesSortedFixed", ref));
+    doc.add(new SortedDocValuesField("dvBytesSortedVar", ref));
+    doc.add(new BinaryDocValuesField("dvBytesStraightFixed", ref));
+    doc.add(new BinaryDocValuesField("dvBytesStraightVar", ref));
+    doc.add(new DoubleDocValuesField("dvDouble", (double)id));
+    doc.add(new FloatDocValuesField("dvFloat", (float)id));
+    doc.add(new NumericDocValuesField("dvInt", id));
+    doc.add(new NumericDocValuesField("dvLong", id));
+    doc.add(new NumericDocValuesField("dvPacked", id));
+    doc.add(new NumericDocValuesField("dvShort", (short)id));
+    doc.add(new SortedSetDocValuesField("dvSortedSet", ref));
+    doc.add(new SortedNumericDocValuesField("dvSortedNumeric", id));
+    // a field with both offsets and term vectors for a cross-check
+    FieldType customType3 = new FieldType(TextField.TYPE_STORED);
+    customType3.setStoreTermVectors(true);
+    customType3.setStoreTermVectorPositions(true);
+    customType3.setStoreTermVectorOffsets(true);
+    customType3.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    doc.add(new Field("content5", "here is more content with aaa aaa aaa", customType3));
+    // a field that omits only positions
+    FieldType customType4 = new FieldType(TextField.TYPE_STORED);
+    customType4.setStoreTermVectors(true);
+    customType4.setStoreTermVectorPositions(false);
+    customType4.setStoreTermVectorOffsets(true);
+    customType4.setIndexOptions(IndexOptions.DOCS_AND_FREQS);
+    doc.add(new Field("content6", "here is more content with aaa aaa aaa", customType4));
+    // TODO: 
+    //   index different norms types via similarity (we use a random one currently?!)
+    //   remove any analyzer randomness, explicitly add payloads for certain fields.
+    writer.addDocument(doc);
+  }
+
+  private void addNoProxDoc(IndexWriter writer) throws IOException {
+    Document doc = new Document();
+    FieldType customType = new FieldType(TextField.TYPE_STORED);
+    customType.setIndexOptions(IndexOptions.DOCS_ONLY);
+    Field f = new Field("content3", "aaa", customType);
+    doc.add(f);
+    FieldType customType2 = new FieldType();
+    customType2.setStored(true);
+    customType2.setIndexOptions(IndexOptions.DOCS_ONLY);
+    f = new Field("content4", "aaa", customType2);
+    doc.add(f);
+    writer.addDocument(doc);
+  }
+
+  private int countDocs(DocsEnum docs) throws IOException {
+    int count = 0;
+    while((docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+      count ++;
+    }
+    return count;
+  }
+
+  // flex: test basics of TermsEnum api on non-flex index
+  public void testNextIntoWrongField() throws Exception {
+    for (String name : oldNames) {
+      Directory dir = oldIndexDirs.get(name);
+      IndexReader r = DirectoryReader.open(dir);
+      TermsEnum terms = MultiFields.getFields(r).terms("content").iterator(null);
+      BytesRef t = terms.next();
+      assertNotNull(t);
+
+      // content field only has term aaa:
+      assertEquals("aaa", t.utf8ToString());
+      assertNull(terms.next());
+
+      BytesRef aaaTerm = new BytesRef("aaa");
+
+      // should be found exactly
+      assertEquals(TermsEnum.SeekStatus.FOUND,
+                   terms.seekCeil(aaaTerm));
+      assertEquals(35, countDocs(TestUtil.docs(random(), terms, null, null, DocsEnum.FLAG_NONE)));
+      assertNull(terms.next());
+
+      // should hit end of field
+      assertEquals(TermsEnum.SeekStatus.END,
+                   terms.seekCeil(new BytesRef("bbb")));
+      assertNull(terms.next());
+
+      // should seek to aaa
+      assertEquals(TermsEnum.SeekStatus.NOT_FOUND,
+                   terms.seekCeil(new BytesRef("a")));
+      assertTrue(terms.term().bytesEquals(aaaTerm));
+      assertEquals(35, countDocs(TestUtil.docs(random(), terms, null, null, DocsEnum.FLAG_NONE)));
+      assertNull(terms.next());
+
+      assertEquals(TermsEnum.SeekStatus.FOUND,
+                   terms.seekCeil(aaaTerm));
+      assertEquals(35, countDocs(TestUtil.docs(random(), terms, null, null, DocsEnum.FLAG_NONE)));
+      assertNull(terms.next());
+
+      r.close();
+    }
+  }
+  
+  /** 
+   * Test that we didn't forget to bump the current Constants.LUCENE_MAIN_VERSION.
+   * This is important so that we can determine which version of lucene wrote the segment.
+   */
+  public void testOldVersions() throws Exception {
+    // first create a little index with the current code and get the version
+    Directory currentDir = newDirectory();
+    RandomIndexWriter riw = new RandomIndexWriter(random(), currentDir);
+    riw.addDocument(new Document());
+    riw.close();
+    DirectoryReader ir = DirectoryReader.open(currentDir);
+    SegmentReader air = (SegmentReader)ir.leaves().get(0).reader();
+    Version currentVersion = air.getSegmentInfo().info.getVersion();
+    assertNotNull(currentVersion); // only 3.0 segments can have a null version
+    ir.close();
+    currentDir.close();
+    
+    // now check all the old indexes, their version should be < the current version
+    for (String name : oldNames) {
+      Directory dir = oldIndexDirs.get(name);
+      DirectoryReader r = DirectoryReader.open(dir);
+      for (AtomicReaderContext context : r.leaves()) {
+        air = (SegmentReader) context.reader();
+        Version oldVersion = air.getSegmentInfo().info.getVersion();
+        assertNotNull(oldVersion); // only 3.0 segments can have a null version
+        assertTrue("current Version.LATEST is <= an old index: did you forget to bump it?!",
+                   currentVersion.onOrAfter(oldVersion));
+      }
+      r.close();
+    }
+  }
+  
+  public void testNumericFields() throws Exception {
+    for (String name : oldNames) {
+      
+      Directory dir = oldIndexDirs.get(name);
+      IndexReader reader = DirectoryReader.open(dir);
+      IndexSearcher searcher = newSearcher(reader);
+      
+      for (int id=10; id<15; id++) {
+        ScoreDoc[] hits = searcher.search(NumericRangeQuery.newIntRange("trieInt", NumericUtils.PRECISION_STEP_DEFAULT_32, Integer.valueOf(id), Integer.valueOf(id), true, true), 100).scoreDocs;
+        assertEquals("wrong number of hits", 1, hits.length);
+        StoredDocument d = searcher.doc(hits[0].doc);
+        assertEquals(String.valueOf(id), d.get("id"));
+        
+        hits = searcher.search(NumericRangeQuery.newLongRange("trieLong", NumericUtils.PRECISION_STEP_DEFAULT, Long.valueOf(id), Long.valueOf(id), true, true), 100).scoreDocs;
+        assertEquals("wrong number of hits", 1, hits.length);
+        d = searcher.doc(hits[0].doc);
+        assertEquals(String.valueOf(id), d.get("id"));
+      }
+      
+      // check that also lower-precision fields are ok
+      ScoreDoc[] hits = searcher.search(NumericRangeQuery.newIntRange("trieInt", NumericUtils.PRECISION_STEP_DEFAULT_32, Integer.MIN_VALUE, Integer.MAX_VALUE, false, false), 100).scoreDocs;
+      assertEquals("wrong number of hits", 34, hits.length);
+      
+      hits = searcher.search(NumericRangeQuery.newLongRange("trieLong", NumericUtils.PRECISION_STEP_DEFAULT, Long.MIN_VALUE, Long.MAX_VALUE, false, false), 100).scoreDocs;
+      assertEquals("wrong number of hits", 34, hits.length);
+      
+      // check decoding of terms
+      Terms terms = MultiFields.getTerms(searcher.getIndexReader(), "trieInt");
+      TermsEnum termsEnum = NumericUtils.filterPrefixCodedInts(terms.iterator(null));
+      while (termsEnum.next() != null) {
+        int val = NumericUtils.prefixCodedToInt(termsEnum.term());
+        assertTrue("value in id bounds", val >= 0 && val < 35);
+      }
+      
+      terms = MultiFields.getTerms(searcher.getIndexReader(), "trieLong");
+      termsEnum = NumericUtils.filterPrefixCodedLongs(terms.iterator(null));
+      while (termsEnum.next() != null) {
+        long val = NumericUtils.prefixCodedToLong(termsEnum.term());
+        assertTrue("value in id bounds", val >= 0L && val < 35L);
+      }
+      
+      reader.close();
+    }
+  }
+  
+  private int checkAllSegmentsUpgraded(Directory dir) throws IOException {
+    final SegmentInfos infos = new SegmentInfos();
+    infos.read(dir);
+    if (VERBOSE) {
+      System.out.println("checkAllSegmentsUpgraded: " + infos);
+    }
+    for (SegmentCommitInfo si : infos) {
+      assertEquals(Version.LATEST, si.info.getVersion());
+    }
+    return infos.size();
+  }
+  
+  private int getNumberOfSegments(Directory dir) throws IOException {
+    final SegmentInfos infos = new SegmentInfos();
+    infos.read(dir);
+    return infos.size();
+  }
+
+  public void testUpgradeOldIndex() throws Exception {
+    List<String> names = new ArrayList<>(oldNames.length + oldSingleSegmentNames.length);
+    names.addAll(Arrays.asList(oldNames));
+    names.addAll(Arrays.asList(oldSingleSegmentNames));
+    for(String name : names) {
+      if (VERBOSE) {
+        System.out.println("testUpgradeOldIndex: index=" +name);
+      }
+      Directory dir = newDirectory(oldIndexDirs.get(name));
+
+      newIndexUpgrader(dir).upgrade();
+
+      checkAllSegmentsUpgraded(dir);
+      
+      dir.close();
+    }
+  }
+
+  public void testCommandLineArgs() throws Exception {
+
+    PrintStream savedSystemOut = System.out;
+    System.setOut(new PrintStream(new ByteArrayOutputStream(), false, "UTF-8"));
+    try {
+      for (String name : oldIndexDirs.keySet()) {
+        File dir = createTempDir(name);
+        File dataFile = new File(TestBackwardsCompatibility.class.getResource("index." + name + ".zip").toURI());
+        TestUtil.unzip(dataFile, dir);
+        
+        String path = dir.getAbsolutePath();
+        
+        List<String> args = new ArrayList<>();
+        if (random().nextBoolean()) {
+          args.add("-verbose");
+        }
+        if (random().nextBoolean()) {
+          args.add("-delete-prior-commits");
+        }
+        if (random().nextBoolean()) {
+          // TODO: need to better randomize this, but ...
+          //  - LuceneTestCase.FS_DIRECTORIES is private
+          //  - newFSDirectory returns BaseDirectoryWrapper
+          //  - BaseDirectoryWrapper doesn't expose delegate
+          Class<? extends FSDirectory> dirImpl = random().nextBoolean() ?
+              SimpleFSDirectory.class : NIOFSDirectory.class;
+          
+          args.add("-dir-impl");
+          args.add(dirImpl.getName());
+        }
+        args.add(path);
+        
+        IndexUpgrader upgrader = null;
+        try {
+          upgrader = IndexUpgrader.parseArgs(args.toArray(new String[0]));
+        } catch (Exception e) {
+          throw new AssertionError("unable to parse args: " + args, e);
+        }
+        upgrader.upgrade();
+        
+        Directory upgradedDir = newFSDirectory(dir);
+        try {
+          checkAllSegmentsUpgraded(upgradedDir);
+        } finally {
+          upgradedDir.close();
+        }
+      }
+    } finally {
+      System.setOut(savedSystemOut);
+    }
+  }
+
+  public void testUpgradeOldSingleSegmentIndexWithAdditions() throws Exception {
+    for (String name : oldSingleSegmentNames) {
+      if (VERBOSE) {
+        System.out.println("testUpgradeOldSingleSegmentIndexWithAdditions: index=" +name);
+      }
+      Directory dir = newDirectory(oldIndexDirs.get(name));
+      if (dir instanceof MockDirectoryWrapper) {
+        // we need to ensure we delete old commits for this test,
+        // otherwise IndexUpgrader gets angry
+        ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);
+      }
+
+      assertEquals("Original index must be single segment", 1, getNumberOfSegments(dir));
+
+      // create a bunch of dummy segments
+      int id = 40;
+      RAMDirectory ramDir = new RAMDirectory();
+      for (int i = 0; i < 3; i++) {
+        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:
+        MergePolicy mp = random().nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();
+        IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()))
+          .setMergePolicy(mp).setCommitOnClose(false);
+        IndexWriter w = new IndexWriter(ramDir, iwc);
+        // add few more docs:
+        for(int j = 0; j < RANDOM_MULTIPLIER * random().nextInt(30); j++) {
+          addDoc(w, id++);
+        }
+        try {
+          w.commit();
+        } finally {
+          w.close();
+        }
+      }
+      
+      // add dummy segments (which are all in current
+      // version) to single segment index
+      MergePolicy mp = random().nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();
+      IndexWriterConfig iwc = new IndexWriterConfig(null)
+        .setMergePolicy(mp).setCommitOnClose(false);
+      IndexWriter w = new IndexWriter(dir, iwc);
+      w.addIndexes(ramDir);
+      try {
+        w.commit();
+      } finally {
+        w.close();
+      }
+      
+      // determine count of segments in modified index
+      final int origSegCount = getNumberOfSegments(dir);
+      
+      // ensure there is only one commit
+      assertEquals(1, DirectoryReader.listCommits(dir).size());
+      newIndexUpgrader(dir).upgrade();
+
+      final int segCount = checkAllSegmentsUpgraded(dir);
+      assertEquals("Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged",
+        origSegCount, segCount);
+      
+      dir.close();
+    }
+  }
+
+  public static final String moreTermsIndex = "moreterms.40.zip";
+
+  public void testMoreTerms() throws Exception {
+    File oldIndexDir = createTempDir("moreterms");
+    TestUtil.unzip(getDataFile(moreTermsIndex), oldIndexDir);
+    Directory dir = newFSDirectory(oldIndexDir);
+    // TODO: more tests
+    TestUtil.checkIndex(dir);
+    dir.close();
+  }
+
+  public static final String dvUpdatesIndex = "dvupdates.48.zip";
+
+  private void assertNumericDocValues(AtomicReader r, String f, String cf) throws IOException {
+    NumericDocValues ndvf = r.getNumericDocValues(f);
+    NumericDocValues ndvcf = r.getNumericDocValues(cf);
+    for (int i = 0; i < r.maxDoc(); i++) {
+      assertEquals(ndvcf.get(i), ndvf.get(i)*2);
+    }
+  }
+  
+  private void assertBinaryDocValues(AtomicReader r, String f, String cf) throws IOException {
+    BinaryDocValues bdvf = r.getBinaryDocValues(f);
+    BinaryDocValues bdvcf = r.getBinaryDocValues(cf);
+    for (int i = 0; i < r.maxDoc(); i++) {
+      assertEquals(TestDocValuesUpdatesOnOldSegments.getValue(bdvcf, i), TestDocValuesUpdatesOnOldSegments.getValue(bdvf, i)*2);
+    }
+  }
+  
+  private void verifyDocValues(Directory dir) throws IOException {
+    DirectoryReader reader = DirectoryReader.open(dir);
+    for (AtomicReaderContext context : reader.leaves()) {
+      AtomicReader r = context.reader();
+      assertNumericDocValues(r, "ndv1", "ndv1_c");
+      assertNumericDocValues(r, "ndv2", "ndv2_c");
+      assertBinaryDocValues(r, "bdv1", "bdv1_c");
+      assertBinaryDocValues(r, "bdv2", "bdv2_c");
+    }
+    reader.close();
+  }
+  
+  public void testDocValuesUpdates() throws Exception {
+    File oldIndexDir = createTempDir("dvupdates");
+    TestUtil.unzip(getDataFile(dvUpdatesIndex), oldIndexDir);
+    Directory dir = newFSDirectory(oldIndexDir);
+    
+    verifyDocValues(dir);
+    
+    // update fields and verify index
+    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(dir, conf);
+    updateNumeric(writer, "1", "ndv1", "ndv1_c", 300L);
+    updateNumeric(writer, "1", "ndv2", "ndv2_c", 300L);
+    updateBinary(writer, "1", "bdv1", "bdv1_c", 300L);
+    updateBinary(writer, "1", "bdv2", "bdv2_c", 300L);
+    writer.commit();
+    verifyDocValues(dir);
+    
+    // merge all segments
+    writer.forceMerge(1);
+    writer.commit();
+    verifyDocValues(dir);
+    
+    writer.close();
+    dir.close();
+  }
+  
+  // LUCENE-5907
+  public void testUpgradeWithNRTReader() throws Exception {
+    for (String name : oldNames) {
+      Directory dir = newDirectory(oldIndexDirs.get(name));
+
+      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
+                                           .setOpenMode(OpenMode.APPEND));
+      writer.addDocument(new Document());
+      DirectoryReader r = DirectoryReader.open(writer, true);
+      writer.commit();
+      r.close();
+      writer.forceMerge(1);
+      writer.commit();
+      writer.rollback();
+      new SegmentInfos().read(dir);
+      dir.close();
+    }
+  }
+
+  // LUCENE-5907
+  public void testUpgradeThenMultipleCommits() throws Exception {
+    for (String name : oldNames) {
+      Directory dir = newDirectory(oldIndexDirs.get(name));
+
+      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
+                                           .setOpenMode(OpenMode.APPEND));
+      writer.addDocument(new Document());
+      writer.commit();
+      writer.addDocument(new Document());
+      writer.commit();
+      writer.close();
+      dir.close();
+    }
+  }
+}
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/dvupdates.48.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/dvupdates.48.zip
new file mode 100755
index 0000000..900c670
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/dvupdates.48.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.cfs.zip
new file mode 100644
index 0000000..4974749
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.nocfs.zip
new file mode 100644
index 0000000..9699080
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.optimized.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.optimized.cfs.zip
new file mode 100644
index 0000000..209c436
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.optimized.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.optimized.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.optimized.nocfs.zip
new file mode 100644
index 0000000..0eaffd0
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.40.optimized.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.41.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.41.cfs.zip
new file mode 100644
index 0000000..da2745e
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.41.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.41.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.41.nocfs.zip
new file mode 100644
index 0000000..c056bcb
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.41.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.42.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.42.cfs.zip
new file mode 100644
index 0000000..5945fe5
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.42.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.42.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.42.nocfs.zip
new file mode 100644
index 0000000..11de1f1
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.42.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.45.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.45.cfs.zip
new file mode 100644
index 0000000..10a8a1a
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.45.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.45.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.45.nocfs.zip
new file mode 100644
index 0000000..7825e2a
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.45.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.461.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.461.cfs.zip
new file mode 100644
index 0000000..8f18185
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.461.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.461.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.461.nocfs.zip
new file mode 100644
index 0000000..cf0173c
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.461.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.49.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.49.cfs.zip
new file mode 100644
index 0000000..b77750c
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.49.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/index.49.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.49.nocfs.zip
new file mode 100644
index 0000000..b6af927
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/index.49.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/moreterms.40.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/moreterms.40.zip
new file mode 100644
index 0000000..53ad7ce
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/moreterms.40.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.19.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.19.cfs.zip
new file mode 100644
index 0000000..4fd9b32
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.19.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.19.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.19.nocfs.zip
new file mode 100644
index 0000000..e0d9142
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.19.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.20.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.20.cfs.zip
new file mode 100644
index 0000000..4b931ae
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.20.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.20.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.20.nocfs.zip
new file mode 100644
index 0000000..1275cdf
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.20.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.21.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.21.cfs.zip
new file mode 100644
index 0000000..473c138
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.21.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.21.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.21.nocfs.zip
new file mode 100644
index 0000000..d0582d0
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.21.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.22.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.22.cfs.zip
new file mode 100644
index 0000000..1236307
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.22.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.22.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.22.nocfs.zip
new file mode 100644
index 0000000..216ddf3
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.22.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.23.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.23.cfs.zip
new file mode 100644
index 0000000..b5fdeef
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.23.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.23.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.23.nocfs.zip
new file mode 100644
index 0000000..9137ae6
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.23.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.24.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.24.cfs.zip
new file mode 100644
index 0000000..2c666a9
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.24.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.24.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.24.nocfs.zip
new file mode 100644
index 0000000..c223875
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.24.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.29.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.29.cfs.zip
new file mode 100644
index 0000000..c694c78
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.29.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.29.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.29.nocfs.zip
new file mode 100644
index 0000000..298cab7
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.29.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.30.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.30.cfs.zip
new file mode 100644
index 0000000..d5978c8
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.30.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.30.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.30.nocfs.zip
new file mode 100644
index 0000000..28cd83b
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.30.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.31.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.31.cfs.zip
new file mode 100644
index 0000000..8f123a7
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.31.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.31.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.31.nocfs.zip
new file mode 100644
index 0000000..21434e1
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.31.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.32.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.32.cfs.zip
new file mode 100644
index 0000000..eff3153
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.32.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.32.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.32.nocfs.zip
new file mode 100644
index 0000000..0b345da
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.32.nocfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.34.cfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.34.cfs.zip
new file mode 100644
index 0000000..257e9d8
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.34.cfs.zip differ
diff --git a/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.34.nocfs.zip b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.34.nocfs.zip
new file mode 100644
index 0000000..935d6a1
Binary files /dev/null and b/lucene/backward-codecs/src/test/org/apache/lucene/index/unsupported.34.nocfs.zip differ
diff --git a/lucene/common-build.xml b/lucene/common-build.xml
index 14095ed..acdb275 100644
--- a/lucene/common-build.xml
+++ b/lucene/common-build.xml
@@ -227,6 +227,7 @@
   <property name="src.dir" location="src/java"/>
   <property name="resources.dir" location="${src.dir}/../resources"/>
   <property name="tests.src.dir" location="src/test"/>
+  <property name="tests.resources.dir" location="${tests.src.dir}/../test-resources"/>
   <available property="module.has.tests" type="dir" file="${tests.src.dir}"/>
   <property name="build.dir" location="build"/>
   <!-- Needed in case a module needs the original build, also for compile-tools to be called from a module -->
@@ -794,10 +795,14 @@
   </target>
 
   <target name="compile-test" depends="compile-core,compile-test-framework">
-  	<compile-test-macro srcdir="${tests.src.dir}" destdir="${build.dir}/classes/test"
-  						test.classpath="test.classpath"/>
+    <compile-test-macro srcdir="${tests.src.dir}" destdir="${build.dir}/classes/test" test.classpath="test.classpath"/>
+
+    <!-- Copy the resources folder (if existent) -->
+    <copy todir="${build.dir}/classes/test">
+      <fileset dir="${tests.resources.dir}" erroronmissingdir="no"/>
+    </copy>
   </target>
-	
+
   <macrodef name="compile-test-macro" description="Compiles junit tests.">
   	<attribute name="srcdir"/>
   	<attribute name="destdir"/>
@@ -1720,6 +1725,7 @@ ${ant.project.name}.test.dependencies=${test.classpath.list}
 
       <!-- TODO: Check all resource files. Currently not all stopword and similar files have no header! -->
       <fileset dir="${resources.dir}" includes="META-INF/**" erroronmissingdir="false"/>
+      <fileset dir="${tests.resources.dir}" includes="META-INF/**" erroronmissingdir="false"/>
       
       <!-- BSD 4-clause stuff (is disallowed below) -->
       <rat:substringMatcher licenseFamilyCategory="BSD4 "
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java
index d8cf4692..5ffcb6c 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java
@@ -23,7 +23,6 @@ import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.StoredFieldsReader;
 import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.MergePolicy;
 import org.apache.lucene.index.SegmentInfo;
@@ -32,14 +31,12 @@ import org.apache.lucene.store.IOContext;
 
 
 /**
- * A {@link StoredFieldsFormat} that is very similar to
- * {@link Lucene40StoredFieldsFormat} but compresses documents in chunks in
+ * A {@link StoredFieldsFormat} that compresses documents in chunks in
  * order to improve the compression ratio.
  * <p>
  * For a chunk size of <tt>chunkSize</tt> bytes, this {@link StoredFieldsFormat}
  * does not support documents larger than (<tt>2<sup>31</sup> - chunkSize</tt>)
- * bytes. In case this is a problem, you should use another format, such as
- * {@link Lucene40StoredFieldsFormat}.
+ * bytes.
  * <p>
  * For optimal performance, you should use a {@link MergePolicy} that returns
  * segments that have the biggest byte size first.
@@ -81,8 +78,6 @@ public class CompressingStoredFieldsFormat extends StoredFieldsFormat {
    * <p>
    * <code>chunkSize</code> is the minimum byte size of a chunk of documents.
    * A value of <code>1</code> can make sense if there is redundancy across
-   * fields. In that case, both performance and compression ratio should be
-   * better than with {@link Lucene40StoredFieldsFormat} with compressed
    * fields.
    * <p>
    * Higher values of <code>chunkSize</code> should improve the compression
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
deleted file mode 100644
index bb7f75f..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
+++ /dev/null
@@ -1,118 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-
-/**
- * Implements the Lucene 4.0 index format, with configurable per-field postings formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene40 package documentation for file format details.
- * @deprecated Only for reading old 4.0 segments
- */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene42Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
-@Deprecated
-public class Lucene40Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
-  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene40Codec.this.getPostingsFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene40Codec() {
-    super("Lucene40");
-  }
-  
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return infosFormat;
-  }
-  
-  private final DocValuesFormat defaultDVFormat = new Lucene40DocValuesFormat();
-
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return defaultDVFormat;
-  }
-
-  private final NormsFormat normsFormat = new Lucene40NormsFormat();
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene40"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene40");
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java
deleted file mode 100644
index 4f350ef..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java
+++ /dev/null
@@ -1,206 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Lucene 4.0 DocValues format.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.dv.cfs</tt>: {@link CompoundFileDirectory compound container}</li>
- *   <li><tt>.dv.cfe</tt>: {@link CompoundFileDirectory compound entries}</li>
- * </ul>
- * Entries within the compound file:
- * <ul>
- *   <li><tt>&lt;segment&gt;_&lt;fieldNumber&gt;.dat</tt>: data values</li>
- *   <li><tt>&lt;segment&gt;_&lt;fieldNumber&gt;.idx</tt>: index into the .dat for DEREF types</li>
- * </ul>
- * <p>
- * There are several many types of {@code DocValues} with different encodings.
- * From the perspective of filenames, all types store their values in <tt>.dat</tt>
- * entries within the compound file. In the case of dereferenced/sorted types, the <tt>.dat</tt>
- * actually contains only the unique values, and an additional <tt>.idx</tt> file contains
- * pointers to these unique values.
- * </p>
- * Formats:
- * <ul>
- *    <li>{@code VAR_INTS} .dat --&gt; Header, PackedType, MinValue, 
- *        DefaultValue, PackedStream</li>
- *    <li>{@code FIXED_INTS_8} .dat --&gt; Header, ValueSize, 
- *        {@link DataOutput#writeByte Byte}<sup>maxdoc</sup></li>
- *    <li>{@code FIXED_INTS_16} .dat --&gt; Header, ValueSize,
- *        {@link DataOutput#writeShort Short}<sup>maxdoc</sup></li>
- *    <li>{@code FIXED_INTS_32} .dat --&gt; Header, ValueSize,
- *        {@link DataOutput#writeInt Int32}<sup>maxdoc</sup></li>
- *    <li>{@code FIXED_INTS_64} .dat --&gt; Header, ValueSize,
- *        {@link DataOutput#writeLong Int64}<sup>maxdoc</sup></li>
- *    <li>{@code FLOAT_32} .dat --&gt; Header, ValueSize, Float32<sup>maxdoc</sup></li>
- *    <li>{@code FLOAT_64} .dat --&gt; Header, ValueSize, Float64<sup>maxdoc</sup></li>
- *    <li>{@code BYTES_FIXED_STRAIGHT} .dat --&gt; Header, ValueSize,
- *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>maxdoc</sup></li>
- *    <li>{@code BYTES_VAR_STRAIGHT} .idx --&gt; Header, TotalBytes, Addresses</li>
- *    <li>{@code BYTES_VAR_STRAIGHT} .dat --&gt; Header,
-          ({@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>maxdoc</sup></li>
- *    <li>{@code BYTES_FIXED_DEREF} .idx --&gt; Header, NumValues, Addresses</li>
- *    <li>{@code BYTES_FIXED_DEREF} .dat --&gt; Header, ValueSize,
- *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>NumValues</sup></li>
- *    <li>{@code BYTES_VAR_DEREF} .idx --&gt; Header, TotalVarBytes, Addresses</li>
- *    <li>{@code BYTES_VAR_DEREF} .dat --&gt; Header,
- *        (LengthPrefix + {@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>NumValues</sup></li>
- *    <li>{@code BYTES_FIXED_SORTED} .idx --&gt; Header, NumValues, Ordinals</li>
- *    <li>{@code BYTES_FIXED_SORTED} .dat --&gt; Header, ValueSize,
- *        ({@link DataOutput#writeByte Byte} * ValueSize)<sup>NumValues</sup></li>
- *    <li>{@code BYTES_VAR_SORTED} .idx --&gt; Header, TotalVarBytes, Addresses, Ordinals</li>
- *    <li>{@code BYTES_VAR_SORTED} .dat --&gt; Header,
- *        ({@link DataOutput#writeByte Byte} * <i>variable ValueSize</i>)<sup>NumValues</sup></li>
- * </ul>
- * Data Types:
- * <ul>
- *    <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *    <li>PackedType --&gt; {@link DataOutput#writeByte Byte}</li>
- *    <li>MaxAddress, MinValue, DefaultValue --&gt; {@link DataOutput#writeLong Int64}</li>
- *    <li>PackedStream, Addresses, Ordinals --&gt; {@link PackedInts}</li>
- *    <li>ValueSize, NumValues --&gt; {@link DataOutput#writeInt Int32}</li>
- *    <li>Float32 --&gt; 32-bit float encoded with {@link Float#floatToRawIntBits(float)}
- *                       then written as {@link DataOutput#writeInt Int32}</li>
- *    <li>Float64 --&gt; 64-bit float encoded with {@link Double#doubleToRawLongBits(double)}
- *                       then written as {@link DataOutput#writeLong Int64}</li>
- *    <li>TotalBytes --&gt; {@link DataOutput#writeVLong VLong}</li>
- *    <li>TotalVarBytes --&gt; {@link DataOutput#writeLong Int64}</li>
- *    <li>LengthPrefix --&gt; Length of the data value as {@link DataOutput#writeVInt VInt} (maximum
- *                       of 2 bytes)</li>
- * </ul>
- * Notes:
- * <ul>
- *    <li>PackedType is a 0 when compressed, 1 when the stream is written as 64-bit integers.</li>
- *    <li>Addresses stores pointers to the actual byte location (indexed by docid). In the VAR_STRAIGHT
- *        case, each entry can have a different length, so to determine the length, docid+1 is 
- *        retrieved. A sentinel address is written at the end for the VAR_STRAIGHT case, so the Addresses 
- *        stream contains maxdoc+1 indices. For the deduplicated VAR_DEREF case, each length
- *        is encoded as a prefix to the data itself as a {@link DataOutput#writeVInt VInt} 
- *        (maximum of 2 bytes).</li>
- *    <li>Ordinals stores the term ID in sorted order (indexed by docid). In the FIXED_SORTED case,
- *        the address into the .dat can be computed from the ordinal as 
- *        <code>Header+ValueSize+(ordinal*ValueSize)</code> because the byte length is fixed.
- *        In the VAR_SORTED case, there is double indirection (docid -> ordinal -> address), but
- *        an additional sentinel ordinal+address is always written (so there are NumValues+1 ordinals). To
- *        determine the length, ord+1's address is looked up as well.</li>
- *    <li>{@code BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT} in contrast to other straight 
- *        variants uses a <tt>.idx</tt> file to improve lookup perfromance. In contrast to 
- *        {@code BYTES_VAR_DEREF BYTES_VAR_DEREF} it doesn't apply deduplication of the document values.
- *    </li>
- * </ul>
- * <p>
- * Limitations:
- * <ul>
- *   <li> Binary doc values can be at most {@link #MAX_BINARY_FIELD_LENGTH} in length.
- * </ul>
- * @deprecated Only for reading old 4.0 and 4.1 segments
- */
-@Deprecated
-// NOTE: not registered in SPI, doesnt respect segment suffix, etc
-// for back compat only!
-public class Lucene40DocValuesFormat extends DocValuesFormat {
-  
-  /** Maximum length for each binary doc values field. */
-  public static final int MAX_BINARY_FIELD_LENGTH = (1 << 15) - 2;
-  
-  /** Sole constructor. */
-  public Lucene40DocValuesFormat() {
-    super("Lucene40");
-  }
-  
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-  
-  @Override
-  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
-    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
-                                                     "dv", 
-                                                     IndexFileNames.COMPOUND_FILE_EXTENSION);
-    return new Lucene40DocValuesReader(state, filename, Lucene40FieldInfosReader.LEGACY_DV_TYPE_KEY);
-  }
-  
-  // constants for VAR_INTS
-  static final String VAR_INTS_CODEC_NAME = "PackedInts";
-  static final int VAR_INTS_VERSION_START = 0;
-  static final int VAR_INTS_VERSION_CURRENT = VAR_INTS_VERSION_START;
-  static final byte VAR_INTS_PACKED = 0x00;
-  static final byte VAR_INTS_FIXED_64 = 0x01;
-  
-  // constants for FIXED_INTS_8, FIXED_INTS_16, FIXED_INTS_32, FIXED_INTS_64
-  static final String INTS_CODEC_NAME = "Ints";
-  static final int INTS_VERSION_START = 0;
-  static final int INTS_VERSION_CURRENT = INTS_VERSION_START;
-  
-  // constants for FLOAT_32, FLOAT_64
-  static final String FLOATS_CODEC_NAME = "Floats";
-  static final int FLOATS_VERSION_START = 0;
-  static final int FLOATS_VERSION_CURRENT = FLOATS_VERSION_START;
-  
-  // constants for BYTES_FIXED_STRAIGHT
-  static final String BYTES_FIXED_STRAIGHT_CODEC_NAME = "FixedStraightBytes";
-  static final int BYTES_FIXED_STRAIGHT_VERSION_START = 0;
-  static final int BYTES_FIXED_STRAIGHT_VERSION_CURRENT = BYTES_FIXED_STRAIGHT_VERSION_START;
-  
-  // constants for BYTES_VAR_STRAIGHT
-  static final String BYTES_VAR_STRAIGHT_CODEC_NAME_IDX = "VarStraightBytesIdx";
-  static final String BYTES_VAR_STRAIGHT_CODEC_NAME_DAT = "VarStraightBytesDat";
-  static final int BYTES_VAR_STRAIGHT_VERSION_START = 0;
-  static final int BYTES_VAR_STRAIGHT_VERSION_CURRENT = BYTES_VAR_STRAIGHT_VERSION_START;
-  
-  // constants for BYTES_FIXED_DEREF
-  static final String BYTES_FIXED_DEREF_CODEC_NAME_IDX = "FixedDerefBytesIdx";
-  static final String BYTES_FIXED_DEREF_CODEC_NAME_DAT = "FixedDerefBytesDat";
-  static final int BYTES_FIXED_DEREF_VERSION_START = 0;
-  static final int BYTES_FIXED_DEREF_VERSION_CURRENT = BYTES_FIXED_DEREF_VERSION_START;
-  
-  // constants for BYTES_VAR_DEREF
-  static final String BYTES_VAR_DEREF_CODEC_NAME_IDX = "VarDerefBytesIdx";
-  static final String BYTES_VAR_DEREF_CODEC_NAME_DAT = "VarDerefBytesDat";
-  static final int BYTES_VAR_DEREF_VERSION_START = 0;
-  static final int BYTES_VAR_DEREF_VERSION_CURRENT = BYTES_VAR_DEREF_VERSION_START;
-  
-  // constants for BYTES_FIXED_SORTED
-  static final String BYTES_FIXED_SORTED_CODEC_NAME_IDX = "FixedSortedBytesIdx";
-  static final String BYTES_FIXED_SORTED_CODEC_NAME_DAT = "FixedSortedBytesDat";
-  static final int BYTES_FIXED_SORTED_VERSION_START = 0;
-  static final int BYTES_FIXED_SORTED_VERSION_CURRENT = BYTES_FIXED_SORTED_VERSION_START;
-  
-  // constants for BYTES_VAR_SORTED
-  // NOTE THIS IS NOT A BUG! 4.0 actually screwed this up (VAR_SORTED and VAR_DEREF have same codec header)
-  static final String BYTES_VAR_SORTED_CODEC_NAME_IDX = "VarDerefBytesIdx";
-  static final String BYTES_VAR_SORTED_CODEC_NAME_DAT = "VarDerefBytesDat";
-  static final int BYTES_VAR_SORTED_VERSION_START = 0;
-  static final int BYTES_VAR_SORTED_VERSION_CURRENT = BYTES_VAR_SORTED_VERSION_START;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
deleted file mode 100644
index f70d0fb..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesReader.java
+++ /dev/null
@@ -1,661 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosReader.LegacyDocValuesType;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Reads the 4.0 format of norms/docvalues
- * @lucene.experimental
- * @deprecated Only for reading old 4.0 and 4.1 segments
- */
-@Deprecated
-final class Lucene40DocValuesReader extends DocValuesProducer {
-  private final Directory dir;
-  private final SegmentReadState state;
-  private final String legacyKey;
-  private static final String segmentSuffix = "dv";
-
-  // ram instances we have already loaded
-  private final Map<Integer,NumericDocValues> numericInstances =
-      new HashMap<>();
-  private final Map<Integer,BinaryDocValues> binaryInstances =
-      new HashMap<>();
-  private final Map<Integer,SortedDocValues> sortedInstances =
-      new HashMap<>();
-
-  private final AtomicLong ramBytesUsed;
-
-  Lucene40DocValuesReader(SegmentReadState state, String filename, String legacyKey) throws IOException {
-    this.state = state;
-    this.legacyKey = legacyKey;
-    this.dir = new CompoundFileDirectory(state.directory, filename, state.context, false);
-    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOf(getClass()));
-  }
-
-  @Override
-  public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOException {
-    NumericDocValues instance = numericInstances.get(field.number);
-    if (instance == null) {
-      String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-      IndexInput input = dir.openInput(fileName, state.context);
-      boolean success = false;
-      try {
-        switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
-          case VAR_INTS:
-            instance = loadVarIntsField(field, input);
-            break;
-          case FIXED_INTS_8:
-            instance = loadByteField(field, input);
-            break;
-          case FIXED_INTS_16:
-            instance = loadShortField(field, input);
-            break;
-          case FIXED_INTS_32:
-            instance = loadIntField(field, input);
-            break;
-          case FIXED_INTS_64:
-            instance = loadLongField(field, input);
-            break;
-          case FLOAT_32:
-            instance = loadFloatField(field, input);
-            break;
-          case FLOAT_64:
-            instance = loadDoubleField(field, input);
-            break;
-          default:
-            throw new AssertionError();
-        }
-        CodecUtil.checkEOF(input);
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(input);
-        } else {
-          IOUtils.closeWhileHandlingException(input);
-        }
-      }
-      numericInstances.put(field.number, instance);
-    }
-    return instance;
-  }
-
-  private NumericDocValues loadVarIntsField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.VAR_INTS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.VAR_INTS_VERSION_START,
-                                 Lucene40DocValuesFormat.VAR_INTS_VERSION_CURRENT);
-    byte header = input.readByte();
-    if (header == Lucene40DocValuesFormat.VAR_INTS_FIXED_64) {
-      int maxDoc = state.segmentInfo.getDocCount();
-      final long values[] = new long[maxDoc];
-      for (int i = 0; i < values.length; i++) {
-        values[i] = input.readLong();
-      }
-      ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
-      return new NumericDocValues() {
-        @Override
-        public long get(int docID) {
-          return values[docID];
-        }
-      };
-    } else if (header == Lucene40DocValuesFormat.VAR_INTS_PACKED) {
-      final long minValue = input.readLong();
-      final long defaultValue = input.readLong();
-      final PackedInts.Reader reader = PackedInts.getReader(input);
-      ramBytesUsed.addAndGet(reader.ramBytesUsed());
-      return new NumericDocValues() {
-        @Override
-        public long get(int docID) {
-          final long value = reader.get(docID);
-          if (value == defaultValue) {
-            return 0;
-          } else {
-            return minValue + value;
-          }
-        }
-      };
-    } else {
-      throw new CorruptIndexException("invalid VAR_INTS header byte: " + header + " (resource=" + input + ")");
-    }
-  }
-
-  private NumericDocValues loadByteField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.INTS_VERSION_START,
-                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 1) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final byte values[] = new byte[maxDoc];
-    input.readBytes(values, 0, values.length);
-    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  private NumericDocValues loadShortField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.INTS_VERSION_START,
-                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 2) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final short values[] = new short[maxDoc];
-    for (int i = 0; i < values.length; i++) {
-      values[i] = input.readShort();
-    }
-    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  private NumericDocValues loadIntField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.INTS_VERSION_START,
-                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 4) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final int values[] = new int[maxDoc];
-    for (int i = 0; i < values.length; i++) {
-      values[i] = input.readInt();
-    }
-    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  private NumericDocValues loadLongField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.INTS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.INTS_VERSION_START,
-                                 Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 8) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final long values[] = new long[maxDoc];
-    for (int i = 0; i < values.length; i++) {
-      values[i] = input.readLong();
-    }
-    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  private NumericDocValues loadFloatField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.FLOATS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.FLOATS_VERSION_START,
-                                 Lucene40DocValuesFormat.FLOATS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 4) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final int values[] = new int[maxDoc];
-    for (int i = 0; i < values.length; i++) {
-      values[i] = input.readInt();
-    }
-    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  private NumericDocValues loadDoubleField(FieldInfo field, IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, Lucene40DocValuesFormat.FLOATS_CODEC_NAME,
-                                 Lucene40DocValuesFormat.FLOATS_VERSION_START,
-                                 Lucene40DocValuesFormat.FLOATS_VERSION_CURRENT);
-    int valueSize = input.readInt();
-    if (valueSize != 8) {
-      throw new CorruptIndexException("invalid valueSize: " + valueSize);
-    }
-    int maxDoc = state.segmentInfo.getDocCount();
-    final long values[] = new long[maxDoc];
-    for (int i = 0; i < values.length; i++) {
-      values[i] = input.readLong();
-    }
-    ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(values));
-    return new NumericDocValues() {
-      @Override
-      public long get(int docID) {
-        return values[docID];
-      }
-    };
-  }
-
-  @Override
-  public synchronized BinaryDocValues getBinary(FieldInfo field) throws IOException {
-    BinaryDocValues instance = binaryInstances.get(field.number);
-    if (instance == null) {
-      switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
-        case BYTES_FIXED_STRAIGHT:
-          instance = loadBytesFixedStraight(field);
-          break;
-        case BYTES_VAR_STRAIGHT:
-          instance = loadBytesVarStraight(field);
-          break;
-        case BYTES_FIXED_DEREF:
-          instance = loadBytesFixedDeref(field);
-          break;
-        case BYTES_VAR_DEREF:
-          instance = loadBytesVarDeref(field);
-          break;
-        default:
-          throw new AssertionError();
-      }
-      binaryInstances.put(field.number, instance);
-    }
-    return instance;
-  }
-
-  private BinaryDocValues loadBytesFixedStraight(FieldInfo field) throws IOException {
-    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    IndexInput input = dir.openInput(fileName, state.context);
-    boolean success = false;
-    try {
-      CodecUtil.checkHeader(input, Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_CODEC_NAME,
-                                   Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_START,
-                                   Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_CURRENT);
-      final int fixedLength = input.readInt();
-      PagedBytes bytes = new PagedBytes(16);
-      bytes.copy(input, fixedLength * (long)state.segmentInfo.getDocCount());
-      final PagedBytes.Reader bytesReader = bytes.freeze(true);
-      CodecUtil.checkEOF(input);
-      success = true;
-      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed());
-      return new BinaryDocValues() {
-
-        @Override
-        public BytesRef get(int docID) {
-          final BytesRef term = new BytesRef();
-          bytesReader.fillSlice(term, fixedLength * (long)docID, fixedLength);
-          return term;
-        }
-      };
-    } finally {
-      if (success) {
-        IOUtils.close(input);
-      } else {
-        IOUtils.closeWhileHandlingException(input);
-      }
-    }
-  }
-
-  private BinaryDocValues loadBytesVarStraight(FieldInfo field) throws IOException {
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-    IndexInput data = null;
-    IndexInput index = null;
-    boolean success = false;
-    try {
-      data = dir.openInput(dataName, state.context);
-      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_DAT,
-                                  Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_START,
-                                  Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
-      index = dir.openInput(indexName, state.context);
-      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_IDX,
-                                   Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_START,
-                                   Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
-      long totalBytes = index.readVLong();
-      PagedBytes bytes = new PagedBytes(16);
-      bytes.copy(data, totalBytes);
-      final PagedBytes.Reader bytesReader = bytes.freeze(true);
-      final PackedInts.Reader reader = PackedInts.getReader(index);
-      CodecUtil.checkEOF(data);
-      CodecUtil.checkEOF(index);
-      success = true;
-      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + reader.ramBytesUsed());
-      return new BinaryDocValues() {
-        @Override
-        public BytesRef get(int docID) {
-          final BytesRef term = new BytesRef();
-          long startAddress = reader.get(docID);
-          long endAddress = reader.get(docID+1);
-          bytesReader.fillSlice(term, startAddress, (int)(endAddress - startAddress));
-          return term;
-        }
-      };
-    } finally {
-      if (success) {
-        IOUtils.close(data, index);
-      } else {
-        IOUtils.closeWhileHandlingException(data, index);
-      }
-    }
-  }
-
-  private BinaryDocValues loadBytesFixedDeref(FieldInfo field) throws IOException {
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-    IndexInput data = null;
-    IndexInput index = null;
-    boolean success = false;
-    try {
-      data = dir.openInput(dataName, state.context);
-      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_DAT,
-                                  Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_START,
-                                  Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
-      index = dir.openInput(indexName, state.context);
-      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_IDX,
-                                   Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_START,
-                                   Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
-
-      final int fixedLength = data.readInt();
-      final int valueCount = index.readInt();
-      PagedBytes bytes = new PagedBytes(16);
-      bytes.copy(data, fixedLength * (long) valueCount);
-      final PagedBytes.Reader bytesReader = bytes.freeze(true);
-      final PackedInts.Reader reader = PackedInts.getReader(index);
-      CodecUtil.checkEOF(data);
-      CodecUtil.checkEOF(index);
-      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + reader.ramBytesUsed());
-      success = true;
-      return new BinaryDocValues() {
-        @Override
-        public BytesRef get(int docID) {
-          final BytesRef term = new BytesRef();
-          final long offset = fixedLength * reader.get(docID);
-          bytesReader.fillSlice(term, offset, fixedLength);
-          return term;
-        }
-      };
-    } finally {
-      if (success) {
-        IOUtils.close(data, index);
-      } else {
-        IOUtils.closeWhileHandlingException(data, index);
-      }
-    }
-  }
-
-  private BinaryDocValues loadBytesVarDeref(FieldInfo field) throws IOException {
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-    IndexInput data = null;
-    IndexInput index = null;
-    boolean success = false;
-    try {
-      data = dir.openInput(dataName, state.context);
-      CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_DAT,
-                                  Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_START,
-                                  Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
-      index = dir.openInput(indexName, state.context);
-      CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_IDX,
-                                   Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_START,
-                                   Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
-
-      final long totalBytes = index.readLong();
-      final PagedBytes bytes = new PagedBytes(16);
-      bytes.copy(data, totalBytes);
-      final PagedBytes.Reader bytesReader = bytes.freeze(true);
-      final PackedInts.Reader reader = PackedInts.getReader(index);
-      CodecUtil.checkEOF(data);
-      CodecUtil.checkEOF(index);
-      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + reader.ramBytesUsed());
-      success = true;
-      return new BinaryDocValues() {
-        
-        @Override
-        public BytesRef get(int docID) {
-          final BytesRef term = new BytesRef();
-          long startAddress = reader.get(docID);
-          BytesRef lengthBytes = new BytesRef();
-          bytesReader.fillSlice(lengthBytes, startAddress, 1);
-          byte code = lengthBytes.bytes[lengthBytes.offset];
-          if ((code & 128) == 0) {
-            // length is 1 byte
-            bytesReader.fillSlice(term, startAddress + 1, (int) code);
-          } else {
-            bytesReader.fillSlice(lengthBytes, startAddress + 1, 1);
-            int length = ((code & 0x7f) << 8) | (lengthBytes.bytes[lengthBytes.offset] & 0xff);
-            bytesReader.fillSlice(term, startAddress + 2, length);
-          }
-          return term;
-        }
-      };
-    } finally {
-      if (success) {
-        IOUtils.close(data, index);
-      } else {
-        IOUtils.closeWhileHandlingException(data, index);
-      }
-    }
-  }
-
-  @Override
-  public synchronized SortedDocValues getSorted(FieldInfo field) throws IOException {
-    SortedDocValues instance = sortedInstances.get(field.number);
-    if (instance == null) {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-      String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-      IndexInput data = null;
-      IndexInput index = null;
-      boolean success = false;
-      try {
-        data = dir.openInput(dataName, state.context);
-        index = dir.openInput(indexName, state.context);
-        switch(LegacyDocValuesType.valueOf(field.getAttribute(legacyKey))) {
-          case BYTES_FIXED_SORTED:
-            instance = loadBytesFixedSorted(field, data, index);
-            break;
-          case BYTES_VAR_SORTED:
-            instance = loadBytesVarSorted(field, data, index);
-            break;
-          default:
-            throw new AssertionError();
-        }
-        CodecUtil.checkEOF(data);
-        CodecUtil.checkEOF(index);
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(data, index);
-        } else {
-          IOUtils.closeWhileHandlingException(data, index);
-        }
-      }
-      sortedInstances.put(field.number, instance);
-    }
-    return instance;
-  }
-
-  private SortedDocValues loadBytesFixedSorted(FieldInfo field, IndexInput data, IndexInput index) throws IOException {
-    CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_DAT,
-                                Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_START,
-                                Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
-    CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_IDX,
-                                 Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_START,
-                                 Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
-
-    final int fixedLength = data.readInt();
-    final int valueCount = index.readInt();
-
-    PagedBytes bytes = new PagedBytes(16);
-    bytes.copy(data, fixedLength * (long) valueCount);
-    final PagedBytes.Reader bytesReader = bytes.freeze(true);
-    final PackedInts.Reader reader = PackedInts.getReader(index);
-    ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + reader.ramBytesUsed());
-
-    return correctBuggyOrds(new SortedDocValues() {
-      @Override
-      public int getOrd(int docID) {
-        return (int) reader.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        final BytesRef term = new BytesRef();
-        bytesReader.fillSlice(term, fixedLength * (long) ord, fixedLength);
-        return term;
-      }
-
-      @Override
-      public int getValueCount() {
-        return valueCount;
-      }
-    });
-  }
-
-  private SortedDocValues loadBytesVarSorted(FieldInfo field, IndexInput data, IndexInput index) throws IOException {
-    CodecUtil.checkHeader(data, Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_DAT,
-                                Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_START,
-                                Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
-    CodecUtil.checkHeader(index, Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_IDX,
-                                 Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_START,
-                                 Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
-
-    long maxAddress = index.readLong();
-    PagedBytes bytes = new PagedBytes(16);
-    bytes.copy(data, maxAddress);
-    final PagedBytes.Reader bytesReader = bytes.freeze(true);
-    final PackedInts.Reader addressReader = PackedInts.getReader(index);
-    final PackedInts.Reader ordsReader = PackedInts.getReader(index);
-
-    final int valueCount = addressReader.size() - 1;
-    ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + addressReader.ramBytesUsed() + ordsReader.ramBytesUsed());
-
-    return correctBuggyOrds(new SortedDocValues() {
-      @Override
-      public int getOrd(int docID) {
-        return (int)ordsReader.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        final BytesRef term = new BytesRef();
-        long startAddress = addressReader.get(ord);
-        long endAddress = addressReader.get(ord+1);
-        bytesReader.fillSlice(term, startAddress, (int)(endAddress - startAddress));
-        return term;
-      }
-
-      @Override
-      public int getValueCount() {
-        return valueCount;
-      }
-    });
-  }
-
-  // detects and corrects LUCENE-4717 in old indexes
-  private SortedDocValues correctBuggyOrds(final SortedDocValues in) {
-    final int maxDoc = state.segmentInfo.getDocCount();
-    for (int i = 0; i < maxDoc; i++) {
-      if (in.getOrd(i) == 0) {
-        return in; // ok
-      }
-    }
-
-    // we had ord holes, return an ord-shifting-impl that corrects the bug
-    return new SortedDocValues() {
-      @Override
-      public int getOrd(int docID) {
-        return in.getOrd(docID) - 1;
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        return in.lookupOrd(ord+1);
-      }
-
-      @Override
-      public int getValueCount() {
-        return in.getValueCount() - 1;
-      }
-    };
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
-    throw new IllegalStateException("Lucene 4.0 does not support SortedNumeric: how did you pull this off?");
-  }
-
-  @Override
-  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
-    throw new IllegalStateException("Lucene 4.0 does not support SortedSet: how did you pull this off?");
-  }
-
-  @Override
-  public Bits getDocsWithField(FieldInfo field) throws IOException {
-    return new Bits.MatchAllBits(state.segmentInfo.getDocCount());
-  }
-
-  @Override
-  public void close() throws IOException {
-    dir.close();
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return ramBytesUsed.get();
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java
deleted file mode 100644
index bf51c65..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java
+++ /dev/null
@@ -1,129 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FieldInfosReader;
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.store.DataOutput; // javadoc
-
-/**
- * Lucene 4.0 Field Infos format.
- * <p>
- * <p>Field names are stored in the field info file, with suffix <tt>.fnm</tt>.</p>
- * <p>FieldInfos (.fnm) --&gt; Header,FieldsCount, &lt;FieldName,FieldNumber,
- * FieldBits,DocValuesBits,Attributes&gt; <sup>FieldsCount</sup></p>
- * <p>Data types:
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#checkHeader CodecHeader}</li>
- *   <li>FieldsCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>FieldName --&gt; {@link DataOutput#writeString String}</li>
- *   <li>FieldBits, DocValuesBits --&gt; {@link DataOutput#writeByte Byte}</li>
- *   <li>FieldNumber --&gt; {@link DataOutput#writeInt VInt}</li>
- *   <li>Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
- * </ul>
- * </p>
- * Field Descriptions:
- * <ul>
- *   <li>FieldsCount: the number of fields in this file.</li>
- *   <li>FieldName: name of the field as a UTF-8 String.</li>
- *   <li>FieldNumber: the field's number. Note that unlike previous versions of
- *       Lucene, the fields are not numbered implicitly by their order in the
- *       file, instead explicitly.</li>
- *   <li>FieldBits: a byte containing field options.
- *       <ul>
- *         <li>The low-order bit is one for indexed fields, and zero for non-indexed
- *             fields.</li>
- *         <li>The second lowest-order bit is one for fields that have term vectors
- *             stored, and zero for fields without term vectors.</li>
- *         <li>If the third lowest order-bit is set (0x4), offsets are stored into
- *             the postings list in addition to positions.</li>
- *         <li>Fourth bit is unused.</li>
- *         <li>If the fifth lowest-order bit is set (0x10), norms are omitted for the
- *             indexed field.</li>
- *         <li>If the sixth lowest-order bit is set (0x20), payloads are stored for the
- *             indexed field.</li>
- *         <li>If the seventh lowest-order bit is set (0x40), term frequencies and
- *             positions omitted for the indexed field.</li>
- *         <li>If the eighth lowest-order bit is set (0x80), positions are omitted for the
- *             indexed field.</li>
- *       </ul>
- *    </li>
- *    <li>DocValuesBits: a byte containing per-document value types. The type
- *        recorded as two four-bit integers, with the high-order bits representing
- *        <code>norms</code> options, and the low-order bits representing 
- *        {@code DocValues} options. Each four-bit integer can be decoded as such:
- *        <ul>
- *          <li>0: no DocValues for this field.</li>
- *          <li>1: variable-width signed integers. ({@code Type#VAR_INTS VAR_INTS})</li>
- *          <li>2: 32-bit floating point values. ({@code Type#FLOAT_32 FLOAT_32})</li>
- *          <li>3: 64-bit floating point values. ({@code Type#FLOAT_64 FLOAT_64})</li>
- *          <li>4: fixed-length byte array values. ({@code Type#BYTES_FIXED_STRAIGHT BYTES_FIXED_STRAIGHT})</li>
- *          <li>5: fixed-length dereferenced byte array values. ({@code Type#BYTES_FIXED_DEREF BYTES_FIXED_DEREF})</li>
- *          <li>6: variable-length byte array values. ({@code Type#BYTES_VAR_STRAIGHT BYTES_VAR_STRAIGHT})</li>
- *          <li>7: variable-length dereferenced byte array values. ({@code Type#BYTES_VAR_DEREF BYTES_VAR_DEREF})</li>
- *          <li>8: 16-bit signed integers. ({@code Type#FIXED_INTS_16 FIXED_INTS_16})</li>
- *          <li>9: 32-bit signed integers. ({@code Type#FIXED_INTS_32 FIXED_INTS_32})</li>
- *          <li>10: 64-bit signed integers. ({@code Type#FIXED_INTS_64 FIXED_INTS_64})</li>
- *          <li>11: 8-bit signed integers. ({@code Type#FIXED_INTS_8 FIXED_INTS_8})</li>
- *          <li>12: fixed-length sorted byte array values. ({@code Type#BYTES_FIXED_SORTED BYTES_FIXED_SORTED})</li>
- *          <li>13: variable-length sorted byte array values. ({@code Type#BYTES_VAR_SORTED BYTES_VAR_SORTED})</li>
- *        </ul>
- *    </li>
- *    <li>Attributes: a key-value map of codec-private attributes.</li>
- * </ul>
- *
- * @lucene.experimental
- * @deprecated Only for reading old 4.0 and 4.1 segments
- */
-@Deprecated
-public class Lucene40FieldInfosFormat extends FieldInfosFormat {
-  private final FieldInfosReader reader = new Lucene40FieldInfosReader();
-  
-  /** Sole constructor. */
-  public Lucene40FieldInfosFormat() {
-  }
-
-  @Override
-  public FieldInfosReader getFieldInfosReader() throws IOException {
-    return reader;
-  }
-
-  @Override
-  public FieldInfosWriter getFieldInfosWriter() throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-  
-  /** Extension of field infos */
-  static final String FIELD_INFOS_EXTENSION = "fnm";
-  
-  static final String CODEC_NAME = "Lucene40FieldInfos";
-  static final int FORMAT_START = 0;
-  static final int FORMAT_CURRENT = FORMAT_START;
-  
-  static final byte IS_INDEXED = 0x1;
-  static final byte STORE_TERMVECTOR = 0x2;
-  static final byte STORE_OFFSETS_IN_POSTINGS = 0x4;
-  static final byte OMIT_NORMS = 0x10;
-  static final byte STORE_PAYLOADS = 0x20;
-  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
-  static final byte OMIT_POSITIONS = -128;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java
deleted file mode 100644
index 6fc195c..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java
+++ /dev/null
@@ -1,153 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldInfosReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.0 FieldInfos reader.
- * 
- * @lucene.experimental
- * @see Lucene40FieldInfosFormat
- * @deprecated Only for reading old 4.0 and 4.1 segments
- */
-@Deprecated
-class Lucene40FieldInfosReader extends FieldInfosReader {
-
-  /** Sole constructor. */
-  public Lucene40FieldInfosReader() {
-  }
-
-  @Override
-  public FieldInfos read(Directory directory, String segmentName, String segmentSuffix, IOContext iocontext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene40FieldInfosFormat.FIELD_INFOS_EXTENSION);
-    IndexInput input = directory.openInput(fileName, iocontext);
-    
-    boolean success = false;
-    try {
-      CodecUtil.checkHeader(input, Lucene40FieldInfosFormat.CODEC_NAME, 
-                                   Lucene40FieldInfosFormat.FORMAT_START, 
-                                   Lucene40FieldInfosFormat.FORMAT_CURRENT);
-
-      final int size = input.readVInt(); //read in the size
-      FieldInfo infos[] = new FieldInfo[size];
-
-      for (int i = 0; i < size; i++) {
-        String name = input.readString();
-        final int fieldNumber = input.readVInt();
-        byte bits = input.readByte();
-        boolean isIndexed = (bits & Lucene40FieldInfosFormat.IS_INDEXED) != 0;
-        boolean storeTermVector = (bits & Lucene40FieldInfosFormat.STORE_TERMVECTOR) != 0;
-        boolean omitNorms = (bits & Lucene40FieldInfosFormat.OMIT_NORMS) != 0;
-        boolean storePayloads = (bits & Lucene40FieldInfosFormat.STORE_PAYLOADS) != 0;
-        final IndexOptions indexOptions;
-        if (!isIndexed) {
-          indexOptions = null;
-        } else if ((bits & Lucene40FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_ONLY;
-        } else if ((bits & Lucene40FieldInfosFormat.OMIT_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_AND_FREQS;
-        } else if ((bits & Lucene40FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS) != 0) {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
-        } else {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        }
-
-        // LUCENE-3027: past indices were able to write
-        // storePayloads=true when omitTFAP is also true,
-        // which is invalid.  We correct that, here:
-        if (isIndexed && indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
-          storePayloads = false;
-        }
-        // DV Types are packed in one byte
-        byte val = input.readByte();
-        final LegacyDocValuesType oldValuesType = getDocValuesType((byte) (val & 0x0F));
-        final LegacyDocValuesType oldNormsType = getDocValuesType((byte) ((val >>> 4) & 0x0F));
-        final Map<String,String> attributes = input.readStringStringMap();;
-        if (oldValuesType.mapping != null) {
-          attributes.put(LEGACY_DV_TYPE_KEY, oldValuesType.name());
-        }
-        if (oldNormsType.mapping != null) {
-          if (oldNormsType.mapping != DocValuesType.NUMERIC) {
-            throw new CorruptIndexException("invalid norm type: " + oldNormsType + " (resource=" + input + ")");
-          }
-          attributes.put(LEGACY_NORM_TYPE_KEY, oldNormsType.name());
-        }
-        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, oldValuesType.mapping, oldNormsType.mapping, -1, Collections.unmodifiableMap(attributes));
-      }
-
-      CodecUtil.checkEOF(input);
-      FieldInfos fieldInfos = new FieldInfos(infos);
-      success = true;
-      return fieldInfos;
-    } finally {
-      if (success) {
-        input.close();
-      } else {
-        IOUtils.closeWhileHandlingException(input);
-      }
-    }
-  }
-  
-  static final String LEGACY_DV_TYPE_KEY = Lucene40FieldInfosReader.class.getSimpleName() + ".dvtype";
-  static final String LEGACY_NORM_TYPE_KEY = Lucene40FieldInfosReader.class.getSimpleName() + ".normtype";
-  
-  // mapping of 4.0 types -> 4.2 types
-  static enum LegacyDocValuesType {
-    NONE(null),
-    VAR_INTS(DocValuesType.NUMERIC),
-    FLOAT_32(DocValuesType.NUMERIC),
-    FLOAT_64(DocValuesType.NUMERIC),
-    BYTES_FIXED_STRAIGHT(DocValuesType.BINARY),
-    BYTES_FIXED_DEREF(DocValuesType.BINARY),
-    BYTES_VAR_STRAIGHT(DocValuesType.BINARY),
-    BYTES_VAR_DEREF(DocValuesType.BINARY),
-    FIXED_INTS_16(DocValuesType.NUMERIC),
-    FIXED_INTS_32(DocValuesType.NUMERIC),
-    FIXED_INTS_64(DocValuesType.NUMERIC),
-    FIXED_INTS_8(DocValuesType.NUMERIC),
-    BYTES_FIXED_SORTED(DocValuesType.SORTED),
-    BYTES_VAR_SORTED(DocValuesType.SORTED);
-    
-    final DocValuesType mapping;
-    LegacyDocValuesType(DocValuesType mapping) {
-      this.mapping = mapping;
-    }
-  }
-  
-  // decodes a 4.0 type
-  private static LegacyDocValuesType getDocValuesType(byte b) {
-    return LegacyDocValuesType.values()[b];
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
deleted file mode 100644
index ed2f507..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
+++ /dev/null
@@ -1,63 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.CompoundFileDirectory;
-
-/**
- * Lucene 4.0 Norms Format.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.nrm.cfs</tt>: {@link CompoundFileDirectory compound container}</li>
- *   <li><tt>.nrm.cfe</tt>: {@link CompoundFileDirectory compound entries}</li>
- * </ul>
- * Norms are implemented as DocValues, so other than file extension, norms are 
- * written exactly the same way as {@link Lucene40DocValuesFormat DocValues}.
- * 
- * @see Lucene40DocValuesFormat
- * @lucene.experimental
- * @deprecated Only for reading old 4.0 and 4.1 segments
- */
-@Deprecated
-public class Lucene40NormsFormat extends NormsFormat {
-
-  /** Sole constructor. */
-  public Lucene40NormsFormat() {}
-  
-  @Override
-  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  @Override
-  public NormsProducer normsProducer(SegmentReadState state) throws IOException {
-    String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
-                                                     "nrm", 
-                                                     IndexFileNames.COMPOUND_FILE_EXTENSION);
-    return new Lucene40NormsReader(state, filename);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java
deleted file mode 100644
index 6ca7bc7..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java
+++ /dev/null
@@ -1,59 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-
-/**
- * Reads 4.0/4.1 norms.
- * Implemented the same as docvalues, but with a different filename.
- * @deprecated Only for reading old 4.0 and 4.1 segments
- */
-@Deprecated
-class Lucene40NormsReader extends NormsProducer {
-  private final Lucene40DocValuesReader impl;
-  
-  public Lucene40NormsReader(SegmentReadState state, String filename) throws IOException {
-    impl = new Lucene40DocValuesReader(state, filename, Lucene40FieldInfosReader.LEGACY_NORM_TYPE_KEY);
-  }
-  
-  @Override
-  public NumericDocValues getNorms(FieldInfo field) throws IOException {
-    return impl.getNumeric(field);
-  }
-  
-  @Override
-  public void close() throws IOException {
-    impl.close();
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return impl.ramBytesUsed();
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    impl.checkIntegrity();
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java
deleted file mode 100644
index 435a5ee..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java
+++ /dev/null
@@ -1,52 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.PostingsBaseFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/** 
- * Provides a {@link PostingsReaderBase} and {@link
- * PostingsWriterBase}.
- *
- * @deprecated Only for reading old 4.0 segments */
-
-// TODO: should these also be named / looked up via SPI?
-@Deprecated
-public final class Lucene40PostingsBaseFormat extends PostingsBaseFormat {
-
-  /** Sole constructor. */
-  public Lucene40PostingsBaseFormat() {
-    super("Lucene40");
-  }
-
-  @Override
-  public PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException {
-    return new Lucene40PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
-  }
-
-  @Override
-  public PostingsWriterBase postingsWriterBase(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
deleted file mode 100644
index 93b21a7..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
+++ /dev/null
@@ -1,281 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase; // javadocs
-import org.apache.lucene.codecs.blocktree.BlockTreeTermsReader;
-import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
-import org.apache.lucene.index.DocsEnum; // javadocs
-import org.apache.lucene.index.FieldInfo.IndexOptions; // javadocs
-import org.apache.lucene.index.FieldInfos; // javadocs
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.DataOutput; // javadocs
-import org.apache.lucene.util.fst.FST; // javadocs
-
-/** 
- * Lucene 4.0 Postings format.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.tim</tt>: <a href="#Termdictionary">Term Dictionary</a></li>
- *   <li><tt>.tip</tt>: <a href="#Termindex">Term Index</a></li>
- *   <li><tt>.frq</tt>: <a href="#Frequencies">Frequencies</a></li>
- *   <li><tt>.prx</tt>: <a href="#Positions">Positions</a></li>
- * </ul>
- * </p>
- * <p>
- * <a name="Termdictionary" id="Termdictionary"></a>
- * <h3>Term Dictionary</h3>
- *
- * <p>The .tim file contains the list of terms in each
- * field along with per-term statistics (such as docfreq)
- * and pointers to the frequencies, positions and
- * skip data in the .frq and .prx files.
- * See {@link BlockTreeTermsWriter} for more details on the format.
- * </p>
- *
- * <p>NOTE: The term dictionary can plug into different postings implementations:
- * the postings writer/reader are actually responsible for encoding 
- * and decoding the Postings Metadata and Term Metadata sections described here:</p>
- * <ul>
- *    <li>Postings Metadata --&gt; Header, SkipInterval, MaxSkipLevels, SkipMinimum</li>
- *    <li>Term Metadata --&gt; FreqDelta, SkipDelta?, ProxDelta?
- *    <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *    <li>SkipInterval,MaxSkipLevels,SkipMinimum --&gt; {@link DataOutput#writeInt Uint32}</li>
- *    <li>SkipDelta,FreqDelta,ProxDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
- * </ul>
- * <p>Notes:</p>
- * <ul>
- *    <li>Header is a {@link CodecUtil#writeHeader CodecHeader} storing the version information
- *        for the postings.</li>
- *    <li>SkipInterval is the fraction of TermDocs stored in skip tables. It is used to accelerate 
- *        {@link DocsEnum#advance(int)}. Larger values result in smaller indexes, greater 
- *        acceleration, but fewer accelerable cases, while smaller values result in bigger indexes, 
- *        less acceleration (in case of a small value for MaxSkipLevels) and more accelerable cases.
- *        </li>
- *    <li>MaxSkipLevels is the max. number of skip levels stored for each term in the .frq file. A 
- *        low value results in smaller indexes but less acceleration, a larger value results in 
- *        slightly larger indexes but greater acceleration. See format of .frq file for more 
- *        information about skip levels.</li>
- *    <li>SkipMinimum is the minimum document frequency a term must have in order to write any 
- *        skip data at all.</li>
- *    <li>FreqDelta determines the position of this term's TermFreqs within the .frq
- *        file. In particular, it is the difference between the position of this term's
- *        data in that file and the position of the previous term's data (or zero, for
- *        the first term in the block).</li>
- *    <li>ProxDelta determines the position of this term's TermPositions within the
- *        .prx file. In particular, it is the difference between the position of this
- *        term's data in that file and the position of the previous term's data (or zero,
- *        for the first term in the block. For fields that omit position data, this will
- *        be 0 since prox information is not stored.</li>
- *    <li>SkipDelta determines the position of this term's SkipData within the .frq
- *        file. In particular, it is the number of bytes after TermFreqs that the
- *        SkipData starts. In other words, it is the length of the TermFreq data.
- *        SkipDelta is only stored if DocFreq is not smaller than SkipMinimum.</li>
- * </ul>
- * <a name="Termindex" id="Termindex"></a>
- * <h3>Term Index</h3>
- * <p>The .tip file contains an index into the term dictionary, so that it can be 
- * accessed randomly.  See {@link BlockTreeTermsWriter} for more details on the format.</p>
- * <a name="Frequencies" id="Frequencies"></a>
- * <h3>Frequencies</h3>
- * <p>The .frq file contains the lists of documents which contain each term, along
- * with the frequency of the term in that document (except when frequencies are
- * omitted: {@link IndexOptions#DOCS_ONLY}).</p>
- * <ul>
- *   <li>FreqFile (.frq) --&gt; Header, &lt;TermFreqs, SkipData?&gt; <sup>TermCount</sup></li>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>TermFreqs --&gt; &lt;TermFreq&gt; <sup>DocFreq</sup></li>
- *   <li>TermFreq --&gt; DocDelta[, Freq?]</li>
- *   <li>SkipData --&gt; &lt;&lt;SkipLevelLength, SkipLevel&gt;
- *       <sup>NumSkipLevels-1</sup>, SkipLevel&gt; &lt;SkipDatum&gt;</li>
- *   <li>SkipLevel --&gt; &lt;SkipDatum&gt; <sup>DocFreq/(SkipInterval^(Level +
- *       1))</sup></li>
- *   <li>SkipDatum --&gt;
- *       DocSkip,PayloadLength?,OffsetLength?,FreqSkip,ProxSkip,SkipChildLevelPointer?</li>
- *   <li>DocDelta,Freq,DocSkip,PayloadLength,OffsetLength,FreqSkip,ProxSkip --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>SkipChildLevelPointer --&gt; {@link DataOutput#writeVLong VLong}</li>
- * </ul>
- * <p>TermFreqs are ordered by term (the term is implicit, from the term dictionary).</p>
- * <p>TermFreq entries are ordered by increasing document number.</p>
- * <p>DocDelta: if frequencies are indexed, this determines both the document
- * number and the frequency. In particular, DocDelta/2 is the difference between
- * this document number and the previous document number (or zero when this is the
- * first document in a TermFreqs). When DocDelta is odd, the frequency is one.
- * When DocDelta is even, the frequency is read as another VInt. If frequencies
- * are omitted, DocDelta contains the gap (not multiplied by 2) between document
- * numbers and no frequency information is stored.</p>
- * <p>For example, the TermFreqs for a term which occurs once in document seven
- * and three times in document eleven, with frequencies indexed, would be the
- * following sequence of VInts:</p>
- * <p>15, 8, 3</p>
- * <p>If frequencies were omitted ({@link IndexOptions#DOCS_ONLY}) it would be this
- * sequence of VInts instead:</p>
- * <p>7,4</p>
- * <p>DocSkip records the document number before every SkipInterval <sup>th</sup>
- * document in TermFreqs. If payloads and offsets are disabled for the term's field, then
- * DocSkip represents the difference from the previous value in the sequence. If
- * payloads and/or offsets are enabled for the term's field, then DocSkip/2 represents the
- * difference from the previous value in the sequence. In this case when
- * DocSkip is odd, then PayloadLength and/or OffsetLength are stored indicating the length of 
- * the last payload/offset before the SkipInterval<sup>th</sup> document in TermPositions.</p>
- * <p>PayloadLength indicates the length of the last payload.</p>
- * <p>OffsetLength indicates the length of the last offset (endOffset-startOffset).</p>
- * <p>
- * FreqSkip and ProxSkip record the position of every SkipInterval <sup>th</sup>
- * entry in FreqFile and ProxFile, respectively. File positions are relative to
- * the start of TermFreqs and Positions, to the previous SkipDatum in the
- * sequence.</p>
- * <p>For example, if DocFreq=35 and SkipInterval=16, then there are two SkipData
- * entries, containing the 15 <sup>th</sup> and 31 <sup>st</sup> document numbers
- * in TermFreqs. The first FreqSkip names the number of bytes after the beginning
- * of TermFreqs that the 16 <sup>th</sup> SkipDatum starts, and the second the
- * number of bytes after that that the 32 <sup>nd</sup> starts. The first ProxSkip
- * names the number of bytes after the beginning of Positions that the 16
- * <sup>th</sup> SkipDatum starts, and the second the number of bytes after that
- * that the 32 <sup>nd</sup> starts.</p>
- * <p>Each term can have multiple skip levels. The amount of skip levels for a
- * term is NumSkipLevels = Min(MaxSkipLevels,
- * floor(log(DocFreq/log(SkipInterval)))). The number of SkipData entries for a
- * skip level is DocFreq/(SkipInterval^(Level + 1)), whereas the lowest skip level
- * is Level=0.<br>
- * Example: SkipInterval = 4, MaxSkipLevels = 2, DocFreq = 35. Then skip level 0
- * has 8 SkipData entries, containing the 3<sup>rd</sup>, 7<sup>th</sup>,
- * 11<sup>th</sup>, 15<sup>th</sup>, 19<sup>th</sup>, 23<sup>rd</sup>,
- * 27<sup>th</sup>, and 31<sup>st</sup> document numbers in TermFreqs. Skip level
- * 1 has 2 SkipData entries, containing the 15<sup>th</sup> and 31<sup>st</sup>
- * document numbers in TermFreqs.<br>
- * The SkipData entries on all upper levels &gt; 0 contain a SkipChildLevelPointer
- * referencing the corresponding SkipData entry in level-1. In the example has
- * entry 15 on level 1 a pointer to entry 15 on level 0 and entry 31 on level 1 a
- * pointer to entry 31 on level 0.
- * </p>
- * <a name="Positions" id="Positions"></a>
- * <h3>Positions</h3>
- * <p>The .prx file contains the lists of positions that each term occurs at
- * within documents. Note that fields omitting positional data do not store
- * anything into this file, and if all fields in the index omit positional data
- * then the .prx file will not exist.</p>
- * <ul>
- *   <li>ProxFile (.prx) --&gt; Header, &lt;TermPositions&gt; <sup>TermCount</sup></li>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>TermPositions --&gt; &lt;Positions&gt; <sup>DocFreq</sup></li>
- *   <li>Positions --&gt; &lt;PositionDelta,PayloadLength?,OffsetDelta?,OffsetLength?,PayloadData?&gt; <sup>Freq</sup></li>
- *   <li>PositionDelta,OffsetDelta,OffsetLength,PayloadLength --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>PayloadData --&gt; {@link DataOutput#writeByte byte}<sup>PayloadLength</sup></li>
- * </ul>
- * <p>TermPositions are ordered by term (the term is implicit, from the term dictionary).</p>
- * <p>Positions entries are ordered by increasing document number (the document
- * number is implicit from the .frq file).</p>
- * <p>PositionDelta is, if payloads are disabled for the term's field, the
- * difference between the position of the current occurrence in the document and
- * the previous occurrence (or zero, if this is the first occurrence in this
- * document). If payloads are enabled for the term's field, then PositionDelta/2
- * is the difference between the current and the previous position. If payloads
- * are enabled and PositionDelta is odd, then PayloadLength is stored, indicating
- * the length of the payload at the current term position.</p>
- * <p>For example, the TermPositions for a term which occurs as the fourth term in
- * one document, and as the fifth and ninth term in a subsequent document, would
- * be the following sequence of VInts (payloads disabled):</p>
- * <p>4, 5, 4</p>
- * <p>PayloadData is metadata associated with the current term position. If
- * PayloadLength is stored at the current position, then it indicates the length
- * of this payload. If PayloadLength is not stored, then this payload has the same
- * length as the payload at the previous position.</p>
- * <p>OffsetDelta/2 is the difference between this position's startOffset from the
- * previous occurrence (or zero, if this is the first occurrence in this document).
- * If OffsetDelta is odd, then the length (endOffset-startOffset) differs from the
- * previous occurrence and an OffsetLength follows. Offset data is only written for
- * {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}.</p>
- * 
- *  @deprecated Only for reading old 4.0 segments */
-
-// TODO: this class could be created by wrapping
-// BlockTreeTermsDict around Lucene40PostingsBaseFormat; ie
-// we should not duplicate the code from that class here:
-@Deprecated
-public class Lucene40PostingsFormat extends PostingsFormat {
-
-  /** minimum items (terms or sub-blocks) per block for BlockTree */
-  protected final int minBlockSize;
-  /** maximum items (terms or sub-blocks) per block for BlockTree */
-  protected final int maxBlockSize;
-
-  /** Creates {@code Lucene40PostingsFormat} with default
-   *  settings. */
-  public Lucene40PostingsFormat() {
-    this(BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-  }
-
-  /** Creates {@code Lucene40PostingsFormat} with custom
-   *  values for {@code minBlockSize} and {@code
-   *  maxBlockSize} passed to block terms dictionary.
-   *  @see BlockTreeTermsWriter#BlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int) */
-  private Lucene40PostingsFormat(int minBlockSize, int maxBlockSize) {
-    super("Lucene40");
-    this.minBlockSize = minBlockSize;
-    assert minBlockSize > 1;
-    this.maxBlockSize = maxBlockSize;
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene40PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
-
-    boolean success = false;
-    try {
-      FieldsProducer ret = new BlockTreeTermsReader(
-                                                    state.directory,
-                                                    state.fieldInfos,
-                                                    state.segmentInfo,
-                                                    postings,
-                                                    state.context,
-                                                    state.segmentSuffix);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        postings.close();
-      }
-    }
-  }
-
-  /** Extension of freq postings file */
-  static final String FREQ_EXTENSION = "frq";
-
-  /** Extension of prox postings file */
-  static final String PROX_EXTENSION = "prx";
-
-  @Override
-  public String toString() {
-    return getName() + "(minBlockSize=" + minBlockSize + " maxBlockSize=" + maxBlockSize + ")";
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
deleted file mode 100644
index 0730d7b..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
+++ /dev/null
@@ -1,1170 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-
-/** 
- * Concrete class that reads the 4.0 frq/prox
- * postings format. 
- *  
- *  @see Lucene40PostingsFormat
- *  @deprecated Only for reading old 4.0 segments */
-@Deprecated
-public class Lucene40PostingsReader extends PostingsReaderBase {
-
-  final static String TERMS_CODEC = "Lucene40PostingsWriterTerms";
-  final static String FRQ_CODEC = "Lucene40PostingsWriterFrq";
-  final static String PRX_CODEC = "Lucene40PostingsWriterPrx";
-
-  //private static boolean DEBUG = BlockTreeTermsWriter.DEBUG;
-  
-  // Increment version to change it:
-  final static int VERSION_START = 0;
-  final static int VERSION_LONG_SKIP = 1;
-  final static int VERSION_CURRENT = VERSION_LONG_SKIP;
-
-  private final IndexInput freqIn;
-  private final IndexInput proxIn;
-  // public static boolean DEBUG = BlockTreeTermsWriter.DEBUG;
-
-  int skipInterval;
-  int maxSkipLevels;
-  int skipMinimum;
-
-  // private String segment;
-
-  /** Sole constructor. */
-  public Lucene40PostingsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo segmentInfo, IOContext ioContext, String segmentSuffix) throws IOException {
-    boolean success = false;
-    IndexInput freqIn = null;
-    IndexInput proxIn = null;
-    try {
-      freqIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION),
-                           ioContext);
-      CodecUtil.checkHeader(freqIn, FRQ_CODEC, VERSION_START, VERSION_CURRENT);
-      // TODO: hasProx should (somehow!) become codec private,
-      // but it's tricky because 1) FIS.hasProx is global (it
-      // could be all fields that have prox are written by a
-      // different codec), 2) the field may have had prox in
-      // the past but all docs w/ that field were deleted.
-      // Really we'd need to init prxOut lazily on write, and
-      // then somewhere record that we actually wrote it so we
-      // know whether to open on read:
-      if (fieldInfos.hasProx()) {
-        proxIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION),
-                             ioContext);
-        CodecUtil.checkHeader(proxIn, PRX_CODEC, VERSION_START, VERSION_CURRENT);
-      } else {
-        proxIn = null;
-      }
-      this.freqIn = freqIn;
-      this.proxIn = proxIn;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(freqIn, proxIn);
-      }
-    }
-  }
-
-  @Override
-  public void init(IndexInput termsIn) throws IOException {
-
-    // Make sure we are talking to the matching past writer
-    CodecUtil.checkHeader(termsIn, TERMS_CODEC, VERSION_START, VERSION_CURRENT);
-
-    skipInterval = termsIn.readInt();
-    maxSkipLevels = termsIn.readInt();
-    skipMinimum = termsIn.readInt();
-  }
-
-  // Must keep final because we do non-standard clone
-  private final static class StandardTermState extends BlockTermState {
-    long freqOffset;
-    long proxOffset;
-    long skipOffset;
-
-    @Override
-    public StandardTermState clone() {
-      StandardTermState other = new StandardTermState();
-      other.copyFrom(this);
-      return other;
-    }
-
-    @Override
-    public void copyFrom(TermState _other) {
-      super.copyFrom(_other);
-      StandardTermState other = (StandardTermState) _other;
-      freqOffset = other.freqOffset;
-      proxOffset = other.proxOffset;
-      skipOffset = other.skipOffset;
-    }
-
-    @Override
-    public String toString() {
-      return super.toString() + " freqFP=" + freqOffset + " proxFP=" + proxOffset + " skipOffset=" + skipOffset;
-    }
-  }
-
-  @Override
-  public BlockTermState newTermState() {
-    return new StandardTermState();
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      if (freqIn != null) {
-        freqIn.close();
-      }
-    } finally {
-      if (proxIn != null) {
-        proxIn.close();
-      }
-    }
-  }
-
-  @Override
-  public void decodeTerm(long[] longs, DataInput in, FieldInfo fieldInfo, BlockTermState _termState, boolean absolute)
-    throws IOException {
-    final StandardTermState termState = (StandardTermState) _termState;
-    // if (DEBUG) System.out.println("SPR: nextTerm seg=" + segment + " tbOrd=" + termState.termBlockOrd + " bytesReader.fp=" + termState.bytesReader.getPosition());
-    final boolean isFirstTerm = termState.termBlockOrd == 0;
-    if (absolute) {
-      termState.freqOffset = 0;
-      termState.proxOffset = 0;
-    }
-
-    termState.freqOffset += in.readVLong();
-    /*
-    if (DEBUG) {
-      System.out.println("  dF=" + termState.docFreq);
-      System.out.println("  freqFP=" + termState.freqOffset);
-    }
-    */
-    assert termState.freqOffset < freqIn.length();
-
-    if (termState.docFreq >= skipMinimum) {
-      termState.skipOffset = in.readVLong();
-      // if (DEBUG) System.out.println("  skipOffset=" + termState.skipOffset + " vs freqIn.length=" + freqIn.length());
-      assert termState.freqOffset + termState.skipOffset < freqIn.length();
-    } else {
-      // undefined
-    }
-
-    if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
-      termState.proxOffset += in.readVLong();
-      // if (DEBUG) System.out.println("  proxFP=" + termState.proxOffset);
-    }
-  }
-    
-  @Override
-  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-    if (canReuse(reuse, liveDocs)) {
-      // if (DEBUG) System.out.println("SPR.docs ts=" + termState);
-      return ((SegmentDocsEnumBase) reuse).reset(fieldInfo, (StandardTermState)termState);
-    }
-    return newDocsEnum(liveDocs, fieldInfo, (StandardTermState)termState);
-  }
-  
-  private boolean canReuse(DocsEnum reuse, Bits liveDocs) {
-    if (reuse != null && (reuse instanceof SegmentDocsEnumBase)) {
-      SegmentDocsEnumBase docsEnum = (SegmentDocsEnumBase) reuse;
-      // If you are using ParellelReader, and pass in a
-      // reused DocsEnum, it could have come from another
-      // reader also using standard codec
-      if (docsEnum.startFreqIn == freqIn) {
-        // we only reuse if the the actual the incoming enum has the same liveDocs as the given liveDocs
-        return liveDocs == docsEnum.liveDocs;
-      }
-    }
-    return false;
-  }
-  
-  private DocsEnum newDocsEnum(Bits liveDocs, FieldInfo fieldInfo, StandardTermState termState) throws IOException {
-    if (liveDocs == null) {
-      return new AllDocsSegmentDocsEnum(freqIn).reset(fieldInfo, termState);
-    } else {
-      return new LiveDocsSegmentDocsEnum(freqIn, liveDocs).reset(fieldInfo, termState);
-    }
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs,
-                                               DocsAndPositionsEnum reuse, int flags)
-    throws IOException {
-
-    boolean hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-
-    // TODO: can we optimize if FLAG_PAYLOADS / FLAG_OFFSETS
-    // isn't passed?
-
-    // TODO: refactor
-    if (fieldInfo.hasPayloads() || hasOffsets) {
-      SegmentFullPositionsEnum docsEnum;
-      if (reuse == null || !(reuse instanceof SegmentFullPositionsEnum)) {
-        docsEnum = new SegmentFullPositionsEnum(freqIn, proxIn);
-      } else {
-        docsEnum = (SegmentFullPositionsEnum) reuse;
-        if (docsEnum.startFreqIn != freqIn) {
-          // If you are using ParellelReader, and pass in a
-          // reused DocsEnum, it could have come from another
-          // reader also using standard codec
-          docsEnum = new SegmentFullPositionsEnum(freqIn, proxIn);
-        }
-      }
-      return docsEnum.reset(fieldInfo, (StandardTermState) termState, liveDocs);
-    } else {
-      SegmentDocsAndPositionsEnum docsEnum;
-      if (reuse == null || !(reuse instanceof SegmentDocsAndPositionsEnum)) {
-        docsEnum = new SegmentDocsAndPositionsEnum(freqIn, proxIn);
-      } else {
-        docsEnum = (SegmentDocsAndPositionsEnum) reuse;
-        if (docsEnum.startFreqIn != freqIn) {
-          // If you are using ParellelReader, and pass in a
-          // reused DocsEnum, it could have come from another
-          // reader also using standard codec
-          docsEnum = new SegmentDocsAndPositionsEnum(freqIn, proxIn);
-        }
-      }
-      return docsEnum.reset(fieldInfo, (StandardTermState) termState, liveDocs);
-    }
-  }
-
-  static final int BUFFERSIZE = 64;
-  
-  private abstract class SegmentDocsEnumBase extends DocsEnum {
-    
-    protected final int[] docs = new int[BUFFERSIZE];
-    protected final int[] freqs = new int[BUFFERSIZE];
-    
-    final IndexInput freqIn; // reuse
-    final IndexInput startFreqIn; // reuse
-    Lucene40SkipListReader skipper; // reuse - lazy loaded
-    
-    protected boolean indexOmitsTF;                               // does current field omit term freq?
-    protected boolean storePayloads;                        // does current field store payloads?
-    protected boolean storeOffsets;                         // does current field store offsets?
-
-    protected int limit;                                    // number of docs in this posting
-    protected int ord;                                      // how many docs we've read
-    protected int doc;                                 // doc we last read
-    protected int accum;                                    // accumulator for doc deltas
-    protected int freq;                                     // freq we last read
-    protected int maxBufferedDocId;
-    
-    protected int start;
-    protected int count;
-
-
-    protected long freqOffset;
-    protected long skipOffset;
-
-    protected boolean skipped;
-    protected final Bits liveDocs;
-    
-    SegmentDocsEnumBase(IndexInput startFreqIn, Bits liveDocs) {
-      this.startFreqIn = startFreqIn;
-      this.freqIn = startFreqIn.clone();
-      this.liveDocs = liveDocs;
-      
-    }
-    
-    
-    DocsEnum reset(FieldInfo fieldInfo, StandardTermState termState) throws IOException {
-      indexOmitsTF = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY;
-      storePayloads = fieldInfo.hasPayloads();
-      storeOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      freqOffset = termState.freqOffset;
-      skipOffset = termState.skipOffset;
-
-      // TODO: for full enum case (eg segment merging) this
-      // seek is unnecessary; maybe we can avoid in such
-      // cases
-      freqIn.seek(termState.freqOffset);
-      limit = termState.docFreq;
-      assert limit > 0;
-      ord = 0;
-      doc = -1;
-      accum = 0;
-      // if (DEBUG) System.out.println("  sde limit=" + limit + " freqFP=" + freqOffset);
-      skipped = false;
-
-      start = -1;
-      count = 0;
-      freq = 1;
-      if (indexOmitsTF) {
-        Arrays.fill(freqs, 1);
-      }
-      maxBufferedDocId = -1;
-      return this;
-    }
-    
-    @Override
-    public final int freq() {
-      return freq;
-    }
-
-    @Override
-    public final int docID() {
-      return doc;
-    }
-    
-    @Override
-    public final int advance(int target) throws IOException {
-      // last doc in our buffer is >= target, binary search + next()
-      if (++start < count && maxBufferedDocId >= target) {
-        if ((count-start) > 32) { // 32 seemed to be a sweetspot here so use binsearch if the pending results are a lot
-          start = binarySearch(count - 1, start, target, docs);
-          return nextDoc();
-        } else {
-          return linearScan(target);
-        }
-      }
-      
-      start = count; // buffer is consumed
-      
-      return doc = skipTo(target);
-    }
-    
-    private final int binarySearch(int hi, int low, int target, int[] docs) {
-      while (low <= hi) {
-        int mid = (hi + low) >>> 1;
-        int doc = docs[mid];
-        if (doc < target) {
-          low = mid + 1;
-        } else if (doc > target) {
-          hi = mid - 1;
-        } else {
-          low = mid;
-          break;
-        }
-      }
-      return low-1;
-    }
-    
-    final int readFreq(final IndexInput freqIn, final int code)
-        throws IOException {
-      if ((code & 1) != 0) { // if low bit is set
-        return 1; // freq is one
-      } else {
-        return freqIn.readVInt(); // else read freq
-      }
-    }
-    
-    protected abstract int linearScan(int scanTo) throws IOException;
-    
-    protected abstract int scanTo(int target) throws IOException;
-
-    protected final int refill() throws IOException {
-      final int doc = nextUnreadDoc();
-      count = 0;
-      start = -1;
-      if (doc == NO_MORE_DOCS) {
-        return NO_MORE_DOCS;
-      }
-      final int numDocs = Math.min(docs.length, limit - ord);
-      ord += numDocs;
-      if (indexOmitsTF) {
-        count = fillDocs(numDocs);
-      } else {
-        count = fillDocsAndFreqs(numDocs);
-      }
-      maxBufferedDocId = count > 0 ? docs[count-1] : NO_MORE_DOCS;
-      return doc;
-    }
-    
-
-    protected abstract int nextUnreadDoc() throws IOException;
-
-
-    private final int fillDocs(int size) throws IOException {
-      final IndexInput freqIn = this.freqIn;
-      final int docs[] = this.docs;
-      int docAc = accum;
-      for (int i = 0; i < size; i++) {
-        docAc += freqIn.readVInt();
-        docs[i] = docAc;
-      }
-      accum = docAc;
-      return size;
-    }
-    
-    private final int fillDocsAndFreqs(int size) throws IOException {
-      final IndexInput freqIn = this.freqIn;
-      final int docs[] = this.docs;
-      final int freqs[] = this.freqs;
-      int docAc = accum;
-      for (int i = 0; i < size; i++) {
-        final int code = freqIn.readVInt();
-        docAc += code >>> 1; // shift off low bit
-        freqs[i] = readFreq(freqIn, code);
-        docs[i] = docAc;
-      }
-      accum = docAc;
-      return size;
-     
-    }
-
-    private final int skipTo(int target) throws IOException {
-      if ((target - skipInterval) >= accum && limit >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and it isn't too close.
-
-        if (skipper == null) {
-          // This is the first time this enum has ever been used for skipping -- do lazy init
-          skipper = new Lucene40SkipListReader(freqIn.clone(), maxSkipLevels, skipInterval);
-        }
-
-        if (!skipped) {
-
-          // This is the first time this posting has
-          // skipped since reset() was called, so now we
-          // load the skip data for this posting
-
-          skipper.init(freqOffset + skipOffset,
-                       freqOffset, 0,
-                       limit, storePayloads, storeOffsets);
-
-          skipped = true;
-        }
-
-        final int newOrd = skipper.skipTo(target); 
-
-        if (newOrd > ord) {
-          // Skipper moved
-
-          ord = newOrd;
-          accum = skipper.getDoc();
-          freqIn.seek(skipper.getFreqPointer());
-        }
-      }
-      return scanTo(target);
-    }
-    
-    @Override
-    public long cost() {
-      return limit;
-    }
-  }
-  
-  private final class AllDocsSegmentDocsEnum extends SegmentDocsEnumBase {
-
-    AllDocsSegmentDocsEnum(IndexInput startFreqIn) {
-      super(startFreqIn, null);
-      assert liveDocs == null;
-    }
-    
-    @Override
-    public final int nextDoc() throws IOException {
-      if (++start < count) {
-        freq = freqs[start];
-        return doc = docs[start];
-      }
-      return doc = refill();
-    }
-    
-
-    @Override
-    protected final int linearScan(int scanTo) throws IOException {
-      final int[] docs = this.docs;
-      final int upTo = count;
-      for (int i = start; i < upTo; i++) {
-        final int d = docs[i];
-        if (scanTo <= d) {
-          start = i;
-          freq = freqs[i];
-          return doc = docs[i];
-        }
-      }
-      return doc = refill();
-    }
-
-    @Override
-    protected int scanTo(int target) throws IOException { 
-      int docAcc = accum;
-      int frq = 1;
-      final IndexInput freqIn = this.freqIn;
-      final boolean omitTF = indexOmitsTF;
-      final int loopLimit = limit;
-      for (int i = ord; i < loopLimit; i++) {
-        int code = freqIn.readVInt();
-        if (omitTF) {
-          docAcc += code;
-        } else {
-          docAcc += code >>> 1; // shift off low bit
-          frq = readFreq(freqIn, code);
-        }
-        if (docAcc >= target) {
-          freq = frq;
-          ord = i + 1;
-          return accum = docAcc;
-        }
-      }
-      ord = limit;
-      freq = frq;
-      accum = docAcc;
-      return NO_MORE_DOCS;
-    }
-
-    @Override
-    protected final int nextUnreadDoc() throws IOException {
-      if (ord++ < limit) {
-        int code = freqIn.readVInt();
-        if (indexOmitsTF) {
-          accum += code;
-        } else {
-          accum += code >>> 1; // shift off low bit
-          freq = readFreq(freqIn, code);
-        }
-        return accum;
-      } else {
-        return NO_MORE_DOCS;
-      }
-    }
-    
-  }
-  
-  private final class LiveDocsSegmentDocsEnum extends SegmentDocsEnumBase {
-
-    LiveDocsSegmentDocsEnum(IndexInput startFreqIn, Bits liveDocs) {
-      super(startFreqIn, liveDocs);
-      assert liveDocs != null;
-    }
-    
-    @Override
-    public final int nextDoc() throws IOException {
-      final Bits liveDocs = this.liveDocs;
-      for (int i = start+1; i < count; i++) {
-        int d = docs[i];
-        if (liveDocs.get(d)) {
-          start = i;
-          freq = freqs[i];
-          return doc = d;
-        }
-      }
-      start = count;
-      return doc = refill();
-    }
-
-    @Override
-    protected final int linearScan(int scanTo) throws IOException {
-      final int[] docs = this.docs;
-      final int upTo = count;
-      final Bits liveDocs = this.liveDocs;
-      for (int i = start; i < upTo; i++) {
-        int d = docs[i];
-        if (scanTo <= d && liveDocs.get(d)) {
-          start = i;
-          freq = freqs[i];
-          return doc = docs[i];
-        }
-      }
-      return doc = refill();
-    }
-    
-    @Override
-    protected int scanTo(int target) throws IOException { 
-      int docAcc = accum;
-      int frq = 1;
-      final IndexInput freqIn = this.freqIn;
-      final boolean omitTF = indexOmitsTF;
-      final int loopLimit = limit;
-      final Bits liveDocs = this.liveDocs;
-      for (int i = ord; i < loopLimit; i++) {
-        int code = freqIn.readVInt();
-        if (omitTF) {
-          docAcc += code;
-        } else {
-          docAcc += code >>> 1; // shift off low bit
-          frq = readFreq(freqIn, code);
-        }
-        if (docAcc >= target && liveDocs.get(docAcc)) {
-          freq = frq;
-          ord = i + 1;
-          return accum = docAcc;
-        }
-      }
-      ord = limit;
-      freq = frq;
-      accum = docAcc;
-      return NO_MORE_DOCS;
-    }
-
-    @Override
-    protected final int nextUnreadDoc() throws IOException {
-      int docAcc = accum;
-      int frq = 1;
-      final IndexInput freqIn = this.freqIn;
-      final boolean omitTF = indexOmitsTF;
-      final int loopLimit = limit;
-      final Bits liveDocs = this.liveDocs;
-      for (int i = ord; i < loopLimit; i++) {
-        int code = freqIn.readVInt();
-        if (omitTF) {
-          docAcc += code;
-        } else {
-          docAcc += code >>> 1; // shift off low bit
-          frq = readFreq(freqIn, code);
-        }
-        if (liveDocs.get(docAcc)) {
-          freq = frq;
-          ord = i + 1;
-          return accum = docAcc;
-        }
-      }
-      ord = limit;
-      freq = frq;
-      accum = docAcc;
-      return NO_MORE_DOCS;
-      
-    }
-  }
-  
-  // TODO specialize DocsAndPosEnum too
-  
-  // Decodes docs & positions. payloads nor offsets are present.
-  private final class SegmentDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    final IndexInput startFreqIn;
-    private final IndexInput freqIn;
-    private final IndexInput proxIn;
-    int limit;                                    // number of docs in this posting
-    int ord;                                      // how many docs we've read
-    int doc = -1;                                 // doc we last read
-    int accum;                                    // accumulator for doc deltas
-    int freq;                                     // freq we last read
-    int position;
-
-    Bits liveDocs;
-
-    long freqOffset;
-    long skipOffset;
-    long proxOffset;
-
-    int posPendingCount;
-
-    boolean skipped;
-    Lucene40SkipListReader skipper;
-    private long lazyProxPointer;
-
-    public SegmentDocsAndPositionsEnum(IndexInput freqIn, IndexInput proxIn) {
-      startFreqIn = freqIn;
-      this.freqIn = freqIn.clone();
-      this.proxIn = proxIn.clone();
-    }
-
-    public SegmentDocsAndPositionsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
-      assert fieldInfo.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-      assert !fieldInfo.hasPayloads();
-
-      this.liveDocs = liveDocs;
-
-      // TODO: for full enum case (eg segment merging) this
-      // seek is unnecessary; maybe we can avoid in such
-      // cases
-      freqIn.seek(termState.freqOffset);
-      lazyProxPointer = termState.proxOffset;
-
-      limit = termState.docFreq;
-      assert limit > 0;
-
-      ord = 0;
-      doc = -1;
-      accum = 0;
-      position = 0;
-
-      skipped = false;
-      posPendingCount = 0;
-
-      freqOffset = termState.freqOffset;
-      proxOffset = termState.proxOffset;
-      skipOffset = termState.skipOffset;
-      // if (DEBUG) System.out.println("StandardR.D&PE reset seg=" + segment + " limit=" + limit + " freqFP=" + freqOffset + " proxFP=" + proxOffset);
-
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      // if (DEBUG) System.out.println("SPR.nextDoc seg=" + segment + " freqIn.fp=" + freqIn.getFilePointer());
-      while(true) {
-        if (ord == limit) {
-          // if (DEBUG) System.out.println("  return END");
-          return doc = NO_MORE_DOCS;
-        }
-
-        ord++;
-
-        // Decode next doc/freq pair
-        final int code = freqIn.readVInt();
-
-        accum += code >>> 1;              // shift off low bit
-        if ((code & 1) != 0) {          // if low bit is set
-          freq = 1;                     // freq is one
-        } else {
-          freq = freqIn.readVInt();     // else read freq
-        }
-        posPendingCount += freq;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          break;
-        }
-      }
-
-      position = 0;
-
-      // if (DEBUG) System.out.println("  return doc=" + doc);
-      return (doc = accum);
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int freq() {
-      return freq;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-
-      //System.out.println("StandardR.D&PE advance target=" + target);
-
-      if ((target - skipInterval) >= doc && limit >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and it isn't too close
-
-        if (skipper == null) {
-          // This is the first time this enum has ever been used for skipping -- do lazy init
-          skipper = new Lucene40SkipListReader(freqIn.clone(), maxSkipLevels, skipInterval);
-        }
-
-        if (!skipped) {
-
-          // This is the first time this posting has
-          // skipped, since reset() was called, so now we
-          // load the skip data for this posting
-
-          skipper.init(freqOffset+skipOffset,
-                       freqOffset, proxOffset,
-                       limit, false, false);
-
-          skipped = true;
-        }
-
-        final int newOrd = skipper.skipTo(target); 
-
-        if (newOrd > ord) {
-          // Skipper moved
-          ord = newOrd;
-          doc = accum = skipper.getDoc();
-          freqIn.seek(skipper.getFreqPointer());
-          lazyProxPointer = skipper.getProxPointer();
-          posPendingCount = 0;
-          position = 0;
-        }
-      }
-        
-      // Now, linear scan for the rest:
-      do {
-        nextDoc();
-      } while (target > doc);
-
-      return doc;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-
-      if (lazyProxPointer != -1) {
-        proxIn.seek(lazyProxPointer);
-        lazyProxPointer = -1;
-      }
-
-      // scan over any docs that were iterated without their positions
-      if (posPendingCount > freq) {
-        position = 0;
-        while(posPendingCount != freq) {
-          if ((proxIn.readByte() & 0x80) == 0) {
-            posPendingCount--;
-          }
-        }
-      }
-
-      position += proxIn.readVInt();
-
-      posPendingCount--;
-
-      assert posPendingCount >= 0: "nextPosition() was called too many times (more than freq() times) posPendingCount=" + posPendingCount;
-
-      return position;
-    }
-
-    @Override
-    public int startOffset() {
-      return -1;
-    }
-
-    @Override
-    public int endOffset() {
-      return -1;
-    }
-
-    /** Returns the payload at this position, or null if no
-     *  payload was indexed. */
-    @Override
-    public BytesRef getPayload() throws IOException {
-      return null;
-    }
-    
-    @Override
-    public long cost() {
-      return limit;
-    }
-  }
-  
-  // Decodes docs & positions & (payloads and/or offsets)
-  private class SegmentFullPositionsEnum extends DocsAndPositionsEnum {
-    final IndexInput startFreqIn;
-    private final IndexInput freqIn;
-    private final IndexInput proxIn;
-
-    int limit;                                    // number of docs in this posting
-    int ord;                                      // how many docs we've read
-    int doc = -1;                                 // doc we last read
-    int accum;                                    // accumulator for doc deltas
-    int freq;                                     // freq we last read
-    int position;
-
-    Bits liveDocs;
-
-    long freqOffset;
-    long skipOffset;
-    long proxOffset;
-
-    int posPendingCount;
-    int payloadLength;
-    boolean payloadPending;
-
-    boolean skipped;
-    Lucene40SkipListReader skipper;
-    private BytesRefBuilder payload;
-    private long lazyProxPointer;
-    
-    boolean storePayloads;
-    boolean storeOffsets;
-    
-    int offsetLength;
-    int startOffset;
-
-    public SegmentFullPositionsEnum(IndexInput freqIn, IndexInput proxIn) {
-      startFreqIn = freqIn;
-      this.freqIn = freqIn.clone();
-      this.proxIn = proxIn.clone();
-    }
-
-    public SegmentFullPositionsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
-      storeOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      storePayloads = fieldInfo.hasPayloads();
-      assert fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-      assert storePayloads || storeOffsets;
-      if (payload == null) {
-        payload = new BytesRefBuilder();
-      }
-
-      this.liveDocs = liveDocs;
-
-      // TODO: for full enum case (eg segment merging) this
-      // seek is unnecessary; maybe we can avoid in such
-      // cases
-      freqIn.seek(termState.freqOffset);
-      lazyProxPointer = termState.proxOffset;
-
-      limit = termState.docFreq;
-      ord = 0;
-      doc = -1;
-      accum = 0;
-      position = 0;
-      startOffset = 0;
-
-      skipped = false;
-      posPendingCount = 0;
-      payloadPending = false;
-
-      freqOffset = termState.freqOffset;
-      proxOffset = termState.proxOffset;
-      skipOffset = termState.skipOffset;
-      //System.out.println("StandardR.D&PE reset seg=" + segment + " limit=" + limit + " freqFP=" + freqOffset + " proxFP=" + proxOffset + " this=" + this);
-
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      while(true) {
-        if (ord == limit) {
-          //System.out.println("StandardR.D&PE seg=" + segment + " nextDoc return doc=END");
-          return doc = NO_MORE_DOCS;
-        }
-
-        ord++;
-
-        // Decode next doc/freq pair
-        final int code = freqIn.readVInt();
-
-        accum += code >>> 1; // shift off low bit
-        if ((code & 1) != 0) { // if low bit is set
-          freq = 1; // freq is one
-        } else {
-          freq = freqIn.readVInt(); // else read freq
-        }
-        posPendingCount += freq;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          break;
-        }
-      }
-
-      position = 0;
-      startOffset = 0;
-
-      //System.out.println("StandardR.D&PE nextDoc seg=" + segment + " return doc=" + doc);
-      return (doc = accum);
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-
-      //System.out.println("StandardR.D&PE advance seg=" + segment + " target=" + target + " this=" + this);
-
-      if ((target - skipInterval) >= doc && limit >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and it isn't too close
-
-        if (skipper == null) {
-          // This is the first time this enum has ever been used for skipping -- do lazy init
-          skipper = new Lucene40SkipListReader(freqIn.clone(), maxSkipLevels, skipInterval);
-        }
-
-        if (!skipped) {
-
-          // This is the first time this posting has
-          // skipped, since reset() was called, so now we
-          // load the skip data for this posting
-          //System.out.println("  init skipper freqOffset=" + freqOffset + " skipOffset=" + skipOffset + " vs len=" + freqIn.length());
-          skipper.init(freqOffset+skipOffset,
-                       freqOffset, proxOffset,
-                       limit, storePayloads, storeOffsets);
-
-          skipped = true;
-        }
-
-        final int newOrd = skipper.skipTo(target); 
-
-        if (newOrd > ord) {
-          // Skipper moved
-          ord = newOrd;
-          doc = accum = skipper.getDoc();
-          freqIn.seek(skipper.getFreqPointer());
-          lazyProxPointer = skipper.getProxPointer();
-          posPendingCount = 0;
-          position = 0;
-          startOffset = 0;
-          payloadPending = false;
-          payloadLength = skipper.getPayloadLength();
-          offsetLength = skipper.getOffsetLength();
-        }
-      }
-        
-      // Now, linear scan for the rest:
-      do {
-        nextDoc();
-      } while (target > doc);
-
-      return doc;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-
-      if (lazyProxPointer != -1) {
-        proxIn.seek(lazyProxPointer);
-        lazyProxPointer = -1;
-      }
-      
-      if (payloadPending && payloadLength > 0) {
-        // payload of last position was never retrieved -- skip it
-        proxIn.seek(proxIn.getFilePointer() + payloadLength);
-        payloadPending = false;
-      }
-
-      // scan over any docs that were iterated without their positions
-      while(posPendingCount > freq) {
-        final int code = proxIn.readVInt();
-
-        if (storePayloads) {
-          if ((code & 1) != 0) {
-            // new payload length
-            payloadLength = proxIn.readVInt();
-            assert payloadLength >= 0;
-          }
-          assert payloadLength != -1;
-        }
-        
-        if (storeOffsets) {
-          if ((proxIn.readVInt() & 1) != 0) {
-            // new offset length
-            offsetLength = proxIn.readVInt();
-          }
-        }
-        
-        if (storePayloads) {
-          proxIn.seek(proxIn.getFilePointer() + payloadLength);
-        }
-
-        posPendingCount--;
-        position = 0;
-        startOffset = 0;
-        payloadPending = false;
-        //System.out.println("StandardR.D&PE skipPos");
-      }
-
-      // read next position
-      if (payloadPending && payloadLength > 0) {
-        // payload wasn't retrieved for last position
-        proxIn.seek(proxIn.getFilePointer()+payloadLength);
-      }
-
-      int code = proxIn.readVInt();
-      if (storePayloads) {
-        if ((code & 1) != 0) {
-          // new payload length
-          payloadLength = proxIn.readVInt();
-          assert payloadLength >= 0;
-        }
-        assert payloadLength != -1;
-          
-        payloadPending = true;
-        code >>>= 1;
-      }
-      position += code;
-      
-      if (storeOffsets) {
-        int offsetCode = proxIn.readVInt();
-        if ((offsetCode & 1) != 0) {
-          // new offset length
-          offsetLength = proxIn.readVInt();
-        }
-        startOffset += offsetCode >>> 1;
-      }
-
-      posPendingCount--;
-
-      assert posPendingCount >= 0: "nextPosition() was called too many times (more than freq() times) posPendingCount=" + posPendingCount;
-
-      //System.out.println("StandardR.D&PE nextPos   return pos=" + position);
-      return position;
-    }
-
-    @Override
-    public int startOffset() throws IOException {
-      return storeOffsets ? startOffset : -1;
-    }
-
-    @Override
-    public int endOffset() throws IOException {
-      return storeOffsets ? startOffset + offsetLength : -1;
-    }
-
-    /** Returns the payload at this position, or null if no
-     *  payload was indexed. */
-    @Override
-    public BytesRef getPayload() throws IOException {
-      if (storePayloads) {
-        if (payloadLength <= 0) {
-          return null;
-        }
-        assert lazyProxPointer == -1;
-        assert posPendingCount < freq;
-        
-        if (payloadPending) {
-          payload.grow(payloadLength);
-
-          proxIn.readBytes(payload.bytes(), 0, payloadLength);
-          payload.setLength(payloadLength);
-          payloadPending = false;
-        }
-
-        return payload.get();
-      } else {
-        return null;
-      }
-    }
-    
-    @Override
-    public long cost() {
-      return limit;
-    }
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return 0;
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {}
-
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java
deleted file mode 100644
index a5cb465..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoFormat.java
+++ /dev/null
@@ -1,98 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.SegmentInfoReader;
-import org.apache.lucene.codecs.SegmentInfoWriter;
-import org.apache.lucene.index.IndexWriter; // javadocs
-import org.apache.lucene.index.SegmentInfo; // javadocs
-import org.apache.lucene.index.SegmentInfos; // javadocs
-import org.apache.lucene.store.DataOutput; // javadocs
-
-/**
- * Lucene 4.0 Segment info format.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.si</tt>: Header, SegVersion, SegSize, IsCompoundFile, Diagnostics, Attributes, Files
- * </ul>
- * </p>
- * Data types:
- * <p>
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>SegSize --&gt; {@link DataOutput#writeInt Int32}</li>
- *   <li>SegVersion --&gt; {@link DataOutput#writeString String}</li>
- *   <li>Files --&gt; {@link DataOutput#writeStringSet Set&lt;String&gt;}</li>
- *   <li>Diagnostics, Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
- *   <li>IsCompoundFile --&gt; {@link DataOutput#writeByte Int8}</li>
- * </ul>
- * </p>
- * Field Descriptions:
- * <p>
- * <ul>
- *   <li>SegVersion is the code version that created the segment.</li>
- *   <li>SegSize is the number of documents contained in the segment index.</li>
- *   <li>IsCompoundFile records whether the segment is written as a compound file or
- *       not. If this is -1, the segment is not a compound file. If it is 1, the segment
- *       is a compound file.</li>
- *   <li>Checksum contains the CRC32 checksum of all bytes in the segments_N file up
- *       until the checksum. This is used to verify integrity of the file on opening the
- *       index.</li>
- *   <li>The Diagnostics Map is privately written by {@link IndexWriter}, as a debugging aid,
- *       for each segment it creates. It includes metadata like the current Lucene
- *       version, OS, Java version, why the segment was created (merge, flush,
- *       addIndexes), etc.</li>
- *   <li>Attributes: a key-value map of codec-private attributes.</li>
- *   <li>Files is a list of files referred to by this segment.</li>
- * </ul>
- * </p>
- * 
- * @see SegmentInfos
- * @lucene.experimental
- * @deprecated Only for reading old 4.0-4.5 segments, and supporting IndexWriter.addIndexes
- */
-@Deprecated
-public class Lucene40SegmentInfoFormat extends SegmentInfoFormat {
-  private final SegmentInfoReader reader = new Lucene40SegmentInfoReader();
-  private final SegmentInfoWriter writer = new Lucene40SegmentInfoWriter();
-
-  /** Sole constructor. */
-  public Lucene40SegmentInfoFormat() {
-  }
-  
-  @Override
-  public SegmentInfoReader getSegmentInfoReader() {
-    return reader;
-  }
-
-  // we must unfortunately support write, to allow addIndexes to write a new .si with rewritten filenames:
-  // see LUCENE-5377
-  @Override
-  public SegmentInfoWriter getSegmentInfoWriter() {
-    return writer;
-  }
-
-  /** File extension used to store {@link SegmentInfo}. */
-  public final static String SI_EXTENSION = "si";
-  static final String CODEC_NAME = "Lucene40SegmentInfo";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java
deleted file mode 100644
index 078b1ae..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoReader.java
+++ /dev/null
@@ -1,85 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.SegmentInfoReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.Version;
-
-/**
- * Lucene 4.0 implementation of {@link SegmentInfoReader}.
- * 
- * @see Lucene40SegmentInfoFormat
- * @lucene.experimental
- * @deprecated Only for reading old 4.0-4.5 segments
- */
-@Deprecated
-public class Lucene40SegmentInfoReader extends SegmentInfoReader {
-
-  /** Sole constructor. */
-  public Lucene40SegmentInfoReader() {
-  }
-
-  @Override
-  public SegmentInfo read(Directory dir, String segment, IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segment, "", Lucene40SegmentInfoFormat.SI_EXTENSION);
-    final IndexInput input = dir.openInput(fileName, context);
-    boolean success = false;
-    try {
-      CodecUtil.checkHeader(input, Lucene40SegmentInfoFormat.CODEC_NAME,
-                                   Lucene40SegmentInfoFormat.VERSION_START,
-                                   Lucene40SegmentInfoFormat.VERSION_CURRENT);
-      final Version version = Version.parse(input.readString());
-      final int docCount = input.readInt();
-      if (docCount < 0) {
-        throw new CorruptIndexException("invalid docCount: " + docCount + " (resource=" + input + ")");
-      }
-      final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
-      final Map<String,String> diagnostics = input.readStringStringMap();
-      input.readStringStringMap(); // read deprecated attributes
-      final Set<String> files = input.readStringSet();
-      
-      CodecUtil.checkEOF(input);
-
-      final SegmentInfo si = new SegmentInfo(dir, version, segment, docCount, isCompoundFile, null, diagnostics);
-      si.setFiles(files);
-
-      success = true;
-
-      return si;
-
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(input);
-      } else {
-        input.close();
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java
deleted file mode 100644
index 61bb7ad..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfoWriter.java
+++ /dev/null
@@ -1,77 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.SegmentInfoWriter;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.0 implementation of {@link SegmentInfoWriter}.
- * 
- * @see Lucene40SegmentInfoFormat
- * @lucene.experimental
- */
-@Deprecated
-public class Lucene40SegmentInfoWriter extends SegmentInfoWriter {
-
-  /** Sole constructor. */
-  public Lucene40SegmentInfoWriter() {
-  }
-
-  /** Save a single segment's info. */
-  @Override
-  public void write(Directory dir, SegmentInfo si, FieldInfos fis, IOContext ioContext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(si.name, "", Lucene40SegmentInfoFormat.SI_EXTENSION);
-    si.addFile(fileName);
-
-    final IndexOutput output = dir.createOutput(fileName, ioContext);
-
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(output, Lucene40SegmentInfoFormat.CODEC_NAME, Lucene40SegmentInfoFormat.VERSION_CURRENT);
-      // Write the Lucene version that created this segment, since 3.1
-      output.writeString(si.getVersion().toString());
-      output.writeInt(si.getDocCount());
-
-      output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
-      output.writeStringStringMap(si.getDiagnostics());
-      output.writeStringStringMap(Collections.<String,String>emptyMap());
-      output.writeStringSet(si.files());
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(output);
-        // TODO: why must we do this? do we not get tracking dir wrapper?
-        IOUtils.deleteFilesIgnoringExceptions(si.dir, fileName);
-      } else {
-        output.close();
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java
deleted file mode 100644
index 1580a39..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java
+++ /dev/null
@@ -1,143 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.codecs.MultiLevelSkipListReader;
-import org.apache.lucene.store.IndexInput;
-
-/**
- * Implements the skip list reader for the 4.0 posting list format
- * that stores positions and payloads.
- * 
- * @see Lucene40PostingsFormat
- * @deprecated Only for reading old 4.0 segments
- */
-@Deprecated
-public class Lucene40SkipListReader extends MultiLevelSkipListReader {
-  private boolean currentFieldStoresPayloads;
-  private boolean currentFieldStoresOffsets;
-  private long freqPointer[];
-  private long proxPointer[];
-  private int payloadLength[];
-  private int offsetLength[];
-  
-  private long lastFreqPointer;
-  private long lastProxPointer;
-  private int lastPayloadLength;
-  private int lastOffsetLength;
-                           
-  /** Sole constructor. */
-  public Lucene40SkipListReader(IndexInput skipStream, int maxSkipLevels, int skipInterval) {
-    super(skipStream, maxSkipLevels, skipInterval);
-    freqPointer = new long[maxSkipLevels];
-    proxPointer = new long[maxSkipLevels];
-    payloadLength = new int[maxSkipLevels];
-    offsetLength = new int[maxSkipLevels];
-  }
-
-  /** Per-term initialization. */
-  public void init(long skipPointer, long freqBasePointer, long proxBasePointer, int df, boolean storesPayloads, boolean storesOffsets) {
-    super.init(skipPointer, df);
-    this.currentFieldStoresPayloads = storesPayloads;
-    this.currentFieldStoresOffsets = storesOffsets;
-    lastFreqPointer = freqBasePointer;
-    lastProxPointer = proxBasePointer;
-
-    Arrays.fill(freqPointer, freqBasePointer);
-    Arrays.fill(proxPointer, proxBasePointer);
-    Arrays.fill(payloadLength, 0);
-    Arrays.fill(offsetLength, 0);
-  }
-
-  /** Returns the freq pointer of the doc to which the last call of 
-   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
-  public long getFreqPointer() {
-    return lastFreqPointer;
-  }
-
-  /** Returns the prox pointer of the doc to which the last call of 
-   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
-  public long getProxPointer() {
-    return lastProxPointer;
-  }
-  
-  /** Returns the payload length of the payload stored just before 
-   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
-   * has skipped.  */
-  public int getPayloadLength() {
-    return lastPayloadLength;
-  }
-  
-  /** Returns the offset length (endOffset-startOffset) of the position stored just before 
-   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
-   * has skipped.  */
-  public int getOffsetLength() {
-    return lastOffsetLength;
-  }
-  
-  @Override
-  protected void seekChild(int level) throws IOException {
-    super.seekChild(level);
-    freqPointer[level] = lastFreqPointer;
-    proxPointer[level] = lastProxPointer;
-    payloadLength[level] = lastPayloadLength;
-    offsetLength[level] = lastOffsetLength;
-  }
-  
-  @Override
-  protected void setLastSkipData(int level) {
-    super.setLastSkipData(level);
-    lastFreqPointer = freqPointer[level];
-    lastProxPointer = proxPointer[level];
-    lastPayloadLength = payloadLength[level];
-    lastOffsetLength = offsetLength[level];
-  }
-
-
-  @Override
-  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
-    int delta;
-    if (currentFieldStoresPayloads || currentFieldStoresOffsets) {
-      // the current field stores payloads and/or offsets.
-      // if the doc delta is odd then we have
-      // to read the current payload/offset lengths
-      // because it differs from the lengths of the
-      // previous payload/offset
-      delta = skipStream.readVInt();
-      if ((delta & 1) != 0) {
-        if (currentFieldStoresPayloads) {
-          payloadLength[level] = skipStream.readVInt();
-        }
-        if (currentFieldStoresOffsets) {
-          offsetLength[level] = skipStream.readVInt();
-        }
-      }
-      delta >>>= 1;
-    } else {
-      delta = skipStream.readVInt();
-    }
-
-    freqPointer[level] += skipStream.readVInt();
-    proxPointer[level] += skipStream.readVInt();
-    
-    return delta;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java
deleted file mode 100644
index dcdf360..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java
+++ /dev/null
@@ -1,99 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.StoredFieldsReader;
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.DataOutput; // javadocs
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/** 
- * Lucene 4.0 Stored Fields Format.
- * <p>Stored fields are represented by two files:</p>
- * <ol>
- * <li><a name="field_index" id="field_index"></a>
- * <p>The field index, or <tt>.fdx</tt> file.</p>
- * <p>This is used to find the location within the field data file of the fields
- * of a particular document. Because it contains fixed-length data, this file may
- * be easily randomly accessed. The position of document <i>n</i> 's field data is
- * the {@link DataOutput#writeLong Uint64} at <i>n*8</i> in this file.</p>
- * <p>This contains, for each document, a pointer to its field data, as
- * follows:</p>
- * <ul>
- * <li>FieldIndex (.fdx) --&gt; &lt;Header&gt;, &lt;FieldValuesPosition&gt; <sup>SegSize</sup></li>
- * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- * <li>FieldValuesPosition --&gt; {@link DataOutput#writeLong Uint64}</li>
- * </ul>
- * </li>
- * <li>
- * <p><a name="field_data" id="field_data"></a>The field data, or <tt>.fdt</tt> file.</p>
- * <p>This contains the stored fields of each document, as follows:</p>
- * <ul>
- * <li>FieldData (.fdt) --&gt; &lt;Header&gt;, &lt;DocFieldData&gt; <sup>SegSize</sup></li>
- * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- * <li>DocFieldData --&gt; FieldCount, &lt;FieldNum, Bits, Value&gt;
- * <sup>FieldCount</sup></li>
- * <li>FieldCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- * <li>FieldNum --&gt; {@link DataOutput#writeVInt VInt}</li>
- * <li>Bits --&gt; {@link DataOutput#writeByte Byte}</li>
- * <ul>
- * <li>low order bit reserved.</li>
- * <li>second bit is one for fields containing binary data</li>
- * <li>third bit reserved.</li>
- * <li>4th to 6th bit (mask: 0x7&lt;&lt;3) define the type of a numeric field:
- * <ul>
- * <li>all bits in mask are cleared if no numeric field at all</li>
- * <li>1&lt;&lt;3: Value is Int</li>
- * <li>2&lt;&lt;3: Value is Long</li>
- * <li>3&lt;&lt;3: Value is Int as Float (as of {@link Float#intBitsToFloat(int)}</li>
- * <li>4&lt;&lt;3: Value is Long as Double (as of {@link Double#longBitsToDouble(long)}</li>
- * </ul>
- * </li>
- * </ul>
- * <li>Value --&gt; String | BinaryValue | Int | Long (depending on Bits)</li>
- * <li>BinaryValue --&gt; ValueSize, &lt;{@link DataOutput#writeByte Byte}&gt;^ValueSize</li>
- * <li>ValueSize --&gt; {@link DataOutput#writeVInt VInt}</li>
- * </li>
- * </ul>
- * </ol>
- * @lucene.experimental */
-public class Lucene40StoredFieldsFormat extends StoredFieldsFormat {
-
-  /** Sole constructor. */
-  public Lucene40StoredFieldsFormat() {
-  }
-
-  @Override
-  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si,
-      FieldInfos fn, IOContext context) throws IOException {
-    return new Lucene40StoredFieldsReader(directory, si, fn, context);
-  }
-
-  @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si,
-      IOContext context) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java
deleted file mode 100644
index a2848dc..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java
+++ /dev/null
@@ -1,264 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.StoredFieldsReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.StoredFieldVisitor;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-import java.io.Closeable;
-import java.nio.charset.StandardCharsets;
-
-/**
- * Class responsible for access to stored document fields.
- * <p/>
- * It uses &lt;segment&gt;.fdt and &lt;segment&gt;.fdx; files.
- * 
- * @see Lucene40StoredFieldsFormat
- * @lucene.internal
- */
-public final class Lucene40StoredFieldsReader extends StoredFieldsReader implements Cloneable, Closeable {
-
-  // NOTE: bit 0 is free here!  You can steal it!
-  static final int FIELD_IS_BINARY = 1 << 1;
-
-  // the old bit 1 << 2 was compressed, is now left out
-
-  private static final int _NUMERIC_BIT_SHIFT = 3;
-  static final int FIELD_IS_NUMERIC_MASK = 0x07 << _NUMERIC_BIT_SHIFT;
-
-  static final int FIELD_IS_NUMERIC_INT = 1 << _NUMERIC_BIT_SHIFT;
-  static final int FIELD_IS_NUMERIC_LONG = 2 << _NUMERIC_BIT_SHIFT;
-  static final int FIELD_IS_NUMERIC_FLOAT = 3 << _NUMERIC_BIT_SHIFT;
-  static final int FIELD_IS_NUMERIC_DOUBLE = 4 << _NUMERIC_BIT_SHIFT;
-
-  // the next possible bits are: 1 << 6; 1 << 7
-  // currently unused: static final int FIELD_IS_NUMERIC_SHORT = 5 << _NUMERIC_BIT_SHIFT;
-  // currently unused: static final int FIELD_IS_NUMERIC_BYTE = 6 << _NUMERIC_BIT_SHIFT;
-
-  static final String CODEC_NAME_IDX = "Lucene40StoredFieldsIndex";
-  static final String CODEC_NAME_DAT = "Lucene40StoredFieldsData";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-  static final long HEADER_LENGTH_IDX = CodecUtil.headerLength(CODEC_NAME_IDX);
-  static final long HEADER_LENGTH_DAT = CodecUtil.headerLength(CODEC_NAME_DAT);
-
-
-
-  /** Extension of stored fields file */
-  public static final String FIELDS_EXTENSION = "fdt";
-  
-  /** Extension of stored fields index file */
-  public static final String FIELDS_INDEX_EXTENSION = "fdx";
-  
-  private static final long RAM_BYTES_USED = RamUsageEstimator.shallowSizeOfInstance(Lucene40StoredFieldsReader.class);
-
-  private final FieldInfos fieldInfos;
-  private final IndexInput fieldsStream;
-  private final IndexInput indexStream;
-  private int numTotalDocs;
-  private int size;
-  private boolean closed;
-
-  /** Returns a cloned FieldsReader that shares open
-   *  IndexInputs with the original one.  It is the caller's
-   *  job not to close the original FieldsReader until all
-   *  clones are called (eg, currently SegmentReader manages
-   *  this logic). */
-  @Override
-  public Lucene40StoredFieldsReader clone() {
-    ensureOpen();
-    return new Lucene40StoredFieldsReader(fieldInfos, numTotalDocs, size, fieldsStream.clone(), indexStream.clone());
-  }
-  
-  /** Used only by clone. */
-  private Lucene40StoredFieldsReader(FieldInfos fieldInfos, int numTotalDocs, int size, IndexInput fieldsStream, IndexInput indexStream) {
-    this.fieldInfos = fieldInfos;
-    this.numTotalDocs = numTotalDocs;
-    this.size = size;
-    this.fieldsStream = fieldsStream;
-    this.indexStream = indexStream;
-  }
-
-  /** Sole constructor. */
-  public Lucene40StoredFieldsReader(Directory d, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
-    final String segment = si.name;
-    boolean success = false;
-    fieldInfos = fn;
-    try {
-      fieldsStream = d.openInput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
-      final String indexStreamFN = IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION);
-      indexStream = d.openInput(indexStreamFN, context);
-      
-      CodecUtil.checkHeader(indexStream, CODEC_NAME_IDX, VERSION_START, VERSION_CURRENT);
-      CodecUtil.checkHeader(fieldsStream, CODEC_NAME_DAT, VERSION_START, VERSION_CURRENT);
-      assert HEADER_LENGTH_DAT == fieldsStream.getFilePointer();
-      assert HEADER_LENGTH_IDX == indexStream.getFilePointer();
-      final long indexSize = indexStream.length() - HEADER_LENGTH_IDX;
-      this.size = (int) (indexSize >> 3);
-      // Verify two sources of "maxDoc" agree:
-      if (this.size != si.getDocCount()) {
-        throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + this.size + " but segmentInfo shows " + si.getDocCount());
-      }
-      numTotalDocs = (int) (indexSize >> 3);
-      success = true;
-    } finally {
-      // With lock-less commits, it's entirely possible (and
-      // fine) to hit a FileNotFound exception above. In
-      // this case, we want to explicitly close any subset
-      // of things that were opened so that we don't have to
-      // wait for a GC to do so.
-      if (!success) {
-        try {
-          close();
-        } catch (Throwable t) {} // ensure we throw our original exception
-      }
-    }
-  }
-
-  /**
-   * @throws AlreadyClosedException if this FieldsReader is closed
-   */
-  private void ensureOpen() throws AlreadyClosedException {
-    if (closed) {
-      throw new AlreadyClosedException("this FieldsReader is closed");
-    }
-  }
-
-  /**
-   * Closes the underlying {@link org.apache.lucene.store.IndexInput} streams.
-   * This means that the Fields values will not be accessible.
-   *
-   * @throws IOException If an I/O error occurs
-   */
-  @Override
-  public final void close() throws IOException {
-    if (!closed) {
-      IOUtils.close(fieldsStream, indexStream);
-      closed = true;
-    }
-  }
-
-  /** Returns number of documents. */
-  public final int size() {
-    return size;
-  }
-
-  private void seekIndex(int docID) throws IOException {
-    indexStream.seek(HEADER_LENGTH_IDX + docID * 8L);
-  }
-
-  @Override
-  public final void visitDocument(int n, StoredFieldVisitor visitor) throws IOException {
-    seekIndex(n);
-    fieldsStream.seek(indexStream.readLong());
-
-    final int numFields = fieldsStream.readVInt();
-    for (int fieldIDX = 0; fieldIDX < numFields; fieldIDX++) {
-      int fieldNumber = fieldsStream.readVInt();
-      FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
-      
-      int bits = fieldsStream.readByte() & 0xFF;
-      assert bits <= (FIELD_IS_NUMERIC_MASK | FIELD_IS_BINARY): "bits=" + Integer.toHexString(bits);
-
-      switch(visitor.needsField(fieldInfo)) {
-        case YES:
-          readField(visitor, fieldInfo, bits);
-          break;
-        case NO: 
-          skipField(bits);
-          break;
-        case STOP: 
-          return;
-      }
-    }
-  }
-
-  private void readField(StoredFieldVisitor visitor, FieldInfo info, int bits) throws IOException {
-    final int numeric = bits & FIELD_IS_NUMERIC_MASK;
-    if (numeric != 0) {
-      switch(numeric) {
-        case FIELD_IS_NUMERIC_INT:
-          visitor.intField(info, fieldsStream.readInt());
-          return;
-        case FIELD_IS_NUMERIC_LONG:
-          visitor.longField(info, fieldsStream.readLong());
-          return;
-        case FIELD_IS_NUMERIC_FLOAT:
-          visitor.floatField(info, Float.intBitsToFloat(fieldsStream.readInt()));
-          return;
-        case FIELD_IS_NUMERIC_DOUBLE:
-          visitor.doubleField(info, Double.longBitsToDouble(fieldsStream.readLong()));
-          return;
-        default:
-          throw new CorruptIndexException("Invalid numeric type: " + Integer.toHexString(numeric));
-      }
-    } else { 
-      final int length = fieldsStream.readVInt();
-      byte bytes[] = new byte[length];
-      fieldsStream.readBytes(bytes, 0, length);
-      if ((bits & FIELD_IS_BINARY) != 0) {
-        visitor.binaryField(info, bytes);
-      } else {
-        visitor.stringField(info, new String(bytes, 0, bytes.length, StandardCharsets.UTF_8));
-      }
-    }
-  }
-  
-  private void skipField(int bits) throws IOException {
-    final int numeric = bits & FIELD_IS_NUMERIC_MASK;
-    if (numeric != 0) {
-      switch(numeric) {
-        case FIELD_IS_NUMERIC_INT:
-        case FIELD_IS_NUMERIC_FLOAT:
-          fieldsStream.readInt();
-          return;
-        case FIELD_IS_NUMERIC_LONG:
-        case FIELD_IS_NUMERIC_DOUBLE:
-          fieldsStream.readLong();
-          return;
-        default: 
-          throw new CorruptIndexException("Invalid numeric type: " + Integer.toHexString(numeric));
-      }
-    } else {
-      final int length = fieldsStream.readVInt();
-      fieldsStream.seek(fieldsStream.getFilePointer() + length);
-    }
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return RAM_BYTES_USED;
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {}
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java
deleted file mode 100644
index efea3bb..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java
+++ /dev/null
@@ -1,131 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.TermVectorsReader;
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.DataOutput; // javadocs
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Lucene 4.0 Term Vectors format.
- * <p>Term Vector support is an optional on a field by field basis. It consists of
- * 3 files.</p>
- * <ol>
- * <li><a name="tvx" id="tvx"></a>
- * <p>The Document Index or .tvx file.</p>
- * <p>For each document, this stores the offset into the document data (.tvd) and
- * field data (.tvf) files.</p>
- * <p>DocumentIndex (.tvx) --&gt; Header,&lt;DocumentPosition,FieldPosition&gt;
- * <sup>NumDocs</sup></p>
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>DocumentPosition --&gt; {@link DataOutput#writeLong UInt64} (offset in the .tvd file)</li>
- *   <li>FieldPosition --&gt; {@link DataOutput#writeLong UInt64} (offset in the .tvf file)</li>
- * </ul>
- * </li>
- * <li><a name="tvd" id="tvd"></a>
- * <p>The Document or .tvd file.</p>
- * <p>This contains, for each document, the number of fields, a list of the fields
- * with term vector info and finally a list of pointers to the field information
- * in the .tvf (Term Vector Fields) file.</p>
- * <p>The .tvd file is used to map out the fields that have term vectors stored
- * and where the field information is in the .tvf file.</p>
- * <p>Document (.tvd) --&gt; Header,&lt;NumFields, FieldNums,
- * FieldPositions&gt; <sup>NumDocs</sup></p>
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>NumFields --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>FieldNums --&gt; &lt;FieldNumDelta&gt; <sup>NumFields</sup></li>
- *   <li>FieldNumDelta --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>FieldPositions --&gt; &lt;FieldPositionDelta&gt; <sup>NumFields-1</sup></li>
- *   <li>FieldPositionDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
- * </ul>
- * </li>
- * <li><a name="tvf" id="tvf"></a>
- * <p>The Field or .tvf file.</p>
- * <p>This file contains, for each field that has a term vector stored, a list of
- * the terms, their frequencies and, optionally, position, offset, and payload
- * information.</p>
- * <p>Field (.tvf) --&gt; Header,&lt;NumTerms, Flags, TermFreqs&gt;
- * <sup>NumFields</sup></p>
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>NumTerms --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>Flags --&gt; {@link DataOutput#writeByte Byte}</li>
- *   <li>TermFreqs --&gt; &lt;TermText, TermFreq, Positions?, PayloadData?, Offsets?&gt;
- *       <sup>NumTerms</sup></li>
- *   <li>TermText --&gt; &lt;PrefixLength, Suffix&gt;</li>
- *   <li>PrefixLength --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>Suffix --&gt; {@link DataOutput#writeString String}</li>
- *   <li>TermFreq --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>Positions --&gt; &lt;PositionDelta PayloadLength?&gt;<sup>TermFreq</sup></li>
- *   <li>PositionDelta --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>PayloadLength --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>PayloadData --&gt; {@link DataOutput#writeByte Byte}<sup>NumPayloadBytes</sup></li>
- *   <li>Offsets --&gt; &lt;{@link DataOutput#writeVInt VInt}, {@link DataOutput#writeVInt VInt}&gt;<sup>TermFreq</sup></li>
- * </ul>
- * <p>Notes:</p>
- * <ul>
- * <li>Flags byte stores whether this term vector has position, offset, payload.
- * information stored.</li>
- * <li>Term byte prefixes are shared. The PrefixLength is the number of initial
- * bytes from the previous term which must be pre-pended to a term's suffix
- * in order to form the term's bytes. Thus, if the previous term's text was "bone"
- * and the term is "boy", the PrefixLength is two and the suffix is "y".</li>
- * <li>PositionDelta is, if payloads are disabled for the term's field, the
- * difference between the position of the current occurrence in the document and
- * the previous occurrence (or zero, if this is the first occurrence in this
- * document). If payloads are enabled for the term's field, then PositionDelta/2
- * is the difference between the current and the previous position. If payloads
- * are enabled and PositionDelta is odd, then PayloadLength is stored, indicating
- * the length of the payload at the current term position.</li>
- * <li>PayloadData is metadata associated with a term position. If
- * PayloadLength is stored at the current position, then it indicates the length
- * of this payload. If PayloadLength is not stored, then this payload has the same
- * length as the payload at the previous position. PayloadData encodes the 
- * concatenated bytes for all of a terms occurrences.</li>
- * <li>Offsets are stored as delta encoded VInts. The first VInt is the
- * startOffset, the second is the endOffset.</li>
- * </ul>
- * </li>
- * </ol>
- */
-public class Lucene40TermVectorsFormat extends TermVectorsFormat {
-
-  /** Sole constructor. */
-  public Lucene40TermVectorsFormat() {
-  }
-  
-  @Override
-  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
-    return new Lucene40TermVectorsReader(directory, segmentInfo, fieldInfos, context);
-  }
-
-  @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
deleted file mode 100644
index 857d653..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
+++ /dev/null
@@ -1,712 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.NoSuchElementException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.TermVectorsReader;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.0 Term Vectors reader.
- * <p>
- * It reads .tvd, .tvf, and .tvx files.
- * 
- * @see Lucene40TermVectorsFormat
- */
-public class Lucene40TermVectorsReader extends TermVectorsReader implements Closeable {
-
-  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1;
-
-  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2;
-  
-  static final byte STORE_PAYLOAD_WITH_TERMVECTOR = 0x4;
-  
-  /** Extension of vectors fields file */
-  static final String VECTORS_FIELDS_EXTENSION = "tvf";
-
-  /** Extension of vectors documents file */
-  static final String VECTORS_DOCUMENTS_EXTENSION = "tvd";
-
-  /** Extension of vectors index file */
-  static final String VECTORS_INDEX_EXTENSION = "tvx";
-  
-  static final String CODEC_NAME_FIELDS = "Lucene40TermVectorsFields";
-  static final String CODEC_NAME_DOCS = "Lucene40TermVectorsDocs";
-  static final String CODEC_NAME_INDEX = "Lucene40TermVectorsIndex";
-
-  static final int VERSION_NO_PAYLOADS = 0;
-  static final int VERSION_PAYLOADS = 1;
-  static final int VERSION_START = VERSION_NO_PAYLOADS;
-  static final int VERSION_CURRENT = VERSION_PAYLOADS;
-  
-  static final long HEADER_LENGTH_FIELDS = CodecUtil.headerLength(CODEC_NAME_FIELDS);
-  static final long HEADER_LENGTH_DOCS = CodecUtil.headerLength(CODEC_NAME_DOCS);
-  static final long HEADER_LENGTH_INDEX = CodecUtil.headerLength(CODEC_NAME_INDEX);
-
-  private FieldInfos fieldInfos;
-
-  private IndexInput tvx;
-  private IndexInput tvd;
-  private IndexInput tvf;
-  private int size;
-  private int numTotalDocs;
-  
-
-  /** Used by clone. */
-  Lucene40TermVectorsReader(FieldInfos fieldInfos, IndexInput tvx, IndexInput tvd, IndexInput tvf, int size, int numTotalDocs) {
-    this.fieldInfos = fieldInfos;
-    this.tvx = tvx;
-    this.tvd = tvd;
-    this.tvf = tvf;
-    this.size = size;
-    this.numTotalDocs = numTotalDocs;
-  }
-    
-  /** Sole constructor. */
-  public Lucene40TermVectorsReader(Directory d, SegmentInfo si, FieldInfos fieldInfos, IOContext context)
-    throws IOException {
-    final String segment = si.name;
-    final int size = si.getDocCount();
-    
-    boolean success = false;
-
-    try {
-      String idxName = IndexFileNames.segmentFileName(segment, "", VECTORS_INDEX_EXTENSION);
-      tvx = d.openInput(idxName, context);
-      final int tvxVersion = CodecUtil.checkHeader(tvx, CODEC_NAME_INDEX, VERSION_START, VERSION_CURRENT);
-      
-      String fn = IndexFileNames.segmentFileName(segment, "", VECTORS_DOCUMENTS_EXTENSION);
-      tvd = d.openInput(fn, context);
-      final int tvdVersion = CodecUtil.checkHeader(tvd, CODEC_NAME_DOCS, VERSION_START, VERSION_CURRENT);
-      fn = IndexFileNames.segmentFileName(segment, "", VECTORS_FIELDS_EXTENSION);
-      tvf = d.openInput(fn, context);
-      final int tvfVersion = CodecUtil.checkHeader(tvf, CODEC_NAME_FIELDS, VERSION_START, VERSION_CURRENT);
-      assert HEADER_LENGTH_INDEX == tvx.getFilePointer();
-      assert HEADER_LENGTH_DOCS == tvd.getFilePointer();
-      assert HEADER_LENGTH_FIELDS == tvf.getFilePointer();
-      assert tvxVersion == tvdVersion;
-      assert tvxVersion == tvfVersion;
-
-      numTotalDocs = (int) (tvx.length()-HEADER_LENGTH_INDEX >> 4);
-
-      this.size = numTotalDocs;
-      assert size == 0 || numTotalDocs == size;
-
-      this.fieldInfos = fieldInfos;
-      success = true;
-    } finally {
-      // With lock-less commits, it's entirely possible (and
-      // fine) to hit a FileNotFound exception above. In
-      // this case, we want to explicitly close any subset
-      // of things that were opened so that we don't have to
-      // wait for a GC to do so.
-      if (!success) {
-        try {
-          close();
-        } catch (Throwable t) {} // ensure we throw our original exception
-      }
-    }
-  }
-
-  // Not private to avoid synthetic access$NNN methods
-  void seekTvx(final int docNum) throws IOException {
-    tvx.seek(docNum * 16L + HEADER_LENGTH_INDEX);
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.close(tvx, tvd, tvf);
-  }
-
-  /**
-   * 
-   * @return The number of documents in the reader
-   */
-  int size() {
-    return size;
-  }
-
-  private class TVFields extends Fields {
-    private final int[] fieldNumbers;
-    private final long[] fieldFPs;
-    private final Map<Integer,Integer> fieldNumberToIndex = new HashMap<>();
-
-    public TVFields(int docID) throws IOException {
-      seekTvx(docID);
-      tvd.seek(tvx.readLong());
-      
-      final int fieldCount = tvd.readVInt();
-      assert fieldCount >= 0;
-      if (fieldCount != 0) {
-        fieldNumbers = new int[fieldCount];
-        fieldFPs = new long[fieldCount];
-        for(int fieldUpto=0;fieldUpto<fieldCount;fieldUpto++) {
-          final int fieldNumber = tvd.readVInt();
-          fieldNumbers[fieldUpto] = fieldNumber;
-          fieldNumberToIndex.put(fieldNumber, fieldUpto);
-        }
-
-        long position = tvx.readLong();
-        fieldFPs[0] = position;
-        for(int fieldUpto=1;fieldUpto<fieldCount;fieldUpto++) {
-          position += tvd.readVLong();
-          fieldFPs[fieldUpto] = position;
-        }
-      } else {
-        // TODO: we can improve writer here, eg write 0 into
-        // tvx file, so we know on first read from tvx that
-        // this doc has no TVs
-        fieldNumbers = null;
-        fieldFPs = null;
-      }
-    }
-    
-    @Override
-    public Iterator<String> iterator() {
-      return new Iterator<String>() {
-        private int fieldUpto;
-
-        @Override
-        public String next() {
-          if (fieldNumbers != null && fieldUpto < fieldNumbers.length) {
-            return fieldInfos.fieldInfo(fieldNumbers[fieldUpto++]).name;
-          } else {
-            throw new NoSuchElementException();
-          }
-        }
-
-        @Override
-        public boolean hasNext() {
-          return fieldNumbers != null && fieldUpto < fieldNumbers.length;
-        }
-
-        @Override
-        public void remove() {
-          throw new UnsupportedOperationException();
-        }
-      };
-    }
-
-    @Override
-    public Terms terms(String field) throws IOException {
-      final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-      if (fieldInfo == null) {
-        // No such field
-        return null;
-      }
-
-      final Integer fieldIndex = fieldNumberToIndex.get(fieldInfo.number);
-      if (fieldIndex == null) {
-        // Term vectors were not indexed for this field
-        return null;
-      }
-
-      return new TVTerms(fieldFPs[fieldIndex]);
-    }
-
-    @Override
-    public int size() {
-      if (fieldNumbers == null) {
-        return 0;
-      } else {
-        return fieldNumbers.length;
-      }
-    }
-  }
-
-  private class TVTerms extends Terms {
-    private final int numTerms;
-    private final long tvfFPStart;
-    private final boolean storePositions;
-    private final boolean storeOffsets;
-    private final boolean storePayloads;
-
-    public TVTerms(long tvfFP) throws IOException {
-      tvf.seek(tvfFP);
-      numTerms = tvf.readVInt();
-      final byte bits = tvf.readByte();
-      storePositions = (bits & STORE_POSITIONS_WITH_TERMVECTOR) != 0;
-      storeOffsets = (bits & STORE_OFFSET_WITH_TERMVECTOR) != 0;
-      storePayloads = (bits & STORE_PAYLOAD_WITH_TERMVECTOR) != 0;
-      tvfFPStart = tvf.getFilePointer();
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      TVTermsEnum termsEnum;
-      if (reuse instanceof TVTermsEnum) {
-        termsEnum = (TVTermsEnum) reuse;
-        if (!termsEnum.canReuse(tvf)) {
-          termsEnum = new TVTermsEnum();
-        }
-      } else {
-        termsEnum = new TVTermsEnum();
-      }
-      termsEnum.reset(numTerms, tvfFPStart, storePositions, storeOffsets, storePayloads);
-      return termsEnum;
-    }
-
-    @Override
-    public long size() {
-      return numTerms;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return -1;
-    }
-
-    @Override
-    public long getSumDocFreq() {
-      // Every term occurs in just one doc:
-      return numTerms;
-    }
-
-    @Override
-    public int getDocCount() {
-      return 1;
-    }
-
-    @Override
-    public boolean hasFreqs() {
-      return true;
-    }
-
-    @Override
-    public boolean hasOffsets() {
-      return storeOffsets;
-    }
-
-    @Override
-    public boolean hasPositions() {
-      return storePositions;
-    }
-    
-    @Override
-    public boolean hasPayloads() {
-      return storePayloads;
-    }
-  }
-
-  private class TVTermsEnum extends TermsEnum {
-    private final IndexInput origTVF;
-    private final IndexInput tvf;
-    private int numTerms;
-    private int nextTerm;
-    private int freq;
-    private BytesRefBuilder lastTerm = new BytesRefBuilder();
-    private BytesRefBuilder term = new BytesRefBuilder();
-    private boolean storePositions;
-    private boolean storeOffsets;
-    private boolean storePayloads;
-    private long tvfFP;
-
-    private int[] positions;
-    private int[] startOffsets;
-    private int[] endOffsets;
-    
-    // one shared byte[] for any term's payloads
-    private int[] payloadOffsets;
-    private int lastPayloadLength;
-    private byte[] payloadData;
-
-    // NOTE: tvf is pre-positioned by caller
-    public TVTermsEnum() {
-      this.origTVF = Lucene40TermVectorsReader.this.tvf;
-      tvf = origTVF.clone();
-    }
-
-    public boolean canReuse(IndexInput tvf) {
-      return tvf == origTVF;
-    }
-
-    public void reset(int numTerms, long tvfFPStart, boolean storePositions, boolean storeOffsets, boolean storePayloads) throws IOException {
-      this.numTerms = numTerms;
-      this.storePositions = storePositions;
-      this.storeOffsets = storeOffsets;
-      this.storePayloads = storePayloads;
-      nextTerm = 0;
-      tvf.seek(tvfFPStart);
-      tvfFP = tvfFPStart;
-      positions = null;
-      startOffsets = null;
-      endOffsets = null;
-      payloadOffsets = null;
-      payloadData = null;
-      lastPayloadLength = -1;
-    }
-
-    // NOTE: slow!  (linear scan)
-    @Override
-    public SeekStatus seekCeil(BytesRef text)
-      throws IOException {
-      if (nextTerm != 0) {
-        final int cmp = text.compareTo(term.get());
-        if (cmp < 0) {
-          nextTerm = 0;
-          tvf.seek(tvfFP);
-        } else if (cmp == 0) {
-          return SeekStatus.FOUND;
-        }
-      }
-
-      while (next() != null) {
-        final int cmp = text.compareTo(term.get());
-        if (cmp < 0) {
-          return SeekStatus.NOT_FOUND;
-        } else if (cmp == 0) {
-          return SeekStatus.FOUND;
-        }
-      }
-
-      return SeekStatus.END;
-    }
-
-    @Override
-    public void seekExact(long ord) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      if (nextTerm >= numTerms) {
-        return null;
-      }
-      term.copyBytes(lastTerm.get());
-      final int start = tvf.readVInt();
-      final int deltaLen = tvf.readVInt();
-      term.setLength(start + deltaLen);
-      term.grow(term.length());
-      tvf.readBytes(term.bytes(), start, deltaLen);
-      freq = tvf.readVInt();
-
-      if (storePayloads) {
-        positions = new int[freq];
-        payloadOffsets = new int[freq];
-        int totalPayloadLength = 0;
-        int pos = 0;
-        for(int posUpto=0;posUpto<freq;posUpto++) {
-          int code = tvf.readVInt();
-          pos += code >>> 1;
-          positions[posUpto] = pos;
-          if ((code & 1) != 0) {
-            // length change
-            lastPayloadLength = tvf.readVInt();
-          }
-          payloadOffsets[posUpto] = totalPayloadLength;
-          totalPayloadLength += lastPayloadLength;
-          assert totalPayloadLength >= 0;
-        }
-        payloadData = new byte[totalPayloadLength];
-        tvf.readBytes(payloadData, 0, payloadData.length);
-      } else if (storePositions /* no payloads */) {
-        // TODO: we could maybe reuse last array, if we can
-        // somehow be careful about consumer never using two
-        // D&PEnums at once...
-        positions = new int[freq];
-        int pos = 0;
-        for(int posUpto=0;posUpto<freq;posUpto++) {
-          pos += tvf.readVInt();
-          positions[posUpto] = pos;
-        }
-      }
-
-      if (storeOffsets) {
-        startOffsets = new int[freq];
-        endOffsets = new int[freq];
-        int offset = 0;
-        for(int posUpto=0;posUpto<freq;posUpto++) {
-          startOffsets[posUpto] = offset + tvf.readVInt();
-          offset = endOffsets[posUpto] = startOffsets[posUpto] + tvf.readVInt();
-        }
-      }
-
-      lastTerm.copyBytes(term.get());
-      nextTerm++;
-      return term.get();
-    }
-
-    @Override
-    public BytesRef term() {
-      return term.get();
-    }
-
-    @Override
-    public long ord() {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public int docFreq() {
-      return 1;
-    }
-
-    @Override
-    public long totalTermFreq() {
-      return freq;
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags /* ignored */) throws IOException {
-      TVDocsEnum docsEnum;
-      if (reuse != null && reuse instanceof TVDocsEnum) {
-        docsEnum = (TVDocsEnum) reuse;
-      } else {
-        docsEnum = new TVDocsEnum();
-      }
-      docsEnum.reset(liveDocs, freq);
-      return docsEnum;
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-
-      if (!storePositions && !storeOffsets) {
-        return null;
-      }
-      
-      TVDocsAndPositionsEnum docsAndPositionsEnum;
-      if (reuse != null && reuse instanceof TVDocsAndPositionsEnum) {
-        docsAndPositionsEnum = (TVDocsAndPositionsEnum) reuse;
-      } else {
-        docsAndPositionsEnum = new TVDocsAndPositionsEnum();
-      }
-      docsAndPositionsEnum.reset(liveDocs, positions, startOffsets, endOffsets, payloadOffsets, payloadData);
-      return docsAndPositionsEnum;
-    }
-  }
-
-  // NOTE: sort of a silly class, since you can get the
-  // freq() already by TermsEnum.totalTermFreq
-  private static class TVDocsEnum extends DocsEnum {
-    private boolean didNext;
-    private int doc = -1;
-    private int freq;
-    private Bits liveDocs;
-
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int nextDoc() {
-      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
-        didNext = true;
-        return (doc = 0);
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      return slowAdvance(target);
-    }
-
-    public void reset(Bits liveDocs, int freq) {
-      this.liveDocs = liveDocs;
-      this.freq = freq;
-      this.doc = -1;
-      didNext = false;
-    }
-    
-    @Override
-    public long cost() {
-      return 1;
-    }
-  }
-
-  private static class TVDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private boolean didNext;
-    private int doc = -1;
-    private int nextPos;
-    private Bits liveDocs;
-    private int[] positions;
-    private int[] startOffsets;
-    private int[] endOffsets;
-    private int[] payloadOffsets;
-    private BytesRef payload = new BytesRef();
-    private byte[] payloadBytes;
-
-    @Override
-    public int freq() throws IOException {
-      if (positions != null) {
-        return positions.length;
-      } else {
-        assert startOffsets != null;
-        return startOffsets.length;
-      }
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int nextDoc() {
-      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
-        didNext = true;
-        return (doc = 0);
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      return slowAdvance(target);
-    }
-
-    public void reset(Bits liveDocs, int[] positions, int[] startOffsets, int[] endOffsets, int[] payloadLengths, byte[] payloadBytes) {
-      this.liveDocs = liveDocs;
-      this.positions = positions;
-      this.startOffsets = startOffsets;
-      this.endOffsets = endOffsets;
-      this.payloadOffsets = payloadLengths;
-      this.payloadBytes = payloadBytes;
-      this.doc = -1;
-      didNext = false;
-      nextPos = 0;
-    }
-
-    @Override
-    public BytesRef getPayload() {
-      if (payloadOffsets == null) {
-        return null;
-      } else {
-        int off = payloadOffsets[nextPos-1];
-        int end = nextPos == payloadOffsets.length ? payloadBytes.length : payloadOffsets[nextPos];
-        if (end - off == 0) {
-          return null;
-        }
-        payload.bytes = payloadBytes;
-        payload.offset = off;
-        payload.length = end - off;
-        return payload;
-      }
-    }
-
-    @Override
-    public int nextPosition() {
-      assert (positions != null && nextPos < positions.length) ||
-        startOffsets != null && nextPos < startOffsets.length;
-
-      if (positions != null) {
-        return positions[nextPos++];
-      } else {
-        nextPos++;
-        return -1;
-      }
-    }
-
-    @Override
-    public int startOffset() {
-      if (startOffsets == null) {
-        return -1;
-      } else {
-        return startOffsets[nextPos-1];
-      }
-    }
-
-    @Override
-    public int endOffset() {
-      if (endOffsets == null) {
-        return -1;
-      } else {
-        return endOffsets[nextPos-1];
-      }
-    }
-    
-    @Override
-    public long cost() {
-      return 1;
-    }
-  }
-
-  @Override
-  public Fields get(int docID) throws IOException {
-    if (tvx != null) {
-      Fields fields = new TVFields(docID);
-      if (fields.size() == 0) {
-        // TODO: we can improve writer here, eg write 0 into
-        // tvx file, so we know on first read from tvx that
-        // this doc has no TVs
-        return null;
-      } else {
-        return fields;
-      }
-    } else {
-      return null;
-    }
-  }
-
-  @Override
-  public TermVectorsReader clone() {
-    IndexInput cloneTvx = null;
-    IndexInput cloneTvd = null;
-    IndexInput cloneTvf = null;
-
-    // These are null when a TermVectorsReader was created
-    // on a segment that did not have term vectors saved
-    if (tvx != null && tvd != null && tvf != null) {
-      cloneTvx = tvx.clone();
-      cloneTvd = tvd.clone();
-      cloneTvf = tvf.clone();
-    }
-    
-    return new Lucene40TermVectorsReader(fieldInfos, cloneTvx, cloneTvd, cloneTvf, size, numTotalDocs);
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return 0;
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {}
-}
-
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
deleted file mode 100644
index 5e4ee03..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
+++ /dev/null
@@ -1,135 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat;
-import org.apache.lucene.codecs.compressing.CompressionMode;
-import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Implements the Lucene 4.1 index format, with configurable per-field postings formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene41 package documentation for file format details.
- * @deprecated Only for reading old 4.0 segments
- * @lucene.experimental
- */
-@Deprecated
-public class Lucene41Codec extends Codec {
-  // TODO: slightly evil
-  private final StoredFieldsFormat fieldsFormat = new CompressingStoredFieldsFormat("Lucene41StoredFields", CompressionMode.FAST, 1 << 14) {
-    @Override
-    public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  };
-  private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
-  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene41Codec.this.getPostingsFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene41Codec() {
-    super("Lucene41");
-  }
-  
-  // TODO: slightly evil
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return infosFormat;
-  }
-  
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene41"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return dvFormat;
-  }
-
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat dvFormat = new Lucene40DocValuesFormat();
-  private final NormsFormat normsFormat = new Lucene40NormsFormat();
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java
index 49d97ea..a00ec12 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java
@@ -22,7 +22,6 @@ import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat;
 import org.apache.lucene.codecs.compressing.CompressingStoredFieldsIndexWriter;
 import org.apache.lucene.codecs.compressing.CompressionMode;
-import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
 import org.apache.lucene.index.StoredFieldVisitor;
 import org.apache.lucene.store.DataOutput;
 import org.apache.lucene.util.packed.PackedInts;
@@ -112,9 +111,7 @@ import org.apache.lucene.util.packed.PackedInts;
  * </ol>
  * <p><b>Known limitations</b></p>
  * <p>This {@link StoredFieldsFormat} does not support individual documents
- * larger than (<tt>2<sup>31</sup> - 2<sup>14</sup></tt>) bytes. In case this
- * is a problem, you should use another format, such as
- * {@link Lucene40StoredFieldsFormat}.</p>
+ * larger than (<tt>2<sup>31</sup> - 2<sup>14</sup></tt>) bytes.</p>
  * @lucene.experimental
  */
 public final class Lucene41StoredFieldsFormat extends CompressingStoredFieldsFormat {
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java
deleted file mode 100644
index 3e522a4..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42Codec.java
+++ /dev/null
@@ -1,149 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
-import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Implements the Lucene 4.2 index format, with configurable per-field postings
- * and docvalues formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene42 package documentation for file format details.
- * @lucene.experimental
- * @deprecated Only for reading old 4.2 segments
- */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene43Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
-@Deprecated
-public class Lucene42Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat();
-  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene42Codec.this.getPostingsFormatForField(field);
-    }
-  };
-  
-  
-  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
-    @Override
-    public DocValuesFormat getDocValuesFormatForField(String field) {
-      return Lucene42Codec.this.getDocValuesFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene42Codec() {
-    super("Lucene42");
-  }
-  
-  @Override
-  public final StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public final TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return infosFormat;
-  }
-  
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene41"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  /** Returns the docvalues format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene42"
-   */
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return defaultDVFormat;
-  }
-  
-  @Override
-  public final DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene42");
-
-  private final NormsFormat normsFormat = new Lucene42NormsFormat() {
-    @Override
-    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  };
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java
deleted file mode 100644
index 02e6001..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesFormat.java
+++ /dev/null
@@ -1,173 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-
-/**
- * Lucene 4.2 DocValues format.
- * <p>
- * Encodes the four per-document value types (Numeric,Binary,Sorted,SortedSet) with seven basic strategies.
- * <p>
- * <ul>
- *    <li>Delta-compressed Numerics: per-document integers written in blocks of 4096. For each block
- *        the minimum value is encoded, and each entry is a delta from that minimum value.
- *    <li>Table-compressed Numerics: when the number of unique values is very small, a lookup table
- *        is written instead. Each per-document entry is instead the ordinal to this table.
- *    <li>Uncompressed Numerics: when all values would fit into a single byte, and the 
- *        <code>acceptableOverheadRatio</code> would pack values into 8 bits per value anyway, they
- *        are written as absolute values (with no indirection or packing) for performance.
- *    <li>GCD-compressed Numerics: when all numbers share a common divisor, such as dates, the greatest
- *        common denominator (GCD) is computed, and quotients are stored using Delta-compressed Numerics.
- *    <li>Fixed-width Binary: one large concatenated byte[] is written, along with the fixed length.
- *        Each document's value can be addressed by maxDoc*length. 
- *    <li>Variable-width Binary: one large concatenated byte[] is written, along with end addresses 
- *        for each document. The addresses are written in blocks of 4096, with the current absolute
- *        start for the block, and the average (expected) delta per entry. For each document the 
- *        deviation from the delta (actual - expected) is written.
- *    <li>Sorted: an FST mapping deduplicated terms to ordinals is written, along with the per-document
- *        ordinals written using one of the numeric strategies above.
- *    <li>SortedSet: an FST mapping deduplicated terms to ordinals is written, along with the per-document
- *        ordinal list written using one of the binary strategies above.  
- * </ul>
- * <p>
- * Files:
- * <ol>
- *   <li><tt>.dvd</tt>: DocValues data</li>
- *   <li><tt>.dvm</tt>: DocValues metadata</li>
- * </ol>
- * <ol>
- *   <li><a name="dvm" id="dvm"></a>
- *   <p>The DocValues metadata or .dvm file.</p>
- *   <p>For DocValues field, this stores metadata, such as the offset into the 
- *      DocValues data (.dvd)</p>
- *   <p>DocValues metadata (.dvm) --&gt; Header,&lt;FieldNumber,EntryType,Entry&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>Entry --&gt; NumericEntry | BinaryEntry | SortedEntry</li>
- *     <li>NumericEntry --&gt; DataOffset,CompressionType,PackedVersion</li>
- *     <li>BinaryEntry --&gt; DataOffset,DataLength,MinLength,MaxLength,PackedVersion?,BlockSize?</li>
- *     <li>SortedEntry --&gt; DataOffset,ValueCount</li>
- *     <li>FieldNumber,PackedVersion,MinLength,MaxLength,BlockSize,ValueCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *     <li>DataOffset,DataLength --&gt; {@link DataOutput#writeLong Int64}</li>
- *     <li>EntryType,CompressionType --&gt; {@link DataOutput#writeByte Byte}</li>
- *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- *   <p>Sorted fields have two entries: a SortedEntry with the FST metadata,
- *      and an ordinary NumericEntry for the document-to-ord metadata.</p>
- *   <p>SortedSet fields have two entries: a SortedEntry with the FST metadata,
- *      and an ordinary BinaryEntry for the document-to-ord-list metadata.</p>
- *   <p>FieldNumber of -1 indicates the end of metadata.</p>
- *   <p>EntryType is a 0 (NumericEntry), 1 (BinaryEntry, or 2 (SortedEntry)</p>
- *   <p>DataOffset is the pointer to the start of the data in the DocValues data (.dvd)</p>
- *   <p>CompressionType indicates how Numeric values will be compressed:
- *      <ul>
- *         <li>0 --&gt; delta-compressed. For each block of 4096 integers, every integer is delta-encoded
- *             from the minimum value within the block. 
- *         <li>1 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
- *             a lookup table of unique values is written, followed by the ordinal for each document.
- *         <li>2 --&gt; uncompressed. When the <code>acceptableOverheadRatio</code> parameter would upgrade the number
- *             of bits required to 8, and all values fit in a byte, these are written as absolute binary values
- *             for performance.
- *         <li>3 --&gt, gcd-compressed. When all integers share a common divisor, only quotients are stored
- *             using blocks of delta-encoded ints.
- *      </ul>
- *   <p>MinLength and MaxLength represent the min and max byte[] value lengths for Binary values.
- *      If they are equal, then all values are of a fixed size, and can be addressed as DataOffset + (docID * length).
- *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
- *      is written for the addresses.
- *   <li><a name="dvd" id="dvd"></a>
- *   <p>The DocValues data or .dvd file.</p>
- *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
- *   <p>DocValues data (.dvd) --&gt; Header,&lt;NumericData | BinaryData | SortedData&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>NumericData --&gt; DeltaCompressedNumerics | TableCompressedNumerics | UncompressedNumerics | GCDCompressedNumerics</li>
- *     <li>BinaryData --&gt;  {@link DataOutput#writeByte Byte}<sup>DataLength</sup>,Addresses</li>
- *     <li>SortedData --&gt; {@link FST FST&lt;Int64&gt;}</li>
- *     <li>DeltaCompressedNumerics --&gt; {@link BlockPackedWriter BlockPackedInts(blockSize=4096)}</li>
- *     <li>TableCompressedNumerics --&gt; TableSize,{@link DataOutput#writeLong Int64}<sup>TableSize</sup>,{@link PackedInts PackedInts}</li>
- *     <li>UncompressedNumerics --&gt; {@link DataOutput#writeByte Byte}<sup>maxdoc</sup></li>
- *     <li>Addresses --&gt; {@link MonotonicBlockPackedWriter MonotonicBlockPackedInts(blockSize=4096)}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- *   <p>SortedSet entries store the list of ordinals in their BinaryData as a
- *      sequences of increasing {@link DataOutput#writeVLong vLong}s, delta-encoded.</p>       
- * </ol>
- * <p>
- * Limitations:
- * <ul>
- *   <li> Binary doc values can be at most {@link #MAX_BINARY_FIELD_LENGTH} in length.
- * </ul>
- * @deprecated Only for reading old 4.2 segments
- */
-@Deprecated
-public class Lucene42DocValuesFormat extends DocValuesFormat {
-
-  /** Maximum length for each binary doc values field. */
-  public static final int MAX_BINARY_FIELD_LENGTH = (1 << 15) - 2;
-  
-  final float acceptableOverheadRatio;
-  
-  /** 
-   * Calls {@link #Lucene42DocValuesFormat(float) 
-   * Lucene42DocValuesFormat(PackedInts.DEFAULT)} 
-   */
-  public Lucene42DocValuesFormat() {
-    this(PackedInts.DEFAULT);
-  }
-  
-  /**
-   * Creates a new Lucene42DocValuesFormat with the specified
-   * <code>acceptableOverheadRatio</code> for NumericDocValues.
-   * @param acceptableOverheadRatio compression parameter for numerics. 
-   *        Currently this is only used when the number of unique values is small.
-   *        
-   * @lucene.experimental
-   */
-  public Lucene42DocValuesFormat(float acceptableOverheadRatio) {
-    super("Lucene42");
-    this.acceptableOverheadRatio = acceptableOverheadRatio;
-  }
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-  
-  @Override
-  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new Lucene42DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-  }
-  
-  static final String DATA_CODEC = "Lucene42DocValuesData";
-  static final String DATA_EXTENSION = "dvd";
-  static final String METADATA_CODEC = "Lucene42DocValuesMetadata";
-  static final String METADATA_EXTENSION = "dvm";
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
deleted file mode 100644
index ef7ed68..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
+++ /dev/null
@@ -1,627 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.IntsRefBuilder;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.BytesRefFSTEnum.InputOutput;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.FST.Arc;
-import org.apache.lucene.util.fst.FST.BytesReader;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util;
-import org.apache.lucene.util.packed.BlockPackedReader;
-import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Reader for {@link Lucene42DocValuesFormat}
- */
-class Lucene42DocValuesProducer extends DocValuesProducer {
-  // metadata maps (just file pointers and minimal stuff)
-  private final Map<Integer,NumericEntry> numerics;
-  private final Map<Integer,BinaryEntry> binaries;
-  private final Map<Integer,FSTEntry> fsts;
-  private final IndexInput data;
-  private final int version;
-  
-  // ram instances we have already loaded
-  private final Map<Integer,NumericDocValues> numericInstances = 
-      new HashMap<>();
-  private final Map<Integer,BinaryDocValues> binaryInstances =
-      new HashMap<>();
-  private final Map<Integer,FST<Long>> fstInstances =
-      new HashMap<>();
-  
-  private final int maxDoc;
-  private final AtomicLong ramBytesUsed;
-  
-  static final byte NUMBER = 0;
-  static final byte BYTES = 1;
-  static final byte FST = 2;
-
-  static final int BLOCK_SIZE = 4096;
-  
-  static final byte DELTA_COMPRESSED = 0;
-  static final byte TABLE_COMPRESSED = 1;
-  static final byte UNCOMPRESSED = 2;
-  static final byte GCD_COMPRESSED = 3;
-  
-  static final int VERSION_START = 0;
-  static final int VERSION_GCD_COMPRESSION = 1;
-  static final int VERSION_CHECKSUM = 2;
-  static final int VERSION_CURRENT = VERSION_CHECKSUM;
-    
-  Lucene42DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    maxDoc = state.segmentInfo.getDocCount();
-    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-    // read in the entries from the metadata file.
-    ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
-    boolean success = false;
-    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
-    try {
-      version = CodecUtil.checkHeader(in, metaCodec, 
-                                      VERSION_START,
-                                      VERSION_CURRENT);
-      numerics = new HashMap<>();
-      binaries = new HashMap<>();
-      fsts = new HashMap<>();
-      readFields(in, state.fieldInfos);
-
-      if (version >= VERSION_CHECKSUM) {
-        CodecUtil.checkFooter(in);
-      } else {
-        CodecUtil.checkEOF(in);
-      }
-      
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(in);
-      } else {
-        IOUtils.closeWhileHandlingException(in);
-      }
-    }
-
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-    this.data = state.directory.openInput(dataName, state.context);
-    success = false;
-    try {
-      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 VERSION_START,
-                                                 VERSION_CURRENT);
-      if (version != version2) {
-        throw new CorruptIndexException("Format versions mismatch");
-      }
-      
-      if (version >= VERSION_CHECKSUM) {
-        // NOTE: data file is too costly to verify checksum against all the bytes on open,
-        // but for now we at least verify proper structure of the checksum footer: which looks
-        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-        // such as file truncation.
-        CodecUtil.retrieveChecksum(data);
-      }
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this.data);
-      }
-    }
-  }
-  
-  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
-    int fieldNumber = meta.readVInt();
-    while (fieldNumber != -1) {
-      if (infos.fieldInfo(fieldNumber) == null) {
-        // trickier to validate more: because we re-use for norms, because we use multiple entries
-        // for "composite" types like sortedset, etc.
-        throw new CorruptIndexException("Invalid field number: " + fieldNumber + " (resource=" + meta + ")");
-      }
-      int fieldType = meta.readByte();
-      if (fieldType == NUMBER) {
-        NumericEntry entry = new NumericEntry();
-        entry.offset = meta.readLong();
-        entry.format = meta.readByte();
-        switch(entry.format) {
-          case DELTA_COMPRESSED:
-          case TABLE_COMPRESSED:
-          case GCD_COMPRESSED:
-          case UNCOMPRESSED:
-               break;
-          default:
-               throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
-        }
-        if (entry.format != UNCOMPRESSED) {
-          entry.packedIntsVersion = meta.readVInt();
-        }
-        numerics.put(fieldNumber, entry);
-      } else if (fieldType == BYTES) {
-        BinaryEntry entry = new BinaryEntry();
-        entry.offset = meta.readLong();
-        entry.numBytes = meta.readLong();
-        entry.minLength = meta.readVInt();
-        entry.maxLength = meta.readVInt();
-        if (entry.minLength != entry.maxLength) {
-          entry.packedIntsVersion = meta.readVInt();
-          entry.blockSize = meta.readVInt();
-        }
-        binaries.put(fieldNumber, entry);
-      } else if (fieldType == FST) {
-        FSTEntry entry = new FSTEntry();
-        entry.offset = meta.readLong();
-        entry.numOrds = meta.readVLong();
-        fsts.put(fieldNumber, entry);
-      } else {
-        throw new CorruptIndexException("invalid entry type: " + fieldType + ", input=" + meta);
-      }
-      fieldNumber = meta.readVInt();
-    }
-  }
-
-  @Override
-  public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOException {
-    NumericDocValues instance = numericInstances.get(field.number);
-    if (instance == null) {
-      instance = loadNumeric(field);
-      numericInstances.put(field.number, instance);
-    }
-    return instance;
-  }
-  
-  @Override
-  public long ramBytesUsed() {
-    return ramBytesUsed.get();
-  }
-  
-  @Override
-  public void checkIntegrity() throws IOException {
-    if (version >= VERSION_CHECKSUM) {
-      CodecUtil.checksumEntireFile(data);
-    }
-  }
-
-  private NumericDocValues loadNumeric(FieldInfo field) throws IOException {
-    NumericEntry entry = numerics.get(field.number);
-    data.seek(entry.offset);
-    switch (entry.format) {
-      case TABLE_COMPRESSED:
-        int size = data.readVInt();
-        if (size > 256) {
-          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, input=" + data);
-        }
-        final long decode[] = new long[size];
-        for (int i = 0; i < decode.length; i++) {
-          decode[i] = data.readLong();
-        }
-        final int formatID = data.readVInt();
-        final int bitsPerValue = data.readVInt();
-        final PackedInts.Reader ordsReader = PackedInts.getReaderNoHeader(data, PackedInts.Format.byId(formatID), entry.packedIntsVersion, maxDoc, bitsPerValue);
-        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(decode) + ordsReader.ramBytesUsed());
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return decode[(int)ordsReader.get(docID)];
-          }
-        };
-      case DELTA_COMPRESSED:
-        final int blockSize = data.readVInt();
-        final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, blockSize, maxDoc, false);
-        ramBytesUsed.addAndGet(reader.ramBytesUsed());
-        return reader;
-      case UNCOMPRESSED:
-        final byte bytes[] = new byte[maxDoc];
-        data.readBytes(bytes, 0, bytes.length);
-        ramBytesUsed.addAndGet(RamUsageEstimator.sizeOf(bytes));
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return bytes[docID];
-          }
-        };
-      case GCD_COMPRESSED:
-        final long min = data.readLong();
-        final long mult = data.readLong();
-        final int quotientBlockSize = data.readVInt();
-        final BlockPackedReader quotientReader = new BlockPackedReader(data, entry.packedIntsVersion, quotientBlockSize, maxDoc, false);
-        ramBytesUsed.addAndGet(quotientReader.ramBytesUsed());
-        return new NumericDocValues() {
-          @Override
-          public long get(int docID) {
-            return min + mult * quotientReader.get(docID);
-          }
-        };
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public synchronized BinaryDocValues getBinary(FieldInfo field) throws IOException {
-    BinaryDocValues instance = binaryInstances.get(field.number);
-    if (instance == null) {
-      instance = loadBinary(field);
-      binaryInstances.put(field.number, instance);
-    }
-    return instance;
-  }
-  
-  private BinaryDocValues loadBinary(FieldInfo field) throws IOException {
-    BinaryEntry entry = binaries.get(field.number);
-    data.seek(entry.offset);
-    PagedBytes bytes = new PagedBytes(16);
-    bytes.copy(data, entry.numBytes);
-    final PagedBytes.Reader bytesReader = bytes.freeze(true);
-    if (entry.minLength == entry.maxLength) {
-      final int fixedLength = entry.minLength;
-      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed());
-      return new BinaryDocValues() {
-        @Override
-        public BytesRef get(int docID) {
-          final BytesRef term = new BytesRef();
-          bytesReader.fillSlice(term, fixedLength * (long)docID, fixedLength);
-          return term;
-        }
-      };
-    } else {
-      final MonotonicBlockPackedReader addresses = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, maxDoc, false);
-      ramBytesUsed.addAndGet(bytesReader.ramBytesUsed() + addresses.ramBytesUsed());
-      return new BinaryDocValues() {
-
-        @Override
-        public BytesRef get(int docID) {
-          long startAddress = docID == 0 ? 0 : addresses.get(docID-1);
-          long endAddress = addresses.get(docID); 
-          final BytesRef term = new BytesRef();
-          bytesReader.fillSlice(term, startAddress, (int) (endAddress - startAddress));
-          return term;
-        }
-      };
-    }
-  }
-  
-  @Override
-  public SortedDocValues getSorted(FieldInfo field) throws IOException {
-    final FSTEntry entry = fsts.get(field.number);
-    FST<Long> instance;
-    synchronized(this) {
-      instance = fstInstances.get(field.number);
-      if (instance == null) {
-        data.seek(entry.offset);
-        instance = new FST<>(data, PositiveIntOutputs.getSingleton());
-        ramBytesUsed.addAndGet(instance.ramBytesUsed());
-        fstInstances.put(field.number, instance);
-      }
-    }
-    final NumericDocValues docToOrd = getNumeric(field);
-    final FST<Long> fst = instance;
-    
-    // per-thread resources
-    final BytesReader in = fst.getBytesReader();
-    final Arc<Long> firstArc = new Arc<>();
-    final Arc<Long> scratchArc = new Arc<>();
-    final IntsRefBuilder scratchInts = new IntsRefBuilder();
-    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
-    
-    return new SortedDocValues() {
-
-      final BytesRefBuilder term = new BytesRefBuilder();
-
-      @Override
-      public int getOrd(int docID) {
-        return (int) docToOrd.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        try {
-          in.setPosition(0);
-          fst.getFirstArc(firstArc);
-          IntsRef output = Util.getByOutput(fst, ord, in, firstArc, scratchArc, scratchInts);
-          term.grow(output.length);
-          term.clear();
-          return Util.toBytesRef(output, term);
-        } catch (IOException bogus) {
-          throw new RuntimeException(bogus);
-        }
-      }
-
-      @Override
-      public int lookupTerm(BytesRef key) {
-        try {
-          InputOutput<Long> o = fstEnum.seekCeil(key);
-          if (o == null) {
-            return -getValueCount()-1;
-          } else if (o.input.equals(key)) {
-            return o.output.intValue();
-          } else {
-            return (int) -o.output-1;
-          }
-        } catch (IOException bogus) {
-          throw new RuntimeException(bogus);
-        }
-      }
-
-      @Override
-      public int getValueCount() {
-        return (int)entry.numOrds;
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        return new FSTTermsEnum(fst);
-      }
-    };
-  }
-  
-  @Override
-  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
-    final FSTEntry entry = fsts.get(field.number);
-    if (entry.numOrds == 0) {
-      return DocValues.emptySortedSet(); // empty FST!
-    }
-    FST<Long> instance;
-    synchronized(this) {
-      instance = fstInstances.get(field.number);
-      if (instance == null) {
-        data.seek(entry.offset);
-        instance = new FST<>(data, PositiveIntOutputs.getSingleton());
-        ramBytesUsed.addAndGet(instance.ramBytesUsed());
-        fstInstances.put(field.number, instance);
-      }
-    }
-    final BinaryDocValues docToOrds = getBinary(field);
-    final FST<Long> fst = instance;
-    
-    // per-thread resources
-    final BytesReader in = fst.getBytesReader();
-    final Arc<Long> firstArc = new Arc<>();
-    final Arc<Long> scratchArc = new Arc<>();
-    final IntsRefBuilder scratchInts = new IntsRefBuilder();
-    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<>(fst);
-    final ByteArrayDataInput input = new ByteArrayDataInput();
-    return new SortedSetDocValues() {
-      final BytesRefBuilder term = new BytesRefBuilder();
-      BytesRef ordsRef;
-      long currentOrd;
-
-      @Override
-      public long nextOrd() {
-        if (input.eof()) {
-          return NO_MORE_ORDS;
-        } else {
-          currentOrd += input.readVLong();
-          return currentOrd;
-        }
-      }
-      
-      @Override
-      public void setDocument(int docID) {
-        ordsRef = docToOrds.get(docID);
-        input.reset(ordsRef.bytes, ordsRef.offset, ordsRef.length);
-        currentOrd = 0;
-      }
-
-      @Override
-      public BytesRef lookupOrd(long ord) {
-        try {
-          in.setPosition(0);
-          fst.getFirstArc(firstArc);
-          IntsRef output = Util.getByOutput(fst, ord, in, firstArc, scratchArc, scratchInts);
-          term.grow(output.length);
-          term.clear();
-          return Util.toBytesRef(output, term);
-        } catch (IOException bogus) {
-          throw new RuntimeException(bogus);
-        }
-      }
-
-      @Override
-      public long lookupTerm(BytesRef key) {
-        try {
-          InputOutput<Long> o = fstEnum.seekCeil(key);
-          if (o == null) {
-            return -getValueCount()-1;
-          } else if (o.input.equals(key)) {
-            return o.output.intValue();
-          } else {
-            return -o.output-1;
-          }
-        } catch (IOException bogus) {
-          throw new RuntimeException(bogus);
-        }
-      }
-
-      @Override
-      public long getValueCount() {
-        return entry.numOrds;
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        return new FSTTermsEnum(fst);
-      }
-    };
-  }
-  
-  @Override
-  public Bits getDocsWithField(FieldInfo field) throws IOException {
-    if (field.getDocValuesType() == FieldInfo.DocValuesType.SORTED_SET) {
-      return DocValues.docsWithValue(getSortedSet(field), maxDoc);
-    } else {
-      return new Bits.MatchAllBits(maxDoc);
-    }
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
-    throw new IllegalStateException("Lucene 4.2 does not support SortedNumeric: how did you pull this off?");
-  }
-
-  @Override
-  public void close() throws IOException {
-    data.close();
-  }
-  
-  static class NumericEntry {
-    long offset;
-    byte format;
-    int packedIntsVersion;
-  }
-  
-  static class BinaryEntry {
-    long offset;
-    long numBytes;
-    int minLength;
-    int maxLength;
-    int packedIntsVersion;
-    int blockSize;
-  }
-  
-  static class FSTEntry {
-    long offset;
-    long numOrds;
-  }
-  
-  // exposes FSTEnum directly as a TermsEnum: avoids binary-search next()
-  static class FSTTermsEnum extends TermsEnum {
-    final BytesRefFSTEnum<Long> in;
-    
-    // this is all for the complicated seek(ord)...
-    // maybe we should add a FSTEnum that supports this operation?
-    final FST<Long> fst;
-    final FST.BytesReader bytesReader;
-    final Arc<Long> firstArc = new Arc<>();
-    final Arc<Long> scratchArc = new Arc<>();
-    final IntsRefBuilder scratchInts = new IntsRefBuilder();
-    final BytesRefBuilder scratchBytes = new BytesRefBuilder();
-    
-    FSTTermsEnum(FST<Long> fst) {
-      this.fst = fst;
-      in = new BytesRefFSTEnum<>(fst);
-      bytesReader = fst.getBytesReader();
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      InputOutput<Long> io = in.next();
-      if (io == null) {
-        return null;
-      } else {
-        return io.input;
-      }
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef text) throws IOException {
-      if (in.seekCeil(text) == null) {
-        return SeekStatus.END;
-      } else if (term().equals(text)) {
-        // TODO: add SeekStatus to FSTEnum like in https://issues.apache.org/jira/browse/LUCENE-3729
-        // to remove this comparision?
-        return SeekStatus.FOUND;
-      } else {
-        return SeekStatus.NOT_FOUND;
-      }
-    }
-
-    @Override
-    public boolean seekExact(BytesRef text) throws IOException {
-      if (in.seekExact(text) == null) {
-        return false;
-      } else {
-        return true;
-      }
-    }
-
-    @Override
-    public void seekExact(long ord) throws IOException {
-      // TODO: would be better to make this simpler and faster.
-      // but we dont want to introduce a bug that corrupts our enum state!
-      bytesReader.setPosition(0);
-      fst.getFirstArc(firstArc);
-      IntsRef output = Util.getByOutput(fst, ord, bytesReader, firstArc, scratchArc, scratchInts);
-      BytesRefBuilder scratchBytes = new BytesRefBuilder();
-      scratchBytes.clear();
-      Util.toBytesRef(output, scratchBytes);
-      // TODO: we could do this lazily, better to try to push into FSTEnum though?
-      in.seekExact(scratchBytes.get());
-    }
-
-    @Override
-    public BytesRef term() throws IOException {
-      return in.current().input;
-    }
-
-    @Override
-    public long ord() throws IOException {
-      return in.current().output;
-    }
-
-    @Override
-    public int docFreq() throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public long totalTermFreq() throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java
deleted file mode 100644
index 5873bc0..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosFormat.java
+++ /dev/null
@@ -1,122 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FieldInfosReader;
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.index.FieldInfo.DocValuesType; // javadoc
-import org.apache.lucene.store.DataOutput; // javadoc
-
-/**
- * Lucene 4.2 Field Infos format.
- * <p>
- * <p>Field names are stored in the field info file, with suffix <tt>.fnm</tt>.</p>
- * <p>FieldInfos (.fnm) --&gt; Header,FieldsCount, &lt;FieldName,FieldNumber,
- * FieldBits,DocValuesBits,Attributes&gt; <sup>FieldsCount</sup></p>
- * <p>Data types:
- * <ul>
- *   <li>Header --&gt; {@link CodecUtil#checkHeader CodecHeader}</li>
- *   <li>FieldsCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>FieldName --&gt; {@link DataOutput#writeString String}</li>
- *   <li>FieldBits, DocValuesBits --&gt; {@link DataOutput#writeByte Byte}</li>
- *   <li>FieldNumber --&gt; {@link DataOutput#writeInt VInt}</li>
- *   <li>Attributes --&gt; {@link DataOutput#writeStringStringMap Map&lt;String,String&gt;}</li>
- * </ul>
- * </p>
- * Field Descriptions:
- * <ul>
- *   <li>FieldsCount: the number of fields in this file.</li>
- *   <li>FieldName: name of the field as a UTF-8 String.</li>
- *   <li>FieldNumber: the field's number. Note that unlike previous versions of
- *       Lucene, the fields are not numbered implicitly by their order in the
- *       file, instead explicitly.</li>
- *   <li>FieldBits: a byte containing field options.
- *       <ul>
- *         <li>The low-order bit is one for indexed fields, and zero for non-indexed
- *             fields.</li>
- *         <li>The second lowest-order bit is one for fields that have term vectors
- *             stored, and zero for fields without term vectors.</li>
- *         <li>If the third lowest order-bit is set (0x4), offsets are stored into
- *             the postings list in addition to positions.</li>
- *         <li>Fourth bit is unused.</li>
- *         <li>If the fifth lowest-order bit is set (0x10), norms are omitted for the
- *             indexed field.</li>
- *         <li>If the sixth lowest-order bit is set (0x20), payloads are stored for the
- *             indexed field.</li>
- *         <li>If the seventh lowest-order bit is set (0x40), term frequencies and
- *             positions omitted for the indexed field.</li>
- *         <li>If the eighth lowest-order bit is set (0x80), positions are omitted for the
- *             indexed field.</li>
- *       </ul>
- *    </li>
- *    <li>DocValuesBits: a byte containing per-document value types. The type
- *        recorded as two four-bit integers, with the high-order bits representing
- *        <code>norms</code> options, and the low-order bits representing 
- *        {@code DocValues} options. Each four-bit integer can be decoded as such:
- *        <ul>
- *          <li>0: no DocValues for this field.</li>
- *          <li>1: NumericDocValues. ({@link DocValuesType#NUMERIC})</li>
- *          <li>2: BinaryDocValues. ({@code DocValuesType#BINARY})</li>
- *          <li>3: SortedDocValues. ({@code DocValuesType#SORTED})</li>
- *        </ul>
- *    </li>
- *    <li>Attributes: a key-value map of codec-private attributes.</li>
- * </ul>
- *
- * @lucene.experimental
- * @deprecated Only for reading old 4.2-4.5 segments
- */
-@Deprecated
-public class Lucene42FieldInfosFormat extends FieldInfosFormat {
-  private final FieldInfosReader reader = new Lucene42FieldInfosReader();
-  
-  /** Sole constructor. */
-  public Lucene42FieldInfosFormat() {
-  }
-
-  @Override
-  public FieldInfosReader getFieldInfosReader() throws IOException {
-    return reader;
-  }
-
-  @Override
-  public FieldInfosWriter getFieldInfosWriter() throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-  
-  /** Extension of field infos */
-  static final String EXTENSION = "fnm";
-  
-  // Codec header
-  static final String CODEC_NAME = "Lucene42FieldInfos";
-  static final int FORMAT_START = 0;
-  static final int FORMAT_CURRENT = FORMAT_START;
-  
-  // Field flags
-  static final byte IS_INDEXED = 0x1;
-  static final byte STORE_TERMVECTOR = 0x2;
-  static final byte STORE_OFFSETS_IN_POSTINGS = 0x4;
-  static final byte OMIT_NORMS = 0x10;
-  static final byte STORE_PAYLOADS = 0x20;
-  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
-  static final byte OMIT_POSITIONS = -128;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java
deleted file mode 100644
index 1919b79..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosReader.java
+++ /dev/null
@@ -1,123 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldInfosReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.2 FieldInfos reader.
- * 
- * @lucene.experimental
- * @deprecated Only for reading old 4.2-4.5 segments
- * @see Lucene42FieldInfosFormat
- */
-@Deprecated
-final class Lucene42FieldInfosReader extends FieldInfosReader {
-
-  /** Sole constructor. */
-  public Lucene42FieldInfosReader() {
-  }
-
-  @Override
-  public FieldInfos read(Directory directory, String segmentName, String segmentSuffix, IOContext iocontext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene42FieldInfosFormat.EXTENSION);
-    IndexInput input = directory.openInput(fileName, iocontext);
-    
-    boolean success = false;
-    try {
-      CodecUtil.checkHeader(input, Lucene42FieldInfosFormat.CODEC_NAME, 
-                                   Lucene42FieldInfosFormat.FORMAT_START, 
-                                   Lucene42FieldInfosFormat.FORMAT_CURRENT);
-
-      final int size = input.readVInt(); //read in the size
-      FieldInfo infos[] = new FieldInfo[size];
-
-      for (int i = 0; i < size; i++) {
-        String name = input.readString();
-        final int fieldNumber = input.readVInt();
-        byte bits = input.readByte();
-        boolean isIndexed = (bits & Lucene42FieldInfosFormat.IS_INDEXED) != 0;
-        boolean storeTermVector = (bits & Lucene42FieldInfosFormat.STORE_TERMVECTOR) != 0;
-        boolean omitNorms = (bits & Lucene42FieldInfosFormat.OMIT_NORMS) != 0;
-        boolean storePayloads = (bits & Lucene42FieldInfosFormat.STORE_PAYLOADS) != 0;
-        final IndexOptions indexOptions;
-        if (!isIndexed) {
-          indexOptions = null;
-        } else if ((bits & Lucene42FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_ONLY;
-        } else if ((bits & Lucene42FieldInfosFormat.OMIT_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_AND_FREQS;
-        } else if ((bits & Lucene42FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS) != 0) {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
-        } else {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        }
-
-        // DV Types are packed in one byte
-        byte val = input.readByte();
-        final DocValuesType docValuesType = getDocValuesType(input, (byte) (val & 0x0F));
-        final DocValuesType normsType = getDocValuesType(input, (byte) ((val >>> 4) & 0x0F));
-        final Map<String,String> attributes = input.readStringStringMap();
-        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, docValuesType, normsType, -1, Collections.unmodifiableMap(attributes));
-      }
-
-      CodecUtil.checkEOF(input);
-      FieldInfos fieldInfos = new FieldInfos(infos);
-      success = true;
-      return fieldInfos;
-    } finally {
-      if (success) {
-        input.close();
-      } else {
-        IOUtils.closeWhileHandlingException(input);
-      }
-    }
-  }
-  
-  private static DocValuesType getDocValuesType(IndexInput input, byte b) throws IOException {
-    if (b == 0) {
-      return null;
-    } else if (b == 1) {
-      return DocValuesType.NUMERIC;
-    } else if (b == 2) {
-      return DocValuesType.BINARY;
-    } else if (b == 3) {
-      return DocValuesType.SORTED;
-    } else if (b == 4) {
-      return DocValuesType.SORTED_SET;
-    } else {
-      throw new CorruptIndexException("invalid docvalues byte: " + b + " (resource=" + input + ")");
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java
deleted file mode 100644
index 2c66075..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsFormat.java
+++ /dev/null
@@ -1,84 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Lucene 4.2 score normalization format.
- * <p>
- * NOTE: this uses the same format as {@link Lucene42DocValuesFormat}
- * Numeric DocValues, but with different file extensions, and passing
- * {@link PackedInts#FASTEST} for uncompressed encoding: trading off
- * space for performance.
- * <p>
- * Files:
- * <ul>
- *   <li><tt>.nvd</tt>: DocValues data</li>
- *   <li><tt>.nvm</tt>: DocValues metadata</li>
- * </ul>
- * @see Lucene42DocValuesFormat
- */
-public class Lucene42NormsFormat extends NormsFormat {
-  final float acceptableOverheadRatio;
-
-  /** 
-   * Calls {@link #Lucene42NormsFormat(float) 
-   * Lucene42DocValuesFormat(PackedInts.FASTEST)} 
-   */
-  public Lucene42NormsFormat() {
-    // note: we choose FASTEST here (otherwise our norms are half as big but 15% slower than previous lucene)
-    this(PackedInts.FASTEST);
-  }
-  
-  /**
-   * Creates a new Lucene42DocValuesFormat with the specified
-   * <code>acceptableOverheadRatio</code> for NumericDocValues.
-   * @param acceptableOverheadRatio compression parameter for numerics. 
-   *        Currently this is only used when the number of unique values is small.
-   *        
-   * @lucene.experimental
-   */
-  public Lucene42NormsFormat(float acceptableOverheadRatio) {
-    this.acceptableOverheadRatio = acceptableOverheadRatio;
-  }
-  
-  @Override
-  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    throw new UnsupportedOperationException("this codec can only be used for reading");
-  }
-  
-  @Override
-  public NormsProducer normsProducer(SegmentReadState state) throws IOException {
-    return new Lucene42NormsProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-  }
-  
-  static final String DATA_CODEC = "Lucene41NormsData";
-  static final String DATA_EXTENSION = "nvd";
-  static final String METADATA_CODEC = "Lucene41NormsMetadata";
-  static final String METADATA_EXTENSION = "nvm";
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java
deleted file mode 100644
index a8b2073..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsProducer.java
+++ /dev/null
@@ -1,59 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.NormsProducer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-
-/**
- * Reads 4.2-4.8 norms.
- * Implemented the same as docvalues, but with a different filename.
- * @deprecated Only for reading old segments
- */
-@Deprecated
-class Lucene42NormsProducer extends NormsProducer {
-  private final Lucene42DocValuesProducer impl;
-  
-  Lucene42NormsProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    impl = new Lucene42DocValuesProducer(state, dataCodec, dataExtension, metaCodec, metaExtension);
-  }
-
-  @Override
-  public NumericDocValues getNorms(FieldInfo field) throws IOException {
-    return impl.getNumeric(field);
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    impl.checkIntegrity();
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return impl.ramBytesUsed();
-  }
-
-  @Override
-  public void close() throws IOException {
-    impl.close();
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java
deleted file mode 100644
index ced5584..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45Codec.java
+++ /dev/null
@@ -1,152 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42FieldInfosFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42NormsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
-import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Implements the Lucene 4.5 index format, with configurable per-field postings
- * and docvalues formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene45 package documentation for file format details.
- * @lucene.experimental
- * @deprecated Only for reading old 4.3-4.5 segments
- */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene46Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
-@Deprecated
-public class Lucene45Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat();
-  private final SegmentInfoFormat infosFormat = new Lucene40SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene45Codec.this.getPostingsFormatForField(field);
-    }
-  };
-  
-  
-  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
-    @Override
-    public DocValuesFormat getDocValuesFormatForField(String field) {
-      return Lucene45Codec.this.getDocValuesFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene45Codec() {
-    super("Lucene45");
-  }
-  
-  @Override
-  public final StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public final TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return infosFormat;
-  }
-  
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene41"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  /** Returns the docvalues format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene45"
-   */
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return defaultDVFormat;
-  }
-  
-  @Override
-  public final DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene45");
-
-  private final NormsFormat normsFormat = new Lucene42NormsFormat() {
-    @Override
-    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  };
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java
deleted file mode 100644
index 54e0af8..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesConsumer.java
+++ /dev/null
@@ -1,433 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable; // javadocs
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.MathUtil;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/** writer for {@link Lucene45DocValuesFormat} */
-class Lucene45DocValuesConsumer extends DocValuesConsumer implements Closeable {
-
-  static final int BLOCK_SIZE = 16384;
-  static final int ADDRESS_INTERVAL = 16;
-
-  /** Compressed using packed blocks of ints. */
-  public static final int DELTA_COMPRESSED = 0;
-  /** Compressed by computing the GCD. */
-  public static final int GCD_COMPRESSED = 1;
-  /** Compressed by giving IDs to unique values. */
-  public static final int TABLE_COMPRESSED = 2;
-  
-  /** Uncompressed binary, written directly (fixed length). */
-  public static final int BINARY_FIXED_UNCOMPRESSED = 0;
-  /** Uncompressed binary, written directly (variable length). */
-  public static final int BINARY_VARIABLE_UNCOMPRESSED = 1;
-  /** Compressed binary with shared prefixes */
-  public static final int BINARY_PREFIX_COMPRESSED = 2;
-
-  /** Standard storage for sorted set values with 1 level of indirection:
-   *  docId -> address -> ord. */
-  public static final int SORTED_SET_WITH_ADDRESSES = 0;
-  /** Single-valued sorted set values, encoded as sorted values, so no level
-   *  of indirection: docId -> ord. */
-  public static final int SORTED_SET_SINGLE_VALUED_SORTED = 1;
-
-  IndexOutput data, meta;
-  final int maxDoc;
-  
-  /** expert: Creates a new writer */
-  public Lucene45DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, Lucene45DocValuesFormat.VERSION_CURRENT);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, Lucene45DocValuesFormat.VERSION_CURRENT);
-      maxDoc = state.segmentInfo.getDocCount();
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-  
-  @Override
-  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
-    checkCanWrite(field);
-    addNumericField(field, values, true);
-  }
-
-  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {
-    long count = 0;
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    long gcd = 0;
-    boolean missing = false;
-    // TODO: more efficient?
-    HashSet<Long> uniqueValues = null;
-    if (optimizeStorage) {
-      uniqueValues = new HashSet<>();
-
-      for (Number nv : values) {
-        final long v;
-        if (nv == null) {
-          v = 0;
-          missing = true;
-        } else {
-          v = nv.longValue();
-        }
-
-        if (gcd != 1) {
-          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
-            // in that case v - minValue might overflow and make the GCD computation return
-            // wrong results. Since these extreme values are unlikely, we just discard
-            // GCD computation for them
-            gcd = 1;
-          } else if (count != 0) { // minValue needs to be set first
-            gcd = MathUtil.gcd(gcd, v - minValue);
-          }
-        }
-
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-
-        if (uniqueValues != null) {
-          if (uniqueValues.add(v)) {
-            if (uniqueValues.size() > 256) {
-              uniqueValues = null;
-            }
-          }
-        }
-
-        ++count;
-      }
-    } else {
-      for (@SuppressWarnings("unused") Number nv : values) {
-        ++count;
-      }
-    }
-    
-    final long delta = maxValue - minValue;
-
-    final int format;
-    if (uniqueValues != null
-        && (PackedInts.bitsRequired(uniqueValues.size() - 1) < PackedInts.unsignedBitsRequired(delta))
-        && count <= Integer.MAX_VALUE) {
-      format = TABLE_COMPRESSED;
-    } else if (gcd != 0 && gcd != 1) {
-      format = GCD_COMPRESSED;
-    } else {
-      format = DELTA_COMPRESSED;
-    }
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene45DocValuesFormat.NUMERIC);
-    meta.writeVInt(format);
-    if (missing) {
-      meta.writeLong(data.getFilePointer());
-      writeMissingBitset(values);
-    } else {
-      meta.writeLong(-1L);
-    }
-    meta.writeVInt(PackedInts.VERSION_CURRENT);
-    meta.writeLong(data.getFilePointer());
-    meta.writeVLong(count);
-    meta.writeVInt(BLOCK_SIZE);
-
-    switch (format) {
-      case GCD_COMPRESSED:
-        meta.writeLong(minValue);
-        meta.writeLong(gcd);
-        final BlockPackedWriter quotientWriter = new BlockPackedWriter(data, BLOCK_SIZE);
-        for (Number nv : values) {
-          long value = nv == null ? 0 : nv.longValue();
-          quotientWriter.add((value - minValue) / gcd);
-        }
-        quotientWriter.finish();
-        break;
-      case DELTA_COMPRESSED:
-        final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-        for (Number nv : values) {
-          writer.add(nv == null ? 0 : nv.longValue());
-        }
-        writer.finish();
-        break;
-      case TABLE_COMPRESSED:
-        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        final HashMap<Long,Integer> encode = new HashMap<>();
-        meta.writeVInt(decode.length);
-        for (int i = 0; i < decode.length; i++) {
-          meta.writeLong(decode[i]);
-          encode.put(decode[i], i);
-        }
-        final int bitsRequired = PackedInts.bitsRequired(uniqueValues.size() - 1);
-        final PackedInts.Writer ordsWriter = PackedInts.getWriterNoHeader(data, PackedInts.Format.PACKED, (int) count, bitsRequired, PackedInts.DEFAULT_BUFFER_SIZE);
-        for (Number nv : values) {
-          ordsWriter.add(encode.get(nv == null ? 0 : nv.longValue()));
-        }
-        ordsWriter.finish();
-        break;
-      default:
-        throw new AssertionError();
-    }
-  }
-  
-  // TODO: in some cases representing missing with minValue-1 wouldn't take up additional space and so on,
-  // but this is very simple, and algorithms only check this for values of 0 anyway (doesnt slow down normal decode)
-  void writeMissingBitset(Iterable<?> values) throws IOException {
-    byte bits = 0;
-    int count = 0;
-    for (Object v : values) {
-      if (count == 8) {
-        data.writeByte(bits);
-        count = 0;
-        bits = 0;
-      }
-      if (v != null) {
-        bits |= 1 << (count & 7);
-      }
-      count++;
-    }
-    if (count > 0) {
-      data.writeByte(bits);
-    }
-  }
-
-  @Override
-  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
-    checkCanWrite(field);
-    // write the byte[] data
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene45DocValuesFormat.BINARY);
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    final long startFP = data.getFilePointer();
-    long count = 0;
-    boolean missing = false;
-    for(BytesRef v : values) {
-      final int length;
-      if (v == null) {
-        length = 0;
-        missing = true;
-      } else {
-        length = v.length;
-      }
-      minLength = Math.min(minLength, length);
-      maxLength = Math.max(maxLength, length);
-      if (v != null) {
-        data.writeBytes(v.bytes, v.offset, v.length);
-      }
-      count++;
-    }
-    meta.writeVInt(minLength == maxLength ? BINARY_FIXED_UNCOMPRESSED : BINARY_VARIABLE_UNCOMPRESSED);
-    if (missing) {
-      meta.writeLong(data.getFilePointer());
-      writeMissingBitset(values);
-    } else {
-      meta.writeLong(-1L);
-    }
-    meta.writeVInt(minLength);
-    meta.writeVInt(maxLength);
-    meta.writeVLong(count);
-    meta.writeLong(startFP);
-    
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
-    // otherwise, we need to record the length fields...
-    if (minLength != maxLength) {
-      meta.writeLong(data.getFilePointer());
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-
-      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-      long addr = 0;
-      for (BytesRef v : values) {
-        if (v != null) {
-          addr += v.length;
-        }
-        writer.add(addr);
-      }
-      writer.finish();
-    }
-  }
-  
-  /** expert: writes a value dictionary for a sorted/sortedset field */
-  protected void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
-    // first check if its a "fixed-length" terms dict
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    for (BytesRef v : values) {
-      minLength = Math.min(minLength, v.length);
-      maxLength = Math.max(maxLength, v.length);
-    }
-    if (minLength == maxLength) {
-      // no index needed: direct addressing by mult
-      addBinaryField(field, values);
-    } else {
-      // header
-      meta.writeVInt(field.number);
-      meta.writeByte(Lucene45DocValuesFormat.BINARY);
-      meta.writeVInt(BINARY_PREFIX_COMPRESSED);
-      meta.writeLong(-1L);
-      // now write the bytes: sharing prefixes within a block
-      final long startFP = data.getFilePointer();
-      // currently, we have to store the delta from expected for every 1/nth term
-      // we could avoid this, but its not much and less overall RAM than the previous approach!
-      RAMOutputStream addressBuffer = new RAMOutputStream();
-      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);
-      BytesRefBuilder lastTerm = new BytesRefBuilder();
-      long count = 0;
-      for (BytesRef v : values) {
-        if (count % ADDRESS_INTERVAL == 0) {
-          termAddresses.add(data.getFilePointer() - startFP);
-          // force the first term in a block to be abs-encoded
-          lastTerm.clear();
-        }
-        
-        // prefix-code
-        int sharedPrefix = StringHelper.bytesDifference(lastTerm.get(), v);
-        data.writeVInt(sharedPrefix);
-        data.writeVInt(v.length - sharedPrefix);
-        data.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);
-        lastTerm.copyBytes(v);
-        count++;
-      }
-      final long indexStartFP = data.getFilePointer();
-      // write addresses of indexed terms
-      termAddresses.finish();
-      addressBuffer.writeTo(data);
-      addressBuffer = null;
-      termAddresses = null;
-      meta.writeVInt(minLength);
-      meta.writeVInt(maxLength);
-      meta.writeVLong(count);
-      meta.writeLong(startFP);
-      meta.writeVInt(ADDRESS_INTERVAL);
-      meta.writeLong(indexStartFP);
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-    }
-  }
-
-  @Override
-  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene45DocValuesFormat.SORTED);
-    addTermsDict(field, values);
-    addNumericField(field, docToOrd, false);
-  }
-  
-  @Override
-  public void addSortedNumericField(FieldInfo field, Iterable<Number> docToValueCount, Iterable<Number> values) throws IOException {
-    throw new UnsupportedOperationException("Lucene 4.5 does not support SORTED_NUMERIC");
-  }
-
-  @Override
-  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene45DocValuesFormat.SORTED_SET);
-
-    if (isSingleValued(docToOrdCount)) {
-      meta.writeVInt(SORTED_SET_SINGLE_VALUED_SORTED);
-      // The field is single-valued, we can encode it as SORTED
-      addSortedField(field, values, singletonView(docToOrdCount, ords, -1L));
-      return;
-    }
-
-    meta.writeVInt(SORTED_SET_WITH_ADDRESSES);
-
-    // write the ord -> byte[] as a binary field
-    addTermsDict(field, values);
-
-    // write the stream of ords as a numeric field
-    // NOTE: we could return an iterator that delta-encodes these within a doc
-    addNumericField(field, ords, false);
-
-    // write the doc -> ord count as a absolute index to the stream
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene45DocValuesFormat.NUMERIC);
-    meta.writeVInt(DELTA_COMPRESSED);
-    meta.writeLong(-1L);
-    meta.writeVInt(PackedInts.VERSION_CURRENT);
-    meta.writeLong(data.getFilePointer());
-    meta.writeVLong(maxDoc);
-    meta.writeVInt(BLOCK_SIZE);
-
-    final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-    long addr = 0;
-    for (Number v : docToOrdCount) {
-      addr += v.longValue();
-      writer.add(addr);
-    }
-    writer.finish();
-  }
-
-  @Override
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1); // write EOF marker
-        CodecUtil.writeFooter(meta); // write checksum
-      }
-      if (data != null) {
-        CodecUtil.writeFooter(data); // write checksum
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-      meta = data = null;
-    }
-  }
-  
-  void checkCanWrite(FieldInfo field) {
-    if ((field.getDocValuesType() == DocValuesType.NUMERIC || 
-        field.getDocValuesType() == DocValuesType.BINARY) && 
-        field.getDocValuesGen() != -1) {
-      // ok
-    } else {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java
deleted file mode 100644
index 04b4821..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesFormat.java
+++ /dev/null
@@ -1,196 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.SmallFloat;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Lucene 4.5 DocValues format.
- * <p>
- * Encodes the four per-document value types (Numeric,Binary,Sorted,SortedSet) with these strategies:
- * <p>
- * {@link DocValuesType#NUMERIC NUMERIC}:
- * <ul>
- *    <li>Delta-compressed: per-document integers written in blocks of 16k. For each block
- *        the minimum value in that block is encoded, and each entry is a delta from that 
- *        minimum value. Each block of deltas is compressed with bitpacking. For more 
- *        information, see {@link BlockPackedWriter}.
- *    <li>Table-compressed: when the number of unique values is very small (&lt; 256), and
- *        when there are unused "gaps" in the range of values used (such as {@link SmallFloat}), 
- *        a lookup table is written instead. Each per-document entry is instead the ordinal 
- *        to this table, and those ordinals are compressed with bitpacking ({@link PackedInts}). 
- *    <li>GCD-compressed: when all numbers share a common divisor, such as dates, the greatest
- *        common denominator (GCD) is computed, and quotients are stored using Delta-compressed Numerics.
- * </ul>
- * <p>
- * {@link DocValuesType#BINARY BINARY}:
- * <ul>
- *    <li>Fixed-width Binary: one large concatenated byte[] is written, along with the fixed length.
- *        Each document's value can be addressed directly with multiplication ({@code docID * length}). 
- *    <li>Variable-width Binary: one large concatenated byte[] is written, along with end addresses 
- *        for each document. The addresses are written in blocks of 16k, with the current absolute
- *        start for the block, and the average (expected) delta per entry. For each document the 
- *        deviation from the delta (actual - expected) is written.
- *    <li>Prefix-compressed Binary: values are written in chunks of 16, with the first value written
- *        completely and other values sharing prefixes. chunk addresses are written in blocks of 16k,
- *        with the current absolute start for the block, and the average (expected) delta per entry. 
- *        For each chunk the deviation from the delta (actual - expected) is written.
- * </ul>
- * <p>
- * {@link DocValuesType#SORTED SORTED}:
- * <ul>
- *    <li>Sorted: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
- *        along with the per-document ordinals written using one of the numeric strategies above.
- * </ul>
- * <p>
- * {@link DocValuesType#SORTED_SET SORTED_SET}:
- * <ul>
- *    <li>SortedSet: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
- *        an ordinal list and per-document index into this list are written using the numeric strategies 
- *        above. 
- * </ul>
- * <p>
- * Files:
- * <ol>
- *   <li><tt>.dvd</tt>: DocValues data</li>
- *   <li><tt>.dvm</tt>: DocValues metadata</li>
- * </ol>
- * <ol>
- *   <li><a name="dvm" id="dvm"></a>
- *   <p>The DocValues metadata or .dvm file.</p>
- *   <p>For DocValues field, this stores metadata, such as the offset into the 
- *      DocValues data (.dvd)</p>
- *   <p>DocValues metadata (.dvm) --&gt; Header,&lt;Entry&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>Entry --&gt; NumericEntry | BinaryEntry | SortedEntry | SortedSetEntry</li>
- *     <li>NumericEntry --&gt; GCDNumericEntry | TableNumericEntry | DeltaNumericEntry</li>
- *     <li>GCDNumericEntry --&gt; NumericHeader,MinValue,GCD</li>
- *     <li>TableNumericEntry --&gt; NumericHeader,TableSize,{@link DataOutput#writeLong Int64}<sup>TableSize</sup></li>
- *     <li>DeltaNumericEntry --&gt; NumericHeader</li>
- *     <li>NumericHeader --&gt; FieldNumber,EntryType,NumericType,MissingOffset,PackedVersion,DataOffset,Count,BlockSize</li>
- *     <li>BinaryEntry --&gt; FixedBinaryEntry | VariableBinaryEntry | PrefixBinaryEntry</li>
- *     <li>FixedBinaryEntry --&gt; BinaryHeader</li>
- *     <li>VariableBinaryEntry --&gt; BinaryHeader,AddressOffset,PackedVersion,BlockSize</li>
- *     <li>PrefixBinaryEntry --&gt; BinaryHeader,AddressInterval,AddressOffset,PackedVersion,BlockSize</li>
- *     <li>BinaryHeader --&gt; FieldNumber,EntryType,BinaryType,MissingOffset,MinLength,MaxLength,DataOffset</li>
- *     <li>SortedEntry --&gt; FieldNumber,EntryType,BinaryEntry,NumericEntry</li>
- *     <li>SortedSetEntry --&gt; EntryType,BinaryEntry,NumericEntry,NumericEntry</li>
- *     <li>FieldNumber,PackedVersion,MinLength,MaxLength,BlockSize,ValueCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *     <li>EntryType,CompressionType --&gt; {@link DataOutput#writeByte Byte}</li>
- *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *     <li>MinValue,GCD,MissingOffset,AddressOffset,DataOffset --&gt; {@link DataOutput#writeLong Int64}</li>
- *     <li>TableSize --&gt; {@link DataOutput#writeVInt vInt}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- *   <p>Sorted fields have two entries: a BinaryEntry with the value metadata,
- *      and an ordinary NumericEntry for the document-to-ord metadata.</p>
- *   <p>SortedSet fields have three entries: a BinaryEntry with the value metadata,
- *      and two NumericEntries for the document-to-ord-index and ordinal list metadata.</p>
- *   <p>FieldNumber of -1 indicates the end of metadata.</p>
- *   <p>EntryType is a 0 (NumericEntry) or 1 (BinaryEntry)</p>
- *   <p>DataOffset is the pointer to the start of the data in the DocValues data (.dvd)</p>
- *   <p>NumericType indicates how Numeric values will be compressed:
- *      <ul>
- *         <li>0 --&gt; delta-compressed. For each block of 16k integers, every integer is delta-encoded
- *             from the minimum value within the block. 
- *         <li>1 --&gt, gcd-compressed. When all integers share a common divisor, only quotients are stored
- *             using blocks of delta-encoded ints.
- *         <li>2 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
- *             a lookup table of unique values is written, followed by the ordinal for each document.
- *      </ul>
- *   <p>BinaryType indicates how Binary values will be stored:
- *      <ul>
- *         <li>0 --&gt; fixed-width. All values have the same length, addressing by multiplication. 
- *         <li>1 --&gt, variable-width. An address for each value is stored.
- *         <li>2 --&gt; prefix-compressed. An address to the start of every interval'th value is stored.
- *      </ul>
- *   <p>MinLength and MaxLength represent the min and max byte[] value lengths for Binary values.
- *      If they are equal, then all values are of a fixed size, and can be addressed as DataOffset + (docID * length).
- *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
- *      is written for the addresses.
- *   <p>MissingOffset points to a byte[] containing a bitset of all documents that had a value for the field.
- *      If its -1, then there are no missing values.
- *   <p>Checksum contains the CRC32 checksum of all bytes in the .dvm file up
- *      until the checksum. This is used to verify integrity of the file on opening the
- *      index.
- *   <li><a name="dvd" id="dvd"></a>
- *   <p>The DocValues data or .dvd file.</p>
- *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
- *   <p>DocValues data (.dvd) --&gt; Header,&lt;NumericData | BinaryData | SortedData&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>NumericData --&gt; DeltaCompressedNumerics | TableCompressedNumerics | GCDCompressedNumerics</li>
- *     <li>BinaryData --&gt;  {@link DataOutput#writeByte Byte}<sup>DataLength</sup>,Addresses</li>
- *     <li>SortedData --&gt; {@link FST FST&lt;Int64&gt;}</li>
- *     <li>DeltaCompressedNumerics --&gt; {@link BlockPackedWriter BlockPackedInts(blockSize=16k)}</li>
- *     <li>TableCompressedNumerics --&gt; {@link PackedInts PackedInts}</li>
- *     <li>GCDCompressedNumerics --&gt; {@link BlockPackedWriter BlockPackedInts(blockSize=16k)}</li>
- *     <li>Addresses --&gt; {@link MonotonicBlockPackedWriter MonotonicBlockPackedInts(blockSize=16k)}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- *   <p>SortedSet entries store the list of ordinals in their BinaryData as a
- *      sequences of increasing {@link DataOutput#writeVLong vLong}s, delta-encoded.</p>
- * </ol>
- * @deprecated Only for reading old 4.3-4.5 segments
- * @lucene.experimental
- */
-@Deprecated
-public class Lucene45DocValuesFormat extends DocValuesFormat {
-
-  /** Sole Constructor */
-  public Lucene45DocValuesFormat() {
-    super("Lucene45");
-  }
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    // really we should be read-only, but to allow posting of dv updates against old segments...
-    return new Lucene45DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
-  }
-
-  @Override
-  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new Lucene45DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
-  }
-  
-  static final String DATA_CODEC = "Lucene45DocValuesData";
-  static final String DATA_EXTENSION = "dvd";
-  static final String META_CODEC = "Lucene45ValuesMetadata";
-  static final String META_EXTENSION = "dvm";
-  static final int VERSION_START = 0;
-  static final int VERSION_SORTED_SET_SINGLE_VALUE_OPTIMIZED = 1;
-  static final int VERSION_CHECKSUM = 2;
-  static final int VERSION_CURRENT = VERSION_CHECKSUM;
-  static final byte NUMERIC = 0;
-  static final byte BINARY = 1;
-  static final byte SORTED = 2;
-  static final byte SORTED_SET = 3;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java
deleted file mode 100644
index 7dedb33..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene45/Lucene45DocValuesProducer.java
+++ /dev/null
@@ -1,909 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.BINARY_FIXED_UNCOMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.BINARY_PREFIX_COMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.BINARY_VARIABLE_UNCOMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.DELTA_COMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.GCD_COMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.SORTED_SET_SINGLE_VALUED_SORTED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.SORTED_SET_WITH_ADDRESSES;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesConsumer.TABLE_COMPRESSED;
-import static org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat.VERSION_SORTED_SET_SINGLE_VALUE_OPTIMIZED;
-
-import java.io.Closeable; // javadocs
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.RandomAccessOrds;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LongValues;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.Version;
-import org.apache.lucene.util.packed.BlockPackedReader;
-import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
-import org.apache.lucene.util.packed.PackedInts;
-
-/** reader for {@link Lucene45DocValuesFormat} */
-class Lucene45DocValuesProducer extends DocValuesProducer implements Closeable {
-  private final Map<Integer,NumericEntry> numerics;
-  private final Map<Integer,BinaryEntry> binaries;
-  private final Map<Integer,SortedSetEntry> sortedSets;
-  private final Map<Integer,NumericEntry> ords;
-  private final Map<Integer,NumericEntry> ordIndexes;
-  private final AtomicLong ramBytesUsed;
-  private final IndexInput data;
-  private final int maxDoc;
-  private final int version;
-  
-  // We need this for pre-4.9 indexes which recorded multiple fields' DocValues
-  // updates under the same generation, and therefore the passed FieldInfos may
-  // not include all the fields that are encoded in this generation. In that
-  // case, we are more lenient about the fields we read and the passed-in
-  // FieldInfos.
-  @Deprecated
-  private final boolean lenientFieldInfoCheck;
-
-  // memory-resident structures
-  private final Map<Integer,MonotonicBlockPackedReader> addressInstances = new HashMap<>();
-  private final Map<Integer,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<>();
-  
-  /** expert: instantiates a new reader */
-  @SuppressWarnings("deprecation")
-  protected Lucene45DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    Version ver = state.segmentInfo.getVersion();
-    lenientFieldInfoCheck = Version.LUCENE_4_9_0.onOrAfter(ver);
-    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-    // read in the entries from the metadata file.
-    ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
-    this.maxDoc = state.segmentInfo.getDocCount();
-    boolean success = false;
-    try {
-      version = CodecUtil.checkHeader(in, metaCodec, 
-                                      Lucene45DocValuesFormat.VERSION_START,
-                                      Lucene45DocValuesFormat.VERSION_CURRENT);
-      numerics = new HashMap<>();
-      ords = new HashMap<>();
-      ordIndexes = new HashMap<>();
-      binaries = new HashMap<>();
-      sortedSets = new HashMap<>();
-      readFields(in, state.fieldInfos);
-
-      if (version >= Lucene45DocValuesFormat.VERSION_CHECKSUM) {
-        CodecUtil.checkFooter(in);
-      } else {
-        CodecUtil.checkEOF(in);
-      }
-
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(in);
-      } else {
-        IOUtils.closeWhileHandlingException(in);
-      }
-    }
-
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-    this.data = state.directory.openInput(dataName, state.context);
-    success = false;
-    try {
-      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 Lucene45DocValuesFormat.VERSION_START,
-                                                 Lucene45DocValuesFormat.VERSION_CURRENT);
-      if (version != version2) {
-        throw new CorruptIndexException("Format versions mismatch");
-      }
-      
-      if (version >= Lucene45DocValuesFormat.VERSION_CHECKSUM) {
-        // NOTE: data file is too costly to verify checksum against all the bytes on open,
-        // but for now we at least verify proper structure of the checksum footer: which looks
-        // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-        // such as file truncation.
-        CodecUtil.retrieveChecksum(data);
-      }
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this.data);
-      }
-    }
-    
-    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
-  }
-
-  private void readSortedField(int fieldNumber, IndexInput meta, FieldInfos infos) throws IOException {
-    // sorted = binary + numeric
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    if (meta.readByte() != Lucene45DocValuesFormat.BINARY) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    BinaryEntry b = readBinaryEntry(meta);
-    binaries.put(fieldNumber, b);
-    
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    if (meta.readByte() != Lucene45DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    NumericEntry n = readNumericEntry(meta);
-    ords.put(fieldNumber, n);
-  }
-
-  private void readSortedSetFieldWithAddresses(int fieldNumber, IndexInput meta, FieldInfos infos) throws IOException {
-    // sortedset = binary + numeric (addresses) + ordIndex
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    if (meta.readByte() != Lucene45DocValuesFormat.BINARY) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    BinaryEntry b = readBinaryEntry(meta);
-    binaries.put(fieldNumber, b);
-
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    if (meta.readByte() != Lucene45DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    NumericEntry n1 = readNumericEntry(meta);
-    ords.put(fieldNumber, n1);
-
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    if (meta.readByte() != Lucene45DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    NumericEntry n2 = readNumericEntry(meta);
-    ordIndexes.put(fieldNumber, n2);
-  }
-
-  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
-    int fieldNumber = meta.readVInt();
-    while (fieldNumber != -1) {
-      if ((lenientFieldInfoCheck && fieldNumber < 0) || (!lenientFieldInfoCheck && infos.fieldInfo(fieldNumber) == null)) {
-        // trickier to validate more: because we re-use for norms, because we use multiple entries
-        // for "composite" types like sortedset, etc.
-        throw new CorruptIndexException("Invalid field number: " + fieldNumber + " (resource=" + meta + ")");
-      }
-      byte type = meta.readByte();
-      if (type == Lucene45DocValuesFormat.NUMERIC) {
-        numerics.put(fieldNumber, readNumericEntry(meta));
-      } else if (type == Lucene45DocValuesFormat.BINARY) {
-        BinaryEntry b = readBinaryEntry(meta);
-        binaries.put(fieldNumber, b);
-      } else if (type == Lucene45DocValuesFormat.SORTED) {
-        readSortedField(fieldNumber, meta, infos);
-      } else if (type == Lucene45DocValuesFormat.SORTED_SET) {
-        SortedSetEntry ss = readSortedSetEntry(meta);
-        sortedSets.put(fieldNumber, ss);
-        if (ss.format == SORTED_SET_WITH_ADDRESSES) {
-          readSortedSetFieldWithAddresses(fieldNumber, meta, infos);
-        } else if (ss.format == SORTED_SET_SINGLE_VALUED_SORTED) {
-          if (meta.readVInt() != fieldNumber) {
-            throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-          }
-          if (meta.readByte() != Lucene45DocValuesFormat.SORTED) {
-            throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-          }
-          readSortedField(fieldNumber, meta, infos);
-        } else {
-          throw new AssertionError();
-        }
-      } else {
-        throw new CorruptIndexException("invalid type: " + type + ", resource=" + meta);
-      }
-      fieldNumber = meta.readVInt();
-    }
-  }
-  
-  static NumericEntry readNumericEntry(IndexInput meta) throws IOException {
-    NumericEntry entry = new NumericEntry();
-    entry.format = meta.readVInt();
-    entry.missingOffset = meta.readLong();
-    entry.packedIntsVersion = meta.readVInt();
-    entry.offset = meta.readLong();
-    entry.count = meta.readVLong();
-    entry.blockSize = meta.readVInt();
-    switch(entry.format) {
-      case GCD_COMPRESSED:
-        entry.minValue = meta.readLong();
-        entry.gcd = meta.readLong();
-        break;
-      case TABLE_COMPRESSED:
-        if (entry.count > Integer.MAX_VALUE) {
-          throw new CorruptIndexException("Cannot use TABLE_COMPRESSED with more than MAX_VALUE values, input=" + meta);
-        }
-        final int uniqueValues = meta.readVInt();
-        if (uniqueValues > 256) {
-          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, input=" + meta);
-        }
-        entry.table = new long[uniqueValues];
-        for (int i = 0; i < uniqueValues; ++i) {
-          entry.table[i] = meta.readLong();
-        }
-        break;
-      case DELTA_COMPRESSED:
-        break;
-      default:
-        throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
-    }
-    return entry;
-  }
-  
-  static BinaryEntry readBinaryEntry(IndexInput meta) throws IOException {
-    BinaryEntry entry = new BinaryEntry();
-    entry.format = meta.readVInt();
-    entry.missingOffset = meta.readLong();
-    entry.minLength = meta.readVInt();
-    entry.maxLength = meta.readVInt();
-    entry.count = meta.readVLong();
-    entry.offset = meta.readLong();
-    switch(entry.format) {
-      case BINARY_FIXED_UNCOMPRESSED:
-        break;
-      case BINARY_PREFIX_COMPRESSED:
-        entry.addressInterval = meta.readVInt();
-        entry.addressesOffset = meta.readLong();
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      case BINARY_VARIABLE_UNCOMPRESSED:
-        entry.addressesOffset = meta.readLong();
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      default:
-        throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
-    }
-    return entry;
-  }
-
-  SortedSetEntry readSortedSetEntry(IndexInput meta) throws IOException {
-    SortedSetEntry entry = new SortedSetEntry();
-    if (version >= VERSION_SORTED_SET_SINGLE_VALUE_OPTIMIZED) {
-      entry.format = meta.readVInt();
-    } else {
-      entry.format = SORTED_SET_WITH_ADDRESSES;
-    }
-    if (entry.format != SORTED_SET_SINGLE_VALUED_SORTED && entry.format != SORTED_SET_WITH_ADDRESSES) {
-      throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
-    }
-    return entry;
-  }
-
-  @Override
-  public NumericDocValues getNumeric(FieldInfo field) throws IOException {
-    NumericEntry entry = numerics.get(field.number);
-    return getNumeric(entry);
-  }
-  
-  @Override
-  public long ramBytesUsed() {
-    return ramBytesUsed.get();
-  }
-  
-  @Override
-  public void checkIntegrity() throws IOException {
-    if (version >= Lucene45DocValuesFormat.VERSION_CHECKSUM) {
-      CodecUtil.checksumEntireFile(data);
-    }
-  }
-
-  LongValues getNumeric(NumericEntry entry) throws IOException {
-    final IndexInput data = this.data.clone();
-    data.seek(entry.offset);
-
-    switch (entry.format) {
-      case DELTA_COMPRESSED:
-        final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
-        return reader;
-      case GCD_COMPRESSED:
-        final long min = entry.minValue;
-        final long mult = entry.gcd;
-        final BlockPackedReader quotientReader = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return min + mult * quotientReader.get(id);
-          }
-        };
-      case TABLE_COMPRESSED:
-        final long table[] = entry.table;
-        final int bitsRequired = PackedInts.bitsRequired(table.length - 1);
-        final PackedInts.Reader ords = PackedInts.getDirectReaderNoHeader(data, PackedInts.Format.PACKED, entry.packedIntsVersion, (int) entry.count, bitsRequired);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return table[(int) ords.get((int) id)];
-          }
-        };
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public BinaryDocValues getBinary(FieldInfo field) throws IOException {
-    BinaryEntry bytes = binaries.get(field.number);
-    switch(bytes.format) {
-      case BINARY_FIXED_UNCOMPRESSED:
-        return getFixedBinary(field, bytes);
-      case BINARY_VARIABLE_UNCOMPRESSED:
-        return getVariableBinary(field, bytes);
-      case BINARY_PREFIX_COMPRESSED:
-        return getCompressedBinary(field, bytes);
-      default:
-        throw new AssertionError();
-    }
-  }
-  
-  private BinaryDocValues getFixedBinary(FieldInfo field, final BinaryEntry bytes) {
-    final IndexInput data = this.data.clone();
-
-    return new LongBinaryDocValues() {
-      final BytesRef term;
-      {
-        term = new BytesRef(bytes.maxLength);
-        term.offset = 0;
-        term.length = bytes.maxLength;
-      }
-
-      @Override
-      public BytesRef get(long id) {
-        long address = bytes.offset + id * bytes.maxLength;
-        try {
-          data.seek(address);
-          data.readBytes(term.bytes, 0, term.length);
-          return term;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for variable-length binary values.
-   *  @lucene.internal */
-  protected MonotonicBlockPackedReader getAddressInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
-    final MonotonicBlockPackedReader addresses;
-    synchronized (addressInstances) {
-      MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
-      if (addrInstance == null) {
-        data.seek(bytes.addressesOffset);
-        addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count, false);
-        addressInstances.put(field.number, addrInstance);
-        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-      addresses = addrInstance;
-    }
-    return addresses;
-  }
-  
-  private BinaryDocValues getVariableBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
-    final IndexInput data = this.data.clone();
-    
-    final MonotonicBlockPackedReader addresses = getAddressInstance(data, field, bytes);
-
-    return new LongBinaryDocValues() {
-      final BytesRef term = new BytesRef(Math.max(0, bytes.maxLength));
-
-      @Override
-      public BytesRef get(long id) {
-        long startAddress = bytes.offset + (id == 0 ? 0 : addresses.get(id-1));
-        long endAddress = bytes.offset + addresses.get(id);
-        int length = (int) (endAddress - startAddress);
-        try {
-          data.seek(startAddress);
-          data.readBytes(term.bytes, 0, length);
-          term.length = length;
-          return term;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for prefix-compressed binary values. 
-   * @lucene.internal */
-  protected MonotonicBlockPackedReader getIntervalInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
-    final MonotonicBlockPackedReader addresses;
-    final long interval = bytes.addressInterval;
-    synchronized (addressInstances) {
-      MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
-      if (addrInstance == null) {
-        data.seek(bytes.addressesOffset);
-        final long size;
-        if (bytes.count % interval == 0) {
-          size = bytes.count / interval;
-        } else {
-          size = 1L + bytes.count / interval;
-        }
-        addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
-        addressInstances.put(field.number, addrInstance);
-        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-      addresses = addrInstance;
-    }
-    return addresses;
-  }
-
-
-  private BinaryDocValues getCompressedBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
-    final IndexInput data = this.data.clone();
-
-    final MonotonicBlockPackedReader addresses = getIntervalInstance(data, field, bytes);
-    
-    return new CompressedBinaryDocValues(bytes, addresses, data);
-  }
-
-  @Override
-  public SortedDocValues getSorted(FieldInfo field) throws IOException {
-    final int valueCount = (int) binaries.get(field.number).count;
-    final BinaryDocValues binary = getBinary(field);
-    NumericEntry entry = ords.get(field.number);
-    IndexInput data = this.data.clone();
-    data.seek(entry.offset);
-    final BlockPackedReader ordinals = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
-    
-    return new SortedDocValues() {
-
-      @Override
-      public int getOrd(int docID) {
-        return (int) ordinals.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        return binary.get(ord);
-      }
-
-      @Override
-      public int getValueCount() {
-        return valueCount;
-      }
-
-      @Override
-      public int lookupTerm(BytesRef key) {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return (int) ((CompressedBinaryDocValues)binary).lookupTerm(key);
-        } else {
-        return super.lookupTerm(key);
-        }
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).getTermsEnum();
-        } else {
-          return super.termsEnum();
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for sortedset ordinal lists
-   * @lucene.internal */
-  protected MonotonicBlockPackedReader getOrdIndexInstance(IndexInput data, FieldInfo field, NumericEntry entry) throws IOException {
-    final MonotonicBlockPackedReader ordIndex;
-    synchronized (ordIndexInstances) {
-      MonotonicBlockPackedReader ordIndexInstance = ordIndexInstances.get(field.number);
-      if (ordIndexInstance == null) {
-        data.seek(entry.offset);
-        ordIndexInstance = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, entry.count, false);
-        ordIndexInstances.put(field.number, ordIndexInstance);
-        ramBytesUsed.addAndGet(ordIndexInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-      ordIndex = ordIndexInstance;
-    }
-    return ordIndex;
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
-    throw new IllegalStateException("Lucene 4.5 does not support SortedNumeric: how did you pull this off?");
-  }
-
-  @Override
-  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
-    SortedSetEntry ss = sortedSets.get(field.number);
-    if (ss.format == SORTED_SET_SINGLE_VALUED_SORTED) {
-      final SortedDocValues values = getSorted(field);
-      return DocValues.singleton(values);
-    } else if (ss.format != SORTED_SET_WITH_ADDRESSES) {
-      throw new AssertionError();
-    }
-
-    final IndexInput data = this.data.clone();
-    final long valueCount = binaries.get(field.number).count;
-    // we keep the byte[]s and list of ords on disk, these could be large
-    final LongBinaryDocValues binary = (LongBinaryDocValues) getBinary(field);
-    final LongValues ordinals = getNumeric(ords.get(field.number));
-    // but the addresses to the ord stream are in RAM
-    final MonotonicBlockPackedReader ordIndex = getOrdIndexInstance(data, field, ordIndexes.get(field.number));
-    
-    return new RandomAccessOrds() {
-      long startOffset;
-      long offset;
-      long endOffset;
-      
-      @Override
-      public long nextOrd() {
-        if (offset == endOffset) {
-          return NO_MORE_ORDS;
-        } else {
-          long ord = ordinals.get(offset);
-          offset++;
-          return ord;
-        }
-      }
-
-      @Override
-      public void setDocument(int docID) {
-        startOffset = offset = (docID == 0 ? 0 : ordIndex.get(docID-1));
-        endOffset = ordIndex.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(long ord) {
-        return binary.get(ord);
-      }
-
-      @Override
-      public long getValueCount() {
-        return valueCount;
-      }
-      
-      @Override
-      public long lookupTerm(BytesRef key) {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).lookupTerm(key);
-        } else {
-          return super.lookupTerm(key);
-        }
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).getTermsEnum();
-        } else {
-          return super.termsEnum();
-        }
-      }
-
-      @Override
-      public long ordAt(int index) {
-        return ordinals.get(startOffset + index);
-      }
-
-      @Override
-      public int cardinality() {
-        return (int) (endOffset - startOffset);
-      }
-    };
-  }
-  
-  private Bits getMissingBits(final long offset) throws IOException {
-    if (offset == -1) {
-      return new Bits.MatchAllBits(maxDoc);
-    } else {
-      final IndexInput in = data.clone();
-      return new Bits() {
-
-        @Override
-        public boolean get(int index) {
-          try {
-            in.seek(offset + (index >> 3));
-            return (in.readByte() & (1 << (index & 7))) != 0;
-          } catch (IOException e) {
-            throw new RuntimeException(e);
-          }
-        }
-
-        @Override
-        public int length() {
-          return maxDoc;
-        }
-      };
-    }
-  }
-
-  @Override
-  public Bits getDocsWithField(FieldInfo field) throws IOException {
-    switch(field.getDocValuesType()) {
-      case SORTED_SET:
-        return DocValues.docsWithValue(getSortedSet(field), maxDoc);
-      case SORTED:
-        return DocValues.docsWithValue(getSorted(field), maxDoc);
-      case BINARY:
-        BinaryEntry be = binaries.get(field.number);
-        return getMissingBits(be.missingOffset);
-      case NUMERIC:
-        NumericEntry ne = numerics.get(field.number);
-        return getMissingBits(ne.missingOffset);
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    data.close();
-  }
-  
-  /** metadata entry for a numeric docvalues field */
-  protected static class NumericEntry {
-    private NumericEntry() {}
-    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
-    long missingOffset;
-    /** offset to the actual numeric values */
-    public long offset;
-
-    int format;
-    /** packed ints version used to encode these numerics */
-    public int packedIntsVersion;
-    /** count of values written */
-    public long count;
-    /** packed ints blocksize */
-    public int blockSize;
-    
-    long minValue;
-    long gcd;
-    long table[];
-  }
-  
-  /** metadata entry for a binary docvalues field */
-  protected static class BinaryEntry {
-    private BinaryEntry() {}
-    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
-    long missingOffset;
-    /** offset to the actual binary values */
-    long offset;
-
-    int format;
-    /** count of values written */
-    public long count;
-    int minLength;
-    int maxLength;
-    /** offset to the addressing data that maps a value to its slice of the byte[] */
-    public long addressesOffset;
-    /** interval of shared prefix chunks (when using prefix-compressed binary) */
-    public long addressInterval;
-    /** packed ints version used to encode addressing information */
-    public int packedIntsVersion;
-    /** packed ints blocksize */
-    public int blockSize;
-  }
-
-  /** metadata entry for a sorted-set docvalues field */
-  protected static class SortedSetEntry {
-    private SortedSetEntry() {}
-    int format;
-  }
-
-  // internally we compose complex dv (sorted/sortedset) from other ones
-  static abstract class LongBinaryDocValues extends BinaryDocValues {
-    @Override
-    public final BytesRef get(int docID) {
-      return get((long) docID);
-    }
-    
-    abstract BytesRef get(long id);
-  }
-  
-  // in the compressed case, we add a few additional operations for
-  // more efficient reverse lookup and enumeration
-  static class CompressedBinaryDocValues extends LongBinaryDocValues {
-    final BinaryEntry bytes;
-    final long interval;
-    final long numValues;
-    final long numIndexValues;
-    final MonotonicBlockPackedReader addresses;
-    final IndexInput data;
-    final TermsEnum termsEnum;
-    
-    public CompressedBinaryDocValues(BinaryEntry bytes, MonotonicBlockPackedReader addresses, IndexInput data) throws IOException {
-      this.bytes = bytes;
-      this.interval = bytes.addressInterval;
-      this.addresses = addresses;
-      this.data = data;
-      this.numValues = bytes.count;
-      this.numIndexValues = addresses.size();
-      this.termsEnum = getTermsEnum(data);
-    }
-    
-    @Override
-    public BytesRef get(long id) {
-      try {
-        termsEnum.seekExact(id);
-        return termsEnum.term();
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    long lookupTerm(BytesRef key) {
-      try {
-        SeekStatus status = termsEnum.seekCeil(key);
-        if (status == SeekStatus.END) {
-          return -numValues-1;
-        } else if (status == SeekStatus.FOUND) {
-          return termsEnum.ord();
-        } else {
-          return -termsEnum.ord()-1;
-        }
-      } catch (IOException bogus) {
-        throw new RuntimeException(bogus);
-      }
-    }
-    
-    TermsEnum getTermsEnum() {
-      try {
-        return getTermsEnum(data.clone());
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    private TermsEnum getTermsEnum(final IndexInput input) throws IOException {
-      input.seek(bytes.offset);
-      
-      return new TermsEnum() {
-        private long currentOrd = -1;
-        // TODO: maxLength is negative when all terms are merged away...
-        private final BytesRef term = new BytesRef(bytes.maxLength < 0 ? 0 : bytes.maxLength);
-
-        @Override
-        public BytesRef next() throws IOException {
-          if (++currentOrd >= numValues) {
-            return null;
-          } else {
-            int start = input.readVInt();
-            int suffix = input.readVInt();
-            input.readBytes(term.bytes, start, suffix);
-            term.length = start + suffix;
-            return term;
-          }
-        }
-
-        @Override
-        public SeekStatus seekCeil(BytesRef text) throws IOException {
-          // binary-search just the index values to find the block,
-          // then scan within the block
-          long low = 0;
-          long high = numIndexValues-1;
-
-          while (low <= high) {
-            long mid = (low + high) >>> 1;
-            seekExact(mid * interval);
-            int cmp = term.compareTo(text);
-
-            if (cmp < 0) {
-              low = mid + 1;
-            } else if (cmp > 0) {
-              high = mid - 1;
-            } else {
-              // we got lucky, found an indexed term
-              return SeekStatus.FOUND;
-            }
-          }
-          
-          if (numIndexValues == 0) {
-            return SeekStatus.END;
-          }
-          
-          // block before insertion point
-          long block = low-1;
-          seekExact(block < 0 ? -1 : block * interval);
-          
-          while (next() != null) {
-            int cmp = term.compareTo(text);
-            if (cmp == 0) {
-              return SeekStatus.FOUND;
-            } else if (cmp > 0) {
-              return SeekStatus.NOT_FOUND;
-            }
-          }
-          
-          return SeekStatus.END;
-        }
-
-        @Override
-        public void seekExact(long ord) throws IOException {
-          long block = ord / interval;
-
-          if (ord >= currentOrd && block == currentOrd / interval) {
-            // seek within current block
-          } else {
-            // position before start of block
-            currentOrd = ord - ord % interval - 1;
-            input.seek(bytes.offset + addresses.get(block));
-          }
-          
-          while (currentOrd < ord) {
-            next();
-          }
-        }
-
-        @Override
-        public BytesRef term() throws IOException {
-          return term;
-        }
-
-        @Override
-        public long ord() throws IOException {
-          return currentOrd;
-        }
-
-        @Override
-        public int docFreq() throws IOException {
-          throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public long totalTermFreq() throws IOException {
-          return -1;
-        }
-
-        @Override
-        public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-          throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-          throw new UnsupportedOperationException();
-        }
-      };
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene45/package.html b/lucene/core/src/java/org/apache/lucene/codecs/lucene45/package.html
deleted file mode 100644
index 890ca6c..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene45/package.html
+++ /dev/null
@@ -1,396 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Lucene 4.5 file format.
-
-<h1>Apache Lucene - Index File Formats</h1>
-<div>
-<ul>
-<li><a href="#Introduction">Introduction</a></li>
-<li><a href="#Definitions">Definitions</a>
-  <ul>
-  <li><a href="#Inverted_Indexing">Inverted Indexing</a></li>
-  <li><a href="#Types_of_Fields">Types of Fields</a></li>
-  <li><a href="#Segments">Segments</a></li>
-  <li><a href="#Document_Numbers">Document Numbers</a></li>
-  </ul>
-</li>
-<li><a href="#Overview">Index Structure Overview</a></li>
-<li><a href="#File_Naming">File Naming</a></li>
-<li><a href="#file-names">Summary of File Extensions</a></li>
-  <ul>
-  <li><a href="#Lock_File">Lock File</a></li>
-  <li><a href="#History">History</a></li>
-  <li><a href="#Limitations">Limitations</a></li>
-  </ul>
-</ul>
-</div>
-<a name="Introduction"></a>
-<h2>Introduction</h2>
-<div>
-<p>This document defines the index file formats used in this version of Lucene.
-If you are using a different version of Lucene, please consult the copy of
-<code>docs/</code> that was distributed with
-the version you are using.</p>
-<p>Apache Lucene is written in Java, but several efforts are underway to write
-<a href="http://wiki.apache.org/lucene-java/LuceneImplementations">versions of
-Lucene in other programming languages</a>. If these versions are to remain
-compatible with Apache Lucene, then a language-independent definition of the
-Lucene index format is required. This document thus attempts to provide a
-complete and independent definition of the Apache Lucene file formats.</p>
-<p>As Lucene evolves, this document should evolve. Versions of Lucene in
-different programming languages should endeavor to agree on file formats, and
-generate new versions of this document.</p>
-</div>
-<a name="Definitions" id="Definitions"></a>
-<h2>Definitions</h2>
-<div>
-<p>The fundamental concepts in Lucene are index, document, field and term.</p>
-<p>An index contains a sequence of documents.</p>
-<ul>
-<li>A document is a sequence of fields.</li>
-<li>A field is a named sequence of terms.</li>
-<li>A term is a sequence of bytes.</li>
-</ul>
-<p>The same sequence of bytes in two different fields is considered a different 
-term. Thus terms are represented as a pair: the string naming the field, and the
-bytes within the field.</p>
-<a name="Inverted_Indexing"></a>
-<h3>Inverted Indexing</h3>
-<p>The index stores statistics about terms in order to make term-based search
-more efficient. Lucene's index falls into the family of indexes known as an
-<i>inverted index.</i> This is because it can list, for a term, the documents
-that contain it. This is the inverse of the natural relationship, in which
-documents list terms.</p>
-<a name="Types_of_Fields"></a>
-<h3>Types of Fields</h3>
-<p>In Lucene, fields may be <i>stored</i>, in which case their text is stored
-in the index literally, in a non-inverted manner. Fields that are inverted are
-called <i>indexed</i>. A field may be both stored and indexed.</p>
-<p>The text of a field may be <i>tokenized</i> into terms to be indexed, or the
-text of a field may be used literally as a term to be indexed. Most fields are
-tokenized, but sometimes it is useful for certain identifier fields to be
-indexed literally.</p>
-<p>See the {@link org.apache.lucene.document.Field Field}
-java docs for more information on Fields.</p>
-<a name="Segments" id="Segments"></a>
-<h3>Segments</h3>
-<p>Lucene indexes may be composed of multiple sub-indexes, or <i>segments</i>.
-Each segment is a fully independent index, which could be searched separately.
-Indexes evolve by:</p>
-<ol>
-<li>Creating new segments for newly added documents.</li>
-<li>Merging existing segments.</li>
-</ol>
-<p>Searches may involve multiple segments and/or multiple indexes, each index
-potentially composed of a set of segments.</p>
-<a name="Document_Numbers"></a>
-<h3>Document Numbers</h3>
-<p>Internally, Lucene refers to documents by an integer <i>document number</i>.
-The first document added to an index is numbered zero, and each subsequent
-document added gets a number one greater than the previous.</p>
-<p>Note that a document's number may change, so caution should be taken when
-storing these numbers outside of Lucene. In particular, numbers may change in
-the following situations:</p>
-<ul>
-<li>
-<p>The numbers stored in each segment are unique only within the segment, and
-must be converted before they can be used in a larger context. The standard
-technique is to allocate each segment a range of values, based on the range of
-numbers used in that segment. To convert a document number from a segment to an
-external value, the segment's <i>base</i> document number is added. To convert
-an external value back to a segment-specific value, the segment is identified
-by the range that the external value is in, and the segment's base value is
-subtracted. For example two five document segments might be combined, so that
-the first segment has a base value of zero, and the second of five. Document
-three from the second segment would have an external value of eight.</p>
-</li>
-<li>
-<p>When documents are deleted, gaps are created in the numbering. These are
-eventually removed as the index evolves through merging. Deleted documents are
-dropped when segments are merged. A freshly-merged segment thus has no gaps in
-its numbering.</p>
-</li>
-</ul>
-</div>
-<a name="Overview" id="Overview"></a>
-<h2>Index Structure Overview</h2>
-<div>
-<p>Each segment index maintains the following:</p>
-<ul>
-<li>
-{@link org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat Segment info}.
-   This contains metadata about a segment, such as the number of documents,
-   what files it uses, 
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene42.Lucene42FieldInfosFormat Field names}. 
-   This contains the set of field names used in the index.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Stored Field values}. 
-This contains, for each document, a list of attribute-value pairs, where the attributes 
-are field names. These are used to store auxiliary information about the document, such as 
-its title, url, or an identifier to access a database. The set of stored fields are what is 
-returned for each hit when searching. This is keyed by document number.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Term dictionary}. 
-A dictionary containing all of the terms used in all of the
-indexed fields of all of the documents. The dictionary also contains the number
-of documents which contain the term, and pointers to the term's frequency and
-proximity data.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Term Frequency data}. 
-For each term in the dictionary, the numbers of all the
-documents that contain that term, and the frequency of the term in that
-document, unless frequencies are omitted (IndexOptions.DOCS_ONLY)
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Term Proximity data}. 
-For each term in the dictionary, the positions that the
-term occurs in each document. Note that this will not exist if all fields in
-all documents omit position data.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene42.Lucene42NormsFormat Normalization factors}. 
-For each field in each document, a value is stored
-that is multiplied into the score for hits on that field.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vectors}. 
-For each field in each document, the term vector (sometimes
-called document vector) may be stored. A term vector consists of term text and
-term frequency. To add Term Vectors to your index see the 
-{@link org.apache.lucene.document.Field Field} constructors
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat Per-document values}. 
-Like stored values, these are also keyed by document
-number, but are generally intended to be loaded into main memory for fast
-access. Whereas stored values are generally intended for summary results from
-searches, per-document values are useful for things like scoring factors.
-</li>
-<li>
-{@link org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat Deleted documents}. 
-An optional file indicating which documents are deleted.
-</li>
-</ul>
-<p>Details on each of these are provided in their linked pages.</p>
-</div>
-<a name="File_Naming"></a>
-<h2>File Naming</h2>
-<div>
-<p>All files belonging to a segment have the same name with varying extensions.
-The extensions correspond to the different file formats described below. When
-using the Compound File format (default in 1.4 and greater) these files (except
-for the Segment info file, the Lock file, and Deleted documents file) are collapsed 
-into a single .cfs file (see below for details)</p>
-<p>Typically, all segments in an index are stored in a single directory,
-although this is not required.</p>
-<p>As of version 2.1 (lock-less commits), file names are never re-used (there
-is one exception, "segments.gen", see below). That is, when any file is saved
-to the Directory it is given a never before used filename. This is achieved
-using a simple generations approach. For example, the first segments file is
-segments_1, then segments_2, etc. The generation is a sequential long integer
-represented in alpha-numeric (base 36) form.</p>
-</div>
-<a name="file-names" id="file-names"></a>
-<h2>Summary of File Extensions</h2>
-<div>
-<p>The following table summarizes the names and extensions of the files in
-Lucene:</p>
-<table cellspacing="1" cellpadding="4">
-<tr>
-<th>Name</th>
-<th>Extension</th>
-<th>Brief Description</th>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.index.SegmentInfos Segments File}</td>
-<td>segments.gen, segments_N</td>
-<td>Stores information about a commit point</td>
-</tr>
-<tr>
-<td><a href="#Lock_File">Lock File</a></td>
-<td>write.lock</td>
-<td>The Write lock prevents multiple IndexWriters from writing to the same
-file.</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat Segment Info}</td>
-<td>.si</td>
-<td>Stores metadata about a segment</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.store.CompoundFileDirectory Compound File}</td>
-<td>.cfs, .cfe</td>
-<td>An optional "virtual" file consisting of all the other index files for
-systems that frequently run out of file handles.</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene42.Lucene42FieldInfosFormat Fields}</td>
-<td>.fnm</td>
-<td>Stores information about the fields</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Field Index}</td>
-<td>.fdx</td>
-<td>Contains pointers to field data</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat Field Data}</td>
-<td>.fdt</td>
-<td>The stored fields for documents</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Term Dictionary}</td>
-<td>.tim</td>
-<td>The term dictionary, stores term info</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Term Index}</td>
-<td>.tip</td>
-<td>The index into the Term Dictionary</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Frequencies}</td>
-<td>.doc</td>
-<td>Contains the list of docs which contain each term along with frequency</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Positions}</td>
-<td>.pos</td>
-<td>Stores position information about where a term occurs in the index</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat Payloads}</td>
-<td>.pay</td>
-<td>Stores additional per-position metadata information such as character offsets and user payloads</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene42.Lucene42NormsFormat Norms}</td>
-<td>.nvd, .nvm</td>
-<td>Encodes length and boost factors for docs and fields</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat Per-Document Values}</td>
-<td>.dvd, .dvm</td>
-<td>Encodes additional scoring factors or other per-document information.</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Index}</td>
-<td>.tvx</td>
-<td>Stores offset into the document data file</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Documents}</td>
-<td>.tvd</td>
-<td>Contains information about each document that has term vectors</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat Term Vector Fields}</td>
-<td>.tvf</td>
-<td>The field level info about term vectors</td>
-</tr>
-<tr>
-<td>{@link org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat Deleted Documents}</td>
-<td>.del</td>
-<td>Info about what files are deleted</td>
-</tr>
-</table>
-</div>
-<a name="Lock_File" id="Lock_File"></a>
-<h2>Lock File</h2>
-The write lock, which is stored in the index directory by default, is named
-"write.lock". If the lock directory is different from the index directory then
-the write lock will be named "XXXX-write.lock" where XXXX is a unique prefix
-derived from the full path to the index directory. When this file is present, a
-writer is currently modifying the index (adding or removing documents). This
-lock file ensures that only one writer is modifying the index at a time.</p>
-<a name="History"></a>
-<h2>History</h2>
-<p>Compatibility notes are provided in this document, describing how file
-formats have changed from prior versions:</p>
-<ul>
-<li>In version 2.1, the file format was changed to allow lock-less commits (ie,
-no more commit lock). The change is fully backwards compatible: you can open a
-pre-2.1 index for searching or adding/deleting of docs. When the new segments
-file is saved (committed), it will be written in the new file format (meaning
-no specific "upgrade" process is needed). But note that once a commit has
-occurred, pre-2.1 Lucene will not be able to read the index.</li>
-<li>In version 2.3, the file format was changed to allow segments to share a
-single set of doc store (vectors &amp; stored fields) files. This allows for
-faster indexing in certain cases. The change is fully backwards compatible (in
-the same way as the lock-less commits change in 2.1).</li>
-<li>In version 2.4, Strings are now written as true UTF-8 byte sequence, not
-Java's modified UTF-8. See <a href="http://issues.apache.org/jira/browse/LUCENE-510">
-LUCENE-510</a> for details.</li>
-<li>In version 2.9, an optional opaque Map&lt;String,String&gt; CommitUserData
-may be passed to IndexWriter's commit methods (and later retrieved), which is
-recorded in the segments_N file. See <a href="http://issues.apache.org/jira/browse/LUCENE-1382">
-LUCENE-1382</a> for details. Also,
-diagnostics were added to each segment written recording details about why it
-was written (due to flush, merge; which OS/JRE was used; etc.). See issue
-<a href="http://issues.apache.org/jira/browse/LUCENE-1654">LUCENE-1654</a> for details.</li>
-<li>In version 3.0, compressed fields are no longer written to the index (they
-can still be read, but on merge the new segment will write them, uncompressed).
-See issue <a href="http://issues.apache.org/jira/browse/LUCENE-1960">LUCENE-1960</a> 
-for details.</li>
-<li>In version 3.1, segments records the code version that created them. See
-<a href="http://issues.apache.org/jira/browse/LUCENE-2720">LUCENE-2720</a> for details. 
-Additionally segments track explicitly whether or not they have term vectors. 
-See <a href="http://issues.apache.org/jira/browse/LUCENE-2811">LUCENE-2811</a> 
-for details.</li>
-<li>In version 3.2, numeric fields are written as natively to stored fields
-file, previously they were stored in text format only.</li>
-<li>In version 3.4, fields can omit position data while still indexing term
-frequencies.</li>
-<li>In version 4.0, the format of the inverted index became extensible via
-the {@link org.apache.lucene.codecs.Codec Codec} api. Fast per-document storage
-({@code DocValues}) was introduced. Normalization factors need no longer be a 
-single byte, they can be any {@link org.apache.lucene.index.NumericDocValues NumericDocValues}. 
-Terms need not be unicode strings, they can be any byte sequence. Term offsets 
-can optionally be indexed into the postings lists. Payloads can be stored in the 
-term vectors.</li>
-<li>In version 4.1, the format of the postings list changed to use either
-of FOR compression or variable-byte encoding, depending upon the frequency
-of the term. Terms appearing only once were changed to inline directly into
-the term dictionary. Stored fields are compressed by default. </li>
-<li>In version 4.2, term vectors are compressed by default. DocValues has 
-a new multi-valued type (SortedSet), that can be used for faceting/grouping/joining
-on multi-valued fields.</li>
-<li>In version 4.5, DocValues were extended to explicitly represent missing values.</li>
-</ul>
-<a name="Limitations" id="Limitations"></a>
-<h2>Limitations</h2>
-<div>
-<p>Lucene uses a Java <code>int</code> to refer to
-document numbers, and the index file format uses an <code>Int32</code>
-on-disk to store document numbers. This is a limitation
-of both the index file format and the current implementation. Eventually these
-should be replaced with either <code>UInt64</code> values, or
-better yet, {@link org.apache.lucene.store.DataOutput#writeVInt VInt} values which have no limit.</p>
-</div>
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java
deleted file mode 100755
index 833c16b..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene46/Lucene46Codec.java
+++ /dev/null
@@ -1,149 +0,0 @@
-package org.apache.lucene.codecs.lucene46;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42NormsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
-import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Implements the Lucene 4.6 index format, with configurable per-field postings
- * and docvalues formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene46 package documentation for file format details.
- * @lucene.experimental
- * @deprecated Only for reading old 4.6-4.8 segments
- */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene46Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
-@Deprecated
-public class Lucene46Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene46FieldInfosFormat();
-  private final SegmentInfoFormat segmentInfosFormat = new Lucene46SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene46Codec.this.getPostingsFormatForField(field);
-    }
-  };
-  
-  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
-    @Override
-    public DocValuesFormat getDocValuesFormatForField(String field) {
-      return Lucene46Codec.this.getDocValuesFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene46Codec() {
-    super("Lucene46");
-  }
-  
-  @Override
-  public final StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public final TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public final FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public final SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfosFormat;
-  }
-  
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene41"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  /** Returns the docvalues format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene45"
-   */
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return defaultDVFormat;
-  }
-  
-  @Override
-  public final DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene45");
-
-  private final NormsFormat normsFormat = new Lucene42NormsFormat() {
-    @Override
-    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  };
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java
deleted file mode 100644
index cf18c203..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49Codec.java
+++ /dev/null
@@ -1,148 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FilterCodec;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41StoredFieldsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42TermVectorsFormat;
-import org.apache.lucene.codecs.lucene46.Lucene46FieldInfosFormat;
-import org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoFormat;
-import org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat;
-import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Implements the Lucene 4.9 index format, with configurable per-field postings
- * and docvalues formats.
- * <p>
- * If you want to reuse functionality of this codec in another codec, extend
- * {@link FilterCodec}.
- *
- * @see org.apache.lucene.codecs.lucene49 package documentation for file format details.
- * @lucene.experimental
- */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene410Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
-public class Lucene49Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene42TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene46FieldInfosFormat();
-  private final SegmentInfoFormat segmentInfosFormat = new Lucene46SegmentInfoFormat();
-  private final LiveDocsFormat liveDocsFormat = new Lucene40LiveDocsFormat();
-  
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene49Codec.this.getPostingsFormatForField(field);
-    }
-  };
-  
-  private final DocValuesFormat docValuesFormat = new PerFieldDocValuesFormat() {
-    @Override
-    public DocValuesFormat getDocValuesFormatForField(String field) {
-      return Lucene49Codec.this.getDocValuesFormatForField(field);
-    }
-  };
-
-  /** Sole constructor. */
-  public Lucene49Codec() {
-    super("Lucene49");
-  }
-  
-  @Override
-  public final StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public final TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public final PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public final FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public final SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfosFormat;
-  }
-  
-  @Override
-  public final LiveDocsFormat liveDocsFormat() {
-    return liveDocsFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene41"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  /** Returns the docvalues format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene49"
-   */
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return defaultDVFormat;
-  }
-  
-  @Override
-  public final DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene41");
-  private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene49");
-
-  private final NormsFormat normsFormat = new Lucene49NormsFormat() {
-    @Override
-    public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  };
-
-  @Override
-  public NormsFormat normsFormat() {
-   return normsFormat;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java
deleted file mode 100644
index 416f070..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesConsumer.java
+++ /dev/null
@@ -1,471 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable; // javadocs
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.HashSet;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.MathUtil;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.packed.DirectWriter;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/** writer for {@link Lucene49DocValuesFormat} */
-class Lucene49DocValuesConsumer extends DocValuesConsumer implements Closeable {
-
-  static final int BLOCK_SIZE = 16384;
-  static final int ADDRESS_INTERVAL = 16;
-
-  /** Compressed using packed blocks of ints. */
-  public static final int DELTA_COMPRESSED = 0;
-  /** Compressed by computing the GCD. */
-  public static final int GCD_COMPRESSED = 1;
-  /** Compressed by giving IDs to unique values. */
-  public static final int TABLE_COMPRESSED = 2;
-  /** Compressed with monotonically increasing values */
-  public static final int MONOTONIC_COMPRESSED = 3;
-  
-  /** Uncompressed binary, written directly (fixed length). */
-  public static final int BINARY_FIXED_UNCOMPRESSED = 0;
-  /** Uncompressed binary, written directly (variable length). */
-  public static final int BINARY_VARIABLE_UNCOMPRESSED = 1;
-  /** Compressed binary with shared prefixes */
-  public static final int BINARY_PREFIX_COMPRESSED = 2;
-
-  /** Standard storage for sorted set values with 1 level of indirection:
-   *  docId -> address -> ord. */
-  public static final int SORTED_WITH_ADDRESSES = 0;
-  /** Single-valued sorted set values, encoded as sorted values, so no level
-   *  of indirection: docId -> ord. */
-  public static final int SORTED_SINGLE_VALUED = 1;
-
-  IndexOutput data, meta;
-  final int maxDoc;
-  
-  /** expert: Creates a new writer */
-  public Lucene49DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, Lucene49DocValuesFormat.VERSION_CURRENT);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, Lucene49DocValuesFormat.VERSION_CURRENT);
-      maxDoc = state.segmentInfo.getDocCount();
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-  
-  @Override
-  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
-    checkCanWrite(field);
-    addNumericField(field, values, true);
-  }
-
-  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {
-    long count = 0;
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    long gcd = 0;
-    boolean missing = false;
-    // TODO: more efficient?
-    HashSet<Long> uniqueValues = null;
-    if (optimizeStorage) {
-      uniqueValues = new HashSet<>();
-
-      for (Number nv : values) {
-        final long v;
-        if (nv == null) {
-          v = 0;
-          missing = true;
-        } else {
-          v = nv.longValue();
-        }
-
-        if (gcd != 1) {
-          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
-            // in that case v - minValue might overflow and make the GCD computation return
-            // wrong results. Since these extreme values are unlikely, we just discard
-            // GCD computation for them
-            gcd = 1;
-          } else if (count != 0) { // minValue needs to be set first
-            gcd = MathUtil.gcd(gcd, v - minValue);
-          }
-        }
-
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-
-        if (uniqueValues != null) {
-          if (uniqueValues.add(v)) {
-            if (uniqueValues.size() > 256) {
-              uniqueValues = null;
-            }
-          }
-        }
-
-        ++count;
-      }
-    } else {
-      for (Number nv : values) {
-        long v = nv.longValue();
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-        ++count;
-      }
-    }
-    
-    final long delta = maxValue - minValue;
-    final int deltaBitsRequired = DirectWriter.unsignedBitsRequired(delta);
-    final int tableBitsRequired = uniqueValues == null
-        ? Integer.MAX_VALUE
-        : DirectWriter.bitsRequired(uniqueValues.size() - 1);
-
-    final int format;
-    if (uniqueValues != null && tableBitsRequired < deltaBitsRequired) {
-      format = TABLE_COMPRESSED;
-    } else if (gcd != 0 && gcd != 1) {
-      final long gcdDelta = (maxValue - minValue) / gcd;
-      final long gcdBitsRequired = DirectWriter.unsignedBitsRequired(gcdDelta);
-      format = gcdBitsRequired < deltaBitsRequired ? GCD_COMPRESSED : DELTA_COMPRESSED;
-    } else {
-      format = DELTA_COMPRESSED;
-    }
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.NUMERIC);
-    meta.writeVInt(format);
-    if (missing) {
-      meta.writeLong(data.getFilePointer());
-      writeMissingBitset(values);
-    } else {
-      meta.writeLong(-1L);
-    }
-    meta.writeLong(data.getFilePointer());
-    meta.writeVLong(count);
-
-    switch (format) {
-      case GCD_COMPRESSED:
-        meta.writeLong(minValue);
-        meta.writeLong(gcd);
-        final long maxDelta = (maxValue - minValue) / gcd;
-        final int bits = DirectWriter.unsignedBitsRequired(maxDelta);
-        meta.writeVInt(bits);
-        final DirectWriter quotientWriter = DirectWriter.getInstance(data, count, bits);
-        for (Number nv : values) {
-          long value = nv == null ? 0 : nv.longValue();
-          quotientWriter.add((value - minValue) / gcd);
-        }
-        quotientWriter.finish();
-        break;
-      case DELTA_COMPRESSED:
-        final long minDelta = delta < 0 ? 0 : minValue;
-        meta.writeLong(minDelta);
-        meta.writeVInt(deltaBitsRequired);
-        final DirectWriter writer = DirectWriter.getInstance(data, count, deltaBitsRequired);
-        for (Number nv : values) {
-          long v = nv == null ? 0 : nv.longValue();
-          writer.add(v - minDelta);
-        }
-        writer.finish();
-        break;
-      case TABLE_COMPRESSED:
-        final Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        Arrays.sort(decode);
-        final HashMap<Long,Integer> encode = new HashMap<>();
-        meta.writeVInt(decode.length);
-        for (int i = 0; i < decode.length; i++) {
-          meta.writeLong(decode[i]);
-          encode.put(decode[i], i);
-        }
-        meta.writeVInt(tableBitsRequired);
-        final DirectWriter ordsWriter = DirectWriter.getInstance(data, count, tableBitsRequired);
-        for (Number nv : values) {
-          ordsWriter.add(encode.get(nv == null ? 0 : nv.longValue()));
-        }
-        ordsWriter.finish();
-        break;
-      default:
-        throw new AssertionError();
-    }
-    meta.writeLong(data.getFilePointer());
-  }
-  
-  // TODO: in some cases representing missing with minValue-1 wouldn't take up additional space and so on,
-  // but this is very simple, and algorithms only check this for values of 0 anyway (doesnt slow down normal decode)
-  void writeMissingBitset(Iterable<?> values) throws IOException {
-    byte bits = 0;
-    int count = 0;
-    for (Object v : values) {
-      if (count == 8) {
-        data.writeByte(bits);
-        count = 0;
-        bits = 0;
-      }
-      if (v != null) {
-        bits |= 1 << (count & 7);
-      }
-      count++;
-    }
-    if (count > 0) {
-      data.writeByte(bits);
-    }
-  }
-
-  @Override
-  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
-    checkCanWrite(field);
-    // write the byte[] data
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.BINARY);
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    final long startFP = data.getFilePointer();
-    long count = 0;
-    boolean missing = false;
-    for(BytesRef v : values) {
-      final int length;
-      if (v == null) {
-        length = 0;
-        missing = true;
-      } else {
-        length = v.length;
-      }
-      minLength = Math.min(minLength, length);
-      maxLength = Math.max(maxLength, length);
-      if (v != null) {
-        data.writeBytes(v.bytes, v.offset, v.length);
-      }
-      count++;
-    }
-    meta.writeVInt(minLength == maxLength ? BINARY_FIXED_UNCOMPRESSED : BINARY_VARIABLE_UNCOMPRESSED);
-    if (missing) {
-      meta.writeLong(data.getFilePointer());
-      writeMissingBitset(values);
-    } else {
-      meta.writeLong(-1L);
-    }
-    meta.writeVInt(minLength);
-    meta.writeVInt(maxLength);
-    meta.writeVLong(count);
-    meta.writeLong(startFP);
-    
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
-    // otherwise, we need to record the length fields...
-    if (minLength != maxLength) {
-      meta.writeLong(data.getFilePointer());
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-
-      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-      long addr = 0;
-      writer.add(addr);
-      for (BytesRef v : values) {
-        if (v != null) {
-          addr += v.length;
-        }
-        writer.add(addr);
-      }
-      writer.finish();
-    }
-  }
-  
-  /** expert: writes a value dictionary for a sorted/sortedset field */
-  private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
-    // first check if its a "fixed-length" terms dict
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    for (BytesRef v : values) {
-      minLength = Math.min(minLength, v.length);
-      maxLength = Math.max(maxLength, v.length);
-    }
-    if (minLength == maxLength) {
-      // no index needed: direct addressing by mult
-      addBinaryField(field, values);
-    } else {
-      // header
-      meta.writeVInt(field.number);
-      meta.writeByte(Lucene49DocValuesFormat.BINARY);
-      meta.writeVInt(BINARY_PREFIX_COMPRESSED);
-      meta.writeLong(-1L);
-      // now write the bytes: sharing prefixes within a block
-      final long startFP = data.getFilePointer();
-      // currently, we have to store the delta from expected for every 1/nth term
-      // we could avoid this, but its not much and less overall RAM than the previous approach!
-      RAMOutputStream addressBuffer = new RAMOutputStream();
-      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);
-      BytesRefBuilder lastTerm = new BytesRefBuilder();
-      lastTerm.grow(Math.max(0, maxLength));
-      long count = 0;
-      for (BytesRef v : values) {
-        if (count % ADDRESS_INTERVAL == 0) {
-          termAddresses.add(data.getFilePointer() - startFP);
-          // force the first term in a block to be abs-encoded
-          lastTerm.clear();
-        }
-        
-        // prefix-code
-        int sharedPrefix = StringHelper.bytesDifference(lastTerm.get(), v);
-        data.writeVInt(sharedPrefix);
-        data.writeVInt(v.length - sharedPrefix);
-        data.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);
-        lastTerm.copyBytes(v);
-        count++;
-      }
-      final long indexStartFP = data.getFilePointer();
-      // write addresses of indexed terms
-      termAddresses.finish();
-      addressBuffer.writeTo(data);
-      addressBuffer = null;
-      termAddresses = null;
-      meta.writeVInt(minLength);
-      meta.writeVInt(maxLength);
-      meta.writeVLong(count);
-      meta.writeLong(startFP);
-      meta.writeVInt(ADDRESS_INTERVAL);
-      meta.writeLong(indexStartFP);
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-    }
-  }
-
-  @Override
-  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.SORTED);
-    addTermsDict(field, values);
-    addNumericField(field, docToOrd, false);
-  }
-
-  @Override
-  public void addSortedNumericField(FieldInfo field, final Iterable<Number> docToValueCount, final Iterable<Number> values) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.SORTED_NUMERIC);
-    if (isSingleValued(docToValueCount)) {
-      meta.writeVInt(SORTED_SINGLE_VALUED);
-      // The field is single-valued, we can encode it as NUMERIC
-      addNumericField(field, singletonView(docToValueCount, values, null));
-    } else {
-      meta.writeVInt(SORTED_WITH_ADDRESSES);
-      // write the stream of values as a numeric field
-      addNumericField(field, values, true);
-      // write the doc -> ord count as a absolute index to the stream
-      addAddresses(field, docToValueCount);
-    }
-  }
-
-  @Override
-  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
-    checkCanWrite(field);
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.SORTED_SET);
-
-    if (isSingleValued(docToOrdCount)) {
-      meta.writeVInt(SORTED_SINGLE_VALUED);
-      // The field is single-valued, we can encode it as SORTED
-      addSortedField(field, values, singletonView(docToOrdCount, ords, -1L));
-    } else {
-      meta.writeVInt(SORTED_WITH_ADDRESSES);
-
-      // write the ord -> byte[] as a binary field
-      addTermsDict(field, values);
-
-      // write the stream of ords as a numeric field
-      // NOTE: we could return an iterator that delta-encodes these within a doc
-      addNumericField(field, ords, false);
-
-      // write the doc -> ord count as a absolute index to the stream
-      addAddresses(field, docToOrdCount);
-    }
-  }
-  
-  // writes addressing information as MONOTONIC_COMPRESSED integer
-  private void addAddresses(FieldInfo field, Iterable<Number> values) throws IOException {
-    meta.writeVInt(field.number);
-    meta.writeByte(Lucene49DocValuesFormat.NUMERIC);
-    meta.writeVInt(MONOTONIC_COMPRESSED);
-    meta.writeLong(-1L);
-    meta.writeLong(data.getFilePointer());
-    meta.writeVLong(maxDoc);
-    meta.writeVInt(PackedInts.VERSION_CURRENT);
-    meta.writeVInt(BLOCK_SIZE);
-
-    final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-    long addr = 0;
-    writer.add(addr);
-    for (Number v : values) {
-      addr += v.longValue();
-      writer.add(addr);
-    }
-    writer.finish();
-    meta.writeLong(data.getFilePointer());
-  }
-
-  @Override
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1); // write EOF marker
-        CodecUtil.writeFooter(meta); // write checksum
-      }
-      if (data != null) {
-        CodecUtil.writeFooter(data); // write checksum
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-      meta = data = null;
-    }
-  }
-  
-  void checkCanWrite(FieldInfo field) {
-    if ((field.getDocValuesType() == DocValuesType.NUMERIC || 
-        field.getDocValuesType() == DocValuesType.BINARY) && 
-        field.getDocValuesGen() != -1) {
-      // ok
-    } else {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java
deleted file mode 100644
index 281c0f2..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesFormat.java
+++ /dev/null
@@ -1,195 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.SmallFloat;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.packed.DirectWriter;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
-
-/**
- * Lucene 4.9 DocValues format.
- * <p>
- * Encodes the five per-document value types (Numeric,Binary,Sorted,SortedSet,SortedNumeric) with these strategies:
- * <p>
- * {@link DocValuesType#NUMERIC NUMERIC}:
- * <ul>
- *    <li>Delta-compressed: per-document integers written as deltas from the minimum value,
- *        compressed with bitpacking. For more information, see {@link DirectWriter}.
- *    <li>Table-compressed: when the number of unique values is very small (&lt; 256), and
- *        when there are unused "gaps" in the range of values used (such as {@link SmallFloat}), 
- *        a lookup table is written instead. Each per-document entry is instead the ordinal 
- *        to this table, and those ordinals are compressed with bitpacking ({@link DirectWriter}). 
- *    <li>GCD-compressed: when all numbers share a common divisor, such as dates, the greatest
- *        common denominator (GCD) is computed, and quotients are stored using Delta-compressed Numerics.
- *    <li>Monotonic-compressed: when all numbers are monotonically increasing offsets, they are written
- *        as blocks of bitpacked integers, encoding the deviation from the expected delta.
- * </ul>
- * <p>
- * {@link DocValuesType#BINARY BINARY}:
- * <ul>
- *    <li>Fixed-width Binary: one large concatenated byte[] is written, along with the fixed length.
- *        Each document's value can be addressed directly with multiplication ({@code docID * length}). 
- *    <li>Variable-width Binary: one large concatenated byte[] is written, along with end addresses 
- *        for each document. The addresses are written as Monotonic-compressed numerics.
- *    <li>Prefix-compressed Binary: values are written in chunks of 16, with the first value written
- *        completely and other values sharing prefixes. chunk addresses are written as Monotonic-compressed
- *        numerics.
- * </ul>
- * <p>
- * {@link DocValuesType#SORTED SORTED}:
- * <ul>
- *    <li>Sorted: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
- *        along with the per-document ordinals written using one of the numeric strategies above.
- * </ul>
- * <p>
- * {@link DocValuesType#SORTED_SET SORTED_SET}:
- * <ul>
- *    <li>SortedSet: a mapping of ordinals to deduplicated terms is written as Prefix-Compressed Binary, 
- *        an ordinal list and per-document index into this list are written using the numeric strategies 
- *        above. 
- * </ul>
- * <p>
- * {@link DocValuesType#SORTED_NUMERIC SORTED_NUMERIC}:
- * <ul>
- *    <li>SortedNumeric: a value list and per-document index into this list are written using the numeric
- *        strategies above.
- * </ul>
- * <p>
- * Files:
- * <ol>
- *   <li><tt>.dvd</tt>: DocValues data</li>
- *   <li><tt>.dvm</tt>: DocValues metadata</li>
- * </ol>
- * <ol>
- *   <li><a name="dvm" id="dvm"></a>
- *   <p>The DocValues metadata or .dvm file.</p>
- *   <p>For DocValues field, this stores metadata, such as the offset into the 
- *      DocValues data (.dvd)</p>
- *   <p>DocValues metadata (.dvm) --&gt; Header,&lt;Entry&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>Entry --&gt; NumericEntry | BinaryEntry | SortedEntry | SortedSetEntry | SortedNumericEntry</li>
- *     <li>NumericEntry --&gt; GCDNumericEntry | TableNumericEntry | DeltaNumericEntry</li>
- *     <li>GCDNumericEntry --&gt; NumericHeader,MinValue,GCD,BitsPerValue</li>
- *     <li>TableNumericEntry --&gt; NumericHeader,TableSize,{@link DataOutput#writeLong Int64}<sup>TableSize</sup>,BitsPerValue</li>
- *     <li>DeltaNumericEntry --&gt; NumericHeader,MinValue,BitsPerValue</li>
- *     <li>MonotonicNumericEntry --&gt; NumericHeader,PackedVersion,BlockSize</li>
- *     <li>NumericHeader --&gt; FieldNumber,EntryType,NumericType,MissingOffset,DataOffset,Count,EndOffset</li>
- *     <li>BinaryEntry --&gt; FixedBinaryEntry | VariableBinaryEntry | PrefixBinaryEntry</li>
- *     <li>FixedBinaryEntry --&gt; BinaryHeader</li>
- *     <li>VariableBinaryEntry --&gt; BinaryHeader,AddressOffset,PackedVersion,BlockSize</li>
- *     <li>PrefixBinaryEntry --&gt; BinaryHeader,AddressInterval,AddressOffset,PackedVersion,BlockSize</li>
- *     <li>BinaryHeader --&gt; FieldNumber,EntryType,BinaryType,MissingOffset,MinLength,MaxLength,DataOffset</li>
- *     <li>SortedEntry --&gt; FieldNumber,EntryType,BinaryEntry,NumericEntry</li>
- *     <li>SortedSetEntry --&gt; EntryType,BinaryEntry,NumericEntry,NumericEntry</li>
- *     <li>SortedNumericEntry --&gt; EntryType,NumericEntry,NumericEntry</li>
- *     <li>FieldNumber,PackedVersion,MinLength,MaxLength,BlockSize,ValueCount --&gt; {@link DataOutput#writeVInt VInt}</li>
- *     <li>EntryType,CompressionType --&gt; {@link DataOutput#writeByte Byte}</li>
- *     <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *     <li>MinValue,GCD,MissingOffset,AddressOffset,DataOffset,EndOffset --&gt; {@link DataOutput#writeLong Int64}</li>
- *     <li>TableSize,BitsPerValue --&gt; {@link DataOutput#writeVInt vInt}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- *   <p>Sorted fields have two entries: a BinaryEntry with the value metadata,
- *      and an ordinary NumericEntry for the document-to-ord metadata.</p>
- *   <p>SortedSet fields have three entries: a BinaryEntry with the value metadata,
- *      and two NumericEntries for the document-to-ord-index and ordinal list metadata.</p>
- *   <p>SortedNumeric fields have two entries: A NumericEntry with the value metadata,
- *      and a numeric entry with the document-to-value index.</p>
- *   <p>FieldNumber of -1 indicates the end of metadata.</p>
- *   <p>EntryType is a 0 (NumericEntry) or 1 (BinaryEntry)</p>
- *   <p>DataOffset is the pointer to the start of the data in the DocValues data (.dvd)</p>
- *   <p>EndOffset is the pointer to the end of the data in the DocValues data (.dvd)</p>
- *   <p>NumericType indicates how Numeric values will be compressed:
- *      <ul>
- *         <li>0 --&gt; delta-compressed. For each block of 16k integers, every integer is delta-encoded
- *             from the minimum value within the block. 
- *         <li>1 --&gt, gcd-compressed. When all integers share a common divisor, only quotients are stored
- *             using blocks of delta-encoded ints.
- *         <li>2 --&gt; table-compressed. When the number of unique numeric values is small and it would save space,
- *             a lookup table of unique values is written, followed by the ordinal for each document.
- *      </ul>
- *   <p>BinaryType indicates how Binary values will be stored:
- *      <ul>
- *         <li>0 --&gt; fixed-width. All values have the same length, addressing by multiplication. 
- *         <li>1 --&gt, variable-width. An address for each value is stored.
- *         <li>2 --&gt; prefix-compressed. An address to the start of every interval'th value is stored.
- *      </ul>
- *   <p>MinLength and MaxLength represent the min and max byte[] value lengths for Binary values.
- *      If they are equal, then all values are of a fixed size, and can be addressed as DataOffset + (docID * length).
- *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
- *      is written for the addresses.
- *   <p>MissingOffset points to a byte[] containing a bitset of all documents that had a value for the field.
- *      If its -1, then there are no missing values.
- *   <p>Checksum contains the CRC32 checksum of all bytes in the .dvm file up
- *      until the checksum. This is used to verify integrity of the file on opening the
- *      index.
- *   <li><a name="dvd" id="dvd"></a>
- *   <p>The DocValues data or .dvd file.</p>
- *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
- *   <p>DocValues data (.dvd) --&gt; Header,&lt;NumericData | BinaryData | SortedData&gt;<sup>NumFields</sup>,Footer</p>
- *   <ul>
- *     <li>NumericData --&gt; DeltaCompressedNumerics | TableCompressedNumerics | GCDCompressedNumerics</li>
- *     <li>BinaryData --&gt;  {@link DataOutput#writeByte Byte}<sup>DataLength</sup>,Addresses</li>
- *     <li>SortedData --&gt; {@link FST FST&lt;Int64&gt;}</li>
- *     <li>DeltaCompressedNumerics,TableCompressedNumerics,GCDCompressedNumerics --&gt; {@link DirectWriter PackedInts}</li>
- *     <li>Addresses --&gt; {@link MonotonicBlockPackedWriter MonotonicBlockPackedInts(blockSize=16k)}</li>
- *     <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
- *   </ul>
- * </ol>
- * @lucene.experimental
- */
-public class Lucene49DocValuesFormat extends DocValuesFormat {
-
-  /** Sole Constructor */
-  public Lucene49DocValuesFormat() {
-    super("Lucene49");
-  }
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new Lucene49DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
-  }
-
-  @Override
-  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new Lucene49DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION);
-  }
-  
-  static final String DATA_CODEC = "Lucene49DocValuesData";
-  static final String DATA_EXTENSION = "dvd";
-  static final String META_CODEC = "Lucene49ValuesMetadata";
-  static final String META_EXTENSION = "dvm";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-  static final byte NUMERIC = 0;
-  static final byte BINARY = 1;
-  static final byte SORTED = 2;
-  static final byte SORTED_SET = 3;
-  static final byte SORTED_NUMERIC = 4;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java
deleted file mode 100644
index 70b8dd8..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene49/Lucene49DocValuesProducer.java
+++ /dev/null
@@ -1,946 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.BINARY_FIXED_UNCOMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.BINARY_PREFIX_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.BINARY_VARIABLE_UNCOMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.DELTA_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.GCD_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.MONOTONIC_COMPRESSED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.SORTED_SINGLE_VALUED;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.SORTED_WITH_ADDRESSES;
-import static org.apache.lucene.codecs.lucene49.Lucene49DocValuesConsumer.TABLE_COMPRESSED;
-
-import java.io.Closeable; // javadocs
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.RandomAccessOrds;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.SortedNumericDocValues;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.RandomAccessInput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LongValues;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.DirectReader;
-import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
-
-/** reader for {@link Lucene49DocValuesFormat} */
-class Lucene49DocValuesProducer extends DocValuesProducer implements Closeable {
-  private final Map<Integer,NumericEntry> numerics;
-  private final Map<Integer,BinaryEntry> binaries;
-  private final Map<Integer,SortedSetEntry> sortedSets;
-  private final Map<Integer,SortedSetEntry> sortedNumerics;
-  private final Map<Integer,NumericEntry> ords;
-  private final Map<Integer,NumericEntry> ordIndexes;
-  private final AtomicLong ramBytesUsed;
-  private final IndexInput data;
-  private final int maxDoc;
-  private final int version;
-
-  // memory-resident structures
-  private final Map<Integer,MonotonicBlockPackedReader> addressInstances = new HashMap<>();
-  private final Map<Integer,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<>();
-  
-  /** expert: instantiates a new reader */
-  Lucene49DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-    // read in the entries from the metadata file.
-    ChecksumIndexInput in = state.directory.openChecksumInput(metaName, state.context);
-    this.maxDoc = state.segmentInfo.getDocCount();
-    boolean success = false;
-    try {
-      version = CodecUtil.checkHeader(in, metaCodec, 
-                                      Lucene49DocValuesFormat.VERSION_START,
-                                      Lucene49DocValuesFormat.VERSION_CURRENT);
-      numerics = new HashMap<>();
-      ords = new HashMap<>();
-      ordIndexes = new HashMap<>();
-      binaries = new HashMap<>();
-      sortedSets = new HashMap<>();
-      sortedNumerics = new HashMap<>();
-      readFields(in, state.fieldInfos);
-
-      CodecUtil.checkFooter(in);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(in);
-      } else {
-        IOUtils.closeWhileHandlingException(in);
-      }
-    }
-
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-    this.data = state.directory.openInput(dataName, state.context);
-    success = false;
-    try {
-      final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 Lucene49DocValuesFormat.VERSION_START,
-                                                 Lucene49DocValuesFormat.VERSION_CURRENT);
-      if (version != version2) {
-        throw new CorruptIndexException("Format versions mismatch");
-      }
-      
-      // NOTE: data file is too costly to verify checksum against all the bytes on open,
-      // but for now we at least verify proper structure of the checksum footer: which looks
-      // for FOOTER_MAGIC + algorithmID. This is cheap and can detect some forms of corruption
-      // such as file truncation.
-      CodecUtil.retrieveChecksum(data);
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this.data);
-      }
-    }
-    
-    ramBytesUsed = new AtomicLong(RamUsageEstimator.shallowSizeOfInstance(getClass()));
-  }
-
-  private void readSortedField(int fieldNumber, IndexInput meta, FieldInfos infos) throws IOException {
-    // sorted = binary + numeric
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    if (meta.readByte() != Lucene49DocValuesFormat.BINARY) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    BinaryEntry b = readBinaryEntry(meta);
-    binaries.put(fieldNumber, b);
-    
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sorted entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    NumericEntry n = readNumericEntry(meta);
-    ords.put(fieldNumber, n);
-  }
-
-  private void readSortedSetFieldWithAddresses(int fieldNumber, IndexInput meta, FieldInfos infos) throws IOException {
-    // sortedset = binary + numeric (addresses) + ordIndex
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    if (meta.readByte() != Lucene49DocValuesFormat.BINARY) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    BinaryEntry b = readBinaryEntry(meta);
-    binaries.put(fieldNumber, b);
-
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    NumericEntry n1 = readNumericEntry(meta);
-    ords.put(fieldNumber, n1);
-
-    if (meta.readVInt() != fieldNumber) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
-      throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-    }
-    NumericEntry n2 = readNumericEntry(meta);
-    ordIndexes.put(fieldNumber, n2);
-  }
-
-  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
-    int fieldNumber = meta.readVInt();
-    while (fieldNumber != -1) {
-      if (infos.fieldInfo(fieldNumber) == null) {
-        // trickier to validate more: because we re-use for norms, because we use multiple entries
-        // for "composite" types like sortedset, etc.
-        throw new CorruptIndexException("Invalid field number: " + fieldNumber + " (resource=" + meta + ")");
-      }
-      byte type = meta.readByte();
-      if (type == Lucene49DocValuesFormat.NUMERIC) {
-        numerics.put(fieldNumber, readNumericEntry(meta));
-      } else if (type == Lucene49DocValuesFormat.BINARY) {
-        BinaryEntry b = readBinaryEntry(meta);
-        binaries.put(fieldNumber, b);
-      } else if (type == Lucene49DocValuesFormat.SORTED) {
-        readSortedField(fieldNumber, meta, infos);
-      } else if (type == Lucene49DocValuesFormat.SORTED_SET) {
-        SortedSetEntry ss = readSortedSetEntry(meta);
-        sortedSets.put(fieldNumber, ss);
-        if (ss.format == SORTED_WITH_ADDRESSES) {
-          readSortedSetFieldWithAddresses(fieldNumber, meta, infos);
-        } else if (ss.format == SORTED_SINGLE_VALUED) {
-          if (meta.readVInt() != fieldNumber) {
-            throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-          }
-          if (meta.readByte() != Lucene49DocValuesFormat.SORTED) {
-            throw new CorruptIndexException("sortedset entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-          }
-          readSortedField(fieldNumber, meta, infos);
-        } else {
-          throw new AssertionError();
-        }
-      } else if (type == Lucene49DocValuesFormat.SORTED_NUMERIC) {
-        SortedSetEntry ss = readSortedSetEntry(meta);
-        sortedNumerics.put(fieldNumber, ss);
-        if (meta.readVInt() != fieldNumber) {
-          throw new CorruptIndexException("sortednumeric entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-        }
-        if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
-          throw new CorruptIndexException("sortednumeric entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-        }
-        numerics.put(fieldNumber, readNumericEntry(meta));
-        if (ss.format == SORTED_WITH_ADDRESSES) {
-          if (meta.readVInt() != fieldNumber) {
-            throw new CorruptIndexException("sortednumeric entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-          }
-          if (meta.readByte() != Lucene49DocValuesFormat.NUMERIC) {
-            throw new CorruptIndexException("sortednumeric entry for field: " + fieldNumber + " is corrupt (resource=" + meta + ")");
-          }
-          NumericEntry ordIndex = readNumericEntry(meta);
-          ordIndexes.put(fieldNumber, ordIndex);
-        } else if (ss.format != SORTED_SINGLE_VALUED) {
-          throw new AssertionError();
-        }
-      } else {
-        throw new CorruptIndexException("invalid type: " + type + ", resource=" + meta);
-      }
-      fieldNumber = meta.readVInt();
-    }
-  }
-  
-  static NumericEntry readNumericEntry(IndexInput meta) throws IOException {
-    NumericEntry entry = new NumericEntry();
-    entry.format = meta.readVInt();
-    entry.missingOffset = meta.readLong();
-    entry.offset = meta.readLong();
-    entry.count = meta.readVLong();
-    switch(entry.format) {
-      case GCD_COMPRESSED:
-        entry.minValue = meta.readLong();
-        entry.gcd = meta.readLong();
-        entry.bitsPerValue = meta.readVInt();
-        break;
-      case TABLE_COMPRESSED:
-        final int uniqueValues = meta.readVInt();
-        if (uniqueValues > 256) {
-          throw new CorruptIndexException("TABLE_COMPRESSED cannot have more than 256 distinct values, input=" + meta);
-        }
-        entry.table = new long[uniqueValues];
-        for (int i = 0; i < uniqueValues; ++i) {
-          entry.table[i] = meta.readLong();
-        }
-        entry.bitsPerValue = meta.readVInt();
-        break;
-      case DELTA_COMPRESSED:
-        entry.minValue = meta.readLong();
-        entry.bitsPerValue = meta.readVInt();
-        break;
-      case MONOTONIC_COMPRESSED:
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      default:
-        throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
-    }
-    entry.endOffset = meta.readLong();
-    return entry;
-  }
-  
-  static BinaryEntry readBinaryEntry(IndexInput meta) throws IOException {
-    BinaryEntry entry = new BinaryEntry();
-    entry.format = meta.readVInt();
-    entry.missingOffset = meta.readLong();
-    entry.minLength = meta.readVInt();
-    entry.maxLength = meta.readVInt();
-    entry.count = meta.readVLong();
-    entry.offset = meta.readLong();
-    switch(entry.format) {
-      case BINARY_FIXED_UNCOMPRESSED:
-        break;
-      case BINARY_PREFIX_COMPRESSED:
-        entry.addressInterval = meta.readVInt();
-        entry.addressesOffset = meta.readLong();
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      case BINARY_VARIABLE_UNCOMPRESSED:
-        entry.addressesOffset = meta.readLong();
-        entry.packedIntsVersion = meta.readVInt();
-        entry.blockSize = meta.readVInt();
-        break;
-      default:
-        throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
-    }
-    return entry;
-  }
-
-  SortedSetEntry readSortedSetEntry(IndexInput meta) throws IOException {
-    SortedSetEntry entry = new SortedSetEntry();
-    entry.format = meta.readVInt();
-    if (entry.format != SORTED_SINGLE_VALUED && entry.format != SORTED_WITH_ADDRESSES) {
-      throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
-    }
-    return entry;
-  }
-
-  @Override
-  public NumericDocValues getNumeric(FieldInfo field) throws IOException {
-    NumericEntry entry = numerics.get(field.number);
-    return getNumeric(entry);
-  }
-  
-  @Override
-  public long ramBytesUsed() {
-    return ramBytesUsed.get();
-  }
-  
-  @Override
-  public void checkIntegrity() throws IOException {
-    CodecUtil.checksumEntireFile(data);
-  }
-
-  LongValues getNumeric(NumericEntry entry) throws IOException {
-    RandomAccessInput slice = this.data.randomAccessSlice(entry.offset, entry.endOffset - entry.offset);
-    switch (entry.format) {
-      case DELTA_COMPRESSED:
-        final long delta = entry.minValue;
-        final LongValues values = DirectReader.getInstance(slice, entry.bitsPerValue);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return delta + values.get(id);
-          }
-        };
-      case GCD_COMPRESSED:
-        final long min = entry.minValue;
-        final long mult = entry.gcd;
-        final LongValues quotientReader = DirectReader.getInstance(slice, entry.bitsPerValue);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return min + mult * quotientReader.get(id);
-          }
-        };
-      case TABLE_COMPRESSED:
-        final long table[] = entry.table;
-        final LongValues ords = DirectReader.getInstance(slice, entry.bitsPerValue);
-        return new LongValues() {
-          @Override
-          public long get(long id) {
-            return table[(int) ords.get(id)];
-          }
-        };
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public BinaryDocValues getBinary(FieldInfo field) throws IOException {
-    BinaryEntry bytes = binaries.get(field.number);
-    switch(bytes.format) {
-      case BINARY_FIXED_UNCOMPRESSED:
-        return getFixedBinary(field, bytes);
-      case BINARY_VARIABLE_UNCOMPRESSED:
-        return getVariableBinary(field, bytes);
-      case BINARY_PREFIX_COMPRESSED:
-        return getCompressedBinary(field, bytes);
-      default:
-        throw new AssertionError();
-    }
-  }
-  
-  private BinaryDocValues getFixedBinary(FieldInfo field, final BinaryEntry bytes) {
-    final IndexInput data = this.data.clone();
-
-    return new LongBinaryDocValues() {
-      final BytesRef term;
-      {
-        term = new BytesRef(bytes.maxLength);
-        term.offset = 0;
-        term.length = bytes.maxLength;
-      }
-      
-      @Override
-      public BytesRef get(long id) {
-        long address = bytes.offset + id * bytes.maxLength;
-        try {
-          data.seek(address);
-          data.readBytes(term.bytes, 0, term.length);
-          return term;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for variable-length binary values. */
-  private MonotonicBlockPackedReader getAddressInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
-    final MonotonicBlockPackedReader addresses;
-    synchronized (addressInstances) {
-      MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
-      if (addrInstance == null) {
-        data.seek(bytes.addressesOffset);
-        addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, bytes.count+1, false);
-        addressInstances.put(field.number, addrInstance);
-        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-      addresses = addrInstance;
-    }
-    return addresses;
-  }
-  
-  private BinaryDocValues getVariableBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
-    final IndexInput data = this.data.clone();
-    
-    final MonotonicBlockPackedReader addresses = getAddressInstance(data, field, bytes);
-
-    return new LongBinaryDocValues() {
-      final BytesRef term = new BytesRef(Math.max(0, bytes.maxLength));
-      
-      @Override
-      public BytesRef get(long id) {
-        long startAddress = bytes.offset + addresses.get(id);
-        long endAddress = bytes.offset + addresses.get(id+1);
-        int length = (int) (endAddress - startAddress);
-        try {
-          data.seek(startAddress);
-          data.readBytes(term.bytes, 0, length);
-          term.length = length;
-          return term;
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for prefix-compressed binary values. */
-  private MonotonicBlockPackedReader getIntervalInstance(IndexInput data, FieldInfo field, BinaryEntry bytes) throws IOException {
-    final MonotonicBlockPackedReader addresses;
-    final long interval = bytes.addressInterval;
-    synchronized (addressInstances) {
-      MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
-      if (addrInstance == null) {
-        data.seek(bytes.addressesOffset);
-        final long size;
-        if (bytes.count % interval == 0) {
-          size = bytes.count / interval;
-        } else {
-          size = 1L + bytes.count / interval;
-        }
-        addrInstance = MonotonicBlockPackedReader.of(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
-        addressInstances.put(field.number, addrInstance);
-        ramBytesUsed.addAndGet(addrInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-      addresses = addrInstance;
-    }
-    return addresses;
-  }
-
-
-  private BinaryDocValues getCompressedBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
-    final IndexInput data = this.data.clone();
-
-    final MonotonicBlockPackedReader addresses = getIntervalInstance(data, field, bytes);
-    
-    return new CompressedBinaryDocValues(bytes, addresses, data);
-  }
-
-  @Override
-  public SortedDocValues getSorted(FieldInfo field) throws IOException {
-    final int valueCount = (int) binaries.get(field.number).count;
-    final BinaryDocValues binary = getBinary(field);
-    NumericEntry entry = ords.get(field.number);
-    final LongValues ordinals = getNumeric(entry);
-    
-    return new SortedDocValues() {
-
-      @Override
-      public int getOrd(int docID) {
-        return (int) ordinals.get(docID);
-      }
-
-      @Override
-      public BytesRef lookupOrd(int ord) {
-        return binary.get(ord);
-      }
-
-      @Override
-      public int getValueCount() {
-        return valueCount;
-      }
-
-      @Override
-      public int lookupTerm(BytesRef key) {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return (int) ((CompressedBinaryDocValues)binary).lookupTerm(key);
-        } else {
-        return super.lookupTerm(key);
-        }
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).getTermsEnum();
-        } else {
-          return super.termsEnum();
-        }
-      }
-    };
-  }
-  
-  /** returns an address instance for sortedset ordinal lists */
-  private MonotonicBlockPackedReader getOrdIndexInstance(IndexInput data, FieldInfo field, NumericEntry entry) throws IOException {
-    final MonotonicBlockPackedReader ordIndex;
-    synchronized (ordIndexInstances) {
-      MonotonicBlockPackedReader ordIndexInstance = ordIndexInstances.get(field.number);
-      if (ordIndexInstance == null) {
-        data.seek(entry.offset);
-        ordIndexInstance = MonotonicBlockPackedReader.of(data, entry.packedIntsVersion, entry.blockSize, entry.count+1, false);
-        ordIndexInstances.put(field.number, ordIndexInstance);
-        ramBytesUsed.addAndGet(ordIndexInstance.ramBytesUsed() + RamUsageEstimator.NUM_BYTES_INT);
-      }
-      ordIndex = ordIndexInstance;
-    }
-    return ordIndex;
-  }
-  
-  @Override
-  public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {
-    SortedSetEntry ss = sortedNumerics.get(field.number);
-    NumericEntry numericEntry = numerics.get(field.number);
-    final LongValues values = getNumeric(numericEntry);
-    if (ss.format == SORTED_SINGLE_VALUED) {
-      final Bits docsWithField = getMissingBits(numericEntry.missingOffset);
-      return DocValues.singleton(values, docsWithField);
-    } else if (ss.format == SORTED_WITH_ADDRESSES) {
-      final IndexInput data = this.data.clone();
-      final MonotonicBlockPackedReader ordIndex = getOrdIndexInstance(data, field, ordIndexes.get(field.number));
-      
-      return new SortedNumericDocValues() {
-        long startOffset;
-        long endOffset;
-        
-        @Override
-        public void setDocument(int doc) {
-          startOffset = ordIndex.get(doc);
-          endOffset = ordIndex.get(doc+1L);
-        }
-
-        @Override
-        public long valueAt(int index) {
-          return values.get(startOffset + index);
-        }
-
-        @Override
-        public int count() {
-          return (int) (endOffset - startOffset);
-        }
-      };
-    } else {
-      throw new AssertionError();
-    }
-  }
-
-  @Override
-  public SortedSetDocValues getSortedSet(FieldInfo field) throws IOException {
-    SortedSetEntry ss = sortedSets.get(field.number);
-    if (ss.format == SORTED_SINGLE_VALUED) {
-      final SortedDocValues values = getSorted(field);
-      return DocValues.singleton(values);
-    } else if (ss.format != SORTED_WITH_ADDRESSES) {
-      throw new AssertionError();
-    }
-
-    final IndexInput data = this.data.clone();
-    final long valueCount = binaries.get(field.number).count;
-    // we keep the byte[]s and list of ords on disk, these could be large
-    final LongBinaryDocValues binary = (LongBinaryDocValues) getBinary(field);
-    final LongValues ordinals = getNumeric(ords.get(field.number));
-    // but the addresses to the ord stream are in RAM
-    final MonotonicBlockPackedReader ordIndex = getOrdIndexInstance(data, field, ordIndexes.get(field.number));
-    
-    return new RandomAccessOrds() {
-      long startOffset;
-      long offset;
-      long endOffset;
-      
-      @Override
-      public long nextOrd() {
-        if (offset == endOffset) {
-          return NO_MORE_ORDS;
-        } else {
-          long ord = ordinals.get(offset);
-          offset++;
-          return ord;
-        }
-      }
-
-      @Override
-      public void setDocument(int docID) {
-        startOffset = offset = ordIndex.get(docID);
-        endOffset = ordIndex.get(docID+1L);
-      }
-
-      @Override
-      public BytesRef lookupOrd(long ord) {
-        return binary.get(ord);
-      }
-
-      @Override
-      public long getValueCount() {
-        return valueCount;
-      }
-      
-      @Override
-      public long lookupTerm(BytesRef key) {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).lookupTerm(key);
-        } else {
-          return super.lookupTerm(key);
-        }
-      }
-
-      @Override
-      public TermsEnum termsEnum() {
-        if (binary instanceof CompressedBinaryDocValues) {
-          return ((CompressedBinaryDocValues)binary).getTermsEnum();
-        } else {
-          return super.termsEnum();
-        }
-      }
-
-      @Override
-      public long ordAt(int index) {
-        return ordinals.get(startOffset + index);
-      }
-
-      @Override
-      public int cardinality() {
-        return (int) (endOffset - startOffset);
-      }
-    };
-  }
-  
-  private Bits getMissingBits(final long offset) throws IOException {
-    if (offset == -1) {
-      return new Bits.MatchAllBits(maxDoc);
-    } else {
-      int length = (int) ((maxDoc + 7L) >>> 3);
-      final RandomAccessInput in = data.randomAccessSlice(offset, length);
-      return new Bits() {
-        @Override
-        public boolean get(int index) {
-          try {
-            return (in.readByte(index >> 3) & (1 << (index & 7))) != 0;
-          } catch (IOException e) {
-            throw new RuntimeException(e);
-          }
-        }
-
-        @Override
-        public int length() {
-          return maxDoc;
-        }
-      };
-    }
-  }
-
-  @Override
-  public Bits getDocsWithField(FieldInfo field) throws IOException {
-    switch(field.getDocValuesType()) {
-      case SORTED_SET:
-        return DocValues.docsWithValue(getSortedSet(field), maxDoc);
-      case SORTED_NUMERIC:
-        return DocValues.docsWithValue(getSortedNumeric(field), maxDoc);
-      case SORTED:
-        return DocValues.docsWithValue(getSorted(field), maxDoc);
-      case BINARY:
-        BinaryEntry be = binaries.get(field.number);
-        return getMissingBits(be.missingOffset);
-      case NUMERIC:
-        NumericEntry ne = numerics.get(field.number);
-        return getMissingBits(ne.missingOffset);
-      default:
-        throw new AssertionError();
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    data.close();
-  }
-  
-  /** metadata entry for a numeric docvalues field */
-  static class NumericEntry {
-    private NumericEntry() {}
-    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
-    long missingOffset;
-    /** offset to the actual numeric values */
-    public long offset;
-    /** end offset to the actual numeric values */
-    public long endOffset;
-    /** bits per value used to pack the numeric values */
-    public int bitsPerValue;
-
-    int format;
-    /** packed ints version used to encode these numerics */
-    public int packedIntsVersion;
-    /** count of values written */
-    public long count;
-    /** packed ints blocksize */
-    public int blockSize;
-    
-    long minValue;
-    long gcd;
-    long table[];
-  }
-  
-  /** metadata entry for a binary docvalues field */
-  static class BinaryEntry {
-    private BinaryEntry() {}
-    /** offset to the bitset representing docsWithField, or -1 if no documents have missing values */
-    long missingOffset;
-    /** offset to the actual binary values */
-    long offset;
-
-    int format;
-    /** count of values written */
-    public long count;
-    int minLength;
-    int maxLength;
-    /** offset to the addressing data that maps a value to its slice of the byte[] */
-    public long addressesOffset;
-    /** interval of shared prefix chunks (when using prefix-compressed binary) */
-    public long addressInterval;
-    /** packed ints version used to encode addressing information */
-    public int packedIntsVersion;
-    /** packed ints blocksize */
-    public int blockSize;
-  }
-
-  /** metadata entry for a sorted-set docvalues field */
-  static class SortedSetEntry {
-    private SortedSetEntry() {}
-    int format;
-  }
-
-  // internally we compose complex dv (sorted/sortedset) from other ones
-  static abstract class LongBinaryDocValues extends BinaryDocValues {
-    @Override
-    public final BytesRef get(int docID) {
-      return get((long)docID);
-    }
-    
-    abstract BytesRef get(long id);
-  }
-  
-  // in the compressed case, we add a few additional operations for
-  // more efficient reverse lookup and enumeration
-  static class CompressedBinaryDocValues extends LongBinaryDocValues {
-    final BinaryEntry bytes;
-    final long interval;
-    final long numValues;
-    final long numIndexValues;
-    final MonotonicBlockPackedReader addresses;
-    final IndexInput data;
-    final TermsEnum termsEnum;
-    
-    public CompressedBinaryDocValues(BinaryEntry bytes, MonotonicBlockPackedReader addresses, IndexInput data) throws IOException {
-      this.bytes = bytes;
-      this.interval = bytes.addressInterval;
-      this.addresses = addresses;
-      this.data = data;
-      this.numValues = bytes.count;
-      this.numIndexValues = addresses.size();
-      this.termsEnum = getTermsEnum(data);
-    }
-    
-    @Override
-    public BytesRef get(long id) {
-      try {
-        termsEnum.seekExact(id);
-        return termsEnum.term();
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    long lookupTerm(BytesRef key) {
-      try {
-        SeekStatus status = termsEnum.seekCeil(key);
-        if (status == SeekStatus.END) {
-          return -numValues-1;
-        } else if (status == SeekStatus.FOUND) {
-          return termsEnum.ord();
-        } else {
-          return -termsEnum.ord()-1;
-        }
-      } catch (IOException bogus) {
-        throw new RuntimeException(bogus);
-      }
-    }
-    
-    TermsEnum getTermsEnum() {
-      try {
-        return getTermsEnum(data.clone());
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    private TermsEnum getTermsEnum(final IndexInput input) throws IOException {
-      input.seek(bytes.offset);
-      
-      return new TermsEnum() {
-        private long currentOrd = -1;
-        // TODO: maxLength is negative when all terms are merged away...
-        private final BytesRef term = new BytesRef(bytes.maxLength < 0 ? 0 : bytes.maxLength);
-
-        @Override
-        public BytesRef next() throws IOException {
-          if (++currentOrd >= numValues) {
-            return null;
-          } else {
-            int start = input.readVInt();
-            int suffix = input.readVInt();
-            input.readBytes(term.bytes, start, suffix);
-            term.length = start + suffix;
-            return term;
-          }
-        }
-
-        @Override
-        public SeekStatus seekCeil(BytesRef text) throws IOException {
-          // binary-search just the index values to find the block,
-          // then scan within the block
-          long low = 0;
-          long high = numIndexValues-1;
-
-          while (low <= high) {
-            long mid = (low + high) >>> 1;
-            seekExact(mid * interval);
-            int cmp = term.compareTo(text);
-
-            if (cmp < 0) {
-              low = mid + 1;
-            } else if (cmp > 0) {
-              high = mid - 1;
-            } else {
-              // we got lucky, found an indexed term
-              return SeekStatus.FOUND;
-            }
-          }
-          
-          if (numIndexValues == 0) {
-            return SeekStatus.END;
-          }
-          
-          // block before insertion point
-          long block = low-1;
-          seekExact(block < 0 ? -1 : block * interval);
-          
-          while (next() != null) {
-            int cmp = term.compareTo(text);
-            if (cmp == 0) {
-              return SeekStatus.FOUND;
-            } else if (cmp > 0) {
-              return SeekStatus.NOT_FOUND;
-            }
-          }
-          
-          return SeekStatus.END;
-        }
-
-        @Override
-        public void seekExact(long ord) throws IOException {
-          long block = ord / interval;
-
-          if (ord >= currentOrd && block == currentOrd / interval) {
-            // seek within current block
-          } else {
-            // position before start of block
-            currentOrd = ord - ord % interval - 1;
-            input.seek(bytes.offset + addresses.get(block));
-          }
-          
-          while (currentOrd < ord) {
-            next();
-          }
-        }
-
-        @Override
-        public BytesRef term() throws IOException {
-          return term;
-        }
-
-        @Override
-        public long ord() throws IOException {
-          return currentOrd;
-        }
-
-        @Override
-        public int docFreq() throws IOException {
-          throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public long totalTermFreq() throws IOException {
-          return -1;
-        }
-
-        @Override
-        public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-          throw new UnsupportedOperationException();
-        }
-
-        @Override
-        public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-          throw new UnsupportedOperationException();
-        }
-      };
-    }
-  }
-}
diff --git a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
index 890fffa..11a1ea9 100644
--- a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
+++ b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
@@ -13,10 +13,4 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-org.apache.lucene.codecs.lucene40.Lucene40Codec
-org.apache.lucene.codecs.lucene41.Lucene41Codec
-org.apache.lucene.codecs.lucene42.Lucene42Codec
-org.apache.lucene.codecs.lucene45.Lucene45Codec
-org.apache.lucene.codecs.lucene46.Lucene46Codec
-org.apache.lucene.codecs.lucene49.Lucene49Codec
 org.apache.lucene.codecs.lucene410.Lucene410Codec
diff --git a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
index 0dbc7e8..8cc6f70 100644
--- a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
+++ b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
@@ -13,7 +13,4 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-org.apache.lucene.codecs.lucene42.Lucene42DocValuesFormat
-org.apache.lucene.codecs.lucene45.Lucene45DocValuesFormat
-org.apache.lucene.codecs.lucene49.Lucene49DocValuesFormat
 org.apache.lucene.codecs.lucene410.Lucene410DocValuesFormat
diff --git a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
index 023d9c9..95e9267 100644
--- a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
+++ b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -13,5 +13,4 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat
 org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java
deleted file mode 100644
index 7e09e86..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestBitVector.java
+++ /dev/null
@@ -1,283 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.TestUtil;
-
-/**
- * <code>TestBitVector</code> tests the <code>BitVector</code>, obviously.
- */
-public class TestBitVector extends LuceneTestCase
-{
-
-    /**
-     * Test the default constructor on BitVectors of various sizes.
-     */
-    public void testConstructSize() throws Exception {
-        doTestConstructOfSize(8);
-        doTestConstructOfSize(20);
-        doTestConstructOfSize(100);
-        doTestConstructOfSize(1000);
-    }
-
-    private void doTestConstructOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        assertEquals(n,bv.size());
-    }
-
-    /**
-     * Test the get() and set() methods on BitVectors of various sizes.
-     */
-    public void testGetSet() throws Exception {
-        doTestGetSetVectorOfSize(8);
-        doTestGetSetVectorOfSize(20);
-        doTestGetSetVectorOfSize(100);
-        doTestGetSetVectorOfSize(1000);
-    }
-
-    private void doTestGetSetVectorOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        for(int i=0;i<bv.size();i++) {
-            // ensure a set bit can be git'
-            assertFalse(bv.get(i));
-            bv.set(i);
-            assertTrue(bv.get(i));
-        }
-    }
-
-    /**
-     * Test the clear() method on BitVectors of various sizes.
-     */
-    public void testClear() throws Exception {
-        doTestClearVectorOfSize(8);
-        doTestClearVectorOfSize(20);
-        doTestClearVectorOfSize(100);
-        doTestClearVectorOfSize(1000);
-    }
-
-    private void doTestClearVectorOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        for(int i=0;i<bv.size();i++) {
-            // ensure a set bit is cleared
-            assertFalse(bv.get(i));
-            bv.set(i);
-            assertTrue(bv.get(i));
-            bv.clear(i);
-            assertFalse(bv.get(i));
-        }
-    }
-
-    /**
-     * Test the count() method on BitVectors of various sizes.
-     */
-    public void testCount() throws Exception {
-        doTestCountVectorOfSize(8);
-        doTestCountVectorOfSize(20);
-        doTestCountVectorOfSize(100);
-        doTestCountVectorOfSize(1000);
-    }
-
-    private void doTestCountVectorOfSize(int n) {
-        BitVector bv = new BitVector(n);
-        // test count when incrementally setting bits
-        for(int i=0;i<bv.size();i++) {
-            assertFalse(bv.get(i));
-            assertEquals(i,bv.count());
-            bv.set(i);
-            assertTrue(bv.get(i));
-            assertEquals(i+1,bv.count());
-        }
-
-        bv = new BitVector(n);
-        // test count when setting then clearing bits
-        for(int i=0;i<bv.size();i++) {
-            assertFalse(bv.get(i));
-            assertEquals(0,bv.count());
-            bv.set(i);
-            assertTrue(bv.get(i));
-            assertEquals(1,bv.count());
-            bv.clear(i);
-            assertFalse(bv.get(i));
-            assertEquals(0,bv.count());
-        }
-    }
-
-    /**
-     * Test writing and construction to/from Directory.
-     */
-    public void testWriteRead() throws Exception {
-        doTestWriteRead(8);
-        doTestWriteRead(20);
-        doTestWriteRead(100);
-        doTestWriteRead(1000);
-    }
-
-    private void doTestWriteRead(int n) throws Exception {
-        MockDirectoryWrapper d = new  MockDirectoryWrapper(random(), new RAMDirectory());
-        d.setPreventDoubleWrite(false);
-        BitVector bv = new BitVector(n);
-        // test count when incrementally setting bits
-        for(int i=0;i<bv.size();i++) {
-            assertFalse(bv.get(i));
-            assertEquals(i,bv.count());
-            bv.set(i);
-            assertTrue(bv.get(i));
-            assertEquals(i+1,bv.count());
-            bv.write(d, "TESTBV", newIOContext(random()));
-            BitVector compare = new BitVector(d, "TESTBV", newIOContext(random()));
-            // compare bit vectors with bits set incrementally
-            assertTrue(doCompare(bv,compare));
-        }
-    }
-    
-    /**
-     * Test r/w when size/count cause switching between bit-set and d-gaps file formats.  
-     */
-    public void testDgaps() throws IOException {
-      doTestDgaps(1,0,1);
-      doTestDgaps(10,0,1);
-      doTestDgaps(100,0,1);
-      doTestDgaps(1000,4,7);
-      doTestDgaps(10000,40,43);
-      doTestDgaps(100000,415,418);
-      doTestDgaps(1000000,3123,3126);
-      // now exercise skipping of fully populated byte in the bitset (they are omitted if bitset is sparse)
-      MockDirectoryWrapper d = new  MockDirectoryWrapper(random(), new RAMDirectory());
-      d.setPreventDoubleWrite(false);
-      BitVector bv = new BitVector(10000);
-      bv.set(0);
-      for (int i = 8; i < 16; i++) {
-        bv.set(i);
-      } // make sure we have once byte full of set bits
-      for (int i = 32; i < 40; i++) {
-        bv.set(i);
-      } // get a second byte full of set bits
-      // add some more bits here 
-      for (int i = 40; i < 10000; i++) {
-        if (random().nextInt(1000) == 0) {
-          bv.set(i);
-        }
-      }
-      bv.write(d, "TESTBV", newIOContext(random()));
-      BitVector compare = new BitVector(d, "TESTBV", newIOContext(random()));
-      assertTrue(doCompare(bv,compare));
-    }
-    
-    private void doTestDgaps(int size, int count1, int count2) throws IOException {
-      MockDirectoryWrapper d = new  MockDirectoryWrapper(random(), new RAMDirectory());
-      d.setPreventDoubleWrite(false);
-      BitVector bv = new BitVector(size);
-      bv.invertAll();
-      for (int i=0; i<count1; i++) {
-        bv.clear(i);
-        assertEquals(i+1,size-bv.count());
-      }
-      bv.write(d, "TESTBV", newIOContext(random()));
-      // gradually increase number of set bits
-      for (int i=count1; i<count2; i++) {
-        BitVector bv2 = new BitVector(d, "TESTBV", newIOContext(random()));
-        assertTrue(doCompare(bv,bv2));
-        bv = bv2;
-        bv.clear(i);
-        assertEquals(i+1, size-bv.count());
-        bv.write(d, "TESTBV", newIOContext(random()));
-      }
-      // now start decreasing number of set bits
-      for (int i=count2-1; i>=count1; i--) {
-        BitVector bv2 = new BitVector(d, "TESTBV", newIOContext(random()));
-        assertTrue(doCompare(bv,bv2));
-        bv = bv2;
-        bv.set(i);
-        assertEquals(i,size-bv.count());
-        bv.write(d, "TESTBV", newIOContext(random()));
-      }
-    }
-
-    public void testSparseWrite() throws IOException {
-      Directory d = newDirectory();
-      final int numBits = 10240;
-      BitVector bv = new BitVector(numBits);
-      bv.invertAll();
-      int numToClear = random().nextInt(5);
-      for(int i=0;i<numToClear;i++) {
-        bv.clear(random().nextInt(numBits));
-      }
-      bv.write(d, "test", newIOContext(random()));
-      final long size = d.fileLength("test");
-      assertTrue("size=" + size, size < 100);
-      d.close();
-    }
-
-    public void testClearedBitNearEnd() throws IOException {
-      Directory d = newDirectory();
-      final int numBits = TestUtil.nextInt(random(), 7, 1000);
-      BitVector bv = new BitVector(numBits);
-      bv.invertAll();
-      bv.clear(numBits- TestUtil.nextInt(random(), 1, 7));
-      bv.write(d, "test", newIOContext(random()));
-      assertEquals(numBits-1, bv.count());
-      d.close();
-    }
-
-    public void testMostlySet() throws IOException {
-      Directory d = newDirectory();
-      final int numBits = TestUtil.nextInt(random(), 30, 1000);
-      for(int numClear=0;numClear<20;numClear++) {
-        BitVector bv = new BitVector(numBits);
-        bv.invertAll();
-        int count = 0;
-        while(count < numClear) {
-          final int bit = random().nextInt(numBits);
-          // Don't use getAndClear, so that count is recomputed
-          if (bv.get(bit)) {
-            bv.clear(bit);
-            count++;
-            assertEquals(numBits-count, bv.count());
-          }
-        }
-      }
-
-      d.close();
-    }
-
-    /**
-     * Compare two BitVectors.
-     * This should really be an equals method on the BitVector itself.
-     * @param bv One bit vector
-     * @param compare The second to compare
-     */
-    private boolean doCompare(BitVector bv, BitVector compare) {
-        boolean equal = true;
-        for(int i=0;i<bv.size();i++) {
-            // bits must be equal
-            if(bv.get(i)!=compare.get(i)) {
-                equal = false;
-                break;
-            }
-        }
-        assertEquals(bv.count(), compare.count());
-        return equal;
-    }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java
deleted file mode 100644
index 048f05a..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40DocValuesFormat.java
+++ /dev/null
@@ -1,46 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseDocValuesFormatTestCase;
-import org.junit.BeforeClass;
-
-/**
- * Tests Lucene40DocValuesFormat
- */
-public class TestLucene40DocValuesFormat extends BaseDocValuesFormatTestCase {
-  private final Codec codec = new Lucene40RWCodec();
-  
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-
-  // LUCENE-4583: This codec should throw IAE on huge binary values:
-  @Override
-  protected boolean codecAcceptsHugeBinaryValues(String field) {
-    return false;
-  }
-
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40NormsFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40NormsFormat.java
deleted file mode 100644
index f3fb65a..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40NormsFormat.java
+++ /dev/null
@@ -1,38 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseNormsFormatTestCase;
-import org.junit.BeforeClass;
-
-
-/** Tests Lucene40's norms format */
-public class TestLucene40NormsFormat extends BaseNormsFormatTestCase {
-  final Codec codec = new Lucene40RWCodec();
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-  
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java
deleted file mode 100644
index a3deacc..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java
+++ /dev/null
@@ -1,40 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BasePostingsFormatTestCase;
-import org.junit.BeforeClass;
-
-/**
- * Tests Lucene40PostingsFormat
- */
-public class TestLucene40PostingsFormat extends BasePostingsFormatTestCase {
-  private final Codec codec = new Lucene40RWCodec();
-
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-  
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java
deleted file mode 100644
index a3f6bc9..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsReader.java
+++ /dev/null
@@ -1,132 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.Collections;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-import org.junit.BeforeClass;
-
-public class TestLucene40PostingsReader extends LuceneTestCase {
-  static final String terms[] = new String[100];
-  static {
-    for (int i = 0; i < terms.length; i++) {
-      terms[i] = Integer.toString(i+1);
-    }
-  }
-  
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-
-  /** tests terms with different probabilities of being in the document.
-   *  depends heavily on term vectors cross-check at checkIndex
-   */
-  public void testPostings() throws Exception {
-    Directory dir = newFSDirectory(createTempDir("postings"));
-    IndexWriterConfig iwc = newIndexWriterConfig(new MockAnalyzer(random()));
-    iwc.setCodec(Codec.forName("Lucene40"));
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
-    
-    Document doc = new Document();
-    
-    // id field
-    FieldType idType = new FieldType(StringField.TYPE_NOT_STORED);
-    idType.setStoreTermVectors(true);
-    Field idField = new Field("id", "", idType);
-    doc.add(idField);
-    
-    // title field: short text field
-    FieldType titleType = new FieldType(TextField.TYPE_NOT_STORED);
-    titleType.setStoreTermVectors(true);
-    titleType.setStoreTermVectorPositions(true);
-    titleType.setStoreTermVectorOffsets(true);
-    titleType.setIndexOptions(indexOptions());
-    Field titleField = new Field("title", "", titleType);
-    doc.add(titleField);
-    
-    // body field: long text field
-    FieldType bodyType = new FieldType(TextField.TYPE_NOT_STORED);
-    bodyType.setStoreTermVectors(true);
-    bodyType.setStoreTermVectorPositions(true);
-    bodyType.setStoreTermVectorOffsets(true);
-    bodyType.setIndexOptions(indexOptions());
-    Field bodyField = new Field("body", "", bodyType);
-    doc.add(bodyField);
-    
-    int numDocs = atLeast(1000);
-    for (int i = 0; i < numDocs; i++) {
-      idField.setStringValue(Integer.toString(i));
-      titleField.setStringValue(fieldValue(1));
-      bodyField.setStringValue(fieldValue(3));
-      iw.addDocument(doc);
-      if (random().nextInt(20) == 0) {
-        iw.deleteDocuments(new Term("id", Integer.toString(i)));
-      }
-    }
-    if (random().nextBoolean()) {
-      // delete 1-100% of docs
-      iw.deleteDocuments(new Term("title", terms[random().nextInt(terms.length)]));
-    }
-    iw.close();
-    dir.close(); // checkindex
-  }
-  
-  IndexOptions indexOptions() {
-    switch(random().nextInt(4)) {
-      case 0: return IndexOptions.DOCS_ONLY;
-      case 1: return IndexOptions.DOCS_AND_FREQS;
-      case 2: return IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-      default: return IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS;
-    }
-  }
-  
-  String fieldValue(int maxTF) {
-    ArrayList<String> shuffled = new ArrayList<>();
-    StringBuilder sb = new StringBuilder();
-    int i = random().nextInt(terms.length);
-    while (i < terms.length) {
-      int tf =  TestUtil.nextInt(random(), 1, maxTF);
-      for (int j = 0; j < tf; j++) {
-        shuffled.add(terms[i]);
-      }
-      i++;
-    }
-    Collections.shuffle(shuffled, random());
-    for (String term : shuffled) {
-      sb.append(term);
-      sb.append(' ');
-    }
-    return sb.toString();
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java
deleted file mode 100644
index 2502d89..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40StoredFieldsFormat.java
+++ /dev/null
@@ -1,35 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseStoredFieldsFormatTestCase;
-import org.junit.BeforeClass;
-
-public class TestLucene40StoredFieldsFormat extends BaseStoredFieldsFormatTestCase {
-  
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-  
-  @Override
-  protected Codec getCodec() {
-    return new Lucene40RWCodec();
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java
deleted file mode 100644
index e97b3b3..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40TermVectorsFormat.java
+++ /dev/null
@@ -1,36 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseTermVectorsFormatTestCase;
-import org.junit.BeforeClass;
-
-public class TestLucene40TermVectorsFormat extends BaseTermVectorsFormatTestCase {
-
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-  
-  @Override
-  protected Codec getCodec() {
-    return new Lucene40RWCodec();
-  }
-  
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
deleted file mode 100644
index 9ac520d..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
+++ /dev/null
@@ -1,199 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.IdentityHashMap;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits.MatchNoBits;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LineFileDocs;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-import org.junit.BeforeClass;
-
-// TODO: really this should be in BaseTestPF or somewhere else? useful test!
-public class TestReuseDocsEnum extends LuceneTestCase {
-
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-  
-  public void testReuseDocsEnumNoReuse() throws IOException {
-    Directory dir = newDirectory();
-    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,
-        newIndexWriterConfig(new MockAnalyzer(random())).setCodec(cp));
-    int numdocs = atLeast(20);
-    createRandomIndex(numdocs, writer, random());
-    writer.commit();
-
-    DirectoryReader open = DirectoryReader.open(dir);
-    for (AtomicReaderContext ctx : open.leaves()) {
-      AtomicReader indexReader = ctx.reader();
-      Terms terms = indexReader.terms("body");
-      TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
-      MatchNoBits bits = new Bits.MatchNoBits(indexReader.maxDoc());
-      while ((iterator.next()) != null) {
-        DocsEnum docs = iterator.docs(random().nextBoolean() ? bits : new Bits.MatchNoBits(indexReader.maxDoc()), null, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      
-      assertEquals(terms.size(), enums.size());
-    }
-    writer.commit();
-    IOUtils.close(writer, open, dir);
-  }
-  
-  // tests for reuse only if bits are the same either null or the same instance
-  public void testReuseDocsEnumSameBitsOrNull() throws IOException {
-    Directory dir = newDirectory();
-    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,
-        newIndexWriterConfig(new MockAnalyzer(random())).setCodec(cp));
-    int numdocs = atLeast(20);
-    createRandomIndex(numdocs, writer, random());
-    writer.commit();
-
-    DirectoryReader open = DirectoryReader.open(dir);
-    for (AtomicReaderContext ctx : open.leaves()) {
-      Terms terms = ctx.reader().terms("body");
-      TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
-      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());
-      DocsEnum docs = null;
-      while ((iterator.next()) != null) {
-        docs = iterator.docs(bits, docs, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      
-      assertEquals(1, enums.size());
-      enums.clear();
-      iterator = terms.iterator(null);
-      docs = null;
-      while ((iterator.next()) != null) {
-        docs = iterator.docs(new Bits.MatchNoBits(open.maxDoc()), docs, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      assertEquals(terms.size(), enums.size());
-      
-      enums.clear();
-      iterator = terms.iterator(null);
-      docs = null;
-      while ((iterator.next()) != null) {
-        docs = iterator.docs(null, docs, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      assertEquals(1, enums.size());  
-    }
-    writer.close();
-    IOUtils.close(open, dir);
-  }
-  
-  // make sure we never reuse from another reader even if it is the same field & codec etc
-  public void testReuseDocsEnumDifferentReader() throws IOException {
-    Directory dir = newDirectory();
-    Codec cp = TestUtil.alwaysPostingsFormat(new Lucene40RWPostingsFormat());
-    MockAnalyzer analyzer = new MockAnalyzer(random());
-    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir,
-        newIndexWriterConfig(analyzer).setCodec(cp));
-    int numdocs = atLeast(20);
-    createRandomIndex(numdocs, writer, random());
-    writer.commit();
-
-    DirectoryReader firstReader = DirectoryReader.open(dir);
-    DirectoryReader secondReader = DirectoryReader.open(dir);
-    List<AtomicReaderContext> leaves = firstReader.leaves();
-    List<AtomicReaderContext> leaves2 = secondReader.leaves();
-    
-    for (AtomicReaderContext ctx : leaves) {
-      Terms terms = ctx.reader().terms("body");
-      TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<>();
-      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());
-      iterator = terms.iterator(null);
-      DocsEnum docs = null;
-      BytesRef term = null;
-      while ((term = iterator.next()) != null) {
-        docs = iterator.docs(null, randomDocsEnum("body", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      assertEquals(terms.size(), enums.size());
-      
-      iterator = terms.iterator(null);
-      enums.clear();
-      docs = null;
-      while ((term = iterator.next()) != null) {
-        docs = iterator.docs(bits, randomDocsEnum("body", term, leaves2, bits), random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-        enums.put(docs, true);
-      }
-      assertEquals(terms.size(), enums.size());
-    }
-    writer.close();
-    IOUtils.close(firstReader, secondReader, dir);
-  }
-  
-  public DocsEnum randomDocsEnum(String field, BytesRef term, List<AtomicReaderContext> readers, Bits bits) throws IOException {
-    if (random().nextInt(10) == 0) {
-      return null;
-    }
-    AtomicReader indexReader = readers.get(random().nextInt(readers.size())).reader();
-    Terms terms = indexReader.terms(field);
-    if (terms == null) {
-      return null;
-    }
-    TermsEnum iterator = terms.iterator(null);
-    if (iterator.seekExact(term)) {
-      return iterator.docs(bits, null, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
-    }
-    return null;
-  }
-
-  /**
-   * populates a writer with random stuff. this must be fully reproducable with
-   * the seed!
-   */
-  public static void createRandomIndex(int numdocs, RandomIndexWriter writer,
-      Random random) throws IOException {
-    LineFileDocs lineFileDocs = new LineFileDocs(random);
-
-    for (int i = 0; i < numdocs; i++) {
-      writer.addDocument(lineFileDocs.nextDoc());
-    }
-    
-    lineFileDocs.close();
-  }
-
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java
index cf296f5..485ed37 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestLucene41StoredFieldsFormat.java
@@ -18,18 +18,12 @@ package org.apache.lucene.codecs.lucene41;
  */
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.index.BaseStoredFieldsFormatTestCase;
-import org.junit.BeforeClass;
 
 public class TestLucene41StoredFieldsFormat extends BaseStoredFieldsFormatTestCase {
-  
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-  
   @Override
   protected Codec getCodec() {
-    return new Lucene41RWCodec();
+    return new Lucene410Codec();
   }
 }
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java
deleted file mode 100644
index a7796c4..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene42/TestLucene42DocValuesFormat.java
+++ /dev/null
@@ -1,44 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
-import org.junit.BeforeClass;
-
-/**
- * Tests Lucene42DocValuesFormat
- */
-public class TestLucene42DocValuesFormat extends BaseCompressingDocValuesFormatTestCase {
-  private final Codec codec = new Lucene42RWCodec();
-
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-
-  @Override
-  protected boolean codecAcceptsHugeBinaryValues(String field) {
-    return false;
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene42/TestLucene42NormsFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene42/TestLucene42NormsFormat.java
deleted file mode 100644
index 9ee7827..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene42/TestLucene42NormsFormat.java
+++ /dev/null
@@ -1,38 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseNormsFormatTestCase;
-import org.junit.BeforeClass;
-
-
-/** Tests Lucene42's norms format */
-public class TestLucene42NormsFormat extends BaseNormsFormatTestCase {
-  final Codec codec = new Lucene42RWCodec();
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-  
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java
deleted file mode 100644
index 67654da..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene45/TestLucene45DocValuesFormat.java
+++ /dev/null
@@ -1,39 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
-import org.junit.BeforeClass;
-
-/**
- * Tests Lucene45DocValuesFormat
- */
-public class TestLucene45DocValuesFormat extends BaseCompressingDocValuesFormatTestCase {
-  private final Codec codec = new Lucene45RWCodec();
-  
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java
deleted file mode 100644
index c6bb58c..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene49/TestLucene49DocValuesFormat.java
+++ /dev/null
@@ -1,39 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.index.BaseCompressingDocValuesFormatTestCase;
-import org.junit.BeforeClass;
-
-/**
- * Tests Lucene49DocValuesFormat
- */
-public class TestLucene49DocValuesFormat extends BaseCompressingDocValuesFormatTestCase {
-  private final Codec codec = new Lucene49RWCodec();
-  
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
-  
-  @Override
-  protected Codec getCodec() {
-    return codec;
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java
index b2f0d90..f84d61a 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/lucene49/TestLucene49NormsFormat.java
@@ -18,19 +18,14 @@ package org.apache.lucene.codecs.lucene49;
  */
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.index.BaseNormsFormatTestCase;
-import org.junit.BeforeClass;
 
 /**
  * Tests Lucene49NormsFormat
  */
 public class TestLucene49NormsFormat extends BaseNormsFormatTestCase {
-  private final Codec codec = new Lucene49RWCodec();
-  
-  @BeforeClass
-  public static void beforeClass() {
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true; // explicitly instantiates ancient codec
-  }
+  private final Codec codec = new Lucene410Codec();
   
   @Override
   protected Codec getCodec() {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java b/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
deleted file mode 100644
index ee9a19f..0000000
--- a/lucene/core/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
+++ /dev/null
@@ -1,1164 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.IntField;
-import org.apache.lucene.document.LongField;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.SortedDocValuesField;
-import org.apache.lucene.document.SortedNumericDocValuesField;
-import org.apache.lucene.document.SortedSetDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.BaseDirectoryWrapper;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.NIOFSDirectory;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.store.SimpleFSDirectory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.InfoStream;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
-import org.apache.lucene.util.NumericUtils;
-import org.apache.lucene.util.TestUtil;
-import org.apache.lucene.util.Version;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-
-/*
-  Verify we can read the pre-5.0 file format, do searches
-  against it, and add documents to it.
-*/
-// note: add this if we make a 4.x impersonator
-// TODO: don't use 4.x codec, its unrealistic since it means
-// we won't even be running the actual code, only the impostor
-// @SuppressCodecs("Lucene4x")
-@SuppressCodecs({"Lucene40", "Lucene41", "Lucene42", "Lucene45", "Lucene46", "Lucene49"})
-public class TestBackwardsCompatibility extends LuceneTestCase {
-
-  // Uncomment these cases & run them on an older Lucene version,
-  // to generate indexes to test backwards compatibility.  These
-  // indexes will be created under directory /tmp/idx/.
-  //
-  // However, you must first disable the Lucene TestSecurityManager,
-  // which will otherwise disallow writing outside of the build/
-  // directory - to do this, comment out the "java.security.manager"
-  // <sysproperty> under the "test-macro" <macrodef>.
-  //
-  // Be sure to create the indexes with the actual format:
-  //  ant test -Dtestcase=TestBackwardsCompatibility -Dversion=x.y.z
-  //      -Dtests.codec=LuceneXY -Dtests.postingsformat=LuceneXY -Dtests.docvaluesformat=LuceneXY
-  //
-  // Zip up the generated indexes:
-  //
-  //    cd /tmp/idx/index.cfs   ; zip index.<VERSION>.cfs.zip *
-  //    cd /tmp/idx/index.nocfs ; zip index.<VERSION>.nocfs.zip *
-  //
-  // Then move those 2 zip files to your trunk checkout and add them
-  // to the oldNames array.
-
-  /*
-  public void testCreateCFS() throws IOException {
-    createIndex("index.cfs", true, false);
-  }
-
-  public void testCreateNoCFS() throws IOException {
-    createIndex("index.nocfs", false, false);
-  }
-  */
-
-/*
-  // These are only needed for the special upgrade test to verify
-  // that also single-segment indexes are correctly upgraded by IndexUpgrader.
-  // You don't need them to be build for non-4.0 (the test is happy with just one
-  // "old" segment format, version is unimportant:
-  
-  public void testCreateSingleSegmentCFS() throws IOException {
-    createIndex("index.singlesegment.cfs", true, true);
-  }
-
-  public void testCreateSingleSegmentNoCFS() throws IOException {
-    createIndex("index.singlesegment.nocfs", false, true);
-  }
-
-*/  
-
-  /*
-  public void testCreateMoreTermsIndex() throws Exception {
-    // we use a real directory name that is not cleaned up,
-    // because this method is only used to create backwards
-    // indexes:
-    File indexDir = new File("moreterms");
-    _TestUtil.rmDir(indexDir);
-    Directory dir = newFSDirectory(indexDir);
-
-    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();
-    mp.setUseCompoundFile(false);
-    mp.setNoCFSRatio(1.0);
-    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);
-    MockAnalyzer analyzer = new MockAnalyzer(random());
-    analyzer.setMaxTokenLength(TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH));
-
-    // TODO: remove randomness
-    IndexWriterConfig conf = new IndexWriterConfig(analyzer)
-      .setMergePolicy(mp);
-    conf.setCodec(Codec.forName("Lucene40"));
-    IndexWriter writer = new IndexWriter(dir, conf);
-    LineFileDocs docs = new LineFileDocs(null, true);
-    for(int i=0;i<50;i++) {
-      writer.addDocument(docs.nextDoc());
-    }
-    writer.close();
-    dir.close();
-
-    // Gives you time to copy the index out!: (there is also
-    // a test option to not remove temp dir...):
-    Thread.sleep(100000);
-  }
-  */
-  
-  private void updateNumeric(IndexWriter writer, String id, String f, String cf, long value) throws IOException {
-    writer.updateNumericDocValue(new Term("id", id), f, value);
-    writer.updateNumericDocValue(new Term("id", id), cf, value*2);
-  }
-  
-  private void updateBinary(IndexWriter writer, String id, String f, String cf, long value) throws IOException {
-    writer.updateBinaryDocValue(new Term("id", id), f, TestBinaryDocValuesUpdates.toBytes(value));
-    writer.updateBinaryDocValue(new Term("id", id), cf, TestBinaryDocValuesUpdates.toBytes(value*2));
-  }
-
-/*  // Creates an index with DocValues updates
-  public void testCreateIndexWithDocValuesUpdates() throws Exception {
-    // we use a real directory name that is not cleaned up,
-    // because this method is only used to create backwards
-    // indexes:
-    File indexDir = new File("/tmp/idx/dvupdates");
-    TestUtil.rm(indexDir);
-    Directory dir = newFSDirectory(indexDir);
-    
-    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))
-      .setUseCompoundFile(false).setMergePolicy(NoMergePolicy.INSTANCE);
-    IndexWriter writer = new IndexWriter(dir, conf);
-    // create an index w/ few doc-values fields, some with updates and some without
-    for (int i = 0; i < 30; i++) {
-      Document doc = new Document();
-      doc.add(new StringField("id", "" + i, Store.NO));
-      doc.add(new NumericDocValuesField("ndv1", i));
-      doc.add(new NumericDocValuesField("ndv1_c", i*2));
-      doc.add(new NumericDocValuesField("ndv2", i*3));
-      doc.add(new NumericDocValuesField("ndv2_c", i*6));
-      doc.add(new BinaryDocValuesField("bdv1", TestBinaryDocValuesUpdates.toBytes(i)));
-      doc.add(new BinaryDocValuesField("bdv1_c", TestBinaryDocValuesUpdates.toBytes(i*2)));
-      doc.add(new BinaryDocValuesField("bdv2", TestBinaryDocValuesUpdates.toBytes(i*3)));
-      doc.add(new BinaryDocValuesField("bdv2_c", TestBinaryDocValuesUpdates.toBytes(i*6)));
-      writer.addDocument(doc);
-      if ((i+1) % 10 == 0) {
-        writer.commit(); // flush every 10 docs
-      }
-    }
-    
-    // first segment: no updates
-    
-    // second segment: update two fields, same gen
-    updateNumeric(writer, "10", "ndv1", "ndv1_c", 100L);
-    updateBinary(writer, "11", "bdv1", "bdv1_c", 100L);
-    writer.commit();
-    
-    // third segment: update few fields, different gens, few docs
-    updateNumeric(writer, "20", "ndv1", "ndv1_c", 100L);
-    updateBinary(writer, "21", "bdv1", "bdv1_c", 100L);
-    writer.commit();
-    updateNumeric(writer, "22", "ndv1", "ndv1_c", 200L); // update the field again
-    writer.commit();
-    
-    writer.close();
-    dir.close();
-  }*/
-
-  final static String[] oldNames = {"40.cfs",
-                                    "40.nocfs",
-                                    "41.cfs",
-                                    "41.nocfs",
-                                    "42.cfs",
-                                    "42.nocfs",
-                                    // TODO: these are on 4x, but something is wrong (they seem to be a too old DV format):
-                                    "45.cfs",
-                                    "45.nocfs",
-                                    "461.cfs",
-                                    "461.nocfs",
-                                    "49.cfs",
-                                    "49.nocfs"
-  };
-  
-  final String[] unsupportedNames = {"19.cfs",
-                                     "19.nocfs",
-                                     "20.cfs",
-                                     "20.nocfs",
-                                     "21.cfs",
-                                     "21.nocfs",
-                                     "22.cfs",
-                                     "22.nocfs",
-                                     "23.cfs",
-                                     "23.nocfs",
-                                     "24.cfs",
-                                     "24.nocfs",
-                                     "29.cfs",
-                                     "29.nocfs",
-                                     "30.cfs",
-                                     "30.nocfs",
-                                     "31.cfs",
-                                     "31.nocfs",
-                                     "32.cfs",
-                                     "32.nocfs",
-                                     "34.cfs",
-                                     "34.nocfs"
-  };
-  
-  final static String[] oldSingleSegmentNames = {"40.optimized.cfs",
-                                                 "40.optimized.nocfs",
-  };
-  
-  static Map<String,Directory> oldIndexDirs;
-
-  /**
-   * Randomizes the use of some of hte constructor variations
-   */
-  private static IndexUpgrader newIndexUpgrader(Directory dir) {
-    final boolean streamType = random().nextBoolean();
-    final int choice = TestUtil.nextInt(random(), 0, 2);
-    switch (choice) {
-      case 0: return new IndexUpgrader(dir);
-      case 1: return new IndexUpgrader(dir, streamType ? null : InfoStream.NO_OUTPUT, false);
-      case 2: return new IndexUpgrader(dir, newIndexWriterConfig(null), false);
-      default: fail("case statement didn't get updated when random bounds changed");
-    }
-    return null; // never get here
-  }
-
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    assertFalse("test infra is broken!", LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE);
-    List<String> names = new ArrayList<>(oldNames.length + oldSingleSegmentNames.length);
-    names.addAll(Arrays.asList(oldNames));
-    names.addAll(Arrays.asList(oldSingleSegmentNames));
-    oldIndexDirs = new HashMap<>();
-    for (String name : names) {
-      File dir = createTempDir(name);
-      File dataFile = new File(TestBackwardsCompatibility.class.getResource("index." + name + ".zip").toURI());
-      TestUtil.unzip(dataFile, dir);
-      oldIndexDirs.put(name, newFSDirectory(dir));
-    }
-  }
-  
-  @AfterClass
-  public static void afterClass() throws Exception {
-    for (Directory d : oldIndexDirs.values()) {
-      d.close();
-    }
-    oldIndexDirs = null;
-  }
-  
-  /** This test checks that *only* IndexFormatTooOldExceptions are thrown when you open and operate on too old indexes! */
-  public void testUnsupportedOldIndexes() throws Exception {
-    for(int i=0;i<unsupportedNames.length;i++) {
-      if (VERBOSE) {
-        System.out.println("TEST: index " + unsupportedNames[i]);
-      }
-      File oldIndxeDir = createTempDir(unsupportedNames[i]);
-      TestUtil.unzip(getDataFile("unsupported." + unsupportedNames[i] + ".zip"), oldIndxeDir);
-      BaseDirectoryWrapper dir = newFSDirectory(oldIndxeDir);
-      // don't checkindex, these are intentionally not supported
-      dir.setCheckIndexOnClose(false);
-
-      IndexReader reader = null;
-      IndexWriter writer = null;
-      try {
-        reader = DirectoryReader.open(dir);
-        fail("DirectoryReader.open should not pass for "+unsupportedNames[i]);
-      } catch (IndexFormatTooOldException e) {
-        // pass
-      } finally {
-        if (reader != null) reader.close();
-        reader = null;
-      }
-
-      try {
-        writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random())).setCommitOnClose(false));
-        fail("IndexWriter creation should not pass for "+unsupportedNames[i]);
-      } catch (IndexFormatTooOldException e) {
-        // pass
-        if (VERBOSE) {
-          System.out.println("TEST: got expected exc:");
-          e.printStackTrace(System.out);
-        }
-        // Make sure exc message includes a path=
-        assertTrue("got exc message: " + e.getMessage(), e.getMessage().indexOf("path=\"") != -1);
-      } finally {
-        // we should fail to open IW, and so it should be null when we get here.
-        // However, if the test fails (i.e., IW did not fail on open), we need
-        // to close IW. However, if merges are run, IW may throw
-        // IndexFormatTooOldException, and we don't want to mask the fail()
-        // above, so close without waiting for merges.
-        if (writer != null) {
-          try {
-            writer.commit();
-          } finally {
-            writer.close();
-          }
-        }
-        writer = null;
-      }
-      
-      ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
-      CheckIndex checker = new CheckIndex(dir);
-      checker.setInfoStream(new PrintStream(bos, false, IOUtils.UTF_8));
-      CheckIndex.Status indexStatus = checker.checkIndex();
-      assertFalse(indexStatus.clean);
-      assertTrue(bos.toString(IOUtils.UTF_8).contains(IndexFormatTooOldException.class.getName()));
-
-      dir.close();
-      TestUtil.rm(oldIndxeDir);
-    }
-  }
-  
-  public void testFullyMergeOldIndex() throws Exception {
-    for (String name : oldNames) {
-      if (VERBOSE) {
-        System.out.println("\nTEST: index=" + name);
-      }
-      Directory dir = newDirectory(oldIndexDirs.get(name));
-      IndexWriter w = new IndexWriter(dir, new IndexWriterConfig(new MockAnalyzer(random())));
-      w.forceMerge(1);
-      w.close();
-      
-      dir.close();
-    }
-  }
-
-  public void testAddOldIndexes() throws IOException {
-    for (String name : oldNames) {
-      if (VERBOSE) {
-        System.out.println("\nTEST: old index " + name);
-      }
-      Directory targetDir = newDirectory();
-      IndexWriter w = new IndexWriter(targetDir, newIndexWriterConfig(new MockAnalyzer(random())));
-      w.addIndexes(oldIndexDirs.get(name));
-      if (VERBOSE) {
-        System.out.println("\nTEST: done adding indices; now close");
-      }
-      w.close();
-      
-      targetDir.close();
-    }
-  }
-
-  public void testAddOldIndexesReader() throws IOException {
-    for (String name : oldNames) {
-      IndexReader reader = DirectoryReader.open(oldIndexDirs.get(name));
-      
-      Directory targetDir = newDirectory();
-      IndexWriter w = new IndexWriter(targetDir, newIndexWriterConfig(new MockAnalyzer(random())));
-      w.addIndexes(reader);
-      w.close();
-      reader.close();
-            
-      targetDir.close();
-    }
-  }
-
-  public void testSearchOldIndex() throws IOException {
-    for (String name : oldNames) {
-      searchIndex(oldIndexDirs.get(name), name);
-    }
-  }
-
-  public void testIndexOldIndexNoAdds() throws IOException {
-    for (String name : oldNames) {
-      Directory dir = newDirectory(oldIndexDirs.get(name));
-      changeIndexNoAdds(random(), dir);
-      dir.close();
-    }
-  }
-
-  public void testIndexOldIndex() throws IOException {
-    for (String name : oldNames) {
-      if (VERBOSE) {
-        System.out.println("TEST: oldName=" + name);
-      }
-      Directory dir = newDirectory(oldIndexDirs.get(name));
-      changeIndexWithAdds(random(), dir, name);
-      dir.close();
-    }
-  }
-
-  private void doTestHits(ScoreDoc[] hits, int expectedCount, IndexReader reader) throws IOException {
-    final int hitCount = hits.length;
-    assertEquals("wrong number of hits", expectedCount, hitCount);
-    for(int i=0;i<hitCount;i++) {
-      reader.document(hits[i].doc);
-      reader.getTermVectors(hits[i].doc);
-    }
-  }
-
-  public void searchIndex(Directory dir, String oldName) throws IOException {
-    //QueryParser parser = new QueryParser("contents", new MockAnalyzer(random));
-    //Query query = parser.parse("handle:1");
-
-    IndexReader reader = DirectoryReader.open(dir);
-    IndexSearcher searcher = newSearcher(reader);
-
-    TestUtil.checkIndex(dir);
-    
-    // true if this is a 4.0+ index
-    final boolean is40Index = MultiFields.getMergedFieldInfos(reader).fieldInfo("content5") != null;
-    // true if this is a 4.2+ index
-    final boolean is42Index = MultiFields.getMergedFieldInfos(reader).fieldInfo("dvSortedSet") != null;
-    // true if this is a 4.9+ index
-    final boolean is49Index = MultiFields.getMergedFieldInfos(reader).fieldInfo("dvSortedNumeric") != null;
-
-    assert is40Index; // NOTE: currently we can only do this on trunk!
-
-    final Bits liveDocs = MultiFields.getLiveDocs(reader);
-
-    for(int i=0;i<35;i++) {
-      if (liveDocs.get(i)) {
-        StoredDocument d = reader.document(i);
-        List<StorableField> fields = d.getFields();
-        boolean isProxDoc = d.getField("content3") == null;
-        if (isProxDoc) {
-          final int numFields = is40Index ? 7 : 5;
-          assertEquals(numFields, fields.size());
-          StorableField f =  d.getField("id");
-          assertEquals(""+i, f.stringValue());
-
-          f = d.getField("utf8");
-          assertEquals("Lu\uD834\uDD1Ece\uD834\uDD60ne \u0000 \u2620 ab\ud917\udc17cd", f.stringValue());
-
-          f =  d.getField("autf8");
-          assertEquals("Lu\uD834\uDD1Ece\uD834\uDD60ne \u0000 \u2620 ab\ud917\udc17cd", f.stringValue());
-      
-          f = d.getField("content2");
-          assertEquals("here is more content with aaa aaa aaa", f.stringValue());
-
-          f = d.getField("fie\u2C77ld");
-          assertEquals("field with non-ascii name", f.stringValue());
-        }
-
-        Fields tfvFields = reader.getTermVectors(i);
-        assertNotNull("i=" + i, tfvFields);
-        Terms tfv = tfvFields.terms("utf8");
-        assertNotNull("docID=" + i + " index=" + oldName, tfv);
-      } else {
-        // Only ID 7 is deleted
-        assertEquals(7, i);
-      }
-    }
-
-    if (is40Index) {
-      // check docvalues fields
-      NumericDocValues dvByte = MultiDocValues.getNumericValues(reader, "dvByte");
-      BinaryDocValues dvBytesDerefFixed = MultiDocValues.getBinaryValues(reader, "dvBytesDerefFixed");
-      BinaryDocValues dvBytesDerefVar = MultiDocValues.getBinaryValues(reader, "dvBytesDerefVar");
-      SortedDocValues dvBytesSortedFixed = MultiDocValues.getSortedValues(reader, "dvBytesSortedFixed");
-      SortedDocValues dvBytesSortedVar = MultiDocValues.getSortedValues(reader, "dvBytesSortedVar");
-      BinaryDocValues dvBytesStraightFixed = MultiDocValues.getBinaryValues(reader, "dvBytesStraightFixed");
-      BinaryDocValues dvBytesStraightVar = MultiDocValues.getBinaryValues(reader, "dvBytesStraightVar");
-      NumericDocValues dvDouble = MultiDocValues.getNumericValues(reader, "dvDouble");
-      NumericDocValues dvFloat = MultiDocValues.getNumericValues(reader, "dvFloat");
-      NumericDocValues dvInt = MultiDocValues.getNumericValues(reader, "dvInt");
-      NumericDocValues dvLong = MultiDocValues.getNumericValues(reader, "dvLong");
-      NumericDocValues dvPacked = MultiDocValues.getNumericValues(reader, "dvPacked");
-      NumericDocValues dvShort = MultiDocValues.getNumericValues(reader, "dvShort");
-      SortedSetDocValues dvSortedSet = null;
-      if (is42Index) {
-        dvSortedSet = MultiDocValues.getSortedSetValues(reader, "dvSortedSet");
-      }
-      SortedNumericDocValues dvSortedNumeric = null;
-      if (is49Index) {
-        dvSortedNumeric = MultiDocValues.getSortedNumericValues(reader, "dvSortedNumeric");
-      }
-      
-      for (int i=0;i<35;i++) {
-        int id = Integer.parseInt(reader.document(i).get("id"));
-        assertEquals(id, dvByte.get(i));
-        
-        byte bytes[] = new byte[] {
-            (byte)(id >>> 24), (byte)(id >>> 16),(byte)(id >>> 8),(byte)id
-        };
-        BytesRef expectedRef = new BytesRef(bytes);
-        
-        BytesRef term = dvBytesDerefFixed.get(i);
-        assertEquals(expectedRef, term);
-        term = dvBytesDerefVar.get(i);
-        assertEquals(expectedRef, term);
-        term = dvBytesSortedFixed.get(i);
-        assertEquals(expectedRef, term);
-        term = dvBytesSortedVar.get(i);
-        assertEquals(expectedRef, term);
-        term = dvBytesStraightFixed.get(i);
-        assertEquals(expectedRef, term);
-        term = dvBytesStraightVar.get(i);
-        assertEquals(expectedRef, term);
-        
-        assertEquals((double)id, Double.longBitsToDouble(dvDouble.get(i)), 0D);
-        assertEquals((float)id, Float.intBitsToFloat((int)dvFloat.get(i)), 0F);
-        assertEquals(id, dvInt.get(i));
-        assertEquals(id, dvLong.get(i));
-        assertEquals(id, dvPacked.get(i));
-        assertEquals(id, dvShort.get(i));
-        if (is42Index) {
-          dvSortedSet.setDocument(i);
-          long ord = dvSortedSet.nextOrd();
-          assertEquals(SortedSetDocValues.NO_MORE_ORDS, dvSortedSet.nextOrd());
-          term = dvSortedSet.lookupOrd(ord);
-          assertEquals(expectedRef, term);
-        }
-        if (is49Index) {
-          dvSortedNumeric.setDocument(i);
-          assertEquals(1, dvSortedNumeric.count());
-          assertEquals(id, dvSortedNumeric.valueAt(0));
-        }
-      }
-    }
-    
-    ScoreDoc[] hits = searcher.search(new TermQuery(new Term(new String("content"), "aaa")), null, 1000).scoreDocs;
-
-    // First document should be #0
-    StoredDocument d = searcher.getIndexReader().document(hits[0].doc);
-    assertEquals("didn't get the right document first", "0", d.get("id"));
-
-    doTestHits(hits, 34, searcher.getIndexReader());
-    
-    if (is40Index) {
-      hits = searcher.search(new TermQuery(new Term(new String("content5"), "aaa")), null, 1000).scoreDocs;
-
-      doTestHits(hits, 34, searcher.getIndexReader());
-    
-      hits = searcher.search(new TermQuery(new Term(new String("content6"), "aaa")), null, 1000).scoreDocs;
-
-      doTestHits(hits, 34, searcher.getIndexReader());
-    }
-
-    hits = searcher.search(new TermQuery(new Term("utf8", "\u0000")), null, 1000).scoreDocs;
-    assertEquals(34, hits.length);
-    hits = searcher.search(new TermQuery(new Term(new String("utf8"), "lu\uD834\uDD1Ece\uD834\uDD60ne")), null, 1000).scoreDocs;
-    assertEquals(34, hits.length);
-    hits = searcher.search(new TermQuery(new Term("utf8", "ab\ud917\udc17cd")), null, 1000).scoreDocs;
-    assertEquals(34, hits.length);
-
-    reader.close();
-  }
-
-  private int compare(String name, String v) {
-    int v0 = Integer.parseInt(name.substring(0, 2));
-    int v1 = Integer.parseInt(v);
-    return v0 - v1;
-  }
-
-  public void changeIndexWithAdds(Random random, Directory dir, String origOldName) throws IOException {
-    // open writer
-    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random))
-                                                 .setOpenMode(OpenMode.APPEND)
-                                                 .setMergePolicy(newLogMergePolicy()));
-    // add 10 docs
-    for(int i=0;i<10;i++) {
-      addDoc(writer, 35+i);
-    }
-
-    // make sure writer sees right total -- writer seems not to know about deletes in .del?
-    final int expected;
-    if (compare(origOldName, "24") < 0) {
-      expected = 44;
-    } else {
-      expected = 45;
-    }
-    assertEquals("wrong doc count", expected, writer.numDocs());
-    writer.close();
-
-    // make sure searching sees right # hits
-    IndexReader reader = DirectoryReader.open(dir);
-    IndexSearcher searcher = newSearcher(reader);
-    ScoreDoc[] hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
-    StoredDocument d = searcher.getIndexReader().document(hits[0].doc);
-    assertEquals("wrong first document", "0", d.get("id"));
-    doTestHits(hits, 44, searcher.getIndexReader());
-    reader.close();
-
-    // fully merge
-    writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random))
-                                    .setOpenMode(OpenMode.APPEND)
-                                    .setMergePolicy(newLogMergePolicy()));
-    writer.forceMerge(1);
-    writer.close();
-
-    reader = DirectoryReader.open(dir);
-    searcher = newSearcher(reader);
-    hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
-    assertEquals("wrong number of hits", 44, hits.length);
-    d = searcher.doc(hits[0].doc);
-    doTestHits(hits, 44, searcher.getIndexReader());
-    assertEquals("wrong first document", "0", d.get("id"));
-    reader.close();
-  }
-
-  public void changeIndexNoAdds(Random random, Directory dir) throws IOException {
-    // make sure searching sees right # hits
-    DirectoryReader reader = DirectoryReader.open(dir);
-    IndexSearcher searcher = newSearcher(reader);
-    ScoreDoc[] hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
-    assertEquals("wrong number of hits", 34, hits.length);
-    StoredDocument d = searcher.doc(hits[0].doc);
-    assertEquals("wrong first document", "0", d.get("id"));
-    reader.close();
-
-    // fully merge
-    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random))
-                                                .setOpenMode(OpenMode.APPEND));
-    writer.forceMerge(1);
-    writer.close();
-
-    reader = DirectoryReader.open(dir);
-    searcher = newSearcher(reader);
-    hits = searcher.search(new TermQuery(new Term("content", "aaa")), null, 1000).scoreDocs;
-    assertEquals("wrong number of hits", 34, hits.length);
-    doTestHits(hits, 34, searcher.getIndexReader());
-    reader.close();
-  }
-
-  public File createIndex(String dirName, boolean doCFS, boolean fullyMerged) throws IOException {
-    // we use a real directory name that is not cleaned up, because this method is only used to create backwards indexes:
-    File indexDir = new File("/tmp/idx", dirName);
-    TestUtil.rm(indexDir);
-    Directory dir = newFSDirectory(indexDir);
-    LogByteSizeMergePolicy mp = new LogByteSizeMergePolicy();
-    mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);
-    mp.setMaxCFSSegmentSizeMB(Double.POSITIVE_INFINITY);
-    // TODO: remove randomness
-    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()))
-      .setMaxBufferedDocs(10).setMergePolicy(mp);
-    IndexWriter writer = new IndexWriter(dir, conf);
-    
-    for(int i=0;i<35;i++) {
-      addDoc(writer, i);
-    }
-    assertEquals("wrong doc count", 35, writer.maxDoc());
-    if (fullyMerged) {
-      writer.forceMerge(1);
-    }
-    writer.close();
-
-    if (!fullyMerged) {
-      // open fresh writer so we get no prx file in the added segment
-      mp = new LogByteSizeMergePolicy();
-      mp.setNoCFSRatio(doCFS ? 1.0 : 0.0);
-      // TODO: remove randomness
-      conf = new IndexWriterConfig(new MockAnalyzer(random()))
-        .setMaxBufferedDocs(10).setMergePolicy(mp);
-      writer = new IndexWriter(dir, conf);
-      addNoProxDoc(writer);
-      writer.close();
-
-      writer = new IndexWriter(dir, conf.setMergePolicy(NoMergePolicy.INSTANCE));
-      Term searchTerm = new Term("id", "7");
-      writer.deleteDocuments(searchTerm);
-      writer.close();
-    }
-    
-    dir.close();
-    
-    return indexDir;
-  }
-
-  private void addDoc(IndexWriter writer, int id) throws IOException
-  {
-    Document doc = new Document();
-    doc.add(new TextField("content", "aaa", Field.Store.NO));
-    doc.add(new StringField("id", Integer.toString(id), Field.Store.YES));
-    FieldType customType2 = new FieldType(TextField.TYPE_STORED);
-    customType2.setStoreTermVectors(true);
-    customType2.setStoreTermVectorPositions(true);
-    customType2.setStoreTermVectorOffsets(true);
-    doc.add(new Field("autf8", "Lu\uD834\uDD1Ece\uD834\uDD60ne \u0000 \u2620 ab\ud917\udc17cd", customType2));
-    doc.add(new Field("utf8", "Lu\uD834\uDD1Ece\uD834\uDD60ne \u0000 \u2620 ab\ud917\udc17cd", customType2));
-    doc.add(new Field("content2", "here is more content with aaa aaa aaa", customType2));
-    doc.add(new Field("fie\u2C77ld", "field with non-ascii name", customType2));
-    // add numeric fields, to test if flex preserves encoding
-    doc.add(new IntField("trieInt", id, Field.Store.NO));
-    doc.add(new LongField("trieLong", (long) id, Field.Store.NO));
-    // add docvalues fields
-    doc.add(new NumericDocValuesField("dvByte", (byte) id));
-    byte bytes[] = new byte[] {
-      (byte)(id >>> 24), (byte)(id >>> 16),(byte)(id >>> 8),(byte)id
-    };
-    BytesRef ref = new BytesRef(bytes);
-    doc.add(new BinaryDocValuesField("dvBytesDerefFixed", ref));
-    doc.add(new BinaryDocValuesField("dvBytesDerefVar", ref));
-    doc.add(new SortedDocValuesField("dvBytesSortedFixed", ref));
-    doc.add(new SortedDocValuesField("dvBytesSortedVar", ref));
-    doc.add(new BinaryDocValuesField("dvBytesStraightFixed", ref));
-    doc.add(new BinaryDocValuesField("dvBytesStraightVar", ref));
-    doc.add(new DoubleDocValuesField("dvDouble", (double)id));
-    doc.add(new FloatDocValuesField("dvFloat", (float)id));
-    doc.add(new NumericDocValuesField("dvInt", id));
-    doc.add(new NumericDocValuesField("dvLong", id));
-    doc.add(new NumericDocValuesField("dvPacked", id));
-    doc.add(new NumericDocValuesField("dvShort", (short)id));
-    doc.add(new SortedSetDocValuesField("dvSortedSet", ref));
-    doc.add(new SortedNumericDocValuesField("dvSortedNumeric", id));
-    // a field with both offsets and term vectors for a cross-check
-    FieldType customType3 = new FieldType(TextField.TYPE_STORED);
-    customType3.setStoreTermVectors(true);
-    customType3.setStoreTermVectorPositions(true);
-    customType3.setStoreTermVectorOffsets(true);
-    customType3.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    doc.add(new Field("content5", "here is more content with aaa aaa aaa", customType3));
-    // a field that omits only positions
-    FieldType customType4 = new FieldType(TextField.TYPE_STORED);
-    customType4.setStoreTermVectors(true);
-    customType4.setStoreTermVectorPositions(false);
-    customType4.setStoreTermVectorOffsets(true);
-    customType4.setIndexOptions(IndexOptions.DOCS_AND_FREQS);
-    doc.add(new Field("content6", "here is more content with aaa aaa aaa", customType4));
-    // TODO: 
-    //   index different norms types via similarity (we use a random one currently?!)
-    //   remove any analyzer randomness, explicitly add payloads for certain fields.
-    writer.addDocument(doc);
-  }
-
-  private void addNoProxDoc(IndexWriter writer) throws IOException {
-    Document doc = new Document();
-    FieldType customType = new FieldType(TextField.TYPE_STORED);
-    customType.setIndexOptions(IndexOptions.DOCS_ONLY);
-    Field f = new Field("content3", "aaa", customType);
-    doc.add(f);
-    FieldType customType2 = new FieldType();
-    customType2.setStored(true);
-    customType2.setIndexOptions(IndexOptions.DOCS_ONLY);
-    f = new Field("content4", "aaa", customType2);
-    doc.add(f);
-    writer.addDocument(doc);
-  }
-
-  private int countDocs(DocsEnum docs) throws IOException {
-    int count = 0;
-    while((docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
-      count ++;
-    }
-    return count;
-  }
-
-  // flex: test basics of TermsEnum api on non-flex index
-  public void testNextIntoWrongField() throws Exception {
-    for (String name : oldNames) {
-      Directory dir = oldIndexDirs.get(name);
-      IndexReader r = DirectoryReader.open(dir);
-      TermsEnum terms = MultiFields.getFields(r).terms("content").iterator(null);
-      BytesRef t = terms.next();
-      assertNotNull(t);
-
-      // content field only has term aaa:
-      assertEquals("aaa", t.utf8ToString());
-      assertNull(terms.next());
-
-      BytesRef aaaTerm = new BytesRef("aaa");
-
-      // should be found exactly
-      assertEquals(TermsEnum.SeekStatus.FOUND,
-                   terms.seekCeil(aaaTerm));
-      assertEquals(35, countDocs(TestUtil.docs(random(), terms, null, null, DocsEnum.FLAG_NONE)));
-      assertNull(terms.next());
-
-      // should hit end of field
-      assertEquals(TermsEnum.SeekStatus.END,
-                   terms.seekCeil(new BytesRef("bbb")));
-      assertNull(terms.next());
-
-      // should seek to aaa
-      assertEquals(TermsEnum.SeekStatus.NOT_FOUND,
-                   terms.seekCeil(new BytesRef("a")));
-      assertTrue(terms.term().bytesEquals(aaaTerm));
-      assertEquals(35, countDocs(TestUtil.docs(random(), terms, null, null, DocsEnum.FLAG_NONE)));
-      assertNull(terms.next());
-
-      assertEquals(TermsEnum.SeekStatus.FOUND,
-                   terms.seekCeil(aaaTerm));
-      assertEquals(35, countDocs(TestUtil.docs(random(), terms, null, null, DocsEnum.FLAG_NONE)));
-      assertNull(terms.next());
-
-      r.close();
-    }
-  }
-  
-  /** 
-   * Test that we didn't forget to bump the current Constants.LUCENE_MAIN_VERSION.
-   * This is important so that we can determine which version of lucene wrote the segment.
-   */
-  public void testOldVersions() throws Exception {
-    // first create a little index with the current code and get the version
-    Directory currentDir = newDirectory();
-    RandomIndexWriter riw = new RandomIndexWriter(random(), currentDir);
-    riw.addDocument(new Document());
-    riw.close();
-    DirectoryReader ir = DirectoryReader.open(currentDir);
-    SegmentReader air = (SegmentReader)ir.leaves().get(0).reader();
-    Version currentVersion = air.getSegmentInfo().info.getVersion();
-    assertNotNull(currentVersion); // only 3.0 segments can have a null version
-    ir.close();
-    currentDir.close();
-    
-    // now check all the old indexes, their version should be < the current version
-    for (String name : oldNames) {
-      Directory dir = oldIndexDirs.get(name);
-      DirectoryReader r = DirectoryReader.open(dir);
-      for (AtomicReaderContext context : r.leaves()) {
-        air = (SegmentReader) context.reader();
-        Version oldVersion = air.getSegmentInfo().info.getVersion();
-        assertNotNull(oldVersion); // only 3.0 segments can have a null version
-        assertTrue("current Version.LATEST is <= an old index: did you forget to bump it?!",
-                   currentVersion.onOrAfter(oldVersion));
-      }
-      r.close();
-    }
-  }
-  
-  public void testNumericFields() throws Exception {
-    for (String name : oldNames) {
-      
-      Directory dir = oldIndexDirs.get(name);
-      IndexReader reader = DirectoryReader.open(dir);
-      IndexSearcher searcher = newSearcher(reader);
-      
-      for (int id=10; id<15; id++) {
-        ScoreDoc[] hits = searcher.search(NumericRangeQuery.newIntRange("trieInt", NumericUtils.PRECISION_STEP_DEFAULT_32, Integer.valueOf(id), Integer.valueOf(id), true, true), 100).scoreDocs;
-        assertEquals("wrong number of hits", 1, hits.length);
-        StoredDocument d = searcher.doc(hits[0].doc);
-        assertEquals(String.valueOf(id), d.get("id"));
-        
-        hits = searcher.search(NumericRangeQuery.newLongRange("trieLong", NumericUtils.PRECISION_STEP_DEFAULT, Long.valueOf(id), Long.valueOf(id), true, true), 100).scoreDocs;
-        assertEquals("wrong number of hits", 1, hits.length);
-        d = searcher.doc(hits[0].doc);
-        assertEquals(String.valueOf(id), d.get("id"));
-      }
-      
-      // check that also lower-precision fields are ok
-      ScoreDoc[] hits = searcher.search(NumericRangeQuery.newIntRange("trieInt", NumericUtils.PRECISION_STEP_DEFAULT_32, Integer.MIN_VALUE, Integer.MAX_VALUE, false, false), 100).scoreDocs;
-      assertEquals("wrong number of hits", 34, hits.length);
-      
-      hits = searcher.search(NumericRangeQuery.newLongRange("trieLong", NumericUtils.PRECISION_STEP_DEFAULT, Long.MIN_VALUE, Long.MAX_VALUE, false, false), 100).scoreDocs;
-      assertEquals("wrong number of hits", 34, hits.length);
-      
-      // check decoding of terms
-      Terms terms = MultiFields.getTerms(searcher.getIndexReader(), "trieInt");
-      TermsEnum termsEnum = NumericUtils.filterPrefixCodedInts(terms.iterator(null));
-      while (termsEnum.next() != null) {
-        int val = NumericUtils.prefixCodedToInt(termsEnum.term());
-        assertTrue("value in id bounds", val >= 0 && val < 35);
-      }
-      
-      terms = MultiFields.getTerms(searcher.getIndexReader(), "trieLong");
-      termsEnum = NumericUtils.filterPrefixCodedLongs(terms.iterator(null));
-      while (termsEnum.next() != null) {
-        long val = NumericUtils.prefixCodedToLong(termsEnum.term());
-        assertTrue("value in id bounds", val >= 0L && val < 35L);
-      }
-      
-      reader.close();
-    }
-  }
-  
-  private int checkAllSegmentsUpgraded(Directory dir) throws IOException {
-    final SegmentInfos infos = new SegmentInfos();
-    infos.read(dir);
-    if (VERBOSE) {
-      System.out.println("checkAllSegmentsUpgraded: " + infos);
-    }
-    for (SegmentCommitInfo si : infos) {
-      assertEquals(Version.LATEST, si.info.getVersion());
-    }
-    return infos.size();
-  }
-  
-  private int getNumberOfSegments(Directory dir) throws IOException {
-    final SegmentInfos infos = new SegmentInfos();
-    infos.read(dir);
-    return infos.size();
-  }
-
-  public void testUpgradeOldIndex() throws Exception {
-    List<String> names = new ArrayList<>(oldNames.length + oldSingleSegmentNames.length);
-    names.addAll(Arrays.asList(oldNames));
-    names.addAll(Arrays.asList(oldSingleSegmentNames));
-    for(String name : names) {
-      if (VERBOSE) {
-        System.out.println("testUpgradeOldIndex: index=" +name);
-      }
-      Directory dir = newDirectory(oldIndexDirs.get(name));
-
-      newIndexUpgrader(dir).upgrade();
-
-      checkAllSegmentsUpgraded(dir);
-      
-      dir.close();
-    }
-  }
-
-  public void testCommandLineArgs() throws Exception {
-
-    PrintStream savedSystemOut = System.out;
-    System.setOut(new PrintStream(new ByteArrayOutputStream(), false, "UTF-8"));
-    try {
-      for (String name : oldIndexDirs.keySet()) {
-        File dir = createTempDir(name);
-        File dataFile = new File(TestBackwardsCompatibility.class.getResource("index." + name + ".zip").toURI());
-        TestUtil.unzip(dataFile, dir);
-        
-        String path = dir.getAbsolutePath();
-        
-        List<String> args = new ArrayList<>();
-        if (random().nextBoolean()) {
-          args.add("-verbose");
-        }
-        if (random().nextBoolean()) {
-          args.add("-delete-prior-commits");
-        }
-        if (random().nextBoolean()) {
-          // TODO: need to better randomize this, but ...
-          //  - LuceneTestCase.FS_DIRECTORIES is private
-          //  - newFSDirectory returns BaseDirectoryWrapper
-          //  - BaseDirectoryWrapper doesn't expose delegate
-          Class<? extends FSDirectory> dirImpl = random().nextBoolean() ?
-              SimpleFSDirectory.class : NIOFSDirectory.class;
-          
-          args.add("-dir-impl");
-          args.add(dirImpl.getName());
-        }
-        args.add(path);
-        
-        IndexUpgrader upgrader = null;
-        try {
-          upgrader = IndexUpgrader.parseArgs(args.toArray(new String[0]));
-        } catch (Exception e) {
-          throw new AssertionError("unable to parse args: " + args, e);
-        }
-        upgrader.upgrade();
-        
-        Directory upgradedDir = newFSDirectory(dir);
-        try {
-          checkAllSegmentsUpgraded(upgradedDir);
-        } finally {
-          upgradedDir.close();
-        }
-      }
-    } finally {
-      System.setOut(savedSystemOut);
-    }
-  }
-
-  public void testUpgradeOldSingleSegmentIndexWithAdditions() throws Exception {
-    for (String name : oldSingleSegmentNames) {
-      if (VERBOSE) {
-        System.out.println("testUpgradeOldSingleSegmentIndexWithAdditions: index=" +name);
-      }
-      Directory dir = newDirectory(oldIndexDirs.get(name));
-      if (dir instanceof MockDirectoryWrapper) {
-        // we need to ensure we delete old commits for this test,
-        // otherwise IndexUpgrader gets angry
-        ((MockDirectoryWrapper)dir).setEnableVirusScanner(false);
-      }
-
-      assertEquals("Original index must be single segment", 1, getNumberOfSegments(dir));
-
-      // create a bunch of dummy segments
-      int id = 40;
-      RAMDirectory ramDir = new RAMDirectory();
-      for (int i = 0; i < 3; i++) {
-        // only use Log- or TieredMergePolicy, to make document addition predictable and not suddenly merge:
-        MergePolicy mp = random().nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();
-        IndexWriterConfig iwc = new IndexWriterConfig(new MockAnalyzer(random()))
-          .setMergePolicy(mp).setCommitOnClose(false);
-        IndexWriter w = new IndexWriter(ramDir, iwc);
-        // add few more docs:
-        for(int j = 0; j < RANDOM_MULTIPLIER * random().nextInt(30); j++) {
-          addDoc(w, id++);
-        }
-        try {
-          w.commit();
-        } finally {
-          w.close();
-        }
-      }
-      
-      // add dummy segments (which are all in current
-      // version) to single segment index
-      MergePolicy mp = random().nextBoolean() ? newLogMergePolicy() : newTieredMergePolicy();
-      IndexWriterConfig iwc = new IndexWriterConfig(null)
-        .setMergePolicy(mp).setCommitOnClose(false);
-      IndexWriter w = new IndexWriter(dir, iwc);
-      w.addIndexes(ramDir);
-      try {
-        w.commit();
-      } finally {
-        w.close();
-      }
-      
-      // determine count of segments in modified index
-      final int origSegCount = getNumberOfSegments(dir);
-      
-      // ensure there is only one commit
-      assertEquals(1, DirectoryReader.listCommits(dir).size());
-      newIndexUpgrader(dir).upgrade();
-
-      final int segCount = checkAllSegmentsUpgraded(dir);
-      assertEquals("Index must still contain the same number of segments, as only one segment was upgraded and nothing else merged",
-        origSegCount, segCount);
-      
-      dir.close();
-    }
-  }
-
-  public static final String moreTermsIndex = "moreterms.40.zip";
-
-  public void testMoreTerms() throws Exception {
-    File oldIndexDir = createTempDir("moreterms");
-    TestUtil.unzip(getDataFile(moreTermsIndex), oldIndexDir);
-    Directory dir = newFSDirectory(oldIndexDir);
-    // TODO: more tests
-    TestUtil.checkIndex(dir);
-    dir.close();
-  }
-
-  public static final String dvUpdatesIndex = "dvupdates.48.zip";
-
-  private void assertNumericDocValues(AtomicReader r, String f, String cf) throws IOException {
-    NumericDocValues ndvf = r.getNumericDocValues(f);
-    NumericDocValues ndvcf = r.getNumericDocValues(cf);
-    for (int i = 0; i < r.maxDoc(); i++) {
-      assertEquals(ndvcf.get(i), ndvf.get(i)*2);
-    }
-  }
-  
-  private void assertBinaryDocValues(AtomicReader r, String f, String cf) throws IOException {
-    BinaryDocValues bdvf = r.getBinaryDocValues(f);
-    BinaryDocValues bdvcf = r.getBinaryDocValues(cf);
-    for (int i = 0; i < r.maxDoc(); i++) {
-      assertEquals(TestBinaryDocValuesUpdates.getValue(bdvcf, i), TestBinaryDocValuesUpdates.getValue(bdvf, i)*2);
-    }
-  }
-  
-  private void verifyDocValues(Directory dir) throws IOException {
-    DirectoryReader reader = DirectoryReader.open(dir);
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
-      assertNumericDocValues(r, "ndv1", "ndv1_c");
-      assertNumericDocValues(r, "ndv2", "ndv2_c");
-      assertBinaryDocValues(r, "bdv1", "bdv1_c");
-      assertBinaryDocValues(r, "bdv2", "bdv2_c");
-    }
-    reader.close();
-  }
-  
-  public void testDocValuesUpdates() throws Exception {
-    File oldIndexDir = createTempDir("dvupdates");
-    TestUtil.unzip(getDataFile(dvUpdatesIndex), oldIndexDir);
-    Directory dir = newFSDirectory(oldIndexDir);
-    
-    verifyDocValues(dir);
-    
-    // update fields and verify index
-    IndexWriterConfig conf = new IndexWriterConfig(new MockAnalyzer(random()));
-    IndexWriter writer = new IndexWriter(dir, conf);
-    updateNumeric(writer, "1", "ndv1", "ndv1_c", 300L);
-    updateNumeric(writer, "1", "ndv2", "ndv2_c", 300L);
-    updateBinary(writer, "1", "bdv1", "bdv1_c", 300L);
-    updateBinary(writer, "1", "bdv2", "bdv2_c", 300L);
-    writer.commit();
-    verifyDocValues(dir);
-    
-    // merge all segments
-    writer.forceMerge(1);
-    writer.commit();
-    verifyDocValues(dir);
-    
-    writer.close();
-    dir.close();
-  }
-  
-  // LUCENE-5907
-  public void testUpgradeWithNRTReader() throws Exception {
-    for (String name : oldNames) {
-      Directory dir = newDirectory(oldIndexDirs.get(name));
-
-      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
-                                           .setOpenMode(OpenMode.APPEND));
-      writer.addDocument(new Document());
-      DirectoryReader r = DirectoryReader.open(writer, true);
-      writer.commit();
-      r.close();
-      writer.forceMerge(1);
-      writer.commit();
-      writer.rollback();
-      new SegmentInfos().read(dir);
-      dir.close();
-    }
-  }
-
-  // LUCENE-5907
-  public void testUpgradeThenMultipleCommits() throws Exception {
-    for (String name : oldNames) {
-      Directory dir = newDirectory(oldIndexDirs.get(name));
-
-      IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random()))
-                                           .setOpenMode(OpenMode.APPEND));
-      writer.addDocument(new Document());
-      writer.commit();
-      writer.addDocument(new Document());
-      writer.commit();
-      writer.close();
-      dir.close();
-    }
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java b/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
index 7df11e1..38d23e5 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestBinaryDocValuesUpdates.java
@@ -9,13 +9,8 @@ import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.asserting.AssertingDocValuesFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40RWCodec;
-import org.apache.lucene.codecs.lucene41.Lucene41RWCodec;
-import org.apache.lucene.codecs.lucene42.Lucene42RWCodec;
-import org.apache.lucene.codecs.lucene45.Lucene45RWCodec;
 import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.codecs.lucene410.Lucene410DocValuesFormat;
 import org.apache.lucene.document.BinaryDocValuesField;
@@ -32,7 +27,6 @@ import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.TestUtil;
 import org.junit.Test;
 
@@ -55,8 +49,6 @@ import com.carrotsearch.randomizedtesting.generators.RandomPicks;
  * limitations under the License.
  */
 
-@SuppressCodecs({"Lucene40","Lucene41","Lucene42","Lucene45"})
-@SuppressWarnings("resource")
 public class TestBinaryDocValuesUpdates extends LuceneTestCase {
 
   static long getValue(BinaryDocValues bdv, int idx) {
@@ -861,38 +853,6 @@ public class TestBinaryDocValuesUpdates extends LuceneTestCase {
     dir.close();
   }
   
-  public void testUpdateOldSegments() throws Exception {
-    Codec[] oldCodecs = new Codec[] { new Lucene40RWCodec(), new Lucene41RWCodec(), new Lucene42RWCodec(), new Lucene45RWCodec() };
-    Directory dir = newDirectory();
-    
-    boolean oldValue = OLD_FORMAT_IMPERSONATION_IS_ACTIVE;
-    // create a segment with an old Codec
-    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
-    conf.setCodec(oldCodecs[random().nextInt(oldCodecs.length)]);
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
-    IndexWriter writer = new IndexWriter(dir, conf);
-    Document doc = new Document();
-    doc.add(new StringField("id", "doc", Store.NO));
-    doc.add(new BinaryDocValuesField("f", toBytes(5L)));
-    writer.addDocument(doc);
-    writer.close();
-    
-    conf = newIndexWriterConfig(new MockAnalyzer(random()));
-    writer = new IndexWriter(dir, conf);
-    writer.updateBinaryDocValue(new Term("id", "doc"), "f", toBytes(4L));
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = false;
-    try {
-      writer.close();
-      fail("should not have succeeded to update a segment written with an old Codec");
-    } catch (UnsupportedOperationException e) {
-      writer.rollback(); 
-    } finally {
-      OLD_FORMAT_IMPERSONATION_IS_ACTIVE = oldValue;
-    }
-    
-    dir.close();
-  }
-  
   public void testStressMultiThreading() throws Exception {
     final Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java b/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
index 3c8f892..e0a0a88 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
@@ -27,9 +27,6 @@ import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.lucene40.Lucene40RWCodec;
-import org.apache.lucene.codecs.lucene41.Lucene41RWCodec;
-import org.apache.lucene.codecs.lucene42.Lucene42RWCodec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.NumericDocValuesField;
@@ -850,29 +847,4 @@ public class TestCodecs extends LuceneTestCase {
     dir.close();
   }
   
-  public void testDisableImpersonation() throws Exception {
-    Codec[] oldCodecs = new Codec[] { new Lucene40RWCodec(), new Lucene41RWCodec(), new Lucene42RWCodec() };
-    Directory dir = newDirectory();
-    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
-    conf.setCodec(oldCodecs[random().nextInt(oldCodecs.length)]);
-    IndexWriter writer = new IndexWriter(dir, conf);
-    
-    Document doc = new Document();
-    doc.add(new StringField("f", "bar", Store.YES));
-    doc.add(new NumericDocValuesField("n", 18L));
-    
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = false;
-    try {
-      writer.addDocument(doc);
-      writer.close();
-      fail("should not have succeeded to impersonate an old format!");
-    } catch (UnsupportedOperationException e) {
-      writer.rollback();
-    } finally {
-      OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
-    }
-    
-    dir.close();
-  }
-  
 }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java b/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
index dcb765c..93d4fac 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestNumericDocValuesUpdates.java
@@ -12,10 +12,6 @@ import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.asserting.AssertingDocValuesFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40RWCodec;
-import org.apache.lucene.codecs.lucene41.Lucene41RWCodec;
-import org.apache.lucene.codecs.lucene42.Lucene42RWCodec;
-import org.apache.lucene.codecs.lucene45.Lucene45RWCodec;
 import org.apache.lucene.codecs.lucene410.Lucene410DocValuesFormat;
 import org.apache.lucene.codecs.lucene410.Lucene410Codec;
 import org.apache.lucene.document.BinaryDocValuesField;
@@ -854,39 +850,6 @@ public class TestNumericDocValuesUpdates extends LuceneTestCase {
   }
   
   @Test
-  public void testUpdateOldSegments() throws Exception {
-    Codec[] oldCodecs = new Codec[] { new Lucene40RWCodec(), new Lucene41RWCodec(), new Lucene42RWCodec(), new Lucene45RWCodec() };
-    Directory dir = newDirectory();
-    
-    boolean oldValue = OLD_FORMAT_IMPERSONATION_IS_ACTIVE;
-    // create a segment with an old Codec
-    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
-    conf.setCodec(oldCodecs[random().nextInt(oldCodecs.length)]);
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
-    IndexWriter writer = new IndexWriter(dir, conf);
-    Document doc = new Document();
-    doc.add(new StringField("id", "doc", Store.NO));
-    doc.add(new NumericDocValuesField("f", 5));
-    writer.addDocument(doc);
-    writer.close();
-    
-    conf = newIndexWriterConfig(new MockAnalyzer(random()));
-    writer = new IndexWriter(dir, conf);
-    writer.updateNumericDocValue(new Term("id", "doc"), "f", 4L);
-    OLD_FORMAT_IMPERSONATION_IS_ACTIVE = false;
-    try {
-      writer.close();
-      fail("should not have succeeded to update a segment written with an old Codec");
-    } catch (UnsupportedOperationException e) {
-      writer.rollback(); 
-    } finally {
-      OLD_FORMAT_IMPERSONATION_IS_ACTIVE = oldValue;
-    }
-    
-    dir.close();
-  }
-  
-  @Test
   public void testStressMultiThreading() throws Exception {
     final Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
diff --git a/lucene/core/src/test/org/apache/lucene/index/dvupdates.48.zip b/lucene/core/src/test/org/apache/lucene/index/dvupdates.48.zip
deleted file mode 100755
index 900c670..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/dvupdates.48.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.40.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.40.cfs.zip
deleted file mode 100644
index 4974749..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.40.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.40.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.40.nocfs.zip
deleted file mode 100644
index 9699080..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.40.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.40.optimized.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.40.optimized.cfs.zip
deleted file mode 100644
index 209c436..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.40.optimized.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.40.optimized.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.40.optimized.nocfs.zip
deleted file mode 100644
index 0eaffd0..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.40.optimized.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.41.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.41.cfs.zip
deleted file mode 100644
index da2745e..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.41.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.41.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.41.nocfs.zip
deleted file mode 100644
index c056bcb..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.41.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.42.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.42.cfs.zip
deleted file mode 100644
index 5945fe5..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.42.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.42.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.42.nocfs.zip
deleted file mode 100644
index 11de1f1..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.42.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.45.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.45.cfs.zip
deleted file mode 100644
index 10a8a1a..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.45.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.45.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.45.nocfs.zip
deleted file mode 100644
index 7825e2a..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.45.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.461.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.461.cfs.zip
deleted file mode 100644
index 8f18185..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.461.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.461.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.461.nocfs.zip
deleted file mode 100644
index cf0173c..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.461.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.49.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.49.cfs.zip
deleted file mode 100644
index b77750c..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.49.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/index.49.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/index.49.nocfs.zip
deleted file mode 100644
index b6af927..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/index.49.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/moreterms.40.zip b/lucene/core/src/test/org/apache/lucene/index/moreterms.40.zip
deleted file mode 100644
index 53ad7ce..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/moreterms.40.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.19.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.19.cfs.zip
deleted file mode 100644
index 4fd9b32..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.19.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.19.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.19.nocfs.zip
deleted file mode 100644
index e0d9142..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.19.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.20.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.20.cfs.zip
deleted file mode 100644
index 4b931ae..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.20.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.20.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.20.nocfs.zip
deleted file mode 100644
index 1275cdf..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.20.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.21.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.21.cfs.zip
deleted file mode 100644
index 473c138..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.21.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.21.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.21.nocfs.zip
deleted file mode 100644
index d0582d0..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.21.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.22.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.22.cfs.zip
deleted file mode 100644
index 1236307..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.22.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.22.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.22.nocfs.zip
deleted file mode 100644
index 216ddf3..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.22.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.23.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.23.cfs.zip
deleted file mode 100644
index b5fdeef..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.23.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.23.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.23.nocfs.zip
deleted file mode 100644
index 9137ae6..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.23.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.24.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.24.cfs.zip
deleted file mode 100644
index 2c666a9..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.24.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.24.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.24.nocfs.zip
deleted file mode 100644
index c223875..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.24.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.29.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.29.cfs.zip
deleted file mode 100644
index c694c78..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.29.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.29.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.29.nocfs.zip
deleted file mode 100644
index 298cab7..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.29.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.30.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.30.cfs.zip
deleted file mode 100644
index d5978c8..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.30.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.30.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.30.nocfs.zip
deleted file mode 100644
index 28cd83b..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.30.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.31.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.31.cfs.zip
deleted file mode 100644
index 8f123a7..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.31.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.31.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.31.nocfs.zip
deleted file mode 100644
index 21434e1..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.31.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.32.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.32.cfs.zip
deleted file mode 100644
index eff3153..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.32.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.32.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.32.nocfs.zip
deleted file mode 100644
index 0b345da..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.32.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.34.cfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.34.cfs.zip
deleted file mode 100644
index 257e9d8..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.34.cfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/index/unsupported.34.nocfs.zip b/lucene/core/src/test/org/apache/lucene/index/unsupported.34.nocfs.zip
deleted file mode 100644
index 935d6a1..0000000
Binary files a/lucene/core/src/test/org/apache/lucene/index/unsupported.34.nocfs.zip and /dev/null differ
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java b/lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
index 712514b..fe61a2a 100644
--- a/lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
+++ b/lucene/core/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
@@ -55,7 +55,7 @@ public class TestFileSwitchDirectory extends BaseDirectoryTestCase {
     IndexWriter writer = new IndexWriter(
         fsd,
         new IndexWriterConfig(new MockAnalyzer(random())).
-            setMergePolicy(newLogMergePolicy(false)).setCodec(Codec.forName("Lucene40")).setUseCompoundFile(false)
+            setMergePolicy(newLogMergePolicy(false)).setCodec(Codec.forName("Lucene410")).setUseCompoundFile(false)
     );
     TestIndexWriterReader.createIndexNoClose(true, "ram", writer);
     IndexReader reader = DirectoryReader.open(writer, true);
diff --git a/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java b/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java
index af67fb8..304cafa 100644
--- a/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java
+++ b/lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCacheVsDocValues.java
@@ -25,7 +25,6 @@ import java.util.List;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.lucene42.Lucene42DocValuesFormat;
 import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -211,6 +210,8 @@ public class TestFieldCacheVsDocValues extends LuceneTestCase {
     d.close();
   }
 
+  private static final int LARGE_BINARY_FIELD_LENGTH = (1 << 15) - 2;
+
   // TODO: get this out of here and into the deprecated codecs (4.0, 4.2)
   public void testHugeBinaryValueLimit() throws Exception {
     // We only test DVFormats that have a limit
@@ -226,7 +227,7 @@ public class TestFieldCacheVsDocValues extends LuceneTestCase {
       // Sometimes make all values fixed length since some
       // codecs have different code paths for this:
       numDocs = TestUtil.nextInt(random(), 10, 20);
-      fixedLength = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;
+      fixedLength = LARGE_BINARY_FIELD_LENGTH;
     } else {
       numDocs = TestUtil.nextInt(random(), 100, 200);
     }
@@ -243,9 +244,9 @@ public class TestFieldCacheVsDocValues extends LuceneTestCase {
       if (doFixed) {
         numBytes = fixedLength;
       } else if (docID == 0 || random().nextInt(5) == 3) {
-        numBytes = Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH;
+        numBytes = LARGE_BINARY_FIELD_LENGTH;
       } else {
-        numBytes = TestUtil.nextInt(random(), 1, Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);
+        numBytes = TestUtil.nextInt(random(), 1, LARGE_BINARY_FIELD_LENGTH);
       }
       totalBytes += numBytes;
       if (totalBytes > 5 * 1024*1024) {
diff --git a/lucene/module-build.xml b/lucene/module-build.xml
index df0e9d7..cc5ca70 100644
--- a/lucene/module-build.xml
+++ b/lucene/module-build.xml
@@ -433,6 +433,28 @@
     <property name="codecs-javadocs.uptodate" value="true"/>
   </target>
 
+  <property name="backward-codecs.jar" value="${common.dir}/build/backward-codecs/lucene-backward-codecs-${version}.jar"/>
+  <target name="check-backward-codecs-uptodate" unless="backward-codecs.uptodate">
+    <module-uptodate name="backward-codecs" jarfile="${backward-codecs.jar}" property="backward-codecs.uptodate"/>
+  </target>
+  <target name="jar-backward-codecs" unless="backward-codecs.uptodate" depends="check-backward-codecs-uptodate">
+    <ant dir="${common.dir}/backward-codecs" target="jar-core" inheritall="false">
+      <propertyset refid="uptodate.and.compiled.properties"/>
+    </ant>
+    <property name="backward-codecs.uptodate" value="true"/>
+  </target>
+
+  <property name="backward-codecs-javadoc.jar" value="${common.dir}/build/backward-codecs/lucene-backward-codecs-${version}-javadoc.jar"/>
+  <target name="check-backward-codecs-javadocs-uptodate" unless="backward-codecs-javadocs.uptodate">
+    <module-uptodate name="backward-codecs" jarfile="${backward-codecs-javadoc.jar}" property="backward-codecs-javadocs.uptodate"/>
+  </target>
+  <target name="javadocs-backward-codecs" unless="backward-codecs-javadocs.uptodate" depends="check-backward-codecs-javadocs-uptodate">
+    <ant dir="${common.dir}/backward-codecs" target="javadocs" inheritAll="false">
+      <propertyset refid="uptodate.and.compiled.properties"/>
+    </ant>
+    <property name="backward-codecs-javadocs.uptodate" value="true"/>
+  </target>
+
   <property name="expressions.jar" value="${common.dir}/build/expressions/lucene-expressions-${version}.jar"/>
   <target name="check-expressions-uptodate" unless="expressions.uptodate">
     <module-uptodate name="expressions" jarfile="${expressions.jar}" property="expressions.uptodate"/>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
deleted file mode 100644
index 2f63685..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesWriter.java
+++ /dev/null
@@ -1,547 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.TreeSet;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.MissingOrdRemapper;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosReader.LegacyDocValuesType;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-class Lucene40DocValuesWriter extends DocValuesConsumer {
-  private final Directory dir;
-  private final SegmentWriteState state;
-  private final String legacyKey;
-  private final static String segmentSuffix = "dv";
-
-  // note: intentionally ignores seg suffix
-  Lucene40DocValuesWriter(SegmentWriteState state, String filename, String legacyKey) throws IOException {
-    this.state = state;
-    this.legacyKey = legacyKey;
-    this.dir = new CompoundFileDirectory(state.directory, filename, state.context, true);
-  }
-  
-  @Override
-  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
-    // examine the values to determine best type to use
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    for (Number n : values) {
-      long v = n == null ? 0 : n.longValue();
-      minValue = Math.min(minValue, v);
-      maxValue = Math.max(maxValue, v);
-    }
-    
-    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    IndexOutput data = dir.createOutput(fileName, state.context);
-    boolean success = false;
-    try {
-      if (minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 4) {
-        // fits in a byte[], would be more than 4bpv, just write byte[]
-        addBytesField(field, data, values);
-      } else if (minValue >= Short.MIN_VALUE && maxValue <= Short.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 8) {
-        // fits in a short[], would be more than 8bpv, just write short[]
-        addShortsField(field, data, values);
-      } else if (minValue >= Integer.MIN_VALUE && maxValue <= Integer.MAX_VALUE && PackedInts.bitsRequired(maxValue-minValue) > 16) {
-        // fits in a int[], would be more than 16bpv, just write int[]
-        addIntsField(field, data, values);
-      } else {
-        addVarIntsField(field, data, values, minValue, maxValue);
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data);
-      } else {
-        IOUtils.closeWhileHandlingException(data);
-      }
-    }
-  }
-
-  private void addBytesField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_8.name());
-    CodecUtil.writeHeader(output, 
-                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
-                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    output.writeInt(1); // size
-    for (Number n : values) {
-      output.writeByte(n == null ? 0 : n.byteValue());
-    }
-  }
-  
-  private void addShortsField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_16.name());
-    CodecUtil.writeHeader(output, 
-                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
-                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    output.writeInt(2); // size
-    for (Number n : values) {
-      output.writeShort(n == null ? 0 : n.shortValue());
-    }
-  }
-  
-  private void addIntsField(FieldInfo field, IndexOutput output, Iterable<Number> values) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.FIXED_INTS_32.name());
-    CodecUtil.writeHeader(output, 
-                          Lucene40DocValuesFormat.INTS_CODEC_NAME, 
-                          Lucene40DocValuesFormat.INTS_VERSION_CURRENT);
-    output.writeInt(4); // size
-    for (Number n : values) {
-      output.writeInt(n == null ? 0 : n.intValue());
-    }
-  }
-  
-  private void addVarIntsField(FieldInfo field, IndexOutput output, Iterable<Number> values, long minValue, long maxValue) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.VAR_INTS.name());
-    
-    CodecUtil.writeHeader(output, 
-                          Lucene40DocValuesFormat.VAR_INTS_CODEC_NAME, 
-                          Lucene40DocValuesFormat.VAR_INTS_VERSION_CURRENT);
-    
-    final long delta = maxValue - minValue;
-    
-    if (delta < 0) {
-      // writes longs
-      output.writeByte(Lucene40DocValuesFormat.VAR_INTS_FIXED_64);
-      for (Number n : values) {
-        output.writeLong(n == null ? 0 : n.longValue());
-      }
-    } else {
-      // writes packed ints
-      output.writeByte(Lucene40DocValuesFormat.VAR_INTS_PACKED);
-      output.writeLong(minValue);
-      output.writeLong(0 - minValue); // default value (representation of 0)
-      PackedInts.Writer writer = PackedInts.getWriter(output, 
-                                                      state.segmentInfo.getDocCount(),
-                                                      PackedInts.bitsRequired(delta), 
-                                                      PackedInts.DEFAULT);
-      for (Number n : values) {
-        long v = n == null ? 0 : n.longValue();
-        writer.add(v - minValue);
-      }
-      writer.finish();
-    }
-  }
-
-  @Override
-  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
-    // examine the values to determine best type to use
-    HashSet<BytesRef> uniqueValues = new HashSet<>();
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    for (BytesRef b : values) {
-      if (b == null) {
-        b = new BytesRef(); // 4.0 doesnt distinguish
-      }
-      if (b.length > Lucene40DocValuesFormat.MAX_BINARY_FIELD_LENGTH) {
-        throw new IllegalArgumentException("DocValuesField \"" + field.name + "\" is too large, must be <= " + Lucene40DocValuesFormat.MAX_BINARY_FIELD_LENGTH);
-      }
-      minLength = Math.min(minLength, b.length);
-      maxLength = Math.max(maxLength, b.length);
-      if (uniqueValues != null) {
-        if (uniqueValues.add(BytesRef.deepCopyOf(b))) {
-          if (uniqueValues.size() > 256) {
-            uniqueValues = null;
-          }
-        }
-      }
-    }
-    
-    int maxDoc = state.segmentInfo.getDocCount();
-    final boolean fixed = minLength == maxLength;
-    final boolean dedup = uniqueValues != null && uniqueValues.size() * 2 < maxDoc;
-    
-    if (dedup) {
-      // we will deduplicate and deref values
-      boolean success = false;
-      IndexOutput data = null;
-      IndexOutput index = null;
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-      String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-      try {
-        data = dir.createOutput(dataName, state.context);
-        index = dir.createOutput(indexName, state.context);
-        if (fixed) {
-          addFixedDerefBytesField(field, data, index, values, minLength);
-        } else {
-          addVarDerefBytesField(field, data, index, values);
-        }
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(data, index);
-        } else {
-          IOUtils.closeWhileHandlingException(data, index);
-        }
-      }
-    } else {
-      // we dont deduplicate, just write values straight
-      if (fixed) {
-        // fixed byte[]
-        String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-        IndexOutput data = dir.createOutput(fileName, state.context);
-        boolean success = false;
-        try {
-          addFixedStraightBytesField(field, data, values, minLength);
-          success = true;
-        } finally {
-          if (success) {
-            IOUtils.close(data);
-          } else {
-            IOUtils.closeWhileHandlingException(data);
-          }
-        }
-      } else {
-        // variable byte[]
-        boolean success = false;
-        IndexOutput data = null;
-        IndexOutput index = null;
-        String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-        String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-        try {
-          data = dir.createOutput(dataName, state.context);
-          index = dir.createOutput(indexName, state.context);
-          addVarStraightBytesField(field, data, index, values);
-          success = true;
-        } finally {
-          if (success) {
-            IOUtils.close(data, index);
-          } else {
-            IOUtils.closeWhileHandlingException(data, index);
-          }
-        }
-      }
-    }
-  }
-  
-  private void addFixedStraightBytesField(FieldInfo field, IndexOutput output, Iterable<BytesRef> values, int length) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_STRAIGHT.name());
-
-    CodecUtil.writeHeader(output, 
-                          Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_CODEC_NAME,
-                          Lucene40DocValuesFormat.BYTES_FIXED_STRAIGHT_VERSION_CURRENT);
-    
-    output.writeInt(length);
-    for (BytesRef v : values) {
-      if (v != null) {
-        output.writeBytes(v.bytes, v.offset, v.length);
-      }
-    }
-  }
-  
-  // NOTE: 4.0 file format docs are crazy/wrong here...
-  private void addVarStraightBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_STRAIGHT.name());
-    
-    CodecUtil.writeHeader(data, 
-                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_DAT,
-                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
-    
-    CodecUtil.writeHeader(index, 
-                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_CODEC_NAME_IDX,
-                          Lucene40DocValuesFormat.BYTES_VAR_STRAIGHT_VERSION_CURRENT);
-    
-    /* values */
-    
-    final long startPos = data.getFilePointer();
-    
-    for (BytesRef v : values) {
-      if (v != null) {
-        data.writeBytes(v.bytes, v.offset, v.length);
-      }
-    }
-    
-    /* addresses */
-    
-    final long maxAddress = data.getFilePointer() - startPos;
-    index.writeVLong(maxAddress);
-    
-    final int maxDoc = state.segmentInfo.getDocCount();
-    assert maxDoc != Integer.MAX_VALUE; // unsupported by the 4.0 impl
-    
-    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc+1, PackedInts.bitsRequired(maxAddress), PackedInts.DEFAULT);
-    long currentPosition = 0;
-    for (BytesRef v : values) {
-      w.add(currentPosition);
-      if (v != null) {
-        currentPosition += v.length;
-      }
-    }
-    // write sentinel
-    assert currentPosition == maxAddress;
-    w.add(currentPosition);
-    w.finish();
-  }
-  
-  private void addFixedDerefBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, int length) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_DEREF.name());
-
-    CodecUtil.writeHeader(data, 
-                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_DAT,
-                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
-    
-    CodecUtil.writeHeader(index, 
-                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_CODEC_NAME_IDX,
-                          Lucene40DocValuesFormat.BYTES_FIXED_DEREF_VERSION_CURRENT);
-    
-    // deduplicate
-    TreeSet<BytesRef> dictionary = new TreeSet<>();
-    for (BytesRef v : values) {
-      dictionary.add(v == null ? new BytesRef() : BytesRef.deepCopyOf(v));
-    }
-    
-    /* values */
-    data.writeInt(length);
-    for (BytesRef v : dictionary) {
-      data.writeBytes(v.bytes, v.offset, v.length);
-    }
-    
-    /* ordinals */
-    int valueCount = dictionary.size();
-    assert valueCount > 0;
-    index.writeInt(valueCount);
-    final int maxDoc = state.segmentInfo.getDocCount();
-    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
-
-    for (BytesRef v : values) {
-      if (v == null) {
-        v = new BytesRef();
-      }
-      int ord = dictionary.headSet(v).size();
-      w.add(ord);
-    }
-    w.finish();
-  }
-  
-  private void addVarDerefBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_DEREF.name());
-
-    CodecUtil.writeHeader(data, 
-                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_DAT,
-                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
-    
-    CodecUtil.writeHeader(index, 
-                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_CODEC_NAME_IDX,
-                          Lucene40DocValuesFormat.BYTES_VAR_DEREF_VERSION_CURRENT);
-    
-    // deduplicate
-    TreeSet<BytesRef> dictionary = new TreeSet<>();
-    for (BytesRef v : values) {
-      dictionary.add(v == null ? new BytesRef() : BytesRef.deepCopyOf(v));
-    }
-    
-    /* values */
-    long startPosition = data.getFilePointer();
-    long currentAddress = 0;
-    HashMap<BytesRef,Long> valueToAddress = new HashMap<>();
-    for (BytesRef v : dictionary) {
-      currentAddress = data.getFilePointer() - startPosition;
-      valueToAddress.put(v, currentAddress);
-      writeVShort(data, v.length);
-      data.writeBytes(v.bytes, v.offset, v.length);
-    }
-    
-    /* ordinals */
-    long totalBytes = data.getFilePointer() - startPosition;
-    index.writeLong(totalBytes);
-    final int maxDoc = state.segmentInfo.getDocCount();
-    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(currentAddress), PackedInts.DEFAULT);
-
-    for (BytesRef v : values) {
-      w.add(valueToAddress.get(v == null ? new BytesRef() : v));
-    }
-    w.finish();
-  }
-  
-  // the little vint encoding used for var-deref
-  private static void writeVShort(IndexOutput o, int i) throws IOException {
-    assert i >= 0 && i <= Short.MAX_VALUE;
-    if (i < 128) {
-      o.writeByte((byte)i);
-    } else {
-      o.writeByte((byte) (0x80 | (i >> 8)));
-      o.writeByte((byte) (i & 0xff));
-    }
-  }
-
-  @Override
-  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    // examine the values to determine best type to use
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    for (BytesRef b : values) {
-      minLength = Math.min(minLength, b.length);
-      maxLength = Math.max(maxLength, b.length);
-    }
-    
-    // but dont use fixed if there are missing values (we are simulating how lucene40 wrote dv...)
-    boolean anyMissing = false;
-    for (Number n : docToOrd) {
-      if (n.longValue() == -1) {
-        anyMissing = true;
-        break;
-      }
-    }
-    
-    boolean success = false;
-    IndexOutput data = null;
-    IndexOutput index = null;
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "dat");
-    String indexName = IndexFileNames.segmentFileName(state.segmentInfo.name + "_" + Integer.toString(field.number), segmentSuffix, "idx");
-    
-    try {
-      data = dir.createOutput(dataName, state.context);
-      index = dir.createOutput(indexName, state.context);
-      if (minLength == maxLength && !anyMissing) {
-        // fixed byte[]
-        addFixedSortedBytesField(field, data, index, values, docToOrd, minLength);
-      } else {
-        // var byte[]
-        // three cases for simulating the old writer:
-        // 1. no missing
-        // 2. missing (and empty string in use): remap ord=-1 -> ord=0
-        // 3. missing (and empty string not in use): remap all ords +1, insert empty string into values
-        if (!anyMissing) {
-          addVarSortedBytesField(field, data, index, values, docToOrd);
-        } else if (minLength == 0) {
-          addVarSortedBytesField(field, data, index, values, MissingOrdRemapper.mapMissingToOrd0(docToOrd));
-        } else {
-          addVarSortedBytesField(field, data, index, MissingOrdRemapper.insertEmptyValue(values), MissingOrdRemapper.mapAllOrds(docToOrd));
-        }
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, index);
-      } else {
-        IOUtils.closeWhileHandlingException(data, index);
-      }
-    }
-  }
-  
-  private void addFixedSortedBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, Iterable<Number> docToOrd, int length) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_FIXED_SORTED.name());
-
-    CodecUtil.writeHeader(data, 
-                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_DAT,
-                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
-    
-    CodecUtil.writeHeader(index, 
-                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_CODEC_NAME_IDX,
-                          Lucene40DocValuesFormat.BYTES_FIXED_SORTED_VERSION_CURRENT);
-    
-    /* values */
-    
-    data.writeInt(length);
-    int valueCount = 0;
-    for (BytesRef v : values) {
-      data.writeBytes(v.bytes, v.offset, v.length);
-      valueCount++;
-    }
-    
-    /* ordinals */
-    
-    index.writeInt(valueCount);
-    int maxDoc = state.segmentInfo.getDocCount();
-    assert valueCount > 0;
-    final PackedInts.Writer w = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
-    for (Number n : docToOrd) {
-      w.add(n.longValue());
-    }
-    w.finish();
-  }
-  
-  private void addVarSortedBytesField(FieldInfo field, IndexOutput data, IndexOutput index, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    field.putAttribute(legacyKey, LegacyDocValuesType.BYTES_VAR_SORTED.name());
-    
-    CodecUtil.writeHeader(data, 
-                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_DAT,
-                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
-
-    CodecUtil.writeHeader(index, 
-                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_CODEC_NAME_IDX,
-                          Lucene40DocValuesFormat.BYTES_VAR_SORTED_VERSION_CURRENT);
-
-    /* values */
-    
-    final long startPos = data.getFilePointer();
-    
-    int valueCount = 0;
-    for (BytesRef v : values) {
-      data.writeBytes(v.bytes, v.offset, v.length);
-      valueCount++;
-    }
-    
-    /* addresses */
-    
-    final long maxAddress = data.getFilePointer() - startPos;
-    index.writeLong(maxAddress);
-    
-    assert valueCount != Integer.MAX_VALUE; // unsupported by the 4.0 impl
-    
-    final PackedInts.Writer w = PackedInts.getWriter(index, valueCount+1, PackedInts.bitsRequired(maxAddress), PackedInts.DEFAULT);
-    long currentPosition = 0;
-    for (BytesRef v : values) {
-      w.add(currentPosition);
-      currentPosition += v.length;
-    }
-    // write sentinel
-    assert currentPosition == maxAddress;
-    w.add(currentPosition);
-    w.finish();
-    
-    /* ordinals */
-    
-    final int maxDoc = state.segmentInfo.getDocCount();
-    assert valueCount > 0;
-    final PackedInts.Writer ords = PackedInts.getWriter(index, maxDoc, PackedInts.bitsRequired(valueCount-1), PackedInts.DEFAULT);
-    for (Number n : docToOrd) {
-      ords.add(n.longValue());
-    }
-    ords.finish();
-  }
-  
-  @Override
-  public void addSortedNumericField(FieldInfo field, Iterable<Number> docToValueCount, Iterable<Number> values) throws IOException {
-    throw new UnsupportedOperationException("Lucene 4.0 does not support SortedNumeric docvalues");
-  }
-  
-  @Override
-  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrdCount, Iterable<Number> ords) throws IOException {
-    throw new UnsupportedOperationException("Lucene 4.0 does not support SortedSet docvalues");
-  }
-
-  @Override
-  public void close() throws IOException {
-    dir.close();
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java
deleted file mode 100644
index 125e0b9..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java
+++ /dev/null
@@ -1,104 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosReader.LegacyDocValuesType;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.0 FieldInfos writer.
- * 
- * @see Lucene40FieldInfosFormat
- * @lucene.experimental
- */
-@Deprecated
-public class Lucene40FieldInfosWriter extends FieldInfosWriter {
-
-  /** Sole constructor. */
-  public Lucene40FieldInfosWriter() {
-  }
-  
-  @Override
-  public void write(Directory directory, String segmentName, String segmentSuffix, FieldInfos infos, IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene40FieldInfosFormat.FIELD_INFOS_EXTENSION);
-    IndexOutput output = directory.createOutput(fileName, context);
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(output, Lucene40FieldInfosFormat.CODEC_NAME, Lucene40FieldInfosFormat.FORMAT_CURRENT);
-      output.writeVInt(infos.size());
-      for (FieldInfo fi : infos) {
-        IndexOptions indexOptions = fi.getIndexOptions();
-        byte bits = 0x0;
-        if (fi.hasVectors()) bits |= Lucene40FieldInfosFormat.STORE_TERMVECTOR;
-        if (fi.omitsNorms()) bits |= Lucene40FieldInfosFormat.OMIT_NORMS;
-        if (fi.hasPayloads()) bits |= Lucene40FieldInfosFormat.STORE_PAYLOADS;
-        if (fi.isIndexed()) {
-          bits |= Lucene40FieldInfosFormat.IS_INDEXED;
-          assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
-          if (indexOptions == IndexOptions.DOCS_ONLY) {
-            bits |= Lucene40FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
-            bits |= Lucene40FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
-            bits |= Lucene40FieldInfosFormat.OMIT_POSITIONS;
-          }
-        }
-        output.writeString(fi.name);
-        output.writeVInt(fi.number);
-        output.writeByte(bits);
-
-        // pack the DV types in one byte
-        final byte dv = docValuesByte(fi.getDocValuesType(), fi.getAttribute(Lucene40FieldInfosReader.LEGACY_DV_TYPE_KEY));
-        final byte nrm = docValuesByte(fi.getNormType(), fi.getAttribute(Lucene40FieldInfosReader.LEGACY_NORM_TYPE_KEY));
-        assert (dv & (~0xF)) == 0 && (nrm & (~0x0F)) == 0;
-        byte val = (byte) (0xff & ((nrm << 4) | dv));
-        output.writeByte(val);
-        output.writeStringStringMap(fi.attributes());
-      }
-      success = true;
-    } finally {
-      if (success) {
-        output.close();
-      } else {
-        IOUtils.closeWhileHandlingException(output);
-      }
-    }
-  }
-  
-  /** 4.0-style docvalues byte */
-  public byte docValuesByte(DocValuesType type, String legacyTypeAtt) {
-    if (type == null) {
-      assert legacyTypeAtt == null;
-      return 0;
-    } else {
-      assert legacyTypeAtt != null;
-      return (byte) LegacyDocValuesType.valueOf(legacyTypeAtt).ordinal();
-    }
-  }  
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
deleted file mode 100644
index 2c489a4..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
+++ /dev/null
@@ -1,321 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Consumes doc & freq, writing them using the current
- *  index file format */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PushPostingsWriterBase;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsEnum;  // javadocs
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Concrete class that writes the 4.0 frq/prx postings format.
- * 
- * @see Lucene40PostingsFormat
- * @lucene.experimental 
- */
-public final class Lucene40PostingsWriter extends PushPostingsWriterBase {
-
-  final IndexOutput freqOut;
-  final IndexOutput proxOut;
-  final Lucene40SkipListWriter skipListWriter;
-  /** Expert: The fraction of TermDocs entries stored in skip tables,
-   * used to accelerate {@link DocsEnum#advance(int)}.  Larger values result in
-   * smaller indexes, greater acceleration, but fewer accelerable cases, while
-   * smaller values result in bigger indexes, less acceleration and more
-   * accelerable cases. More detailed experiments would be useful here. */
-  static final int DEFAULT_SKIP_INTERVAL = 16;
-  final int skipInterval;
-  
-  /**
-   * Expert: minimum docFreq to write any skip data at all
-   */
-  final int skipMinimum;
-
-  /** Expert: The maximum number of skip levels. Smaller values result in 
-   * slightly smaller indexes, but slower skipping in big posting lists.
-   */
-  final int maxSkipLevels = 10;
-  final int totalNumDocs;
-
-  // Starts a new term
-  long freqStart;
-  long proxStart;
-  int lastPayloadLength;
-  int lastOffsetLength;
-  int lastPosition;
-  int lastOffset;
-
-  final static StandardTermState emptyState = new StandardTermState();
-  StandardTermState lastState;
-
-  // private String segment;
-
-  /** Creates a {@link Lucene40PostingsWriter}, with the
-   *  {@link #DEFAULT_SKIP_INTERVAL}. */
-  public Lucene40PostingsWriter(SegmentWriteState state) throws IOException {
-    this(state, DEFAULT_SKIP_INTERVAL);
-  }
-  
-  /** Creates a {@link Lucene40PostingsWriter}, with the
-   *  specified {@code skipInterval}. */
-  public Lucene40PostingsWriter(SegmentWriteState state, int skipInterval) throws IOException {
-    super();
-    this.skipInterval = skipInterval;
-    this.skipMinimum = skipInterval; /* set to the same for now */
-    // this.segment = state.segmentName;
-    String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION);
-    freqOut = state.directory.createOutput(fileName, state.context);
-    boolean success = false;
-    IndexOutput proxOut = null;
-    try {
-      CodecUtil.writeHeader(freqOut, Lucene40PostingsReader.FRQ_CODEC, Lucene40PostingsReader.VERSION_CURRENT);
-      // TODO: this is a best effort, if one of these fields has no postings
-      // then we make an empty prx file, same as if we are wrapped in 
-      // per-field postingsformat. maybe... we shouldn't
-      // bother w/ this opto?  just create empty prx file...?
-      if (state.fieldInfos.hasProx()) {
-        // At least one field does not omit TF, so create the
-        // prox file
-        fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION);
-        proxOut = state.directory.createOutput(fileName, state.context);
-        CodecUtil.writeHeader(proxOut, Lucene40PostingsReader.PRX_CODEC, Lucene40PostingsReader.VERSION_CURRENT);
-      } else {
-        // Every field omits TF so we will write no prox file
-        proxOut = null;
-      }
-      this.proxOut = proxOut;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(freqOut, proxOut);
-      }
-    }
-
-    totalNumDocs = state.segmentInfo.getDocCount();
-
-    skipListWriter = new Lucene40SkipListWriter(skipInterval,
-                                               maxSkipLevels,
-                                               totalNumDocs,
-                                               freqOut,
-                                               proxOut);
-  }
-
-  @Override
-  public void init(IndexOutput termsOut) throws IOException {
-    CodecUtil.writeHeader(termsOut, Lucene40PostingsReader.TERMS_CODEC, Lucene40PostingsReader.VERSION_CURRENT);
-    termsOut.writeInt(skipInterval);                // write skipInterval
-    termsOut.writeInt(maxSkipLevels);               // write maxSkipLevels
-    termsOut.writeInt(skipMinimum);                 // write skipMinimum
-  }
-
-  @Override
-  public BlockTermState newTermState() {
-    return new StandardTermState();
-  }
-
-  @Override
-  public void startTerm() {
-    freqStart = freqOut.getFilePointer();
-    //if (DEBUG) System.out.println("SPW: startTerm freqOut.fp=" + freqStart);
-    if (proxOut != null) {
-      proxStart = proxOut.getFilePointer();
-    }
-    // force first payload to write its length
-    lastPayloadLength = -1;
-    // force first offset to write its length
-    lastOffsetLength = -1;
-    skipListWriter.resetSkip();
-  }
-
-  // Currently, this instance is re-used across fields, so
-  // our parent calls setField whenever the field changes
-  @Override
-  public int setField(FieldInfo fieldInfo) {
-    super.setField(fieldInfo);
-    //System.out.println("SPW: setField");
-    /*
-    if (BlockTreeTermsWriter.DEBUG && fieldInfo.name.equals("id")) {
-      DEBUG = true;
-    } else {
-      DEBUG = false;
-    }
-    */
-
-    lastState = emptyState;
-    //System.out.println("  set init blockFreqStart=" + freqStart);
-    //System.out.println("  set init blockProxStart=" + proxStart);
-    return 0;
-  }
-
-  int lastDocID;
-  int df;
-
-  @Override
-  public void startDoc(int docID, int termDocFreq) throws IOException {
-    // if (DEBUG) System.out.println("SPW:   startDoc seg=" + segment + " docID=" + docID + " tf=" + termDocFreq + " freqOut.fp=" + freqOut.getFilePointer());
-
-    final int delta = docID - lastDocID;
-    
-    if (docID < 0 || (df > 0 && delta <= 0)) {
-      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " ) (freqOut: " + freqOut + ")");
-    }
-
-    if ((++df % skipInterval) == 0) {
-      skipListWriter.setSkipData(lastDocID, writePayloads, lastPayloadLength, writeOffsets, lastOffsetLength);
-      skipListWriter.bufferSkip(df);
-    }
-
-    assert docID < totalNumDocs: "docID=" + docID + " totalNumDocs=" + totalNumDocs;
-
-    lastDocID = docID;
-    if (indexOptions == IndexOptions.DOCS_ONLY) {
-      freqOut.writeVInt(delta);
-    } else if (1 == termDocFreq) {
-      freqOut.writeVInt((delta<<1) | 1);
-    } else {
-      freqOut.writeVInt(delta<<1);
-      freqOut.writeVInt(termDocFreq);
-    }
-
-    lastPosition = 0;
-    lastOffset = 0;
-  }
-
-  /** Add a new position & payload */
-  @Override
-  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
-    //if (DEBUG) System.out.println("SPW:     addPos pos=" + position + " payload=" + (payload == null ? "null" : (payload.length + " bytes")) + " proxFP=" + proxOut.getFilePointer());
-    assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 : "invalid indexOptions: " + indexOptions;
-    assert proxOut != null;
-
-    final int delta = position - lastPosition;
-    
-    assert delta >= 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
-
-    lastPosition = position;
-
-    int payloadLength = 0;
-
-    if (writePayloads) {
-      payloadLength = payload == null ? 0 : payload.length;
-
-      if (payloadLength != lastPayloadLength) {
-        lastPayloadLength = payloadLength;
-        proxOut.writeVInt((delta<<1)|1);
-        proxOut.writeVInt(payloadLength);
-      } else {
-        proxOut.writeVInt(delta << 1);
-      }
-    } else {
-      proxOut.writeVInt(delta);
-    }
-    
-    if (writeOffsets) {
-      // don't use startOffset - lastEndOffset, because this creates lots of negative vints for synonyms,
-      // and the numbers aren't that much smaller anyways.
-      int offsetDelta = startOffset - lastOffset;
-      int offsetLength = endOffset - startOffset;
-      assert offsetDelta >= 0 && offsetLength >= 0 : "startOffset=" + startOffset + ",lastOffset=" + lastOffset + ",endOffset=" + endOffset;
-      if (offsetLength != lastOffsetLength) {
-        proxOut.writeVInt(offsetDelta << 1 | 1);
-        proxOut.writeVInt(offsetLength);
-      } else {
-        proxOut.writeVInt(offsetDelta << 1);
-      }
-      lastOffset = startOffset;
-      lastOffsetLength = offsetLength;
-    }
-    
-    if (payloadLength > 0) {
-      proxOut.writeBytes(payload.bytes, payload.offset, payloadLength);
-    }
-  }
-
-  @Override
-  public void finishDoc() {
-  }
-
-  private static class StandardTermState extends BlockTermState {
-    public long freqStart;
-    public long proxStart;
-    public long skipOffset;
-  }
-
-  /** Called when we are done adding docs to this term */
-  @Override
-  public void finishTerm(BlockTermState _state) throws IOException {
-    StandardTermState state = (StandardTermState) _state;
-    // if (DEBUG) System.out.println("SPW: finishTerm seg=" + segment + " freqStart=" + freqStart);
-    assert state.docFreq > 0;
-
-    // TODO: wasteful we are counting this (counting # docs
-    // for this term) in two places?
-    assert state.docFreq == df;
-    state.freqStart = freqStart;
-    state.proxStart = proxStart;
-    if (df >= skipMinimum) {
-      state.skipOffset = skipListWriter.writeSkip(freqOut)-freqStart;
-    } else {
-      state.skipOffset = -1;
-    }
-    lastDocID = 0;
-    df = 0;
-  }
-
-  @Override
-  public void encodeTerm(long[] empty, DataOutput out, FieldInfo fieldInfo, BlockTermState _state, boolean absolute) throws IOException {
-    StandardTermState state = (StandardTermState)_state;
-    if (absolute) {
-      lastState = emptyState;
-    }
-    out.writeVLong(state.freqStart - lastState.freqStart);
-    if (state.skipOffset != -1) {
-      assert state.skipOffset > 0;
-      out.writeVLong(state.skipOffset);
-    }
-    if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
-      out.writeVLong(state.proxStart - lastState.proxStart);
-    }
-    lastState = state;
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      freqOut.close();
-    } finally {
-      if (proxOut != null) {
-        proxOut.close();
-      }
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java
deleted file mode 100644
index 2052ae2..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWCodec.java
+++ /dev/null
@@ -1,74 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.util.LuceneTestCase;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Read-write version of Lucene40Codec for testing */
-@SuppressWarnings("deprecation")
-public final class Lucene40RWCodec extends Lucene40Codec {
-  
-  private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat() {
-    @Override
-    public FieldInfosWriter getFieldInfosWriter() throws IOException {
-      if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-        return super.getFieldInfosWriter();
-      } else {
-        return new Lucene40FieldInfosWriter();
-      }
-    }
-  };
-  
-  private final DocValuesFormat docValues = new Lucene40RWDocValuesFormat();
-  private final NormsFormat norms = new Lucene40RWNormsFormat();
-  private final StoredFieldsFormat stored = new Lucene40RWStoredFieldsFormat();
-  private final TermVectorsFormat vectors = new Lucene40RWTermVectorsFormat();
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfos;
-  }
-
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValues;
-  }
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return stored;
-  }
-
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectors;
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java
deleted file mode 100644
index 64b7f02..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWDocValuesFormat.java
+++ /dev/null
@@ -1,42 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** Read-write version of {@link Lucene40DocValuesFormat} for testing */
-@SuppressWarnings("deprecation")
-public class Lucene40RWDocValuesFormat extends Lucene40DocValuesFormat {
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-      return super.fieldsConsumer(state);
-    } else {
-      String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
-          "dv", 
-          IndexFileNames.COMPOUND_FILE_EXTENSION);
-      return new Lucene40DocValuesWriter(state, filename, Lucene40FieldInfosReader.LEGACY_DV_TYPE_KEY);
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java
deleted file mode 100644
index c7ea1da..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWNormsFormat.java
+++ /dev/null
@@ -1,54 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** Read-write version of {@link Lucene40NormsFormat} for testing */
-@SuppressWarnings("deprecation")
-public class Lucene40RWNormsFormat extends Lucene40NormsFormat {
-
-  @Override
-  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-      return super.normsConsumer(state);
-    } else {
-      String filename = IndexFileNames.segmentFileName(state.segmentInfo.name, 
-          "nrm", 
-          IndexFileNames.COMPOUND_FILE_EXTENSION);
-      final Lucene40DocValuesWriter impl = new Lucene40DocValuesWriter(state, filename, Lucene40FieldInfosReader.LEGACY_NORM_TYPE_KEY);
-      return new NormsConsumer() {
-        @Override
-        public void addNormsField(FieldInfo field, Iterable<Number> values) throws IOException {
-          impl.addNumericField(field, values);
-        }
-        
-        @Override
-        public void close() throws IOException {
-          impl.close();
-        }
-      };
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java
deleted file mode 100644
index 3243744..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWPostingsFormat.java
+++ /dev/null
@@ -1,57 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Read-write version of {@link Lucene40PostingsFormat} for testing.
- */
-@SuppressWarnings("deprecation")
-public class Lucene40RWPostingsFormat extends Lucene40PostingsFormat {
-  
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-      return super.fieldsConsumer(state);
-    } else {
-      PostingsWriterBase docs = new Lucene40PostingsWriter(state);
-      
-      // TODO: should we make the terms index more easily
-      // pluggable?  Ie so that this codec would record which
-      // index impl was used, and switch on loading?
-      // Or... you must make a new Codec for this?
-      boolean success = false;
-      try {
-        FieldsConsumer ret = new BlockTreeTermsWriter(state, docs, minBlockSize, maxBlockSize);
-        success = true;
-        return ret;
-      } finally {
-        if (!success) {
-          docs.close();
-        }
-      }
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java
deleted file mode 100644
index f0d430f..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWStoredFieldsFormat.java
+++ /dev/null
@@ -1,41 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** 
- * Simulates writing Lucene 4.0 Stored Fields Format.
- */ 
-public class Lucene40RWStoredFieldsFormat extends Lucene40StoredFieldsFormat {
-
-  @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
-    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    } else {
-      return new Lucene40StoredFieldsWriter(directory, si.name, context);
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java
deleted file mode 100644
index 81c2ac3..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40RWTermVectorsFormat.java
+++ /dev/null
@@ -1,41 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** 
- * Simulates writing Lucene 4.0 Stored Fields Format.
- */ 
-public class Lucene40RWTermVectorsFormat extends Lucene40TermVectorsFormat {
-
-  @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
-    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-      throw new UnsupportedOperationException("this codec can only be used for reading");
-    } else {
-      return new Lucene40TermVectorsWriter(directory, segmentInfo.name, context);
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java
deleted file mode 100644
index 62bd304..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java
+++ /dev/null
@@ -1,153 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.codecs.MultiLevelSkipListWriter;
-
-
-/**
- * Implements the skip list writer for the 4.0 posting list format
- * that stores positions and payloads.
- * 
- * @see Lucene40PostingsFormat
- * @deprecated Only for reading old 4.0 segments
- */
-@Deprecated
-public class Lucene40SkipListWriter extends MultiLevelSkipListWriter {
-  private int[] lastSkipDoc;
-  private int[] lastSkipPayloadLength;
-  private int[] lastSkipOffsetLength;
-  private long[] lastSkipFreqPointer;
-  private long[] lastSkipProxPointer;
-  
-  private IndexOutput freqOutput;
-  private IndexOutput proxOutput;
-
-  private int curDoc;
-  private boolean curStorePayloads;
-  private boolean curStoreOffsets;
-  private int curPayloadLength;
-  private int curOffsetLength;
-  private long curFreqPointer;
-  private long curProxPointer;
-
-  /** Sole constructor. */
-  public Lucene40SkipListWriter(int skipInterval, int numberOfSkipLevels, int docCount, IndexOutput freqOutput, IndexOutput proxOutput) {
-    super(skipInterval, numberOfSkipLevels, docCount);
-    this.freqOutput = freqOutput;
-    this.proxOutput = proxOutput;
-    
-    lastSkipDoc = new int[numberOfSkipLevels];
-    lastSkipPayloadLength = new int[numberOfSkipLevels];
-    lastSkipOffsetLength = new int[numberOfSkipLevels];
-    lastSkipFreqPointer = new long[numberOfSkipLevels];
-    lastSkipProxPointer = new long[numberOfSkipLevels];
-  }
-
-  /**
-   * Sets the values for the current skip data. 
-   */
-  public void setSkipData(int doc, boolean storePayloads, int payloadLength, boolean storeOffsets, int offsetLength) {
-    assert storePayloads || payloadLength == -1;
-    assert storeOffsets  || offsetLength == -1;
-    this.curDoc = doc;
-    this.curStorePayloads = storePayloads;
-    this.curPayloadLength = payloadLength;
-    this.curStoreOffsets = storeOffsets;
-    this.curOffsetLength = offsetLength;
-    this.curFreqPointer = freqOutput.getFilePointer();
-    if (proxOutput != null)
-      this.curProxPointer = proxOutput.getFilePointer();
-  }
-
-  @Override
-  public void resetSkip() {
-    super.resetSkip();
-    Arrays.fill(lastSkipDoc, 0);
-    Arrays.fill(lastSkipPayloadLength, -1);  // we don't have to write the first length in the skip list
-    Arrays.fill(lastSkipOffsetLength, -1);  // we don't have to write the first length in the skip list
-    Arrays.fill(lastSkipFreqPointer, freqOutput.getFilePointer());
-    if (proxOutput != null)
-      Arrays.fill(lastSkipProxPointer, proxOutput.getFilePointer());
-  }
-  
-  @Override
-  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
-    // To efficiently store payloads/offsets in the posting lists we do not store the length of
-    // every payload/offset. Instead we omit the length if the previous lengths were the same
-    //
-    // However, in order to support skipping, the length at every skip point must be known.
-    // So we use the same length encoding that we use for the posting lists for the skip data as well:
-    // Case 1: current field does not store payloads/offsets
-    //           SkipDatum                 --> DocSkip, FreqSkip, ProxSkip
-    //           DocSkip,FreqSkip,ProxSkip --> VInt
-    //           DocSkip records the document number before every SkipInterval th  document in TermFreqs. 
-    //           Document numbers are represented as differences from the previous value in the sequence.
-    // Case 2: current field stores payloads/offsets
-    //           SkipDatum                 --> DocSkip, PayloadLength?,OffsetLength?,FreqSkip,ProxSkip
-    //           DocSkip,FreqSkip,ProxSkip --> VInt
-    //           PayloadLength,OffsetLength--> VInt    
-    //         In this case DocSkip/2 is the difference between
-    //         the current and the previous value. If DocSkip
-    //         is odd, then a PayloadLength encoded as VInt follows,
-    //         if DocSkip is even, then it is assumed that the
-    //         current payload/offset lengths equals the lengths at the previous
-    //         skip point
-    int delta = curDoc - lastSkipDoc[level];
-    
-    if (curStorePayloads || curStoreOffsets) {
-      assert curStorePayloads || curPayloadLength == lastSkipPayloadLength[level];
-      assert curStoreOffsets  || curOffsetLength == lastSkipOffsetLength[level];
-
-      if (curPayloadLength == lastSkipPayloadLength[level] && curOffsetLength == lastSkipOffsetLength[level]) {
-        // the current payload/offset lengths equals the lengths at the previous skip point,
-        // so we don't store the lengths again
-        skipBuffer.writeVInt(delta << 1);
-      } else {
-        // the payload and/or offset length is different from the previous one. We shift the DocSkip, 
-        // set the lowest bit and store the current payload and/or offset lengths as VInts.
-        skipBuffer.writeVInt(delta << 1 | 1);
-
-        if (curStorePayloads) {
-          skipBuffer.writeVInt(curPayloadLength);
-          lastSkipPayloadLength[level] = curPayloadLength;
-        }
-        if (curStoreOffsets) {
-          skipBuffer.writeVInt(curOffsetLength);
-          lastSkipOffsetLength[level] = curOffsetLength;
-        }
-      }
-    } else {
-      // current field does not store payloads or offsets
-      skipBuffer.writeVInt(delta);
-    }
-
-    skipBuffer.writeVInt((int) (curFreqPointer - lastSkipFreqPointer[level]));
-    skipBuffer.writeVInt((int) (curProxPointer - lastSkipProxPointer[level]));
-
-    lastSkipDoc[level] = curDoc;
-    
-    lastSkipFreqPointer[level] = curFreqPointer;
-    lastSkipProxPointer[level] = curProxPointer;
-  }
-
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java
deleted file mode 100644
index 0b8b350..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java
+++ /dev/null
@@ -1,188 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/**
- * Copyright 2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not
- * use this file except in compliance with the License. You may obtain a copy of
- * the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsReader.*;
-
-
-/** 
- * Class responsible for writing stored document fields.
- * <p/>
- * It uses &lt;segment&gt;.fdt and &lt;segment&gt;.fdx; files.
- * 
- * @see Lucene40StoredFieldsFormat
- * @lucene.experimental 
- */
-public final class Lucene40StoredFieldsWriter extends StoredFieldsWriter {
-
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput fieldsStream;
-  private IndexOutput indexStream;
-  private final RAMOutputStream fieldsBuffer = new RAMOutputStream();
-
-  /** Sole constructor. */
-  public Lucene40StoredFieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    assert directory != null;
-    this.directory = directory;
-    this.segment = segment;
-
-    boolean success = false;
-    try {
-      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
-      indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION), context);
-
-      CodecUtil.writeHeader(fieldsStream, CODEC_NAME_DAT, VERSION_CURRENT);
-      CodecUtil.writeHeader(indexStream, CODEC_NAME_IDX, VERSION_CURRENT);
-      assert HEADER_LENGTH_DAT == fieldsStream.getFilePointer();
-      assert HEADER_LENGTH_IDX == indexStream.getFilePointer();
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
-
-  int numStoredFields;
-
-  // Writes the contents of buffer into the fields stream
-  // and adds a new entry for this document into the index
-  // stream.  This assumes the buffer was already written
-  // in the correct fields format.
-  @Override
-  public void startDocument() throws IOException {
-    indexStream.writeLong(fieldsStream.getFilePointer());
-  }
-
-  @Override
-  public void finishDocument() throws IOException {
-    fieldsStream.writeVInt(numStoredFields);
-    fieldsBuffer.writeTo(fieldsStream);
-    fieldsBuffer.reset();
-    numStoredFields = 0;
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(fieldsStream, indexStream);
-    } finally {
-      fieldsStream = indexStream = null;
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      close();
-    } catch (Throwable ignored) {}
-    IOUtils.deleteFilesIgnoringExceptions(directory,
-        IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION),
-        IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION));
-  }
-
-  @Override
-  public void writeField(FieldInfo info, StorableField field) throws IOException {
-    numStoredFields++;
-
-    fieldsBuffer.writeVInt(info.number);
-    int bits = 0;
-    final BytesRef bytes;
-    final String string;
-    // TODO: maybe a field should serialize itself?
-    // this way we don't bake into indexer all these
-    // specific encodings for different fields?  and apps
-    // can customize...
-
-    Number number = field.numericValue();
-    if (number != null) {
-      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
-        bits |= FIELD_IS_NUMERIC_INT;
-      } else if (number instanceof Long) {
-        bits |= FIELD_IS_NUMERIC_LONG;
-      } else if (number instanceof Float) {
-        bits |= FIELD_IS_NUMERIC_FLOAT;
-      } else if (number instanceof Double) {
-        bits |= FIELD_IS_NUMERIC_DOUBLE;
-      } else {
-        throw new IllegalArgumentException("cannot store numeric type " + number.getClass());
-      }
-      string = null;
-      bytes = null;
-    } else {
-      bytes = field.binaryValue();
-      if (bytes != null) {
-        bits |= FIELD_IS_BINARY;
-        string = null;
-      } else {
-        string = field.stringValue();
-        if (string == null) {
-          throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
-        }
-      }
-    }
-
-    fieldsBuffer.writeByte((byte) bits);
-
-    if (bytes != null) {
-      fieldsBuffer.writeVInt(bytes.length);
-      fieldsBuffer.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-    } else if (string != null) {
-      fieldsBuffer.writeString(field.stringValue());
-    } else {
-      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
-        fieldsBuffer.writeInt(number.intValue());
-      } else if (number instanceof Long) {
-        fieldsBuffer.writeLong(number.longValue());
-      } else if (number instanceof Float) {
-        fieldsBuffer.writeInt(Float.floatToIntBits(number.floatValue()));
-      } else if (number instanceof Double) {
-        fieldsBuffer.writeLong(Double.doubleToLongBits(number.doubleValue()));
-      } else {
-        throw new AssertionError("Cannot get here");
-      }
-    }
-  }
-
-  @Override
-  public void finish(FieldInfos fis, int numDocs) {
-    long indexFP = indexStream.getFilePointer();
-    if (HEADER_LENGTH_IDX+((long) numDocs)*8 != indexFP)
-      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
-      // we detect that the bug has struck, here, and
-      // throw an exception to prevent the corruption from
-      // entering the index.  See LUCENE-1282 for
-      // details.
-      throw new RuntimeException("fdx size mismatch: docCount is " + numDocs + " but fdx file size is " + indexFP + " (wrote numDocs=" + ((indexFP-HEADER_LENGTH_IDX)/8.0) + " file=" + indexStream.toString() + "; now aborting this merge to prevent index corruption");
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java
deleted file mode 100644
index 5b5b301..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java
+++ /dev/null
@@ -1,303 +0,0 @@
-package org.apache.lucene.codecs.lucene40;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefBuilder;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-import static org.apache.lucene.codecs.lucene40.Lucene40TermVectorsReader.*;
-
-
-// TODO: make a new 4.0 TV format that encodes better
-//   - use startOffset (not endOffset) as base for delta on
-//     next startOffset because today for syns or ngrams or
-//     WDF or shingles etc. we are encoding negative vints
-//     (= slow, 5 bytes per)
-//   - if doc has no term vectors, write 0 into the tvx
-//     file; saves a seek to tvd only to read a 0 vint (and
-//     saves a byte in tvd)
-
-/**
- * Lucene 4.0 Term Vectors writer.
- * <p>
- * It writes .tvd, .tvf, and .tvx files.
- * 
- * @see Lucene40TermVectorsFormat
- */
-public final class Lucene40TermVectorsWriter extends TermVectorsWriter {
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput tvx = null, tvd = null, tvf = null;
-  
-  /** Sole constructor. */
-  public Lucene40TermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    this.directory = directory;
-    this.segment = segment;
-    boolean success = false;
-    try {
-      // Open files for TermVector storage
-      tvx = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION), context);
-      CodecUtil.writeHeader(tvx, CODEC_NAME_INDEX, VERSION_CURRENT);
-      tvd = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_DOCUMENTS_EXTENSION), context);
-      CodecUtil.writeHeader(tvd, CODEC_NAME_DOCS, VERSION_CURRENT);
-      tvf = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_FIELDS_EXTENSION), context);
-      CodecUtil.writeHeader(tvf, CODEC_NAME_FIELDS, VERSION_CURRENT);
-      assert HEADER_LENGTH_INDEX == tvx.getFilePointer();
-      assert HEADER_LENGTH_DOCS == tvd.getFilePointer();
-      assert HEADER_LENGTH_FIELDS == tvf.getFilePointer();
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
- 
-  @Override
-  public void startDocument(int numVectorFields) throws IOException {
-    lastFieldName = null;
-    this.numVectorFields = numVectorFields;
-    tvx.writeLong(tvd.getFilePointer());
-    tvx.writeLong(tvf.getFilePointer());
-    tvd.writeVInt(numVectorFields);
-    fieldCount = 0;
-    fps = ArrayUtil.grow(fps, numVectorFields);
-  }
-  
-  private long fps[] = new long[10]; // pointers to the tvf before writing each field 
-  private int fieldCount = 0;        // number of fields we have written so far for this document
-  private int numVectorFields = 0;   // total number of fields we will write for this document
-  private String lastFieldName;
-
-  @Override
-  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets, boolean payloads) throws IOException {
-    assert lastFieldName == null || info.name.compareTo(lastFieldName) > 0: "fieldName=" + info.name + " lastFieldName=" + lastFieldName;
-    lastFieldName = info.name;
-    this.positions = positions;
-    this.offsets = offsets;
-    this.payloads = payloads;
-    lastTerm.clear();
-    lastPayloadLength = -1; // force first payload to write its length
-    fps[fieldCount++] = tvf.getFilePointer();
-    tvd.writeVInt(info.number);
-    tvf.writeVInt(numTerms);
-    byte bits = 0x0;
-    if (positions)
-      bits |= Lucene40TermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR;
-    if (offsets)
-      bits |= Lucene40TermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR;
-    if (payloads)
-      bits |= Lucene40TermVectorsReader.STORE_PAYLOAD_WITH_TERMVECTOR;
-    tvf.writeByte(bits);
-  }
-  
-  @Override
-  public void finishDocument() throws IOException {
-    assert fieldCount == numVectorFields;
-    for (int i = 1; i < fieldCount; i++) {
-      tvd.writeVLong(fps[i] - fps[i-1]);
-    }
-  }
-
-  private final BytesRefBuilder lastTerm = new BytesRefBuilder();
-
-  // NOTE: we override addProx, so we don't need to buffer when indexing.
-  // we also don't buffer during bulk merges.
-  private int offsetStartBuffer[] = new int[10];
-  private int offsetEndBuffer[] = new int[10];
-  private BytesRefBuilder payloadData = new BytesRefBuilder();
-  private int bufferedIndex = 0;
-  private int bufferedFreq = 0;
-  private boolean positions = false;
-  private boolean offsets = false;
-  private boolean payloads = false;
-
-  @Override
-  public void startTerm(BytesRef term, int freq) throws IOException {
-    final int prefix = StringHelper.bytesDifference(lastTerm.get(), term);
-    final int suffix = term.length - prefix;
-    tvf.writeVInt(prefix);
-    tvf.writeVInt(suffix);
-    tvf.writeBytes(term.bytes, term.offset + prefix, suffix);
-    tvf.writeVInt(freq);
-    lastTerm.copyBytes(term);
-    lastPosition = lastOffset = 0;
-    
-    if (offsets && positions) {
-      // we might need to buffer if its a non-bulk merge
-      offsetStartBuffer = ArrayUtil.grow(offsetStartBuffer, freq);
-      offsetEndBuffer = ArrayUtil.grow(offsetEndBuffer, freq);
-    }
-    bufferedIndex = 0;
-    bufferedFreq = freq;
-    payloadData.clear();
-  }
-
-  int lastPosition = 0;
-  int lastOffset = 0;
-  int lastPayloadLength = -1; // force first payload to write its length
-
-  BytesRefBuilder scratch = new BytesRefBuilder(); // used only by this optimized flush below
-
-  @Override
-  public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
-    if (payloads) {
-      // TODO, maybe overkill and just call super.addProx() in this case?
-      // we do avoid buffering the offsets in RAM though.
-      for (int i = 0; i < numProx; i++) {
-        int code = positions.readVInt();
-        if ((code & 1) == 1) {
-          int length = positions.readVInt();
-          scratch.grow(length);
-          scratch.setLength(length);
-          positions.readBytes(scratch.bytes(), 0, scratch.length());
-          writePosition(code >>> 1, scratch.get());
-        } else {
-          writePosition(code >>> 1, null);
-        }
-      }
-      tvf.writeBytes(payloadData.bytes(), 0, payloadData.length());
-    } else if (positions != null) {
-      // pure positions, no payloads
-      for (int i = 0; i < numProx; i++) {
-        tvf.writeVInt(positions.readVInt() >>> 1);
-      }
-    }
-    
-    if (offsets != null) {
-      for (int i = 0; i < numProx; i++) {
-        tvf.writeVInt(offsets.readVInt());
-        tvf.writeVInt(offsets.readVInt());
-      }
-    }
-  }
-
-  @Override
-  public void addPosition(int position, int startOffset, int endOffset, BytesRef payload) throws IOException {
-    if (positions && (offsets || payloads)) {
-      // write position delta
-      writePosition(position - lastPosition, payload);
-      lastPosition = position;
-      
-      // buffer offsets
-      if (offsets) {
-        offsetStartBuffer[bufferedIndex] = startOffset;
-        offsetEndBuffer[bufferedIndex] = endOffset;
-      }
-      
-      bufferedIndex++;
-    } else if (positions) {
-      // write position delta
-      writePosition(position - lastPosition, payload);
-      lastPosition = position;
-    } else if (offsets) {
-      // write offset deltas
-      tvf.writeVInt(startOffset - lastOffset);
-      tvf.writeVInt(endOffset - startOffset);
-      lastOffset = endOffset;
-    }
-  }
-  
-  @Override
-  public void finishTerm() throws IOException {
-    if (bufferedIndex > 0) {
-      // dump buffer
-      assert positions && (offsets || payloads);
-      assert bufferedIndex == bufferedFreq;
-      if (payloads) {
-        tvf.writeBytes(payloadData.bytes(), 0, payloadData.length());
-      }
-      if (offsets) {
-        for (int i = 0; i < bufferedIndex; i++) {
-          tvf.writeVInt(offsetStartBuffer[i] - lastOffset);
-          tvf.writeVInt(offsetEndBuffer[i] - offsetStartBuffer[i]);
-          lastOffset = offsetEndBuffer[i];
-        }
-      }
-    }
-  }
-
-  private void writePosition(int delta, BytesRef payload) throws IOException {
-    if (payloads) {
-      int payloadLength = payload == null ? 0 : payload.length;
-
-      if (payloadLength != lastPayloadLength) {
-        lastPayloadLength = payloadLength;
-        tvf.writeVInt((delta<<1)|1);
-        tvf.writeVInt(payloadLength);
-      } else {
-        tvf.writeVInt(delta << 1);
-      }
-      if (payloadLength > 0) {
-        if (payloadLength + payloadData.length() < 0) {
-          // we overflowed the payload buffer, just throw UOE
-          // having > Integer.MAX_VALUE bytes of payload for a single term in a single doc is nuts.
-          throw new UnsupportedOperationException("A term cannot have more than Integer.MAX_VALUE bytes of payload data in a single document");
-        }
-        payloadData.append(payload);
-      }
-    } else {
-      tvf.writeVInt(delta);
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      close();
-    } catch (Throwable ignored) {}
-    IOUtils.deleteFilesIgnoringExceptions(directory, IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION),
-        IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_DOCUMENTS_EXTENSION),
-        IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_FIELDS_EXTENSION));
-  }
-  
-  @Override
-  public void finish(FieldInfos fis, int numDocs) {
-    long indexFP = tvx.getFilePointer();
-    if (HEADER_LENGTH_INDEX+((long) numDocs)*16 != indexFP)
-      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
-      // we detect that the bug has struck, here, and
-      // throw an exception to prevent the corruption from
-      // entering the index.  See LUCENE-1282 for
-      // details.
-      throw new RuntimeException("tvx size mismatch: mergedDocs is " + numDocs + " but tvx size is " + indexFP + " (wrote numDocs=" + ((indexFP - HEADER_LENGTH_INDEX)/16.0) + " file=" + tvx.toString() + "; now aborting this merge to prevent index corruption");
-  }
-
-  /** Close all streams. */
-  @Override
-  public void close() throws IOException {
-    // make an effort to close all streams we can but remember and re-throw
-    // the first exception encountered in this process
-    IOUtils.close(tvx, tvd, tvf);
-    tvx = tvd = tvf = null;
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/package.html
deleted file mode 100644
index c83302c..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Support for testing {@link org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat}.
-</body>
-</html>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java
deleted file mode 100644
index c5a1cc9..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41/Lucene41RWCodec.java
+++ /dev/null
@@ -1,80 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosWriter;
-import org.apache.lucene.codecs.lucene40.Lucene40RWDocValuesFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40RWNormsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40RWTermVectorsFormat;
-import org.apache.lucene.util.LuceneTestCase;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Read-write version of {@link Lucene41Codec} for testing.
- */
-@SuppressWarnings("deprecation")
-public class Lucene41RWCodec extends Lucene41Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
-  private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat() {
-    @Override
-    public FieldInfosWriter getFieldInfosWriter() throws IOException {
-      if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-        return super.getFieldInfosWriter();
-      } else {
-        return new Lucene40FieldInfosWriter();
-      }
-    }
-  };
-  
-  private final DocValuesFormat docValues = new Lucene40RWDocValuesFormat();
-  private final NormsFormat norms = new Lucene40RWNormsFormat();
-  private final TermVectorsFormat vectors = new Lucene40RWTermVectorsFormat();
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfos;
-  }
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValues;
-  }
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectors;
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41/package.html
deleted file mode 100644
index f9d0012..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Support for testing {@link org.apache.lucene.codecs.lucene41.Lucene41Codec}.
-</body>
-</html>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
deleted file mode 100644
index 48762c5..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesConsumer.java
+++ /dev/null
@@ -1,387 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.NoSuchElementException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.MissingOrdRemapper;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.ByteArrayDataOutput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.IntsRefBuilder;
-import org.apache.lucene.util.MathUtil;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.FST.INPUT_TYPE;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
-import org.apache.lucene.util.packed.PackedInts;
-
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.VERSION_GCD_COMPRESSION;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.BLOCK_SIZE;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.BYTES;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.NUMBER;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.FST;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.DELTA_COMPRESSED;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.GCD_COMPRESSED;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.TABLE_COMPRESSED;
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.UNCOMPRESSED;
-
-/**
- * Writer for {@link Lucene42DocValuesFormat}
- */
-class Lucene42DocValuesConsumer extends DocValuesConsumer {
-  final IndexOutput data, meta;
-  final int maxDoc;
-  final float acceptableOverheadRatio;
-  
-  Lucene42DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension, float acceptableOverheadRatio) throws IOException {
-    this.acceptableOverheadRatio = acceptableOverheadRatio;
-    maxDoc = state.segmentInfo.getDocCount();
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      // this writer writes the format 4.2 did!
-      CodecUtil.writeHeader(data, dataCodec, VERSION_GCD_COMPRESSION);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, VERSION_GCD_COMPRESSION);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-
-  @Override
-  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
-    addNumericField(field, values, true);
-  }
-
-  void addNumericField(FieldInfo field, Iterable<Number> values, boolean optimizeStorage) throws IOException {
-    meta.writeVInt(field.number);
-    meta.writeByte(NUMBER);
-    meta.writeLong(data.getFilePointer());
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    long gcd = 0;
-    // TODO: more efficient?
-    HashSet<Long> uniqueValues = null;
-    if (optimizeStorage) {
-      uniqueValues = new HashSet<>();
-
-      long count = 0;
-      for (Number nv : values) {
-        // TODO: support this as MemoryDVFormat (and be smart about missing maybe)
-        final long v = nv == null ? 0 : nv.longValue();
-
-        if (gcd != 1) {
-          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
-            // in that case v - minValue might overflow and make the GCD computation return
-            // wrong results. Since these extreme values are unlikely, we just discard
-            // GCD computation for them
-            gcd = 1;
-          } else if (count != 0) { // minValue needs to be set first
-            gcd = MathUtil.gcd(gcd, v - minValue);
-          }
-        }
-
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-
-        if (uniqueValues != null) {
-          if (uniqueValues.add(v)) {
-            if (uniqueValues.size() > 256) {
-              uniqueValues = null;
-            }
-          }
-        }
-
-        ++count;
-      }
-      assert count == maxDoc;
-    }
-
-    if (uniqueValues != null) {
-      // small number of unique values
-      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);
-      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);
-      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {
-        meta.writeByte(UNCOMPRESSED); // uncompressed
-        for (Number nv : values) {
-          data.writeByte(nv == null ? 0 : (byte) nv.longValue());
-        }
-      } else {
-        meta.writeByte(TABLE_COMPRESSED); // table-compressed
-        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        final HashMap<Long,Integer> encode = new HashMap<>();
-        data.writeVInt(decode.length);
-        for (int i = 0; i < decode.length; i++) {
-          data.writeLong(decode[i]);
-          encode.put(decode[i], i);
-        }
-
-        meta.writeVInt(PackedInts.VERSION_CURRENT);
-        data.writeVInt(formatAndBits.format.getId());
-        data.writeVInt(formatAndBits.bitsPerValue);
-
-        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);
-        for(Number nv : values) {
-          writer.add(encode.get(nv == null ? 0 : nv.longValue()));
-        }
-        writer.finish();
-      }
-    } else if (gcd != 0 && gcd != 1) {
-      meta.writeByte(GCD_COMPRESSED);
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      data.writeLong(minValue);
-      data.writeLong(gcd);
-      data.writeVInt(BLOCK_SIZE);
-
-      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-      for (Number nv : values) {
-        long value = nv == null ? 0 : nv.longValue();
-        writer.add((value - minValue) / gcd);
-      }
-      writer.finish();
-    } else {
-      meta.writeByte(DELTA_COMPRESSED); // delta-compressed
-
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      data.writeVInt(BLOCK_SIZE);
-
-      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-      for (Number nv : values) {
-        writer.add(nv == null ? 0 : nv.longValue());
-      }
-      writer.finish();
-    }
-  }
-  
-  @Override
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1); // write EOF marker
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-    }
-  }
-
-  @Override
-  public void addBinaryField(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
-    // write the byte[] data
-    meta.writeVInt(field.number);
-    meta.writeByte(BYTES);
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    final long startFP = data.getFilePointer();
-    for(BytesRef v : values) {
-      final int length = v == null ? 0 : v.length;
-      if (length > Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH) {
-        throw new IllegalArgumentException("DocValuesField \"" + field.name + "\" is too large, must be <= " + Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);
-      }
-      minLength = Math.min(minLength, length);
-      maxLength = Math.max(maxLength, length);
-      if (v != null) {
-        data.writeBytes(v.bytes, v.offset, v.length);
-      }
-    }
-    meta.writeLong(startFP);
-    meta.writeLong(data.getFilePointer() - startFP);
-    meta.writeVInt(minLength);
-    meta.writeVInt(maxLength);
-    
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
-    // otherwise, we need to record the length fields...
-    if (minLength != maxLength) {
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      meta.writeVInt(BLOCK_SIZE);
-
-      final MonotonicBlockPackedWriter writer = new MonotonicBlockPackedWriter(data, BLOCK_SIZE);
-      long addr = 0;
-      for (BytesRef v : values) {
-        if (v != null) {
-          addr += v.length;
-        }
-        writer.add(addr);
-      }
-      writer.finish();
-    }
-  }
-  
-  private void writeFST(FieldInfo field, Iterable<BytesRef> values) throws IOException {
-    meta.writeVInt(field.number);
-    meta.writeByte(FST);
-    meta.writeLong(data.getFilePointer());
-    PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-    Builder<Long> builder = new Builder<>(INPUT_TYPE.BYTE1, outputs);
-    IntsRefBuilder scratch = new IntsRefBuilder();
-    long ord = 0;
-    for (BytesRef v : values) {
-      builder.add(Util.toIntsRef(v, scratch), ord);
-      ord++;
-    }
-    FST<Long> fst = builder.finish();
-    if (fst != null) {
-      fst.save(data);
-    }
-    meta.writeVLong(ord);
-  }
-
-  @Override
-  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    // three cases for simulating the old writer:
-    // 1. no missing
-    // 2. missing (and empty string in use): remap ord=-1 -> ord=0
-    // 3. missing (and empty string not in use): remap all ords +1, insert empty string into values
-    boolean anyMissing = false;
-    for (Number n : docToOrd) {
-      if (n.longValue() == -1) {
-        anyMissing = true;
-        break;
-      }
-    }
-    
-    boolean hasEmptyString = false;
-    for (BytesRef b : values) {
-      hasEmptyString = b.length == 0;
-      break;
-    }
-    
-    if (!anyMissing) {
-      // nothing to do
-    } else if (hasEmptyString) {
-      docToOrd = MissingOrdRemapper.mapMissingToOrd0(docToOrd);
-    } else {
-      docToOrd = MissingOrdRemapper.mapAllOrds(docToOrd);
-      values = MissingOrdRemapper.insertEmptyValue(values);
-    }
-    
-    // write the ordinals as numerics
-    addNumericField(field, docToOrd, false);
-    
-    // write the values as FST
-    writeFST(field, values);
-  }
-
-  // note: this might not be the most efficient... but its fairly simple
-  @Override
-  public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
-    // write the ordinals as a binary field
-    addBinaryField(field, new Iterable<BytesRef>() {
-      @Override
-      public Iterator<BytesRef> iterator() {
-        return new SortedSetIterator(docToOrdCount.iterator(), ords.iterator());
-      }
-    });
-      
-    // write the values as FST
-    writeFST(field, values);
-  }
-  
-  // per-document vint-encoded byte[]
-  static class SortedSetIterator implements Iterator<BytesRef> {
-    byte[] buffer = new byte[10];
-    ByteArrayDataOutput out = new ByteArrayDataOutput();
-    BytesRef ref = new BytesRef();
-    
-    final Iterator<Number> counts;
-    final Iterator<Number> ords;
-    
-    SortedSetIterator(Iterator<Number> counts, Iterator<Number> ords) {
-      this.counts = counts;
-      this.ords = ords;
-    }
-    
-    @Override
-    public boolean hasNext() {
-      return counts.hasNext();
-    }
-
-    @Override
-    public BytesRef next() {
-      if (!hasNext()) {
-        throw new NoSuchElementException();
-      }
-      
-      int count = counts.next().intValue();
-      int maxSize = count*9; // worst case
-      if (maxSize > buffer.length) {
-        buffer = ArrayUtil.grow(buffer, maxSize);
-      }
-      
-      try {
-        encodeValues(count);
-      } catch (IOException bogus) {
-        throw new RuntimeException(bogus);
-      }
-      
-      ref.bytes = buffer;
-      ref.offset = 0;
-      ref.length = out.getPosition();
-
-      return ref;
-    }
-    
-    // encodes count values to buffer
-    private void encodeValues(int count) throws IOException {
-      out.reset(buffer);
-      long lastOrd = 0;
-      for (int i = 0; i < count; i++) {
-        long ord = ords.next().longValue();
-        out.writeVLong(ord - lastOrd);
-        lastOrd = ord;
-      }
-    }
-
-    @Override
-    public void remove() {
-      throw new UnsupportedOperationException();
-    }
-  }
-
-  @Override
-  public void addSortedNumericField(FieldInfo field, Iterable<Number> docToValueCount, Iterable<Number> values) throws IOException {
-    throw new UnsupportedOperationException("Lucene 4.2 does not support SORTED_NUMERIC");
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java
deleted file mode 100644
index e3f7b6c..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42FieldInfosWriter.java
+++ /dev/null
@@ -1,109 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Lucene 4.2 FieldInfos writer.
- * 
- * @see Lucene42FieldInfosFormat
- * @lucene.experimental
- */
-@Deprecated
-public final class Lucene42FieldInfosWriter extends FieldInfosWriter {
-  
-  /** Sole constructor. */
-  public Lucene42FieldInfosWriter() {
-  }
-  
-  @Override
-  public void write(Directory directory, String segmentName, String segmentSuffix, FieldInfos infos, IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene42FieldInfosFormat.EXTENSION);
-    IndexOutput output = directory.createOutput(fileName, context);
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(output, Lucene42FieldInfosFormat.CODEC_NAME, Lucene42FieldInfosFormat.FORMAT_CURRENT);
-      output.writeVInt(infos.size());
-      for (FieldInfo fi : infos) {
-        IndexOptions indexOptions = fi.getIndexOptions();
-        byte bits = 0x0;
-        if (fi.hasVectors()) bits |= Lucene42FieldInfosFormat.STORE_TERMVECTOR;
-        if (fi.omitsNorms()) bits |= Lucene42FieldInfosFormat.OMIT_NORMS;
-        if (fi.hasPayloads()) bits |= Lucene42FieldInfosFormat.STORE_PAYLOADS;
-        if (fi.isIndexed()) {
-          bits |= Lucene42FieldInfosFormat.IS_INDEXED;
-          assert indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
-          if (indexOptions == IndexOptions.DOCS_ONLY) {
-            bits |= Lucene42FieldInfosFormat.OMIT_TERM_FREQ_AND_POSITIONS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
-            bits |= Lucene42FieldInfosFormat.STORE_OFFSETS_IN_POSTINGS;
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
-            bits |= Lucene42FieldInfosFormat.OMIT_POSITIONS;
-          }
-        }
-        output.writeString(fi.name);
-        output.writeVInt(fi.number);
-        output.writeByte(bits);
-
-        // pack the DV types in one byte
-        final byte dv = docValuesByte(fi.getDocValuesType());
-        final byte nrm = docValuesByte(fi.getNormType());
-        assert (dv & (~0xF)) == 0 && (nrm & (~0x0F)) == 0;
-        byte val = (byte) (0xff & ((nrm << 4) | dv));
-        output.writeByte(val);
-        output.writeStringStringMap(fi.attributes());
-      }
-      success = true;
-    } finally {
-      if (success) {
-        output.close();
-      } else {
-        IOUtils.closeWhileHandlingException(output);
-      }
-    }
-  }
-  
-  private static byte docValuesByte(DocValuesType type) {
-    if (type == null) {
-      return 0;
-    } else if (type == DocValuesType.NUMERIC) {
-      return 1;
-    } else if (type == DocValuesType.BINARY) {
-      return 2;
-    } else if (type == DocValuesType.SORTED) {
-      return 3;
-    } else if (type == DocValuesType.SORTED_SET) {
-      return 4;
-    } else {
-      throw new AssertionError();
-    }
-  }  
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java
deleted file mode 100644
index dc1825d..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42NormsConsumer.java
+++ /dev/null
@@ -1,196 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.MathUtil;
-import org.apache.lucene.util.packed.BlockPackedWriter;
-import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
-import org.apache.lucene.util.packed.PackedInts;
-
-import static org.apache.lucene.codecs.lucene42.Lucene42DocValuesProducer.VERSION_CURRENT;
-
-/**
- * Writer for {@link Lucene42NormsFormat}
- */
-class Lucene42NormsConsumer extends NormsConsumer { 
-  static final byte NUMBER = 0;
-
-  static final int BLOCK_SIZE = 4096;
-  
-  static final byte DELTA_COMPRESSED = 0;
-  static final byte TABLE_COMPRESSED = 1;
-  static final byte UNCOMPRESSED = 2;
-  static final byte GCD_COMPRESSED = 3;
-
-  IndexOutput data, meta;
-  final int maxDoc;
-  final float acceptableOverheadRatio;
-  
-  Lucene42NormsConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension, float acceptableOverheadRatio) throws IOException {
-    this.acceptableOverheadRatio = acceptableOverheadRatio;
-    maxDoc = state.segmentInfo.getDocCount();
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, VERSION_CURRENT);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, VERSION_CURRENT);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-
-  @Override
-  public void addNormsField(FieldInfo field, Iterable<Number> values) throws IOException {
-    meta.writeVInt(field.number);
-    meta.writeByte(NUMBER);
-    meta.writeLong(data.getFilePointer());
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    long gcd = 0;
-    // TODO: more efficient?
-    HashSet<Long> uniqueValues = null;
-    if (true) {
-      uniqueValues = new HashSet<>();
-
-      long count = 0;
-      for (Number nv : values) {
-        assert nv != null;
-        final long v = nv.longValue();
-
-        if (gcd != 1) {
-          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {
-            // in that case v - minValue might overflow and make the GCD computation return
-            // wrong results. Since these extreme values are unlikely, we just discard
-            // GCD computation for them
-            gcd = 1;
-          } else if (count != 0) { // minValue needs to be set first
-            gcd = MathUtil.gcd(gcd, v - minValue);
-          }
-        }
-
-        minValue = Math.min(minValue, v);
-        maxValue = Math.max(maxValue, v);
-
-        if (uniqueValues != null) {
-          if (uniqueValues.add(v)) {
-            if (uniqueValues.size() > 256) {
-              uniqueValues = null;
-            }
-          }
-        }
-
-        ++count;
-      }
-      assert count == maxDoc;
-    }
-
-    if (uniqueValues != null) {
-      // small number of unique values
-      final int bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);
-      FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(maxDoc, bitsPerValue, acceptableOverheadRatio);
-      if (formatAndBits.bitsPerValue == 8 && minValue >= Byte.MIN_VALUE && maxValue <= Byte.MAX_VALUE) {
-        meta.writeByte(UNCOMPRESSED); // uncompressed
-        for (Number nv : values) {
-          data.writeByte(nv == null ? 0 : (byte) nv.longValue());
-        }
-      } else {
-        meta.writeByte(TABLE_COMPRESSED); // table-compressed
-        Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-        final HashMap<Long,Integer> encode = new HashMap<>();
-        data.writeVInt(decode.length);
-        for (int i = 0; i < decode.length; i++) {
-          data.writeLong(decode[i]);
-          encode.put(decode[i], i);
-        }
-
-        meta.writeVInt(PackedInts.VERSION_CURRENT);
-        data.writeVInt(formatAndBits.format.getId());
-        data.writeVInt(formatAndBits.bitsPerValue);
-
-        final PackedInts.Writer writer = PackedInts.getWriterNoHeader(data, formatAndBits.format, maxDoc, formatAndBits.bitsPerValue, PackedInts.DEFAULT_BUFFER_SIZE);
-        for(Number nv : values) {
-          writer.add(encode.get(nv == null ? 0 : nv.longValue()));
-        }
-        writer.finish();
-      }
-    } else if (gcd != 0 && gcd != 1) {
-      meta.writeByte(GCD_COMPRESSED);
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      data.writeLong(minValue);
-      data.writeLong(gcd);
-      data.writeVInt(BLOCK_SIZE);
-
-      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-      for (Number nv : values) {
-        long value = nv == null ? 0 : nv.longValue();
-        writer.add((value - minValue) / gcd);
-      }
-      writer.finish();
-    } else {
-      meta.writeByte(DELTA_COMPRESSED); // delta-compressed
-
-      meta.writeVInt(PackedInts.VERSION_CURRENT);
-      data.writeVInt(BLOCK_SIZE);
-
-      final BlockPackedWriter writer = new BlockPackedWriter(data, BLOCK_SIZE);
-      for (Number nv : values) {
-        writer.add(nv == null ? 0 : nv.longValue());
-      }
-      writer.finish();
-    }
-  }
-  
-  @Override
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1); // write EOF marker
-        CodecUtil.writeFooter(meta); // write checksum
-      }
-      if (data != null) {
-        CodecUtil.writeFooter(data); // write checksum
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-      meta = data = null;
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java
deleted file mode 100644
index 68f3859..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42RWCodec.java
+++ /dev/null
@@ -1,62 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Read-write version of {@link Lucene42Codec} for testing.
- */
-@SuppressWarnings("deprecation")
-public class Lucene42RWCodec extends Lucene42Codec {
-
-  private static final DocValuesFormat dv = new Lucene42RWDocValuesFormat();
-  private static final NormsFormat norms = new Lucene42RWNormsFormat();
-
-  private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat() {
-    @Override
-    public FieldInfosWriter getFieldInfosWriter() throws IOException {
-      if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-        return super.getFieldInfosWriter();
-      } else {
-        return new Lucene42FieldInfosWriter();
-      }
-    }
-  };
-
-  @Override
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return dv;
-  }
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }  
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java
deleted file mode 100644
index 569ba9e..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42RWDocValuesFormat.java
+++ /dev/null
@@ -1,41 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Read-write version of {@link Lucene42DocValuesFormat} for testing.
- */
-@SuppressWarnings("deprecation")
-public class Lucene42RWDocValuesFormat extends Lucene42DocValuesFormat {
-  
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-      return super.fieldsConsumer(state);
-    } else {
-      // note: we choose DEFAULT here (its reasonably fast, and for small bpv has tiny waste)
-      return new Lucene42DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION, acceptableOverheadRatio);
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java
deleted file mode 100644
index fbca416..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/Lucene42RWNormsFormat.java
+++ /dev/null
@@ -1,39 +0,0 @@
-package org.apache.lucene.codecs.lucene42;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.NormsConsumer;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Read-write version of {@link Lucene42NormsFormat}
- */
-public class Lucene42RWNormsFormat extends Lucene42NormsFormat {
-
-  @Override
-  public NormsConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    if (LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-      return new Lucene42NormsConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION, acceptableOverheadRatio);
-    } else {
-      return super.normsConsumer(state);
-    }
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/package.html
deleted file mode 100644
index f1c62d1..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene42/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Support for testing {@link org.apache.lucene.codecs.lucene42.Lucene42Codec}.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java
deleted file mode 100644
index 0263b39..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene45/Lucene45RWCodec.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42FieldInfosFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42FieldInfosWriter;
-import org.apache.lucene.codecs.lucene42.Lucene42RWNormsFormat;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Read-write version of {@link Lucene45Codec} for testing.
- */
-@SuppressWarnings("deprecation")
-public class Lucene45RWCodec extends Lucene45Codec {
-  
-  private final FieldInfosFormat fieldInfosFormat = new Lucene42FieldInfosFormat() {
-    @Override
-    public FieldInfosWriter getFieldInfosWriter() throws IOException {
-      if (!LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-        return super.getFieldInfosWriter();
-      } else {
-        return new Lucene42FieldInfosWriter();
-      }
-    }
-  };
-
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  private static final DocValuesFormat docValues = new Lucene45RWDocValuesFormat();
-  
-  @Override
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return docValues;
-  }
-
-  private static final NormsFormat norms = new Lucene42RWNormsFormat();
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java
deleted file mode 100644
index 87a6dd3..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene45/Lucene45RWDocValuesFormat.java
+++ /dev/null
@@ -1,45 +0,0 @@
-package org.apache.lucene.codecs.lucene45;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Read-write version of {@link Lucene45DocValuesFormat} for testing.
- */
-public class Lucene45RWDocValuesFormat extends Lucene45DocValuesFormat {
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    if (LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-      return new Lucene45DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION) {
-        @Override
-        void checkCanWrite(FieldInfo field) {
-           // allow writing all fields 
-        }
-      };
-    } else {
-      return super.fieldsConsumer(state);
-    }
-  }
-}
\ No newline at end of file
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene45/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene45/package.html
deleted file mode 100644
index 85480b8..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene45/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Support for testing {@link org.apache.lucene.codecs.lucene45.Lucene45Codec}.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java
deleted file mode 100644
index 1d27f49..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene46/Lucene46RWCodec.java
+++ /dev/null
@@ -1,44 +0,0 @@
-package org.apache.lucene.codecs.lucene46;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.lucene42.Lucene42RWNormsFormat;
-import org.apache.lucene.codecs.lucene45.Lucene45RWDocValuesFormat;
-
-/**
- * Read-write version of {@link Lucene46Codec} for testing.
- */
-@SuppressWarnings("deprecation")
-public class Lucene46RWCodec extends Lucene46Codec {
-  
-  private static final DocValuesFormat docValues = new Lucene45RWDocValuesFormat();
-  
-  @Override
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return docValues;
-  }
-  
-  private static final NormsFormat norms = new Lucene42RWNormsFormat();
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene46/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene46/package.html
deleted file mode 100644
index 8abed4c..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene46/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Support for testing {@link org.apache.lucene.codecs.lucene46.Lucene46Codec}.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java
deleted file mode 100644
index 4884d83..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene49/Lucene49RWCodec.java
+++ /dev/null
@@ -1,42 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.NormsFormat;
-
-/**
- * Read-write version of {@link Lucene49Codec} for testing.
- */
-@SuppressWarnings("deprecation")
-public class Lucene49RWCodec extends Lucene49Codec {
-  
-  private static final DocValuesFormat docValues = new Lucene49RWDocValuesFormat();
-  
-  @Override
-  public DocValuesFormat getDocValuesFormatForField(String field) {
-    return docValues;
-  }
-  
-  private static final NormsFormat norms = new Lucene49NormsFormat();
-
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java
deleted file mode 100644
index 53c1fe5..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene49/Lucene49RWDocValuesFormat.java
+++ /dev/null
@@ -1,44 +0,0 @@
-package org.apache.lucene.codecs.lucene49;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** Read-write version of {@link Lucene49DocValuesFormat} for testing */
-public class Lucene49RWDocValuesFormat extends Lucene49DocValuesFormat {
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    if (LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE) {
-      return new Lucene49DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, META_CODEC, META_EXTENSION) {
-        @Override
-        void checkCanWrite(FieldInfo field) {
-          // allow writing all fields 
-        }
-      };
-    } else {
-      return super.fieldsConsumer(state);
-    }
-  }
-  
-}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene49/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene49/package.html
deleted file mode 100644
index 30924a6..0000000
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene49/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Support for testing {@link org.apache.lucene.codecs.lucene49.Lucene49Codec}.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java b/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
index efc53f8..6af4815 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
@@ -34,14 +34,7 @@ import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.asserting.AssertingCodec;
 import org.apache.lucene.codecs.cheapbastard.CheapBastardCodec;
 import org.apache.lucene.codecs.compressing.CompressingCodec;
-import org.apache.lucene.codecs.lucene40.Lucene40RWCodec;
-import org.apache.lucene.codecs.lucene40.Lucene40RWPostingsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41RWCodec;
 import org.apache.lucene.codecs.lucene410.Lucene410Codec;
-import org.apache.lucene.codecs.lucene42.Lucene42RWCodec;
-import org.apache.lucene.codecs.lucene45.Lucene45RWCodec;
-import org.apache.lucene.codecs.lucene46.Lucene46RWCodec;
-import org.apache.lucene.codecs.lucene49.Lucene49RWCodec;
 import org.apache.lucene.codecs.mockrandom.MockRandomPostingsFormat;
 import org.apache.lucene.codecs.simpletext.SimpleTextCodec;
 import org.apache.lucene.index.RandomCodec;
@@ -160,56 +153,7 @@ final class TestRuleSetupAndRestoreClassEnv extends AbstractBeforeAfterRule {
     
     savedCodec = Codec.getDefault();
     int randomVal = random.nextInt(11);
-    if ("Lucene40".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) &&
-                                          "random".equals(TEST_POSTINGSFORMAT) &&
-                                          "random".equals(TEST_DOCVALUESFORMAT) &&
-                                          randomVal == 0 &&
-                                          !shouldAvoidCodec("Lucene40"))) {
-      codec = Codec.forName("Lucene40");
-      LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
-      assert codec instanceof Lucene40RWCodec : "fix your classpath to have tests-framework.jar before lucene-core.jar";
-      assert (PostingsFormat.forName("Lucene40") instanceof Lucene40RWPostingsFormat) : "fix your classpath to have tests-framework.jar before lucene-core.jar";
-    } else if ("Lucene41".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) &&
-                                                 "random".equals(TEST_POSTINGSFORMAT) &&
-                                                 "random".equals(TEST_DOCVALUESFORMAT) &&
-                                                 randomVal == 1 &&
-                                                 !shouldAvoidCodec("Lucene41"))) { 
-      codec = Codec.forName("Lucene41");
-      LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
-      assert codec instanceof Lucene41RWCodec : "fix your classpath to have tests-framework.jar before lucene-core.jar";
-    } else if ("Lucene42".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) &&
-                                                 "random".equals(TEST_POSTINGSFORMAT) &&
-                                                 "random".equals(TEST_DOCVALUESFORMAT) &&
-                                                  randomVal == 2 &&
-                                                  !shouldAvoidCodec("Lucene42"))) { 
-      codec = Codec.forName("Lucene42");
-      LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
-      assert codec instanceof Lucene42RWCodec : "fix your classpath to have tests-framework.jar before lucene-core.jar";
-    } else if ("Lucene45".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) &&
-                                                 "random".equals(TEST_POSTINGSFORMAT) &&
-                                                 "random".equals(TEST_DOCVALUESFORMAT) &&
-                                                  randomVal == 3 &&
-                                                  !shouldAvoidCodec("Lucene45"))) { 
-      codec = Codec.forName("Lucene45");
-      LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
-      assert codec instanceof Lucene45RWCodec : "fix your classpath to have tests-framework.jar before lucene-core.jar";
-    } else if ("Lucene46".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) &&
-                                                 "random".equals(TEST_POSTINGSFORMAT) &&
-                                                 "random".equals(TEST_DOCVALUESFORMAT) &&
-                                                  randomVal == 4 &&
-                                                  !shouldAvoidCodec("Lucene46"))) { 
-      codec = Codec.forName("Lucene46");
-      LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
-      assert codec instanceof Lucene46RWCodec : "fix your classpath to have tests-framework.jar before lucene-core.jar";
-    } else if ("Lucene49".equals(TEST_CODEC) || ("random".equals(TEST_CODEC) &&
-                                                 "random".equals(TEST_POSTINGSFORMAT) &&
-                                                 "random".equals(TEST_DOCVALUESFORMAT) &&
-                                                  randomVal == 5 &&
-                                                  !shouldAvoidCodec("Lucene49"))) { 
-      codec = Codec.forName("Lucene49");
-      LuceneTestCase.OLD_FORMAT_IMPERSONATION_IS_ACTIVE = true;
-      assert codec instanceof Lucene49RWCodec : "fix your classpath to have tests-framework.jar before lucene-core.jar";
-    } else if (("random".equals(TEST_POSTINGSFORMAT) == false) || ("random".equals(TEST_DOCVALUESFORMAT) == false)) {
+    if (("random".equals(TEST_POSTINGSFORMAT) == false) || ("random".equals(TEST_DOCVALUESFORMAT) == false)) {
       // the user wired postings or DV: this is messy
       // refactor into RandomCodec....
       
diff --git a/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.Codec b/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
index 2f5cc92..282f5dd 100644
--- a/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
+++ b/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
@@ -19,9 +19,3 @@ org.apache.lucene.codecs.compressing.FastCompressingCodec
 org.apache.lucene.codecs.compressing.FastDecompressionCompressingCodec
 org.apache.lucene.codecs.compressing.HighCompressionCompressingCodec
 org.apache.lucene.codecs.compressing.dummy.DummyCompressingCodec
-org.apache.lucene.codecs.lucene40.Lucene40RWCodec
-org.apache.lucene.codecs.lucene41.Lucene41RWCodec
-org.apache.lucene.codecs.lucene42.Lucene42RWCodec
-org.apache.lucene.codecs.lucene45.Lucene45RWCodec
-org.apache.lucene.codecs.lucene46.Lucene46RWCodec
-org.apache.lucene.codecs.lucene49.Lucene49RWCodec
diff --git a/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat b/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
index 2086be1..d179833 100644
--- a/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
+++ b/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
@@ -14,4 +14,3 @@
 #  limitations under the License.
 
 org.apache.lucene.codecs.asserting.AssertingDocValuesFormat
-org.apache.lucene.codecs.lucene42.Lucene42RWDocValuesFormat
diff --git a/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
index f85f32d..ed5309c 100644
--- a/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
+++ b/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -21,4 +21,3 @@ org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapFixedInterval
 org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapDocFreqInterval
 org.apache.lucene.codecs.bloom.TestBloomFilteredLucene41Postings
 org.apache.lucene.codecs.asserting.AssertingPostingsFormat
-org.apache.lucene.codecs.lucene40.Lucene40RWPostingsFormat
diff --git a/solr/common-build.xml b/solr/common-build.xml
index 8a33d96..c14e786 100644
--- a/solr/common-build.xml
+++ b/solr/common-build.xml
@@ -88,6 +88,7 @@
     <pathelement location="${analyzers-kuromoji.jar}"/>
     <pathelement location="${analyzers-phonetic.jar}"/>
     <pathelement location="${codecs.jar}"/>
+    <pathelement location="${backward-codecs.jar}"/>
     <pathelement location="${highlighter.jar}"/>
     <pathelement location="${memory.jar}"/>
     <pathelement location="${misc.jar}"/>

