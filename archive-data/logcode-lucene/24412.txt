GitDiffStart: 2ac412f6b7796b6ceaaa1fba26875cdf19d597ec | Fri Feb 11 18:11:05 2005 +0000
diff --git a/contrib/similarity/.cvsignore b/contrib/similarity/.cvsignore
new file mode 100755
index 0000000..9d0b71a
--- /dev/null
+++ b/contrib/similarity/.cvsignore
@@ -0,0 +1,2 @@
+build
+dist
diff --git a/contrib/similarity/README.txt b/contrib/similarity/README.txt
new file mode 100755
index 0000000..11c3956
--- /dev/null
+++ b/contrib/similarity/README.txt
@@ -0,0 +1 @@
+Document similarity measures. 
\ No newline at end of file
diff --git a/contrib/similarity/build.xml b/contrib/similarity/build.xml
new file mode 100755
index 0000000..1eaac43
--- /dev/null
+++ b/contrib/similarity/build.xml
@@ -0,0 +1,10 @@
+<?xml version="1.0"?>
+
+<project name="similarity" default="default">
+
+  <description>
+    Hits highlighter
+  </description>
+
+  <import file="../common.xml"/>
+</project>
diff --git a/contrib/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis.java b/contrib/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
new file mode 100755
index 0000000..b6dcd04
--- /dev/null
+++ b/contrib/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
@@ -0,0 +1,818 @@
+/**
+ * Copyright 2004 The Apache Software Foundation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.search.similar;
+
+import java.io.*;
+import java.util.*;
+import java.net.*;
+
+import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.standard.*;
+import org.apache.lucene.document.*;
+import org.apache.lucene.search.*;
+import org.apache.lucene.index.*;
+import org.apache.lucene.util.*;
+
+/**
+ * Generate "more like this" similarity queries. 
+ * Based on this mail:
+ * <code><pre>
+ * Lucene does let you access the document frequency of terms, with IndexReader.docFreq().
+ * Term frequencies can be computed by re-tokenizing the text, which, for a single document,
+ * is usually fast enough.  But looking up the docFreq() of every term in the document is
+ * probably too slow.
+ * 
+ * You can use some heuristics to prune the set of terms, to avoid calling docFreq() too much,
+ * or at all.  Since you're trying to maximize a tf*idf score, you're probably most interested
+ * in terms with a high tf. Choosing a tf threshold even as low as two or three will radically
+ * reduce the number of terms under consideration.  Another heuristic is that terms with a
+ * high idf (i.e., a low df) tend to be longer.  So you could threshold the terms by the
+ * number of characters, not selecting anything less than, e.g., six or seven characters.
+ * With these sorts of heuristics you can usually find small set of, e.g., ten or fewer terms
+ * that do a pretty good job of characterizing a document.
+ * 
+ * It all depends on what you're trying to do.  If you're trying to eek out that last percent
+ * of precision and recall regardless of computational difficulty so that you can win a TREC
+ * competition, then the techniques I mention above are useless.  But if you're trying to
+ * provide a "more like this" button on a search results page that does a decent job and has
+ * good performance, such techniques might be useful.
+ * 
+ * An efficient, effective "more-like-this" query generator would be a great contribution, if
+ * anyone's interested.  I'd imagine that it would take a Reader or a String (the document's
+ * text), analyzer Analyzer, and return a set of representative terms using heuristics like those
+ * above.  The frequency and length thresholds could be parameters, etc.
+ * 
+ * Doug
+ * </pre></code>
+ *
+ *
+ * <p>
+ * <h3>Initial Usage</h3>
+ *
+ * This class has lots of options to try to make it efficient and flexible.
+ * See the body of {@link #main main()} below in the source for real code, or
+ * if you want pseudo code, the simpliest possible usage is as follows. The bold
+ * fragment is specific to this class.
+ *
+ * <code><pre>
+ *
+ * IndexReader ir = ...
+ * IndexSearcher is = ...
+ * <b>
+ * MoreLikeThis mlt = new MoreLikeThis(ir);
+ * Reader target = ... </b><em>// orig source of doc you want to find similarities to</em><b>
+ * Query query = mlt.like( target);
+ * </b>
+ * Hits hits = is.search(query);
+ * <em>// now the usual iteration thru 'hits' - the only thing to watch for is to make sure
+ * you ignore the doc if it matches your 'target' document, as it should be similar to itself </em>
+ *
+ * </pre></code>
+ *
+ * Thus you:
+ * <ol>
+ * <li> do your normal, Lucene setup for searching,
+ * <li> create a MoreLikeThis,
+ * <li> get the text of the doc you want to find similaries to
+ * <li> then call one of the like() calls to generate a similarity query
+ * <li> call the searcher to find the similar docs
+ * </ol>
+ *
+ * <h3>More Advanced Usage</h3>
+ *
+ * You may want to use {@link #setFieldNames setFieldNames(...)} so you can examine
+ * multiple fields (e.g. body and title) for similarity.
+ * <p>
+ *
+ * Depending on the size of your index and the size and makeup of your documents you
+ * may want to call the other set methods to control how the similarity queries are
+ * generated:
+ * <ul>
+ * <li> {@link #setMinTermFreq setMinTermFreq(...)}
+ * <li> {@link #setMinDocFreq setMinDocFreq(...)}
+ * <li> {@link #setMinWordLen setMinWordLen(...)}
+ * <li> {@link #setMaxWordLen setMaxWordLen(...)}
+ * <li> {@link #setMaxQueryTerms setMaxQueryTerms(...)}
+ * <li> {@link #setMaxNumTokensParsed setMaxNumTokensParsed(...)}
+ * </ul> 
+ *
+ * <hr>
+ * <pre>
+ * Changes: Mark Harwood 29/02/04
+ * Some bugfixing, some refactoring, some optimisation.
+ *  - bugfix: retrieveTerms(int docNum) was not working for indexes without a termvector -added missing code
+ *  - bugfix: No significant terms being created for fields with a termvector - because 
+ *            was only counting one occurence per term/field pair in calculations(ie not including frequency info from TermVector) 
+ *  - refactor: moved common code into isNoiseWord()
+ *  - optimise: when no termvector support available - used maxNumTermsParsed to limit amount of tokenization
+ * </pre>
+ * 
+ * @author David Spencer
+ * @author Bruce Ritchie
+ * @author Mark Harwood
+ */
+public final class MoreLikeThis {
+
+	/**
+	 * Default maximum number of tokens to parse in each example doc field that is not stored with TermVector support.
+	 * @see #getMaxNumTokensParsed
+	 */
+    public static final int DEFAULT_MAX_NUM_TOKENS_PARSED=5000;
+       
+
+	/**
+     * Default analyzer to parse source doc with.
+	 * @see #getAnalyzer
+     */
+    public static final Analyzer DEFAULT_ANALYZER = new StandardAnalyzer();
+
+    /**
+     * Ignore terms with less than this frequency in the source doc.
+	 * @see #getMinTermFreq
+	 * @see #setMinTermFreq	 
+     */
+    public static final int DEFAULT_MIN_TERM_FREQ = 2;
+
+    /**
+     * Ignore words which do not occur in at least this many docs.
+	 * @see #getMinDocFreq
+	 * @see #setMinDocFreq	 
+     */
+    public static final int DEFALT_MIN_DOC_FREQ = 5;
+
+    /**
+     * Boost terms in query based on score.
+	 * @see #isBoost
+	 * @see #setBoost 
+     */
+    public static final boolean DEFAULT_BOOST = false;
+
+    /**
+     * Default field names. Null is used to specify that the field names should be looked
+     * up at runtime from the provided reader.
+     */
+    public static final String[] DEFAULT_FIELD_NAMES = new String[] { "contents"};
+
+    /**
+     * Ignore words less than this length or if 0 then this has no effect.
+	 * @see #getMinWordLen
+	 * @see #setMinWordLen	 
+     */
+    public static final int DEFAULT_MIN_WORD_LENGTH = 0;
+
+    /**
+     * Ignore words greater than this length or if 0 then this has no effect.
+	 * @see #getMaxWordLen
+	 * @see #setMaxWordLen	 
+     */
+    public static final int DEFAULT_MAX_WORD_LENGTH = 0;
+
+    /**
+     * Return a Query with no more than this many terms.
+     *
+     * @see BooleanQuery#getMaxClauseCount
+	 * @see #getMaxQueryTerms
+	 * @see #setMaxQueryTerms	 
+     */
+    public static final int DEFAULT_MAX_QUERY_TERMS = 25;
+
+    /**
+     * Analyzer that will be used to parse the doc.
+     */
+    private Analyzer analyzer = DEFAULT_ANALYZER;
+
+    /**
+     * Ignore words less freqent that this.
+     */
+    private int minTermFreq = DEFAULT_MIN_TERM_FREQ;
+
+    /**
+     * Ignore words which do not occur in at least this many docs.
+     */
+    private int minDocFreq = DEFALT_MIN_DOC_FREQ;
+
+    /**
+     * Should we apply a boost to the Query based on the scores?
+     */
+    private boolean boost = DEFAULT_BOOST;
+
+    /**
+     * Field name we'll analyze.
+     */
+    private String[] fieldNames = DEFAULT_FIELD_NAMES;
+
+	/**
+	 * The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
+	 */
+	private int maxNumTokensParsed=DEFAULT_MAX_NUM_TOKENS_PARSED;   
+    
+
+
+    /**
+     * Ignore words if less than this len.
+     */
+    private int minWordLen = DEFAULT_MIN_WORD_LENGTH;
+
+    /**
+     * Ignore words if greater than this len.
+     */
+    private int maxWordLen = DEFAULT_MAX_WORD_LENGTH;
+
+    /**
+     * Don't return a query longer than this.
+     */
+    private int maxQueryTerms = DEFAULT_MAX_QUERY_TERMS;
+
+    /**
+     * For idf() calculations.
+     */
+    private Similarity similarity = new DefaultSimilarity();
+
+    /**
+     * IndexReader to use
+     */
+    private final IndexReader ir;
+
+    /**
+     * Constructor requiring an IndexReader.
+     */
+    public MoreLikeThis(IndexReader ir) {
+        this.ir = ir;
+    }
+
+    /**
+     * Returns an analyzer that will be used to parse source doc with. The default analyzer
+     * is the {@link #DEFAULT_ANALYZER}.
+     *
+     * @return the analyzer that will be used to parse source doc with.
+	 * @see #DEFAULT_ANALYZER
+     */
+    public Analyzer getAnalyzer() {
+        return analyzer;
+    }
+
+    /**
+     * Sets the analyzer to use. An analyzer is not required for generating a query with the
+     * {@link #like(int)} method, all other 'like' methods require an analyzer.
+     *
+     * @param analyzer the analyzer to use to tokenize text.
+     */
+    public void setAnalyzer(Analyzer analyzer) {
+        this.analyzer = analyzer;
+    }
+
+    /**
+     * Returns the frequency below which terms will be ignored in the source doc. The default
+     * frequency is the {@link #DEFAULT_MIN_TERM_FREQ}.
+     *
+     * @return the frequency below which terms will be ignored in the source doc.
+     */
+    public int getMinTermFreq() {
+        return minTermFreq;
+    }
+
+    /**
+     * Sets the frequency below which terms will be ignored in the source doc.
+     *
+     * @param minTermFreq the frequency below which terms will be ignored in the source doc.
+     */
+    public void setMinTermFreq(int minTermFreq) {
+        this.minTermFreq = minTermFreq;
+    }
+
+    /**
+     * Returns the frequency at which words will be ignored which do not occur in at least this
+     * many docs. The default frequency is {@link #DEFALT_MIN_DOC_FREQ}.
+     *
+     * @return the frequency at which words will be ignored which do not occur in at least this
+     * many docs.
+     */
+    public int getMinDocFreq() {
+        return minDocFreq;
+    }
+
+    /**
+     * Sets the frequency at which words will be ignored which do not occur in at least this
+     * many docs.
+     *
+     * @param minDocFreq the frequency at which words will be ignored which do not occur in at
+     * least this many docs.
+     */
+    public void setMinDocFreq(int minDocFreq) {
+        this.minDocFreq = minDocFreq;
+    }
+
+    /**
+     * Returns whether to boost terms in query based on "score" or not. The default is
+     * {@link #DEFAULT_BOOST}.
+     *
+     * @return whether to boost terms in query based on "score" or not.
+	 * @see #setBoost
+     */
+    public boolean isBoost() {
+        return boost;
+    }
+
+    /**
+     * Sets whether to boost terms in query based on "score" or not.
+     *
+     * @param boost true to boost terms in query based on "score", false otherwise.
+	 * @see #isBoost
+     */
+    public void setBoost(boolean boost) {
+        this.boost = boost;
+    }
+
+    /**
+     * Returns the field names that will be used when generating the 'More Like This' query.
+     * The default field names that will be used is {@link #DEFAULT_FIELD_NAMES}.
+     *
+     * @return the field names that will be used when generating the 'More Like This' query.
+     */
+    public String[] getFieldNames() {
+        return fieldNames;
+    }
+
+    /**
+     * Sets the field names that will be used when generating the 'More Like This' query.
+     * Set this to null for the field names to be determined at runtime from the IndexReader
+     * provided in the constructor.
+     *
+     * @param fieldNames the field names that will be used when generating the 'More Like This'
+     * query.
+     */
+    public void setFieldNames(String[] fieldNames) {
+        this.fieldNames = fieldNames;
+    }
+
+    /**
+     * Returns the minimum word length below which words will be ignored. Set this to 0 for no
+     * minimum word length. The default is {@link #DEFAULT_MIN_WORD_LENGTH}.
+     *
+     * @return the minimum word length below which words will be ignored.
+     */
+    public int getMinWordLen() {
+        return minWordLen;
+    }
+
+    /**
+     * Sets the minimum word length below which words will be ignored.
+     *
+     * @param minWordLen the minimum word length below which words will be ignored.
+     */
+    public void setMinWordLen(int minWordLen) {
+        this.minWordLen = minWordLen;
+    }
+
+    /**
+     * Returns the maximum word length above which words will be ignored. Set this to 0 for no
+     * maximum word length. The default is {@link #DEFAULT_MAX_WORD_LENGTH}.
+     *
+     * @return the maximum word length above which words will be ignored.
+     */
+    public int getMaxWordLen() {
+        return maxWordLen;
+    }
+
+    /**
+     * Sets the maximum word length above which words will be ignored.
+     *
+     * @param maxWordLen the maximum word length above which words will be ignored.
+     */
+    public void setMaxWordLen(int maxWordLen) {
+        this.maxWordLen = maxWordLen;
+    }
+
+    /**
+     * Returns the maximum number of query terms that will be included in any generated query.
+     * The default is {@link #DEFAULT_MAX_QUERY_TERMS}.
+     *
+     * @return the maximum number of query terms that will be included in any generated query.
+     */
+    public int getMaxQueryTerms() {
+        return maxQueryTerms;
+    }
+
+    /**
+     * Sets the maximum number of query terms that will be included in any generated query.
+     *
+     * @param maxQueryTerms the maximum number of query terms that will be included in any
+     * generated query.
+     */
+    public void setMaxQueryTerms(int maxQueryTerms) {
+        this.maxQueryTerms = maxQueryTerms;
+    }
+
+	/**
+	 * @return The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
+	 * @see #DEFAULT_MAX_NUM_TOKENS_PARSED
+	 */
+	public int getMaxNumTokensParsed()
+	{
+		return maxNumTokensParsed;
+	}
+
+	/**
+	 * @param i The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
+	 */
+	public void setMaxNumTokensParsed(int i)
+	{
+		maxNumTokensParsed = i;
+	}
+
+
+
+
+    /**
+     * Return a query that will return docs like the passed lucene document ID.
+     *
+     * @param docNum the documentID of the lucene doc to generate the 'More Like This" query for.
+     * @return a query that will return docs like the passed lucene document ID.
+     */
+    public Query like(int docNum) throws IOException {
+        if (fieldNames == null) {
+            // gather list of valid fields from lucene
+            Collection fields = ir.getFieldNames(true);
+            fieldNames = (String[]) fields.toArray(new String[fields.size()]);
+        }
+
+        return createQuery(retrieveTerms(docNum));
+    }
+
+    /**
+     * Return a query that will return docs like the passed file.
+     *
+     * @return a query that will return docs like the passed file.
+     */
+    public Query like(File f) throws IOException {
+        if (fieldNames == null) {
+            // gather list of valid fields from lucene
+            Collection fields = ir.getFieldNames(true);
+            fieldNames = (String[]) fields.toArray(new String[fields.size()]);
+        }
+
+        return like(new FileReader(f));
+    }
+
+    /**
+     * Return a query that will return docs like the passed URL.
+     *
+     * @return a query that will return docs like the passed URL.
+     */
+    public Query like(URL u) throws IOException {
+        return like(new InputStreamReader(u.openConnection().getInputStream()));
+    }
+
+    /**
+     * Return a query that will return docs like the passed stream.
+     *
+     * @return a query that will return docs like the passed stream.
+     */
+    public Query like(java.io.InputStream is) throws IOException {
+        return like(new InputStreamReader(is));
+    }
+
+    /**
+     * Return a query that will return docs like the passed Reader.
+     *
+     * @return a query that will return docs like the passed Reader.
+     */
+    public Query like(Reader r) throws IOException {
+        return createQuery(retrieveTerms(r));
+    }
+
+    /**
+     * Create the More like query from a PriorityQueue
+     */
+    private Query createQuery(PriorityQueue q) {
+        BooleanQuery query = new BooleanQuery();
+        Object cur;
+        int qterms = 0;
+        float bestScore = 0;
+
+        while (((cur = q.pop()) != null)) {
+            Object[] ar = (Object[]) cur;
+            TermQuery tq = new TermQuery(new Term((String) ar[1], (String) ar[0]));
+
+            if (boost) {
+                if (qterms == 0) {
+                    bestScore = ((Float) ar[2]).floatValue();
+                }
+                float myScore = ((Float) ar[2]).floatValue();
+
+                tq.setBoost(myScore / bestScore);
+            }
+
+            try {
+                query.add(tq, false, false);
+            }
+            catch (BooleanQuery.TooManyClauses ignore) {
+                break;
+            }
+
+            qterms++;
+            if (maxQueryTerms > 0 && qterms >= maxQueryTerms) {
+                break;
+            }
+        }
+
+        return query;
+    }
+
+    /**
+     * Create a PriorityQueue from a word->tf map.
+     *
+     * @param words a map of words keyed on the word(String) with Int objects as the values.
+     */
+    private PriorityQueue createQueue(Map words) throws IOException {
+        // have collected all words in doc and their freqs
+        int numDocs = ir.numDocs();
+        FreqQ res = new FreqQ(words.size()); // will order words by score
+
+        Iterator it = words.keySet().iterator();
+        while (it.hasNext()) { // for every word
+            String word = (String) it.next();
+
+            int tf = ((Int) words.get(word)).x; // term freq in the source doc
+            if (minTermFreq > 0 && tf < minTermFreq) {
+                continue; // filter out words that don't occur enough times in the source
+            }
+
+            // go through all the fields and find the largest document frequency
+            String topField = fieldNames[0];
+            int docFreq = 0;
+            for (int i = 0; i < fieldNames.length; i++) {
+                int freq = ir.docFreq(new Term(fieldNames[i], word));
+                topField = (freq > docFreq) ? fieldNames[i] : topField;
+                docFreq = (freq > docFreq) ? freq : docFreq;
+            }
+
+            if (minDocFreq > 0 && docFreq < minDocFreq) {
+                continue; // filter out words that don't occur in enough docs
+            }
+
+            if (docFreq == 0) {
+                continue; // index update problem?
+            }
+
+            float idf = similarity.idf(docFreq, numDocs);
+            float score = tf * idf;
+
+            // only really need 1st 3 entries, other ones are for troubleshooting
+            res.insert(new Object[]{word,                   // the word
+                                    topField,               // the top field
+                                    new Float(score),       // overall score
+                                    new Float(idf),         // idf
+                                    new Integer(docFreq),   // freq in all docs
+                                    new Integer(tf)
+            });
+        }
+        return res;
+    }
+
+    /**
+     * Describe the parameters that control how the "more like this" query is formed.
+     */
+    public String describeParams() {
+        StringBuffer sb = new StringBuffer();
+        sb.append("\t" + "maxQueryTerms  : " + maxQueryTerms + "\n");
+        sb.append("\t" + "minWordLen     : " + minWordLen + "\n");
+        sb.append("\t" + "maxWordLen     : " + maxWordLen + "\n");
+        sb.append("\t" + "fieldNames     : \"");
+        String delim = "";
+        for (int i = 0; i < fieldNames.length; i++) {
+            String fieldName = fieldNames[i];
+            sb.append(delim).append(fieldName);
+            delim = ", ";
+        }
+        sb.append("\n");
+        sb.append("\t" + "boost          : " + boost + "\n");
+        sb.append("\t" + "minTermFreq    : " + minTermFreq + "\n");
+        sb.append("\t" + "minDocFreq     : " + minDocFreq + "\n");
+        return sb.toString();
+    }
+
+    /**
+     * Test driver.
+     * Pass in "-i INDEX" and then either "-fn FILE" or "-url URL".
+     */
+    public static void main(String[] a) throws Throwable {
+        String indexName = "localhost_index";
+        String fn = "c:/Program Files/Apache Group/Apache/htdocs/manual/vhosts/index.html.en";
+        URL url = null;
+        for (int i = 0; i < a.length; i++) {
+            if (a[i].equals("-i")) {
+                indexName = a[++i];
+            }
+            else if (a[i].equals("-f")) {
+                fn = a[++i];
+            }
+            else if (a[i].equals("-url")) {
+                url = new URL(a[++i]);
+            }
+        }
+
+        PrintStream o = System.out;
+        IndexReader r = IndexReader.open(indexName);
+        o.println("Open index " + indexName + " which has " + r.numDocs() + " docs");
+
+        MoreLikeThis mlt = new MoreLikeThis(r);
+
+        o.println("Query generation parameters:");
+        o.println(mlt.describeParams());
+        o.println();
+
+        Query query = null;
+        if (url != null) {
+            o.println("Parsing URL: " + url);
+            query = mlt.like(url);
+        }
+        else if (fn != null) {
+            o.println("Parsing file: " + fn);
+            query = mlt.like(new File(fn));
+        }
+
+        o.println("q: " + query);
+        o.println();
+        IndexSearcher searcher = new IndexSearcher(indexName);
+
+        Hits hits = searcher.search(query);
+        int len = hits.length();
+        o.println("found: " + len + " documents matching");
+        o.println();
+        for (int i = 0; i < Math.min(25, len); i++) {
+            Document d = hits.doc(i);
+            o.println("score  : " + hits.score(i));
+            o.println("url    : " + d.get("url"));
+            o.println("\ttitle  : " + d.get("title"));
+            o.println("\tsummary: " + d.get("summary"));
+            o.println();
+        }
+    }
+
+    /**
+     * Find words for a more-like-this query former.
+     *
+     * @param docNum the id of the lucene document from which to find terms
+     */
+    private PriorityQueue retrieveTerms(int docNum) throws IOException {
+        Map termFreqMap = new HashMap();
+        for (int i = 0; i < fieldNames.length; i++) {
+            String fieldName = fieldNames[i];
+            TermFreqVector vector = ir.getTermFreqVector(docNum, fieldName);
+
+            // field does not store term vector info
+            if (vector == null) {
+            	Document d=ir.document(docNum);
+            	String text=d.get(fieldName);
+            	if(text!=null)
+            	{
+					addTermFrequencies(new StringReader(text), termFreqMap, fieldName);
+            	}
+            }
+            else {
+				addTermFrequencies(termFreqMap, vector);
+            }
+
+        }
+
+        return createQueue(termFreqMap);
+    }
+
+	/**
+	 * Adds terms and frequencies found in vector into the Map termFreqMap
+	 * @param termFreqMap a Map of terms and their frequencies
+	 * @param vector List of terms and their frequencies for a doc/field
+	 */
+	private void addTermFrequencies(Map termFreqMap, TermFreqVector vector)
+	{
+		String[] terms = vector.getTerms();
+		int freqs[]=vector.getTermFrequencies();
+		for (int j = 0; j < terms.length; j++) {
+		    String term = terms[j];
+		
+			if(isNoiseWord(term)){
+				continue;
+			}
+		    // increment frequency
+		    Int cnt = (Int) termFreqMap.get(term);
+		    if (cnt == null) {
+		    	cnt=new Int();
+				termFreqMap.put(term, cnt);
+				cnt.x=freqs[j];				
+		    }
+		    else {
+		        cnt.x+=freqs[j];
+		    }
+		}
+	}
+	/**
+	 * Adds term frequencies found by tokenizing text from reader into the Map words
+	 * @param r a source of text to be tokenized
+	 * @param termFreqMap a Map of terms and their frequencies
+	 * @param fieldName Used by analyzer for any special per-field analysis
+	 */
+	private void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)
+		throws IOException
+	{
+		   TokenStream ts = analyzer.tokenStream(fieldName, r);
+			org.apache.lucene.analysis.Token token;
+			int tokenCount=0;
+			while ((token = ts.next()) != null) { // for every token
+				String word = token.termText();
+				tokenCount++;
+				if(tokenCount>maxNumTokensParsed)
+				{
+					break;
+				}
+				if(isNoiseWord(word)){
+					continue;
+				}
+				
+				// increment frequency
+				Int cnt = (Int) termFreqMap.get(word);
+				if (cnt == null) {
+					termFreqMap.put(word, new Int());
+				}
+				else {
+					cnt.x++;
+				}
+			}
+	}
+	
+	
+	/** determines if the passed term is likely to be of interest in "more like" comparisons 
+	 * 
+	 * @param term The word being considered
+	 * @return true if should be ignored, false if should be used in further analysis
+	 */
+	private boolean isNoiseWord(String term)
+	{
+		int len = term.length();
+		if (minWordLen > 0 && len < minWordLen) {
+			return true;
+		}
+		if (maxWordLen > 0 && len < maxWordLen) {
+			return true;
+		}
+		return false;
+	}
+	
+
+    /**
+     * Find words for a more-like-this query former.
+     *
+     * @param r the reader that has the content of the document
+     */
+    public PriorityQueue retrieveTerms(Reader r) throws IOException {
+        Map words = new HashMap();
+        for (int i = 0; i < fieldNames.length; i++) {
+            String fieldName = fieldNames[i];
+			addTermFrequencies(r, words, fieldName);
+        }
+        return createQueue(words);
+    }
+
+    /**
+     * PriorityQueue that orders words by score.
+     */
+    private static class FreqQ extends PriorityQueue {
+        FreqQ (int s) {
+            initialize(s);
+        }
+
+        protected boolean lessThan(Object a, Object b) {
+            Object[] aa = (Object[]) a;
+            Object[] bb = (Object[]) b;
+            Float fa = (Float) aa[2];
+            Float fb = (Float) bb[2];
+            return fa.floatValue() > fb.floatValue();
+        }
+    }
+
+    /**
+     * Use for frequencies and to avoid renewing Integers.
+     */
+    private static class Int {
+        int x;
+
+        Int() {
+            x = 1;
+        }
+    }
+    
+    
+}
diff --git a/contrib/similarity/src/java/org/apache/lucene/search/similar/SimilarityQueries.java b/contrib/similarity/src/java/org/apache/lucene/search/similar/SimilarityQueries.java
new file mode 100755
index 0000000..f04bb5a
--- /dev/null
+++ b/contrib/similarity/src/java/org/apache/lucene/search/similar/SimilarityQueries.java
@@ -0,0 +1,118 @@
+/**
+ * Copyright 2004 The Apache Software Foundation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.search.similar;
+
+import java.io.*;
+import java.util.*;
+import java.net.*;
+
+import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.standard.*;
+import org.apache.lucene.document.*;
+import org.apache.lucene.search.*;
+import org.apache.lucene.index.*;
+import org.apache.lucene.util.*;
+
+/**
+ * Simple similarity measures.
+ *
+ *
+ * @see MoreLikeThis
+ */
+public final class SimilarityQueries
+{
+	/**
+	 *
+	 */
+	private SimilarityQueries()
+	{
+	}
+	
+	/**
+     * Simple similarity query generators.
+	 * Takes every unique word and forms a boolean query where all words are optional.
+	 * After you get this you'll use to to query your {@link IndexSearcher} for similar docs.
+	 * The only caveat is the first hit returned <b>should be</b> your source document - you'll
+	 * need to then ignore that.
+	 *
+	 * <p>
+	 *
+	 * So, if you have a code fragment like this:
+	 * <br>
+	 * <code>
+	 * Query q = formSimilaryQuery( "I use Lucene to search fast. Fast searchers are good", new StandardAnalyzer(), "contents", null);
+	 * </code>
+	 *
+	 * <p>
+	 *
+	 
+	 * The query returned, in string form, will be <code>'(i use lucene to search fast searchers are good')</code>.
+	 *
+	 * <p>
+	 * The philosophy behind this method is "two documents are similar if they share lots of words".
+	 * Note that behind the scenes, Lucenes scoring algorithm will tend to give two documents a higher similarity score if the share more uncommon words.
+	 *
+	 * <P>
+	 * This method is fail-safe in that if a long 'body' is passed in and
+	 * {@link BooleanQuery#add BooleanQuery.add()} (used internally)
+	 * throws
+	 * {@link org.apache.lucene.search.BooleanQuery.TooManyClauses BooleanQuery.TooManyClauses}, the
+	 * query as it is will be returned.
+	 *
+	 * 
+	 * 
+	 *
+	 *
+	 * @param body the body of the document you want to find similar documents to
+	 * @param a the analyzer to use to parse the body
+	 * @param field the field you want to search on, probably something like "contents" or "body"
+	 * @param stop optional set of stop words to ignore
+	 * @return a query with all unique words in 'body'
+	 * @throws IOException this can't happen...
+	 */
+    public static Query formSimilarQuery( String body,
+										  Analyzer a,
+										  String field,
+										  Set stop)
+										  throws IOException
+	{	
+		TokenStream ts = a.tokenStream( field, new StringReader( body));
+		org.apache.lucene.analysis.Token t;
+		BooleanQuery tmp = new BooleanQuery();
+		Set already = new HashSet(); // ignore dups
+		while ( (t = ts.next()) != null)
+		{
+			String word = t.termText();
+			// ignore opt stop words
+			if ( stop != null &&
+				 stop.contains( word)) continue;
+			// ignore dups
+			if ( ! already.add( word)) continue;
+			// add to query
+			TermQuery tq = new TermQuery( new Term( field, word));
+			try
+			{
+				tmp.add( tq, false, false);
+			}
+			catch( BooleanQuery.TooManyClauses too)
+			{
+				// fail-safe, just return what we have, not the end of the world
+				break;
+			}
+		}
+		return tmp;
+	}
+}
diff --git a/contrib/similarity/src/java/org/apache/lucene/search/similar/package.html b/contrib/similarity/src/java/org/apache/lucene/search/similar/package.html
new file mode 100755
index 0000000..c75171e
--- /dev/null
+++ b/contrib/similarity/src/java/org/apache/lucene/search/similar/package.html
@@ -0,0 +1,5 @@
+<html>
+<body>
+Document similarity query generators.
+</body>
+</html>
\ No newline at end of file
diff --git a/contrib/spellchecker/build.xml b/contrib/spellchecker/build.xml
new file mode 100755
index 0000000..d6f8917
--- /dev/null
+++ b/contrib/spellchecker/build.xml
@@ -0,0 +1,156 @@
+<?xml version="1.0" encoding="UTF-8"?>
+
+<project basedir="." default="rebuild" name="Spelling checker">
+
+<property name="lucene.lib" value="d:/dev/lib/lucene.jar"/>
+<property name="lucenetest.lib" value="D:/dev/jakarta-lucene/build/classes/test"/>
+
+
+<property name="name" value="spellchecker"/>
+<property name="Name" value="spellchecker"/>
+<property name="version" value="1.1"/>
+<property name="year" value="2004"/>
+<property name="final.name" value="${name}-${version}"/>
+<property name="java" location="src/java"/>
+<property name="test" location="src/test"/>
+<property name="build.dir" location="build"/>
+<property name="build.java" location="${build.dir}/classes/java"/>
+<property name="build.test" location="${build.dir}/classes/test"/>
+<property name="build.javadocs" location="doc"/>
+<property name="javadoc.link" value="http://java.sun.com/j2se/1.4/docs/api/"/>
+<property name="javac.debug" value="off"/>
+<property name="junit.output.dir" location="${build.dir}/test"/>
+<property name="junit.reports" location="${build.dir}/test/reports"/>
+
+
+
+   <!-- Build classpath -->
+  <path id="classpath">
+    <pathelement location="${lucene.lib}"/>
+
+    <pathelement location="${build.java}"/>
+  </path>
+
+  <path id="test.classpath">
+    <path refid="classpath"/>
+	<pathelement location="${lucenetest.lib}"/>
+    <pathelement location="${build.dir}/classes/test"/>
+  </path>
+  <!--Patternset to exclude files from the output directory:-->
+
+  <!-- ================================================================== -->
+  <!-- C O M P I L E                                                      -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+  <target name="javacompile"
+    description="Compiles core classes">
+    <mkdir dir="${build.java}"/>
+    <javac
+      srcdir="${java}"
+      includes="**/*.java"
+      destdir="${build.java}"
+      debug="${javac.debug}"
+	  optimize="on">
+      <classpath refid="classpath"/>
+    </javac>
+  </target>
+
+  <!-- ================================================================== -->
+  <!-- J A R                                                              -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+  <target name="jar"  depends="javacompile" description="Generates the Jar file">
+    <jar 
+	destfile="${build.dir}/${final.name}.jar"
+    basedir="${build.java}" />
+  </target>
+
+  <!-- ================================================================== -->
+  <!-- J A V A D O C                                                      -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+  <target name="javadoc">
+    <mkdir dir="${build.javadocs}"/>
+    <javadoc
+      sourcepath="${java}"
+      overview="src/java/overview.html"
+      packagenames="org.apache.lucene.*"
+      destdir="${build.javadocs}"
+      author="true"
+      version="true"
+      use="true"
+      link="${javadoc.link}"
+      windowtitle="${Name} ${version} API"
+      doctitle="${Name} ${version} API"
+      bottom="Author: Nicolas Maisonneuve (${year})"  >
+      </javadoc>
+  </target>
+
+  <!-- ================================================================== -->
+  <!-- C L E A N                                                          -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+  <target name="clean">
+    <delete failonerror="false" includeemptydirs="true">
+      <fileset dir="${build.dir}"/>
+    </delete>
+  </target>
+
+
+  <!-- ================================================================== -->
+  <!-- B U I L D  T E S T                                                 -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+  <target name="compile-test" depends="javacompile">
+    <mkdir dir="${build.test}"/>
+    <javac
+      srcdir="${test}"
+      includes="**/*.java"
+      destdir="${build.test}"
+      debug="true">
+      <classpath refid="test.classpath"/>
+    </javac>
+  </target>
+
+  <!-- ================================================================== -->
+  <!-- R U N  T E S T S                                                   -->
+  <!-- ================================================================== -->
+  <!--                                                                    -->
+  <!-- ================================================================== -->
+  <target name="test" depends="compile-test" description="Runs unit tests">
+    <fail unless="junit.present">
+      ##################################################################
+      JUnit not found.
+      Please make sure junit.jar is in ANT_HOME/lib, or made available
+      to Ant using other mechanisms like -lib or CLASSPATH.
+      ##################################################################
+	  </fail>
+    <mkdir dir="${junit.output.dir}"/>
+    <junit printsummary="off" haltonfailure="no"
+      errorProperty="tests.failed" failureProperty="tests.failed">
+      <classpath refid="junit.classpath"/>
+      <sysproperty key="dataDir" file="src/test"/>
+      <sysproperty key="tempDir" file="${build.dir}/test"/>
+      <formatter type="xml"/>
+      <formatter type="brief" usefile="false"/>
+      <batchtest fork="yes" todir="${junit.output.dir}" unless="testcase">
+        <fileset dir="src/test" includes="**/Test*.java"/>
+      </batchtest>
+      <batchtest fork="yes" todir="${junit.output.dir}" if="testcase">
+        <fileset dir="src/test" includes="**/${testcase}.java"/>
+      </batchtest>
+    </junit>
+
+    <fail if="tests.failed">Tests failed!</fail>
+  </target>
+
+  <target depends="javacompile" name="make"/>
+
+  <target depends="clean,make" name="rebuild"/>
+
+</project>
\ No newline at end of file
diff --git a/contrib/spellchecker/src/java/org/apache/lucene/search/spell/Dictionary.java b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/Dictionary.java
new file mode 100755
index 0000000..979621a
--- /dev/null
+++ b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/Dictionary.java
@@ -0,0 +1,33 @@
+package org.apache.lucene.search.spell;
+/**
+ * Copyright 2002-2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Iterator;
+
+/**
+ * A simple interface representing a Dictionary
+ * @author Nicolas Maisonneuve
+ * @version 1.0
+ */
+public interface Dictionary {
+
+    /**
+     * return all the words present in the dictionnary
+     * @return Iterator
+     */
+    public Iterator getWordsIterator();
+
+}
diff --git a/contrib/spellchecker/src/java/org/apache/lucene/search/spell/LuceneDictionary.java b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/LuceneDictionary.java
new file mode 100755
index 0000000..d94cedb
--- /dev/null
+++ b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/LuceneDictionary.java
@@ -0,0 +1,94 @@
+package org.apache.lucene.search.spell;
+
+/**
+ * Copyright 2002-2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.IndexReader;
+import java.util.Iterator;
+import org.apache.lucene.index.TermEnum;
+import org.apache.lucene.index.Term;
+import java.io.*;
+
+/**
+ *  Lucene Dictionnary
+ * @author Nicolas Maisonneuve
+ */
+public class LuceneDictionary
+implements Dictionary {
+    IndexReader reader;
+    String field;
+
+    public LuceneDictionary (IndexReader reader, String field) {
+        this.reader=reader;
+        this.field=field;
+
+    }
+
+
+    public final Iterator getWordsIterator () {
+        return new LuceneIterator();
+    }
+
+
+final  class LuceneIterator    implements Iterator {
+      private  TermEnum enum;
+      private  Term actualTerm;
+      private  boolean has_next_called;
+
+        public LuceneIterator () {
+            try {
+                enum=reader.terms(new Term(field, ""));
+            }
+            catch (IOException ex) {
+                ex.printStackTrace();
+            }
+        }
+
+
+        public Object next () {
+            if (!has_next_called)  {hasNext();}
+             has_next_called=false;
+            return (actualTerm!=null) ? actualTerm.text(): null;
+        }
+
+
+        public boolean hasNext () {
+             has_next_called=true;
+            try {
+                // if there is still words
+                if (!enum.next()) {
+                    actualTerm=null;
+                    return false;
+                }
+                //  if the next word are in the field
+                actualTerm=enum.term();
+                String fieldt=actualTerm.field();
+                if (fieldt!=field) {
+                    actualTerm=null;
+                    return false;
+                }
+                return true;
+            }
+            catch (IOException ex) {
+                ex.printStackTrace();
+                return false;
+            }
+        }
+
+
+        public void remove () {};
+    }
+}
diff --git a/contrib/spellchecker/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java
new file mode 100755
index 0000000..230b923
--- /dev/null
+++ b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.search.spell;
+
+/**
+ * Copyright 2002-2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import java.util.Iterator;
+import java.io.InputStream;
+import java.io.BufferedReader;
+import java.io.InputStreamReader;
+import java.io.*;
+
+
+/**
+ * dictionary represented by a file text
+ * Format allowed: 1 word per line:
+ * word1
+ * word2
+ * word3
+ *
+ * @author Nicolas Maisonneuve
+ */
+public class PlainTextDictionary implements Dictionary {
+
+    private BufferedReader in;
+    private String line;
+    private boolean has_next_called;
+
+    public PlainTextDictionary (File file) throws FileNotFoundException {
+        in=new BufferedReader(new FileReader(file));
+    }
+
+
+    public PlainTextDictionary (InputStream dictFile) {
+        in=new BufferedReader(new InputStreamReader(System.in));
+    }
+
+
+    public Iterator getWordsIterator () {
+
+        return new fileIterator();
+    }
+
+
+    final class fileIterator
+    implements Iterator {
+        public Object next () {
+            if (!has_next_called) {
+                hasNext();
+            }
+            has_next_called=false;
+            return line;
+        }
+
+
+        public boolean hasNext () {
+            has_next_called=true;
+            try {
+                line=in.readLine();
+            }
+            catch (IOException ex) {
+                ex.printStackTrace();
+                line=null;
+                return false;
+            }
+            return (line!=null)?true:false;
+        }
+
+
+        public void remove () {};
+    }
+
+}
diff --git a/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
new file mode 100755
index 0000000..93be0a6
--- /dev/null
+++ b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
@@ -0,0 +1,363 @@
+package org.apache.lucene.search.spell;
+
+
+/**
+ * Copyright 2002-2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TermEnum;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Hits;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.Directory;
+import java.util.*;
+
+
+/**
+ *  <p>
+ *	Spell Checker class  (Main class) <br/>
+ * (initially inspired by the David Spencer code)
+ *  </p>
+ *  
+ *  <p>
+ *  Spell Checker spellchecker= new SpellChecker (spellDirectory);<br/>
+ *  <br/>
+ *  //To index a field of a user index <br/>
+ *  spellchecker.indexDictionary(new LuceneDictionary(my_lucene_reader, a_field));<br/>
+ *<br/>
+ *   //To index a file containing words  <br/>
+ *  spellchecker.indexDictionary(new PlainTextDictionary(new File("myfile.txt")));<br/>
+ *</p>
+ * 
+ * @author Nicolas Maisonneuve
+ * @version 1.0
+ */
+public class SpellChecker {
+
+    /**
+     * Field name for each word in the ngram index.
+     */
+    public static final String F_WORD="word";
+
+
+    /**
+     * the spell index
+     */
+    Directory spellindex;
+
+    /**
+     * Boost value for start and end grams
+     */private float bStart=2.0f;
+      private float bEnd=1.0f;
+  
+
+    private IndexReader reader;
+    float min=0.5f;
+
+    public void setSpellIndex (Directory spellindex) {
+        this.spellindex=spellindex;
+    }
+
+
+    /**
+     *  Set the accuraty 0<min<1 default 0.5
+     * @param min float
+     */
+    public void setAccuraty (float min) {
+        this.min=min;
+    }
+
+
+    public SpellChecker (Directory gramIndex) {
+        this.setSpellIndex(gramIndex);
+    }
+
+
+    /**
+     * Suggest similar words
+     * @param word String the word you want a spell check done on
+     * @param num_sug int the number of suggest words
+     * @throws IOException
+     * @return String[]
+     */
+    public String[] suggestSimilar (String word, int num_sug) throws IOException {
+        return this.suggestSimilar(word, num_sug, null, null, false);
+    }
+
+
+    /**
+     * Suggest similar words (restricted or not of a field of a user index)
+     * @param word String the word you want a spell check done on
+     * @param num_sug int the number of suggest words
+     * @param IndexReader the indexReader of the user index (can be null see field param)
+     * @param field String the field of the user index: if field is not null ,the suggest
+     * words are restricted to the words present in this field.
+     * @param morePopular boolean return only the suggest words that are more frequent than the searched word
+     * (only if restricted mode = (indexReader!=null and field!=null)
+     * @throws IOException
+     * @return String[] the sorted list of the suggest words with this 2 criteri
+     * first criteria : the edit distance, second criteria (only if restricted mode): the popularity
+     * of the suggest words in the field of the user index
+     */
+    public String[] suggestSimilar (String word, int num_sug, IndexReader ir, String field
+    , boolean morePopular) throws IOException {
+
+        final TRStringDistance sd=new TRStringDistance(word);
+        final int lengthWord=word.length();
+
+        final int goalFreq=(morePopular&&ir!=null)?ir.docFreq(new Term(field, word)):0;
+        if (!morePopular&&goalFreq>0) {
+            return new String[] {
+            word}; // return the word if it exist in the index and i don't want a more popular word
+        }
+
+        BooleanQuery query=new BooleanQuery();
+        String[] grams;
+        String key;
+
+        for (int ng=getMin(lengthWord); ng<=getMax(lengthWord); ng++) {
+
+            key="gram"+ng; // form key
+
+            grams=formGrams(word, ng); // form word into ngrams (allow dups too)
+
+            if (grams.length==0) {
+                continue; // hmm
+            }
+
+            if (bStart>0) { // should we boost prefixes?
+                add(query, "start"+ng, grams[0], bStart); // matches start of word
+
+            }
+            if (bEnd>0) { // should we boost suffixes
+                add(query, "end"+ng, grams[grams.length-1], bEnd); // matches end of word
+
+            }
+            for (int i=0; i<grams.length; i++) {
+                add(query, key, grams[i]);
+            }
+
+        }
+
+        IndexSearcher searcher=new IndexSearcher(this.spellindex);
+        Hits hits=searcher.search(query);
+        SuggestWordQueue sugqueue=new SuggestWordQueue(num_sug);
+
+        int stop=Math.min(hits.length(), 10*num_sug); // go thru more than 'maxr' matches in case the distance filter triggers
+        SuggestWord sugword=new SuggestWord();
+        for (int i=0; i<stop; i++) {
+
+            sugword.string=hits.doc(i).get(F_WORD); // get orig word)
+
+            if (sugword.string==word) {
+                continue; // don't suggest a word for itself, that would be silly
+            }
+
+            //edit distance/normalize with the min word length
+            sugword.score=1.0f-((float) sd.getDistance(sugword.string)/Math.min(sugword.string.length(), lengthWord));
+            if (sugword.score<min) {
+                continue;
+            }
+
+            if (ir!=null) { // use the user index
+                sugword.freq=ir.docFreq(new Term(field, sugword.string)); // freq in the index
+                if ((morePopular&&goalFreq>sugword.freq)||sugword.freq<1) { // don't suggest a word that is not present in the field
+                    continue;
+                }
+            }
+            sugqueue.insert(sugword);
+            if (sugqueue.size()==num_sug) {
+                //if queue full , maintain the min score
+                min=((SuggestWord) sugqueue.top()).score;
+            }
+            sugword=new SuggestWord();
+        }
+
+        // convert to array string
+        String[] list=new String[sugqueue.size()];
+        for (int i=sugqueue.size()-1; i>=0; i--) {
+            list[i]=((SuggestWord) sugqueue.pop()).string;
+        }
+
+        searcher.close();
+        return list;
+    }
+
+
+    /**
+     * Add a clause to a boolean query.
+     */
+    private static void add (BooleanQuery q, String k, String v, float boost) {
+        Query tq=new TermQuery(new Term(k, v));
+        tq.setBoost(boost);
+        q.add(new BooleanClause(tq, false, false));
+    }
+
+
+    /**
+     * Add a clause to a boolean query.
+     */
+    private static void add (BooleanQuery q, String k, String v) {
+        q.add(new BooleanClause(new TermQuery(new Term(k, v)), false, false));
+    }
+
+
+    /**
+     * Form all ngrams for a given word.
+     * @param text the word to parse
+     * @param ng the ngram length e.g. 3
+     * @return an array of all ngrams in the word and note that duplicates are not removed
+     */
+    private static String[] formGrams (String text, int ng) {
+        int len=text.length();
+        String[] res=new String[len-ng+1];
+        for (int i=0; i<len-ng+1; i++) {
+            res[i]=text.substring(i, i+ng);
+        }
+        return res;
+    }
+
+
+    public void clearIndex () throws IOException {
+        IndexReader.unlock(spellindex);
+        IndexWriter writer=new IndexWriter(spellindex, null, true);
+        writer.close();
+    }
+
+
+    /**
+     * if the word exist in the index
+     * @param word String
+     * @throws IOException
+     * @return boolean
+     */
+    public boolean exist (String word) throws IOException {
+        if (reader==null) {
+            reader=IndexReader.open(spellindex);
+        }
+        return reader.docFreq(new Term(F_WORD, word))>0;
+    }
+
+
+    /**
+     * Index a Dictionnary
+     * @param dict the dictionnary to index
+     * @throws IOException
+     */
+    public void indexDictionnary (Dictionary dict) throws IOException {
+
+        int ng1, ng2;
+        IndexReader.unlock(spellindex);
+        IndexWriter writer=new IndexWriter(spellindex, new WhitespaceAnalyzer(), !IndexReader.indexExists(spellindex));
+        writer.mergeFactor=300;
+        writer.minMergeDocs=150;
+
+        Iterator iter=dict.getWordsIterator();
+        while (iter.hasNext()) {
+            String word=(String) iter.next();
+
+            int len=word.length();
+            if (len<3) {
+                continue; // too short we bail but "too long" is fine...
+            }
+
+            if (this.exist(word)) { // if the word already exist in the gramindex
+                continue;
+            }
+
+            // ok index the word
+            Document doc=createDocument(word, getMin(len), getMax(len));
+            writer.addDocument(doc);
+        }
+        // close writer
+        writer.optimize();
+        writer.close();
+
+        // close reader
+        reader.close();
+        reader=null;
+    }
+
+
+    private int getMin (int l) {
+        if (l>5) {
+            return 3;
+        }
+        if (l==5) {
+            return 2;
+        }
+        return 1;
+    }
+
+
+    private int getMax (int l) {
+        if (l>5) {
+            return 4;
+        }
+        if (l==5) {
+            return 3;
+        }
+        return 2;
+
+    }
+
+
+    private static Document createDocument (String text, int ng1, int ng2) {
+        Document doc=new Document();
+        doc.add(Field.Keyword(F_WORD, text)); // orig term
+        addGram(text, doc, ng1, ng2);
+        return doc;
+    }
+
+
+    private static void addGram (String text, Document doc, int ng1, int ng2) {
+        int len=text.length();
+        for (int ng=ng1; ng<=ng2; ng++) {
+            String key="gram"+ng;
+            String end=null;
+            for (int i=0; i<len-ng+1; i++) {
+                String gram=text.substring(i, i+ng);
+                doc.add(Field.Keyword(key, gram));
+                if (i==0) {
+                    doc.add(Field.Keyword("start"+ng, gram));
+                }
+                end=gram;
+            }
+            if (end!=null) { // may not be present if len==ng1
+                doc.add(Field.Keyword("end"+ng, end));
+            }
+        }
+    }
+
+
+    protected void finalize () throws Throwable {
+        if (reader!=null) {
+            reader.close();
+        }
+    }
+
+}
diff --git a/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWord.java b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWord.java
new file mode 100755
index 0000000..722d1a3
--- /dev/null
+++ b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWord.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.search.spell;
+
+
+/**
+ * Copyright 2002-2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ *  SuggestWord Class
+ *  used in suggestSimilat method in SpellChecker class
+ *  @author Nicolas Maisonneuve
+ */
+ final class SuggestWord {
+    /**
+     * the score of the word
+     */
+    public float score;
+
+
+    /**
+     * The freq of the word
+     */
+    public int freq;
+
+
+    /**
+     * the suggested word
+     */
+    public String string;
+
+
+    public final int compareTo (SuggestWord a) {
+        //first criteria: the edit distance
+        if (score>a.score) {
+            return 1;
+        }
+        if (score<a.score) {
+            return-1;
+        }
+
+        //second criteria (if first criteria is equal): the popularity
+        if (freq>a.freq) {
+            return 1;
+        }
+
+        if (freq<a.freq) {
+            return-1;
+        }
+
+        return 0;
+    }
+}
diff --git a/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWordQueue.java b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWordQueue.java
new file mode 100755
index 0000000..a96c29d
--- /dev/null
+++ b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWordQueue.java
@@ -0,0 +1,41 @@
+package org.apache.lucene.search.spell;
+
+
+/**
+ * Copyright 2002-2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ *  to sort SuggestWord
+ * @author Nicolas Maisonneuve
+ */
+import org.apache.lucene.util.PriorityQueue;
+
+
+final class SuggestWordQueue
+extends PriorityQueue {
+
+    SuggestWordQueue (int size) {
+        initialize(size);
+    }
+
+    protected final boolean lessThan (Object a, Object b) {
+        SuggestWord wa=(SuggestWord) a;
+        SuggestWord wb=(SuggestWord) b;
+        int val=wa.compareTo(wb);
+        return val<0;
+    }
+
+}
diff --git a/contrib/spellchecker/src/java/org/apache/lucene/search/spell/TRStringDistance.java b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/TRStringDistance.java
new file mode 100755
index 0000000..992d3bb
--- /dev/null
+++ b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/TRStringDistance.java
@@ -0,0 +1,132 @@
+package org.apache.lucene.search.spell;
+
+
+/**
+ * Copyright 2002-2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Edit distance  class
+ */
+public final class TRStringDistance {
+
+    final char[] sa;
+    final int n;
+    final int[][][] cache=new int[30][][];
+
+
+    /**
+     * Optimized to run a bit faster than the static getDistance().
+     * In one benchmark times were 5.3sec using ctr vs 8.5sec w/ static method, thus 37% faster.
+     */
+    public TRStringDistance (String target) {
+        sa=target.toCharArray();
+        n=sa.length;
+    }
+
+
+    //*****************************
+     // Compute Levenshtein distance
+     //*****************************
+      public final int getDistance (String other) {
+          int d[][]; // matrix
+          int cost; // cost
+
+          // Step 1
+          final char[] ta=other.toCharArray();
+          final int m=ta.length;
+          if (n==0) {
+              return m;
+          }
+          if (m==0) {
+              return n;
+          }
+
+          if (m>=cache.length) {
+              d=form(n, m);
+          }
+          else if (cache[m]!=null) {
+              d=cache[m];
+          }
+          else {
+              d=cache[m]=form(n, m);
+
+              // Step 3
+
+          }
+          for (int i=1; i<=n; i++) {
+              final char s_i=sa[i-1];
+
+              // Step 4
+
+              for (int j=1; j<=m; j++) {
+                  final char t_j=ta[j-1];
+
+                  // Step 5
+
+                  if (s_i==t_j) { // same
+                      cost=0;
+                  }
+                  else { // not a match
+                      cost=1;
+
+                      // Step 6
+
+                  }
+                  d[i][j]=min3(d[i-1][j]+1, d[i][j-1]+1, d[i-1][j-1]+cost);
+
+              }
+
+          }
+
+          // Step 7
+          return d[n][m];
+
+      }
+
+
+    /**
+     *
+     */
+    private static int[][] form (int n, int m) {
+        int[][] d=new int[n+1][m+1];
+        // Step 2
+
+        for (int i=0; i<=n; i++) {
+            d[i][0]=i;
+
+        }
+        for (int j=0; j<=m; j++) {
+            d[0][j]=j;
+        }
+        return d;
+    }
+
+
+    //****************************
+     // Get minimum of three values
+     //****************************
+      private static int min3 (int a, int b, int c) {
+          int mi=a;
+          if (b<mi) {
+              mi=b;
+          }
+          if (c<mi) {
+              mi=c;
+          }
+          return mi;
+
+      }
+}
diff --git a/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java b/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
new file mode 100755
index 0000000..f6a2d9c
--- /dev/null
+++ b/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
@@ -0,0 +1,122 @@
+package org.apache.lucene.search.spell;
+
+
+import junit.framework.*;
+import org.apache.lucene.search.spell.*;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.util.English;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexReader;
+import java.io.IOException;
+import org.apache.lucene.store.FSDirectory;
+import org.apache.lucene.store.Directory;
+import java.io.File;
+
+
+/**
+ * Test case
+ * @author Nicolas Maisonneuve
+ */
+
+public class TestSpellChecker
+extends TestCase {
+    private SpellChecker spellChecker;
+    Directory userindex, spellindex;
+
+    protected void setUp () throws Exception {
+        super.setUp();
+
+        //create a user index
+        userindex=new RAMDirectory();
+        IndexWriter writer=new IndexWriter(userindex, new SimpleAnalyzer(), true);
+
+        for (int i=0; i<1000; i++) {
+            Document doc=new Document();
+            doc.add(Field.Text("field1", English.intToEnglish(i)));
+            doc.add(Field.Text("field2", English.intToEnglish(i+1))); // + word thousand
+            writer.addDocument(doc);
+        }
+        writer.close();
+
+        // create the spellChecker
+        File file=new File("d://test");
+        spellindex=FSDirectory.getDirectory(file, true);
+        spellChecker=new SpellChecker(spellindex);
+    }
+
+
+    public void testBuild () {
+        try {
+            IndexReader r=IndexReader.open(userindex);
+
+            spellChecker.clearIndex();
+
+            addwords(r, "field1");
+            int num_field1=this.numdoc();
+
+            addwords(r, "field2");
+            int num_field2=this.numdoc();
+
+            this.assertTrue(num_field2==num_field1+1);
+
+            // test small word
+            String[] l=spellChecker.suggestSimilar("fvie", 2);
+            this.assertTrue(l[0].equals("five"));
+
+            l=spellChecker.suggestSimilar("fiv", 2);
+            this.assertTrue(l[0].equals("five"));
+
+            l=spellChecker.suggestSimilar("ive", 2);
+            this.assertTrue(l[0].equals("five"));
+
+            l=spellChecker.suggestSimilar("fives", 2);
+            this.assertTrue(l[0].equals("five"));
+
+            l=spellChecker.suggestSimilar("fie", 2);
+            this.assertTrue(l[0].equals("five"));
+
+            l=spellChecker.suggestSimilar("fi", 2);
+            this.assertEquals(0,l.length);
+
+            // test restreint to a field
+            l=spellChecker.suggestSimilar("tousand", 10, r, "field1", false);
+            this.assertEquals(0,l.length); // there isn't the term thousand in the field field1
+
+            l=spellChecker.suggestSimilar("tousand", 10, r, "field2", false);
+            this.assertEquals(1,l.length); // there is the term thousand in the field field2
+        }
+        catch (IOException e) {
+            e.printStackTrace();
+            this.assertTrue(false);
+        }
+
+    }
+
+
+    private void addwords (IndexReader r, String field) throws IOException {
+        long time=System.currentTimeMillis();
+        spellChecker.indexDictionnary(new LuceneDictionary(r, field));
+        time=System.currentTimeMillis()-time;
+        System.out.println("time to build "+field+": "+time);
+    }
+
+
+    private int numdoc () throws IOException {
+        IndexReader rs=IndexReader.open(spellindex);
+        int num=rs.numDocs();
+        this.assertTrue(num!=0);
+        System.out.println("num docs: "+num);
+        rs.close();
+        return num;
+    }
+
+
+    protected void tearDown () throws Exception {
+        spellChecker=null;
+        super.tearDown();
+    }
+
+}
diff --git a/sandbox/contributions/similarity/.cvsignore b/sandbox/contributions/similarity/.cvsignore
deleted file mode 100755
index 9d0b71a..0000000
--- a/sandbox/contributions/similarity/.cvsignore
+++ /dev/null
@@ -1,2 +0,0 @@
-build
-dist
diff --git a/sandbox/contributions/similarity/README.txt b/sandbox/contributions/similarity/README.txt
deleted file mode 100755
index 11c3956..0000000
--- a/sandbox/contributions/similarity/README.txt
+++ /dev/null
@@ -1 +0,0 @@
-Document similarity measures. 
\ No newline at end of file
diff --git a/sandbox/contributions/similarity/build.xml b/sandbox/contributions/similarity/build.xml
deleted file mode 100755
index 1eaac43..0000000
--- a/sandbox/contributions/similarity/build.xml
+++ /dev/null
@@ -1,10 +0,0 @@
-<?xml version="1.0"?>
-
-<project name="similarity" default="default">
-
-  <description>
-    Hits highlighter
-  </description>
-
-  <import file="../common.xml"/>
-</project>
diff --git a/sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis.java b/sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
deleted file mode 100755
index b6dcd04..0000000
--- a/sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
+++ /dev/null
@@ -1,818 +0,0 @@
-/**
- * Copyright 2004 The Apache Software Foundation.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.search.similar;
-
-import java.io.*;
-import java.util.*;
-import java.net.*;
-
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.analysis.standard.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.search.*;
-import org.apache.lucene.index.*;
-import org.apache.lucene.util.*;
-
-/**
- * Generate "more like this" similarity queries. 
- * Based on this mail:
- * <code><pre>
- * Lucene does let you access the document frequency of terms, with IndexReader.docFreq().
- * Term frequencies can be computed by re-tokenizing the text, which, for a single document,
- * is usually fast enough.  But looking up the docFreq() of every term in the document is
- * probably too slow.
- * 
- * You can use some heuristics to prune the set of terms, to avoid calling docFreq() too much,
- * or at all.  Since you're trying to maximize a tf*idf score, you're probably most interested
- * in terms with a high tf. Choosing a tf threshold even as low as two or three will radically
- * reduce the number of terms under consideration.  Another heuristic is that terms with a
- * high idf (i.e., a low df) tend to be longer.  So you could threshold the terms by the
- * number of characters, not selecting anything less than, e.g., six or seven characters.
- * With these sorts of heuristics you can usually find small set of, e.g., ten or fewer terms
- * that do a pretty good job of characterizing a document.
- * 
- * It all depends on what you're trying to do.  If you're trying to eek out that last percent
- * of precision and recall regardless of computational difficulty so that you can win a TREC
- * competition, then the techniques I mention above are useless.  But if you're trying to
- * provide a "more like this" button on a search results page that does a decent job and has
- * good performance, such techniques might be useful.
- * 
- * An efficient, effective "more-like-this" query generator would be a great contribution, if
- * anyone's interested.  I'd imagine that it would take a Reader or a String (the document's
- * text), analyzer Analyzer, and return a set of representative terms using heuristics like those
- * above.  The frequency and length thresholds could be parameters, etc.
- * 
- * Doug
- * </pre></code>
- *
- *
- * <p>
- * <h3>Initial Usage</h3>
- *
- * This class has lots of options to try to make it efficient and flexible.
- * See the body of {@link #main main()} below in the source for real code, or
- * if you want pseudo code, the simpliest possible usage is as follows. The bold
- * fragment is specific to this class.
- *
- * <code><pre>
- *
- * IndexReader ir = ...
- * IndexSearcher is = ...
- * <b>
- * MoreLikeThis mlt = new MoreLikeThis(ir);
- * Reader target = ... </b><em>// orig source of doc you want to find similarities to</em><b>
- * Query query = mlt.like( target);
- * </b>
- * Hits hits = is.search(query);
- * <em>// now the usual iteration thru 'hits' - the only thing to watch for is to make sure
- * you ignore the doc if it matches your 'target' document, as it should be similar to itself </em>
- *
- * </pre></code>
- *
- * Thus you:
- * <ol>
- * <li> do your normal, Lucene setup for searching,
- * <li> create a MoreLikeThis,
- * <li> get the text of the doc you want to find similaries to
- * <li> then call one of the like() calls to generate a similarity query
- * <li> call the searcher to find the similar docs
- * </ol>
- *
- * <h3>More Advanced Usage</h3>
- *
- * You may want to use {@link #setFieldNames setFieldNames(...)} so you can examine
- * multiple fields (e.g. body and title) for similarity.
- * <p>
- *
- * Depending on the size of your index and the size and makeup of your documents you
- * may want to call the other set methods to control how the similarity queries are
- * generated:
- * <ul>
- * <li> {@link #setMinTermFreq setMinTermFreq(...)}
- * <li> {@link #setMinDocFreq setMinDocFreq(...)}
- * <li> {@link #setMinWordLen setMinWordLen(...)}
- * <li> {@link #setMaxWordLen setMaxWordLen(...)}
- * <li> {@link #setMaxQueryTerms setMaxQueryTerms(...)}
- * <li> {@link #setMaxNumTokensParsed setMaxNumTokensParsed(...)}
- * </ul> 
- *
- * <hr>
- * <pre>
- * Changes: Mark Harwood 29/02/04
- * Some bugfixing, some refactoring, some optimisation.
- *  - bugfix: retrieveTerms(int docNum) was not working for indexes without a termvector -added missing code
- *  - bugfix: No significant terms being created for fields with a termvector - because 
- *            was only counting one occurence per term/field pair in calculations(ie not including frequency info from TermVector) 
- *  - refactor: moved common code into isNoiseWord()
- *  - optimise: when no termvector support available - used maxNumTermsParsed to limit amount of tokenization
- * </pre>
- * 
- * @author David Spencer
- * @author Bruce Ritchie
- * @author Mark Harwood
- */
-public final class MoreLikeThis {
-
-	/**
-	 * Default maximum number of tokens to parse in each example doc field that is not stored with TermVector support.
-	 * @see #getMaxNumTokensParsed
-	 */
-    public static final int DEFAULT_MAX_NUM_TOKENS_PARSED=5000;
-       
-
-	/**
-     * Default analyzer to parse source doc with.
-	 * @see #getAnalyzer
-     */
-    public static final Analyzer DEFAULT_ANALYZER = new StandardAnalyzer();
-
-    /**
-     * Ignore terms with less than this frequency in the source doc.
-	 * @see #getMinTermFreq
-	 * @see #setMinTermFreq	 
-     */
-    public static final int DEFAULT_MIN_TERM_FREQ = 2;
-
-    /**
-     * Ignore words which do not occur in at least this many docs.
-	 * @see #getMinDocFreq
-	 * @see #setMinDocFreq	 
-     */
-    public static final int DEFALT_MIN_DOC_FREQ = 5;
-
-    /**
-     * Boost terms in query based on score.
-	 * @see #isBoost
-	 * @see #setBoost 
-     */
-    public static final boolean DEFAULT_BOOST = false;
-
-    /**
-     * Default field names. Null is used to specify that the field names should be looked
-     * up at runtime from the provided reader.
-     */
-    public static final String[] DEFAULT_FIELD_NAMES = new String[] { "contents"};
-
-    /**
-     * Ignore words less than this length or if 0 then this has no effect.
-	 * @see #getMinWordLen
-	 * @see #setMinWordLen	 
-     */
-    public static final int DEFAULT_MIN_WORD_LENGTH = 0;
-
-    /**
-     * Ignore words greater than this length or if 0 then this has no effect.
-	 * @see #getMaxWordLen
-	 * @see #setMaxWordLen	 
-     */
-    public static final int DEFAULT_MAX_WORD_LENGTH = 0;
-
-    /**
-     * Return a Query with no more than this many terms.
-     *
-     * @see BooleanQuery#getMaxClauseCount
-	 * @see #getMaxQueryTerms
-	 * @see #setMaxQueryTerms	 
-     */
-    public static final int DEFAULT_MAX_QUERY_TERMS = 25;
-
-    /**
-     * Analyzer that will be used to parse the doc.
-     */
-    private Analyzer analyzer = DEFAULT_ANALYZER;
-
-    /**
-     * Ignore words less freqent that this.
-     */
-    private int minTermFreq = DEFAULT_MIN_TERM_FREQ;
-
-    /**
-     * Ignore words which do not occur in at least this many docs.
-     */
-    private int minDocFreq = DEFALT_MIN_DOC_FREQ;
-
-    /**
-     * Should we apply a boost to the Query based on the scores?
-     */
-    private boolean boost = DEFAULT_BOOST;
-
-    /**
-     * Field name we'll analyze.
-     */
-    private String[] fieldNames = DEFAULT_FIELD_NAMES;
-
-	/**
-	 * The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
-	 */
-	private int maxNumTokensParsed=DEFAULT_MAX_NUM_TOKENS_PARSED;   
-    
-
-
-    /**
-     * Ignore words if less than this len.
-     */
-    private int minWordLen = DEFAULT_MIN_WORD_LENGTH;
-
-    /**
-     * Ignore words if greater than this len.
-     */
-    private int maxWordLen = DEFAULT_MAX_WORD_LENGTH;
-
-    /**
-     * Don't return a query longer than this.
-     */
-    private int maxQueryTerms = DEFAULT_MAX_QUERY_TERMS;
-
-    /**
-     * For idf() calculations.
-     */
-    private Similarity similarity = new DefaultSimilarity();
-
-    /**
-     * IndexReader to use
-     */
-    private final IndexReader ir;
-
-    /**
-     * Constructor requiring an IndexReader.
-     */
-    public MoreLikeThis(IndexReader ir) {
-        this.ir = ir;
-    }
-
-    /**
-     * Returns an analyzer that will be used to parse source doc with. The default analyzer
-     * is the {@link #DEFAULT_ANALYZER}.
-     *
-     * @return the analyzer that will be used to parse source doc with.
-	 * @see #DEFAULT_ANALYZER
-     */
-    public Analyzer getAnalyzer() {
-        return analyzer;
-    }
-
-    /**
-     * Sets the analyzer to use. An analyzer is not required for generating a query with the
-     * {@link #like(int)} method, all other 'like' methods require an analyzer.
-     *
-     * @param analyzer the analyzer to use to tokenize text.
-     */
-    public void setAnalyzer(Analyzer analyzer) {
-        this.analyzer = analyzer;
-    }
-
-    /**
-     * Returns the frequency below which terms will be ignored in the source doc. The default
-     * frequency is the {@link #DEFAULT_MIN_TERM_FREQ}.
-     *
-     * @return the frequency below which terms will be ignored in the source doc.
-     */
-    public int getMinTermFreq() {
-        return minTermFreq;
-    }
-
-    /**
-     * Sets the frequency below which terms will be ignored in the source doc.
-     *
-     * @param minTermFreq the frequency below which terms will be ignored in the source doc.
-     */
-    public void setMinTermFreq(int minTermFreq) {
-        this.minTermFreq = minTermFreq;
-    }
-
-    /**
-     * Returns the frequency at which words will be ignored which do not occur in at least this
-     * many docs. The default frequency is {@link #DEFALT_MIN_DOC_FREQ}.
-     *
-     * @return the frequency at which words will be ignored which do not occur in at least this
-     * many docs.
-     */
-    public int getMinDocFreq() {
-        return minDocFreq;
-    }
-
-    /**
-     * Sets the frequency at which words will be ignored which do not occur in at least this
-     * many docs.
-     *
-     * @param minDocFreq the frequency at which words will be ignored which do not occur in at
-     * least this many docs.
-     */
-    public void setMinDocFreq(int minDocFreq) {
-        this.minDocFreq = minDocFreq;
-    }
-
-    /**
-     * Returns whether to boost terms in query based on "score" or not. The default is
-     * {@link #DEFAULT_BOOST}.
-     *
-     * @return whether to boost terms in query based on "score" or not.
-	 * @see #setBoost
-     */
-    public boolean isBoost() {
-        return boost;
-    }
-
-    /**
-     * Sets whether to boost terms in query based on "score" or not.
-     *
-     * @param boost true to boost terms in query based on "score", false otherwise.
-	 * @see #isBoost
-     */
-    public void setBoost(boolean boost) {
-        this.boost = boost;
-    }
-
-    /**
-     * Returns the field names that will be used when generating the 'More Like This' query.
-     * The default field names that will be used is {@link #DEFAULT_FIELD_NAMES}.
-     *
-     * @return the field names that will be used when generating the 'More Like This' query.
-     */
-    public String[] getFieldNames() {
-        return fieldNames;
-    }
-
-    /**
-     * Sets the field names that will be used when generating the 'More Like This' query.
-     * Set this to null for the field names to be determined at runtime from the IndexReader
-     * provided in the constructor.
-     *
-     * @param fieldNames the field names that will be used when generating the 'More Like This'
-     * query.
-     */
-    public void setFieldNames(String[] fieldNames) {
-        this.fieldNames = fieldNames;
-    }
-
-    /**
-     * Returns the minimum word length below which words will be ignored. Set this to 0 for no
-     * minimum word length. The default is {@link #DEFAULT_MIN_WORD_LENGTH}.
-     *
-     * @return the minimum word length below which words will be ignored.
-     */
-    public int getMinWordLen() {
-        return minWordLen;
-    }
-
-    /**
-     * Sets the minimum word length below which words will be ignored.
-     *
-     * @param minWordLen the minimum word length below which words will be ignored.
-     */
-    public void setMinWordLen(int minWordLen) {
-        this.minWordLen = minWordLen;
-    }
-
-    /**
-     * Returns the maximum word length above which words will be ignored. Set this to 0 for no
-     * maximum word length. The default is {@link #DEFAULT_MAX_WORD_LENGTH}.
-     *
-     * @return the maximum word length above which words will be ignored.
-     */
-    public int getMaxWordLen() {
-        return maxWordLen;
-    }
-
-    /**
-     * Sets the maximum word length above which words will be ignored.
-     *
-     * @param maxWordLen the maximum word length above which words will be ignored.
-     */
-    public void setMaxWordLen(int maxWordLen) {
-        this.maxWordLen = maxWordLen;
-    }
-
-    /**
-     * Returns the maximum number of query terms that will be included in any generated query.
-     * The default is {@link #DEFAULT_MAX_QUERY_TERMS}.
-     *
-     * @return the maximum number of query terms that will be included in any generated query.
-     */
-    public int getMaxQueryTerms() {
-        return maxQueryTerms;
-    }
-
-    /**
-     * Sets the maximum number of query terms that will be included in any generated query.
-     *
-     * @param maxQueryTerms the maximum number of query terms that will be included in any
-     * generated query.
-     */
-    public void setMaxQueryTerms(int maxQueryTerms) {
-        this.maxQueryTerms = maxQueryTerms;
-    }
-
-	/**
-	 * @return The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
-	 * @see #DEFAULT_MAX_NUM_TOKENS_PARSED
-	 */
-	public int getMaxNumTokensParsed()
-	{
-		return maxNumTokensParsed;
-	}
-
-	/**
-	 * @param i The maximum number of tokens to parse in each example doc field that is not stored with TermVector support
-	 */
-	public void setMaxNumTokensParsed(int i)
-	{
-		maxNumTokensParsed = i;
-	}
-
-
-
-
-    /**
-     * Return a query that will return docs like the passed lucene document ID.
-     *
-     * @param docNum the documentID of the lucene doc to generate the 'More Like This" query for.
-     * @return a query that will return docs like the passed lucene document ID.
-     */
-    public Query like(int docNum) throws IOException {
-        if (fieldNames == null) {
-            // gather list of valid fields from lucene
-            Collection fields = ir.getFieldNames(true);
-            fieldNames = (String[]) fields.toArray(new String[fields.size()]);
-        }
-
-        return createQuery(retrieveTerms(docNum));
-    }
-
-    /**
-     * Return a query that will return docs like the passed file.
-     *
-     * @return a query that will return docs like the passed file.
-     */
-    public Query like(File f) throws IOException {
-        if (fieldNames == null) {
-            // gather list of valid fields from lucene
-            Collection fields = ir.getFieldNames(true);
-            fieldNames = (String[]) fields.toArray(new String[fields.size()]);
-        }
-
-        return like(new FileReader(f));
-    }
-
-    /**
-     * Return a query that will return docs like the passed URL.
-     *
-     * @return a query that will return docs like the passed URL.
-     */
-    public Query like(URL u) throws IOException {
-        return like(new InputStreamReader(u.openConnection().getInputStream()));
-    }
-
-    /**
-     * Return a query that will return docs like the passed stream.
-     *
-     * @return a query that will return docs like the passed stream.
-     */
-    public Query like(java.io.InputStream is) throws IOException {
-        return like(new InputStreamReader(is));
-    }
-
-    /**
-     * Return a query that will return docs like the passed Reader.
-     *
-     * @return a query that will return docs like the passed Reader.
-     */
-    public Query like(Reader r) throws IOException {
-        return createQuery(retrieveTerms(r));
-    }
-
-    /**
-     * Create the More like query from a PriorityQueue
-     */
-    private Query createQuery(PriorityQueue q) {
-        BooleanQuery query = new BooleanQuery();
-        Object cur;
-        int qterms = 0;
-        float bestScore = 0;
-
-        while (((cur = q.pop()) != null)) {
-            Object[] ar = (Object[]) cur;
-            TermQuery tq = new TermQuery(new Term((String) ar[1], (String) ar[0]));
-
-            if (boost) {
-                if (qterms == 0) {
-                    bestScore = ((Float) ar[2]).floatValue();
-                }
-                float myScore = ((Float) ar[2]).floatValue();
-
-                tq.setBoost(myScore / bestScore);
-            }
-
-            try {
-                query.add(tq, false, false);
-            }
-            catch (BooleanQuery.TooManyClauses ignore) {
-                break;
-            }
-
-            qterms++;
-            if (maxQueryTerms > 0 && qterms >= maxQueryTerms) {
-                break;
-            }
-        }
-
-        return query;
-    }
-
-    /**
-     * Create a PriorityQueue from a word->tf map.
-     *
-     * @param words a map of words keyed on the word(String) with Int objects as the values.
-     */
-    private PriorityQueue createQueue(Map words) throws IOException {
-        // have collected all words in doc and their freqs
-        int numDocs = ir.numDocs();
-        FreqQ res = new FreqQ(words.size()); // will order words by score
-
-        Iterator it = words.keySet().iterator();
-        while (it.hasNext()) { // for every word
-            String word = (String) it.next();
-
-            int tf = ((Int) words.get(word)).x; // term freq in the source doc
-            if (minTermFreq > 0 && tf < minTermFreq) {
-                continue; // filter out words that don't occur enough times in the source
-            }
-
-            // go through all the fields and find the largest document frequency
-            String topField = fieldNames[0];
-            int docFreq = 0;
-            for (int i = 0; i < fieldNames.length; i++) {
-                int freq = ir.docFreq(new Term(fieldNames[i], word));
-                topField = (freq > docFreq) ? fieldNames[i] : topField;
-                docFreq = (freq > docFreq) ? freq : docFreq;
-            }
-
-            if (minDocFreq > 0 && docFreq < minDocFreq) {
-                continue; // filter out words that don't occur in enough docs
-            }
-
-            if (docFreq == 0) {
-                continue; // index update problem?
-            }
-
-            float idf = similarity.idf(docFreq, numDocs);
-            float score = tf * idf;
-
-            // only really need 1st 3 entries, other ones are for troubleshooting
-            res.insert(new Object[]{word,                   // the word
-                                    topField,               // the top field
-                                    new Float(score),       // overall score
-                                    new Float(idf),         // idf
-                                    new Integer(docFreq),   // freq in all docs
-                                    new Integer(tf)
-            });
-        }
-        return res;
-    }
-
-    /**
-     * Describe the parameters that control how the "more like this" query is formed.
-     */
-    public String describeParams() {
-        StringBuffer sb = new StringBuffer();
-        sb.append("\t" + "maxQueryTerms  : " + maxQueryTerms + "\n");
-        sb.append("\t" + "minWordLen     : " + minWordLen + "\n");
-        sb.append("\t" + "maxWordLen     : " + maxWordLen + "\n");
-        sb.append("\t" + "fieldNames     : \"");
-        String delim = "";
-        for (int i = 0; i < fieldNames.length; i++) {
-            String fieldName = fieldNames[i];
-            sb.append(delim).append(fieldName);
-            delim = ", ";
-        }
-        sb.append("\n");
-        sb.append("\t" + "boost          : " + boost + "\n");
-        sb.append("\t" + "minTermFreq    : " + minTermFreq + "\n");
-        sb.append("\t" + "minDocFreq     : " + minDocFreq + "\n");
-        return sb.toString();
-    }
-
-    /**
-     * Test driver.
-     * Pass in "-i INDEX" and then either "-fn FILE" or "-url URL".
-     */
-    public static void main(String[] a) throws Throwable {
-        String indexName = "localhost_index";
-        String fn = "c:/Program Files/Apache Group/Apache/htdocs/manual/vhosts/index.html.en";
-        URL url = null;
-        for (int i = 0; i < a.length; i++) {
-            if (a[i].equals("-i")) {
-                indexName = a[++i];
-            }
-            else if (a[i].equals("-f")) {
-                fn = a[++i];
-            }
-            else if (a[i].equals("-url")) {
-                url = new URL(a[++i]);
-            }
-        }
-
-        PrintStream o = System.out;
-        IndexReader r = IndexReader.open(indexName);
-        o.println("Open index " + indexName + " which has " + r.numDocs() + " docs");
-
-        MoreLikeThis mlt = new MoreLikeThis(r);
-
-        o.println("Query generation parameters:");
-        o.println(mlt.describeParams());
-        o.println();
-
-        Query query = null;
-        if (url != null) {
-            o.println("Parsing URL: " + url);
-            query = mlt.like(url);
-        }
-        else if (fn != null) {
-            o.println("Parsing file: " + fn);
-            query = mlt.like(new File(fn));
-        }
-
-        o.println("q: " + query);
-        o.println();
-        IndexSearcher searcher = new IndexSearcher(indexName);
-
-        Hits hits = searcher.search(query);
-        int len = hits.length();
-        o.println("found: " + len + " documents matching");
-        o.println();
-        for (int i = 0; i < Math.min(25, len); i++) {
-            Document d = hits.doc(i);
-            o.println("score  : " + hits.score(i));
-            o.println("url    : " + d.get("url"));
-            o.println("\ttitle  : " + d.get("title"));
-            o.println("\tsummary: " + d.get("summary"));
-            o.println();
-        }
-    }
-
-    /**
-     * Find words for a more-like-this query former.
-     *
-     * @param docNum the id of the lucene document from which to find terms
-     */
-    private PriorityQueue retrieveTerms(int docNum) throws IOException {
-        Map termFreqMap = new HashMap();
-        for (int i = 0; i < fieldNames.length; i++) {
-            String fieldName = fieldNames[i];
-            TermFreqVector vector = ir.getTermFreqVector(docNum, fieldName);
-
-            // field does not store term vector info
-            if (vector == null) {
-            	Document d=ir.document(docNum);
-            	String text=d.get(fieldName);
-            	if(text!=null)
-            	{
-					addTermFrequencies(new StringReader(text), termFreqMap, fieldName);
-            	}
-            }
-            else {
-				addTermFrequencies(termFreqMap, vector);
-            }
-
-        }
-
-        return createQueue(termFreqMap);
-    }
-
-	/**
-	 * Adds terms and frequencies found in vector into the Map termFreqMap
-	 * @param termFreqMap a Map of terms and their frequencies
-	 * @param vector List of terms and their frequencies for a doc/field
-	 */
-	private void addTermFrequencies(Map termFreqMap, TermFreqVector vector)
-	{
-		String[] terms = vector.getTerms();
-		int freqs[]=vector.getTermFrequencies();
-		for (int j = 0; j < terms.length; j++) {
-		    String term = terms[j];
-		
-			if(isNoiseWord(term)){
-				continue;
-			}
-		    // increment frequency
-		    Int cnt = (Int) termFreqMap.get(term);
-		    if (cnt == null) {
-		    	cnt=new Int();
-				termFreqMap.put(term, cnt);
-				cnt.x=freqs[j];				
-		    }
-		    else {
-		        cnt.x+=freqs[j];
-		    }
-		}
-	}
-	/**
-	 * Adds term frequencies found by tokenizing text from reader into the Map words
-	 * @param r a source of text to be tokenized
-	 * @param termFreqMap a Map of terms and their frequencies
-	 * @param fieldName Used by analyzer for any special per-field analysis
-	 */
-	private void addTermFrequencies(Reader r, Map termFreqMap, String fieldName)
-		throws IOException
-	{
-		   TokenStream ts = analyzer.tokenStream(fieldName, r);
-			org.apache.lucene.analysis.Token token;
-			int tokenCount=0;
-			while ((token = ts.next()) != null) { // for every token
-				String word = token.termText();
-				tokenCount++;
-				if(tokenCount>maxNumTokensParsed)
-				{
-					break;
-				}
-				if(isNoiseWord(word)){
-					continue;
-				}
-				
-				// increment frequency
-				Int cnt = (Int) termFreqMap.get(word);
-				if (cnt == null) {
-					termFreqMap.put(word, new Int());
-				}
-				else {
-					cnt.x++;
-				}
-			}
-	}
-	
-	
-	/** determines if the passed term is likely to be of interest in "more like" comparisons 
-	 * 
-	 * @param term The word being considered
-	 * @return true if should be ignored, false if should be used in further analysis
-	 */
-	private boolean isNoiseWord(String term)
-	{
-		int len = term.length();
-		if (minWordLen > 0 && len < minWordLen) {
-			return true;
-		}
-		if (maxWordLen > 0 && len < maxWordLen) {
-			return true;
-		}
-		return false;
-	}
-	
-
-    /**
-     * Find words for a more-like-this query former.
-     *
-     * @param r the reader that has the content of the document
-     */
-    public PriorityQueue retrieveTerms(Reader r) throws IOException {
-        Map words = new HashMap();
-        for (int i = 0; i < fieldNames.length; i++) {
-            String fieldName = fieldNames[i];
-			addTermFrequencies(r, words, fieldName);
-        }
-        return createQueue(words);
-    }
-
-    /**
-     * PriorityQueue that orders words by score.
-     */
-    private static class FreqQ extends PriorityQueue {
-        FreqQ (int s) {
-            initialize(s);
-        }
-
-        protected boolean lessThan(Object a, Object b) {
-            Object[] aa = (Object[]) a;
-            Object[] bb = (Object[]) b;
-            Float fa = (Float) aa[2];
-            Float fb = (Float) bb[2];
-            return fa.floatValue() > fb.floatValue();
-        }
-    }
-
-    /**
-     * Use for frequencies and to avoid renewing Integers.
-     */
-    private static class Int {
-        int x;
-
-        Int() {
-            x = 1;
-        }
-    }
-    
-    
-}
diff --git a/sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/SimilarityQueries.java b/sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/SimilarityQueries.java
deleted file mode 100755
index f04bb5a..0000000
--- a/sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/SimilarityQueries.java
+++ /dev/null
@@ -1,118 +0,0 @@
-/**
- * Copyright 2004 The Apache Software Foundation.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.lucene.search.similar;
-
-import java.io.*;
-import java.util.*;
-import java.net.*;
-
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.analysis.standard.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.search.*;
-import org.apache.lucene.index.*;
-import org.apache.lucene.util.*;
-
-/**
- * Simple similarity measures.
- *
- *
- * @see MoreLikeThis
- */
-public final class SimilarityQueries
-{
-	/**
-	 *
-	 */
-	private SimilarityQueries()
-	{
-	}
-	
-	/**
-     * Simple similarity query generators.
-	 * Takes every unique word and forms a boolean query where all words are optional.
-	 * After you get this you'll use to to query your {@link IndexSearcher} for similar docs.
-	 * The only caveat is the first hit returned <b>should be</b> your source document - you'll
-	 * need to then ignore that.
-	 *
-	 * <p>
-	 *
-	 * So, if you have a code fragment like this:
-	 * <br>
-	 * <code>
-	 * Query q = formSimilaryQuery( "I use Lucene to search fast. Fast searchers are good", new StandardAnalyzer(), "contents", null);
-	 * </code>
-	 *
-	 * <p>
-	 *
-	 
-	 * The query returned, in string form, will be <code>'(i use lucene to search fast searchers are good')</code>.
-	 *
-	 * <p>
-	 * The philosophy behind this method is "two documents are similar if they share lots of words".
-	 * Note that behind the scenes, Lucenes scoring algorithm will tend to give two documents a higher similarity score if the share more uncommon words.
-	 *
-	 * <P>
-	 * This method is fail-safe in that if a long 'body' is passed in and
-	 * {@link BooleanQuery#add BooleanQuery.add()} (used internally)
-	 * throws
-	 * {@link org.apache.lucene.search.BooleanQuery.TooManyClauses BooleanQuery.TooManyClauses}, the
-	 * query as it is will be returned.
-	 *
-	 * 
-	 * 
-	 *
-	 *
-	 * @param body the body of the document you want to find similar documents to
-	 * @param a the analyzer to use to parse the body
-	 * @param field the field you want to search on, probably something like "contents" or "body"
-	 * @param stop optional set of stop words to ignore
-	 * @return a query with all unique words in 'body'
-	 * @throws IOException this can't happen...
-	 */
-    public static Query formSimilarQuery( String body,
-										  Analyzer a,
-										  String field,
-										  Set stop)
-										  throws IOException
-	{	
-		TokenStream ts = a.tokenStream( field, new StringReader( body));
-		org.apache.lucene.analysis.Token t;
-		BooleanQuery tmp = new BooleanQuery();
-		Set already = new HashSet(); // ignore dups
-		while ( (t = ts.next()) != null)
-		{
-			String word = t.termText();
-			// ignore opt stop words
-			if ( stop != null &&
-				 stop.contains( word)) continue;
-			// ignore dups
-			if ( ! already.add( word)) continue;
-			// add to query
-			TermQuery tq = new TermQuery( new Term( field, word));
-			try
-			{
-				tmp.add( tq, false, false);
-			}
-			catch( BooleanQuery.TooManyClauses too)
-			{
-				// fail-safe, just return what we have, not the end of the world
-				break;
-			}
-		}
-		return tmp;
-	}
-}
diff --git a/sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/package.html b/sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/package.html
deleted file mode 100755
index c75171e..0000000
--- a/sandbox/contributions/similarity/src/java/org/apache/lucene/search/similar/package.html
+++ /dev/null
@@ -1,5 +0,0 @@
-<html>
-<body>
-Document similarity query generators.
-</body>
-</html>
\ No newline at end of file
diff --git a/sandbox/contributions/spellchecker/build.xml b/sandbox/contributions/spellchecker/build.xml
deleted file mode 100755
index d6f8917..0000000
--- a/sandbox/contributions/spellchecker/build.xml
+++ /dev/null
@@ -1,156 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-
-<project basedir="." default="rebuild" name="Spelling checker">
-
-<property name="lucene.lib" value="d:/dev/lib/lucene.jar"/>
-<property name="lucenetest.lib" value="D:/dev/jakarta-lucene/build/classes/test"/>
-
-
-<property name="name" value="spellchecker"/>
-<property name="Name" value="spellchecker"/>
-<property name="version" value="1.1"/>
-<property name="year" value="2004"/>
-<property name="final.name" value="${name}-${version}"/>
-<property name="java" location="src/java"/>
-<property name="test" location="src/test"/>
-<property name="build.dir" location="build"/>
-<property name="build.java" location="${build.dir}/classes/java"/>
-<property name="build.test" location="${build.dir}/classes/test"/>
-<property name="build.javadocs" location="doc"/>
-<property name="javadoc.link" value="http://java.sun.com/j2se/1.4/docs/api/"/>
-<property name="javac.debug" value="off"/>
-<property name="junit.output.dir" location="${build.dir}/test"/>
-<property name="junit.reports" location="${build.dir}/test/reports"/>
-
-
-
-   <!-- Build classpath -->
-  <path id="classpath">
-    <pathelement location="${lucene.lib}"/>
-
-    <pathelement location="${build.java}"/>
-  </path>
-
-  <path id="test.classpath">
-    <path refid="classpath"/>
-	<pathelement location="${lucenetest.lib}"/>
-    <pathelement location="${build.dir}/classes/test"/>
-  </path>
-  <!--Patternset to exclude files from the output directory:-->
-
-  <!-- ================================================================== -->
-  <!-- C O M P I L E                                                      -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="javacompile"
-    description="Compiles core classes">
-    <mkdir dir="${build.java}"/>
-    <javac
-      srcdir="${java}"
-      includes="**/*.java"
-      destdir="${build.java}"
-      debug="${javac.debug}"
-	  optimize="on">
-      <classpath refid="classpath"/>
-    </javac>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- J A R                                                              -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="jar"  depends="javacompile" description="Generates the Jar file">
-    <jar 
-	destfile="${build.dir}/${final.name}.jar"
-    basedir="${build.java}" />
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- J A V A D O C                                                      -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="javadoc">
-    <mkdir dir="${build.javadocs}"/>
-    <javadoc
-      sourcepath="${java}"
-      overview="src/java/overview.html"
-      packagenames="org.apache.lucene.*"
-      destdir="${build.javadocs}"
-      author="true"
-      version="true"
-      use="true"
-      link="${javadoc.link}"
-      windowtitle="${Name} ${version} API"
-      doctitle="${Name} ${version} API"
-      bottom="Author: Nicolas Maisonneuve (${year})"  >
-      </javadoc>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- C L E A N                                                          -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="clean">
-    <delete failonerror="false" includeemptydirs="true">
-      <fileset dir="${build.dir}"/>
-    </delete>
-  </target>
-
-
-  <!-- ================================================================== -->
-  <!-- B U I L D  T E S T                                                 -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="compile-test" depends="javacompile">
-    <mkdir dir="${build.test}"/>
-    <javac
-      srcdir="${test}"
-      includes="**/*.java"
-      destdir="${build.test}"
-      debug="true">
-      <classpath refid="test.classpath"/>
-    </javac>
-  </target>
-
-  <!-- ================================================================== -->
-  <!-- R U N  T E S T S                                                   -->
-  <!-- ================================================================== -->
-  <!--                                                                    -->
-  <!-- ================================================================== -->
-  <target name="test" depends="compile-test" description="Runs unit tests">
-    <fail unless="junit.present">
-      ##################################################################
-      JUnit not found.
-      Please make sure junit.jar is in ANT_HOME/lib, or made available
-      to Ant using other mechanisms like -lib or CLASSPATH.
-      ##################################################################
-	  </fail>
-    <mkdir dir="${junit.output.dir}"/>
-    <junit printsummary="off" haltonfailure="no"
-      errorProperty="tests.failed" failureProperty="tests.failed">
-      <classpath refid="junit.classpath"/>
-      <sysproperty key="dataDir" file="src/test"/>
-      <sysproperty key="tempDir" file="${build.dir}/test"/>
-      <formatter type="xml"/>
-      <formatter type="brief" usefile="false"/>
-      <batchtest fork="yes" todir="${junit.output.dir}" unless="testcase">
-        <fileset dir="src/test" includes="**/Test*.java"/>
-      </batchtest>
-      <batchtest fork="yes" todir="${junit.output.dir}" if="testcase">
-        <fileset dir="src/test" includes="**/${testcase}.java"/>
-      </batchtest>
-    </junit>
-
-    <fail if="tests.failed">Tests failed!</fail>
-  </target>
-
-  <target depends="javacompile" name="make"/>
-
-  <target depends="clean,make" name="rebuild"/>
-
-</project>
\ No newline at end of file
diff --git a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/Dictionary.java b/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/Dictionary.java
deleted file mode 100755
index 979621a..0000000
--- a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/Dictionary.java
+++ /dev/null
@@ -1,33 +0,0 @@
-package org.apache.lucene.search.spell;
-/**
- * Copyright 2002-2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Iterator;
-
-/**
- * A simple interface representing a Dictionary
- * @author Nicolas Maisonneuve
- * @version 1.0
- */
-public interface Dictionary {
-
-    /**
-     * return all the words present in the dictionnary
-     * @return Iterator
-     */
-    public Iterator getWordsIterator();
-
-}
diff --git a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/LuceneDictionary.java b/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/LuceneDictionary.java
deleted file mode 100755
index d94cedb..0000000
--- a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/LuceneDictionary.java
+++ /dev/null
@@ -1,94 +0,0 @@
-package org.apache.lucene.search.spell;
-
-/**
- * Copyright 2002-2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.IndexReader;
-import java.util.Iterator;
-import org.apache.lucene.index.TermEnum;
-import org.apache.lucene.index.Term;
-import java.io.*;
-
-/**
- *  Lucene Dictionnary
- * @author Nicolas Maisonneuve
- */
-public class LuceneDictionary
-implements Dictionary {
-    IndexReader reader;
-    String field;
-
-    public LuceneDictionary (IndexReader reader, String field) {
-        this.reader=reader;
-        this.field=field;
-
-    }
-
-
-    public final Iterator getWordsIterator () {
-        return new LuceneIterator();
-    }
-
-
-final  class LuceneIterator    implements Iterator {
-      private  TermEnum enum;
-      private  Term actualTerm;
-      private  boolean has_next_called;
-
-        public LuceneIterator () {
-            try {
-                enum=reader.terms(new Term(field, ""));
-            }
-            catch (IOException ex) {
-                ex.printStackTrace();
-            }
-        }
-
-
-        public Object next () {
-            if (!has_next_called)  {hasNext();}
-             has_next_called=false;
-            return (actualTerm!=null) ? actualTerm.text(): null;
-        }
-
-
-        public boolean hasNext () {
-             has_next_called=true;
-            try {
-                // if there is still words
-                if (!enum.next()) {
-                    actualTerm=null;
-                    return false;
-                }
-                //  if the next word are in the field
-                actualTerm=enum.term();
-                String fieldt=actualTerm.field();
-                if (fieldt!=field) {
-                    actualTerm=null;
-                    return false;
-                }
-                return true;
-            }
-            catch (IOException ex) {
-                ex.printStackTrace();
-                return false;
-            }
-        }
-
-
-        public void remove () {};
-    }
-}
diff --git a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java b/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java
deleted file mode 100755
index 230b923..0000000
--- a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.search.spell;
-
-/**
- * Copyright 2002-2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.util.Iterator;
-import java.io.InputStream;
-import java.io.BufferedReader;
-import java.io.InputStreamReader;
-import java.io.*;
-
-
-/**
- * dictionary represented by a file text
- * Format allowed: 1 word per line:
- * word1
- * word2
- * word3
- *
- * @author Nicolas Maisonneuve
- */
-public class PlainTextDictionary implements Dictionary {
-
-    private BufferedReader in;
-    private String line;
-    private boolean has_next_called;
-
-    public PlainTextDictionary (File file) throws FileNotFoundException {
-        in=new BufferedReader(new FileReader(file));
-    }
-
-
-    public PlainTextDictionary (InputStream dictFile) {
-        in=new BufferedReader(new InputStreamReader(System.in));
-    }
-
-
-    public Iterator getWordsIterator () {
-
-        return new fileIterator();
-    }
-
-
-    final class fileIterator
-    implements Iterator {
-        public Object next () {
-            if (!has_next_called) {
-                hasNext();
-            }
-            has_next_called=false;
-            return line;
-        }
-
-
-        public boolean hasNext () {
-            has_next_called=true;
-            try {
-                line=in.readLine();
-            }
-            catch (IOException ex) {
-                ex.printStackTrace();
-                line=null;
-                return false;
-            }
-            return (line!=null)?true:false;
-        }
-
-
-        public void remove () {};
-    }
-
-}
diff --git a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java b/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
deleted file mode 100755
index 93be0a6..0000000
--- a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
+++ /dev/null
@@ -1,363 +0,0 @@
-package org.apache.lucene.search.spell;
-
-
-/**
- * Copyright 2002-2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TermEnum;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Hits;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.Directory;
-import java.util.*;
-
-
-/**
- *  <p>
- *	Spell Checker class  (Main class) <br/>
- * (initially inspired by the David Spencer code)
- *  </p>
- *  
- *  <p>
- *  Spell Checker spellchecker= new SpellChecker (spellDirectory);<br/>
- *  <br/>
- *  //To index a field of a user index <br/>
- *  spellchecker.indexDictionary(new LuceneDictionary(my_lucene_reader, a_field));<br/>
- *<br/>
- *   //To index a file containing words  <br/>
- *  spellchecker.indexDictionary(new PlainTextDictionary(new File("myfile.txt")));<br/>
- *</p>
- * 
- * @author Nicolas Maisonneuve
- * @version 1.0
- */
-public class SpellChecker {
-
-    /**
-     * Field name for each word in the ngram index.
-     */
-    public static final String F_WORD="word";
-
-
-    /**
-     * the spell index
-     */
-    Directory spellindex;
-
-    /**
-     * Boost value for start and end grams
-     */private float bStart=2.0f;
-      private float bEnd=1.0f;
-  
-
-    private IndexReader reader;
-    float min=0.5f;
-
-    public void setSpellIndex (Directory spellindex) {
-        this.spellindex=spellindex;
-    }
-
-
-    /**
-     *  Set the accuraty 0<min<1 default 0.5
-     * @param min float
-     */
-    public void setAccuraty (float min) {
-        this.min=min;
-    }
-
-
-    public SpellChecker (Directory gramIndex) {
-        this.setSpellIndex(gramIndex);
-    }
-
-
-    /**
-     * Suggest similar words
-     * @param word String the word you want a spell check done on
-     * @param num_sug int the number of suggest words
-     * @throws IOException
-     * @return String[]
-     */
-    public String[] suggestSimilar (String word, int num_sug) throws IOException {
-        return this.suggestSimilar(word, num_sug, null, null, false);
-    }
-
-
-    /**
-     * Suggest similar words (restricted or not of a field of a user index)
-     * @param word String the word you want a spell check done on
-     * @param num_sug int the number of suggest words
-     * @param IndexReader the indexReader of the user index (can be null see field param)
-     * @param field String the field of the user index: if field is not null ,the suggest
-     * words are restricted to the words present in this field.
-     * @param morePopular boolean return only the suggest words that are more frequent than the searched word
-     * (only if restricted mode = (indexReader!=null and field!=null)
-     * @throws IOException
-     * @return String[] the sorted list of the suggest words with this 2 criteri
-     * first criteria : the edit distance, second criteria (only if restricted mode): the popularity
-     * of the suggest words in the field of the user index
-     */
-    public String[] suggestSimilar (String word, int num_sug, IndexReader ir, String field
-    , boolean morePopular) throws IOException {
-
-        final TRStringDistance sd=new TRStringDistance(word);
-        final int lengthWord=word.length();
-
-        final int goalFreq=(morePopular&&ir!=null)?ir.docFreq(new Term(field, word)):0;
-        if (!morePopular&&goalFreq>0) {
-            return new String[] {
-            word}; // return the word if it exist in the index and i don't want a more popular word
-        }
-
-        BooleanQuery query=new BooleanQuery();
-        String[] grams;
-        String key;
-
-        for (int ng=getMin(lengthWord); ng<=getMax(lengthWord); ng++) {
-
-            key="gram"+ng; // form key
-
-            grams=formGrams(word, ng); // form word into ngrams (allow dups too)
-
-            if (grams.length==0) {
-                continue; // hmm
-            }
-
-            if (bStart>0) { // should we boost prefixes?
-                add(query, "start"+ng, grams[0], bStart); // matches start of word
-
-            }
-            if (bEnd>0) { // should we boost suffixes
-                add(query, "end"+ng, grams[grams.length-1], bEnd); // matches end of word
-
-            }
-            for (int i=0; i<grams.length; i++) {
-                add(query, key, grams[i]);
-            }
-
-        }
-
-        IndexSearcher searcher=new IndexSearcher(this.spellindex);
-        Hits hits=searcher.search(query);
-        SuggestWordQueue sugqueue=new SuggestWordQueue(num_sug);
-
-        int stop=Math.min(hits.length(), 10*num_sug); // go thru more than 'maxr' matches in case the distance filter triggers
-        SuggestWord sugword=new SuggestWord();
-        for (int i=0; i<stop; i++) {
-
-            sugword.string=hits.doc(i).get(F_WORD); // get orig word)
-
-            if (sugword.string==word) {
-                continue; // don't suggest a word for itself, that would be silly
-            }
-
-            //edit distance/normalize with the min word length
-            sugword.score=1.0f-((float) sd.getDistance(sugword.string)/Math.min(sugword.string.length(), lengthWord));
-            if (sugword.score<min) {
-                continue;
-            }
-
-            if (ir!=null) { // use the user index
-                sugword.freq=ir.docFreq(new Term(field, sugword.string)); // freq in the index
-                if ((morePopular&&goalFreq>sugword.freq)||sugword.freq<1) { // don't suggest a word that is not present in the field
-                    continue;
-                }
-            }
-            sugqueue.insert(sugword);
-            if (sugqueue.size()==num_sug) {
-                //if queue full , maintain the min score
-                min=((SuggestWord) sugqueue.top()).score;
-            }
-            sugword=new SuggestWord();
-        }
-
-        // convert to array string
-        String[] list=new String[sugqueue.size()];
-        for (int i=sugqueue.size()-1; i>=0; i--) {
-            list[i]=((SuggestWord) sugqueue.pop()).string;
-        }
-
-        searcher.close();
-        return list;
-    }
-
-
-    /**
-     * Add a clause to a boolean query.
-     */
-    private static void add (BooleanQuery q, String k, String v, float boost) {
-        Query tq=new TermQuery(new Term(k, v));
-        tq.setBoost(boost);
-        q.add(new BooleanClause(tq, false, false));
-    }
-
-
-    /**
-     * Add a clause to a boolean query.
-     */
-    private static void add (BooleanQuery q, String k, String v) {
-        q.add(new BooleanClause(new TermQuery(new Term(k, v)), false, false));
-    }
-
-
-    /**
-     * Form all ngrams for a given word.
-     * @param text the word to parse
-     * @param ng the ngram length e.g. 3
-     * @return an array of all ngrams in the word and note that duplicates are not removed
-     */
-    private static String[] formGrams (String text, int ng) {
-        int len=text.length();
-        String[] res=new String[len-ng+1];
-        for (int i=0; i<len-ng+1; i++) {
-            res[i]=text.substring(i, i+ng);
-        }
-        return res;
-    }
-
-
-    public void clearIndex () throws IOException {
-        IndexReader.unlock(spellindex);
-        IndexWriter writer=new IndexWriter(spellindex, null, true);
-        writer.close();
-    }
-
-
-    /**
-     * if the word exist in the index
-     * @param word String
-     * @throws IOException
-     * @return boolean
-     */
-    public boolean exist (String word) throws IOException {
-        if (reader==null) {
-            reader=IndexReader.open(spellindex);
-        }
-        return reader.docFreq(new Term(F_WORD, word))>0;
-    }
-
-
-    /**
-     * Index a Dictionnary
-     * @param dict the dictionnary to index
-     * @throws IOException
-     */
-    public void indexDictionnary (Dictionary dict) throws IOException {
-
-        int ng1, ng2;
-        IndexReader.unlock(spellindex);
-        IndexWriter writer=new IndexWriter(spellindex, new WhitespaceAnalyzer(), !IndexReader.indexExists(spellindex));
-        writer.mergeFactor=300;
-        writer.minMergeDocs=150;
-
-        Iterator iter=dict.getWordsIterator();
-        while (iter.hasNext()) {
-            String word=(String) iter.next();
-
-            int len=word.length();
-            if (len<3) {
-                continue; // too short we bail but "too long" is fine...
-            }
-
-            if (this.exist(word)) { // if the word already exist in the gramindex
-                continue;
-            }
-
-            // ok index the word
-            Document doc=createDocument(word, getMin(len), getMax(len));
-            writer.addDocument(doc);
-        }
-        // close writer
-        writer.optimize();
-        writer.close();
-
-        // close reader
-        reader.close();
-        reader=null;
-    }
-
-
-    private int getMin (int l) {
-        if (l>5) {
-            return 3;
-        }
-        if (l==5) {
-            return 2;
-        }
-        return 1;
-    }
-
-
-    private int getMax (int l) {
-        if (l>5) {
-            return 4;
-        }
-        if (l==5) {
-            return 3;
-        }
-        return 2;
-
-    }
-
-
-    private static Document createDocument (String text, int ng1, int ng2) {
-        Document doc=new Document();
-        doc.add(Field.Keyword(F_WORD, text)); // orig term
-        addGram(text, doc, ng1, ng2);
-        return doc;
-    }
-
-
-    private static void addGram (String text, Document doc, int ng1, int ng2) {
-        int len=text.length();
-        for (int ng=ng1; ng<=ng2; ng++) {
-            String key="gram"+ng;
-            String end=null;
-            for (int i=0; i<len-ng+1; i++) {
-                String gram=text.substring(i, i+ng);
-                doc.add(Field.Keyword(key, gram));
-                if (i==0) {
-                    doc.add(Field.Keyword("start"+ng, gram));
-                }
-                end=gram;
-            }
-            if (end!=null) { // may not be present if len==ng1
-                doc.add(Field.Keyword("end"+ng, end));
-            }
-        }
-    }
-
-
-    protected void finalize () throws Throwable {
-        if (reader!=null) {
-            reader.close();
-        }
-    }
-
-}
diff --git a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWord.java b/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWord.java
deleted file mode 100755
index 722d1a3..0000000
--- a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWord.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.apache.lucene.search.spell;
-
-
-/**
- * Copyright 2002-2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- *  SuggestWord Class
- *  used in suggestSimilat method in SpellChecker class
- *  @author Nicolas Maisonneuve
- */
- final class SuggestWord {
-    /**
-     * the score of the word
-     */
-    public float score;
-
-
-    /**
-     * The freq of the word
-     */
-    public int freq;
-
-
-    /**
-     * the suggested word
-     */
-    public String string;
-
-
-    public final int compareTo (SuggestWord a) {
-        //first criteria: the edit distance
-        if (score>a.score) {
-            return 1;
-        }
-        if (score<a.score) {
-            return-1;
-        }
-
-        //second criteria (if first criteria is equal): the popularity
-        if (freq>a.freq) {
-            return 1;
-        }
-
-        if (freq<a.freq) {
-            return-1;
-        }
-
-        return 0;
-    }
-}
diff --git a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWordQueue.java b/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWordQueue.java
deleted file mode 100755
index a96c29d..0000000
--- a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/SuggestWordQueue.java
+++ /dev/null
@@ -1,41 +0,0 @@
-package org.apache.lucene.search.spell;
-
-
-/**
- * Copyright 2002-2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- *  to sort SuggestWord
- * @author Nicolas Maisonneuve
- */
-import org.apache.lucene.util.PriorityQueue;
-
-
-final class SuggestWordQueue
-extends PriorityQueue {
-
-    SuggestWordQueue (int size) {
-        initialize(size);
-    }
-
-    protected final boolean lessThan (Object a, Object b) {
-        SuggestWord wa=(SuggestWord) a;
-        SuggestWord wb=(SuggestWord) b;
-        int val=wa.compareTo(wb);
-        return val<0;
-    }
-
-}
diff --git a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/TRStringDistance.java b/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/TRStringDistance.java
deleted file mode 100755
index 992d3bb..0000000
--- a/sandbox/contributions/spellchecker/src/java/org/apache/lucene/search/spell/TRStringDistance.java
+++ /dev/null
@@ -1,132 +0,0 @@
-package org.apache.lucene.search.spell;
-
-
-/**
- * Copyright 2002-2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Edit distance  class
- */
-public final class TRStringDistance {
-
-    final char[] sa;
-    final int n;
-    final int[][][] cache=new int[30][][];
-
-
-    /**
-     * Optimized to run a bit faster than the static getDistance().
-     * In one benchmark times were 5.3sec using ctr vs 8.5sec w/ static method, thus 37% faster.
-     */
-    public TRStringDistance (String target) {
-        sa=target.toCharArray();
-        n=sa.length;
-    }
-
-
-    //*****************************
-     // Compute Levenshtein distance
-     //*****************************
-      public final int getDistance (String other) {
-          int d[][]; // matrix
-          int cost; // cost
-
-          // Step 1
-          final char[] ta=other.toCharArray();
-          final int m=ta.length;
-          if (n==0) {
-              return m;
-          }
-          if (m==0) {
-              return n;
-          }
-
-          if (m>=cache.length) {
-              d=form(n, m);
-          }
-          else if (cache[m]!=null) {
-              d=cache[m];
-          }
-          else {
-              d=cache[m]=form(n, m);
-
-              // Step 3
-
-          }
-          for (int i=1; i<=n; i++) {
-              final char s_i=sa[i-1];
-
-              // Step 4
-
-              for (int j=1; j<=m; j++) {
-                  final char t_j=ta[j-1];
-
-                  // Step 5
-
-                  if (s_i==t_j) { // same
-                      cost=0;
-                  }
-                  else { // not a match
-                      cost=1;
-
-                      // Step 6
-
-                  }
-                  d[i][j]=min3(d[i-1][j]+1, d[i][j-1]+1, d[i-1][j-1]+cost);
-
-              }
-
-          }
-
-          // Step 7
-          return d[n][m];
-
-      }
-
-
-    /**
-     *
-     */
-    private static int[][] form (int n, int m) {
-        int[][] d=new int[n+1][m+1];
-        // Step 2
-
-        for (int i=0; i<=n; i++) {
-            d[i][0]=i;
-
-        }
-        for (int j=0; j<=m; j++) {
-            d[0][j]=j;
-        }
-        return d;
-    }
-
-
-    //****************************
-     // Get minimum of three values
-     //****************************
-      private static int min3 (int a, int b, int c) {
-          int mi=a;
-          if (b<mi) {
-              mi=b;
-          }
-          if (c<mi) {
-              mi=c;
-          }
-          return mi;
-
-      }
-}
diff --git a/sandbox/contributions/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java b/sandbox/contributions/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
deleted file mode 100755
index f6a2d9c..0000000
--- a/sandbox/contributions/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
+++ /dev/null
@@ -1,122 +0,0 @@
-package org.apache.lucene.search.spell;
-
-
-import junit.framework.*;
-import org.apache.lucene.search.spell.*;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.util.English;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexReader;
-import java.io.IOException;
-import org.apache.lucene.store.FSDirectory;
-import org.apache.lucene.store.Directory;
-import java.io.File;
-
-
-/**
- * Test case
- * @author Nicolas Maisonneuve
- */
-
-public class TestSpellChecker
-extends TestCase {
-    private SpellChecker spellChecker;
-    Directory userindex, spellindex;
-
-    protected void setUp () throws Exception {
-        super.setUp();
-
-        //create a user index
-        userindex=new RAMDirectory();
-        IndexWriter writer=new IndexWriter(userindex, new SimpleAnalyzer(), true);
-
-        for (int i=0; i<1000; i++) {
-            Document doc=new Document();
-            doc.add(Field.Text("field1", English.intToEnglish(i)));
-            doc.add(Field.Text("field2", English.intToEnglish(i+1))); // + word thousand
-            writer.addDocument(doc);
-        }
-        writer.close();
-
-        // create the spellChecker
-        File file=new File("d://test");
-        spellindex=FSDirectory.getDirectory(file, true);
-        spellChecker=new SpellChecker(spellindex);
-    }
-
-
-    public void testBuild () {
-        try {
-            IndexReader r=IndexReader.open(userindex);
-
-            spellChecker.clearIndex();
-
-            addwords(r, "field1");
-            int num_field1=this.numdoc();
-
-            addwords(r, "field2");
-            int num_field2=this.numdoc();
-
-            this.assertTrue(num_field2==num_field1+1);
-
-            // test small word
-            String[] l=spellChecker.suggestSimilar("fvie", 2);
-            this.assertTrue(l[0].equals("five"));
-
-            l=spellChecker.suggestSimilar("fiv", 2);
-            this.assertTrue(l[0].equals("five"));
-
-            l=spellChecker.suggestSimilar("ive", 2);
-            this.assertTrue(l[0].equals("five"));
-
-            l=spellChecker.suggestSimilar("fives", 2);
-            this.assertTrue(l[0].equals("five"));
-
-            l=spellChecker.suggestSimilar("fie", 2);
-            this.assertTrue(l[0].equals("five"));
-
-            l=spellChecker.suggestSimilar("fi", 2);
-            this.assertEquals(0,l.length);
-
-            // test restreint to a field
-            l=spellChecker.suggestSimilar("tousand", 10, r, "field1", false);
-            this.assertEquals(0,l.length); // there isn't the term thousand in the field field1
-
-            l=spellChecker.suggestSimilar("tousand", 10, r, "field2", false);
-            this.assertEquals(1,l.length); // there is the term thousand in the field field2
-        }
-        catch (IOException e) {
-            e.printStackTrace();
-            this.assertTrue(false);
-        }
-
-    }
-
-
-    private void addwords (IndexReader r, String field) throws IOException {
-        long time=System.currentTimeMillis();
-        spellChecker.indexDictionnary(new LuceneDictionary(r, field));
-        time=System.currentTimeMillis()-time;
-        System.out.println("time to build "+field+": "+time);
-    }
-
-
-    private int numdoc () throws IOException {
-        IndexReader rs=IndexReader.open(spellindex);
-        int num=rs.numDocs();
-        this.assertTrue(num!=0);
-        System.out.println("num docs: "+num);
-        rs.close();
-        return num;
-    }
-
-
-    protected void tearDown () throws Exception {
-        spellChecker=null;
-        super.tearDown();
-    }
-
-}

