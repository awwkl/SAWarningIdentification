GitDiffStart: 43fc21c41e7988583ed97fcacf8927c44966c1ee | Thu May 10 01:06:43 2012 +0000
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUCollationKeyFilterFactory.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUCollationKeyFilterFactory.java
index e5fd999..2386533 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUCollationKeyFilterFactory.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUCollationKeyFilterFactory.java
@@ -22,6 +22,7 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
@@ -31,7 +32,7 @@ import com.ibm.icu.text.RuleBasedCollator;
 import com.ibm.icu.util.ULocale;
 
 @Deprecated
-public class TestICUCollationKeyFilterFactory extends BaseTokenTestCase {
+public class TestICUCollationKeyFilterFactory extends BaseTokenStreamTestCase {
 
   /*
    * Turkish has some funny casing.
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUFoldingFilterFactory.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUFoldingFilterFactory.java
index b7e49e5..bce4228 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUFoldingFilterFactory.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUFoldingFilterFactory.java
@@ -20,19 +20,20 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /** basic tests for {@link ICUFoldingFilterFactory} */
-public class TestICUFoldingFilterFactory extends BaseTokenTestCase {
+public class TestICUFoldingFilterFactory extends BaseTokenStreamTestCase {
   
   /** basic tests to ensure the folding is working */
   public void test() throws Exception {
     Reader reader = new StringReader("R√©sum√©");
     ICUFoldingFilterFactory factory = new ICUFoldingFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    Tokenizer tokenizer = new WhitespaceTokenizer(DEFAULT_VERSION, reader);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Tokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader);
     TokenStream stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] { "resume" });
   }
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUNormalizer2FilterFactory.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUNormalizer2FilterFactory.java
index 5cfc39a..a8702cd 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUNormalizer2FilterFactory.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUNormalizer2FilterFactory.java
@@ -19,21 +19,25 @@ package org.apache.solr.analysis;
 
 import java.io.Reader;
 import java.io.StringReader;
+import java.util.Collections;
+import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /** basic tests for {@link ICUNormalizer2FilterFactory} */
-public class TestICUNormalizer2FilterFactory extends BaseTokenTestCase {
+public class TestICUNormalizer2FilterFactory extends BaseTokenStreamTestCase {
   
   /** Test nfkc_cf defaults */
   public void testDefaults() throws Exception {
     Reader reader = new StringReader("This is a Ôº¥Ô?ÔΩ??");
     ICUNormalizer2FilterFactory factory = new ICUNormalizer2FilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
-    Tokenizer tokenizer = new WhitespaceTokenizer(DEFAULT_VERSION, reader);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
+    Tokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader);
     TokenStream stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] { "this", "is", "a", "test" });
   }
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUTokenizerFactory.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUTokenizerFactory.java
index 8b6992e..cc516f1 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUTokenizerFactory.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUTokenizerFactory.java
@@ -20,10 +20,11 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 
 /** basic tests for {@link ICUTokenizerFactory} **/
-public class TestICUTokenizerFactory extends BaseTokenTestCase {
+public class TestICUTokenizerFactory extends BaseTokenStreamTestCase {
   public void testMixedText() throws Exception {
     Reader reader = new StringReader("‡∏?∏≤‡∏£‡?‡∏µ‡?‡π??‡π??‡π?∏≠‡∏??‡∏??‡∏?∏ß‡π?∏≤‡∏?∏≤‡∏??‡∏?  This is a test ‡∫?∫ß‡ª?∫≤‡∫?∫≠‡∫?");
     ICUTokenizerFactory factory = new ICUTokenizerFactory();
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUTransformFilterFactory.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUTransformFilterFactory.java
index 9df2c99..07cb3e5 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUTransformFilterFactory.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestICUTransformFilterFactory.java
@@ -22,12 +22,13 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 
 /** basic tests for {@link ICUTransformFilterFactory} */
-public class TestICUTransformFilterFactory extends BaseTokenTestCase {
+public class TestICUTransformFilterFactory extends BaseTokenStreamTestCase {
   
   /** ensure the transform is working */
   public void test() throws Exception {
@@ -36,7 +37,7 @@ public class TestICUTransformFilterFactory extends BaseTokenTestCase {
     Map<String,String> args = new HashMap<String,String>();
     args.put("id", "Traditional-Simplified");
     factory.init(args);
-    Tokenizer tokenizer = new WhitespaceTokenizer(DEFAULT_VERSION, reader);
+    Tokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader);
     TokenStream stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] { "ÁÆ????" });
   }
@@ -49,7 +50,7 @@ public class TestICUTransformFilterFactory extends BaseTokenTestCase {
     Map<String,String> args = new HashMap<String,String>();
     args.put("id", "Cyrillic-Latin");
     factory.init(args);
-    Tokenizer tokenizer = new WhitespaceTokenizer(DEFAULT_VERSION, reader);
+    Tokenizer tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader);
     TokenStream stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] { "Rossijska√¢",  "Federaci√¢" });
     
@@ -57,7 +58,7 @@ public class TestICUTransformFilterFactory extends BaseTokenTestCase {
     reader = new StringReader("Rossijska√¢ Federaci√¢");
     args.put("direction", "reverse");
     factory.init(args);
-    tokenizer = new WhitespaceTokenizer(DEFAULT_VERSION, reader);
+    tokenizer = new WhitespaceTokenizer(TEST_VERSION_CURRENT, reader);
     stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] { "?–æ??–∏–π?–∫–∞?", "–§–µ–¥–µ?–∞?–∏?" });
   }
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestMorfologikFilterFactory.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestMorfologikFilterFactory.java
index ddf5e37..599fd42 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestMorfologikFilterFactory.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestMorfologikFilterFactory.java
@@ -4,6 +4,7 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.schema.IndexSchema;
@@ -28,16 +29,16 @@ import org.apache.solr.schema.IndexSchema;
 /**
  * Test for {@link MorfologikFilterFactory}.
  */
-public class TestMorfologikFilterFactory extends BaseTokenTestCase {
+public class TestMorfologikFilterFactory extends BaseTokenStreamTestCase {
   public void testCreateDictionary() throws Exception {
     StringReader reader = new StringReader("rowery bilety");
     Map<String,String> initParams = new HashMap<String,String>();
     initParams.put(MorfologikFilterFactory.DICTIONARY_SCHEMA_ATTRIBUTE,
         "morfologik");
     MorfologikFilterFactory factory = new MorfologikFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(initParams);
-    TokenStream ts = factory.create(new WhitespaceTokenizer(DEFAULT_VERSION,
+    TokenStream ts = factory.create(new WhitespaceTokenizer(TEST_VERSION_CURRENT,
         reader));
     assertTokenStreamContents(ts, new String[] {"rower", "bilet"});
   }
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestSmartChineseFactories.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestSmartChineseFactories.java
index dea2754..a4806ea 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestSmartChineseFactories.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestSmartChineseFactories.java
@@ -19,6 +19,7 @@ package org.apache.solr.analysis;
 
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
@@ -27,11 +28,11 @@ import org.apache.lucene.analysis.core.WhitespaceTokenizer;
  * Tests for {@link SmartChineseSentenceTokenizerFactory} and 
  * {@link SmartChineseWordTokenFilterFactory}
  */
-public class TestSmartChineseFactories extends BaseTokenTestCase {
+public class TestSmartChineseFactories extends BaseTokenStreamTestCase {
   /** Test showing the behavior with whitespace */
   public void testSimple() throws Exception {
     String sentence = "??¥≠‰π∞‰???????Ë£???";
-    WhitespaceTokenizer ws = new WhitespaceTokenizer(DEFAULT_VERSION, new StringReader(sentence));
+    WhitespaceTokenizer ws = new WhitespaceTokenizer(TEST_VERSION_CURRENT, new StringReader(sentence));
     SmartChineseWordTokenFilterFactory factory = new SmartChineseWordTokenFilterFactory();
     TokenStream ts = factory.create(ws);
     // TODO: fix smart chinese to not emit punctuation tokens
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestStempelPolishStemFilterFactory.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestStempelPolishStemFilterFactory.java
index dee9aed..4f69866 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestStempelPolishStemFilterFactory.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/analysis/TestStempelPolishStemFilterFactory.java
@@ -2,6 +2,7 @@ package org.apache.solr.analysis;
 
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.core.WhitespaceTokenizer;
 import org.apache.solr.core.SolrResourceLoader;
@@ -26,12 +27,12 @@ import org.apache.solr.core.SolrResourceLoader;
 /**
  * Tests for {@link StempelPolishStemFilterFactory}
  */
-public class TestStempelPolishStemFilterFactory extends BaseTokenTestCase {
+public class TestStempelPolishStemFilterFactory extends BaseTokenStreamTestCase {
   public void testBasics() throws Exception {
     StringReader document = new StringReader("studenta studenci");
     StempelPolishStemFilterFactory factory = new StempelPolishStemFilterFactory();
     factory.inform(new SolrResourceLoader(null, null));
-    TokenStream ts = factory.create(new WhitespaceTokenizer(DEFAULT_VERSION, document));
+    TokenStream ts = factory.create(new WhitespaceTokenizer(TEST_VERSION_CURRENT, document));
     assertTokenStreamContents(ts,
         new String[] { "student", "student" });
   }
diff --git a/solr/core/src/test/org/apache/solr/analysis/CommonGramsFilterFactoryTest.java b/solr/core/src/test/org/apache/solr/analysis/CommonGramsFilterFactoryTest.java
index 4a4ae09..25ca68c 100644
--- a/solr/core/src/test/org/apache/solr/analysis/CommonGramsFilterFactoryTest.java
+++ b/solr/core/src/test/org/apache/solr/analysis/CommonGramsFilterFactoryTest.java
@@ -17,6 +17,7 @@ package org.apache.solr.analysis;
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -25,6 +26,7 @@ import org.apache.lucene.analysis.util.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
 import java.io.StringReader;
+import java.util.Collections;
 import java.util.Map;
 import java.util.HashMap;
 
@@ -33,7 +35,7 @@ import java.util.HashMap;
  * used by the StopFilterFactoryTest TODO: consider creating separate test files
  * so this won't break if stop filter test files change
  **/
-public class CommonGramsFilterFactoryTest extends BaseTokenTestCase {
+public class CommonGramsFilterFactoryTest extends BaseTokenStreamTestCase {
 
   public void testInform() throws Exception {
     ResourceLoader loader = new SolrResourceLoader(null, null);
@@ -42,7 +44,7 @@ public class CommonGramsFilterFactoryTest extends BaseTokenTestCase {
     Map<String, String> args = new HashMap<String, String>();
     args.put("words", "stop-1.txt");
     args.put("ignoreCase", "true");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     CharArraySet words = factory.getCommonWords();
@@ -54,7 +56,7 @@ public class CommonGramsFilterFactoryTest extends BaseTokenTestCase {
 
     factory = new CommonGramsFilterFactory();
     args.put("words", "stop-1.txt, stop-2.txt");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     words = factory.getCommonWords();
@@ -67,7 +69,7 @@ public class CommonGramsFilterFactoryTest extends BaseTokenTestCase {
     factory = new CommonGramsFilterFactory();
     args.put("words", "stop-snowball.txt");
     args.put("format", "snowball");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     words = factory.getCommonWords();
@@ -89,8 +91,9 @@ public class CommonGramsFilterFactoryTest extends BaseTokenTestCase {
     ResourceLoader loader = new SolrResourceLoader(null, null);
     assertTrue("loader is null and it shouldn't be", loader != null);
     CommonGramsFilterFactory factory = new CommonGramsFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     factory.inform(loader);
     CharArraySet words = factory.getCommonWords();
     assertTrue("words is null and it shouldn't be", words != null);
diff --git a/solr/core/src/test/org/apache/solr/analysis/CommonGramsQueryFilterFactoryTest.java b/solr/core/src/test/org/apache/solr/analysis/CommonGramsQueryFilterFactoryTest.java
index 6349ef0..4b0213a 100644
--- a/solr/core/src/test/org/apache/solr/analysis/CommonGramsQueryFilterFactoryTest.java
+++ b/solr/core/src/test/org/apache/solr/analysis/CommonGramsQueryFilterFactoryTest.java
@@ -16,6 +16,7 @@
  */
 package org.apache.solr.analysis;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -24,6 +25,7 @@ import org.apache.lucene.analysis.util.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
 import java.io.StringReader;
+import java.util.Collections;
 import java.util.Map;
 import java.util.HashMap;
 
@@ -32,7 +34,7 @@ import java.util.HashMap;
  * used by the StopFilterFactoryTest TODO: consider creating separate test files
  * so this won't break if stop filter test files change
  **/
-public class CommonGramsQueryFilterFactoryTest extends BaseTokenTestCase {
+public class CommonGramsQueryFilterFactoryTest extends BaseTokenStreamTestCase {
 
   public void testInform() throws Exception {
     ResourceLoader loader = new SolrResourceLoader(null, null);
@@ -41,7 +43,7 @@ public class CommonGramsQueryFilterFactoryTest extends BaseTokenTestCase {
     Map<String, String> args = new HashMap<String, String>();
     args.put("words", "stop-1.txt");
     args.put("ignoreCase", "true");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     CharArraySet words = factory.getCommonWords();
@@ -53,7 +55,7 @@ public class CommonGramsQueryFilterFactoryTest extends BaseTokenTestCase {
 
     factory = new CommonGramsQueryFilterFactory();
     args.put("words", "stop-1.txt, stop-2.txt");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     words = factory.getCommonWords();
@@ -64,7 +66,7 @@ public class CommonGramsQueryFilterFactoryTest extends BaseTokenTestCase {
         .isIgnoreCase() == true);
 
     factory = new CommonGramsQueryFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     args.put("words", "stop-snowball.txt");
     args.put("format", "snowball");
     factory.init(args);
@@ -88,8 +90,9 @@ public class CommonGramsQueryFilterFactoryTest extends BaseTokenTestCase {
     ResourceLoader loader = new SolrResourceLoader(null, null);
     assertTrue("loader is null and it shouldn't be", loader != null);
     CommonGramsQueryFilterFactory factory = new CommonGramsQueryFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     factory.inform(loader);
     CharArraySet words = factory.getCommonWords();
     assertTrue("words is null and it shouldn't be", words != null);
diff --git a/solr/core/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterFactoryTest.java b/solr/core/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterFactoryTest.java
index 2ee91c5..c9c59c3 100644
--- a/solr/core/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterFactoryTest.java
+++ b/solr/core/src/test/org/apache/solr/analysis/DoubleMetaphoneFilterFactoryTest.java
@@ -20,12 +20,13 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.phonetic.DoubleMetaphoneFilter;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
-public class DoubleMetaphoneFilterFactoryTest extends BaseTokenTestCase {
+public class DoubleMetaphoneFilterFactoryTest extends BaseTokenStreamTestCase {
 
   public void testDefaults() throws Exception {
     DoubleMetaphoneFilterFactory factory = new DoubleMetaphoneFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/LengthFilterTest.java b/solr/core/src/test/org/apache/solr/analysis/LengthFilterTest.java
index 676d98c..30f2f12 100644
--- a/solr/core/src/test/org/apache/solr/analysis/LengthFilterTest.java
+++ b/solr/core/src/test/org/apache/solr/analysis/LengthFilterTest.java
@@ -21,10 +21,11 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
-public class LengthFilterTest extends BaseTokenTestCase {
+public class LengthFilterTest extends BaseTokenStreamTestCase {
 
   public void test() throws IOException {
     LengthFilterFactory factory = new LengthFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/SnowballPorterFilterFactoryTest.java b/solr/core/src/test/org/apache/solr/analysis/SnowballPorterFilterFactoryTest.java
index e324952..db84862 100644
--- a/solr/core/src/test/org/apache/solr/analysis/SnowballPorterFilterFactoryTest.java
+++ b/solr/core/src/test/org/apache/solr/analysis/SnowballPorterFilterFactoryTest.java
@@ -16,6 +16,7 @@ package org.apache.solr.analysis;
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -34,7 +35,7 @@ import java.util.List;
 import java.util.Map;
 import java.util.ArrayList;
 
-public class SnowballPorterFilterFactoryTest extends BaseTokenTestCase {
+public class SnowballPorterFilterFactoryTest extends BaseTokenStreamTestCase {
 
   public void test() throws IOException {
     EnglishStemmer stemmer = new EnglishStemmer();
@@ -50,7 +51,7 @@ public class SnowballPorterFilterFactoryTest extends BaseTokenTestCase {
     Map<String, String> args = new HashMap<String, String>();
     args.put("language", "English");
 
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(new LinesMockSolrResourceLoader(new ArrayList<String>()));
     Tokenizer tokenizer = new MockTokenizer(
@@ -88,7 +89,7 @@ public class SnowballPorterFilterFactoryTest extends BaseTokenTestCase {
     Map<String,String> args = new HashMap<String,String>();
     args.put("protected", "protwords.txt");
     args.put("language", "English");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     Reader reader = new StringReader("ridding of some stemming");
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestArabicFilters.java b/solr/core/src/test/org/apache/solr/analysis/TestArabicFilters.java
index f0375da..c0e45ec 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestArabicFilters.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestArabicFilters.java
@@ -19,7 +19,10 @@ package org.apache.solr.analysis;
 
 import java.io.Reader;
 import java.io.StringReader;
+import java.util.Collections;
+import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.CharReader;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -27,7 +30,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the Arabic filter Factories are working.
  */
-public class TestArabicFilters extends BaseTokenTestCase {
+public class TestArabicFilters extends BaseTokenStreamTestCase {
   /**
    * Test ArabicLetterTokenizerFactory
    * @deprecated (3.1) Remove in Lucene 5.0
@@ -36,8 +39,9 @@ public class TestArabicFilters extends BaseTokenTestCase {
   public void testTokenizer() throws Exception {
     Reader reader = new StringReader("ÿß?ÿ∞?? ????ÿ™ ÿ£??ÿß???");
     ArabicLetterTokenizerFactory factory = new ArabicLetterTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, new String[] {"ÿß?ÿ∞??", "????ÿ™", "ÿ£??ÿß???"});
   }
@@ -48,11 +52,12 @@ public class TestArabicFilters extends BaseTokenTestCase {
   public void testNormalizer() throws Exception {
     Reader reader = new StringReader("ÿß?ÿ∞?? ????ÿ™ ÿ£??ÿß???");
     StandardTokenizerFactory factory = new StandardTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     ArabicNormalizationFilterFactory filterFactory = new ArabicNormalizationFilterFactory();
-    filterFactory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
-    filterFactory.init(EMPTY_PARAMS);
+    filterFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
+    filterFactory.init(args);
     Tokenizer tokenizer = factory.create(reader);
     TokenStream stream = filterFactory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] {"ÿß?ÿ∞??", "???ÿ™", "ÿß??ÿß???"});
@@ -64,12 +69,13 @@ public class TestArabicFilters extends BaseTokenTestCase {
   public void testStemmer() throws Exception {
     Reader reader = new StringReader("ÿß?ÿ∞?? ????ÿ™ ÿ£??ÿß???");
     StandardTokenizerFactory factory = new StandardTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     ArabicNormalizationFilterFactory normFactory = new ArabicNormalizationFilterFactory();
-    normFactory.setLuceneMatchVersion(DEFAULT_VERSION);
+    normFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     ArabicStemFilterFactory stemFactory = new ArabicStemFilterFactory();
-    factory.init(EMPTY_PARAMS);
-    normFactory.init(EMPTY_PARAMS);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
+    normFactory.init(args);
     Tokenizer tokenizer = factory.create(reader);
     TokenStream stream = normFactory.create(tokenizer);
     stream = stemFactory.create(stream);
@@ -83,8 +89,9 @@ public class TestArabicFilters extends BaseTokenTestCase {
     Reader reader = new StringReader("??????±ÿ?");
     PersianCharFilterFactory charfilterFactory = new PersianCharFilterFactory();
     StandardTokenizerFactory tokenizerFactory = new StandardTokenizerFactory();
-    tokenizerFactory.setLuceneMatchVersion(DEFAULT_VERSION);
-    tokenizerFactory.init(EMPTY_PARAMS);
+    tokenizerFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    tokenizerFactory.init(args);
     TokenStream stream = tokenizerFactory.create(charfilterFactory.create(CharReader.get(reader)));
     assertTokenStreamContents(stream, new String[] { "??", "ÿÆ?ÿ±ÿØ" });
   }
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestBeiderMorseFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestBeiderMorseFilterFactory.java
index 5d4e8e4..fd85a61 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestBeiderMorseFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestBeiderMorseFilterFactory.java
@@ -18,18 +18,21 @@ package org.apache.solr.analysis;
  */
 
 import java.io.StringReader;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /** Simple tests for {@link BeiderMorseFilterFactory} */
-public class TestBeiderMorseFilterFactory extends BaseTokenTestCase {
+public class TestBeiderMorseFilterFactory extends BaseTokenStreamTestCase {
   public void testBasics() throws Exception {
     BeiderMorseFilterFactory factory = new BeiderMorseFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     TokenStream ts = factory.create(new MockTokenizer(new StringReader("Weinberg"), MockTokenizer.WHITESPACE, false));
     assertTokenStreamContents(ts,
         new String[] { "vDnbirk", "vanbirk", "vinbirk", "wDnbirk", "wanbirk", "winbirk" },
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestBrazilianStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestBrazilianStemFilterFactory.java
index 520fdf6..83f2145 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestBrazilianStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestBrazilianStemFilterFactory.java
@@ -20,6 +20,7 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -27,7 +28,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the Brazilian stem filter factory is working.
  */
-public class TestBrazilianStemFilterFactory extends BaseTokenTestCase {
+public class TestBrazilianStemFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually stems and normalizes text.
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestBulgarianStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestBulgarianStemFilterFactory.java
index 65ed14e..2222954 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestBulgarianStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestBulgarianStemFilterFactory.java
@@ -20,6 +20,7 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -27,7 +28,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the Bulgarian stem filter factory is working.
  */
-public class TestBulgarianStemFilterFactory extends BaseTokenTestCase {
+public class TestBulgarianStemFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually stems text.
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestCJKBigramFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestCJKBigramFilterFactory.java
index e9acbbe..0a988db 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestCJKBigramFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestCJKBigramFilterFactory.java
@@ -19,9 +19,11 @@ package org.apache.solr.analysis;
 
 import java.io.Reader;
 import java.io.StringReader;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.standard.StandardTokenizer;
 
@@ -29,12 +31,13 @@ import org.apache.lucene.analysis.standard.StandardTokenizer;
  * Simple tests to ensure the CJK bigram factory is working.
  * @deprecated
  */
-public class TestCJKBigramFilterFactory extends BaseTokenTestCase {
+public class TestCJKBigramFilterFactory extends BaseTokenStreamTestCase {
   public void testDefaults() throws Exception {
     Reader reader = new StringReader("Â§????????Ë©??????°„???");
     CJKBigramFilterFactory factory = new CJKBigramFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     TokenStream stream = factory.create(new StandardTokenizer(TEST_VERSION_CURRENT, reader));
     assertTokenStreamContents(stream,
         new String[] { "Â§??", "???", "???", "Â≠??", "???", "??©¶", "Ë©??", "È®??", "???", "?Ω„?", "?°„?" });
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestCJKTokenizerFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestCJKTokenizerFactory.java
index 9e2571b..9369b97 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestCJKTokenizerFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestCJKTokenizerFactory.java
@@ -20,6 +20,7 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
@@ -27,7 +28,7 @@ import org.apache.lucene.analysis.TokenStream;
  * @deprecated
  */
 @Deprecated
-public class TestCJKTokenizerFactory extends BaseTokenTestCase {
+public class TestCJKTokenizerFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the tokenizer actually tokenizes CJK text correctly
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestCJKWidthFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestCJKWidthFilterFactory.java
index b48fdc5..c6b1df5 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestCJKWidthFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestCJKWidthFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the CJKWidthFilterFactory is working
  */
-public class TestCJKWidthFilterFactory extends BaseTokenTestCase {
+public class TestCJKWidthFilterFactory extends BaseTokenStreamTestCase {
   public void test() throws Exception {
     Reader reader = new StringReader("Ôº¥Ô?ÔΩ?? Ôº??Ôº??");
     CJKWidthFilterFactory factory = new CJKWidthFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestCapitalizationFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestCapitalizationFilterFactory.java
index 384c732..810902b 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestCapitalizationFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestCapitalizationFilterFactory.java
@@ -21,6 +21,7 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -29,7 +30,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * 
  */
-public class TestCapitalizationFilterFactory extends BaseTokenTestCase {
+public class TestCapitalizationFilterFactory extends BaseTokenStreamTestCase {
   
   public void testCapitalization() throws Exception 
   {
@@ -38,7 +39,7 @@ public class TestCapitalizationFilterFactory extends BaseTokenTestCase {
     args.put( CapitalizationFilterFactory.ONLY_FIRST_WORD, "true" );  
     
     CapitalizationFilterFactory factory = new CapitalizationFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init( args );
     assertTokenStreamContents(factory.create(
         new MockTokenizer(new StringReader("kiTTEN"), MockTokenizer.WHITESPACE, false)),
@@ -95,7 +96,7 @@ public class TestCapitalizationFilterFactory extends BaseTokenTestCase {
     
     // Now try some prefixes
     factory = new CapitalizationFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     args.put( "okPrefix", "McK" );  // all words
     factory.init( args );
     assertTokenStreamContents(factory.create(
@@ -122,7 +123,7 @@ public class TestCapitalizationFilterFactory extends BaseTokenTestCase {
     args.put( CapitalizationFilterFactory.ONLY_FIRST_WORD, "true" );
 
     CapitalizationFilterFactory factory = new CapitalizationFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init( args );
     factory.forceFirstLetter = true;
     assertTokenStreamContents(factory.create(
@@ -150,7 +151,7 @@ public class TestCapitalizationFilterFactory extends BaseTokenTestCase {
     args.put(CapitalizationFilterFactory.ONLY_FIRST_WORD, "true");
     args.put(CapitalizationFilterFactory.MIN_WORD_LENGTH, "5");
     CapitalizationFilterFactory factory = new CapitalizationFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     Tokenizer tokenizer = new MockTokenizer(new StringReader(
         "helo testing"), MockTokenizer.WHITESPACE, false);
@@ -166,7 +167,7 @@ public class TestCapitalizationFilterFactory extends BaseTokenTestCase {
     Map<String,String> args = new HashMap<String,String>();
     args.put(CapitalizationFilterFactory.MAX_WORD_COUNT, "2");
     CapitalizationFilterFactory factory = new CapitalizationFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     Tokenizer tokenizer = new MockTokenizer(new StringReader(
         "one two three four"), MockTokenizer.WHITESPACE, false);
@@ -181,7 +182,7 @@ public class TestCapitalizationFilterFactory extends BaseTokenTestCase {
     Map<String,String> args = new HashMap<String,String>();
     args.put(CapitalizationFilterFactory.MAX_WORD_COUNT, "2");
     CapitalizationFilterFactory factory = new CapitalizationFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     Tokenizer tokenizer = new MockTokenizer(new StringReader(
         "one two three four"), MockTokenizer.KEYWORD, false);
@@ -198,7 +199,7 @@ public class TestCapitalizationFilterFactory extends BaseTokenTestCase {
     Map<String,String> args = new HashMap<String,String>();
     args.put(CapitalizationFilterFactory.MAX_TOKEN_LENGTH, "2");
     CapitalizationFilterFactory factory = new CapitalizationFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     Tokenizer tokenizer = new MockTokenizer(new StringReader(
         "this is a test"), MockTokenizer.WHITESPACE, false);
@@ -214,7 +215,7 @@ public class TestCapitalizationFilterFactory extends BaseTokenTestCase {
     args.put(CapitalizationFilterFactory.KEEP, "kitten");
     args.put(CapitalizationFilterFactory.FORCE_FIRST_LETTER, "true");
     CapitalizationFilterFactory factory = new CapitalizationFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     Tokenizer tokenizer = new MockTokenizer(new StringReader("kitten"), MockTokenizer.WHITESPACE, false);
     TokenStream ts = factory.create(tokenizer);
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java
index 96240f7..fd5b227 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestChineseFilterFactory.java
@@ -20,6 +20,7 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -27,7 +28,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the Chinese filter factory is working.
  */
-public class TestChineseFilterFactory extends BaseTokenTestCase {
+public class TestChineseFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually normalizes text (numerics, stopwords)
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestChineseTokenizerFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestChineseTokenizerFactory.java
index bbbaa12..fce6114 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestChineseTokenizerFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestChineseTokenizerFactory.java
@@ -20,12 +20,13 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Chinese tokenizer factory is working.
  */
-public class TestChineseTokenizerFactory extends BaseTokenTestCase {
+public class TestChineseTokenizerFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the tokenizer actually tokenizes chinese text correctly
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java
index 99fc6cc..494c9c7 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestCollationKeyFilterFactory.java
@@ -28,12 +28,13 @@ import java.util.List;
 import java.util.Locale;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.util.ResourceLoader;
 
-public class TestCollationKeyFilterFactory extends BaseTokenTestCase {
+public class TestCollationKeyFilterFactory extends BaseTokenStreamTestCase {
 
   /*
    * Turkish has some funny casing.
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestCzechStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestCzechStemFilterFactory.java
index f040ffc..7035187 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestCzechStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestCzechStemFilterFactory.java
@@ -20,6 +20,7 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -27,7 +28,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the Czech stem filter factory is working.
  */
-public class TestCzechStemFilterFactory extends BaseTokenTestCase {
+public class TestCzechStemFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually stems text.
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestDelimitedPayloadTokenFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestDelimitedPayloadTokenFilterFactory.java
index af1ef78..18bb2bf 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestDelimitedPayloadTokenFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestDelimitedPayloadTokenFilterFactory.java
@@ -21,6 +21,7 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.payloads.DelimitedPayloadTokenFilter;
@@ -30,7 +31,7 @@ import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.util.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
-public class TestDelimitedPayloadTokenFilterFactory extends BaseTokenTestCase {
+public class TestDelimitedPayloadTokenFilterFactory extends BaseTokenStreamTestCase {
 
   public void testEncoder() throws Exception {
     Map<String,String> args = new HashMap<String, String>();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestDictionaryCompoundWordTokenFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestDictionaryCompoundWordTokenFilterFactory.java
index 5bcdd8b..cc3c264 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestDictionaryCompoundWordTokenFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestDictionaryCompoundWordTokenFilterFactory.java
@@ -22,6 +22,7 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -31,7 +32,7 @@ import org.apache.solr.core.SolrResourceLoader;
 /**
  * Simple tests to ensure the Dictionary compound filter factory is working.
  */
-public class TestDictionaryCompoundWordTokenFilterFactory extends BaseTokenTestCase {
+public class TestDictionaryCompoundWordTokenFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually decompounds text.
    */
@@ -42,7 +43,7 @@ public class TestDictionaryCompoundWordTokenFilterFactory extends BaseTokenTestC
     ResourceLoader loader = new SolrResourceLoader(null, null);
     Map<String,String> args = new HashMap<String,String>();
     args.put("dictionary", "compoundDictionary.txt");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     TokenStream stream = factory.create(tokenizer);
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestElisionFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestElisionFilterFactory.java
index 8b57e47..a9dc80f 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestElisionFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestElisionFilterFactory.java
@@ -19,9 +19,11 @@ package org.apache.solr.analysis;
 
 import java.io.Reader;
 import java.io.StringReader;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -31,7 +33,7 @@ import org.apache.solr.core.SolrResourceLoader;
 /**
  * Simple tests to ensure the French elision filter factory is working.
  */
-public class TestElisionFilterFactory extends BaseTokenTestCase {
+public class TestElisionFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually normalizes text.
    */
@@ -39,8 +41,7 @@ public class TestElisionFilterFactory extends BaseTokenTestCase {
     Reader reader = new StringReader("l'avion");
     Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
     ElisionFilterFactory factory = new ElisionFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     ResourceLoader loader = new SolrResourceLoader(null, null);
     Map<String,String> args = new HashMap<String,String>();
     args.put("articles", "frenchArticles.txt");
@@ -57,10 +58,10 @@ public class TestElisionFilterFactory extends BaseTokenTestCase {
     Reader reader = new StringReader("l'avion");
     Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
     ElisionFilterFactory factory = new ElisionFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     ResourceLoader loader = new SolrResourceLoader(null, null);
-    factory.init(new HashMap<String,String>());
     factory.inform(loader);
     TokenStream stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] { "avion" });
@@ -73,8 +74,7 @@ public class TestElisionFilterFactory extends BaseTokenTestCase {
     Reader reader = new StringReader("L'avion");
     Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
     ElisionFilterFactory factory = new ElisionFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     ResourceLoader loader = new SolrResourceLoader(null, null);
     Map<String,String> args = new HashMap<String,String>();
     args.put("articles", "frenchArticles.txt");
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestEnglishMinimalStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestEnglishMinimalStemFilterFactory.java
index 65ddf2f..2903d95 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestEnglishMinimalStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestEnglishMinimalStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the English minimal stem factory is working.
  */
-public class TestEnglishMinimalStemFilterFactory extends BaseTokenTestCase {
+public class TestEnglishMinimalStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("bricks");
     EnglishMinimalStemFilterFactory factory = new EnglishMinimalStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestFinnishLightStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestFinnishLightStemFilterFactory.java
index 7defdf5..fc2daec 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestFinnishLightStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestFinnishLightStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Finnish light stem factory is working.
  */
-public class TestFinnishLightStemFilterFactory extends BaseTokenTestCase {
+public class TestFinnishLightStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("aseistettujen");
     FinnishLightStemFilterFactory factory = new FinnishLightStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestFrenchLightStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestFrenchLightStemFilterFactory.java
index 79aa41a..076222a 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestFrenchLightStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestFrenchLightStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the French light stem factory is working.
  */
-public class TestFrenchLightStemFilterFactory extends BaseTokenTestCase {
+public class TestFrenchLightStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("administrativement");
     FrenchLightStemFilterFactory factory = new FrenchLightStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestFrenchMinimalStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestFrenchMinimalStemFilterFactory.java
index ad5f917..8f6a277 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestFrenchMinimalStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestFrenchMinimalStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the French minimal stem factory is working.
  */
-public class TestFrenchMinimalStemFilterFactory extends BaseTokenTestCase {
+public class TestFrenchMinimalStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("chevaux");
     FrenchMinimalStemFilterFactory factory = new FrenchMinimalStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestGalicianMinimalStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestGalicianMinimalStemFilterFactory.java
index ea3e32b..7c108cb 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestGalicianMinimalStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestGalicianMinimalStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Galician plural stem factory is working.
  */
-public class TestGalicianMinimalStemFilterFactory extends BaseTokenTestCase {
+public class TestGalicianMinimalStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("elefantes");
     GalicianMinimalStemFilterFactory factory = new GalicianMinimalStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestGalicianStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestGalicianStemFilterFactory.java
index 8b69e45..1674c76 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestGalicianStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestGalicianStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Galician stem factory is working.
  */
-public class TestGalicianStemFilterFactory extends BaseTokenTestCase {
+public class TestGalicianStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("cari√±osa");
     GalicianStemFilterFactory factory = new GalicianStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestGermanLightStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestGermanLightStemFilterFactory.java
index 6281174..5ebdb63 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestGermanLightStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestGermanLightStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the German light stem factory is working.
  */
-public class TestGermanLightStemFilterFactory extends BaseTokenTestCase {
+public class TestGermanLightStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("h√§user");
     GermanLightStemFilterFactory factory = new GermanLightStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestGermanMinimalStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestGermanMinimalStemFilterFactory.java
index 32fe243..1c81b97 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestGermanMinimalStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestGermanMinimalStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the German minimal stem factory is working.
  */
-public class TestGermanMinimalStemFilterFactory extends BaseTokenTestCase {
+public class TestGermanMinimalStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("bilder");
     GermanMinimalStemFilterFactory factory = new GermanMinimalStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestGermanNormalizationFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestGermanNormalizationFilterFactory.java
index 8f4a3e6..644c097 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestGermanNormalizationFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestGermanNormalizationFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the German normalization factory is working.
  */
-public class TestGermanNormalizationFilterFactory extends BaseTokenTestCase {
+public class TestGermanNormalizationFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("wei?bier");
     GermanNormalizationFilterFactory factory = new GermanNormalizationFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestGermanStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestGermanStemFilterFactory.java
index 80730e5..104bcf6 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestGermanStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestGermanStemFilterFactory.java
@@ -20,6 +20,7 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -27,7 +28,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the German stem filter factory is working.
  */
-public class TestGermanStemFilterFactory extends BaseTokenTestCase {
+public class TestGermanStemFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually stems text.
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestGreekLowerCaseFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestGreekLowerCaseFilterFactory.java
index 9f4e383..25d5aa5 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestGreekLowerCaseFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestGreekLowerCaseFilterFactory.java
@@ -19,7 +19,10 @@ package org.apache.solr.analysis;
 
 import java.io.Reader;
 import java.io.StringReader;
+import java.util.Collections;
+import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -27,7 +30,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the Greek lowercase filter factory is working.
  */
-public class TestGreekLowerCaseFilterFactory extends BaseTokenTestCase {
+public class TestGreekLowerCaseFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually lowercases (and a bit more) greek text.
    */
@@ -35,8 +38,9 @@ public class TestGreekLowerCaseFilterFactory extends BaseTokenTestCase {
     Reader reader = new StringReader("?Œ¨?Œø? ??Œ™?Œ£");
     Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
     GreekLowerCaseFilterFactory factory = new GreekLowerCaseFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     TokenStream stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] { "ŒºŒ±ŒπŒø?", "ŒºŒ±ŒπŒø?" });
   }
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestGreekStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestGreekStemFilterFactory.java
index 3a48e78..af3923f 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestGreekStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestGreekStemFilterFactory.java
@@ -3,6 +3,7 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -28,11 +29,11 @@ import org.apache.lucene.analysis.el.GreekLowerCaseFilter;
 /**
  * Simple tests to ensure the Greek stem filter factory is working.
  */
-public class TestGreekStemFilterFactory extends BaseTokenTestCase {
+public class TestGreekStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("Œ¨ŒΩŒ∏???Œø?");
     Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
-    TokenStream normalized = new GreekLowerCaseFilter(DEFAULT_VERSION, tokenizer);
+    TokenStream normalized = new GreekLowerCaseFilter(TEST_VERSION_CURRENT, tokenizer);
     GreekStemFilterFactory factory = new GreekStemFilterFactory();
     TokenStream stream = factory.create(normalized);
     assertTokenStreamContents(stream, new String[] { "Œ±ŒΩŒ∏???" });
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestHTMLStripCharFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestHTMLStripCharFilterFactory.java
index c240deb..fd4f9b8 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestHTMLStripCharFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestHTMLStripCharFilterFactory.java
@@ -22,15 +22,12 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.analysis.CharReader;
-import org.apache.lucene.analysis.CharStream;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.*;
 
 /**
  * Simple tests to ensure this factory is working
  */
-public class TestHTMLStripCharFilterFactory extends BaseTokenTestCase {
+public class TestHTMLStripCharFilterFactory extends BaseTokenStreamTestCase {
 
 
   public void testNothingChanged() throws IOException {
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestHindiFilters.java b/solr/core/src/test/org/apache/solr/analysis/TestHindiFilters.java
index b2c4cc7..466f0d4 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestHindiFilters.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestHindiFilters.java
@@ -19,25 +19,29 @@ package org.apache.solr.analysis;
 
 import java.io.Reader;
 import java.io.StringReader;
+import java.util.Collections;
+import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 
 /**
  * Simple tests to ensure the Hindi filter Factories are working.
  */
-public class TestHindiFilters extends BaseTokenTestCase {
+public class TestHindiFilters extends BaseTokenStreamTestCase {
   /**
    * Test IndicNormalizationFilterFactory
    */
   public void testIndicNormalizer() throws Exception {
     Reader reader = new StringReader("‡¶§‡??? ‡§?§æ‡•?§∞");
     StandardTokenizerFactory factory = new StandardTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     IndicNormalizationFilterFactory filterFactory = new IndicNormalizationFilterFactory();
-    filterFactory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
-    filterFactory.init(EMPTY_PARAMS);
+    filterFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
+    filterFactory.init(args);
     Tokenizer tokenizer = factory.create(reader);
     TokenStream stream = filterFactory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] { "‡ß?", "‡§?§∞" });
@@ -49,12 +53,13 @@ public class TestHindiFilters extends BaseTokenTestCase {
   public void testHindiNormalizer() throws Exception {
     Reader reader = new StringReader("‡•?§ø‡§§‡§æ‡§?");
     StandardTokenizerFactory factory = new StandardTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     IndicNormalizationFilterFactory indicFilterFactory = new IndicNormalizationFilterFactory();
     HindiNormalizationFilterFactory hindiFilterFactory = new HindiNormalizationFilterFactory();
-    hindiFilterFactory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
-    hindiFilterFactory.init(EMPTY_PARAMS);
+    hindiFilterFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
+    hindiFilterFactory.init(args);
     Tokenizer tokenizer = factory.create(reader);
     TokenStream stream = indicFilterFactory.create(tokenizer);
     stream = hindiFilterFactory.create(stream);
@@ -67,13 +72,14 @@ public class TestHindiFilters extends BaseTokenTestCase {
   public void testStemmer() throws Exception {
     Reader reader = new StringReader("‡§?§ø‡§§‡§æ‡§??‡§?");
     StandardTokenizerFactory factory = new StandardTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     IndicNormalizationFilterFactory indicFilterFactory = new IndicNormalizationFilterFactory();
     HindiNormalizationFilterFactory hindiFilterFactory = new HindiNormalizationFilterFactory();
     HindiStemFilterFactory stemFactory = new HindiStemFilterFactory();
-    stemFactory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
-    stemFactory.init(EMPTY_PARAMS);
+    stemFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
+    stemFactory.init(args);
     Tokenizer tokenizer = factory.create(reader);
     TokenStream stream = indicFilterFactory.create(tokenizer);
     stream = hindiFilterFactory.create(stream);
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestHungarianLightStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestHungarianLightStemFilterFactory.java
index a8fb9ce..5ee5d25 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestHungarianLightStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestHungarianLightStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Hungarian light stem factory is working.
  */
-public class TestHungarianLightStemFilterFactory extends BaseTokenTestCase {
+public class TestHungarianLightStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("h√°zakat");
     HungarianLightStemFilterFactory factory = new HungarianLightStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestHunspellStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestHunspellStemFilterFactory.java
index 99de05e..93e8e52 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestHunspellStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestHunspellStemFilterFactory.java
@@ -22,6 +22,7 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.solr.core.SolrResourceLoader;
@@ -30,13 +31,13 @@ import org.apache.solr.schema.IndexSchema;
 /**
  * Simple tests to ensure the Hunspell stemmer loads from factory
  */
-public class TestHunspellStemFilterFactory extends BaseTokenTestCase {
+public class TestHunspellStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     HunspellStemFilterFactory factory = new HunspellStemFilterFactory();
     Map<String,String> args = new HashMap<String,String>();
     args.put("dictionary", "hunspell-test.dic");
     args.put("affix", "hunspell-test.aff");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(new SolrResourceLoader("solr"));
     
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestHyphenationCompoundWordTokenFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestHyphenationCompoundWordTokenFilterFactory.java
index 2020882..d0b05f4 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestHyphenationCompoundWordTokenFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestHyphenationCompoundWordTokenFilterFactory.java
@@ -22,6 +22,7 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -31,7 +32,7 @@ import org.apache.solr.core.SolrResourceLoader;
 /**
  * Simple tests to ensure the Hyphenation compound filter factory is working.
  */
-public class TestHyphenationCompoundWordTokenFilterFactory extends BaseTokenTestCase {
+public class TestHyphenationCompoundWordTokenFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the factory works with hyphenation grammar+dictionary: using default options.
    */
@@ -43,7 +44,7 @@ public class TestHyphenationCompoundWordTokenFilterFactory extends BaseTokenTest
     Map<String,String> args = new HashMap<String,String>();
     args.put("hyphenator", "da_UTF8.xml");
     args.put("dictionary", "da_compoundDictionary.txt");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     TokenStream stream = factory.create(tokenizer);
@@ -68,7 +69,7 @@ public class TestHyphenationCompoundWordTokenFilterFactory extends BaseTokenTest
     args.put("hyphenator", "da_UTF8.xml");
     args.put("minSubwordSize", "2");
     args.put("maxSubwordSize", "4");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     TokenStream stream = factory.create(tokenizer);
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestIndonesianStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestIndonesianStemFilterFactory.java
index 77b1d54..cb19764 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestIndonesianStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestIndonesianStemFilterFactory.java
@@ -22,6 +22,7 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -29,7 +30,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the Indonesian stem filter factory is working.
  */
-public class TestIndonesianStemFilterFactory extends BaseTokenTestCase {
+public class TestIndonesianStemFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually stems text.
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestIrishLowerCaseFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestIrishLowerCaseFilterFactory.java
index bea2fa9..3c839de 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestIrishLowerCaseFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestIrishLowerCaseFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Irish lowercase filter factory is working.
  */
-public class TestIrishLowerCaseFilterFactory extends BaseTokenTestCase {
+public class TestIrishLowerCaseFilterFactory extends BaseTokenStreamTestCase {
   public void testCasing() throws Exception {
     Reader reader = new StringReader("nAthair tUISCE hARD");
     IrishLowerCaseFilterFactory factory = new IrishLowerCaseFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestItalianLightStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestItalianLightStemFilterFactory.java
index 34a307f..ba1f3b26 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestItalianLightStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestItalianLightStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Italian light stem factory is working.
  */
-public class TestItalianLightStemFilterFactory extends BaseTokenTestCase {
+public class TestItalianLightStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("ragazzo ragazzi");
     ItalianLightStemFilterFactory factory = new ItalianLightStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestJapaneseBaseFormFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestJapaneseBaseFormFilterFactory.java
index e581552..67b5874 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestJapaneseBaseFormFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestJapaneseBaseFormFilterFactory.java
@@ -19,18 +19,22 @@ package org.apache.solr.analysis;
 
 import java.io.IOException;
 import java.io.StringReader;
+import java.util.Collections;
+import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.solr.core.SolrResourceLoader;
 
 /**
  * Simple tests for {@link JapaneseBaseFormFilterFactory}
  */
-public class TestJapaneseBaseFormFilterFactory extends BaseTokenTestCase {
+public class TestJapaneseBaseFormFilterFactory extends BaseTokenStreamTestCase {
   public void testBasics() throws IOException {
     JapaneseTokenizerFactory tokenizerFactory = new JapaneseTokenizerFactory();
-    tokenizerFactory.setLuceneMatchVersion(DEFAULT_VERSION);
-    tokenizerFactory.init(EMPTY_PARAMS);
+    tokenizerFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    tokenizerFactory.init(args);
     tokenizerFactory.inform(new SolrResourceLoader(null, null));
     TokenStream ts = tokenizerFactory.create(new StringReader("?????????È®?????????æ„?"));
     JapaneseBaseFormFilterFactory factory = new JapaneseBaseFormFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestJapanesePartOfSpeechStopFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestJapanesePartOfSpeechStopFilterFactory.java
index ea55572..48cd9e1 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestJapanesePartOfSpeechStopFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestJapanesePartOfSpeechStopFilterFactory.java
@@ -2,9 +2,11 @@ package org.apache.solr.analysis;
 
 import java.io.IOException;
 import java.io.StringReader;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.solr.core.SolrResourceLoader;
 
@@ -28,15 +30,16 @@ import org.apache.solr.core.SolrResourceLoader;
 /**
  * Simple tests for {@link JapanesePartOfSpeechStopFilterFactory}
  */
-public class TestJapanesePartOfSpeechStopFilterFactory extends BaseTokenTestCase {
+public class TestJapanesePartOfSpeechStopFilterFactory extends BaseTokenStreamTestCase {
   public void testBasics() throws IOException {
     String tags = 
         "#  verb-main:\n" +
         "???-???\n";
     
     JapaneseTokenizerFactory tokenizerFactory = new JapaneseTokenizerFactory();
-    tokenizerFactory.setLuceneMatchVersion(DEFAULT_VERSION);
-    tokenizerFactory.init(EMPTY_PARAMS);
+    tokenizerFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> tokenizerArgs = Collections.emptyMap();
+    tokenizerFactory.init(tokenizerArgs);
     tokenizerFactory.inform(new SolrResourceLoader(null, null));
     TokenStream ts = tokenizerFactory.create(new StringReader("Áß???∂È??π„??º„?????????"));
     JapanesePartOfSpeechStopFilterFactory factory = new JapanesePartOfSpeechStopFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestJapaneseTokenizerFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestJapaneseTokenizerFactory.java
index b58d6a1..44087ca 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestJapaneseTokenizerFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestJapaneseTokenizerFactory.java
@@ -19,20 +19,23 @@ package org.apache.solr.analysis;
 
 import java.io.IOException;
 import java.io.StringReader;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.solr.core.SolrResourceLoader;
 
 /**
  * Simple tests for {@link JapaneseTokenizerFactory}
  */
-public class TestJapaneseTokenizerFactory extends BaseTokenTestCase {
+public class TestJapaneseTokenizerFactory extends BaseTokenStreamTestCase {
   public void testSimple() throws IOException {
     JapaneseTokenizerFactory factory = new JapaneseTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     factory.inform(new SolrResourceLoader(null, null));
     TokenStream ts = factory.create(new StringReader("???????ß„????"));
     assertTokenStreamContents(ts,
@@ -47,8 +50,9 @@ public class TestJapaneseTokenizerFactory extends BaseTokenTestCase {
    */
   public void testDefaults() throws IOException {
     JapaneseTokenizerFactory factory = new JapaneseTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     factory.inform(new SolrResourceLoader(null, null));
     TokenStream ts = factory.create(new StringReader("?∑„??????????????≥„????"));
     assertTokenStreamContents(ts,
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestKStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestKStemFilterFactory.java
index b6b25fd..a4029ee 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestKStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestKStemFilterFactory.java
@@ -3,6 +3,7 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
@@ -26,7 +27,7 @@ import org.apache.lucene.analysis.TokenStream;
 /**
  * Simple tests to ensure the kstem filter factory is working.
  */
-public class TestKStemFilterFactory extends BaseTokenTestCase {
+public class TestKStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("bricks");
     KStemFilterFactory factory = new KStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestKeepFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestKeepFilterFactory.java
index 5a4cefc..1f4c79c 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestKeepFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestKeepFilterFactory.java
@@ -16,6 +16,7 @@ package org.apache.solr.analysis;
  * limitations under the License.
  */
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.analysis.util.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
@@ -27,7 +28,7 @@ import java.util.HashMap;
  *
  *
  **/
-public class TestKeepFilterFactory extends BaseTokenTestCase{
+public class TestKeepFilterFactory extends BaseTokenStreamTestCase {
 
   public void testInform() throws Exception {
     ResourceLoader loader = new SolrResourceLoader(null, null);
@@ -36,7 +37,7 @@ public class TestKeepFilterFactory extends BaseTokenTestCase{
     Map<String, String> args = new HashMap<String, String>();
     args.put("words", "keep-1.txt");
     args.put("ignoreCase", "true");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     CharArraySet words = factory.getWords();
@@ -46,7 +47,7 @@ public class TestKeepFilterFactory extends BaseTokenTestCase{
 
     factory = new KeepWordFilterFactory();
     args.put("words", "keep-1.txt, keep-2.txt");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     words = factory.getWords();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestKeywordMarkerFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestKeywordMarkerFilterFactory.java
index 0ed3e45..73cc42f 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestKeywordMarkerFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestKeywordMarkerFilterFactory.java
@@ -23,6 +23,7 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.en.PorterStemFilter;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
@@ -33,7 +34,7 @@ import org.apache.solr.core.SolrResourceLoader;
 /**
  * Simple tests to ensure the keyword marker filter factory is working.
  */
-public class TestKeywordMarkerFilterFactory extends BaseTokenTestCase {
+public class TestKeywordMarkerFilterFactory extends BaseTokenStreamTestCase {
   public void testKeywords() throws IOException {
     Reader reader = new StringReader("dogs cats");
     Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
@@ -41,7 +42,7 @@ public class TestKeywordMarkerFilterFactory extends BaseTokenTestCase {
     Map<String,String> args = new HashMap<String,String>();
     ResourceLoader loader = new SolrResourceLoader(null, null);
     args.put("protected", "protwords.txt");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     
@@ -57,7 +58,7 @@ public class TestKeywordMarkerFilterFactory extends BaseTokenTestCase {
     ResourceLoader loader = new SolrResourceLoader(null, null);
     args.put("protected", "protwords.txt");
     args.put("ignoreCase", "true");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestLatvianStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestLatvianStemFilterFactory.java
index 027168e..28b102b 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestLatvianStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestLatvianStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Latvian stem factory is working.
  */
-public class TestLatvianStemFilterFactory extends BaseTokenTestCase {
+public class TestLatvianStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("tirgiem tirgus");
     LatvianStemFilterFactory factory = new LatvianStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java b/solr/core/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java
index a0ec832..dcbb160 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestMultiWordSynonyms.java
@@ -17,6 +17,7 @@
 
 package org.apache.solr.analysis;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.util.ResourceLoader;
@@ -33,7 +34,7 @@ import java.util.Map;
 /**
  * @since solr 1.4
  */
-public class TestMultiWordSynonyms extends BaseTokenTestCase {
+public class TestMultiWordSynonyms extends BaseTokenStreamTestCase {
 
   /**
    * @deprecated Remove this test in 5.0
@@ -54,7 +55,7 @@ public class TestMultiWordSynonyms extends BaseTokenTestCase {
     SynonymFilterFactory factory = new SynonymFilterFactory();
     Map<String,String> args = new HashMap<String,String>();
     args.put("synonyms", "synonyms.txt");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(new StringMockSolrResourceLoader("a b c,d"));
     TokenStream ts = factory.create(new MockTokenizer(new StringReader("a e"), MockTokenizer.WHITESPACE, false));
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestNGramFilters.java b/solr/core/src/test/org/apache/solr/analysis/TestNGramFilters.java
index 7c3eb17..f1b0706 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestNGramFilters.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestNGramFilters.java
@@ -22,6 +22,7 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -29,7 +30,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the NGram filter factories are working.
  */
-public class TestNGramFilters extends BaseTokenTestCase {
+public class TestNGramFilters extends BaseTokenStreamTestCase {
   /**
    * Test NGramTokenizerFactory
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestNorwegianLightStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestNorwegianLightStemFilterFactory.java
index 1ea4f11..751c13d 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestNorwegianLightStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestNorwegianLightStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Norwegian Light stem factory is working.
  */
-public class TestNorwegianLightStemFilterFactory extends BaseTokenTestCase {
+public class TestNorwegianLightStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("epler eple");
     NorwegianLightStemFilterFactory factory = new NorwegianLightStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestNorwegianMinimalStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestNorwegianMinimalStemFilterFactory.java
index 879cd9b..c76b3cb 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestNorwegianMinimalStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestNorwegianMinimalStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Norwegian Minimal stem factory is working.
  */
-public class TestNorwegianMinimalStemFilterFactory extends BaseTokenTestCase {
+public class TestNorwegianMinimalStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("eple eplet epler eplene eplets eplenes");
     NorwegianMinimalStemFilterFactory factory = new NorwegianMinimalStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilterFactory.java
index fba5c73..8161923 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceCharFilterFactory.java
@@ -22,15 +22,12 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
-import org.apache.lucene.analysis.CharReader;
-import org.apache.lucene.analysis.CharStream;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.*;
 
 /**
  * Simple tests to ensure this factory is working
  */
-public class TestPatternReplaceCharFilterFactory extends BaseTokenTestCase {
+public class TestPatternReplaceCharFilterFactory extends BaseTokenStreamTestCase {
   
   //           1111
   // 01234567890123
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceFilterFactory.java
index ce39f83..60a7ee1 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestPatternReplaceFilterFactory.java
@@ -17,6 +17,7 @@
 
 package org.apache.solr.analysis;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
@@ -27,7 +28,7 @@ import java.util.Map;
 /**
  * Simple tests to ensure this factory is working
  */
-public class TestPatternReplaceFilterFactory extends BaseTokenTestCase {
+public class TestPatternReplaceFilterFactory extends BaseTokenStreamTestCase {
 
   public void testReplaceAll() throws Exception {
     String input = "aabfooaabfooabfoob ab caaaaaaaaab";
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestPatternTokenizerFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestPatternTokenizerFactory.java
index 1361775..5f490f2 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestPatternTokenizerFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestPatternTokenizerFactory.java
@@ -21,11 +21,11 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.TokenStream;
 
 /** Simple Tests to ensure this factory is working */
-public class TestPatternTokenizerFactory extends BaseTokenTestCase 
-{
+public class TestPatternTokenizerFactory extends BaseTokenStreamTestCase {
   public void testFactory() throws Exception {
     final String INPUT = "G√ºnther G√ºnther is here";
 
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestPersianNormalizationFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestPersianNormalizationFilterFactory.java
index 372609b..7f0dc86 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestPersianNormalizationFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestPersianNormalizationFilterFactory.java
@@ -20,6 +20,7 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -27,7 +28,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the Persian normalization factory is working.
  */
-public class TestPersianNormalizationFilterFactory extends BaseTokenTestCase {
+public class TestPersianNormalizationFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually normalizes persian text.
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestPhoneticFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestPhoneticFilterFactory.java
index e1792f9..e20ae5f 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestPhoneticFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestPhoneticFilterFactory.java
@@ -23,6 +23,7 @@ import java.util.Map;
 
 import org.apache.commons.codec.language.Metaphone;
 import org.apache.commons.codec.language.Caverphone2;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -31,7 +32,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  *
  */
-public class TestPhoneticFilterFactory extends BaseTokenTestCase {
+public class TestPhoneticFilterFactory extends BaseTokenStreamTestCase {
   
   private static final int REPEATS = 100000;
 
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestPorterStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestPorterStemFilterFactory.java
index d3eebaa..43ae2ec 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestPorterStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestPorterStemFilterFactory.java
@@ -20,6 +20,7 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -27,7 +28,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the Porter stem filter factory is working.
  */
-public class TestPorterStemFilterFactory extends BaseTokenTestCase {
+public class TestPorterStemFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually stems text.
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestPortugueseLightStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestPortugueseLightStemFilterFactory.java
index d66c5da..a2e3c37 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestPortugueseLightStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestPortugueseLightStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Portuguese Light stem factory is working.
  */
-public class TestPortugueseLightStemFilterFactory extends BaseTokenTestCase {
+public class TestPortugueseLightStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("evidentemente");
     PortugueseLightStemFilterFactory factory = new PortugueseLightStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestPortugueseMinimalStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestPortugueseMinimalStemFilterFactory.java
index 9b1249b..209b657 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestPortugueseMinimalStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestPortugueseMinimalStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Portuguese Minimal stem factory is working.
  */
-public class TestPortugueseMinimalStemFilterFactory extends BaseTokenTestCase {
+public class TestPortugueseMinimalStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("quest√µes");
     PortugueseMinimalStemFilterFactory factory = new PortugueseMinimalStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestPortugueseStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestPortugueseStemFilterFactory.java
index 900c00d..7b42175 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestPortugueseStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestPortugueseStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Portuguese stem factory is working.
  */
-public class TestPortugueseStemFilterFactory extends BaseTokenTestCase {
+public class TestPortugueseStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("maluquice");
     PortugueseStemFilterFactory factory = new PortugueseStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilterFactory.java
index 7795fd6..58e4dda 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilterFactory.java
@@ -17,6 +17,7 @@
 
 package org.apache.solr.analysis;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
@@ -27,7 +28,7 @@ import java.util.Iterator;
 import java.util.Arrays;
 
 /** Simple tests to ensure this factory is working */
-public class TestRemoveDuplicatesTokenFilterFactory extends BaseTokenTestCase {
+public class TestRemoveDuplicatesTokenFilterFactory extends BaseTokenStreamTestCase {
 
   public static Token tok(int pos, String t, int start, int end) {
     Token tok = new Token(t,start,end);
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestReverseStringFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestReverseStringFilterFactory.java
index b8d8d98..9064a5c 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestReverseStringFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestReverseStringFilterFactory.java
@@ -19,7 +19,10 @@ package org.apache.solr.analysis;
 
 import java.io.Reader;
 import java.io.StringReader;
+import java.util.Collections;
+import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -27,7 +30,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the Reverse string filter factory is working.
  */
-public class TestReverseStringFilterFactory extends BaseTokenTestCase {
+public class TestReverseStringFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually reverses text.
    */
@@ -35,8 +38,9 @@ public class TestReverseStringFilterFactory extends BaseTokenTestCase {
     Reader reader = new StringReader("simple test");
     Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
     ReverseStringFilterFactory factory = new ReverseStringFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     TokenStream stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] { "elpmis", "tset" });
   }
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java
index e58ee30..195a203 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestReversedWildcardFilterFactory.java
@@ -40,7 +40,7 @@ import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
-import static org.apache.solr.analysis.BaseTokenTestCase.*;
+import static org.apache.lucene.analysis.BaseTokenStreamTestCase.*;
 
 public class TestReversedWildcardFilterFactory extends SolrTestCaseJ4 {
   Map<String,String> args = new HashMap<String, String>();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestRussianFilters.java b/solr/core/src/test/org/apache/solr/analysis/TestRussianFilters.java
index 2d4cb5f..20be1e5 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestRussianFilters.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestRussianFilters.java
@@ -19,21 +19,25 @@ package org.apache.solr.analysis;
 
 import java.io.Reader;
 import java.io.StringReader;
+import java.util.Collections;
+import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Tokenizer;
 
 /**
  * Simple tests to ensure the Russian filter factories are working.
  */
-public class TestRussianFilters extends BaseTokenTestCase {
+public class TestRussianFilters extends BaseTokenStreamTestCase {
   /**
    * Test RussianLetterTokenizerFactory
    */
   public void testTokenizer() throws Exception {
     Reader reader = new StringReader("?–º–µ??–µ ? ?–µ–º –æ ?–∏–ª–µ ?–ª–µ–∫??–æ–º–∞–≥–Ω–∏?–Ω–æ–π 100");
     RussianLetterTokenizerFactory factory = new RussianLetterTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, new String[] {"?–º–µ??–µ", "?", "?–µ–º", "–æ",
         "?–∏–ª–µ", "?–ª–µ–∫??–æ–º–∞–≥–Ω–∏?–Ω–æ–π", "100"});
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestRussianLightStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestRussianLightStemFilterFactory.java
index 11cfdaf..15ead2b 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestRussianLightStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestRussianLightStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Russian light stem factory is working.
  */
-public class TestRussianLightStemFilterFactory extends BaseTokenTestCase {
+public class TestRussianLightStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("–∂??–Ω–∞–ª?");
     RussianLightStemFilterFactory factory = new RussianLightStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestShingleFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestShingleFilterFactory.java
index 25c9873..d5712c9 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestShingleFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestShingleFilterFactory.java
@@ -22,13 +22,14 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Shingle filter factory works.
  */
-public class TestShingleFilterFactory extends BaseTokenTestCase { 
+public class TestShingleFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Test the defaults
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestSpanishLightStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestSpanishLightStemFilterFactory.java
index c4ef814..23c8a55 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestSpanishLightStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestSpanishLightStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Spanish Light stem factory is working.
  */
-public class TestSpanishLightStemFilterFactory extends BaseTokenTestCase {
+public class TestSpanishLightStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("sociedades");
     SpanishLightStemFilterFactory factory = new SpanishLightStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestStandardFactories.java b/solr/core/src/test/org/apache/solr/analysis/TestStandardFactories.java
index 0ea79ee..ef7605e 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestStandardFactories.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestStandardFactories.java
@@ -19,9 +19,11 @@ package org.apache.solr.analysis;
 
 import java.io.Reader;
 import java.io.StringReader;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -29,15 +31,16 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the standard lucene factories are working.
  */
-public class TestStandardFactories extends BaseTokenTestCase {
+public class TestStandardFactories extends BaseTokenStreamTestCase {
   /**
    * Test StandardTokenizerFactory
    */
   public void testStandardTokenizer() throws Exception {
     Reader reader = new StringReader("Wha\u0301t's this thing do?");
     StandardTokenizerFactory factory = new StandardTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"Wha\u0301t's", "this", "thing", "do" });
@@ -54,7 +57,7 @@ public class TestStandardFactories extends BaseTokenTestCase {
     Map<String,String> args = new HashMap<String,String>();
     args.put("maxTokenLength", "1000");
     StandardTokenizerFactory factory = new StandardTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
@@ -67,8 +70,9 @@ public class TestStandardFactories extends BaseTokenTestCase {
   public void testClassicTokenizer() throws Exception {
     Reader reader = new StringReader("What's this thing do?");
     ClassicTokenizerFactory factory = new ClassicTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"What's", "this", "thing", "do" });
@@ -85,7 +89,7 @@ public class TestStandardFactories extends BaseTokenTestCase {
     Map<String,String> args = new HashMap<String,String>();
     args.put("maxTokenLength", "1000");
     ClassicTokenizerFactory factory = new ClassicTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
@@ -98,11 +102,12 @@ public class TestStandardFactories extends BaseTokenTestCase {
   public void testStandardFilter() throws Exception {
     Reader reader = new StringReader("What's this thing do?");
     ClassicTokenizerFactory factory = new ClassicTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     ClassicFilterFactory filterFactory = new ClassicFilterFactory();
-    filterFactory.setLuceneMatchVersion(DEFAULT_VERSION);
-    filterFactory.init(EMPTY_PARAMS);
+    filterFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    filterFactory.init(args);
     Tokenizer tokenizer = factory.create(reader);
     TokenStream stream = filterFactory.create(tokenizer);
     assertTokenStreamContents(stream, 
@@ -115,8 +120,9 @@ public class TestStandardFactories extends BaseTokenTestCase {
   public void testKeywordTokenizer() throws Exception {
     Reader reader = new StringReader("What's this thing do?");
     KeywordTokenizerFactory factory = new KeywordTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"What's this thing do?"});
@@ -128,8 +134,9 @@ public class TestStandardFactories extends BaseTokenTestCase {
   public void testWhitespaceTokenizer() throws Exception {
     Reader reader = new StringReader("What's this thing do?");
     WhitespaceTokenizerFactory factory = new WhitespaceTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"What's", "this", "thing", "do?"});
@@ -141,8 +148,9 @@ public class TestStandardFactories extends BaseTokenTestCase {
   public void testLetterTokenizer() throws Exception {
     Reader reader = new StringReader("What's this thing do?");
     LetterTokenizerFactory factory = new LetterTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"What", "s", "this", "thing", "do"});
@@ -154,8 +162,9 @@ public class TestStandardFactories extends BaseTokenTestCase {
   public void testLowerCaseTokenizer() throws Exception {
     Reader reader = new StringReader("What's this thing do?");
     LowerCaseTokenizerFactory factory = new LowerCaseTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"what", "s", "this", "thing", "do"});
@@ -168,8 +177,9 @@ public class TestStandardFactories extends BaseTokenTestCase {
     Reader reader = new StringReader("?esk√°");
     Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
     ASCIIFoldingFilterFactory factory = new ASCIIFoldingFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     TokenStream stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] { "Ceska" });
   }
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestStemmerOverrideFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestStemmerOverrideFilterFactory.java
index 3ac17b0..d36d1af 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestStemmerOverrideFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestStemmerOverrideFilterFactory.java
@@ -23,6 +23,7 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.en.PorterStemFilter;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
@@ -33,7 +34,7 @@ import org.apache.solr.core.SolrResourceLoader;
 /**
  * Simple tests to ensure the stemmer override filter factory is working.
  */
-public class TestStemmerOverrideFilterFactory extends BaseTokenTestCase {
+public class TestStemmerOverrideFilterFactory extends BaseTokenStreamTestCase {
   public void testKeywords() throws IOException {
     // our stemdict stems dogs to 'cat'
     Reader reader = new StringReader("testing dogs");
@@ -42,7 +43,7 @@ public class TestStemmerOverrideFilterFactory extends BaseTokenTestCase {
     Map<String,String> args = new HashMap<String,String>();
     ResourceLoader loader = new SolrResourceLoader(null, null);
     args.put("dictionary", "stemdict.txt");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     
@@ -58,7 +59,7 @@ public class TestStemmerOverrideFilterFactory extends BaseTokenTestCase {
     ResourceLoader loader = new SolrResourceLoader(null, null);
     args.put("dictionary", "stemdict.txt");
     args.put("ignoreCase", "true");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestStopFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestStopFilterFactory.java
index a98a439..27bd10c 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestStopFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestStopFilterFactory.java
@@ -17,6 +17,7 @@ package org.apache.solr.analysis;
  */
 
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.analysis.util.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
@@ -28,7 +29,7 @@ import java.util.HashMap;
  *
  *
  **/
-public class TestStopFilterFactory extends BaseTokenTestCase {
+public class TestStopFilterFactory extends BaseTokenStreamTestCase {
 
   public void testInform() throws Exception {
     ResourceLoader loader = new SolrResourceLoader(null, null);
@@ -37,7 +38,7 @@ public class TestStopFilterFactory extends BaseTokenTestCase {
     Map<String, String> args = new HashMap<String, String>();
     args.put("words", "stop-1.txt");
     args.put("ignoreCase", "true");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     CharArraySet words = factory.getStopWords();
@@ -47,7 +48,7 @@ public class TestStopFilterFactory extends BaseTokenTestCase {
 
     factory = new StopFilterFactory();
     args.put("words", "stop-1.txt, stop-2.txt");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     words = factory.getStopWords();
@@ -56,7 +57,7 @@ public class TestStopFilterFactory extends BaseTokenTestCase {
     assertTrue(factory.isIgnoreCase() + " does not equal: " + true, factory.isIgnoreCase() == true);
 
     factory = new StopFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     args.put("words", "stop-snowball.txt");
     args.put("format", "snowball");
     factory.init(args);
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestSwedishLightStemFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestSwedishLightStemFilterFactory.java
index 5882369..873b621 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestSwedishLightStemFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestSwedishLightStemFilterFactory.java
@@ -20,13 +20,14 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure the Swedish Light stem factory is working.
  */
-public class TestSwedishLightStemFilterFactory extends BaseTokenTestCase {
+public class TestSwedishLightStemFilterFactory extends BaseTokenStreamTestCase {
   public void testStemming() throws Exception {
     Reader reader = new StringReader("√§pplen √§pple");
     SwedishLightStemFilterFactory factory = new SwedishLightStemFilterFactory();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestSynonymFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestSynonymFilterFactory.java
index da5f035..7b24b67 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestSynonymFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestSynonymFilterFactory.java
@@ -26,6 +26,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.synonym.SynonymFilter;
@@ -33,13 +34,13 @@ import org.apache.lucene.util.Version;
 import org.apache.lucene.analysis.util.ResourceLoader;
 import org.apache.solr.core.SolrResourceLoader;
 
-public class TestSynonymFilterFactory extends BaseTokenTestCase {
+public class TestSynonymFilterFactory extends BaseTokenStreamTestCase {
   /** test that we can parse and use the solr syn file */
   public void testSynonyms() throws Exception {
     SynonymFilterFactory factory = new SynonymFilterFactory();
     Map<String,String> args = new HashMap<String,String>();
     args.put("synonyms", "synonyms.txt");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(new SolrResourceLoader(null, null));
     TokenStream ts = factory.create(new MockTokenizer(new StringReader("GB"), MockTokenizer.WHITESPACE, false));
@@ -90,7 +91,7 @@ public class TestSynonymFilterFactory extends BaseTokenTestCase {
     SynonymFilterFactory factory = new SynonymFilterFactory();
     Map<String,String> args = new HashMap<String,String>();
     args.put("synonyms", "synonyms.txt");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(new StringMockSolrResourceLoader("")); // empty file!
     TokenStream ts = factory.create(new MockTokenizer(new StringReader("GB"), MockTokenizer.WHITESPACE, false));
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestThaiWordFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestThaiWordFilterFactory.java
index d632bf9..532fd51 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestThaiWordFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestThaiWordFilterFactory.java
@@ -19,7 +19,10 @@ package org.apache.solr.analysis;
 
 import java.io.Reader;
 import java.io.StringReader;
+import java.util.Collections;
+import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -28,7 +31,7 @@ import org.apache.lucene.analysis.th.ThaiWordFilter;
 /**
  * Simple tests to ensure the Thai word filter factory is working.
  */
-public class TestThaiWordFilterFactory extends BaseTokenTestCase {
+public class TestThaiWordFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually decomposes text.
    */
@@ -37,8 +40,9 @@ public class TestThaiWordFilterFactory extends BaseTokenTestCase {
     Reader reader = new StringReader("‡∏?∏≤‡∏£‡?‡∏µ‡?‡π??‡π??‡π?∏≠‡∏??‡∏??‡∏?∏ß‡π?∏≤‡∏?∏≤‡∏??‡∏?");
     Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
     ThaiWordFilterFactory factory = new ThaiWordFilterFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     TokenStream stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] {"‡∏?∏≤‡∏?", "‡∏?∏µ‡π?", "‡π??‡π?",
         "‡∏??‡∏??", "‡π?∏™‡∏??", "‡∏ß‡?‡∏?", "‡∏?∏≤‡∏?", "‡∏?∏µ"});
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestTrimFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestTrimFilterFactory.java
index 92390d1..a2f3183 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestTrimFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestTrimFilterFactory.java
@@ -21,13 +21,14 @@ import java.io.StringReader;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 
 /**
  * Simple tests to ensure this factory is working
  */
-public class TestTrimFilterFactory extends BaseTokenTestCase {
+public class TestTrimFilterFactory extends BaseTokenStreamTestCase {
   public void testTrimming() throws Exception {
     TrimFilterFactory factory = new TrimFilterFactory();
     Map<String,String> args = new HashMap<String,String>();
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestTurkishLowerCaseFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestTurkishLowerCaseFilterFactory.java
index d6899cf..6ea51de 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestTurkishLowerCaseFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestTurkishLowerCaseFilterFactory.java
@@ -20,6 +20,7 @@ package org.apache.solr.analysis;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
@@ -27,7 +28,7 @@ import org.apache.lucene.analysis.Tokenizer;
 /**
  * Simple tests to ensure the Turkish lowercase filter factory is working.
  */
-public class TestTurkishLowerCaseFilterFactory extends BaseTokenTestCase {
+public class TestTurkishLowerCaseFilterFactory extends BaseTokenStreamTestCase {
   /**
    * Ensure the filter actually lowercases text.
    */
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestTypeTokenFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestTypeTokenFilterFactory.java
index a1bc5f7..cde4827 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestTypeTokenFilterFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestTypeTokenFilterFactory.java
@@ -17,6 +17,7 @@ package org.apache.solr.analysis;
  */
 
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.NumericTokenStream;
 import org.apache.lucene.analysis.util.InitializationException;
 import org.apache.lucene.analysis.util.ResourceLoader;
@@ -30,7 +31,7 @@ import java.util.Set;
 /**
  * Testcase for {@link TypeTokenFilterFactory}
  */
-public class TestTypeTokenFilterFactory extends BaseTokenTestCase {
+public class TestTypeTokenFilterFactory extends BaseTokenStreamTestCase {
 
   @Test
   public void testInform() throws Exception {
@@ -39,7 +40,7 @@ public class TestTypeTokenFilterFactory extends BaseTokenTestCase {
     Map<String, String> args = new HashMap<String, String>();
     args.put("types", "stoptypes-1.txt");
     args.put("enablePositionIncrements", "true");
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     factory.inform(loader);
     Set<String> types = factory.getStopTypes();
@@ -65,7 +66,7 @@ public class TestTypeTokenFilterFactory extends BaseTokenTestCase {
     Map<String, String> args = new HashMap<String, String>();
     args.put("types", "stoptypes-1.txt, stoptypes-2.txt");
     args.put("enablePositionIncrements", "false");
-    typeTokenFilterFactory.setLuceneMatchVersion(DEFAULT_VERSION);
+    typeTokenFilterFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     typeTokenFilterFactory.init(args);
     NumericTokenStream input = new NumericTokenStream();
     input.setIntValue(123);
@@ -79,7 +80,7 @@ public class TestTypeTokenFilterFactory extends BaseTokenTestCase {
       args.put("types", "stoptypes-1.txt, stoptypes-2.txt");
       args.put("enablePositionIncrements", "false");
       args.put("useWhitelist","true");
-      typeTokenFilterFactory.setLuceneMatchVersion(DEFAULT_VERSION);
+      typeTokenFilterFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
       typeTokenFilterFactory.init(args);
       NumericTokenStream input = new NumericTokenStream();
       input.setIntValue(123);
@@ -92,7 +93,7 @@ public class TestTypeTokenFilterFactory extends BaseTokenTestCase {
       TypeTokenFilterFactory typeTokenFilterFactory = new TypeTokenFilterFactory();
       Map<String, String> args = new HashMap<String, String>();
       args.put("enablePositionIncrements", "false");
-      typeTokenFilterFactory.setLuceneMatchVersion(DEFAULT_VERSION);
+      typeTokenFilterFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
       typeTokenFilterFactory.init(args);
       typeTokenFilterFactory.inform(new SolrResourceLoader(null, null));
       fail("not supplying 'types' parameter should cause an InitializationException");
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory.java
index d5bc41a..bd59fb2 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory.java
@@ -23,6 +23,7 @@ import java.util.Collections;
 import java.util.HashMap;
 import java.util.Map;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.util.Version;
 
@@ -30,13 +31,14 @@ import org.apache.lucene.util.Version;
  * A few tests based on org.apache.lucene.analysis.TestUAX29URLEmailTokenizer
  */
 
-public class TestUAX29URLEmailTokenizerFactory extends BaseTokenTestCase {
+public class TestUAX29URLEmailTokenizerFactory extends BaseTokenStreamTestCase {
 
   public void testUAX29URLEmailTokenizer() throws Exception {
     Reader reader = new StringReader("Wha\u0301t's this thing do?");
     UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"Wha\u0301t's", "this", "thing", "do" });
@@ -45,8 +47,9 @@ public class TestUAX29URLEmailTokenizerFactory extends BaseTokenTestCase {
   public void testArabic() throws Exception {
     Reader reader = new StringReader("ÿß????? ÿß??ÿ´ÿßÿ¶?? ÿß?ÿ£?? ÿπ? ????ÿ®?ÿØ?ÿß ?ÿ≥?? \"ÿß?ÿ≠???ÿ© ÿ®ÿß?ÿ£ÿ±?ÿß?: ?ÿµÿ© ????ÿ®?ÿØ?ÿß\" (ÿ®ÿß?ÿ•?ÿ¨??ÿ≤?ÿ©: Truth in Numbers: The Wikipedia Story)? ÿ≥?ÿ™? ÿ•ÿ∑?ÿß?? ?? 2008.");
     UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"ÿß?????", "ÿß??ÿ´ÿßÿ¶??", "ÿß?ÿ£??", "ÿπ?", "????ÿ®?ÿØ?ÿß", "?ÿ≥??", "ÿß?ÿ≠???ÿ©", "ÿ®ÿß?ÿ£ÿ±?ÿß?", "?ÿµÿ©", "????ÿ®?ÿØ?ÿß",
@@ -56,8 +59,9 @@ public class TestUAX29URLEmailTokenizerFactory extends BaseTokenTestCase {
   public void testChinese() throws Exception {
     Reader reader = new StringReader("???‰∏??‰∫∫„?? Ôº??Ôº?? Ôº¥Ô?ÔΩ??ÔΩ? ");
     UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"??", "??", "‰∏?", "??", "‰∫?", "Ôº??Ôº??", "Ôº¥Ô?ÔΩ??ÔΩ?"});
@@ -66,8 +70,9 @@ public class TestUAX29URLEmailTokenizerFactory extends BaseTokenTestCase {
   public void testKorean() throws Exception {
     Reader reader = new StringReader("???????? ????????");
     UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"????????", "????????"});
@@ -76,8 +81,9 @@ public class TestUAX29URLEmailTokenizerFactory extends BaseTokenTestCase {
   public void testHyphen() throws Exception {
     Reader reader = new StringReader("some-dashed-phrase");
     UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"some", "dashed", "phrase"});
@@ -101,8 +107,9 @@ public class TestUAX29URLEmailTokenizerFactory extends BaseTokenTestCase {
         + "http://[a42:a7b6::]/qSmxSUU4z/%52qVl4\n";
     Reader reader = new StringReader(textWithURLs);
     UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] { 
@@ -142,8 +149,9 @@ public class TestUAX29URLEmailTokenizerFactory extends BaseTokenTestCase {
          + "lv'p@tqk.vj5s0tgl.0dlu7su3iyiaz.dqso.494.3hb76.XN--MGBAAM7A8H\n";
     Reader reader = new StringReader(textWithEmails);
     UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] { 
@@ -176,7 +184,7 @@ public class TestUAX29URLEmailTokenizerFactory extends BaseTokenTestCase {
     Map<String,String> args = new HashMap<String,String>();
     args.put("maxTokenLength", "1000");
     UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
@@ -188,8 +196,9 @@ public class TestUAX29URLEmailTokenizerFactory extends BaseTokenTestCase {
   public void testMatchVersion() throws Exception {
     Reader reader = new StringReader("???");
     UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();
-    factory.setLuceneMatchVersion(DEFAULT_VERSION);
-    factory.init(EMPTY_PARAMS);
+    factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
+    Map<String, String> args = Collections.emptyMap();
+    factory.init(args);
     Tokenizer stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"???"});
@@ -197,7 +206,7 @@ public class TestUAX29URLEmailTokenizerFactory extends BaseTokenTestCase {
     reader = new StringReader("???");
     factory = new UAX29URLEmailTokenizerFactory();
     factory.setLuceneMatchVersion(Version.LUCENE_31);
-    factory.init(EMPTY_PARAMS);
+    factory.init(args);
     stream = factory.create(reader);
     assertTokenStreamContents(stream, 
         new String[] {"??"}); // old broken behavior
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestWikipediaTokenizerFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestWikipediaTokenizerFactory.java
index 57fa7b6..a2248ed 100644
--- a/solr/core/src/test/org/apache/solr/analysis/TestWikipediaTokenizerFactory.java
+++ b/solr/core/src/test/org/apache/solr/analysis/TestWikipediaTokenizerFactory.java
@@ -4,6 +4,7 @@ import java.io.IOException;
 import java.io.Reader;
 import java.io.StringReader;
 
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.wikipedia.WikipediaTokenizer;
 
@@ -27,7 +28,7 @@ import org.apache.lucene.analysis.wikipedia.WikipediaTokenizer;
 /**
  * Simple tests to ensure the wikipedia tokenizer is working.
  */
-public class TestWikipediaTokenizerFactory extends BaseTokenTestCase {
+public class TestWikipediaTokenizerFactory extends BaseTokenStreamTestCase {
   public void testTokenizer() throws IOException {
     Reader reader = new StringReader("This is a [[Category:foo]]");
     WikipediaTokenizerFactory factory = new WikipediaTokenizerFactory();
diff --git a/solr/test-framework/src/java/org/apache/solr/analysis/BaseTokenTestCase.java b/solr/test-framework/src/java/org/apache/solr/analysis/BaseTokenTestCase.java
deleted file mode 100644
index 221cf62..0000000
--- a/solr/test-framework/src/java/org/apache/solr/analysis/BaseTokenTestCase.java
+++ /dev/null
@@ -1,45 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.analysis;
-
-import java.util.Collections;
-import java.util.Map;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.util.Version;
-import org.apache.solr.core.Config;
-
-/**
- * General token testing helper functions
- */
-public abstract class BaseTokenTestCase extends BaseTokenStreamTestCase{
-  
-  protected static final Map<String, String> EMPTY_PARAMS = Collections.emptyMap();
-
-  /** The default test version for easy testing */
-  public static final Version DEFAULT_VERSION;
-  
-  static {
-    String rawVersion = System.getProperty("tests.luceneMatchVersion", "LUCENE_CURRENT");
-    try {
-      DEFAULT_VERSION = Version.parseLeniently(rawVersion);
-    } catch (IllegalArgumentException iae) {
-      throw new RuntimeException("Test Lucene Match Version [" + rawVersion + "] is invalid", iae);
-    }
-  }
-}

