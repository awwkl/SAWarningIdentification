GitDiffStart: 839219831d32bea1560656264935323e734093d9 | Sun Feb 5 15:44:49 2012 +0000
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosFormat.java
index c633e35..428a4f5 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosFormat.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosFormat.java
@@ -32,7 +32,7 @@ import org.apache.lucene.index.SegmentInfo;
  * @lucene.experimental
  */
 @Deprecated
-public class Lucene3xFieldInfosFormat extends FieldInfosFormat {
+class Lucene3xFieldInfosFormat extends FieldInfosFormat {
   private final FieldInfosReader reader = new Lucene3xFieldInfosReader();
   
   @Override
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosReader.java
index d59e5f4..8cacae5 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosReader.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFieldInfosReader.java
@@ -37,7 +37,7 @@ import org.apache.lucene.store.IndexInput;
  * @deprecated
  */
 @Deprecated
-public class Lucene3xFieldInfosReader extends FieldInfosReader {
+class Lucene3xFieldInfosReader extends FieldInfosReader {
   /** Extension of field infos */
   static final String FIELD_INFOS_EXTENSION = "fnm";
   
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFields.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFields.java
index 7f2b3ba..a3edb7b 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFields.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFields.java
@@ -49,7 +49,7 @@ import org.apache.lucene.util.UnicodeUtil;
  * @deprecated (4.0)
  */
 @Deprecated
-public class Lucene3xFields extends FieldsProducer {
+class Lucene3xFields extends FieldsProducer {
   
   private static final boolean DEBUG_SURROGATES = false;
 
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsFormat.java
index 1a7fef6..df0d527 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsFormat.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xNormsFormat.java
@@ -34,7 +34,7 @@ import org.apache.lucene.index.SegmentReadState;
  * @lucene.experimental
  */
 @Deprecated
-public class Lucene3xNormsFormat extends NormsFormat {
+class Lucene3xNormsFormat extends NormsFormat {
 
 
   @Override
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xPostingsFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xPostingsFormat.java
index 2ce5cd4..75f9600 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xPostingsFormat.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xPostingsFormat.java
@@ -36,7 +36,7 @@ import org.apache.lucene.index.SegmentReadState;
  * @lucene.experimental
  */
 @Deprecated
-public class Lucene3xPostingsFormat extends PostingsFormat {
+class Lucene3xPostingsFormat extends PostingsFormat {
 
   /** Extension of terms file */
   public static final String TERMS_EXTENSION = "tis";
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosFormat.java
index 772a1f0..61d1946 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosFormat.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosFormat.java
@@ -28,7 +28,7 @@ import org.apache.lucene.codecs.SegmentInfosWriter;
  * @lucene.experimental
  */
 @Deprecated
-public class Lucene3xSegmentInfosFormat extends SegmentInfosFormat {
+class Lucene3xSegmentInfosFormat extends SegmentInfosFormat {
   private final SegmentInfosReader reader = new Lucene3xSegmentInfosReader();
   
   @Override
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosReader.java
index 624dcdd..b7054ba 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosReader.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xSegmentInfosReader.java
@@ -23,8 +23,6 @@ import java.util.Map;
 
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.SegmentInfosReader;
-// TODO we need a 3x version of stored fields reader
-import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsReader;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexFormatTooOldException;
 import org.apache.lucene.index.SegmentInfo;
@@ -40,7 +38,7 @@ import org.apache.lucene.store.IOContext;
  * @deprecated
  */
 @Deprecated
-public class Lucene3xSegmentInfosReader extends SegmentInfosReader {
+class Lucene3xSegmentInfosReader extends SegmentInfosReader {
 
   @Override
   public void read(Directory directory, String segmentsFileName, ChecksumIndexInput input, SegmentInfos infos, IOContext context) throws IOException { 
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsFormat.java
index dae458b..20a005c 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsFormat.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsFormat.java
@@ -30,7 +30,7 @@ import org.apache.lucene.store.IOContext;
 
 /** @deprecated */
 @Deprecated
-public class Lucene3xStoredFieldsFormat extends StoredFieldsFormat {
+class Lucene3xStoredFieldsFormat extends StoredFieldsFormat {
 
   @Override
   public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si,
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsReader.java
index 9d3d33e..a0774b1 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsReader.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xStoredFieldsReader.java
@@ -46,7 +46,7 @@ import java.util.Set;
  * @deprecated
  */
 @Deprecated
-public final class Lucene3xStoredFieldsReader extends StoredFieldsReader implements Cloneable, Closeable {
+final class Lucene3xStoredFieldsReader extends StoredFieldsReader implements Cloneable, Closeable {
   private final static int FORMAT_SIZE = 4;
 
   /** Extension of stored fields file */
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsFormat.java
index c0fe31f..58a5a7c 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsFormat.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsFormat.java
@@ -35,7 +35,7 @@ import org.apache.lucene.store.IOContext;
  * @lucene.experimental
  */
 @Deprecated
-public class Lucene3xTermVectorsFormat extends TermVectorsFormat {
+class Lucene3xTermVectorsFormat extends TermVectorsFormat {
 
   @Override
   public TermVectorsReader vectorsReader(Directory directory,SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsReader.java
index b60d36c..62406fc 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsReader.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xTermVectorsReader.java
@@ -48,7 +48,7 @@ import org.apache.lucene.util.IOUtils;
 
 /** @deprecated */
 @Deprecated
-public class Lucene3xTermVectorsReader extends TermVectorsReader {
+class Lucene3xTermVectorsReader extends TermVectorsReader {
 
   // NOTE: if you make a new format, it must be larger than
   // the current format
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermDocs.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermDocs.java
index f5020a2..15ea832 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermDocs.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermDocs.java
@@ -30,7 +30,7 @@ import org.apache.lucene.util.Bits;
 /** @deprecated (4.0)
  *  @lucene.experimental */
 @Deprecated
-public class SegmentTermDocs {
+class SegmentTermDocs {
   //protected SegmentReader parent;
   private final FieldInfos fieldInfos;
   private final TermInfosReader tis;
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermEnum.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermEnum.java
index 281eed1..e8636fd 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermEnum.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermEnum.java
@@ -31,7 +31,7 @@ import org.apache.lucene.index.IndexFormatTooNewException;
  * @lucene.experimental */
 
 @Deprecated
-public final class SegmentTermEnum implements Cloneable {
+final class SegmentTermEnum implements Cloneable {
   private IndexInput input;
   FieldInfos fieldInfos;
   long size;
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermPositions.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermPositions.java
index bdd45bb..bdb0961 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermPositions.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermPositions.java
@@ -29,7 +29,7 @@ import org.apache.lucene.store.IndexInput;
  * @deprecated (4.0)
  */
 @Deprecated
-public final class SegmentTermPositions
+final class SegmentTermPositions
 extends SegmentTermDocs  {
   private IndexInput proxStream;
   private IndexInput proxStreamOrig;
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfo.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfo.java
index 9b2efe5..ad715b4 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfo.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfo.java
@@ -23,7 +23,7 @@ package org.apache.lucene.codecs.lucene3x;
  * indexing. */
 
 @Deprecated
-public class TermInfo {
+class TermInfo {
   /** The number of documents which contain the term. */
   public int docFreq = 0;
 
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReader.java
index c1d8c2c..e862367 100644
--- a/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReader.java
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReader.java
@@ -38,7 +38,7 @@ import org.apache.lucene.util.DoubleBarrelLRUCache;
  * @lucene.experimental
  */
 @Deprecated
-public final class TermInfosReader {
+final class TermInfosReader {
   private final Directory directory;
   private final String segment;
   private final FieldInfos fieldInfos;
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWCodec.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWCodec.java
new file mode 100644
index 0000000..e76756a
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWCodec.java
@@ -0,0 +1,126 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.StringHelper;
+
+/**
+ * Writes 3.x-like indexes (not perfect emulation yet) for testing only!
+ * @lucene.experimental
+ */
+public class PreFlexRWCodec extends Lucene3xCodec {
+  private final PostingsFormat postings = new PreFlexRWPostingsFormat();
+  private final Lucene3xNormsFormat norms = new PreFlexRWNormsFormat();
+  private final FieldInfosFormat fieldInfos = new PreFlexRWFieldInfosFormat();
+  private final TermVectorsFormat termVectors = new PreFlexRWTermVectorsFormat();
+  private final SegmentInfosFormat segmentInfos = new PreFlexRWSegmentInfosFormat();
+  private final StoredFieldsFormat storedFields = new PreFlexRWStoredFieldsFormat();
+  // TODO: this should really be a different impl
+  private final LiveDocsFormat liveDocs = new Lucene40LiveDocsFormat();
+  
+  @Override
+  public PostingsFormat postingsFormat() {
+    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
+      return postings;
+    } else {
+      return super.postingsFormat();
+    }
+  }
+
+  @Override
+  public Lucene3xNormsFormat normsFormat() {
+    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
+      return norms;
+    } else {
+      return super.normsFormat();
+    }
+  }
+
+  @Override
+  public SegmentInfosFormat segmentInfosFormat() {
+    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
+      return segmentInfos ;
+    } else {
+      return super.segmentInfosFormat();
+    }
+  }
+
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
+      return fieldInfos;
+    } else {
+      return super.fieldInfosFormat();
+    }
+  }
+
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
+      return termVectors;
+    } else {
+      return super.termVectorsFormat();
+    }
+  }
+
+  @Override
+  public LiveDocsFormat liveDocsFormat() {
+    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
+      return liveDocs;
+    } else {
+      return super.liveDocsFormat();
+    }
+  }
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
+      return storedFields;
+    } else {
+      return super.storedFieldsFormat();
+    }
+  }
+
+  @Override
+  public void files(SegmentInfo info, Set<String> files) throws IOException {
+    if (info.getUseCompoundFile() && LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
+      // because we don't fully emulate 3.x codec, PreFlexRW actually writes 4.x format CFS files.
+      // so we must check segment version here to see if its a "real" 3.x segment or a "fake"
+      // one that we wrote with a 4.x-format CFS+CFE, in this case we must add the .CFE
+      String version = info.getVersion();
+      if (version != null && StringHelper.getVersionComparator().compare("4.0", version) <= 0) {
+        files.add(IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
+      }
+    }
+    
+    super.files(info, files);
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosFormat.java
new file mode 100644
index 0000000..9f34c33
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosFormat.java
@@ -0,0 +1,40 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.codecs.FieldInfosWriter;
+
+/**
+ * 
+ * @lucene.internal
+ * @lucene.experimental
+ */
+class PreFlexRWFieldInfosFormat extends Lucene3xFieldInfosFormat {
+
+  @Override
+  public FieldInfosReader getFieldInfosReader() throws IOException {
+    return new PreFlexRWFieldInfosReader();
+  }
+
+  @Override
+  public FieldInfosWriter getFieldInfosWriter() throws IOException {
+    return new PreFlexRWFieldInfosWriter();
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosReader.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosReader.java
new file mode 100644
index 0000000..ff149ac
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosReader.java
@@ -0,0 +1,117 @@
+package org.apache.lucene.codecs.lucene3x;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexFormatTooNewException;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * @lucene.internal
+ * @lucene.experimental
+ */
+class PreFlexRWFieldInfosReader extends FieldInfosReader {
+  static final int FORMAT_MINIMUM = PreFlexRWFieldInfosWriter.FORMAT_START;
+
+  @Override
+  public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", PreFlexRWFieldInfosWriter.FIELD_INFOS_EXTENSION);
+    IndexInput input = directory.openInput(fileName, iocontext);
+
+    boolean hasVectors = false;
+    boolean hasFreq = false;
+    boolean hasProx = false;
+    
+    try {
+      final int format = input.readVInt();
+
+      if (format > FORMAT_MINIMUM) {
+        throw new IndexFormatTooOldException(input, format, FORMAT_MINIMUM, PreFlexRWFieldInfosWriter.FORMAT_CURRENT);
+      }
+      if (format < PreFlexRWFieldInfosWriter.FORMAT_CURRENT && format != PreFlexRWFieldInfosWriter.FORMAT_PREFLEX_RW) {
+        throw new IndexFormatTooNewException(input, format, FORMAT_MINIMUM, PreFlexRWFieldInfosWriter.FORMAT_CURRENT);
+      }
+
+      final int size = input.readVInt(); //read in the size
+      FieldInfo infos[] = new FieldInfo[size];
+
+      for (int i = 0; i < size; i++) {
+        String name = input.readString();
+        final int fieldNumber = format == PreFlexRWFieldInfosWriter.FORMAT_PREFLEX_RW ? input.readInt() : i;
+        byte bits = input.readByte();
+        boolean isIndexed = (bits & PreFlexRWFieldInfosWriter.IS_INDEXED) != 0;
+        boolean storeTermVector = (bits & PreFlexRWFieldInfosWriter.STORE_TERMVECTOR) != 0;
+        boolean omitNorms = (bits & PreFlexRWFieldInfosWriter.OMIT_NORMS) != 0;
+        boolean storePayloads = (bits & PreFlexRWFieldInfosWriter.STORE_PAYLOADS) != 0;
+        final IndexOptions indexOptions;
+        if ((bits & PreFlexRWFieldInfosWriter.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
+          indexOptions = IndexOptions.DOCS_ONLY;
+        } else if ((bits & PreFlexRWFieldInfosWriter.OMIT_POSITIONS) != 0) {
+          if (format <= PreFlexRWFieldInfosWriter.FORMAT_OMIT_POSITIONS) {
+            indexOptions = IndexOptions.DOCS_AND_FREQS;
+          } else {
+            throw new CorruptIndexException("Corrupt fieldinfos, OMIT_POSITIONS set but format=" + format + " (resource: " + input + ")");
+          }
+        } else {
+          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+        }
+
+        // LUCENE-3027: past indices were able to write
+        // storePayloads=true when omitTFAP is also true,
+        // which is invalid.  We correct that, here:
+        if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+          storePayloads = false;
+        }
+        hasVectors |= storeTermVector;
+        hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
+        
+        Type normType = isIndexed && !omitNorms ? Type.FIXED_INTS_8 : null;
+        if (format == PreFlexRWFieldInfosWriter.FORMAT_PREFLEX_RW && normType != null) {
+          // RW can have norms but doesn't write them
+          normType = input.readByte() != 0 ? Type.FIXED_INTS_8 : null;
+        }
+        
+        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
+          omitNorms, storePayloads, indexOptions, null, normType);
+      }
+
+      if (input.getFilePointer() != input.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
+      }
+      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
+    } finally {
+      input.close();
+    }
+  }
+  
+  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    files.add(IndexFileNames.segmentFileName(info.name, "", PreFlexRWFieldInfosWriter.FIELD_INFOS_EXTENSION));
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosWriter.java
new file mode 100644
index 0000000..84f666c
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldInfosWriter.java
@@ -0,0 +1,95 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * @lucene.internal
+ * @lucene.experimental
+ */
+class PreFlexRWFieldInfosWriter extends FieldInfosWriter {
+  // TODO move to test-framework preflex RW?
+  
+  /** Extension of field infos */
+  static final String FIELD_INFOS_EXTENSION = "fnm";
+  
+  // First used in 2.9; prior to 2.9 there was no format header
+  static final int FORMAT_START = -2;
+  // First used in 3.4: omit only positional information
+  static final int FORMAT_OMIT_POSITIONS = -3;
+  
+  static final int FORMAT_PREFLEX_RW = Integer.MIN_VALUE;
+
+  // whenever you add a new format, make it 1 smaller (negative version logic)!
+  static final int FORMAT_CURRENT = FORMAT_OMIT_POSITIONS;
+  
+  static final byte IS_INDEXED = 0x1;
+  static final byte STORE_TERMVECTOR = 0x2;
+  static final byte OMIT_NORMS = 0x10;
+  static final byte STORE_PAYLOADS = 0x20;
+  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
+  static final byte OMIT_POSITIONS = -128;
+  
+  @Override
+  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
+    IndexOutput output = directory.createOutput(fileName, context);
+    try {
+      output.writeVInt(FORMAT_PREFLEX_RW);
+      output.writeVInt(infos.size());
+      for (FieldInfo fi : infos) {
+        assert fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !fi.storePayloads;
+        byte bits = 0x0;
+        if (fi.isIndexed) bits |= IS_INDEXED;
+        if (fi.storeTermVector) bits |= STORE_TERMVECTOR;
+        if (fi.omitNorms) bits |= OMIT_NORMS;
+        if (fi.storePayloads) bits |= STORE_PAYLOADS;
+        if (fi.indexOptions == IndexOptions.DOCS_ONLY) {
+          bits |= OMIT_TERM_FREQ_AND_POSITIONS;
+        } else if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS) {
+          bits |= OMIT_POSITIONS;
+        }
+        output.writeString(fi.name);
+        /*
+         * we need to write the field number since IW tries
+         * to stabelize the field numbers across segments so the
+         * FI ordinal is not necessarily equivalent to the field number 
+         */
+        output.writeInt(fi.number);
+        output.writeByte(bits);
+        if (fi.isIndexed && !fi.omitNorms) {
+          // to allow null norm types we need to indicate if norms are written 
+          // only in RW case
+          output.writeByte((byte) (fi.getNormType() == null ? 0 : 1));
+        }
+      }
+    } finally {
+      output.close();
+    }
+  }
+  
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldsWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldsWriter.java
new file mode 100644
index 0000000..d0701a1
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWFieldsWriter.java
@@ -0,0 +1,225 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.PostingsConsumer;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.codecs.lucene40.Lucene40SkipListWriter;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+class PreFlexRWFieldsWriter extends FieldsConsumer {
+
+  private final TermInfosWriter termsOut;
+  private final IndexOutput freqOut;
+  private final IndexOutput proxOut;
+  private final Lucene40SkipListWriter skipListWriter;
+  private final int totalNumDocs;
+
+  public PreFlexRWFieldsWriter(SegmentWriteState state) throws IOException {
+    termsOut = new TermInfosWriter(state.directory,
+                                   state.segmentName,
+                                   state.fieldInfos,
+                                   state.termIndexInterval);
+
+    boolean success = false;
+    try {
+      final String freqFile = IndexFileNames.segmentFileName(state.segmentName, "", Lucene3xPostingsFormat.FREQ_EXTENSION);
+      freqOut = state.directory.createOutput(freqFile, state.context);
+      totalNumDocs = state.numDocs;
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(termsOut);
+      }
+    }
+
+    success = false;
+    try {
+      if (state.fieldInfos.hasProx()) {
+        final String proxFile = IndexFileNames.segmentFileName(state.segmentName, "", Lucene3xPostingsFormat.PROX_EXTENSION);
+        proxOut = state.directory.createOutput(proxFile, state.context);
+      } else {
+        proxOut = null;
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(termsOut, freqOut);
+      }
+    }
+
+    skipListWriter = new Lucene40SkipListWriter(termsOut.skipInterval,
+                                               termsOut.maxSkipLevels,
+                                               totalNumDocs,
+                                               freqOut,
+                                               proxOut);
+    //System.out.println("\nw start seg=" + segment);
+  }
+
+  @Override
+  public TermsConsumer addField(FieldInfo field) throws IOException {
+    assert field.number != -1;
+    if (field.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
+      throw new UnsupportedOperationException("this codec cannot index offsets");
+    }
+    //System.out.println("w field=" + field.name + " storePayload=" + field.storePayloads + " number=" + field.number);
+    return new PreFlexTermsWriter(field);
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.close(termsOut, freqOut, proxOut);
+  }
+
+  private class PreFlexTermsWriter extends TermsConsumer {
+    private final FieldInfo fieldInfo;
+    private final boolean omitTF;
+    private final boolean storePayloads;
+    
+    private final TermInfo termInfo = new TermInfo();
+    private final PostingsWriter postingsWriter = new PostingsWriter();
+
+    public PreFlexTermsWriter(FieldInfo fieldInfo) {
+      this.fieldInfo = fieldInfo;
+      omitTF = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY;
+      storePayloads = fieldInfo.storePayloads;
+    }
+
+    private class PostingsWriter extends PostingsConsumer {
+      private int lastDocID;
+      private int lastPayloadLength = -1;
+      private int lastPosition;
+      private int df;
+
+      public PostingsWriter reset() {
+        df = 0;
+        lastDocID = 0;
+        lastPayloadLength = -1;
+        return this;
+      }
+
+      @Override
+      public void startDoc(int docID, int termDocFreq) throws IOException {
+        //System.out.println("    w doc=" + docID);
+
+        final int delta = docID - lastDocID;
+        if (docID < 0 || (df > 0 && delta <= 0)) {
+          throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " )");
+        }
+
+        if ((++df % termsOut.skipInterval) == 0) {
+          skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength, false, 0);
+          skipListWriter.bufferSkip(df);
+        }
+
+        lastDocID = docID;
+
+        assert docID < totalNumDocs: "docID=" + docID + " totalNumDocs=" + totalNumDocs;
+
+        if (omitTF) {
+          freqOut.writeVInt(delta);
+        } else {
+          final int code = delta << 1;
+          if (termDocFreq == 1) {
+            freqOut.writeVInt(code|1);
+          } else {
+            freqOut.writeVInt(code);
+            freqOut.writeVInt(termDocFreq);
+          }
+        }
+        lastPosition = 0;
+      }
+
+      @Override
+      public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
+        assert proxOut != null;
+        assert startOffset == -1;
+        assert endOffset == -1;
+        //System.out.println("      w pos=" + position + " payl=" + payload);
+        final int delta = position - lastPosition;
+        lastPosition = position;
+
+        if (storePayloads) {
+          final int payloadLength = payload == null ? 0 : payload.length;
+          if (payloadLength != lastPayloadLength) {
+            //System.out.println("        write payload len=" + payloadLength);
+            lastPayloadLength = payloadLength;
+            proxOut.writeVInt((delta<<1)|1);
+            proxOut.writeVInt(payloadLength);
+          } else {
+            proxOut.writeVInt(delta << 1);
+          }
+          if (payloadLength > 0) {
+            proxOut.writeBytes(payload.bytes, payload.offset, payload.length);
+          }
+        } else {
+          proxOut.writeVInt(delta);
+        }
+      }
+
+      @Override
+      public void finishDoc() throws IOException {
+      }
+    }
+
+    @Override
+    public PostingsConsumer startTerm(BytesRef text) throws IOException {
+      //System.out.println("  w term=" + text.utf8ToString());
+      skipListWriter.resetSkip();
+      termInfo.freqPointer = freqOut.getFilePointer();
+      if (proxOut != null) {
+        termInfo.proxPointer = proxOut.getFilePointer();
+      }
+      return postingsWriter.reset();
+    }
+
+    @Override
+    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
+      if (stats.docFreq > 0) {
+        long skipPointer = skipListWriter.writeSkip(freqOut);
+        termInfo.docFreq = stats.docFreq;
+        termInfo.skipOffset = (int) (skipPointer - termInfo.freqPointer);
+        //System.out.println("  w finish term=" + text.utf8ToString() + " fnum=" + fieldInfo.number);
+        termsOut.add(fieldInfo.number,
+                     text,
+                     termInfo);
+      }
+    }
+
+    @Override
+    public void finish(long sumTotalTermCount, long sumDocFreq, int docCount) throws IOException {
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() throws IOException {
+      return BytesRef.getUTF8SortedAsUTF16Comparator();
+    }
+  }
+}
\ No newline at end of file
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.java
new file mode 100644
index 0000000..2bb5482
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsConsumer.java
@@ -0,0 +1,288 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.PerDocConsumer;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Writes and Merges Lucene 3.x norms format
+ * @lucene.experimental
+ */
+class PreFlexRWNormsConsumer extends PerDocConsumer {
+  
+  /** norms header placeholder */
+  private static final byte[] NORMS_HEADER = new byte[]{'N','R','M',-1};
+  
+  /** Extension of norms file */
+  private static final String NORMS_EXTENSION = "nrm";
+  
+  /** Extension of separate norms file
+   * @deprecated */
+  @Deprecated
+  private static final String SEPARATE_NORMS_EXTENSION = "s";
+
+  private final Directory directory;
+
+  private final String segment;
+
+  private final IOContext context;
+
+  private NormsWriter writer;
+  
+  public PreFlexRWNormsConsumer(Directory directory, String segment, IOContext context){
+    this.directory = directory;
+    this.segment = segment;
+    this.context = context;
+  }
+
+  @Override
+  public void merge(MergeState mergeState) throws IOException {
+    getNormsWriter().merge(mergeState);
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (writer != null) {
+      writer.finish();
+    }
+  }
+  
+  @Override
+  protected boolean canMerge(FieldInfo info) {
+    return info.normsPresent();
+  }
+
+  @Override
+  protected Type getDocValuesType(FieldInfo info) {
+    return info.getNormType();
+  }
+
+  @Override
+  public DocValuesConsumer addValuesField(Type type, FieldInfo fieldInfo)
+      throws IOException {
+    if (type != Type.FIXED_INTS_8) {
+      throw new UnsupportedOperationException("Codec only supports single byte norm values. Type give: " + type);
+    }
+    return new Lucene3xNormsDocValuesConsumer(fieldInfo);
+  }
+  
+  class Lucene3xNormsDocValuesConsumer extends DocValuesConsumer {
+    // Holds all docID/norm pairs we've seen
+    private int[] docIDs = new int[1];
+    private byte[] norms = new byte[1];
+    private int upto;
+    private final FieldInfo fi;
+    
+    Lucene3xNormsDocValuesConsumer(FieldInfo fieldInfo) {
+      fi = fieldInfo;
+    }
+
+    @Override
+    public void finish(int docCount) throws IOException {
+      final NormsWriter normsWriter = getNormsWriter();
+      boolean success = false;
+      try {
+        int uptoDoc = 0;
+        normsWriter.setNumTotalDocs(docCount);
+        if (upto > 0) {
+          normsWriter.startField(fi);
+          int docID = 0;
+          for (; docID < docCount; docID++) {
+            if (uptoDoc < upto && docIDs[uptoDoc] == docID) {
+              normsWriter.writeNorm(norms[uptoDoc]);
+              uptoDoc++;
+            } else {
+              normsWriter.writeNorm((byte) 0);
+            }
+          }
+          // we should have consumed every norm
+          assert uptoDoc == upto;
+  
+        } else {
+          // Fill entire field with default norm:
+          normsWriter.startField(fi);
+          for (; upto < docCount; upto++)
+            normsWriter.writeNorm((byte) 0);
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          normsWriter.abort();
+        }
+      }
+    }
+    
+    @Override
+    public void add(int docID, IndexableField docValue) throws IOException {
+      add(docID, docValue.numericValue().longValue());
+    }
+    
+    protected void add(int docID, long value) {
+      if (docIDs.length <= upto) {
+        assert docIDs.length == upto;
+        docIDs = ArrayUtil.grow(docIDs, 1 + upto);
+      }
+      if (norms.length <= upto) {
+        assert norms.length == upto;
+        norms = ArrayUtil.grow(norms, 1 + upto);
+      }
+      norms[upto] = (byte) value;
+      
+      docIDs[upto] = docID;
+      upto++;
+    }
+    
+    
+  }
+  
+  public NormsWriter getNormsWriter() throws IOException {
+    if (writer == null) {
+      writer = new NormsWriter(directory, segment, context);
+    }
+    return writer;
+  }
+  
+  private static class NormsWriter {
+    
+    private final IndexOutput output;
+    private int normCount = 0;
+    private int numTotalDocs = 0;
+    
+    public NormsWriter(Directory directory, String segment, IOContext context) throws IOException {
+      final String normsFileName = IndexFileNames.segmentFileName(segment, "", NORMS_EXTENSION);
+      boolean success = false;
+      IndexOutput out = null;
+      try {
+        out = directory.createOutput(normsFileName, context);
+        output = out;
+        output.writeBytes(NORMS_HEADER, 0, NORMS_HEADER.length);
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(out);
+        }
+      }
+      
+    }
+    
+    
+    public void setNumTotalDocs(int numTotalDocs) {
+      assert this.numTotalDocs == 0 || numTotalDocs == this.numTotalDocs;
+      this.numTotalDocs = numTotalDocs;
+    }
+    
+    public void startField(FieldInfo info) throws IOException {
+      assert info.omitNorms == false;
+      normCount++;
+    }
+    
+    public void writeNorm(byte norm) throws IOException {
+      output.writeByte(norm);
+    }
+    
+    public void abort() throws IOException {
+      IOUtils.close(output);
+    }
+    
+    public void finish() throws IOException {
+      IOUtils.close(output);
+      
+      if (4+normCount*(long)numTotalDocs != output.getFilePointer()) {
+        throw new IOException(".nrm file size mismatch: expected=" + (4+normCount*(long)numTotalDocs) + " actual=" + output.getFilePointer());
+      }
+    }
+    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely
+    /** we override merge and bulk-merge norms when there are no deletions */
+    public void merge(MergeState mergeState) throws IOException {
+      int numMergedDocs = 0;
+      for (FieldInfo fi : mergeState.fieldInfos) {
+        if (fi.normsPresent()) {
+          startField(fi);
+          int numMergedDocsForField = 0;
+          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
+            final int maxDoc = reader.reader.maxDoc();
+            byte[] normBuffer;
+            DocValues normValues = reader.reader.normValues(fi.name);
+            if (normValues == null) {
+              // Can be null if this segment doesn't have
+              // any docs with this field
+              normBuffer = new byte[maxDoc];
+              Arrays.fill(normBuffer, (byte)0);
+            } else {
+              Source directSource = normValues.getDirectSource();
+              assert directSource.hasArray();
+              normBuffer = (byte[]) directSource.getArray();
+            }
+            if (reader.liveDocs == null) {
+              //optimized case for segments without deleted docs
+              output.writeBytes(normBuffer, maxDoc);
+              numMergedDocsForField += maxDoc;
+            } else {
+              // this segment has deleted docs, so we have to
+              // check for every doc if it is deleted or not
+              final Bits liveDocs = reader.liveDocs;
+              for (int k = 0; k < maxDoc; k++) {
+                if (liveDocs.get(k)) {
+                  numMergedDocsForField++;
+                  output.writeByte(normBuffer[k]);
+                }
+              }
+            }
+            mergeState.checkAbort.work(maxDoc);
+          }
+          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;
+          numMergedDocs = numMergedDocsForField;
+        }
+      }
+      this.numTotalDocs = numMergedDocs;
+    }
+  }
+
+  @Override
+  public void abort() {
+    try {
+      try {
+        if (writer != null) {
+          writer.abort();
+        }
+      } finally {
+        directory.deleteFile(IndexFileNames.segmentFileName(segment, "",
+            NORMS_EXTENSION));
+      }
+    } catch (IOException e) {
+      // ignore
+    }
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsFormat.java
new file mode 100644
index 0000000..f549680
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWNormsFormat.java
@@ -0,0 +1,34 @@
+package org.apache.lucene.codecs.lucene3x;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.codecs.PerDocConsumer;
+import org.apache.lucene.index.PerDocWriteState;
+
+/**
+ * @lucene.internal
+ * @lucene.experimental
+ */
+class PreFlexRWNormsFormat extends Lucene3xNormsFormat {
+
+  @Override
+  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
+    return new PreFlexRWNormsConsumer(state.directory, state.segmentName, state.context);
+  }
+
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWPostingsFormat.java
new file mode 100644
index 0000000..9b14b26
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWPostingsFormat.java
@@ -0,0 +1,74 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.util.LuceneTestCase;
+
+/** Codec, only for testing, that can write and read the
+ *  pre-flex index format.
+ *
+ * @lucene.experimental
+ */
+class PreFlexRWPostingsFormat extends Lucene3xPostingsFormat {
+
+  public PreFlexRWPostingsFormat() {
+    // NOTE: we impersonate the PreFlex codec so that it can
+    // read the segments we write!
+  }
+  
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return new PreFlexRWFieldsWriter(state);
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+
+    // Whenever IW opens readers, eg for merging, we have to
+    // keep terms order in UTF16:
+
+    return new Lucene3xFields(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.termsIndexDivisor) {
+      @Override
+      protected boolean sortTermsByUnicode() {
+        // We carefully peek into stack track above us: if
+        // we are part of a "merge", we must sort by UTF16:
+        boolean unicodeSortOrder = true;
+
+        StackTraceElement[] trace = new Exception().getStackTrace();
+        for (int i = 0; i < trace.length; i++) {
+          //System.out.println(trace[i].getClassName());
+          if ("merge".equals(trace[i].getMethodName())) {
+            unicodeSortOrder = false;
+            if (LuceneTestCase.VERBOSE) {
+              System.out.println("NOTE: PreFlexRW codec: forcing legacy UTF16 term sort order");
+            }
+            break;
+          }
+        }
+
+        return unicodeSortOrder;
+      }
+    };
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosFormat.java
new file mode 100644
index 0000000..c970f08
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosFormat.java
@@ -0,0 +1,32 @@
+package org.apache.lucene.codecs.lucene3x;
+
+import org.apache.lucene.codecs.SegmentInfosWriter;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @lucene.experimental
+ */
+class PreFlexRWSegmentInfosFormat extends Lucene3xSegmentInfosFormat {
+  private final SegmentInfosWriter writer = new PreFlexRWSegmentInfosWriter();
+  
+  @Override
+  public SegmentInfosWriter getSegmentInfosWriter() {
+    return writer;
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosWriter.java
new file mode 100644
index 0000000..eb1633d
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWSegmentInfosWriter.java
@@ -0,0 +1,118 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.lucene.codecs.SegmentInfosWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.store.ChecksumIndexOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FlushInfo;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * PreFlex implementation of {@link SegmentInfosWriter}.
+ * @lucene.experimental
+ */
+class PreFlexRWSegmentInfosWriter extends SegmentInfosWriter {
+
+  @Override
+  public IndexOutput writeInfos(Directory dir, String segmentFileName, String codecID, SegmentInfos infos, IOContext context)
+          throws IOException {
+    IndexOutput out = createOutput(dir, segmentFileName, new IOContext(new FlushInfo(infos.size(), infos.totalDocCount())));
+    boolean success = false;
+    try {
+      out.writeInt(SegmentInfos.FORMAT_3_1); // write FORMAT
+      // we don't write a codec - this is 3.x
+      out.writeLong(infos.version);
+      out.writeInt(infos.counter); // write counter
+      out.writeInt(infos.size()); // write infos
+      for (SegmentInfo si : infos) {
+        writeInfo(out, si);
+      }
+      out.writeStringStringMap(infos.getUserData());
+      success = true;
+      return out;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  /** Save a single segment's info. */
+  private void writeInfo(IndexOutput output, SegmentInfo si) throws IOException {
+    // we are about to write this SI in 3.x format, dropping all codec information, etc.
+    // so it had better be a 3.x segment or you will get very confusing errors later.
+    assert si.getCodec() instanceof Lucene3xCodec : "broken test, trying to mix preflex with other codecs";
+    assert si.getDelCount() <= si.docCount: "delCount=" + si.getDelCount() + " docCount=" + si.docCount + " segment=" + si.name;
+    // Write the Lucene version that created this segment, since 3.1
+    output.writeString(si.getVersion());
+    output.writeString(si.name);
+    output.writeInt(si.docCount);
+    output.writeLong(si.getDelGen());
+
+    output.writeInt(si.getDocStoreOffset());
+    if (si.getDocStoreOffset() != -1) {
+      output.writeString(si.getDocStoreSegment());
+      output.writeByte((byte) (si.getDocStoreIsCompoundFile() ? 1:0));
+    }
+    // pre-4.0 indexes write a byte if there is a single norms file
+    output.writeByte((byte) 1);
+
+    Map<Integer,Long> normGen = si.getNormGen();
+    if (normGen == null) {
+      output.writeInt(SegmentInfo.NO);
+    } else {
+      output.writeInt(normGen.size());
+      for (Entry<Integer,Long> entry : normGen.entrySet()) {
+        output.writeLong(entry.getValue());
+      }
+    }
+
+    output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
+    output.writeInt(si.getDelCount());
+    output.writeByte((byte) (si.getHasProxInternal()));
+    output.writeStringStringMap(si.getDiagnostics());
+    output.writeByte((byte) (si.getHasVectorsInternal()));
+  }
+  
+  protected IndexOutput createOutput(Directory dir, String segmentFileName, IOContext context)
+      throws IOException {
+    IndexOutput plainOut = dir.createOutput(segmentFileName, context);
+    ChecksumIndexOutput out = new ChecksumIndexOutput(plainOut);
+    return out;
+  }
+
+  @Override
+  public void prepareCommit(IndexOutput segmentOutput) throws IOException {
+    ((ChecksumIndexOutput)segmentOutput).prepareCommit();
+  }
+
+  @Override
+  public void finishCommit(IndexOutput out) throws IOException {
+    ((ChecksumIndexOutput)out).finishCommit();
+    out.close();
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsFormat.java
new file mode 100644
index 0000000..4d543bd
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsFormat.java
@@ -0,0 +1,16 @@
+package org.apache.lucene.codecs.lucene3x;
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+class PreFlexRWStoredFieldsFormat extends Lucene3xStoredFieldsFormat {
+
+  @Override
+  public StoredFieldsWriter fieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    return new PreFlexRWStoredFieldsWriter(directory, segment, context);
+  }
+  
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsWriter.java
new file mode 100644
index 0000000..a580b12
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWStoredFieldsWriter.java
@@ -0,0 +1,155 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you may not
+ * use this file except in compliance with the License. You may obtain a copy of
+ * the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/** @lucene.experimental */
+final class PreFlexRWStoredFieldsWriter extends StoredFieldsWriter {
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput fieldsStream;
+  private IndexOutput indexStream;
+
+  public PreFlexRWStoredFieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    assert directory != null;
+    this.directory = directory;
+    this.segment = segment;
+
+    boolean success = false;
+    try {
+      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene3xStoredFieldsReader.FIELDS_EXTENSION), context);
+      indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene3xStoredFieldsReader.FIELDS_INDEX_EXTENSION), context);
+
+      fieldsStream.writeInt(Lucene3xStoredFieldsReader.FORMAT_CURRENT);
+      indexStream.writeInt(Lucene3xStoredFieldsReader.FORMAT_CURRENT);
+
+      success = true;
+    } finally {
+      if (!success) {
+        abort();
+      }
+    }
+  }
+
+  // Writes the contents of buffer into the fields stream
+  // and adds a new entry for this document into the index
+  // stream.  This assumes the buffer was already written
+  // in the correct fields format.
+  public void startDocument(int numStoredFields) throws IOException {
+    indexStream.writeLong(fieldsStream.getFilePointer());
+    fieldsStream.writeVInt(numStoredFields);
+  }
+
+  public void close() throws IOException {
+    try {
+      IOUtils.close(fieldsStream, indexStream);
+    } finally {
+      fieldsStream = indexStream = null;
+    }
+  }
+
+  public void abort() {
+    try {
+      close();
+    } catch (IOException ignored) {}
+    IOUtils.deleteFilesIgnoringExceptions(directory,
+        IndexFileNames.segmentFileName(segment, "", Lucene3xStoredFieldsReader.FIELDS_EXTENSION),
+        IndexFileNames.segmentFileName(segment, "", Lucene3xStoredFieldsReader.FIELDS_INDEX_EXTENSION));
+  }
+
+  public void writeField(FieldInfo info, IndexableField field) throws IOException {
+    fieldsStream.writeVInt(info.number);
+    int bits = 0;
+    final BytesRef bytes;
+    final String string;
+    // TODO: maybe a field should serialize itself?
+    // this way we don't bake into indexer all these
+    // specific encodings for different fields?  and apps
+    // can customize...
+
+    Number number = field.numericValue();
+    if (number != null) {
+      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
+        bits |= Lucene3xStoredFieldsReader.FIELD_IS_NUMERIC_INT;
+      } else if (number instanceof Long) {
+        bits |= Lucene3xStoredFieldsReader.FIELD_IS_NUMERIC_LONG;
+      } else if (number instanceof Float) {
+        bits |= Lucene3xStoredFieldsReader.FIELD_IS_NUMERIC_FLOAT;
+      } else if (number instanceof Double) {
+        bits |= Lucene3xStoredFieldsReader.FIELD_IS_NUMERIC_DOUBLE;
+      } else {
+        throw new IllegalArgumentException("cannot store numeric type " + number.getClass());
+      }
+      string = null;
+      bytes = null;
+    } else {
+      bytes = field.binaryValue();
+      if (bytes != null) {
+        bits |= Lucene3xStoredFieldsReader.FIELD_IS_BINARY;
+        string = null;
+      } else {
+        string = field.stringValue();
+        if (string == null) {
+          throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
+        }
+      }
+    }
+
+    fieldsStream.writeByte((byte) bits);
+
+    if (bytes != null) {
+      fieldsStream.writeVInt(bytes.length);
+      fieldsStream.writeBytes(bytes.bytes, bytes.offset, bytes.length);
+    } else if (string != null) {
+      fieldsStream.writeString(field.stringValue());
+    } else {
+      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
+        fieldsStream.writeInt(number.intValue());
+      } else if (number instanceof Long) {
+        fieldsStream.writeLong(number.longValue());
+      } else if (number instanceof Float) {
+        fieldsStream.writeInt(Float.floatToIntBits(number.floatValue()));
+      } else if (number instanceof Double) {
+        fieldsStream.writeLong(Double.doubleToLongBits(number.doubleValue()));
+      } else {
+        assert false;
+      }
+    }
+  }
+
+  @Override
+  public void finish(int numDocs) throws IOException {
+    if (4+((long) numDocs)*8 != indexStream.getFilePointer())
+      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
+      // we detect that the bug has struck, here, and
+      // throw an exception to prevent the corruption from
+      // entering the index.  See LUCENE-1282 for
+      // details.
+      throw new RuntimeException("fdx size mismatch: docCount is " + numDocs + " but fdx file size is " + indexStream.getFilePointer() + " file=" + indexStream.toString() + "; now aborting this merge to prevent index corruption");
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsFormat.java
new file mode 100644
index 0000000..4f0a750
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsFormat.java
@@ -0,0 +1,62 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.LuceneTestCase;
+
+class PreFlexRWTermVectorsFormat extends Lucene3xTermVectorsFormat {
+
+  @Override
+  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    return new PreFlexRWTermVectorsWriter(directory, segment, context);
+  }
+
+  @Override
+  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
+    return new Lucene3xTermVectorsReader(directory, segmentInfo, fieldInfos, context) {
+      @Override
+      protected boolean sortTermsByUnicode() {
+        // We carefully peek into stack track above us: if
+        // we are part of a "merge", we must sort by UTF16:
+        boolean unicodeSortOrder = true;
+
+        StackTraceElement[] trace = new Exception().getStackTrace();
+        for (int i = 0; i < trace.length; i++) {
+          //System.out.println(trace[i].getClassName());
+          if ("merge".equals(trace[i].getMethodName())) {
+            unicodeSortOrder = false;
+            if (LuceneTestCase.VERBOSE) {
+              System.out.println("NOTE: PreFlexRW codec: forcing legacy UTF16 vector term sort order");
+            }
+            break;
+          }
+        }
+
+        return unicodeSortOrder;
+      }
+    };
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsWriter.java
new file mode 100644
index 0000000..a893bf1
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/PreFlexRWTermVectorsWriter.java
@@ -0,0 +1,220 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+
+final class PreFlexRWTermVectorsWriter extends TermVectorsWriter {
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput tvx = null, tvd = null, tvf = null;
+
+  public PreFlexRWTermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    this.directory = directory;
+    this.segment = segment;
+    boolean success = false;
+    try {
+      // Open files for TermVector storage
+      tvx = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_INDEX_EXTENSION), context);
+      tvx.writeInt(Lucene3xTermVectorsReader.FORMAT_CURRENT);
+      tvd = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_DOCUMENTS_EXTENSION), context);
+      tvd.writeInt(Lucene3xTermVectorsReader.FORMAT_CURRENT);
+      tvf = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_FIELDS_EXTENSION), context);
+      tvf.writeInt(Lucene3xTermVectorsReader.FORMAT_CURRENT);
+      success = true;
+    } finally {
+      if (!success) {
+        abort();
+      }
+    }
+  }
+ 
+  @Override
+  public void startDocument(int numVectorFields) throws IOException {
+    lastFieldName = null;
+    this.numVectorFields = numVectorFields;
+    tvx.writeLong(tvd.getFilePointer());
+    tvx.writeLong(tvf.getFilePointer());
+    tvd.writeVInt(numVectorFields);
+    fieldCount = 0;
+    fps = ArrayUtil.grow(fps, numVectorFields);
+  }
+  
+  private long fps[] = new long[10]; // pointers to the tvf before writing each field 
+  private int fieldCount = 0;        // number of fields we have written so far for this document
+  private int numVectorFields = 0;   // total number of fields we will write for this document
+  private String lastFieldName;
+
+  @Override
+  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets) throws IOException {
+    assert lastFieldName == null || info.name.compareTo(lastFieldName) > 0: "fieldName=" + info.name + " lastFieldName=" + lastFieldName;
+    lastFieldName = info.name;
+    this.positions = positions;
+    this.offsets = offsets;
+    lastTerm.length = 0;
+    fps[fieldCount++] = tvf.getFilePointer();
+    tvd.writeVInt(info.number);
+    tvf.writeVInt(numTerms);
+    byte bits = 0x0;
+    if (positions)
+      bits |= Lucene3xTermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR;
+    if (offsets)
+      bits |= Lucene3xTermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR;
+    tvf.writeByte(bits);
+    
+    assert fieldCount <= numVectorFields;
+    if (fieldCount == numVectorFields) {
+      // last field of the document
+      // this is crazy because the file format is crazy!
+      for (int i = 1; i < fieldCount; i++) {
+        tvd.writeVLong(fps[i] - fps[i-1]);
+      }
+    }
+  }
+  
+  private final BytesRef lastTerm = new BytesRef(10);
+
+  // NOTE: we override addProx, so we don't need to buffer when indexing.
+  // we also don't buffer during bulk merges.
+  private int offsetStartBuffer[] = new int[10];
+  private int offsetEndBuffer[] = new int[10];
+  private int offsetIndex = 0;
+  private int offsetFreq = 0;
+  private boolean positions = false;
+  private boolean offsets = false;
+
+  @Override
+  public void startTerm(BytesRef term, int freq) throws IOException {
+    final int prefix = StringHelper.bytesDifference(lastTerm, term);
+    final int suffix = term.length - prefix;
+    tvf.writeVInt(prefix);
+    tvf.writeVInt(suffix);
+    tvf.writeBytes(term.bytes, term.offset + prefix, suffix);
+    tvf.writeVInt(freq);
+    lastTerm.copyBytes(term);
+    lastPosition = lastOffset = 0;
+    
+    if (offsets && positions) {
+      // we might need to buffer if its a non-bulk merge
+      offsetStartBuffer = ArrayUtil.grow(offsetStartBuffer, freq);
+      offsetEndBuffer = ArrayUtil.grow(offsetEndBuffer, freq);
+      offsetIndex = 0;
+      offsetFreq = freq;
+    }
+  }
+
+  int lastPosition = 0;
+  int lastOffset = 0;
+
+  @Override
+  public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
+    // TODO: technically we could just copy bytes and not re-encode if we knew the length...
+    if (positions != null) {
+      for (int i = 0; i < numProx; i++) {
+        tvf.writeVInt(positions.readVInt());
+      }
+    }
+    
+    if (offsets != null) {
+      for (int i = 0; i < numProx; i++) {
+        tvf.writeVInt(offsets.readVInt());
+        tvf.writeVInt(offsets.readVInt());
+      }
+    }
+  }
+
+  @Override
+  public void addPosition(int position, int startOffset, int endOffset) throws IOException {
+    if (positions && offsets) {
+      // write position delta
+      tvf.writeVInt(position - lastPosition);
+      lastPosition = position;
+      
+      // buffer offsets
+      offsetStartBuffer[offsetIndex] = startOffset;
+      offsetEndBuffer[offsetIndex] = endOffset;
+      offsetIndex++;
+      
+      // dump buffer if we are done
+      if (offsetIndex == offsetFreq) {
+        for (int i = 0; i < offsetIndex; i++) {
+          tvf.writeVInt(offsetStartBuffer[i] - lastOffset);
+          tvf.writeVInt(offsetEndBuffer[i] - offsetStartBuffer[i]);
+          lastOffset = offsetEndBuffer[i];
+        }
+      }
+    } else if (positions) {
+      // write position delta
+      tvf.writeVInt(position - lastPosition);
+      lastPosition = position;
+    } else if (offsets) {
+      // write offset deltas
+      tvf.writeVInt(startOffset - lastOffset);
+      tvf.writeVInt(endOffset - startOffset);
+      lastOffset = endOffset;
+    }
+  }
+
+  @Override
+  public void abort() {
+    try {
+      close();
+    } catch (IOException ignored) {}
+    IOUtils.deleteFilesIgnoringExceptions(directory, IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_INDEX_EXTENSION),
+        IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_DOCUMENTS_EXTENSION),
+        IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_FIELDS_EXTENSION));
+  }
+  
+  @Override
+  public void finish(int numDocs) throws IOException {
+    if (4+((long) numDocs)*16 != tvx.getFilePointer())
+      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
+      // we detect that the bug has struck, here, and
+      // throw an exception to prevent the corruption from
+      // entering the index.  See LUCENE-1282 for
+      // details.
+      throw new RuntimeException("tvx size mismatch: mergedDocs is " + numDocs + " but tvx size is " + tvx.getFilePointer() + " file=" + tvx.toString() + "; now aborting this merge to prevent index corruption");
+  }
+
+  /** Close all streams. */
+  @Override
+  public void close() throws IOException {
+    // make an effort to close all streams we can but remember and re-throw
+    // the first exception encountered in this process
+    IOUtils.close(tvx, tvd, tvf);
+    tvx = tvd = tvf = null;
+  }
+  
+  @Override
+  public Comparator<BytesRef> getComparator() throws IOException {
+    return BytesRef.getUTF8SortedAsUTF16Comparator();
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/TermInfosWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/TermInfosWriter.java
new file mode 100644
index 0000000..8fd6aa4
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene3x/TermInfosWriter.java
@@ -0,0 +1,274 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.UnicodeUtil;
+
+
+/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
+  Directory.  A TermInfos can be written once, in order.  */
+
+final class TermInfosWriter implements Closeable {
+  /** The file format version, a negative number. */
+  public static final int FORMAT = -3;
+
+  // Changed strings to true utf8 with length-in-bytes not
+  // length-in-chars
+  public static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = -4;
+
+  // NOTE: always change this if you switch to a new format!
+  public static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
+
+  private FieldInfos fieldInfos;
+  private IndexOutput output;
+  private TermInfo lastTi = new TermInfo();
+  private long size;
+
+  // TODO: the default values for these two parameters should be settable from
+  // IndexWriter.  However, once that's done, folks will start setting them to
+  // ridiculous values and complaining that things don't work well, as with
+  // mergeFactor.  So, let's wait until a number of folks find that alternate
+  // values work better.  Note that both of these values are stored in the
+  // segment, so that it's safe to change these w/o rebuilding all indexes.
+
+  /** Expert: The fraction of terms in the "dictionary" which should be stored
+   * in RAM.  Smaller values use more memory, but make searching slightly
+   * faster, while larger values use less memory and make searching slightly
+   * slower.  Searching is typically not dominated by dictionary lookup, so
+   * tweaking this is rarely useful.*/
+  int indexInterval = 128;
+
+  /** Expert: The fraction of {@link TermDocs} entries stored in skip tables,
+   * used to accelerate {@link TermDocs#skipTo(int)}.  Larger values result in
+   * smaller indexes, greater acceleration, but fewer accelerable cases, while
+   * smaller values result in bigger indexes, less acceleration and more
+   * accelerable cases. More detailed experiments would be useful here. */
+  int skipInterval = 16;
+  
+  /** Expert: The maximum number of skip levels. Smaller values result in 
+   * slightly smaller indexes, but slower skipping in big posting lists.
+   */
+  int maxSkipLevels = 10;
+
+  private long lastIndexPointer;
+  private boolean isIndex;
+  private final BytesRef lastTerm = new BytesRef();
+  private int lastFieldNumber = -1;
+
+  private TermInfosWriter other;
+
+  TermInfosWriter(Directory directory, String segment, FieldInfos fis,
+                  int interval)
+       throws IOException {
+    initialize(directory, segment, fis, interval, false);
+    boolean success = false;
+    try {
+      other = new TermInfosWriter(directory, segment, fis, interval, true);
+      other.other = this;
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(output);
+
+        try {
+          directory.deleteFile(IndexFileNames.segmentFileName(segment, "",
+              (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
+                  : Lucene3xPostingsFormat.TERMS_EXTENSION)));
+        } catch (IOException ignored) {
+        }
+      }
+    }
+  }
+
+  private TermInfosWriter(Directory directory, String segment, FieldInfos fis,
+                          int interval, boolean isIndex) throws IOException {
+    initialize(directory, segment, fis, interval, isIndex);
+  }
+
+  private void initialize(Directory directory, String segment, FieldInfos fis,
+                          int interval, boolean isi) throws IOException {
+    indexInterval = interval;
+    fieldInfos = fis;
+    isIndex = isi;
+    output = directory.createOutput(IndexFileNames.segmentFileName(segment, "",
+        (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
+            : Lucene3xPostingsFormat.TERMS_EXTENSION)), IOContext.DEFAULT);
+    boolean success = false;
+    try {
+      output.writeInt(FORMAT_CURRENT);              // write format
+      output.writeLong(0);                          // leave space for size
+      output.writeInt(indexInterval);               // write indexInterval
+      output.writeInt(skipInterval);                // write skipInterval
+      output.writeInt(maxSkipLevels);               // write maxSkipLevels
+      assert initUTF16Results();
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(output);
+
+        try {
+          directory.deleteFile(IndexFileNames.segmentFileName(segment, "",
+              (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
+                  : Lucene3xPostingsFormat.TERMS_EXTENSION)));
+        } catch (IOException ignored) {
+        }
+      }
+    }
+  }
+
+  // Currently used only by assert statements
+  CharsRef utf16Result1;
+  CharsRef utf16Result2;
+  private final BytesRef scratchBytes = new BytesRef();
+
+  // Currently used only by assert statements
+  private boolean initUTF16Results() {
+    utf16Result1 = new CharsRef(10);
+    utf16Result2 = new CharsRef(10);
+    return true;
+  }
+
+  // Currently used only by assert statement
+  private int compareToLastTerm(int fieldNumber, BytesRef term) {
+
+    if (lastFieldNumber != fieldNumber) {
+      final int cmp = fieldInfos.fieldName(lastFieldNumber).compareTo(fieldInfos.fieldName(fieldNumber));
+      // If there is a field named "" (empty string) then we
+      // will get 0 on this comparison, yet, it's "OK".  But
+      // it's not OK if two different field numbers map to
+      // the same name.
+      if (cmp != 0 || lastFieldNumber != -1)
+        return cmp;
+    }
+
+    scratchBytes.copyBytes(term);
+    assert lastTerm.offset == 0;
+    UnicodeUtil.UTF8toUTF16(lastTerm.bytes, 0, lastTerm.length, utf16Result1);
+
+    assert scratchBytes.offset == 0;
+    UnicodeUtil.UTF8toUTF16(scratchBytes.bytes, 0, scratchBytes.length, utf16Result2);
+
+    final int len;
+    if (utf16Result1.length < utf16Result2.length)
+      len = utf16Result1.length;
+    else
+      len = utf16Result2.length;
+
+    for(int i=0;i<len;i++) {
+      final char ch1 = utf16Result1.chars[i];
+      final char ch2 = utf16Result2.chars[i];
+      if (ch1 != ch2)
+        return ch1-ch2;
+    }
+    if (utf16Result1.length == 0 && lastFieldNumber == -1) {
+      // If there is a field named "" (empty string) with a term text of "" (empty string) then we
+      // will get 0 on this comparison, yet, it's "OK". 
+      return -1;
+    }
+    return utf16Result1.length - utf16Result2.length;
+  }
+
+  /** Adds a new <<fieldNumber, termBytes>, TermInfo> pair to the set.
+    Term must be lexicographically greater than all previous Terms added.
+    TermInfo pointers must be positive and greater than all previous.*/
+  public void add(int fieldNumber, BytesRef term, TermInfo ti)
+    throws IOException {
+
+    assert compareToLastTerm(fieldNumber, term) < 0 ||
+      (isIndex && term.length == 0 && lastTerm.length == 0) :
+      "Terms are out of order: field=" + fieldInfos.fieldName(fieldNumber) + " (number " + fieldNumber + ")" +
+        " lastField=" + fieldInfos.fieldName(lastFieldNumber) + " (number " + lastFieldNumber + ")" +
+        " text=" + term.utf8ToString() + " lastText=" + lastTerm.utf8ToString();
+
+    assert ti.freqPointer >= lastTi.freqPointer: "freqPointer out of order (" + ti.freqPointer + " < " + lastTi.freqPointer + ")";
+    assert ti.proxPointer >= lastTi.proxPointer: "proxPointer out of order (" + ti.proxPointer + " < " + lastTi.proxPointer + ")";
+
+    if (!isIndex && size % indexInterval == 0)
+      other.add(lastFieldNumber, lastTerm, lastTi);                      // add an index term
+
+    writeTerm(fieldNumber, term);                        // write term
+
+    output.writeVInt(ti.docFreq);                       // write doc freq
+    output.writeVLong(ti.freqPointer - lastTi.freqPointer); // write pointers
+    output.writeVLong(ti.proxPointer - lastTi.proxPointer);
+
+    if (ti.docFreq >= skipInterval) {
+      output.writeVInt(ti.skipOffset);
+    }
+
+    if (isIndex) {
+      output.writeVLong(other.output.getFilePointer() - lastIndexPointer);
+      lastIndexPointer = other.output.getFilePointer(); // write pointer
+    }
+
+    lastFieldNumber = fieldNumber;
+    lastTi.set(ti);
+    size++;
+  }
+
+  private void writeTerm(int fieldNumber, BytesRef term)
+       throws IOException {
+
+    //System.out.println("  tiw.write field=" + fieldNumber + " term=" + term.utf8ToString());
+
+    // TODO: UTF16toUTF8 could tell us this prefix
+    // Compute prefix in common with last term:
+    int start = 0;
+    final int limit = term.length < lastTerm.length ? term.length : lastTerm.length;
+    while(start < limit) {
+      if (term.bytes[start+term.offset] != lastTerm.bytes[start+lastTerm.offset])
+        break;
+      start++;
+    }
+
+    final int length = term.length - start;
+    output.writeVInt(start);                     // write shared prefix length
+    output.writeVInt(length);                  // write delta length
+    output.writeBytes(term.bytes, start+term.offset, length);  // write delta bytes
+    output.writeVInt(fieldNumber); // write field num
+    lastTerm.copyBytes(term);
+  }
+
+  /** Called to complete TermInfos creation. */
+  public void close() throws IOException {
+    try {
+      output.seek(4);          // write size after format
+      output.writeLong(size);
+    } finally {
+      try {
+        output.close();
+      } finally {
+        if (!isIndex) {
+          other.close();
+        }
+      }
+    }
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWCodec.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWCodec.java
deleted file mode 100644
index 5b7029c..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWCodec.java
+++ /dev/null
@@ -1,129 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfosFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene3x.Lucene3xCodec;
-import org.apache.lucene.codecs.lucene3x.Lucene3xNormsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.StringHelper;
-
-/**
- * Writes 3.x-like indexes (not perfect emulation yet) for testing only!
- * @lucene.experimental
- */
-public class PreFlexRWCodec extends Lucene3xCodec {
-  private final PostingsFormat postings = new PreFlexRWPostingsFormat();
-  private final Lucene3xNormsFormat norms = new PreFlexRWNormsFormat();
-  private final FieldInfosFormat fieldInfos = new PreFlexRWFieldInfosFormat();
-  private final TermVectorsFormat termVectors = new PreFlexRWTermVectorsFormat();
-  private final SegmentInfosFormat segmentInfos = new PreFlexRWSegmentInfosFormat();
-  private final StoredFieldsFormat storedFields = new PreFlexRWStoredFieldsFormat();
-  // TODO: this should really be a different impl
-  private final LiveDocsFormat liveDocs = new Lucene40LiveDocsFormat();
-  
-  @Override
-  public PostingsFormat postingsFormat() {
-    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
-      return postings;
-    } else {
-      return super.postingsFormat();
-    }
-  }
-
-  @Override
-  public Lucene3xNormsFormat normsFormat() {
-    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
-      return norms;
-    } else {
-      return super.normsFormat();
-    }
-  }
-
-  @Override
-  public SegmentInfosFormat segmentInfosFormat() {
-    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
-      return segmentInfos ;
-    } else {
-      return super.segmentInfosFormat();
-    }
-  }
-
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
-      return fieldInfos;
-    } else {
-      return super.fieldInfosFormat();
-    }
-  }
-
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
-      return termVectors;
-    } else {
-      return super.termVectorsFormat();
-    }
-  }
-
-  @Override
-  public LiveDocsFormat liveDocsFormat() {
-    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
-      return liveDocs;
-    } else {
-      return super.liveDocsFormat();
-    }
-  }
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
-      return storedFields;
-    } else {
-      return super.storedFieldsFormat();
-    }
-  }
-
-  @Override
-  public void files(SegmentInfo info, Set<String> files) throws IOException {
-    if (info.getUseCompoundFile() && LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
-      // because we don't fully emulate 3.x codec, PreFlexRW actually writes 4.x format CFS files.
-      // so we must check segment version here to see if its a "real" 3.x segment or a "fake"
-      // one that we wrote with a 4.x-format CFS+CFE, in this case we must add the .CFE
-      String version = info.getVersion();
-      if (version != null && StringHelper.getVersionComparator().compare("4.0", version) <= 0) {
-        files.add(IndexFileNames.segmentFileName(info.name, "", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
-      }
-    }
-    
-    super.files(info, files);
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldInfosFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldInfosFormat.java
deleted file mode 100644
index 8fdf874..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldInfosFormat.java
+++ /dev/null
@@ -1,41 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldInfosReader;
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.codecs.lucene3x.Lucene3xFieldInfosFormat;
-
-/**
- * 
- * @lucene.internal
- * @lucene.experimental
- */
-public class PreFlexRWFieldInfosFormat extends Lucene3xFieldInfosFormat {
-
-  @Override
-  public FieldInfosReader getFieldInfosReader() throws IOException {
-    return new PreFlexRWFieldInfosReader();
-  }
-
-  @Override
-  public FieldInfosWriter getFieldInfosWriter() throws IOException {
-    return new PreFlexRWFieldInfosWriter();
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldInfosReader.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldInfosReader.java
deleted file mode 100644
index e08d3ad..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldInfosReader.java
+++ /dev/null
@@ -1,117 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.codecs.FieldInfosReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexFormatTooNewException;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-
-/**
- * @lucene.internal
- * @lucene.experimental
- */
-public class PreFlexRWFieldInfosReader extends FieldInfosReader {
-  static final int FORMAT_MINIMUM = PreFlexRWFieldInfosWriter.FORMAT_START;
-
-  @Override
-  public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", PreFlexRWFieldInfosWriter.FIELD_INFOS_EXTENSION);
-    IndexInput input = directory.openInput(fileName, iocontext);
-
-    boolean hasVectors = false;
-    boolean hasFreq = false;
-    boolean hasProx = false;
-    
-    try {
-      final int format = input.readVInt();
-
-      if (format > FORMAT_MINIMUM) {
-        throw new IndexFormatTooOldException(input, format, FORMAT_MINIMUM, PreFlexRWFieldInfosWriter.FORMAT_CURRENT);
-      }
-      if (format < PreFlexRWFieldInfosWriter.FORMAT_CURRENT && format != PreFlexRWFieldInfosWriter.FORMAT_PREFLEX_RW) {
-        throw new IndexFormatTooNewException(input, format, FORMAT_MINIMUM, PreFlexRWFieldInfosWriter.FORMAT_CURRENT);
-      }
-
-      final int size = input.readVInt(); //read in the size
-      FieldInfo infos[] = new FieldInfo[size];
-
-      for (int i = 0; i < size; i++) {
-        String name = input.readString();
-        final int fieldNumber = format == PreFlexRWFieldInfosWriter.FORMAT_PREFLEX_RW ? input.readInt() : i;
-        byte bits = input.readByte();
-        boolean isIndexed = (bits & PreFlexRWFieldInfosWriter.IS_INDEXED) != 0;
-        boolean storeTermVector = (bits & PreFlexRWFieldInfosWriter.STORE_TERMVECTOR) != 0;
-        boolean omitNorms = (bits & PreFlexRWFieldInfosWriter.OMIT_NORMS) != 0;
-        boolean storePayloads = (bits & PreFlexRWFieldInfosWriter.STORE_PAYLOADS) != 0;
-        final IndexOptions indexOptions;
-        if ((bits & PreFlexRWFieldInfosWriter.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_ONLY;
-        } else if ((bits & PreFlexRWFieldInfosWriter.OMIT_POSITIONS) != 0) {
-          if (format <= PreFlexRWFieldInfosWriter.FORMAT_OMIT_POSITIONS) {
-            indexOptions = IndexOptions.DOCS_AND_FREQS;
-          } else {
-            throw new CorruptIndexException("Corrupt fieldinfos, OMIT_POSITIONS set but format=" + format + " (resource: " + input + ")");
-          }
-        } else {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        }
-
-        // LUCENE-3027: past indices were able to write
-        // storePayloads=true when omitTFAP is also true,
-        // which is invalid.  We correct that, here:
-        if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-          storePayloads = false;
-        }
-        hasVectors |= storeTermVector;
-        hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
-        
-        Type normType = isIndexed && !omitNorms ? Type.FIXED_INTS_8 : null;
-        if (format == PreFlexRWFieldInfosWriter.FORMAT_PREFLEX_RW && normType != null) {
-          // RW can have norms but doesn't write them
-          normType = input.readByte() != 0 ? Type.FIXED_INTS_8 : null;
-        }
-        
-        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, null, normType);
-      }
-
-      if (input.getFilePointer() != input.length()) {
-        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
-      }
-      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
-    } finally {
-      input.close();
-    }
-  }
-  
-  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(info.name, "", PreFlexRWFieldInfosWriter.FIELD_INFOS_EXTENSION));
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldInfosWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldInfosWriter.java
deleted file mode 100644
index 6a5d28e..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldInfosWriter.java
+++ /dev/null
@@ -1,95 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-
-/**
- * @lucene.internal
- * @lucene.experimental
- */
-public class PreFlexRWFieldInfosWriter extends FieldInfosWriter {
-  // TODO move to test-framework preflex RW?
-  
-  /** Extension of field infos */
-  static final String FIELD_INFOS_EXTENSION = "fnm";
-  
-  // First used in 2.9; prior to 2.9 there was no format header
-  static final int FORMAT_START = -2;
-  // First used in 3.4: omit only positional information
-  static final int FORMAT_OMIT_POSITIONS = -3;
-  
-  static final int FORMAT_PREFLEX_RW = Integer.MIN_VALUE;
-
-  // whenever you add a new format, make it 1 smaller (negative version logic)!
-  static final int FORMAT_CURRENT = FORMAT_OMIT_POSITIONS;
-  
-  static final byte IS_INDEXED = 0x1;
-  static final byte STORE_TERMVECTOR = 0x2;
-  static final byte OMIT_NORMS = 0x10;
-  static final byte STORE_PAYLOADS = 0x20;
-  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
-  static final byte OMIT_POSITIONS = -128;
-  
-  @Override
-  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
-    IndexOutput output = directory.createOutput(fileName, context);
-    try {
-      output.writeVInt(FORMAT_PREFLEX_RW);
-      output.writeVInt(infos.size());
-      for (FieldInfo fi : infos) {
-        assert fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !fi.storePayloads;
-        byte bits = 0x0;
-        if (fi.isIndexed) bits |= IS_INDEXED;
-        if (fi.storeTermVector) bits |= STORE_TERMVECTOR;
-        if (fi.omitNorms) bits |= OMIT_NORMS;
-        if (fi.storePayloads) bits |= STORE_PAYLOADS;
-        if (fi.indexOptions == IndexOptions.DOCS_ONLY) {
-          bits |= OMIT_TERM_FREQ_AND_POSITIONS;
-        } else if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS) {
-          bits |= OMIT_POSITIONS;
-        }
-        output.writeString(fi.name);
-        /*
-         * we need to write the field number since IW tries
-         * to stabelize the field numbers across segments so the
-         * FI ordinal is not necessarily equivalent to the field number 
-         */
-        output.writeInt(fi.number);
-        output.writeByte(bits);
-        if (fi.isIndexed && !fi.omitNorms) {
-          // to allow null norm types we need to indicate if norms are written 
-          // only in RW case
-          output.writeByte((byte) (fi.getNormType() == null ? 0 : 1));
-        }
-      }
-    } finally {
-      output.close();
-    }
-  }
-  
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldsWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldsWriter.java
deleted file mode 100644
index 11aee56..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWFieldsWriter.java
+++ /dev/null
@@ -1,227 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.PostingsConsumer;
-import org.apache.lucene.codecs.TermStats;
-import org.apache.lucene.codecs.TermsConsumer;
-import org.apache.lucene.codecs.lucene3x.Lucene3xPostingsFormat;
-import org.apache.lucene.codecs.lucene3x.TermInfo;
-import org.apache.lucene.codecs.lucene40.Lucene40SkipListWriter;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-class PreFlexRWFieldsWriter extends FieldsConsumer {
-
-  private final TermInfosWriter termsOut;
-  private final IndexOutput freqOut;
-  private final IndexOutput proxOut;
-  private final Lucene40SkipListWriter skipListWriter;
-  private final int totalNumDocs;
-
-  public PreFlexRWFieldsWriter(SegmentWriteState state) throws IOException {
-    termsOut = new TermInfosWriter(state.directory,
-                                   state.segmentName,
-                                   state.fieldInfos,
-                                   state.termIndexInterval);
-
-    boolean success = false;
-    try {
-      final String freqFile = IndexFileNames.segmentFileName(state.segmentName, "", Lucene3xPostingsFormat.FREQ_EXTENSION);
-      freqOut = state.directory.createOutput(freqFile, state.context);
-      totalNumDocs = state.numDocs;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(termsOut);
-      }
-    }
-
-    success = false;
-    try {
-      if (state.fieldInfos.hasProx()) {
-        final String proxFile = IndexFileNames.segmentFileName(state.segmentName, "", Lucene3xPostingsFormat.PROX_EXTENSION);
-        proxOut = state.directory.createOutput(proxFile, state.context);
-      } else {
-        proxOut = null;
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(termsOut, freqOut);
-      }
-    }
-
-    skipListWriter = new Lucene40SkipListWriter(termsOut.skipInterval,
-                                               termsOut.maxSkipLevels,
-                                               totalNumDocs,
-                                               freqOut,
-                                               proxOut);
-    //System.out.println("\nw start seg=" + segment);
-  }
-
-  @Override
-  public TermsConsumer addField(FieldInfo field) throws IOException {
-    assert field.number != -1;
-    if (field.indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
-      throw new UnsupportedOperationException("this codec cannot index offsets");
-    }
-    //System.out.println("w field=" + field.name + " storePayload=" + field.storePayloads + " number=" + field.number);
-    return new PreFlexTermsWriter(field);
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.close(termsOut, freqOut, proxOut);
-  }
-
-  private class PreFlexTermsWriter extends TermsConsumer {
-    private final FieldInfo fieldInfo;
-    private final boolean omitTF;
-    private final boolean storePayloads;
-    
-    private final TermInfo termInfo = new TermInfo();
-    private final PostingsWriter postingsWriter = new PostingsWriter();
-
-    public PreFlexTermsWriter(FieldInfo fieldInfo) {
-      this.fieldInfo = fieldInfo;
-      omitTF = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY;
-      storePayloads = fieldInfo.storePayloads;
-    }
-
-    private class PostingsWriter extends PostingsConsumer {
-      private int lastDocID;
-      private int lastPayloadLength = -1;
-      private int lastPosition;
-      private int df;
-
-      public PostingsWriter reset() {
-        df = 0;
-        lastDocID = 0;
-        lastPayloadLength = -1;
-        return this;
-      }
-
-      @Override
-      public void startDoc(int docID, int termDocFreq) throws IOException {
-        //System.out.println("    w doc=" + docID);
-
-        final int delta = docID - lastDocID;
-        if (docID < 0 || (df > 0 && delta <= 0)) {
-          throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " )");
-        }
-
-        if ((++df % termsOut.skipInterval) == 0) {
-          skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength, false, 0);
-          skipListWriter.bufferSkip(df);
-        }
-
-        lastDocID = docID;
-
-        assert docID < totalNumDocs: "docID=" + docID + " totalNumDocs=" + totalNumDocs;
-
-        if (omitTF) {
-          freqOut.writeVInt(delta);
-        } else {
-          final int code = delta << 1;
-          if (termDocFreq == 1) {
-            freqOut.writeVInt(code|1);
-          } else {
-            freqOut.writeVInt(code);
-            freqOut.writeVInt(termDocFreq);
-          }
-        }
-        lastPosition = 0;
-      }
-
-      @Override
-      public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
-        assert proxOut != null;
-        assert startOffset == -1;
-        assert endOffset == -1;
-        //System.out.println("      w pos=" + position + " payl=" + payload);
-        final int delta = position - lastPosition;
-        lastPosition = position;
-
-        if (storePayloads) {
-          final int payloadLength = payload == null ? 0 : payload.length;
-          if (payloadLength != lastPayloadLength) {
-            //System.out.println("        write payload len=" + payloadLength);
-            lastPayloadLength = payloadLength;
-            proxOut.writeVInt((delta<<1)|1);
-            proxOut.writeVInt(payloadLength);
-          } else {
-            proxOut.writeVInt(delta << 1);
-          }
-          if (payloadLength > 0) {
-            proxOut.writeBytes(payload.bytes, payload.offset, payload.length);
-          }
-        } else {
-          proxOut.writeVInt(delta);
-        }
-      }
-
-      @Override
-      public void finishDoc() throws IOException {
-      }
-    }
-
-    @Override
-    public PostingsConsumer startTerm(BytesRef text) throws IOException {
-      //System.out.println("  w term=" + text.utf8ToString());
-      skipListWriter.resetSkip();
-      termInfo.freqPointer = freqOut.getFilePointer();
-      if (proxOut != null) {
-        termInfo.proxPointer = proxOut.getFilePointer();
-      }
-      return postingsWriter.reset();
-    }
-
-    @Override
-    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
-      if (stats.docFreq > 0) {
-        long skipPointer = skipListWriter.writeSkip(freqOut);
-        termInfo.docFreq = stats.docFreq;
-        termInfo.skipOffset = (int) (skipPointer - termInfo.freqPointer);
-        //System.out.println("  w finish term=" + text.utf8ToString() + " fnum=" + fieldInfo.number);
-        termsOut.add(fieldInfo.number,
-                     text,
-                     termInfo);
-      }
-    }
-
-    @Override
-    public void finish(long sumTotalTermCount, long sumDocFreq, int docCount) throws IOException {
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() throws IOException {
-      return BytesRef.getUTF8SortedAsUTF16Comparator();
-    }
-  }
-}
\ No newline at end of file
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWNormsConsumer.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWNormsConsumer.java
deleted file mode 100644
index e39a07c..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWNormsConsumer.java
+++ /dev/null
@@ -1,288 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Writes and Merges Lucene 3.x norms format
- * @lucene.experimental
- */
-class PreFlexRWNormsConsumer extends PerDocConsumer {
-  
-  /** norms header placeholder */
-  private static final byte[] NORMS_HEADER = new byte[]{'N','R','M',-1};
-  
-  /** Extension of norms file */
-  private static final String NORMS_EXTENSION = "nrm";
-  
-  /** Extension of separate norms file
-   * @deprecated */
-  @Deprecated
-  private static final String SEPARATE_NORMS_EXTENSION = "s";
-
-  private final Directory directory;
-
-  private final String segment;
-
-  private final IOContext context;
-
-  private NormsWriter writer;
-  
-  public PreFlexRWNormsConsumer(Directory directory, String segment, IOContext context){
-    this.directory = directory;
-    this.segment = segment;
-    this.context = context;
-  }
-
-  @Override
-  public void merge(MergeState mergeState) throws IOException {
-    getNormsWriter().merge(mergeState);
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (writer != null) {
-      writer.finish();
-    }
-  }
-  
-  @Override
-  protected boolean canMerge(FieldInfo info) {
-    return info.normsPresent();
-  }
-
-  @Override
-  protected Type getDocValuesType(FieldInfo info) {
-    return info.getNormType();
-  }
-
-  @Override
-  public DocValuesConsumer addValuesField(Type type, FieldInfo fieldInfo)
-      throws IOException {
-    if (type != Type.FIXED_INTS_8) {
-      throw new UnsupportedOperationException("Codec only supports single byte norm values. Type give: " + type);
-    }
-    return new Lucene3xNormsDocValuesConsumer(fieldInfo);
-  }
-  
-  class Lucene3xNormsDocValuesConsumer extends DocValuesConsumer {
-    // Holds all docID/norm pairs we've seen
-    private int[] docIDs = new int[1];
-    private byte[] norms = new byte[1];
-    private int upto;
-    private final FieldInfo fi;
-    
-    Lucene3xNormsDocValuesConsumer(FieldInfo fieldInfo) {
-      fi = fieldInfo;
-    }
-
-    @Override
-    public void finish(int docCount) throws IOException {
-      final NormsWriter normsWriter = getNormsWriter();
-      boolean success = false;
-      try {
-        int uptoDoc = 0;
-        normsWriter.setNumTotalDocs(docCount);
-        if (upto > 0) {
-          normsWriter.startField(fi);
-          int docID = 0;
-          for (; docID < docCount; docID++) {
-            if (uptoDoc < upto && docIDs[uptoDoc] == docID) {
-              normsWriter.writeNorm(norms[uptoDoc]);
-              uptoDoc++;
-            } else {
-              normsWriter.writeNorm((byte) 0);
-            }
-          }
-          // we should have consumed every norm
-          assert uptoDoc == upto;
-  
-        } else {
-          // Fill entire field with default norm:
-          normsWriter.startField(fi);
-          for (; upto < docCount; upto++)
-            normsWriter.writeNorm((byte) 0);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          normsWriter.abort();
-        }
-      }
-    }
-    
-    @Override
-    public void add(int docID, IndexableField docValue) throws IOException {
-      add(docID, docValue.numericValue().longValue());
-    }
-    
-    protected void add(int docID, long value) {
-      if (docIDs.length <= upto) {
-        assert docIDs.length == upto;
-        docIDs = ArrayUtil.grow(docIDs, 1 + upto);
-      }
-      if (norms.length <= upto) {
-        assert norms.length == upto;
-        norms = ArrayUtil.grow(norms, 1 + upto);
-      }
-      norms[upto] = (byte) value;
-      
-      docIDs[upto] = docID;
-      upto++;
-    }
-    
-    
-  }
-  
-  public NormsWriter getNormsWriter() throws IOException {
-    if (writer == null) {
-      writer = new NormsWriter(directory, segment, context);
-    }
-    return writer;
-  }
-  
-  private static class NormsWriter {
-    
-    private final IndexOutput output;
-    private int normCount = 0;
-    private int numTotalDocs = 0;
-    
-    public NormsWriter(Directory directory, String segment, IOContext context) throws IOException {
-      final String normsFileName = IndexFileNames.segmentFileName(segment, "", NORMS_EXTENSION);
-      boolean success = false;
-      IndexOutput out = null;
-      try {
-        out = directory.createOutput(normsFileName, context);
-        output = out;
-        output.writeBytes(NORMS_HEADER, 0, NORMS_HEADER.length);
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(out);
-        }
-      }
-      
-    }
-    
-    
-    public void setNumTotalDocs(int numTotalDocs) {
-      assert this.numTotalDocs == 0 || numTotalDocs == this.numTotalDocs;
-      this.numTotalDocs = numTotalDocs;
-    }
-    
-    public void startField(FieldInfo info) throws IOException {
-      assert info.omitNorms == false;
-      normCount++;
-    }
-    
-    public void writeNorm(byte norm) throws IOException {
-      output.writeByte(norm);
-    }
-    
-    public void abort() throws IOException {
-      IOUtils.close(output);
-    }
-    
-    public void finish() throws IOException {
-      IOUtils.close(output);
-      
-      if (4+normCount*(long)numTotalDocs != output.getFilePointer()) {
-        throw new IOException(".nrm file size mismatch: expected=" + (4+normCount*(long)numTotalDocs) + " actual=" + output.getFilePointer());
-      }
-    }
-    // TODO: we can actually use the defaul DV merge here and drop this specific stuff entirely
-    /** we override merge and bulk-merge norms when there are no deletions */
-    public void merge(MergeState mergeState) throws IOException {
-      int numMergedDocs = 0;
-      for (FieldInfo fi : mergeState.fieldInfos) {
-        if (fi.normsPresent()) {
-          startField(fi);
-          int numMergedDocsForField = 0;
-          for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
-            final int maxDoc = reader.reader.maxDoc();
-            byte[] normBuffer;
-            DocValues normValues = reader.reader.normValues(fi.name);
-            if (normValues == null) {
-              // Can be null if this segment doesn't have
-              // any docs with this field
-              normBuffer = new byte[maxDoc];
-              Arrays.fill(normBuffer, (byte)0);
-            } else {
-              Source directSource = normValues.getDirectSource();
-              assert directSource.hasArray();
-              normBuffer = (byte[]) directSource.getArray();
-            }
-            if (reader.liveDocs == null) {
-              //optimized case for segments without deleted docs
-              output.writeBytes(normBuffer, maxDoc);
-              numMergedDocsForField += maxDoc;
-            } else {
-              // this segment has deleted docs, so we have to
-              // check for every doc if it is deleted or not
-              final Bits liveDocs = reader.liveDocs;
-              for (int k = 0; k < maxDoc; k++) {
-                if (liveDocs.get(k)) {
-                  numMergedDocsForField++;
-                  output.writeByte(normBuffer[k]);
-                }
-              }
-            }
-            mergeState.checkAbort.work(maxDoc);
-          }
-          assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;
-          numMergedDocs = numMergedDocsForField;
-        }
-      }
-      this.numTotalDocs = numMergedDocs;
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      try {
-        if (writer != null) {
-          writer.abort();
-        }
-      } finally {
-        directory.deleteFile(IndexFileNames.segmentFileName(segment, "",
-            NORMS_EXTENSION));
-      }
-    } catch (IOException e) {
-      // ignore
-    }
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWNormsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWNormsFormat.java
deleted file mode 100644
index 9f37bef..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWNormsFormat.java
+++ /dev/null
@@ -1,35 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.codecs.lucene3x.Lucene3xNormsFormat;
-import org.apache.lucene.index.PerDocWriteState;
-
-/**
- * @lucene.internal
- * @lucene.experimental
- */
-public class PreFlexRWNormsFormat extends Lucene3xNormsFormat {
-
-  @Override
-  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new PreFlexRWNormsConsumer(state.directory, state.segmentName, state.context);
-  }
-
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWPostingsFormat.java
deleted file mode 100644
index 261ad9c..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWPostingsFormat.java
+++ /dev/null
@@ -1,76 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.lucene3x.Lucene3xFields;
-import org.apache.lucene.codecs.lucene3x.Lucene3xPostingsFormat;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** Codec, only for testing, that can write and read the
- *  pre-flex index format.
- *
- * @lucene.experimental
- */
-public class PreFlexRWPostingsFormat extends Lucene3xPostingsFormat {
-
-  public PreFlexRWPostingsFormat() {
-    // NOTE: we impersonate the PreFlex codec so that it can
-    // read the segments we write!
-  }
-  
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new PreFlexRWFieldsWriter(state);
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-
-    // Whenever IW opens readers, eg for merging, we have to
-    // keep terms order in UTF16:
-
-    return new Lucene3xFields(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.termsIndexDivisor) {
-      @Override
-      protected boolean sortTermsByUnicode() {
-        // We carefully peek into stack track above us: if
-        // we are part of a "merge", we must sort by UTF16:
-        boolean unicodeSortOrder = true;
-
-        StackTraceElement[] trace = new Exception().getStackTrace();
-        for (int i = 0; i < trace.length; i++) {
-          //System.out.println(trace[i].getClassName());
-          if ("merge".equals(trace[i].getMethodName())) {
-            unicodeSortOrder = false;
-            if (LuceneTestCase.VERBOSE) {
-              System.out.println("NOTE: PreFlexRW codec: forcing legacy UTF16 term sort order");
-            }
-            break;
-          }
-        }
-
-        return unicodeSortOrder;
-      }
-    };
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWSegmentInfosFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWSegmentInfosFormat.java
deleted file mode 100644
index 4e8c94f..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWSegmentInfosFormat.java
+++ /dev/null
@@ -1,33 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-import org.apache.lucene.codecs.SegmentInfosWriter;
-import org.apache.lucene.codecs.lucene3x.Lucene3xSegmentInfosFormat;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @lucene.experimental
- */
-public class PreFlexRWSegmentInfosFormat extends Lucene3xSegmentInfosFormat {
-  private final SegmentInfosWriter writer = new PreFlexRWSegmentInfosWriter();
-  
-  @Override
-  public SegmentInfosWriter getSegmentInfosWriter() {
-    return writer;
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWSegmentInfosWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWSegmentInfosWriter.java
deleted file mode 100644
index 9896a18..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWSegmentInfosWriter.java
+++ /dev/null
@@ -1,119 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.lucene.codecs.SegmentInfosWriter;
-import org.apache.lucene.codecs.lucene3x.Lucene3xCodec;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.ChecksumIndexOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FlushInfo;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * PreFlex implementation of {@link SegmentInfosWriter}.
- * @lucene.experimental
- */
-public class PreFlexRWSegmentInfosWriter extends SegmentInfosWriter {
-
-  @Override
-  public IndexOutput writeInfos(Directory dir, String segmentFileName, String codecID, SegmentInfos infos, IOContext context)
-          throws IOException {
-    IndexOutput out = createOutput(dir, segmentFileName, new IOContext(new FlushInfo(infos.size(), infos.totalDocCount())));
-    boolean success = false;
-    try {
-      out.writeInt(SegmentInfos.FORMAT_3_1); // write FORMAT
-      // we don't write a codec - this is 3.x
-      out.writeLong(infos.version);
-      out.writeInt(infos.counter); // write counter
-      out.writeInt(infos.size()); // write infos
-      for (SegmentInfo si : infos) {
-        writeInfo(out, si);
-      }
-      out.writeStringStringMap(infos.getUserData());
-      success = true;
-      return out;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  /** Save a single segment's info. */
-  private void writeInfo(IndexOutput output, SegmentInfo si) throws IOException {
-    // we are about to write this SI in 3.x format, dropping all codec information, etc.
-    // so it had better be a 3.x segment or you will get very confusing errors later.
-    assert si.getCodec() instanceof Lucene3xCodec : "broken test, trying to mix preflex with other codecs";
-    assert si.getDelCount() <= si.docCount: "delCount=" + si.getDelCount() + " docCount=" + si.docCount + " segment=" + si.name;
-    // Write the Lucene version that created this segment, since 3.1
-    output.writeString(si.getVersion());
-    output.writeString(si.name);
-    output.writeInt(si.docCount);
-    output.writeLong(si.getDelGen());
-
-    output.writeInt(si.getDocStoreOffset());
-    if (si.getDocStoreOffset() != -1) {
-      output.writeString(si.getDocStoreSegment());
-      output.writeByte((byte) (si.getDocStoreIsCompoundFile() ? 1:0));
-    }
-    // pre-4.0 indexes write a byte if there is a single norms file
-    output.writeByte((byte) 1);
-
-    Map<Integer,Long> normGen = si.getNormGen();
-    if (normGen == null) {
-      output.writeInt(SegmentInfo.NO);
-    } else {
-      output.writeInt(normGen.size());
-      for (Entry<Integer,Long> entry : normGen.entrySet()) {
-        output.writeLong(entry.getValue());
-      }
-    }
-
-    output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
-    output.writeInt(si.getDelCount());
-    output.writeByte((byte) (si.getHasProxInternal()));
-    output.writeStringStringMap(si.getDiagnostics());
-    output.writeByte((byte) (si.getHasVectorsInternal()));
-  }
-  
-  protected IndexOutput createOutput(Directory dir, String segmentFileName, IOContext context)
-      throws IOException {
-    IndexOutput plainOut = dir.createOutput(segmentFileName, context);
-    ChecksumIndexOutput out = new ChecksumIndexOutput(plainOut);
-    return out;
-  }
-
-  @Override
-  public void prepareCommit(IndexOutput segmentOutput) throws IOException {
-    ((ChecksumIndexOutput)segmentOutput).prepareCommit();
-  }
-
-  @Override
-  public void finishCommit(IndexOutput out) throws IOException {
-    ((ChecksumIndexOutput)out).finishCommit();
-    out.close();
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWStoredFieldsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWStoredFieldsFormat.java
deleted file mode 100644
index 76b1a22..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWStoredFieldsFormat.java
+++ /dev/null
@@ -1,17 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.codecs.lucene3x.Lucene3xStoredFieldsFormat;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-public class PreFlexRWStoredFieldsFormat extends Lucene3xStoredFieldsFormat {
-
-  @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    return new PreFlexRWStoredFieldsWriter(directory, segment, context);
-  }
-  
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWStoredFieldsWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWStoredFieldsWriter.java
deleted file mode 100644
index 7888b79..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWStoredFieldsWriter.java
+++ /dev/null
@@ -1,156 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-/**
- * Copyright 2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not
- * use this file except in compliance with the License. You may obtain a copy of
- * the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.codecs.lucene3x.Lucene3xStoredFieldsReader;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/** @lucene.experimental */
-public final class PreFlexRWStoredFieldsWriter extends StoredFieldsWriter {
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput fieldsStream;
-  private IndexOutput indexStream;
-
-  public PreFlexRWStoredFieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    assert directory != null;
-    this.directory = directory;
-    this.segment = segment;
-
-    boolean success = false;
-    try {
-      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene3xStoredFieldsReader.FIELDS_EXTENSION), context);
-      indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene3xStoredFieldsReader.FIELDS_INDEX_EXTENSION), context);
-
-      fieldsStream.writeInt(Lucene3xStoredFieldsReader.FORMAT_CURRENT);
-      indexStream.writeInt(Lucene3xStoredFieldsReader.FORMAT_CURRENT);
-
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
-
-  // Writes the contents of buffer into the fields stream
-  // and adds a new entry for this document into the index
-  // stream.  This assumes the buffer was already written
-  // in the correct fields format.
-  public void startDocument(int numStoredFields) throws IOException {
-    indexStream.writeLong(fieldsStream.getFilePointer());
-    fieldsStream.writeVInt(numStoredFields);
-  }
-
-  public void close() throws IOException {
-    try {
-      IOUtils.close(fieldsStream, indexStream);
-    } finally {
-      fieldsStream = indexStream = null;
-    }
-  }
-
-  public void abort() {
-    try {
-      close();
-    } catch (IOException ignored) {}
-    IOUtils.deleteFilesIgnoringExceptions(directory,
-        IndexFileNames.segmentFileName(segment, "", Lucene3xStoredFieldsReader.FIELDS_EXTENSION),
-        IndexFileNames.segmentFileName(segment, "", Lucene3xStoredFieldsReader.FIELDS_INDEX_EXTENSION));
-  }
-
-  public void writeField(FieldInfo info, IndexableField field) throws IOException {
-    fieldsStream.writeVInt(info.number);
-    int bits = 0;
-    final BytesRef bytes;
-    final String string;
-    // TODO: maybe a field should serialize itself?
-    // this way we don't bake into indexer all these
-    // specific encodings for different fields?  and apps
-    // can customize...
-
-    Number number = field.numericValue();
-    if (number != null) {
-      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
-        bits |= Lucene3xStoredFieldsReader.FIELD_IS_NUMERIC_INT;
-      } else if (number instanceof Long) {
-        bits |= Lucene3xStoredFieldsReader.FIELD_IS_NUMERIC_LONG;
-      } else if (number instanceof Float) {
-        bits |= Lucene3xStoredFieldsReader.FIELD_IS_NUMERIC_FLOAT;
-      } else if (number instanceof Double) {
-        bits |= Lucene3xStoredFieldsReader.FIELD_IS_NUMERIC_DOUBLE;
-      } else {
-        throw new IllegalArgumentException("cannot store numeric type " + number.getClass());
-      }
-      string = null;
-      bytes = null;
-    } else {
-      bytes = field.binaryValue();
-      if (bytes != null) {
-        bits |= Lucene3xStoredFieldsReader.FIELD_IS_BINARY;
-        string = null;
-      } else {
-        string = field.stringValue();
-        if (string == null) {
-          throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
-        }
-      }
-    }
-
-    fieldsStream.writeByte((byte) bits);
-
-    if (bytes != null) {
-      fieldsStream.writeVInt(bytes.length);
-      fieldsStream.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-    } else if (string != null) {
-      fieldsStream.writeString(field.stringValue());
-    } else {
-      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
-        fieldsStream.writeInt(number.intValue());
-      } else if (number instanceof Long) {
-        fieldsStream.writeLong(number.longValue());
-      } else if (number instanceof Float) {
-        fieldsStream.writeInt(Float.floatToIntBits(number.floatValue()));
-      } else if (number instanceof Double) {
-        fieldsStream.writeLong(Double.doubleToLongBits(number.doubleValue()));
-      } else {
-        assert false;
-      }
-    }
-  }
-
-  @Override
-  public void finish(int numDocs) throws IOException {
-    if (4+((long) numDocs)*8 != indexStream.getFilePointer())
-      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
-      // we detect that the bug has struck, here, and
-      // throw an exception to prevent the corruption from
-      // entering the index.  See LUCENE-1282 for
-      // details.
-      throw new RuntimeException("fdx size mismatch: docCount is " + numDocs + " but fdx file size is " + indexStream.getFilePointer() + " file=" + indexStream.toString() + "; now aborting this merge to prevent index corruption");
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWTermVectorsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWTermVectorsFormat.java
deleted file mode 100644
index 8646f97..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWTermVectorsFormat.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.TermVectorsReader;
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.codecs.lucene3x.Lucene3xTermVectorsFormat;
-import org.apache.lucene.codecs.lucene3x.Lucene3xTermVectorsReader;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.LuceneTestCase;
-
-public class PreFlexRWTermVectorsFormat extends Lucene3xTermVectorsFormat {
-
-  @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    return new PreFlexRWTermVectorsWriter(directory, segment, context);
-  }
-
-  @Override
-  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
-    return new Lucene3xTermVectorsReader(directory, segmentInfo, fieldInfos, context) {
-      @Override
-      protected boolean sortTermsByUnicode() {
-        // We carefully peek into stack track above us: if
-        // we are part of a "merge", we must sort by UTF16:
-        boolean unicodeSortOrder = true;
-
-        StackTraceElement[] trace = new Exception().getStackTrace();
-        for (int i = 0; i < trace.length; i++) {
-          //System.out.println(trace[i].getClassName());
-          if ("merge".equals(trace[i].getMethodName())) {
-            unicodeSortOrder = false;
-            if (LuceneTestCase.VERBOSE) {
-              System.out.println("NOTE: PreFlexRW codec: forcing legacy UTF16 vector term sort order");
-            }
-            break;
-          }
-        }
-
-        return unicodeSortOrder;
-      }
-    };
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWTermVectorsWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWTermVectorsWriter.java
deleted file mode 100644
index b53fee5..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWTermVectorsWriter.java
+++ /dev/null
@@ -1,221 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.codecs.lucene3x.Lucene3xTermVectorsReader;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-public final class PreFlexRWTermVectorsWriter extends TermVectorsWriter {
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput tvx = null, tvd = null, tvf = null;
-
-  public PreFlexRWTermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    this.directory = directory;
-    this.segment = segment;
-    boolean success = false;
-    try {
-      // Open files for TermVector storage
-      tvx = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_INDEX_EXTENSION), context);
-      tvx.writeInt(Lucene3xTermVectorsReader.FORMAT_CURRENT);
-      tvd = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_DOCUMENTS_EXTENSION), context);
-      tvd.writeInt(Lucene3xTermVectorsReader.FORMAT_CURRENT);
-      tvf = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_FIELDS_EXTENSION), context);
-      tvf.writeInt(Lucene3xTermVectorsReader.FORMAT_CURRENT);
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
- 
-  @Override
-  public void startDocument(int numVectorFields) throws IOException {
-    lastFieldName = null;
-    this.numVectorFields = numVectorFields;
-    tvx.writeLong(tvd.getFilePointer());
-    tvx.writeLong(tvf.getFilePointer());
-    tvd.writeVInt(numVectorFields);
-    fieldCount = 0;
-    fps = ArrayUtil.grow(fps, numVectorFields);
-  }
-  
-  private long fps[] = new long[10]; // pointers to the tvf before writing each field 
-  private int fieldCount = 0;        // number of fields we have written so far for this document
-  private int numVectorFields = 0;   // total number of fields we will write for this document
-  private String lastFieldName;
-
-  @Override
-  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets) throws IOException {
-    assert lastFieldName == null || info.name.compareTo(lastFieldName) > 0: "fieldName=" + info.name + " lastFieldName=" + lastFieldName;
-    lastFieldName = info.name;
-    this.positions = positions;
-    this.offsets = offsets;
-    lastTerm.length = 0;
-    fps[fieldCount++] = tvf.getFilePointer();
-    tvd.writeVInt(info.number);
-    tvf.writeVInt(numTerms);
-    byte bits = 0x0;
-    if (positions)
-      bits |= Lucene3xTermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR;
-    if (offsets)
-      bits |= Lucene3xTermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR;
-    tvf.writeByte(bits);
-    
-    assert fieldCount <= numVectorFields;
-    if (fieldCount == numVectorFields) {
-      // last field of the document
-      // this is crazy because the file format is crazy!
-      for (int i = 1; i < fieldCount; i++) {
-        tvd.writeVLong(fps[i] - fps[i-1]);
-      }
-    }
-  }
-  
-  private final BytesRef lastTerm = new BytesRef(10);
-
-  // NOTE: we override addProx, so we don't need to buffer when indexing.
-  // we also don't buffer during bulk merges.
-  private int offsetStartBuffer[] = new int[10];
-  private int offsetEndBuffer[] = new int[10];
-  private int offsetIndex = 0;
-  private int offsetFreq = 0;
-  private boolean positions = false;
-  private boolean offsets = false;
-
-  @Override
-  public void startTerm(BytesRef term, int freq) throws IOException {
-    final int prefix = StringHelper.bytesDifference(lastTerm, term);
-    final int suffix = term.length - prefix;
-    tvf.writeVInt(prefix);
-    tvf.writeVInt(suffix);
-    tvf.writeBytes(term.bytes, term.offset + prefix, suffix);
-    tvf.writeVInt(freq);
-    lastTerm.copyBytes(term);
-    lastPosition = lastOffset = 0;
-    
-    if (offsets && positions) {
-      // we might need to buffer if its a non-bulk merge
-      offsetStartBuffer = ArrayUtil.grow(offsetStartBuffer, freq);
-      offsetEndBuffer = ArrayUtil.grow(offsetEndBuffer, freq);
-      offsetIndex = 0;
-      offsetFreq = freq;
-    }
-  }
-
-  int lastPosition = 0;
-  int lastOffset = 0;
-
-  @Override
-  public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
-    // TODO: technically we could just copy bytes and not re-encode if we knew the length...
-    if (positions != null) {
-      for (int i = 0; i < numProx; i++) {
-        tvf.writeVInt(positions.readVInt());
-      }
-    }
-    
-    if (offsets != null) {
-      for (int i = 0; i < numProx; i++) {
-        tvf.writeVInt(offsets.readVInt());
-        tvf.writeVInt(offsets.readVInt());
-      }
-    }
-  }
-
-  @Override
-  public void addPosition(int position, int startOffset, int endOffset) throws IOException {
-    if (positions && offsets) {
-      // write position delta
-      tvf.writeVInt(position - lastPosition);
-      lastPosition = position;
-      
-      // buffer offsets
-      offsetStartBuffer[offsetIndex] = startOffset;
-      offsetEndBuffer[offsetIndex] = endOffset;
-      offsetIndex++;
-      
-      // dump buffer if we are done
-      if (offsetIndex == offsetFreq) {
-        for (int i = 0; i < offsetIndex; i++) {
-          tvf.writeVInt(offsetStartBuffer[i] - lastOffset);
-          tvf.writeVInt(offsetEndBuffer[i] - offsetStartBuffer[i]);
-          lastOffset = offsetEndBuffer[i];
-        }
-      }
-    } else if (positions) {
-      // write position delta
-      tvf.writeVInt(position - lastPosition);
-      lastPosition = position;
-    } else if (offsets) {
-      // write offset deltas
-      tvf.writeVInt(startOffset - lastOffset);
-      tvf.writeVInt(endOffset - startOffset);
-      lastOffset = endOffset;
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      close();
-    } catch (IOException ignored) {}
-    IOUtils.deleteFilesIgnoringExceptions(directory, IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_INDEX_EXTENSION),
-        IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_DOCUMENTS_EXTENSION),
-        IndexFileNames.segmentFileName(segment, "", Lucene3xTermVectorsReader.VECTORS_FIELDS_EXTENSION));
-  }
-  
-  @Override
-  public void finish(int numDocs) throws IOException {
-    if (4+((long) numDocs)*16 != tvx.getFilePointer())
-      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
-      // we detect that the bug has struck, here, and
-      // throw an exception to prevent the corruption from
-      // entering the index.  See LUCENE-1282 for
-      // details.
-      throw new RuntimeException("tvx size mismatch: mergedDocs is " + numDocs + " but tvx size is " + tvx.getFilePointer() + " file=" + tvx.toString() + "; now aborting this merge to prevent index corruption");
-  }
-
-  /** Close all streams. */
-  @Override
-  public void close() throws IOException {
-    // make an effort to close all streams we can but remember and re-throw
-    // the first exception encountered in this process
-    IOUtils.close(tvx, tvd, tvf);
-    tvx = tvd = tvf = null;
-  }
-  
-  @Override
-  public Comparator<BytesRef> getComparator() throws IOException {
-    return BytesRef.getUTF8SortedAsUTF16Comparator();
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/TermInfosWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/TermInfosWriter.java
deleted file mode 100644
index 56d3d93..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/TermInfosWriter.java
+++ /dev/null
@@ -1,276 +0,0 @@
-package org.apache.lucene.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.codecs.lucene3x.Lucene3xPostingsFormat;
-import org.apache.lucene.codecs.lucene3x.TermInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.UnicodeUtil;
-
-
-/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
-  Directory.  A TermInfos can be written once, in order.  */
-
-final class TermInfosWriter implements Closeable {
-  /** The file format version, a negative number. */
-  public static final int FORMAT = -3;
-
-  // Changed strings to true utf8 with length-in-bytes not
-  // length-in-chars
-  public static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = -4;
-
-  // NOTE: always change this if you switch to a new format!
-  public static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
-
-  private FieldInfos fieldInfos;
-  private IndexOutput output;
-  private TermInfo lastTi = new TermInfo();
-  private long size;
-
-  // TODO: the default values for these two parameters should be settable from
-  // IndexWriter.  However, once that's done, folks will start setting them to
-  // ridiculous values and complaining that things don't work well, as with
-  // mergeFactor.  So, let's wait until a number of folks find that alternate
-  // values work better.  Note that both of these values are stored in the
-  // segment, so that it's safe to change these w/o rebuilding all indexes.
-
-  /** Expert: The fraction of terms in the "dictionary" which should be stored
-   * in RAM.  Smaller values use more memory, but make searching slightly
-   * faster, while larger values use less memory and make searching slightly
-   * slower.  Searching is typically not dominated by dictionary lookup, so
-   * tweaking this is rarely useful.*/
-  int indexInterval = 128;
-
-  /** Expert: The fraction of {@link TermDocs} entries stored in skip tables,
-   * used to accelerate {@link TermDocs#skipTo(int)}.  Larger values result in
-   * smaller indexes, greater acceleration, but fewer accelerable cases, while
-   * smaller values result in bigger indexes, less acceleration and more
-   * accelerable cases. More detailed experiments would be useful here. */
-  int skipInterval = 16;
-  
-  /** Expert: The maximum number of skip levels. Smaller values result in 
-   * slightly smaller indexes, but slower skipping in big posting lists.
-   */
-  int maxSkipLevels = 10;
-
-  private long lastIndexPointer;
-  private boolean isIndex;
-  private final BytesRef lastTerm = new BytesRef();
-  private int lastFieldNumber = -1;
-
-  private TermInfosWriter other;
-
-  TermInfosWriter(Directory directory, String segment, FieldInfos fis,
-                  int interval)
-       throws IOException {
-    initialize(directory, segment, fis, interval, false);
-    boolean success = false;
-    try {
-      other = new TermInfosWriter(directory, segment, fis, interval, true);
-      other.other = this;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(output);
-
-        try {
-          directory.deleteFile(IndexFileNames.segmentFileName(segment, "",
-              (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
-                  : Lucene3xPostingsFormat.TERMS_EXTENSION)));
-        } catch (IOException ignored) {
-        }
-      }
-    }
-  }
-
-  private TermInfosWriter(Directory directory, String segment, FieldInfos fis,
-                          int interval, boolean isIndex) throws IOException {
-    initialize(directory, segment, fis, interval, isIndex);
-  }
-
-  private void initialize(Directory directory, String segment, FieldInfos fis,
-                          int interval, boolean isi) throws IOException {
-    indexInterval = interval;
-    fieldInfos = fis;
-    isIndex = isi;
-    output = directory.createOutput(IndexFileNames.segmentFileName(segment, "",
-        (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
-            : Lucene3xPostingsFormat.TERMS_EXTENSION)), IOContext.DEFAULT);
-    boolean success = false;
-    try {
-      output.writeInt(FORMAT_CURRENT);              // write format
-      output.writeLong(0);                          // leave space for size
-      output.writeInt(indexInterval);               // write indexInterval
-      output.writeInt(skipInterval);                // write skipInterval
-      output.writeInt(maxSkipLevels);               // write maxSkipLevels
-      assert initUTF16Results();
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(output);
-
-        try {
-          directory.deleteFile(IndexFileNames.segmentFileName(segment, "",
-              (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
-                  : Lucene3xPostingsFormat.TERMS_EXTENSION)));
-        } catch (IOException ignored) {
-        }
-      }
-    }
-  }
-
-  // Currently used only by assert statements
-  CharsRef utf16Result1;
-  CharsRef utf16Result2;
-  private final BytesRef scratchBytes = new BytesRef();
-
-  // Currently used only by assert statements
-  private boolean initUTF16Results() {
-    utf16Result1 = new CharsRef(10);
-    utf16Result2 = new CharsRef(10);
-    return true;
-  }
-
-  // Currently used only by assert statement
-  private int compareToLastTerm(int fieldNumber, BytesRef term) {
-
-    if (lastFieldNumber != fieldNumber) {
-      final int cmp = fieldInfos.fieldName(lastFieldNumber).compareTo(fieldInfos.fieldName(fieldNumber));
-      // If there is a field named "" (empty string) then we
-      // will get 0 on this comparison, yet, it's "OK".  But
-      // it's not OK if two different field numbers map to
-      // the same name.
-      if (cmp != 0 || lastFieldNumber != -1)
-        return cmp;
-    }
-
-    scratchBytes.copyBytes(term);
-    assert lastTerm.offset == 0;
-    UnicodeUtil.UTF8toUTF16(lastTerm.bytes, 0, lastTerm.length, utf16Result1);
-
-    assert scratchBytes.offset == 0;
-    UnicodeUtil.UTF8toUTF16(scratchBytes.bytes, 0, scratchBytes.length, utf16Result2);
-
-    final int len;
-    if (utf16Result1.length < utf16Result2.length)
-      len = utf16Result1.length;
-    else
-      len = utf16Result2.length;
-
-    for(int i=0;i<len;i++) {
-      final char ch1 = utf16Result1.chars[i];
-      final char ch2 = utf16Result2.chars[i];
-      if (ch1 != ch2)
-        return ch1-ch2;
-    }
-    if (utf16Result1.length == 0 && lastFieldNumber == -1) {
-      // If there is a field named "" (empty string) with a term text of "" (empty string) then we
-      // will get 0 on this comparison, yet, it's "OK". 
-      return -1;
-    }
-    return utf16Result1.length - utf16Result2.length;
-  }
-
-  /** Adds a new <<fieldNumber, termBytes>, TermInfo> pair to the set.
-    Term must be lexicographically greater than all previous Terms added.
-    TermInfo pointers must be positive and greater than all previous.*/
-  public void add(int fieldNumber, BytesRef term, TermInfo ti)
-    throws IOException {
-
-    assert compareToLastTerm(fieldNumber, term) < 0 ||
-      (isIndex && term.length == 0 && lastTerm.length == 0) :
-      "Terms are out of order: field=" + fieldInfos.fieldName(fieldNumber) + " (number " + fieldNumber + ")" +
-        " lastField=" + fieldInfos.fieldName(lastFieldNumber) + " (number " + lastFieldNumber + ")" +
-        " text=" + term.utf8ToString() + " lastText=" + lastTerm.utf8ToString();
-
-    assert ti.freqPointer >= lastTi.freqPointer: "freqPointer out of order (" + ti.freqPointer + " < " + lastTi.freqPointer + ")";
-    assert ti.proxPointer >= lastTi.proxPointer: "proxPointer out of order (" + ti.proxPointer + " < " + lastTi.proxPointer + ")";
-
-    if (!isIndex && size % indexInterval == 0)
-      other.add(lastFieldNumber, lastTerm, lastTi);                      // add an index term
-
-    writeTerm(fieldNumber, term);                        // write term
-
-    output.writeVInt(ti.docFreq);                       // write doc freq
-    output.writeVLong(ti.freqPointer - lastTi.freqPointer); // write pointers
-    output.writeVLong(ti.proxPointer - lastTi.proxPointer);
-
-    if (ti.docFreq >= skipInterval) {
-      output.writeVInt(ti.skipOffset);
-    }
-
-    if (isIndex) {
-      output.writeVLong(other.output.getFilePointer() - lastIndexPointer);
-      lastIndexPointer = other.output.getFilePointer(); // write pointer
-    }
-
-    lastFieldNumber = fieldNumber;
-    lastTi.set(ti);
-    size++;
-  }
-
-  private void writeTerm(int fieldNumber, BytesRef term)
-       throws IOException {
-
-    //System.out.println("  tiw.write field=" + fieldNumber + " term=" + term.utf8ToString());
-
-    // TODO: UTF16toUTF8 could tell us this prefix
-    // Compute prefix in common with last term:
-    int start = 0;
-    final int limit = term.length < lastTerm.length ? term.length : lastTerm.length;
-    while(start < limit) {
-      if (term.bytes[start+term.offset] != lastTerm.bytes[start+lastTerm.offset])
-        break;
-      start++;
-    }
-
-    final int length = term.length - start;
-    output.writeVInt(start);                     // write shared prefix length
-    output.writeVInt(length);                  // write delta length
-    output.writeBytes(term.bytes, start+term.offset, length);  // write delta bytes
-    output.writeVInt(fieldNumber); // write field num
-    lastTerm.copyBytes(term);
-  }
-
-  /** Called to complete TermInfos creation. */
-  public void close() throws IOException {
-    try {
-      output.seek(4);          // write size after format
-      output.writeLong(size);
-    } finally {
-      try {
-        output.close();
-      } finally {
-        if (!isIndex) {
-          other.close();
-        }
-      }
-    }
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/util/LuceneTestCase.java b/lucene/src/test-framework/java/org/apache/lucene/util/LuceneTestCase.java
index d213559..eb7143d 100644
--- a/lucene/src/test-framework/java/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/src/test-framework/java/org/apache/lucene/util/LuceneTestCase.java
@@ -36,8 +36,8 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.appending.AppendingCodec;
+import org.apache.lucene.codecs.lucene3x.PreFlexRWCodec;
 import org.apache.lucene.codecs.lucene40.Lucene40Codec;
-import org.apache.lucene.codecs.preflexrw.PreFlexRWCodec;
 import org.apache.lucene.codecs.simpletext.SimpleTextCodec;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
diff --git a/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.codecs.Codec b/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.codecs.Codec
index be7693b..3bf4313 100644
--- a/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.codecs.Codec
+++ b/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.codecs.Codec
@@ -13,4 +13,4 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-org.apache.lucene.codecs.preflexrw.PreFlexRWCodec
+org.apache.lucene.codecs.lucene3x.PreFlexRWCodec
diff --git a/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestImpersonation.java b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestImpersonation.java
index a0d486a..f88894c 100644
--- a/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestImpersonation.java
+++ b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestImpersonation.java
@@ -18,7 +18,7 @@ package org.apache.lucene.codecs.lucene3x;
  */
 
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.preflexrw.PreFlexRWCodec;
+import org.apache.lucene.codecs.lucene3x.PreFlexRWCodec;
 import org.apache.lucene.util.LuceneTestCase;
 
 /**
diff --git a/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestSurrogates.java b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestSurrogates.java
index 65b2803..4350627 100644
--- a/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestSurrogates.java
+++ b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestSurrogates.java
@@ -18,7 +18,7 @@ package org.apache.lucene.codecs.lucene3x;
  */
 
 import org.apache.lucene.store.*;
-import org.apache.lucene.codecs.preflexrw.PreFlexRWCodec;
+import org.apache.lucene.codecs.lucene3x.PreFlexRWCodec;
 import org.apache.lucene.document.*;
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.index.*;
diff --git a/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestTermInfosReaderIndex.java b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestTermInfosReaderIndex.java
index 7756a11..b8f71eb 100644
--- a/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestTermInfosReaderIndex.java
+++ b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestTermInfosReaderIndex.java
@@ -28,9 +28,9 @@ import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.codecs.lucene3x.Lucene3xPostingsFormat;
+import org.apache.lucene.codecs.lucene3x.PreFlexRWCodec;
 import org.apache.lucene.codecs.lucene3x.SegmentTermEnum;
 import org.apache.lucene.codecs.lucene3x.TermInfosReaderIndex;
-import org.apache.lucene.codecs.preflexrw.PreFlexRWCodec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.CorruptIndexException;
diff --git a/lucene/src/test/org/apache/lucene/index/TestMixedCodecs.java b/lucene/src/test/org/apache/lucene/index/TestMixedCodecs.java
index 11005ba..7d6cbf9 100644
--- a/lucene/src/test/org/apache/lucene/index/TestMixedCodecs.java
+++ b/lucene/src/test/org/apache/lucene/index/TestMixedCodecs.java
@@ -22,7 +22,7 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.preflexrw.PreFlexRWCodec;
+import org.apache.lucene.codecs.lucene3x.PreFlexRWCodec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.store.Directory;

