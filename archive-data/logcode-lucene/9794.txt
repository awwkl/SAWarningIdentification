GitDiffStart: b931c1f43aea832af3927e1e815a46769357ad4e | Fri Jan 18 15:13:54 2013 +0000
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
index 46a602b..b893558 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
@@ -27,7 +27,7 @@ import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.NormsFormat;
 import org.apache.lucene.codecs.StoredFieldsFormat;
 import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene41.Lucene41SimpleNormsFormat;
+import org.apache.lucene.codecs.lucene41.Lucene41NormsFormat;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 
 /**
@@ -97,7 +97,7 @@ public final class Lucene40Codec extends Codec {
   }
 
   // nocommit need a read-only Lucene40SimpleNormsFormat:
-  private final NormsFormat simpleNormsFormat = new Lucene41SimpleNormsFormat();
+  private final NormsFormat simpleNormsFormat = new Lucene41NormsFormat();
 
   @Override
   public NormsFormat normsFormat() {
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
index ebf3aaa..81fd38d 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
@@ -130,7 +130,7 @@ public class Lucene41Codec extends Codec {
   // nocommit
   private final DocValuesFormat defaultDVFormat = DocValuesFormat.forName("Lucene41");
 
-  private final NormsFormat simpleNormsFormat = new Lucene41SimpleNormsFormat();
+  private final NormsFormat simpleNormsFormat = new Lucene41NormsFormat();
 
   @Override
   public NormsFormat normsFormat() {
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41DocValuesConsumer.java
new file mode 100644
index 0000000..e992e32
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41DocValuesConsumer.java
@@ -0,0 +1,246 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.FST.INPUT_TYPE;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util;
+import org.apache.lucene.util.packed.PackedInts;
+import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
+
+/**
+ * Writes numbers one of two ways:
+ * 1. packed ints as deltas from minValue
+ * 2. packed ints as ordinals to a table (if the number of values is small, e.g. <= 256)
+ * 
+ * the latter is typically much smaller with lucene's sims, as only some byte values are used,
+ * but its often a nonlinear mapping, especially if you dont use crazy boosts.
+ */
+class Lucene41DocValuesConsumer extends DocValuesConsumer {
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+  
+  static final byte NUMBER = 0;
+  static final byte BYTES = 1;
+  static final byte FST = 2;
+  
+  final IndexOutput data, meta;
+  
+  Lucene41DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    boolean success = false;
+    try {
+      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+      data = state.directory.createOutput(dataName, state.context);
+      CodecUtil.writeHeader(data, dataCodec, VERSION_CURRENT);
+      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+      meta = state.directory.createOutput(metaName, state.context);
+      CodecUtil.writeHeader(meta, metaCodec, VERSION_CURRENT);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this);
+      }
+    }
+  }
+  
+  @Override
+  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
+    meta.writeVInt(field.number);
+    meta.writeByte(NUMBER);
+    meta.writeLong(data.getFilePointer());
+    long minValue = Long.MAX_VALUE;
+    long maxValue = Long.MIN_VALUE;
+    int count = 0;
+    // TODO: more efficient?
+    HashSet<Long> uniqueValues = new HashSet<Long>();
+    for(Number nv : values) {
+      long v = nv.longValue();
+      minValue = Math.min(minValue, v);
+      maxValue = Math.max(maxValue, v);
+      count++;
+      if (uniqueValues != null) {
+        if (uniqueValues.add(v)) {
+          if (uniqueValues.size() > 256) {
+            uniqueValues = null;
+          }
+        }
+      }
+    }
+
+    long delta = maxValue - minValue;
+    final int bitsPerValue;
+    if (delta < 0) {
+      bitsPerValue = 64;
+      meta.writeByte((byte)0); // delta-compressed
+    } else if (uniqueValues != null && PackedInts.bitsRequired(uniqueValues.size()-1) < PackedInts.bitsRequired(delta)) {
+      // smaller to tableize
+      bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);
+      minValue = 0; // we will write indexes into the table instead of values
+      meta.writeByte((byte)1); // table-compressed
+      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
+      final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();
+      data.writeVInt(decode.length);
+      for (int i = 0; i < decode.length; i++) {
+        data.writeLong(decode[i]);
+        encode.put(decode[i], i);
+      }
+      final Iterable<Number> original = values;
+      values = new Iterable<Number>() {
+        @Override
+        public Iterator<Number> iterator() {
+          final Iterator<Number> inner = original.iterator();
+          return new Iterator<Number>() {
+            @Override
+            public boolean hasNext() {
+              return inner.hasNext();
+            }
+
+            @Override
+            public Number next() {
+              return encode.get(inner.next());
+            }
+
+            @Override
+            public void remove() { throw new UnsupportedOperationException(); }
+          };
+        }
+      };
+    } else {
+      bitsPerValue = PackedInts.bitsRequired(delta);
+      meta.writeByte((byte)0); // delta-compressed
+    }
+
+    data.writeLong(minValue);
+
+    FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(count, bitsPerValue, PackedInts.COMPACT);   
+    final PackedInts.Writer writer = PackedInts.getWriter(data, count, formatAndBits.bitsPerValue, 0);
+    for(Number nv : values) {
+      writer.add(nv.longValue() - minValue);
+    }
+    writer.finish();
+  }
+  
+  @Override
+  public void close() throws IOException {
+    // nocommit: just write this to a RAMfile or something and flush it here, with #fields first.
+    // this meta is a tiny file so this hurts nobody
+    boolean success = false;
+    try {
+      if (meta != null) {
+        meta.writeVInt(-1);
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(data, meta);
+      } else {
+        IOUtils.closeWhileHandlingException(data, meta);
+      }
+    }
+  }
+
+  @Override
+  public void addBinaryField(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
+    // write the byte[] data
+    meta.writeVInt(field.number);
+    meta.writeByte(BYTES);
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    final long startFP = data.getFilePointer();
+    for(BytesRef v : values) {
+      minLength = Math.min(minLength, v.length);
+      maxLength = Math.max(maxLength, v.length);
+      data.writeBytes(v.bytes, v.offset, v.length);
+    }
+    meta.writeLong(startFP);
+    meta.writeLong(data.getFilePointer() - startFP);
+    meta.writeVInt(minLength);
+    meta.writeVInt(maxLength);
+    
+    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
+    // otherwise, we need to record the length fields...
+    // TODO: make this more efficient. this is just as inefficient as 4.0 codec.... we can do much better.
+    if (minLength != maxLength) {
+      addNumericField(field, new Iterable<Number>() {
+        @Override
+        public Iterator<Number> iterator() {
+          final Iterator<BytesRef> inner = values.iterator();
+          return new Iterator<Number>() {
+            long addr = 0;
+
+            @Override
+            public boolean hasNext() {
+              return inner.hasNext();
+            }
+
+            @Override
+            public Number next() {
+              BytesRef b = inner.next();
+              addr += b.length;
+              return Long.valueOf(addr);
+            }
+
+            @Override
+            public void remove() { throw new UnsupportedOperationException(); } 
+          };
+        }
+      });
+    }
+  }
+
+  @Override
+  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
+    // write the ordinals as numerics
+    addNumericField(field, docToOrd);
+    
+    // write the values as FST
+    meta.writeVInt(field.number);
+    meta.writeByte(FST);
+    meta.writeLong(data.getFilePointer());
+    PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton(true);
+    Builder<Long> builder = new Builder<Long>(INPUT_TYPE.BYTE1, outputs);
+    IntsRef scratch = new IntsRef();
+    long ord = 0;
+    for (BytesRef v : values) {
+      builder.add(Util.toIntsRef(v, scratch), ord);
+      ord++;
+    }
+    FST<Long> fst = builder.finish();
+    fst.save(data);
+    meta.writeVInt((int)ord);
+  }
+  
+  // nocommit: can/should we make override merge + make it smarter to pull the values 
+  // directly from disk for fields that arent already loaded up in ram?
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41DocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41DocValuesFormat.java
new file mode 100644
index 0000000..8556ed6
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41DocValuesFormat.java
@@ -0,0 +1,48 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+public class Lucene41DocValuesFormat extends DocValuesFormat {
+
+  public Lucene41DocValuesFormat() {
+    super("Lucene41");
+  }
+
+  @Override
+  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return new Lucene41DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+  
+  @Override
+  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new Lucene41DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+  
+  private static final String DATA_CODEC = "Lucene41DocValuesData";
+  private static final String DATA_EXTENSION = "dvd";
+  private static final String METADATA_CODEC = "Lucene41DocValuesMetadata";
+  private static final String METADATA_EXTENSION = "dvm";
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41DocValuesProducer.java
new file mode 100644
index 0000000..795d870
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41DocValuesProducer.java
@@ -0,0 +1,288 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.NumericDocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SortedDocValues;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.BytesRefFSTEnum.InputOutput;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.FST.Arc;
+import org.apache.lucene.util.fst.FST.BytesReader;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util;
+import org.apache.lucene.util.packed.PackedInts;
+
+class Lucene41DocValuesProducer extends DocValuesProducer {
+  // metadata maps (just file pointers and minimal stuff)
+  private final Map<Integer,NumericEntry> numerics;
+  private final Map<Integer,BinaryEntry> binaries;
+  private final Map<Integer,FSTEntry> fsts;
+  private final IndexInput data;
+  
+  // ram instances we have already loaded
+  private final Map<Integer,NumericDocValues> numericInstances = 
+      new HashMap<Integer,NumericDocValues>();
+  
+  // if this thing needs some TL state then we might put something
+  // else in this map.
+  private final Map<Integer,BinaryDocValues> binaryInstances =
+      new HashMap<Integer,BinaryDocValues>();
+  
+  private final Map<Integer,FST<Long>> fstInstances =
+      new HashMap<Integer,FST<Long>>();
+    
+  Lucene41DocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
+    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
+    // read in the entries from the metadata file.
+    IndexInput in = state.directory.openInput(metaName, state.context);
+    boolean success = false;
+    try {
+      CodecUtil.checkHeader(in, metaCodec, 
+                                Lucene41DocValuesConsumer.VERSION_START,
+                                Lucene41DocValuesConsumer.VERSION_START);
+      numerics = new HashMap<Integer,NumericEntry>();
+      binaries = new HashMap<Integer,BinaryEntry>();
+      fsts = new HashMap<Integer,FSTEntry>();
+      readFields(in, state.fieldInfos);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(in);
+      } else {
+        IOUtils.closeWhileHandlingException(in);
+      }
+    }
+    
+    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
+    data = state.directory.openInput(dataName, state.context);
+    CodecUtil.checkHeader(data, dataCodec, 
+                                Lucene41DocValuesConsumer.VERSION_START,
+                                Lucene41DocValuesConsumer.VERSION_START);
+  }
+  
+  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
+    int fieldNumber = meta.readVInt();
+    while (fieldNumber != -1) {
+      int fieldType = meta.readByte();
+      if (fieldType == Lucene41DocValuesConsumer.NUMBER) {
+        NumericEntry entry = new NumericEntry();
+        entry.offset = meta.readLong();
+        entry.tableized = meta.readByte() != 0;
+        numerics.put(fieldNumber, entry);
+      } else if (fieldType == Lucene41DocValuesConsumer.BYTES) {
+        BinaryEntry entry = new BinaryEntry();
+        entry.offset = meta.readLong();
+        entry.numBytes = meta.readLong();
+        entry.minLength = meta.readVInt();
+        entry.maxLength = meta.readVInt();
+        binaries.put(fieldNumber, entry);
+      } else if (fieldType == Lucene41DocValuesConsumer.FST) {
+        FSTEntry entry = new FSTEntry();
+        entry.offset = meta.readLong();
+        entry.numOrds = meta.readVInt();
+        fsts.put(fieldNumber, entry);
+      } else {
+        throw new CorruptIndexException("invalid entry type: " + fieldType + ", input=" + meta);
+      }
+      fieldNumber = meta.readVInt();
+    }
+  }
+
+  @Override
+  public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOException {
+    NumericDocValues instance = numericInstances.get(field.number);
+    if (instance == null) {
+      instance = loadNumeric(field);
+      numericInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+  
+  private NumericDocValues loadNumeric(FieldInfo field) throws IOException {
+    NumericEntry entry = numerics.get(field.number);
+    data.seek(entry.offset);
+    if (entry.tableized) {
+      int size = data.readVInt();
+      final long decode[] = new long[size];
+      for (int i = 0; i < decode.length; i++) {
+        decode[i] = data.readLong();
+      }
+      final long minValue = data.readLong();
+      assert minValue == 0;
+      final PackedInts.Reader reader = PackedInts.getReader(data);
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          return decode[(int)reader.get(docID)];
+        }
+      };
+    } else {
+      final long minValue = data.readLong();
+      final PackedInts.Reader reader = PackedInts.getReader(data);
+      return new NumericDocValues() {
+        @Override
+        public long get(int docID) {
+          return minValue + reader.get(docID);
+        }
+      };
+    }
+  }
+
+  @Override
+  public synchronized BinaryDocValues getBinary(FieldInfo field) throws IOException {
+    BinaryDocValues instance = binaryInstances.get(field.number);
+    if (instance == null) {
+      instance = loadBinary(field);
+      binaryInstances.put(field.number, instance);
+    }
+    return instance;
+  }
+  
+  private BinaryDocValues loadBinary(FieldInfo field) throws IOException {
+    BinaryEntry entry = binaries.get(field.number);
+    data.seek(entry.offset);
+    assert entry.numBytes < Integer.MAX_VALUE; // nocommit
+    final byte[] bytes = new byte[(int)entry.numBytes];
+    data.readBytes(bytes, 0, bytes.length);
+    if (entry.minLength == entry.maxLength) {
+      final int fixedLength = entry.minLength;
+      return new BinaryDocValues() {
+        @Override
+        public void get(int docID, BytesRef result) {
+          result.bytes = bytes;
+          result.offset = docID * fixedLength;
+          result.length = fixedLength;
+        }
+      };
+    } else {
+      final NumericDocValues addresses = getNumeric(field);
+      return new BinaryDocValues() {
+        @Override
+        public void get(int docID, BytesRef result) {
+          int startAddress = docID == 0 ? 0 : (int) addresses.get(docID-1);
+          int endAddress = (int)addresses.get(docID); 
+          result.bytes = bytes;
+          result.offset = startAddress;
+          result.length = endAddress - startAddress;
+        }
+      };
+    }
+  }
+  
+  @Override
+  public SortedDocValues getSorted(FieldInfo field) throws IOException {
+    final FSTEntry entry = fsts.get(field.number);
+    FST<Long> instance;
+    synchronized(this) {
+      instance = fstInstances.get(field.number);
+      if (instance == null) {
+        data.seek(entry.offset);
+        instance = new FST<Long>(data, PositiveIntOutputs.getSingleton(true));
+        fstInstances.put(field.number, instance);
+      }
+    }
+    final NumericDocValues docToOrd = getNumeric(field);
+    final FST<Long> fst = instance;
+    
+    // per-thread resources
+    final BytesReader in = fst.getBytesReader(0);
+    final Arc<Long> firstArc = new Arc<Long>();
+    final Arc<Long> scratchArc = new Arc<Long>();
+    final IntsRef scratchInts = new IntsRef();
+    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst); 
+    
+    return new SortedDocValues() {
+      @Override
+      public int getOrd(int docID) {
+        return (int) docToOrd.get(docID);
+      }
+
+      @Override
+      public void lookupOrd(int ord, BytesRef result) {
+        try {
+          in.setPosition(0);
+          fst.getFirstArc(firstArc);
+          Util.toBytesRef(Util.getByOutput(fst, ord, in, firstArc, scratchArc, scratchInts), result);
+        } catch (IOException bogus) {
+          throw new RuntimeException(bogus);
+        }
+      }
+
+      @Override
+      public int lookupTerm(BytesRef key, BytesRef spare) {
+        try {
+          InputOutput<Long> o = fstEnum.seekCeil(key);
+          if (o == null) {
+            return -getValueCount()-1;
+          } else if (o.input.equals(key)) {
+            return o.output.intValue();
+          } else {
+            return (int) -o.output-1;
+          }
+        } catch (IOException bogus) {
+          throw new RuntimeException(bogus);
+        }
+      }
+
+      @Override
+      public int getValueCount() {
+        return entry.numOrds;
+      }
+    };
+  }
+  
+  @Override
+  public void close() throws IOException {
+    data.close();
+  }
+  
+  static class NumericEntry {
+    long offset;
+    boolean tableized;
+  }
+  
+  static class BinaryEntry {
+    long offset;
+    long numBytes;
+    int minLength;
+    int maxLength;
+  }
+  
+  static class FSTEntry {
+    long offset;
+    int numOrds;
+  }
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41NormsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41NormsFormat.java
new file mode 100644
index 0000000..812204a
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41NormsFormat.java
@@ -0,0 +1,44 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+public class Lucene41NormsFormat extends NormsFormat {
+
+  @Override
+  public DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException {
+    return new Lucene41DocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+  
+  @Override
+  public DocValuesProducer normsProducer(SegmentReadState state) throws IOException {
+    return new Lucene41DocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
+  }
+  
+  private static final String DATA_CODEC = "Lucene41NormsData";
+  private static final String DATA_EXTENSION = "nvd";
+  private static final String METADATA_CODEC = "Lucene41NormsMetadata";
+  private static final String METADATA_EXTENSION = "nvm";
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesConsumer.java
deleted file mode 100644
index 533c9cf..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesConsumer.java
+++ /dev/null
@@ -1,246 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.FST.INPUT_TYPE;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util;
-import org.apache.lucene.util.packed.PackedInts;
-import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
-
-/**
- * Writes numbers one of two ways:
- * 1. packed ints as deltas from minValue
- * 2. packed ints as ordinals to a table (if the number of values is small, e.g. <= 256)
- * 
- * the latter is typically much smaller with lucene's sims, as only some byte values are used,
- * but its often a nonlinear mapping, especially if you dont use crazy boosts.
- */
-class Lucene41SimpleDocValuesConsumer extends DocValuesConsumer {
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-  
-  static final byte NUMBER = 0;
-  static final byte BYTES = 1;
-  static final byte FST = 2;
-  
-  final IndexOutput data, meta;
-  
-  Lucene41SimpleDocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    boolean success = false;
-    try {
-      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-      data = state.directory.createOutput(dataName, state.context);
-      CodecUtil.writeHeader(data, dataCodec, VERSION_CURRENT);
-      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-      meta = state.directory.createOutput(metaName, state.context);
-      CodecUtil.writeHeader(meta, metaCodec, VERSION_CURRENT);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this);
-      }
-    }
-  }
-  
-  @Override
-  public void addNumericField(FieldInfo field, Iterable<Number> values) throws IOException {
-    meta.writeVInt(field.number);
-    meta.writeByte(NUMBER);
-    meta.writeLong(data.getFilePointer());
-    long minValue = Long.MAX_VALUE;
-    long maxValue = Long.MIN_VALUE;
-    int count = 0;
-    // TODO: more efficient?
-    HashSet<Long> uniqueValues = new HashSet<Long>();
-    for(Number nv : values) {
-      long v = nv.longValue();
-      minValue = Math.min(minValue, v);
-      maxValue = Math.max(maxValue, v);
-      count++;
-      if (uniqueValues != null) {
-        if (uniqueValues.add(v)) {
-          if (uniqueValues.size() > 256) {
-            uniqueValues = null;
-          }
-        }
-      }
-    }
-
-    long delta = maxValue - minValue;
-    final int bitsPerValue;
-    if (delta < 0) {
-      bitsPerValue = 64;
-      meta.writeByte((byte)0); // delta-compressed
-    } else if (uniqueValues != null && PackedInts.bitsRequired(uniqueValues.size()-1) < PackedInts.bitsRequired(delta)) {
-      // smaller to tableize
-      bitsPerValue = PackedInts.bitsRequired(uniqueValues.size()-1);
-      minValue = 0; // we will write indexes into the table instead of values
-      meta.writeByte((byte)1); // table-compressed
-      Long[] decode = uniqueValues.toArray(new Long[uniqueValues.size()]);
-      final HashMap<Long,Integer> encode = new HashMap<Long,Integer>();
-      data.writeVInt(decode.length);
-      for (int i = 0; i < decode.length; i++) {
-        data.writeLong(decode[i]);
-        encode.put(decode[i], i);
-      }
-      final Iterable<Number> original = values;
-      values = new Iterable<Number>() {
-        @Override
-        public Iterator<Number> iterator() {
-          final Iterator<Number> inner = original.iterator();
-          return new Iterator<Number>() {
-            @Override
-            public boolean hasNext() {
-              return inner.hasNext();
-            }
-
-            @Override
-            public Number next() {
-              return encode.get(inner.next());
-            }
-
-            @Override
-            public void remove() { throw new UnsupportedOperationException(); }
-          };
-        }
-      };
-    } else {
-      bitsPerValue = PackedInts.bitsRequired(delta);
-      meta.writeByte((byte)0); // delta-compressed
-    }
-
-    data.writeLong(minValue);
-
-    FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(count, bitsPerValue, PackedInts.COMPACT);   
-    final PackedInts.Writer writer = PackedInts.getWriter(data, count, formatAndBits.bitsPerValue, 0);
-    for(Number nv : values) {
-      writer.add(nv.longValue() - minValue);
-    }
-    writer.finish();
-  }
-  
-  @Override
-  public void close() throws IOException {
-    // nocommit: just write this to a RAMfile or something and flush it here, with #fields first.
-    // this meta is a tiny file so this hurts nobody
-    boolean success = false;
-    try {
-      if (meta != null) {
-        meta.writeVInt(-1);
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(data, meta);
-      } else {
-        IOUtils.closeWhileHandlingException(data, meta);
-      }
-    }
-  }
-
-  @Override
-  public void addBinaryField(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
-    // write the byte[] data
-    meta.writeVInt(field.number);
-    meta.writeByte(BYTES);
-    int minLength = Integer.MAX_VALUE;
-    int maxLength = Integer.MIN_VALUE;
-    final long startFP = data.getFilePointer();
-    for(BytesRef v : values) {
-      minLength = Math.min(minLength, v.length);
-      maxLength = Math.max(maxLength, v.length);
-      data.writeBytes(v.bytes, v.offset, v.length);
-    }
-    meta.writeLong(startFP);
-    meta.writeLong(data.getFilePointer() - startFP);
-    meta.writeVInt(minLength);
-    meta.writeVInt(maxLength);
-    
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
-    // otherwise, we need to record the length fields...
-    // TODO: make this more efficient. this is just as inefficient as 4.0 codec.... we can do much better.
-    if (minLength != maxLength) {
-      addNumericField(field, new Iterable<Number>() {
-        @Override
-        public Iterator<Number> iterator() {
-          final Iterator<BytesRef> inner = values.iterator();
-          return new Iterator<Number>() {
-            long addr = 0;
-
-            @Override
-            public boolean hasNext() {
-              return inner.hasNext();
-            }
-
-            @Override
-            public Number next() {
-              BytesRef b = inner.next();
-              addr += b.length;
-              return Long.valueOf(addr);
-            }
-
-            @Override
-            public void remove() { throw new UnsupportedOperationException(); } 
-          };
-        }
-      });
-    }
-  }
-
-  @Override
-  public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
-    // write the ordinals as numerics
-    addNumericField(field, docToOrd);
-    
-    // write the values as FST
-    meta.writeVInt(field.number);
-    meta.writeByte(FST);
-    meta.writeLong(data.getFilePointer());
-    PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton(true);
-    Builder<Long> builder = new Builder<Long>(INPUT_TYPE.BYTE1, outputs);
-    IntsRef scratch = new IntsRef();
-    long ord = 0;
-    for (BytesRef v : values) {
-      builder.add(Util.toIntsRef(v, scratch), ord);
-      ord++;
-    }
-    FST<Long> fst = builder.finish();
-    fst.save(data);
-    meta.writeVInt((int)ord);
-  }
-  
-  // nocommit: can/should we make override merge + make it smarter to pull the values 
-  // directly from disk for fields that arent already loaded up in ram?
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesFormat.java
deleted file mode 100644
index f444d41..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesFormat.java
+++ /dev/null
@@ -1,48 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-public class Lucene41SimpleDocValuesFormat extends DocValuesFormat {
-
-  public Lucene41SimpleDocValuesFormat() {
-    super("Lucene41");
-  }
-
-  @Override
-  public DocValuesConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new Lucene41SimpleDocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-  }
-  
-  @Override
-  public DocValuesProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new Lucene41SimpleDocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-  }
-  
-  private static final String DATA_CODEC = "Lucene41DocValuesData";
-  private static final String DATA_EXTENSION = "dvd";
-  private static final String METADATA_CODEC = "Lucene41DocValuesMetadata";
-  private static final String METADATA_EXTENSION = "dvm";
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesProducer.java
deleted file mode 100644
index 6af6196..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleDocValuesProducer.java
+++ /dev/null
@@ -1,288 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.NumericDocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SortedDocValues;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.BytesRefFSTEnum.InputOutput;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.FST.Arc;
-import org.apache.lucene.util.fst.FST.BytesReader;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util;
-import org.apache.lucene.util.packed.PackedInts;
-
-class Lucene41SimpleDocValuesProducer extends DocValuesProducer {
-  // metadata maps (just file pointers and minimal stuff)
-  private final Map<Integer,NumericEntry> numerics;
-  private final Map<Integer,BinaryEntry> binaries;
-  private final Map<Integer,FSTEntry> fsts;
-  private final IndexInput data;
-  
-  // ram instances we have already loaded
-  private final Map<Integer,NumericDocValues> numericInstances = 
-      new HashMap<Integer,NumericDocValues>();
-  
-  // if this thing needs some TL state then we might put something
-  // else in this map.
-  private final Map<Integer,BinaryDocValues> binaryInstances =
-      new HashMap<Integer,BinaryDocValues>();
-  
-  private final Map<Integer,FST<Long>> fstInstances =
-      new HashMap<Integer,FST<Long>>();
-    
-  Lucene41SimpleDocValuesProducer(SegmentReadState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {
-    String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);
-    // read in the entries from the metadata file.
-    IndexInput in = state.directory.openInput(metaName, state.context);
-    boolean success = false;
-    try {
-      CodecUtil.checkHeader(in, metaCodec, 
-                                Lucene41SimpleDocValuesConsumer.VERSION_START,
-                                Lucene41SimpleDocValuesConsumer.VERSION_START);
-      numerics = new HashMap<Integer,NumericEntry>();
-      binaries = new HashMap<Integer,BinaryEntry>();
-      fsts = new HashMap<Integer,FSTEntry>();
-      readFields(in, state.fieldInfos);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(in);
-      } else {
-        IOUtils.closeWhileHandlingException(in);
-      }
-    }
-    
-    String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
-    data = state.directory.openInput(dataName, state.context);
-    CodecUtil.checkHeader(data, dataCodec, 
-                                Lucene41SimpleDocValuesConsumer.VERSION_START,
-                                Lucene41SimpleDocValuesConsumer.VERSION_START);
-  }
-  
-  private void readFields(IndexInput meta, FieldInfos infos) throws IOException {
-    int fieldNumber = meta.readVInt();
-    while (fieldNumber != -1) {
-      int fieldType = meta.readByte();
-      if (fieldType == Lucene41SimpleDocValuesConsumer.NUMBER) {
-        NumericEntry entry = new NumericEntry();
-        entry.offset = meta.readLong();
-        entry.tableized = meta.readByte() != 0;
-        numerics.put(fieldNumber, entry);
-      } else if (fieldType == Lucene41SimpleDocValuesConsumer.BYTES) {
-        BinaryEntry entry = new BinaryEntry();
-        entry.offset = meta.readLong();
-        entry.numBytes = meta.readLong();
-        entry.minLength = meta.readVInt();
-        entry.maxLength = meta.readVInt();
-        binaries.put(fieldNumber, entry);
-      } else if (fieldType == Lucene41SimpleDocValuesConsumer.FST) {
-        FSTEntry entry = new FSTEntry();
-        entry.offset = meta.readLong();
-        entry.numOrds = meta.readVInt();
-        fsts.put(fieldNumber, entry);
-      } else {
-        throw new CorruptIndexException("invalid entry type: " + fieldType + ", input=" + meta);
-      }
-      fieldNumber = meta.readVInt();
-    }
-  }
-
-  @Override
-  public synchronized NumericDocValues getNumeric(FieldInfo field) throws IOException {
-    NumericDocValues instance = numericInstances.get(field.number);
-    if (instance == null) {
-      instance = loadNumeric(field);
-      numericInstances.put(field.number, instance);
-    }
-    return instance;
-  }
-  
-  private NumericDocValues loadNumeric(FieldInfo field) throws IOException {
-    NumericEntry entry = numerics.get(field.number);
-    data.seek(entry.offset);
-    if (entry.tableized) {
-      int size = data.readVInt();
-      final long decode[] = new long[size];
-      for (int i = 0; i < decode.length; i++) {
-        decode[i] = data.readLong();
-      }
-      final long minValue = data.readLong();
-      assert minValue == 0;
-      final PackedInts.Reader reader = PackedInts.getReader(data);
-      return new NumericDocValues() {
-        @Override
-        public long get(int docID) {
-          return decode[(int)reader.get(docID)];
-        }
-      };
-    } else {
-      final long minValue = data.readLong();
-      final PackedInts.Reader reader = PackedInts.getReader(data);
-      return new NumericDocValues() {
-        @Override
-        public long get(int docID) {
-          return minValue + reader.get(docID);
-        }
-      };
-    }
-  }
-
-  @Override
-  public synchronized BinaryDocValues getBinary(FieldInfo field) throws IOException {
-    BinaryDocValues instance = binaryInstances.get(field.number);
-    if (instance == null) {
-      instance = loadBinary(field);
-      binaryInstances.put(field.number, instance);
-    }
-    return instance;
-  }
-  
-  private BinaryDocValues loadBinary(FieldInfo field) throws IOException {
-    BinaryEntry entry = binaries.get(field.number);
-    data.seek(entry.offset);
-    assert entry.numBytes < Integer.MAX_VALUE; // nocommit
-    final byte[] bytes = new byte[(int)entry.numBytes];
-    data.readBytes(bytes, 0, bytes.length);
-    if (entry.minLength == entry.maxLength) {
-      final int fixedLength = entry.minLength;
-      return new BinaryDocValues() {
-        @Override
-        public void get(int docID, BytesRef result) {
-          result.bytes = bytes;
-          result.offset = docID * fixedLength;
-          result.length = fixedLength;
-        }
-      };
-    } else {
-      final NumericDocValues addresses = getNumeric(field);
-      return new BinaryDocValues() {
-        @Override
-        public void get(int docID, BytesRef result) {
-          int startAddress = docID == 0 ? 0 : (int) addresses.get(docID-1);
-          int endAddress = (int)addresses.get(docID); 
-          result.bytes = bytes;
-          result.offset = startAddress;
-          result.length = endAddress - startAddress;
-        }
-      };
-    }
-  }
-  
-  @Override
-  public SortedDocValues getSorted(FieldInfo field) throws IOException {
-    final FSTEntry entry = fsts.get(field.number);
-    FST<Long> instance;
-    synchronized(this) {
-      instance = fstInstances.get(field.number);
-      if (instance == null) {
-        data.seek(entry.offset);
-        instance = new FST<Long>(data, PositiveIntOutputs.getSingleton(true));
-        fstInstances.put(field.number, instance);
-      }
-    }
-    final NumericDocValues docToOrd = getNumeric(field);
-    final FST<Long> fst = instance;
-    
-    // per-thread resources
-    final BytesReader in = fst.getBytesReader(0);
-    final Arc<Long> firstArc = new Arc<Long>();
-    final Arc<Long> scratchArc = new Arc<Long>();
-    final IntsRef scratchInts = new IntsRef();
-    final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst); 
-    
-    return new SortedDocValues() {
-      @Override
-      public int getOrd(int docID) {
-        return (int) docToOrd.get(docID);
-      }
-
-      @Override
-      public void lookupOrd(int ord, BytesRef result) {
-        try {
-          in.setPosition(0);
-          fst.getFirstArc(firstArc);
-          Util.toBytesRef(Util.getByOutput(fst, ord, in, firstArc, scratchArc, scratchInts), result);
-        } catch (IOException bogus) {
-          throw new RuntimeException(bogus);
-        }
-      }
-
-      @Override
-      public int lookupTerm(BytesRef key, BytesRef spare) {
-        try {
-          InputOutput<Long> o = fstEnum.seekCeil(key);
-          if (o == null) {
-            return -getValueCount()-1;
-          } else if (o.input.equals(key)) {
-            return o.output.intValue();
-          } else {
-            return (int) -o.output-1;
-          }
-        } catch (IOException bogus) {
-          throw new RuntimeException(bogus);
-        }
-      }
-
-      @Override
-      public int getValueCount() {
-        return entry.numOrds;
-      }
-    };
-  }
-  
-  @Override
-  public void close() throws IOException {
-    data.close();
-  }
-  
-  static class NumericEntry {
-    long offset;
-    boolean tableized;
-  }
-  
-  static class BinaryEntry {
-    long offset;
-    long numBytes;
-    int minLength;
-    int maxLength;
-  }
-  
-  static class FSTEntry {
-    long offset;
-    int numOrds;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleNormsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleNormsFormat.java
deleted file mode 100644
index c81eee2..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41SimpleNormsFormat.java
+++ /dev/null
@@ -1,44 +0,0 @@
-package org.apache.lucene.codecs.lucene41;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.DocValuesProducer;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-public class Lucene41SimpleNormsFormat extends NormsFormat {
-
-  @Override
-  public DocValuesConsumer normsConsumer(SegmentWriteState state) throws IOException {
-    return new Lucene41SimpleDocValuesConsumer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-  }
-  
-  @Override
-  public DocValuesProducer normsProducer(SegmentReadState state) throws IOException {
-    return new Lucene41SimpleDocValuesProducer(state, DATA_CODEC, DATA_EXTENSION, METADATA_CODEC, METADATA_EXTENSION);
-  }
-  
-  private static final String DATA_CODEC = "Lucene41NormsData";
-  private static final String DATA_EXTENSION = "nvd";
-  private static final String METADATA_CODEC = "Lucene41NormsMetadata";
-  private static final String METADATA_EXTENSION = "nvm";
-}
diff --git a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
index c5e93af..ddb5385 100644
--- a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
+++ b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.DocValuesFormat
@@ -13,4 +13,4 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-org.apache.lucene.codecs.lucene41.Lucene41SimpleDocValuesFormat
+org.apache.lucene.codecs.lucene41.Lucene41DocValuesFormat

