GitDiffStart: 541a7263fa6bd9812bcdf261533eb728ad9ce0ba | Tue Feb 22 17:14:57 2011 +0000
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java b/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
index 7cdcc0a..b79e3fc 100644
--- a/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriter.java
@@ -94,7 +94,6 @@ final class TermVectorsTermsWriter extends TermsHashConsumer {
       // vector output files, we must abort this segment
       // because those files will be in an unknown
       // state:
-      hasVectors = true;
       tvx = docWriter.directory.createOutput(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_INDEX_EXTENSION));
       tvd = docWriter.directory.createOutput(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_DOCUMENTS_EXTENSION));
       tvf = docWriter.directory.createOutput(IndexFileNames.segmentFileName(docWriter.getSegment(), "", IndexFileNames.VECTORS_FIELDS_EXTENSION));
@@ -112,7 +111,7 @@ final class TermVectorsTermsWriter extends TermsHashConsumer {
 
     assert docWriter.writer.testPoint("TermVectorsTermsWriter.finishDocument start");
 
-    if (numVectorFields == 0) {
+    if (!hasVectors) {
       return;
     }
 
@@ -201,7 +200,6 @@ final class TermVectorsTermsWriter extends TermsHashConsumer {
   @Override
   void startDocument() throws IOException {
     assert clearLastVectorFieldName();
-    perFields = new TermVectorsTermsWriterPerField[1];
     reset();
   }
 
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriterPerField.java b/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriterPerField.java
index c27de33..9f2ecc6 100644
--- a/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriterPerField.java
+++ b/lucene/src/java/org/apache/lucene/index/TermVectorsTermsWriterPerField.java
@@ -70,6 +70,7 @@ final class TermVectorsTermsWriterPerField extends TermsHashConsumerPerField {
     }
 
     if (doVectors) {
+      termsWriter.hasVectors = true;
       if (termsWriter.tvx != null) {
         if (termsHashPerField.bytesHash.size() != 0) {
           // Only necessary if previous doc hit a

