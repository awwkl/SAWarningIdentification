GitDiffStart: 8b0b5334725f0e6e49926a59170447b9ba22284b | Mon Mar 24 18:45:55 2008 +0000
diff --git a/src/java/org/apache/lucene/index/DocumentsWriter.java b/src/java/org/apache/lucene/index/DocumentsWriter.java
index 81dd1f6..a6896d0 100644
--- a/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -271,7 +271,7 @@ final class DocumentsWriter {
     List flushedFiles = files();
 
     if (infoStream != null)
-      infoStream.println("\ncloseDocStore: " + flushedFiles.size() + " files to flush to segment " + docStoreSegment + " numDocs=" + numDocsInStore);
+      message("closeDocStore: " + flushedFiles.size() + " files to flush to segment " + docStoreSegment + " numDocs=" + numDocsInStore);
 
     if (flushedFiles.size() > 0) {
       files = null;
@@ -312,6 +312,10 @@ final class DocumentsWriter {
     return abortedFiles;
   }
 
+  void message(String message) {
+    writer.message("DW: " + message);
+  }
+
   /* Returns list of files in use by this instance,
    * including any flushed segments. */
   synchronized List files() {
@@ -360,7 +364,7 @@ final class DocumentsWriter {
     try {
 
       if (infoStream != null)
-        infoStream.println("docWriter: now abort");
+        message("docWriter: now abort");
 
       // Forcefully remove waiting ThreadStates from line
       for(int i=0;i<numWaiting;i++)
@@ -539,7 +543,7 @@ final class DocumentsWriter {
     assert numDocsInRAM > 0;
 
     if (infoStream != null)
-      infoStream.println("\nflush postings as segment " + segment + " numDocs=" + numDocsInRAM);
+      message("flush postings as segment " + segment + " numDocs=" + numDocsInRAM);
     
     boolean success = false;
 
@@ -721,7 +725,7 @@ final class DocumentsWriter {
     if (infoStream != null) {
       final long newSegmentSize = segmentSize(segmentName);
       String message = "  oldRAMSize=" + numBytesUsed + " newFlushedSize=" + newSegmentSize + " docs/MB=" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + " new/old=" + nf.format(100.0*newSegmentSize/numBytesUsed) + "%";
-      infoStream.println(message);
+      message(message);
     }
 
     resetPostingsData();
@@ -1161,10 +1165,10 @@ final class DocumentsWriter {
       return false;
 
     if (infoStream != null)
-      infoStream.println("apply " + deletesFlushed.numTerms + " buffered deleted terms and " +
-                         deletesFlushed.docIDs.size() + " deleted docIDs and " +
-                         deletesFlushed.queries.size() + " deleted queries on " +
-                         + infos.size() + " segments.");
+      message("apply " + deletesFlushed.numTerms + " buffered deleted terms and " +
+              deletesFlushed.docIDs.size() + " deleted docIDs and " +
+              deletesFlushed.queries.size() + " deleted queries on " +
+              + infos.size() + " segments.");
 
     final int infosEnd = infos.size();
 
@@ -1541,13 +1545,13 @@ final class DocumentsWriter {
 
     if (numBytesAlloc > freeTrigger) {
       if (infoStream != null)
-        infoStream.println("  RAM: now balance allocations: usedMB=" + toMB(numBytesUsed) +
-                           " vs trigger=" + toMB(flushTrigger) +
-                           " allocMB=" + toMB(numBytesAlloc) +
-                           " vs trigger=" + toMB(freeTrigger) +
-                           " postingsFree=" + toMB(postingsFreeCount*POSTING_NUM_BYTE) +
-                           " byteBlockFree=" + toMB(freeByteBlocks.size()*BYTE_BLOCK_SIZE) +
-                           " charBlockFree=" + toMB(freeCharBlocks.size()*CHAR_BLOCK_SIZE*CHAR_NUM_BYTE));
+        message("  RAM: now balance allocations: usedMB=" + toMB(numBytesUsed) +
+                " vs trigger=" + toMB(flushTrigger) +
+                " allocMB=" + toMB(numBytesAlloc) +
+                " vs trigger=" + toMB(freeTrigger) +
+                " postingsFree=" + toMB(postingsFreeCount*POSTING_NUM_BYTE) +
+                " byteBlockFree=" + toMB(freeByteBlocks.size()*BYTE_BLOCK_SIZE) +
+                " charBlockFree=" + toMB(freeCharBlocks.size()*CHAR_BLOCK_SIZE*CHAR_NUM_BYTE));
 
       // When we've crossed 100% of our target Postings
       // RAM usage, try to free up until we're back down
@@ -1567,7 +1571,7 @@ final class DocumentsWriter {
           // Nothing else to free -- must flush now.
           bufferIsFull = true;
           if (infoStream != null)
-            infoStream.println("    nothing to free; now set bufferIsFull");
+            message("    nothing to free; now set bufferIsFull");
           break;
         }
 
@@ -1597,7 +1601,7 @@ final class DocumentsWriter {
       }
       
       if (infoStream != null)
-        infoStream.println("    after free: freedMB=" + nf.format((startBytesAlloc-numBytesAlloc)/1024./1024.) + " usedMB=" + nf.format(numBytesUsed/1024./1024.) + " allocMB=" + nf.format(numBytesAlloc/1024./1024.));
+        message("    after free: freedMB=" + nf.format((startBytesAlloc-numBytesAlloc)/1024./1024.) + " usedMB=" + nf.format(numBytesUsed/1024./1024.) + " allocMB=" + nf.format(numBytesAlloc/1024./1024.));
       
     } else {
       // If we have not crossed the 100% mark, but have
@@ -1607,9 +1611,9 @@ final class DocumentsWriter {
       // flush.
       if (numBytesUsed > flushTrigger) {
         if (infoStream != null)
-          infoStream.println("  RAM: now flush @ usedMB=" + nf.format(numBytesUsed/1024./1024.) +
-                             " allocMB=" + nf.format(numBytesAlloc/1024./1024.) +
-                             " triggerMB=" + nf.format(flushTrigger/1024./1024.));
+          message("  RAM: now flush @ usedMB=" + nf.format(numBytesUsed/1024./1024.) +
+                  " allocMB=" + nf.format(numBytesAlloc/1024./1024.) +
+                  " triggerMB=" + nf.format(flushTrigger/1024./1024.));
 
         bufferIsFull = true;
       }
diff --git a/src/java/org/apache/lucene/index/IndexWriter.java b/src/java/org/apache/lucene/index/IndexWriter.java
index cce6cc3..d18d3e1 100644
--- a/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/src/java/org/apache/lucene/index/IndexWriter.java
@@ -1509,7 +1509,7 @@ public class IndexWriter {
             " mergePolicy=" + mergePolicy +
             " mergeScheduler=" + mergeScheduler +
             " ramBufferSizeMB=" + docWriter.getRAMBufferSizeMB() +
-            " maxBuffereDocs=" + docWriter.getMaxBufferedDocs() +
+            " maxBufferedDocs=" + docWriter.getMaxBufferedDocs() +
             " maxBuffereDeleteTerms=" + docWriter.getMaxBufferedDeleteTerms() +
             " maxFieldLength=" + maxFieldLength +
             " index=" + segString());
@@ -1585,8 +1585,8 @@ public class IndexWriter {
    * try {
    *   writer.close();
    * } finally {
-   *   if (IndexReader.isLocked(directory)) {
-   *     IndexReader.unlock(directory);
+   *   if (IndexWriter.isLocked(directory)) {
+   *     IndexWriter.unlock(directory);
    *   }
    * }
    * </pre>
diff --git a/src/java/org/apache/lucene/index/Posting.java b/src/java/org/apache/lucene/index/Posting.java
index bce6a82..146f7dd 100644
--- a/src/java/org/apache/lucene/index/Posting.java
+++ b/src/java/org/apache/lucene/index/Posting.java
@@ -19,7 +19,9 @@ package org.apache.lucene.index;
 
 /* Used by DocumentsWriter to track postings for a single
  * term.  One of these exists per unique term seen since the
- * last flush. */
+ * last flush.  If you alter this class you must also fix
+ * DocumentWriter.POSTING_NUM_BYTE to reflect the change as
+ * this is how RAM usage is measured. */
 final class Posting {
   int textStart;                                  // Address into char[] blocks where our text is stored
   int docFreq;                                    // # times this term occurs in the current doc
diff --git a/src/java/org/apache/lucene/index/SegmentMerger.java b/src/java/org/apache/lucene/index/SegmentMerger.java
index a0afdb5..60a8328 100644
--- a/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -300,14 +300,19 @@ final class SegmentMerger {
           final IndexReader reader = (IndexReader) readers.elementAt(i);
           final SegmentReader matchingSegmentReader = matchingSegmentReaders[i];
           final FieldsReader matchingFieldsReader;
-          if (matchingSegmentReader != null)
+          final boolean hasMatchingReader;
+          if (matchingSegmentReader != null) {
+            hasMatchingReader = true;
             matchingFieldsReader = matchingSegmentReader.getFieldsReader();
-          else
+          } else {
+            hasMatchingReader = false;
             matchingFieldsReader = null;
+          }
           final int maxDoc = reader.maxDoc();
+          final boolean hasDeletions = reader.hasDeletions();
           for (int j = 0; j < maxDoc;) {
-            if (!reader.isDeleted(j)) { // skip deleted docs
-              if (matchingSegmentReader != null) {
+            if (!hasDeletions || !reader.isDeleted(j)) { // skip deleted docs
+              if (hasMatchingReader) {
                 // We can optimize this case (doing a bulk
                 // byte copy) since the field numbers are
                 // identical
@@ -316,7 +321,13 @@ final class SegmentMerger {
                 do {
                   j++;
                   numDocs++;
-                } while(j < maxDoc && !matchingSegmentReader.isDeleted(j) && numDocs < MAX_RAW_MERGE_DOCS);
+                  if (j >= maxDoc)
+                    break;
+                  if (hasDeletions && matchingSegmentReader.isDeleted(j)) {
+                    j++;
+                    break;
+                  }
+                } while(numDocs < MAX_RAW_MERGE_DOCS);
 
                 IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);
                 fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);
@@ -363,21 +374,29 @@ final class SegmentMerger {
       for (int r = 0; r < readers.size(); r++) {
         final SegmentReader matchingSegmentReader = matchingSegmentReaders[r];
         TermVectorsReader matchingVectorsReader;
+        final boolean hasMatchingReader;
         if (matchingSegmentReader != null) {
           matchingVectorsReader = matchingSegmentReader.termVectorsReaderOrig;
 
           // If the TV* files are an older format then they
           // cannot read raw docs:
-          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs())
+          if (matchingVectorsReader != null && !matchingVectorsReader.canReadRawDocs()) {
             matchingVectorsReader = null;
-        } else
+            hasMatchingReader = false;
+          } else
+            hasMatchingReader = matchingVectorsReader != null;
+
+        } else {
+          hasMatchingReader = false;
           matchingVectorsReader = null;
+        }
         IndexReader reader = (IndexReader) readers.elementAt(r);
+        final boolean hasDeletions = reader.hasDeletions();
         int maxDoc = reader.maxDoc();
         for (int docNum = 0; docNum < maxDoc;) {
           // skip deleted docs
-          if (!reader.isDeleted(docNum)) {
-            if (matchingVectorsReader != null) {
+          if (!hasDeletions || !reader.isDeleted(docNum)) {
+            if (hasMatchingReader) {
               // We can optimize this case (doing a bulk
               // byte copy) since the field numbers are
               // identical
@@ -386,7 +405,13 @@ final class SegmentMerger {
               do {
                 docNum++;
                 numDocs++;
-              } while(docNum < maxDoc && !matchingSegmentReader.isDeleted(docNum) && numDocs < MAX_RAW_MERGE_DOCS);
+                if (docNum >= maxDoc)
+                  break;
+                if (hasDeletions && matchingSegmentReader.isDeleted(docNum)) {
+                  docNum++;
+                  break;
+                }
+              } while(numDocs < MAX_RAW_MERGE_DOCS);
 
               matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);
               termVectorsWriter.addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);

