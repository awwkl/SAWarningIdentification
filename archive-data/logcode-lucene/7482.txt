GitDiffStart: 88f18570790cfe194a29602cf4bc3e770e3f6078 | Thu Nov 21 19:48:43 2013 +0000
diff --git a/TODO b/TODO
index 1bea893..2ecf831 100644
--- a/TODO
+++ b/TODO
@@ -9,7 +9,6 @@ TODO
   - fewer args when constructing a range
   - how to do avg() agg?
   - test needsScores=true / valuesource associations
-  - make FieldTypes optional (if all your dims are flat)?
   - consistently name things "dimension"? calling these fields is CONFUSING
   - later
     - SSDVValueSourceFacets?
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/FastTaxonomyFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/FastTaxonomyFacetCounts.java
index 0d0fca5..65c11c2 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/FastTaxonomyFacetCounts.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/FastTaxonomyFacetCounts.java
@@ -67,6 +67,7 @@ public class FastTaxonomyFacetCounts extends TaxonomyFacets {
           if (b >= 0) {
             prev = ord = ((ord << 7) | b) + prev;
             assert ord < counts.length: "ord=" + ord + " vs maxOrd=" + counts.length;
+            //System.out.println("    ord=" + ord);
             ++counts[ord];
             ord = 0;
           } else {
@@ -119,12 +120,15 @@ public class FastTaxonomyFacetCounts extends TaxonomyFacets {
 
   @Override
   public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
     FacetsConfig.DimConfig dimConfig = verifyDim(dim);
-
+    //System.out.println("ftfc.getTopChildren topN=" + topN);
     FacetLabel cp = FacetLabel.create(dim, path);
     int dimOrd = taxoReader.getOrdinal(cp);
     if (dimOrd == -1) {
-      //System.out.println("no ord for path=" + path);
+      //System.out.println("no ord for dim=" + dim + " path=" + path);
       return null;
     }
 
@@ -137,6 +141,7 @@ public class FastTaxonomyFacetCounts extends TaxonomyFacets {
 
     TopOrdAndIntQueue.OrdAndValue reuse = null;
     while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      //System.out.println("  check ord=" + ord + " label=" + taxoReader.getPath(ord) + " topN=" + topN);
       if (counts[ord] > 0) {
         totCount += counts[ord];
         if (counts[ord] > bottomCount) {
@@ -156,6 +161,7 @@ public class FastTaxonomyFacetCounts extends TaxonomyFacets {
     }
 
     if (totCount == 0) {
+      //System.out.println("  no matches");
       return null;
     }
 
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSideways.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSideways.java
index 53d7b7d..92a9ecf 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSideways.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSideways.java
@@ -29,8 +29,6 @@ import java.util.Set;
 import org.apache.lucene.facet.index.FacetFields;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetFields;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
@@ -110,17 +108,30 @@ public class SimpleDrillSideways {
    *  impl. */
   protected Facets buildFacetsResult(SimpleFacetsCollector drillDowns, SimpleFacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {
 
-    Facets drillDownFacets = new FastTaxonomyFacetCounts(taxoReader, config, drillDowns);
-
-    if (drillSideways == null) {
-      return drillDownFacets;
+    if (taxoReader != null) {
+      Facets drillDownFacets = new FastTaxonomyFacetCounts(taxoReader, config, drillDowns);
+      if (drillSideways == null) {
+        return drillDownFacets;
+      } else {
+        Map<String,Facets> drillSidewaysFacets = new HashMap<String,Facets>();
+        for(int i=0;i<drillSideways.length;i++) {
+          drillSidewaysFacets.put(drillSidewaysDims[i],
+                                  new FastTaxonomyFacetCounts(taxoReader, config, drillSideways[i]));
+        }
+        return new MultiFacets(drillSidewaysFacets, drillDownFacets);
+      }
     } else {
-      Map<String,Facets> drillSidewaysFacets = new HashMap<String,Facets>();
-      for(int i=0;i<drillSideways.length;i++) {
-        drillSidewaysFacets.put(drillSidewaysDims[i],
-                                new FastTaxonomyFacetCounts(taxoReader, config, drillSideways[i]));
+      Facets drillDownFacets = new SortedSetDocValuesFacetCounts(state, drillDowns);
+      if (drillSideways == null) {
+        return drillDownFacets;
+      } else {
+        Map<String,Facets> drillSidewaysFacets = new HashMap<String,Facets>();
+        for(int i=0;i<drillSideways.length;i++) {
+          drillSidewaysFacets.put(drillSidewaysDims[i],
+                                  new SortedSetDocValuesFacetCounts(state, drillSideways[i]));
+        }
+        return new MultiFacets(drillSidewaysFacets, drillDownFacets);
       }
-      return new MultiFacets(drillSidewaysFacets, drillDownFacets);
     }
   }
 
@@ -339,6 +350,14 @@ public class SimpleDrillSideways {
    * Search, sorting by score, and computing
    * drill down and sideways counts.
    */
+  public SimpleDrillSidewaysResult search(SimpleDrillDownQuery query, int topN) throws IOException {
+    return search(null, query, topN);
+  }
+
+  /**
+   * Search, sorting by score, and computing
+   * drill down and sideways counts.
+   */
   public SimpleDrillSidewaysResult search(ScoreDoc after,
                                           SimpleDrillDownQuery query, int topN) throws IOException {
     int limit = searcher.getIndexReader().maxDoc();
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts.java
index 4958ccf..b822c4a 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts.java
@@ -73,6 +73,9 @@ public class SortedSetDocValuesFacetCounts extends Facets {
 
   @Override
   public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
     if (path.length > 0) {
       throw new IllegalArgumentException("path should be 0 length");
     }
@@ -126,8 +129,8 @@ public class SortedSetDocValuesFacetCounts extends Facets {
     for(int i=labelValues.length-1;i>=0;i--) {
       TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
       dv.lookupOrd(ordAndValue.ord, scratch);
-      String s = scratch.utf8ToString();
-      labelValues[i] = new LabelAndValue(s.substring(dim.length()+1, s.length()), ordAndValue.value);
+      String[] parts = DocumentBuilder.stringToPath(scratch.utf8ToString());
+      labelValues[i] = new LabelAndValue(parts[1], ordAndValue.value);
     }
 
     return new SimpleFacetResult(new FacetLabel(dim), dimCount, labelValues);
@@ -135,11 +138,23 @@ public class SortedSetDocValuesFacetCounts extends Facets {
 
   /** Does all the "real work" of tallying up the counts. */
   private final void count(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("ssdv count");
+
+    MultiDocValues.OrdinalMap ordinalMap;
+
+    // nocommit not quite right?  really, we need a way to
+    // verify that this ordinalMap "matches" the leaves in
+    // matchingDocs...
+    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {
+      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;
+    } else {
+      ordinalMap = null;
+    }
 
     for(MatchingDocs hits : matchingDocs) {
 
       AtomicReader reader = hits.context.reader();
-
+      //System.out.println("  reader=" + reader);
       // LUCENE-5090: make sure the provided reader context "matches"
       // the top-level reader passed to the
       // SortedSetDocValuesReaderState, else cryptic
@@ -150,11 +165,14 @@ public class SortedSetDocValuesFacetCounts extends Facets {
       
       SortedSetDocValues segValues = reader.getSortedSetDocValues(field);
       if (segValues == null) {
-        return;
+        // nocommit in trunk this was a "return" which is
+        // wrong; make a failing test
+        continue;
       }
 
       final int maxDoc = reader.maxDoc();
       assert maxDoc == hits.bits.length();
+      //System.out.println("  dv=" + dv);
 
       // nocommit, yet another option is to count all segs
       // first, only in seg-ord space, and then do a
@@ -165,33 +183,38 @@ public class SortedSetDocValuesFacetCounts extends Facets {
       // (distributed faceting).  but this has much higher
       // temp ram req'ts (sum of number of ords across all
       // segs)
-      if (dv instanceof MultiSortedSetDocValues) {
-        MultiDocValues.OrdinalMap ordinalMap = ((MultiSortedSetDocValues) dv).mapping;
+      if (ordinalMap != null) {
         int segOrd = hits.context.ord;
 
         int numSegOrds = (int) segValues.getValueCount();
 
         if (hits.totalHits < numSegOrds/10) {
+          //System.out.println("    remap as-we-go");
           // Remap every ord to global ord as we iterate:
           int doc = 0;
           while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
+            //System.out.println("    doc=" + doc);
             segValues.setDocument(doc);
             int term = (int) segValues.nextOrd();
             while (term != SortedSetDocValues.NO_MORE_ORDS) {
+              //System.out.println("      segOrd=" + segOrd + " ord=" + term + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, term));
               counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;
               term = (int) segValues.nextOrd();
             }
             ++doc;
           }
         } else {
+          //System.out.println("    count in seg ord first");
 
           // First count in seg-ord space:
           final int[] segCounts = new int[numSegOrds];
           int doc = 0;
           while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
+            //System.out.println("    doc=" + doc);
             segValues.setDocument(doc);
             int term = (int) segValues.nextOrd();
             while (term != SortedSetDocValues.NO_MORE_ORDS) {
+              //System.out.println("      ord=" + term);
               segCounts[term]++;
               term = (int) segValues.nextOrd();
             }
@@ -202,6 +225,7 @@ public class SortedSetDocValuesFacetCounts extends Facets {
           for(int ord=0;ord<numSegOrds;ord++) {
             int count = segCounts[ord];
             if (count != 0) {
+              //System.out.println("    migrate segOrd=" + segOrd + " ord=" + ord + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, ord));
               counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;
             }
           }
@@ -229,7 +253,8 @@ public class SortedSetDocValuesFacetCounts extends Facets {
     if (path.length != 1) {
       throw new IllegalArgumentException("path must be length=1");
     }
-
+    // nocommit this is not thread safe in general?  add
+    // jdocs that app must instantiate & use from same thread?
     int ord = (int) dv.lookupTerm(new BytesRef(DocumentBuilder.pathToString(dim, path)));
     if (ord < 0) {
       return -1;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetCounts.java
index c17d9a1..a268e60 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetCounts.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetCounts.java
@@ -103,6 +103,9 @@ public class TaxonomyFacetCounts extends TaxonomyFacets {
 
   @Override
   public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
     FacetsConfig.DimConfig dimConfig = verifyDim(dim);
     FacetLabel cp = FacetLabel.create(dim, path);
     int dimOrd = taxoReader.getOrdinal(cp);
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumFloatAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumFloatAssociations.java
index 60308e0..c7c9385 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumFloatAssociations.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumFloatAssociations.java
@@ -94,6 +94,9 @@ public class TaxonomyFacetSumFloatAssociations extends TaxonomyFacets {
 
   @Override
   public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
     FacetsConfig.DimConfig dimConfig = verifyDim(dim);
     FacetLabel cp = FacetLabel.create(dim, path);
     int dimOrd = taxoReader.getOrdinal(cp);
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumIntAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumIntAssociations.java
index e9b2f67..f6b94df 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumIntAssociations.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumIntAssociations.java
@@ -94,6 +94,9 @@ public class TaxonomyFacetSumIntAssociations extends TaxonomyFacets {
 
   @Override
   public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
     verifyDim(dim);
     FacetLabel cp = FacetLabel.create(dim, path);
     int dimOrd = taxoReader.getOrdinal(cp);
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumValueSource.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumValueSource.java
index 1a08186..29aec03 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumValueSource.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumValueSource.java
@@ -137,6 +137,9 @@ public class TaxonomyFacetSumValueSource extends TaxonomyFacets {
 
   @Override
   public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
     FacetsConfig.DimConfig dimConfig = verifyDim(dim);
     FacetLabel cp = FacetLabel.create(dim, path);
     int dimOrd = taxoReader.getOrdinal(cp);
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/AssertingSubDocsAtOnceCollector.java b/lucene/facet/src/test/org/apache/lucene/facet/search/AssertingSubDocsAtOnceCollector.java
index ba2786d..2ad44b0 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/AssertingSubDocsAtOnceCollector.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/AssertingSubDocsAtOnceCollector.java
@@ -27,7 +27,7 @@ import org.apache.lucene.search.Scorer;
 
 /** Verifies in collect() that all child subScorers are on
  *  the collected doc. */
-class AssertingSubDocsAtOnceCollector extends Collector {
+public class AssertingSubDocsAtOnceCollector extends Collector {
 
   // TODO: allow wrapping another Collector
 
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets.java
deleted file mode 100644
index 07fba01..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets.java
+++ /dev/null
@@ -1,311 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.FacetTestUtils;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.util.PrintTaxonomyStats;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.similarities.DefaultSimilarity;
-import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
-import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestDemoFacets extends FacetTestCase {
-
-  private DirectoryTaxonomyWriter taxoWriter;
-  private RandomIndexWriter writer;
-  private FacetFields facetFields;
-
-  private void add(String ... categoryPaths) throws IOException {
-    Document doc = new Document();
-    
-    List<FacetLabel> paths = new ArrayList<FacetLabel>();
-    for(String categoryPath : categoryPaths) {
-      paths.add(new FacetLabel(categoryPath, '/'));
-    }
-    facetFields.addFields(doc, paths);
-    writer.addDocument(doc);
-  }
-
-  public void test() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    writer = new RandomIndexWriter(random(), dir);
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    // Reused across documents, to add the necessary facet
-    // fields:
-    facetFields = new FacetFields(taxoWriter);
-
-    add("Author/Bob", "Publish Date/2010/10/15");
-    add("Author/Lisa", "Publish Date/2010/10/20");
-    add("Author/Lisa", "Publish Date/2012/1/1");
-    add("Author/Susan", "Publish Date/2012/1/7");
-    add("Author/Frank", "Publish Date/1999/5/5");
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Count both "Publish Date" and "Author" dimensions:
-    FacetSearchParams fsp = new FacetSearchParams(
-        new CountFacetRequest(new FacetLabel("Publish Date"), 10), 
-        new CountFacetRequest(new FacetLabel("Author"), 10));
-
-    // Aggregate the facet counts:
-    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    // Retrieve & verify results:
-    List<FacetResult> results = c.getFacetResults();
-    assertEquals(2, results.size());
-    assertEquals("Publish Date (0)\n  2012 (2)\n  2010 (2)\n  1999 (1)\n",
-        FacetTestUtils.toSimpleString(results.get(0)));
-    assertEquals("Author (0)\n  Lisa (2)\n  Frank (1)\n  Susan (1)\n  Bob (1)\n",
-        FacetTestUtils.toSimpleString(results.get(1)));
-
-    
-    // Now user drills down on Publish Date/2010:
-    fsp = new FacetSearchParams(new CountFacetRequest(new FacetLabel("Author"), 10));
-    DrillDownQuery q2 = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    q2.add(new FacetLabel("Publish Date/2010", '/'));
-    c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
-    searcher.search(q2, c);
-    results = c.getFacetResults();
-    assertEquals(1, results.size());
-    assertEquals("Author (0)\n  Lisa (1)\n  Bob (1)\n",
-        FacetTestUtils.toSimpleString(results.get(0)));
-
-    // Smoke test PrintTaxonomyStats:
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintTaxonomyStats.printStats(taxoReader, new PrintStream(bos, false, "UTF-8"), true);
-    String result = bos.toString("UTF-8");
-    assertTrue(result.indexOf("/Author: 4 immediate children; 5 total categories") != -1);
-    assertTrue(result.indexOf("/Publish Date: 3 immediate children; 12 total categories") != -1);
-    // Make sure at least a few nodes of the tree came out:
-    assertTrue(result.indexOf("  /1999") != -1);
-    assertTrue(result.indexOf("  /2012") != -1);
-    assertTrue(result.indexOf("      /20") != -1);
-
-    taxoReader.close();
-    searcher.getIndexReader().close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  public void testReallyNoNormsForDrillDown() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setSimilarity(new PerFieldSimilarityWrapper() {
-        final Similarity sim = new DefaultSimilarity();
-
-        @Override
-        public Similarity get(String name) {
-          assertEquals("field", name);
-          return sim;
-        }
-      });
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    FacetFields facetFields = new FacetFields(taxoWriter);      
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    facetFields.addFields(doc, Collections.singletonList(new FacetLabel("a/path", '/')));
-    writer.addDocument(doc);
-    writer.close();
-    taxoWriter.close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  public void testAllParents() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    CategoryListParams clp = new CategoryListParams("$facets") {
-        @Override
-        public OrdinalPolicy getOrdinalPolicy(String fieldName) {
-          return OrdinalPolicy.ALL_PARENTS;
-        }
-      };
-    FacetIndexingParams fip = new FacetIndexingParams(clp);
-
-    FacetFields facetFields = new FacetFields(taxoWriter, fip);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    facetFields.addFields(doc, Collections.singletonList(new FacetLabel("a/path", '/')));
-    writer.addDocument(doc);
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-    
-    FacetSearchParams fsp = new FacetSearchParams(fip,
-                                                  new CountFacetRequest(new FacetLabel("a", '/'), 10));
-
-    // Aggregate the facet counts:
-    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-    List<FacetResult> results = c.getFacetResults();
-    assertEquals(1, results.size());
-    assertEquals(1, (int) results.get(0).getFacetResultNode().value);
-
-    // LUCENE-4913:
-    for(FacetResultNode childNode : results.get(0).getFacetResultNode().subResults) {
-      assertTrue(childNode.ordinal != 0);
-    }
-
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  public void testLabelWithDelimiter() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetFields facetFields = new FacetFields(taxoWriter);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    BytesRef br = new BytesRef(new byte[] {(byte) 0xee, (byte) 0x92, (byte) 0xaa, (byte) 0xef, (byte) 0x9d, (byte) 0x89});
-    facetFields.addFields(doc, Collections.singletonList(new FacetLabel("dim/" + br.utf8ToString(), '/')));
-    try {
-      writer.addDocument(doc);
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    writer.close();
-    taxoWriter.close();
-    dir.close();
-    taxoDir.close();
-  }
-  
-  // LUCENE-4583: make sure if we require > 32 KB for one
-  // document, we don't hit exc when using Facet42DocValuesFormat
-  public void testManyFacetsInOneDocument() throws Exception {
-    assumeTrue("default Codec doesn't support huge BinaryDocValues", _TestUtil.fieldSupportsHugeBinaryDocValues(CategoryListParams.DEFAULT_FIELD));
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    
-    FacetFields facetFields = new FacetFields(taxoWriter);
-    
-    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);
-    
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    List<FacetLabel> paths = new ArrayList<FacetLabel>();
-    for (int i = 0; i < numLabels; i++) {
-      paths.add(new FacetLabel("dim", "" + i));
-    }
-    facetFields.addFields(doc, paths);
-    writer.addDocument(doc);
-    
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-    
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-    
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new FacetLabel("dim"), Integer.MAX_VALUE));
-    
-    // Aggregate the facet counts:
-    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
-    
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-    List<FacetResult> results = c.getFacetResults();
-    assertEquals(1, results.size());
-    FacetResultNode root = results.get(0).getFacetResultNode();
-    assertEquals(numLabels, root.subResults.size());
-    Set<String> allLabels = new HashSet<String>();
-    for (FacetResultNode childNode : root.subResults) {
-      assertEquals(2, childNode.label.length);
-      allLabels.add(childNode.label.components[1]);
-      assertEquals(1, (int) childNode.value);
-    }
-    assertEquals(numLabels, allLabels.size());
-    
-    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestDocumentBuilder.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestDocumentBuilder.java
new file mode 100644
index 0000000..988367d
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestDocumentBuilder.java
@@ -0,0 +1,40 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.util._TestUtil;
+
+public class TestDocumentBuilder extends FacetTestCase {
+  public void testPathToStringAndBack() throws Exception {
+    int iters = atLeast(1000);
+    for(int i=0;i<iters;i++) {
+      int numParts = _TestUtil.nextInt(random(), 1, 6);
+      String[] parts = new String[numParts];
+      for(int j=0;j<numParts;j++) {
+        parts[j] = _TestUtil.randomUnicodeString(random());
+      }
+
+      String s = DocumentBuilder.pathToString(parts);
+      String[] parts2 = DocumentBuilder.stringToPath(s);
+      assertTrue(Arrays.equals(parts, parts2));
+    }
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java
index ff28b22..9f9b677 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java
@@ -17,34 +17,62 @@ package org.apache.lucene.facet.simple;
  * limitations under the License.
  */
 
-
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.search.AssertingSubDocsAtOnceCollector;
 import org.apache.lucene.facet.simple.SimpleDrillSideways.SimpleDrillSidewaysResult;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.Filter;
 import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
 import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.InPlaceMergeSorter;
+import org.apache.lucene.util.InfoStream;
+import org.apache.lucene.util._TestUtil;
 
 public class TestSimpleDrillSideways extends FacetTestCase {
 
-  private DirectoryTaxonomyWriter taxoWriter;
-  private RandomIndexWriter writer;
-  private FacetFields facetFields;
-
   public void testBasic() throws Exception {
     Directory dir = newDirectory();
     Directory taxoDir = newDirectory();
 
     // Writes facet ords to a separate directory from the
     // main index:
-    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
 
     FacetsConfig config = new FacetsConfig();
     config.setHierarchical("Publish Date", true);
@@ -79,13 +107,11 @@ public class TestSimpleDrillSideways extends FacetTestCase {
 
     // NRT open
     IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
 
     //System.out.println("searcher=" + searcher);
 
     // NRT open
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
 
     SimpleDrillSideways ds = new SimpleDrillSideways(searcher, config, taxoReader);
 
@@ -209,66 +235,124 @@ public class TestSimpleDrillSideways extends FacetTestCase {
     assertEquals(0, r.hits.totalHits);
     assertNull(r.facets.getTopChildren(10, "Publish Date"));
     assertNull(r.facets.getTopChildren(10, "Author"));
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    dir.close();
-    taxoDir.close();
+    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
   }
 
-  /*
   public void testSometimesInvalidDrillDown() throws Exception {
     Directory dir = newDirectory();
     Directory taxoDir = newDirectory();
-    writer = new RandomIndexWriter(random(), dir);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
 
     // Writes facet ords to a separate directory from the
     // main index:
-    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("Publish Date", true);
 
     // Reused across documents, to add the necessary facet
     // fields:
-    facetFields = new FacetFields(taxoWriter);
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, config);
+
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    writer.addDocument(builder.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    writer.addDocument(builder.build(doc));
 
-    add("Author/Bob", "Publish Date/2010/10/15");
-    add("Author/Lisa", "Publish Date/2010/10/20");
     writer.commit();
+
     // 2nd segment has no Author:
-    add("Foobar/Lisa", "Publish Date/2012/1/1");
+    doc = new Document();
+    doc.add(new FacetField("Foobar", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    writer.addDocument(builder.build(doc));
 
     // NRT open
     IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
 
     //System.out.println("searcher=" + searcher);
 
     // NRT open
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Count both "Publish Date" and "Author" dimensions, in
-    // drill-down:
-    FacetSearchParams fsp = new FacetSearchParams(
-        new CountFacetRequest(new FacetLabel("Publish Date"), 10), 
-        new CountFacetRequest(new FacetLabel("Author"), 10));
 
-    DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new FacetLabel("Author", "Lisa"));
-    DrillSidewaysResult r = new DrillSideways(searcher, taxoReader).search(null, ddq, 10, fsp);
+    SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    SimpleDrillSidewaysResult r = new SimpleDrillSideways(searcher, config, taxoReader).search(null, ddq, 10);
 
     assertEquals(1, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
     // Publish Date is only drill-down, and Lisa published
     // one in 2012 and one in 2010:
-    assertEquals("Publish Date: 2010=1", toString(r.facetResults.get(0)));
+    assertEquals("Publish Date (1)\n  2010 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
     // Author is drill-sideways + drill-down: Lisa
     // (drill-down) published once, and Bob
     // published once:
-    assertEquals("Author: Lisa=1 Bob=1", toString(r.facetResults.get(1)));
+    assertEquals("Author (2)\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
 
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    dir.close();
-    taxoDir.close();
+    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testMultipleRequestsPerDim() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("dim", true);
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    // Reused across documents, to add the necessary facet
+    // fields:
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, config);
+
+    Document doc = new Document();
+    doc.add(new FacetField("dim", "a", "x"));
+    writer.addDocument(builder.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "a", "y"));
+    writer.addDocument(builder.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "a", "z"));
+    writer.addDocument(builder.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "b"));
+    writer.addDocument(builder.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "c"));
+    writer.addDocument(builder.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "d"));
+    writer.addDocument(builder.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    //System.out.println("searcher=" + searcher);
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config);
+    ddq.add("dim", "a");
+    SimpleDrillSidewaysResult r = new SimpleDrillSideways(searcher, config, taxoReader).search(null, ddq, 10);
+
+    assertEquals(3, r.hits.totalHits);
+    assertEquals("dim (6)\n  a (3)\n  b (1)\n  c (1)\n  d (1)\n", r.facets.getTopChildren(10, "dim").toString());
+    assertEquals("dim/a (3)\n  x (1)\n  y (1)\n  z (1)\n", r.facets.getTopChildren(10, "dim", "a").toString());
+
+    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
   }
 
   private static class Doc implements Comparable<Doc> {
@@ -315,61 +399,6 @@ public class TestSimpleDrillSideways extends FacetTestCase {
     }
   }
 
-  public void testMultipleRequestsPerDim() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    writer = new RandomIndexWriter(random(), dir);
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    // Reused across documents, to add the necessary facet
-    // fields:
-    facetFields = new FacetFields(taxoWriter);
-
-    add("dim/a/x");
-    add("dim/a/y");
-    add("dim/a/z");
-    add("dim/b");
-    add("dim/c");
-    add("dim/d");
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    //System.out.println("searcher=" + searcher);
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Two requests against the same dim:
-    FacetSearchParams fsp = new FacetSearchParams(
-        new CountFacetRequest(new FacetLabel("dim"), 10), 
-        new CountFacetRequest(new FacetLabel("dim", "a"), 10));
-
-    DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new FacetLabel("dim", "a"));
-    DrillSidewaysResult r = new DrillSideways(searcher, taxoReader).search(null, ddq, 10, fsp);
-
-    assertEquals(3, r.hits.totalHits);
-    assertEquals(2, r.facetResults.size());
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("dim: a=3 d=1 c=1 b=1", toString(r.facetResults.get(0)));
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published twice, and Frank/Susan/Bob
-    // published once:
-    assertEquals("a (3)\n  z (1)\n  y (1)\n  x (1)\n", FacetTestUtils.toSimpleString(r.facetResults.get(1)));
-
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    dir.close();
-    taxoDir.close();
-  }
-
   public void testRandom() throws Exception {
 
     boolean canUseDV = defaultCodecSupportsSortedSet();
@@ -391,6 +420,7 @@ public class TestSimpleDrillSideways extends FacetTestCase {
     bChance /= sum;
     cChance /= sum;
 
+    // nocommit
     int numDims = _TestUtil.nextInt(random(), 2, 5);
     //int numDims = 3;
     int numDocs = atLeast(3000);
@@ -404,17 +434,8 @@ public class TestSimpleDrillSideways extends FacetTestCase {
     for(int dim=0;dim<numDims;dim++) {
       Set<String> values = new HashSet<String>();
       while (values.size() < valueCount) {
-        String s;
-        while (true) {
-          s = _TestUtil.randomRealisticUnicodeString(random());
-          //s = _TestUtil.randomSimpleString(random());
-          // We cannot include this character else we hit
-          // IllegalArgExc: 
-          if (s.indexOf(FacetIndexingParams.DEFAULT_FACET_DELIM_CHAR) == -1 &&
-              (!canUseDV || s.indexOf('/') == -1)) {
-            break;
-          }
-        }
+        String s = _TestUtil.randomRealisticUnicodeString(random());
+        //String s = _TestUtil.randomSimpleString(random());
         if (s.length() > 0) {
           values.add(s);
         }
@@ -465,8 +486,11 @@ public class TestSimpleDrillSideways extends FacetTestCase {
     iwc.setInfoStream(InfoStream.NO_OUTPUT);
     RandomIndexWriter w = new RandomIndexWriter(random(), d, iwc);
     DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
-    facetFields = new FacetFields(tw);
-    SortedSetDocValuesFacetFields dvFacetFields = new SortedSetDocValuesFacetFields();
+    FacetsConfig config = new FacetsConfig();
+    for(int i=0;i<numDims;i++) {
+      config.setMultiValued("dim"+i, true);
+    }
+    DocumentBuilder builder = new DocumentBuilder(tw, config);
 
     boolean doUseDV = canUseDV && random().nextBoolean();
 
@@ -474,7 +498,6 @@ public class TestSimpleDrillSideways extends FacetTestCase {
       Document doc = new Document();
       doc.add(newStringField("id", rawDoc.id, Field.Store.YES));
       doc.add(newStringField("content", rawDoc.contentToken, Field.Store.NO));
-      List<FacetLabel> paths = new ArrayList<FacetLabel>();
 
       if (VERBOSE) {
         System.out.println("  doc id=" + rawDoc.id + " token=" + rawDoc.contentToken);
@@ -482,8 +505,11 @@ public class TestSimpleDrillSideways extends FacetTestCase {
       for(int dim=0;dim<numDims;dim++) {
         int dimValue = rawDoc.dims[dim];
         if (dimValue != -1) {
-          FacetLabel cp = new FacetLabel("dim" + dim, dimValues[dim][dimValue]);
-          paths.add(cp);
+          if (doUseDV) {
+            doc.add(new SortedSetDocValuesFacetField("dim" + dim, dimValues[dim][dimValue]));
+          } else {
+            doc.add(new FacetField("dim" + dim, dimValues[dim][dimValue]));
+          }
           doc.add(new StringField("dim" + dim, dimValues[dim][dimValue], Field.Store.YES));
           if (VERBOSE) {
             System.out.println("    dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue]));
@@ -491,23 +517,19 @@ public class TestSimpleDrillSideways extends FacetTestCase {
         }
         int dimValue2 = rawDoc.dims2[dim];
         if (dimValue2 != -1) {
-          FacetLabel cp = new FacetLabel("dim" + dim, dimValues[dim][dimValue2]);
-          paths.add(cp);
+          if (doUseDV) {
+            doc.add(new SortedSetDocValuesFacetField("dim" + dim, dimValues[dim][dimValue2]));
+          } else {
+            doc.add(new FacetField("dim" + dim, dimValues[dim][dimValue2]));
+          }
           doc.add(new StringField("dim" + dim, dimValues[dim][dimValue2], Field.Store.YES));
           if (VERBOSE) {
             System.out.println("      dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue2]));
           }
         }
       }
-      if (!paths.isEmpty()) {
-        if (doUseDV) {
-          dvFacetFields.addFields(doc, paths);
-        } else {
-          facetFields.addFields(doc, paths);
-        }
-      }
 
-      w.addDocument(doc);
+      w.addDocument(builder.build(doc));
     }
 
     if (random().nextBoolean()) {
@@ -537,10 +559,10 @@ public class TestSimpleDrillSideways extends FacetTestCase {
       w.forceMerge(1);
     }
     IndexReader r = w.getReader();
-    w.close();
 
     final SortedSetDocValuesReaderState sortedSetDVState;
     IndexSearcher s = newSearcher(r);
+    
     if (doUseDV) {
       sortedSetDVState = new SortedSetDocValuesReaderState(s.getIndexReader());
     } else {
@@ -553,7 +575,6 @@ public class TestSimpleDrillSideways extends FacetTestCase {
 
     // NRT open
     TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
-    tw.close();
 
     int numIters = atLeast(10);
 
@@ -565,27 +586,6 @@ public class TestSimpleDrillSideways extends FacetTestCase {
         System.out.println("\nTEST: iter=" + iter + " baseQuery=" + contentToken + " numDrillDown=" + numDrillDown + " useSortedSetDV=" + doUseDV);
       }
 
-      List<FacetRequest> requests = new ArrayList<FacetRequest>();
-      while(true) {
-        for(int i=0;i<numDims;i++) {
-          // LUCENE-4915: sometimes don't request facet
-          // counts on the dim(s) we drill down on
-          if (random().nextDouble() <= 0.9) {
-            if (VERBOSE) {
-              System.out.println("  do facet request on dim=" + i);
-            }
-            requests.add(new CountFacetRequest(new FacetLabel("dim" + i), dimValues[numDims-1].length));
-          } else {
-            if (VERBOSE) {
-              System.out.println("  skip facet request on dim=" + i);
-            }
-          }
-        }
-        if (!requests.isEmpty()) {
-          break;
-        }
-      }
-      FacetSearchParams fsp = new FacetSearchParams(requests);
       String[][] drillDowns = new String[numDims][];
 
       int count = 0;
@@ -634,16 +634,14 @@ public class TestSimpleDrillSideways extends FacetTestCase {
         baseQuery = new TermQuery(new Term("content", contentToken));
       }
 
-      DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, baseQuery);
+      SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config, baseQuery);
 
       for(int dim=0;dim<numDims;dim++) {
         if (drillDowns[dim] != null) {
-          FacetLabel[] paths = new FacetLabel[drillDowns[dim].length];
           int upto = 0;
           for(String value : drillDowns[dim]) {
-            paths[upto++] = new FacetLabel("dim" + dim, value);
+            ddq.add("dim" + dim, value);
           }
-          ddq.add(paths);
         }
       }
 
@@ -673,7 +671,7 @@ public class TestSimpleDrillSideways extends FacetTestCase {
       // Verify docs are always collected in order.  If we
       // had an AssertingScorer it could catch it when
       // Weight.scoresDocsOutOfOrder lies!:
-      new DrillSideways(s, tr).search(ddq,
+      new SimpleDrillSideways(s, config, tr).search(ddq,
                            new Collector() {
                              int lastDocID;
 
@@ -696,7 +694,7 @@ public class TestSimpleDrillSideways extends FacetTestCase {
                              public boolean acceptsDocsOutOfOrder() {
                                return false;
                              }
-                           }, fsp);
+                           });
 
       // Also separately verify that DS respects the
       // scoreSubDocsAtOnce method, to ensure that all
@@ -706,26 +704,27 @@ public class TestSimpleDrillSideways extends FacetTestCase {
         // drill-down values, beacuse in that case it's
         // easily possible for one of the DD terms to be on
         // a future docID:
-        new DrillSideways(s, tr) {
+        new SimpleDrillSideways(s, config, tr) {
           @Override
           protected boolean scoreSubDocsAtOnce() {
             return true;
           }
-        }.search(ddq, new AssertingSubDocsAtOnceCollector(), fsp);
+        }.search(ddq, new AssertingSubDocsAtOnceCollector());
       }
 
-      SimpleFacetResult expected = slowDrillSidewaysSearch(s, requests, docs, contentToken, drillDowns, dimValues, filter);
+      SimpleTestFacetResult expected = slowDrillSidewaysSearch(s, docs, contentToken, drillDowns, dimValues, filter);
 
       Sort sort = new Sort(new SortField("id", SortField.Type.STRING));
-      DrillSideways ds;
+      // nocommit subclass & override to use FacetsTestCase.getFacetCounts
+      SimpleDrillSideways ds;
       if (doUseDV) {
-        ds = new DrillSideways(s, sortedSetDVState);
+        ds = new SimpleDrillSideways(s, config, sortedSetDVState);
       } else {
-        ds = new DrillSideways(s, tr);
+        ds = new SimpleDrillSideways(s, config, tr);
       }
 
       // Retrieve all facets:
-      DrillSidewaysResult actual = ds.search(ddq, filter, null, numDocs, sort, true, true, fsp);
+      SimpleDrillSidewaysResult actual = ds.search(ddq, filter, null, numDocs, sort, true, true);
 
       TopDocs hits = s.search(baseQuery, numDocs);
       Map<String,Float> scores = new HashMap<String,Float>();
@@ -735,21 +734,7 @@ public class TestSimpleDrillSideways extends FacetTestCase {
       if (VERBOSE) {
         System.out.println("  verify all facets");
       }
-      verifyEquals(requests, dimValues, s, expected, actual, scores, -1, doUseDV);
-
-      // Retrieve topN facets:
-      int topN = _TestUtil.nextInt(random(), 1, 20);
-
-      List<FacetRequest> newRequests = new ArrayList<FacetRequest>();
-      for(FacetRequest oldRequest : requests) {
-        newRequests.add(new CountFacetRequest(oldRequest.categoryPath, topN));
-      }
-      fsp = new FacetSearchParams(newRequests);
-      actual = ds.search(ddq, filter, null, numDocs, sort, true, true, fsp);
-      if (VERBOSE) {
-        System.out.println("  verify topN=" + topN);
-      }
-      verifyEquals(newRequests, dimValues, s, expected, actual, scores, topN, doUseDV);
+      verifyEquals(dimValues, s, expected, actual, scores, doUseDV);
 
       // Make sure drill down doesn't change score:
       TopDocs ddqHits = s.search(ddq, filter, numDocs);
@@ -760,10 +745,7 @@ public class TestSimpleDrillSideways extends FacetTestCase {
       }
     }
 
-    tr.close();
-    r.close();
-    td.close();
-    d.close();
+    IOUtils.close(r, tr, w, tw, d, td);
   }
 
   private static class Counters {
@@ -796,11 +778,10 @@ public class TestSimpleDrillSideways extends FacetTestCase {
     }
   }
 
-  private static class SimpleFacetResult {
+  private static class SimpleTestFacetResult {
     List<Doc> hits;
     int[][] counts;
     int[] uniqueCounts;
-    public SimpleFacetResult() {}
   }
   
   private int[] getTopNOrds(final int[] counts, final String[] values, int topN) {
@@ -854,9 +835,9 @@ public class TestSimpleDrillSideways extends FacetTestCase {
     return topNIDs;
   }
 
-  private SimpleFacetResult slowDrillSidewaysSearch(IndexSearcher s, List<FacetRequest> requests, List<Doc> docs,
-                                                    String contentToken, String[][] drillDowns,
-                                                    String[][] dimValues, Filter onlyEven) throws Exception {
+  private SimpleTestFacetResult slowDrillSidewaysSearch(IndexSearcher s, List<Doc> docs,
+                                                        String contentToken, String[][] drillDowns,
+                                                        String[][] dimValues, Filter onlyEven) throws Exception {
     int numDims = dimValues.length;
 
     List<Doc> hits = new ArrayList<Doc>();
@@ -928,12 +909,11 @@ public class TestSimpleDrillSideways extends FacetTestCase {
 
     Collections.sort(hits);
 
-    SimpleFacetResult res = new SimpleFacetResult();
+    SimpleTestFacetResult res = new SimpleTestFacetResult();
     res.hits = hits;
     res.counts = new int[numDims][];
     res.uniqueCounts = new int[numDims];
-    for (int i = 0; i < requests.size(); i++) {
-      int dim = Integer.parseInt(requests.get(i).categoryPath.components[0].substring(3));
+    for (int dim = 0; dim < numDims; dim++) {
       if (drillDowns[dim] != null) {
         res.counts[dim] = drillSidewaysCounts[dim].counts[dim];
       } else {
@@ -951,8 +931,8 @@ public class TestSimpleDrillSideways extends FacetTestCase {
     return res;
   }
 
-  void verifyEquals(List<FacetRequest> requests, String[][] dimValues, IndexSearcher s, SimpleFacetResult expected,
-                    DrillSidewaysResult actual, Map<String,Float> scores, int topN, boolean isSortedSetDV) throws Exception {
+  void verifyEquals(String[][] dimValues, IndexSearcher s, SimpleTestFacetResult expected,
+                    SimpleDrillSidewaysResult actual, Map<String,Float> scores, boolean isSortedSetDV) throws Exception {
     if (VERBOSE) {
       System.out.println("  verify totHits=" + expected.hits.size());
     }
@@ -968,45 +948,28 @@ public class TestSimpleDrillSideways extends FacetTestCase {
       assertEquals(scores.get(expected.hits.get(i).id), actual.hits.scoreDocs[i].score, 0.0f);
     }
 
-    int numExpected = 0;
     for(int dim=0;dim<expected.counts.length;dim++) {
-      if (expected.counts[dim] != null) {
-        numExpected++;
-      }
-    }
-
-    assertEquals(numExpected, actual.facetResults.size());
-
-    for(int dim=0;dim<expected.counts.length;dim++) {
-      if (expected.counts[dim] == null) {
-        continue;
-      }
-      int idx = -1;
-      for(int i=0;i<requests.size();i++) {
-        if (Integer.parseInt(requests.get(i).categoryPath.components[0].substring(3)) == dim) {
-          idx = i;
-          break;
-        }
-      }
-      assert idx != -1;
-      FacetResult fr = actual.facetResults.get(idx);
-      List<FacetResultNode> subResults = fr.getFacetResultNode().subResults;
+      int topN = random().nextBoolean() ? dimValues[dim].length : _TestUtil.nextInt(random(), 1, dimValues[dim].length);
+      SimpleFacetResult fr = actual.facets.getTopChildren(topN, "dim"+dim);
       if (VERBOSE) {
-        System.out.println("    dim" + dim);
+        System.out.println("    dim" + dim + " topN=" + topN + " (vs " + dimValues[dim].length + " unique values)");
         System.out.println("      actual");
       }
 
+      int idx = 0;
       Map<String,Integer> actualValues = new HashMap<String,Integer>();
-      idx = 0;
-      for(FacetResultNode childNode : subResults) {
-        actualValues.put(childNode.label.components[1], (int) childNode.value);
-        if (VERBOSE) {
-          System.out.println("        " + idx + ": " + new BytesRef(childNode.label.components[1]) + ": " + (int) childNode.value);
-          idx++;
+
+      if (fr != null) {
+        for(LabelAndValue labelValue : fr.labelValues) {
+          actualValues.put(labelValue.label, labelValue.value.intValue());
+          if (VERBOSE) {
+            System.out.println("        " + idx + ": " + new BytesRef(labelValue.label) + ": " + labelValue.value);
+            idx++;
+          }
         }
       }
 
-      if (topN != -1) {
+      if (topN < dimValues[dim].length) {
         int[] topNIDs = getTopNOrds(expected.counts[dim], dimValues[dim], topN);
         if (VERBOSE) {
           idx = 0;
@@ -1022,16 +985,18 @@ public class TestSimpleDrillSideways extends FacetTestCase {
           System.out.println("      topN=" + topN + " expectedTopN=" + topNIDs.length);
         }
 
-        assertEquals(topNIDs.length, subResults.size());
+        if (fr != null) {
+          assertEquals(topNIDs.length, fr.labelValues.length);
+        } else {
+          assertEquals(0, topNIDs.length);
+        }
         for(int i=0;i<topNIDs.length;i++) {
-          FacetResultNode node = subResults.get(i);
           int expectedOrd = topNIDs[i];
-          assertEquals(expected.counts[dim][expectedOrd], (int) node.value);
-          assertEquals(2, node.label.length);
+          assertEquals(expected.counts[dim][expectedOrd], fr.labelValues[i].value.intValue());
           if (isSortedSetDV) {
             // Tie-break facet labels are only in unicode
             // order with SortedSetDVFacets:
-            assertEquals("value @ idx=" + i, dimValues[dim][expectedOrd], node.label.components[1]);
+            assertEquals("value @ idx=" + i, dimValues[dim][expectedOrd], fr.labelValues[i].label);
           }
         }
       } else {
@@ -1062,52 +1027,33 @@ public class TestSimpleDrillSideways extends FacetTestCase {
         assertEquals(setCount, actualValues.size());
       }
 
-      assertEquals("dim=" + dim, expected.uniqueCounts[dim], fr.getNumValidDescendants());
+      // nocommit if we add this to SimpleFR then re-enable this:
+      // assertEquals("dim=" + dim, expected.uniqueCounts[dim], fr.getNumValidDescendants());
     }
   }
 
-  / ** Just gathers counts of values under the dim. * /
-  private String toString(FacetResult fr) {
-    StringBuilder b = new StringBuilder();
-    FacetResultNode node = fr.getFacetResultNode();
-    b.append(node.label);
-    b.append(":");
-    for(FacetResultNode childNode : node.subResults) {
-      b.append(' ');
-      b.append(childNode.label.components[1]);
-      b.append('=');
-      b.append((int) childNode.value);
-    }
-    return b.toString();
-  }
-  
-  @Test
   public void testEmptyIndex() throws Exception {
     // LUCENE-5045: make sure DrillSideways works with an empty index
     Directory dir = newDirectory();
     Directory taxoDir = newDirectory();
-    writer = new RandomIndexWriter(random(), dir);
-    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
     IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
 
     // Count "Author"
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new FacetLabel("Author"), 10));
-
-    DrillSideways ds = new DrillSideways(searcher, taxoReader);
-    DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
-    ddq.add(new FacetLabel("Author", "Lisa"));
+    FacetsConfig config = new FacetsConfig();
+    SimpleDrillSideways ds = new SimpleDrillSideways(searcher, config, taxoReader);
+    SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config);
+    ddq.add("Author", "Lisa");
     
-    DrillSidewaysResult r = ds.search(null, ddq, 10, fsp); // this used to fail on IllegalArgEx
+    SimpleDrillSidewaysResult r = ds.search(ddq, 10); // this used to fail on IllegalArgEx
     assertEquals(0, r.hits.totalHits);
 
-    r = ds.search(ddq, null, null, 10, new Sort(new SortField("foo", Type.INT)), false, false, fsp); // this used to fail on IllegalArgEx
+    r = ds.search(ddq, null, null, 10, new Sort(new SortField("foo", SortField.Type.INT)), false, false); // this used to fail on IllegalArgEx
     assertEquals(0, r.hits.totalHits);
     
-    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
   }
-  */
 }
 
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSortedSetDocValuesFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSortedSetDocValuesFacets.java
index c929540..30de522 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSortedSetDocValuesFacets.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSortedSetDocValuesFacets.java
@@ -23,10 +23,12 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
 
 public class TestSortedSetDocValuesFacets extends FacetTestCase {
 
@@ -186,4 +188,37 @@ public class TestSortedSetDocValuesFacets extends FacetTestCase {
 
   // nocommit in the sparse case test that we are really
   // sorting by the correct dim count
+
+  public void testSlowCompositeReaderWrapper() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(null, new FacetsConfig());
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
+    writer.addDocument(builder.build(doc));
+
+    writer.commit();
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
+    writer.addDocument(builder.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = new IndexSearcher(SlowCompositeReaderWrapper.wrap(writer.getReader()));
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+
+    SimpleFacetsCollector c = new SimpleFacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+    Facets facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    assertEquals("a (2)\n  foo1 (1)\n  foo2 (1)\n", facets.getTopChildren(10, "a").toString());
+
+    IOUtils.close(writer, searcher.getIndexReader(), dir);
+  }
 }

