GitDiffStart: 8a71c443209d385218c79226902ce8750350a620 | Tue Aug 13 04:06:18 2013 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 96087b0..e7c4c97 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -23,18 +23,20 @@ Changes in backwards compatibility policy
   not positioned. This change affects all classes that inherit from
   DocIdSetIterator, including DocsEnum and DocsAndPositionsEnum. (Adrien Grand)
 
-* LUCENE-5089: Update to Morfologik 1.6.0. MorfologikAnalyzer and MorfologikFilter 
-  no longer support multiple "dictionaries" as there is only one dictionary available.
-  (Dawid Weiss)
+* LUCENE-5127: Reduce RAM usage of FixedGapTermsIndex. Remove 
+  IndexWriterConfig.setTermIndexInterval, IndexWriterConfig.setReaderTermsIndexDivisor,
+  and termsIndexDivisor from StandardDirectoryReader. These options have been no-ops
+  with the default codec since Lucene 4.0. If you want to configure the interval for
+  this term index, pass it directly in your codec, where it can also be configured
+  per-field. (Robert Muir)
 
 New Features
 
 * LUCENE-4747: Move to Java 7 as minimum Java version.
   (Robert Muir, Uwe Schindler)
 
-* LUCENE-5089: Update to Morfologik 1.6.0. MorfologikAnalyzer and MorfologikFilter 
-  no longer support multiple "dictionaries" as there is only one dictionary available.
-  (Dawid Weiss)
+* SOLR-3359: Added analyzer attribute/property to SynonymFilterFactory.
+  (Ryo Onodera via Koji Sekiguchi)
 
 Optimizations
 
@@ -57,11 +59,141 @@ New features
 * LUCENE-5098: New broadword utility methods in oal.util.BroadWord.
   (Paul Elschot via Adrien Grand, Dawid Weiss)
 
+* LUCENE-5030: FuzzySuggester now supports optional unicodeAware
+  (default is false).  If true then edits are measured in Unicode code
+  points instead of UTF8 bytes.  (Artem Lukanin via Mike McCandless)
+
+* LUCENE-5118: SpatialStrategy.makeDistanceValueSource() now has an optional
+  multiplier for scaling degrees to another unit. (David Smiley)
+
+* LUCENE-5091: SpanNotQuery can now be configured with pre and post slop to act
+  as a hypothetical SpanNotNearQuery. (Tim Allison via David Smiley)
+
+* LUCENE-4985: FacetsAccumulator.create() is now able to create a 
+  MultiFacetsAccumulator over a mixed set of facet requests. MultiFacetsAccumulator
+  allows wrapping multiple FacetsAccumulators, allowing to easily mix
+  existing and custom ones. TaxonomyFacetsAccumulator supports any
+  FacetRequest which implements createFacetsAggregator and was indexed
+  using the taxonomy index. (Shai Erera)
+
+* LUCENE-5153: AnalyzerWrapper.wrapReader allows wrapping the Reader given to 
+  inputReader. (Shai Erera)
+
+* LUCENE-5155: FacetRequest.getValueOf and .getFacetArraysSource replaced by
+  FacetsAggregator.createOrdinalValueResolver. This gives better options for
+  resolving an ordinal's value by FacetAggregators. (Shai Erera)
+
+Bug Fixes
+
+* LUCENE-5116: IndexWriter.addIndexes(IndexReader...) should drop empty (or all
+  deleted) segments. (Robert Muir, Shai Erera)
+
+* LUCENE-4734: Add FastVectorHighlighter support for proximity queries and
+  phrase queries with gaps or overlapping terms. (Ryan Lauck, Adrien Grand)
+
+* LUCENE-5132: Spatial RecursivePrefixTree Contains predicate will throw an NPE
+  when there's no indexed data and maybe in other circumstances too. (David Smiley)
+
+* LUCENE-5146: AnalyzingSuggester sort comparator read part of the input key as the
+  weight that caused the sorter to never sort by weight first since the weight is only
+  considered if the input is equal causing the malformed weight to be identical as well.
+  (Simon Willnauer)
+
+* LUCENE-5151: Associations FacetsAggregators could enter an infinite loop when
+  some result documents were missing category associations. (Shai Erera)
+
+* LUCENE-5152: Fix MemoryPostingsFormat to not modify borrowed BytesRef from FSTEnum
+  seek/lookup which can cause sideeffects if done on a cached FST root arc.
+  (Simon Willnauer)
+
+* LUCENE-5160: Handle the case where reading from a file or FileChannel returns -1,
+  which could happen in rare cases where something happens to the file between the
+  time we start the read loop (where we check the length) and when we actually do
+  the read. (gsingers, yonik, Robert Muir, Uwe Schindler)
+
+* LUCENE-5166: PostingsHighlighter would throw IOOBE if a term spanned the maxLength
+  boundary, made it into the top-N and went to the formatter.
+  (Manuel Amoabeng, Michael McCandless, Robert Muir)
+
 API Changes
 
 * LUCENE-5094: Add ramBytesUsed() to MultiDocValues.OrdinalMap.
   (Robert Muir)
 
+* LUCENE-5114: Remove unused boolean useCache parameter from
+  TermsEnum.seekCeil and .seekExact (Mike McCandless)
+
+* LUCENE-5128: IndexSearcher.searchAfter throws IllegalArgumentException if 
+  searchAfter exceeds the number of documents in the reader. 
+  (Crocket via Shai Erera)
+
+* LUCENE-5129: CategoryAssociationsContainer no longer supports null
+  association values for categories. If you want to index categories without
+  associations, you should add them using FacetFields. (Shai Erera)
+
+* LUCENE-4876: IndexWriter no longer clones the given IndexWriterConfig. If you
+  need to use the same config more than once, e.g. when sharing between multiple 
+  writers, make sure to clone it before passing to each writer.
+  (Shai Erera, Mike McCandless)
+
+* LUCENE-5144: StandardFacetsAccumulator renamed to OldFacetsAccumulator, and all
+  associated classes were moved under o.a.l.facet.old. The intention to remove it
+  one day, when the features it covers (complements, partitiona, sampling) will be
+  migrated to the new FacetsAggregator and FacetsAccumulator API. Also,
+  FacetRequest.createAggregator was replaced by OldFacetsAccumulator.createAggregator.
+  (Shai Erera)
+
+* LUCENE-5149: CommonTermsQuery now allows to set the minimum number of terms that 
+  should match for its high and low frequent sub-queries. Previously this was only
+  supported on the low frequent terms query. (Simon Willnauer)  
+
+* LUCENE-5156: CompressingTermVectors TermsEnum no longer supports ord().
+  (Robert Muir)
+
+* LUCENE-5161, LUCENE-5164: Fix default chunk sizes in FSDirectory to not be
+  unnecessarily large (now 8192 bytes); also use chunking when writing to index
+  files. FSDirectory#setReadChunkSize() is now deprecated and will be removed
+  in Lucene 5.0.  (Uwe Schindler, Robert Muir, gsingers)
+
+Optimizations
+
+* LUCENE-5088: Added TermFilter to filter docs by a specific term.
+  (Martijn van Groningen)
+
+* LUCENE-5119: DiskDV keeps the document-to-ordinal mapping on disk for 
+  SortedDocValues.  (Robert Muir)
+
+* LUCENE-5145: New AppendingPackedLongBuffer, a new variant of the former
+  AppendingLongBuffer which assumes values are 0-based.
+  (Boaz Leskes via Adrien Grand)
+
+* LUCENE-5145: All Appending*Buffer now support bulk get.
+  (Boaz Leskes via Adrien Grand)
+
+* LUCENE-5140: Fixed a performance regression of span queries caused by
+  LUCENE-4946. (Alan Woodward, Adrien Grand)
+
+* LUCENE-5150: Make WAH8DocIdSet able to inverse its encoding in order to
+  compress dense sets efficiently as well. (Adrien Grand)
+
+* LUCENE-5159: Prefix-code the sorted/sortedset value dictionaries in DiskDV.
+  (Robert Muir)
+
+Documentation
+
+* LUCENE-4894: remove facet userguide as it was outdated. Partially absorbed into
+  package's documentation and classes javadocs. (Shai Erera)
+
+Changes in backwards compatibility policy
+
+* LUCENE-5141: CheckIndex.fixIndex(Status,Codec) is now
+  CheckIndex.fixIndex(Status). If you used to pass a codec to this method, just
+  remove it from the arguments. (Adrien Grand)
+
+* LUCENE-5089, SOLR-5126: Update to Morfologik 1.7.1. MorfologikAnalyzer and MorfologikFilter 
+  no longer support multiple "dictionaries" as there is only one dictionary available.
+  (Dawid Weiss)
+
 ======================= Lucene 4.4.0 =======================
 
 Changes in backwards compatibility policy
@@ -229,6 +361,10 @@ Bug Fixes
   SortedSetDocValuesReaderState and SortedSetDocValuesAccumulator.
   (Robert Muir, Mike McCandless)
 
+* LUCENE-5120: AnalyzingSuggester modifed it's FST's cached root arc if payloads
+  are used and the entire output resided on the root arc on the first access. This
+  caused subsequent suggest calls to fail. (Simon Willnauer)
+
 Optimizations
 
 * LUCENE-4936: Improve numeric doc values compression in case all values share
@@ -335,6 +471,9 @@ API Changes
   longer needed. This method uses an internal reuseable reader, which was
   previously only used by the Field class.  (Uwe Schindler, Robert Muir)
   
+* LUCENE-4542: HunspellStemFilter's maximum recursion level is now configurable.
+  (Piotr, Rafa? Ku? via Adrien Grand)
+  
 Build
 
 * LUCENE-4987: Upgrade randomized testing to version 2.0.10: 
diff --git a/lucene/analysis/build.xml b/lucene/analysis/build.xml
index 48921b3..cd3ed55 100644
--- a/lucene/analysis/build.xml
+++ b/lucene/analysis/build.xml
@@ -134,5 +134,9 @@
   <target name="-ecj-javadoc-lint">
     <forall-analyzers target="-ecj-javadoc-lint"/>
   </target>
+
+  <target name="regenerate">
+    <forall-analyzers target="regenerate"/>
+  </target>
 	
 </project>
diff --git a/lucene/analysis/common/build.xml b/lucene/analysis/common/build.xml
index 75de0e7..2ed5b7a 100644
--- a/lucene/analysis/common/build.xml
+++ b/lucene/analysis/common/build.xml
@@ -29,24 +29,23 @@
 
   <import file="../analysis-module-build.xml"/>
 	
-  <target name="compile-core" depends="jflex-notice, common.compile-core"/>
+  <target name="jflex" depends="jflex-check,clean-jflex,-gen-uax29-supp-macros,
+                                -jflex-StandardAnalyzer,-jflex-UAX29URLEmailTokenizer,
+                                -jflex-wiki-tokenizer,-jflex-HTMLStripCharFilter"/>
 
-  <target name="jflex" depends="jflex-check,clean-jflex,gen-uax29-supp-macros,
-                                jflex-StandardAnalyzer,jflex-UAX29URLEmailTokenizer,
-                                jflex-wiki-tokenizer,jflex-HTMLStripCharFilter"/>
-
-  <target name="gen-uax29-supp-macros">
+  <target name="-gen-uax29-supp-macros">
     <subant target="gen-uax29-supp-macros">
        <fileset dir="../icu" includes="build.xml"/>
     </subant>
   </target>
 
-  <target name="jflex-HTMLStripCharFilter"
+  <target name="-jflex-HTMLStripCharFilter"
           depends="init,jflex-check,generate-jflex-html-char-entities"
           if="jflex.present">
     <taskdef classname="jflex.anttask.JFlexTask" name="jflex">
       <classpath refid="jflex.classpath"/>
     </taskdef>
+    <!-- this logic below looks duplicated with run-jflex, but its not, the regexp is different! -->
     <jflex file="src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex"
            outdir="src/java/org/apache/lucene/analysis/charfilter"
            nobak="on"/>
@@ -54,6 +53,9 @@
     <replaceregexp file="src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java"
                    match="/\*\*\s*\*\s*Creates a new scanner.*this\(new java\.io\.InputStreamReader\(in\)\);\s*\}"
                    replace="" flags="sg"/>
+    <replaceregexp file="src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java"
+                   match="\/\*\s*The following code was generated by JFlex.*"
+                   replace="\/\* The following code was generated by JFlex. \*\/" flags=""/>
   </target>
 
   <target name="generate-jflex-html-char-entities">
@@ -67,14 +69,14 @@
     <fixcrlf file="src/java/org/apache/lucene/analysis/charfilter/HTMLCharacterEntities.jflex" encoding="UTF-8"/>
   </target>
 
-  <target name="jflex-wiki-tokenizer" depends="init,jflex-check" if="jflex.present">
+  <target name="-jflex-wiki-tokenizer" depends="init,jflex-check" if="jflex.present">
     <taskdef classname="jflex.anttask.JFlexTask" name="jflex">
       <classpath refid="jflex.classpath"/>
     </taskdef>
     <run-jflex dir="src/java/org/apache/lucene/analysis/wikipedia" name="WikipediaTokenizerImpl"/>
   </target>
 
-  <target name="jflex-StandardAnalyzer" depends="init,jflex-check" if="jflex.present">
+  <target name="-jflex-StandardAnalyzer" depends="init,jflex-check" if="jflex.present">
     <taskdef classname="jflex.anttask.JFlexTask" name="jflex">
 			<classpath refid="jflex.classpath"/>
     </taskdef>
@@ -82,7 +84,7 @@
     <run-jflex dir="src/java/org/apache/lucene/analysis/standard" name="ClassicTokenizerImpl"/>
   </target>
 
-  <target name="jflex-UAX29URLEmailTokenizer" depends="jflex-check" if="jflex.present">
+  <target name="-jflex-UAX29URLEmailTokenizer" depends="jflex-check" if="jflex.present">
     <taskdef classname="jflex.anttask.JFlexTask" name="jflex">
 			<classpath refid="jflex.classpath"/>
     </taskdef>
@@ -100,6 +102,9 @@
       <replaceregexp file="@{dir}/@{name}.java"
                      match="/\*\*\s*\*\s*Creates a new scanner\..*this\(new java\.io\.InputStreamReader\(in\)\);\s*\}"
                      replace="" flags="sg"/>
+      <replaceregexp file="@{dir}/@{name}.java"
+                     match="\/\*\s*The following code was generated by JFlex.*"
+                     replace="\/\* The following code was generated by JFlex. \*\/" flags=""/>
     </sequential>
   </macrodef>
 
@@ -143,4 +148,6 @@
   </target>
 
   <target name="javadocs" depends="module-build.javadocs"/>
+
+  <target name="regenerate" depends="jflex"/>
 </project>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.SUPPLEMENTARY.jflex-macro b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.SUPPLEMENTARY.jflex-macro
index e96daa0..f0a1e5d 100755
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.SUPPLEMENTARY.jflex-macro
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.SUPPLEMENTARY.jflex-macro
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-// Generated using ICU4J 49.1.0.0 on Sunday, July 15, 2012 5:42:00 AM UTC
+// Generated using ICU4J 49.1.0.0
 // by org.apache.lucene.analysis.icu.GenerateHTMLStripCharFilterSupplementaryMacros
 
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java
index 6f05418..315c0eb 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.java
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/19/12 6:24 PM */
+/* The following code was generated by JFlex. */
 
 package org.apache.lucene.analysis.charfilter;
 
@@ -31,18 +31,10 @@ import org.apache.lucene.analysis.util.CharArrayMap;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.analysis.util.OpenStringBuilder;
 
-
 /**
  * A CharFilter that wraps another Reader and attempts to strip out HTML constructs.
  */
-@SuppressWarnings("fallthrough")
 
-/**
- * This class is a scanner generated by 
- * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 9/19/12 6:24 PM from the specification file
- * <tt>C:/svn/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex</tt>
- */
 public final class HTMLStripCharFilter extends BaseCharFilter {
 
   /** This character denotes the end of file */
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex
index f6f6fcd..655b427 100755
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.jflex
@@ -29,11 +29,9 @@ import org.apache.lucene.analysis.util.CharArrayMap;
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.analysis.util.OpenStringBuilder;
 
-
 /**
  * A CharFilter that wraps another Reader and attempts to strip out HTML constructs.
  */
-@SuppressWarnings("fallthrough")
 %%
 
 %unicode 6.1
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilter.java
index b08da41..4ff0a74 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilter.java
@@ -55,17 +55,31 @@ public final class HunspellStemFilter extends TokenFilter {
   
   private final boolean dedup;
 
+  /** Create a {@link HunspellStemFilter} which deduplicates stems and has a maximum
+   *  recursion level of 2. 
+   *  @see #HunspellStemFilter(TokenStream, HunspellDictionary, int) */
+  public HunspellStemFilter(TokenStream input, HunspellDictionary dictionary) {
+    this(input, dictionary, 2);
+  }
+
   /**
    * Creates a new HunspellStemFilter that will stem tokens from the given TokenStream using affix rules in the provided
    * HunspellDictionary
    *
    * @param input TokenStream whose tokens will be stemmed
    * @param dictionary HunspellDictionary containing the affix rules and words that will be used to stem the tokens
+   * @param recursionCap maximum level of recursion stemmer can go into, defaults to <code>2</code>
    */
-  public HunspellStemFilter(TokenStream input, HunspellDictionary dictionary) {
-    this(input, dictionary, true);
+  public HunspellStemFilter(TokenStream input, HunspellDictionary dictionary, int recursionCap) {
+    this(input, dictionary, true, recursionCap);
   }
-  
+
+  /** Create a {@link HunspellStemFilter} which has a maximum recursion level of 2. 
+   *  @see #HunspellStemFilter(TokenStream, HunspellDictionary, boolean, int) */
+  public HunspellStemFilter(TokenStream input, HunspellDictionary dictionary, boolean dedup) {
+    this(input, dictionary, dedup, 2);
+  }
+
   /**
    * Creates a new HunspellStemFilter that will stem tokens from the given TokenStream using affix rules in the provided
    * HunspellDictionary
@@ -73,11 +87,12 @@ public final class HunspellStemFilter extends TokenFilter {
    * @param input TokenStream whose tokens will be stemmed
    * @param dictionary HunspellDictionary containing the affix rules and words that will be used to stem the tokens
    * @param dedup true if only unique terms should be output.
+   * @param recursionCap maximum level of recursion stemmer can go into, defaults to <code>2</code>
    */
-  public HunspellStemFilter(TokenStream input, HunspellDictionary dictionary, boolean dedup) {
+  public HunspellStemFilter(TokenStream input, HunspellDictionary dictionary, boolean dedup, int recursionCap) {
     super(input);
     this.dedup = dedup;
-    this.stemmer = new HunspellStemmer(dictionary);
+    this.stemmer = new HunspellStemmer(dictionary, recursionCap);
   }
 
   /**
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilterFactory.java
index 252fc6a..63e621c 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemFilterFactory.java
@@ -54,12 +54,14 @@ public class HunspellStemFilterFactory extends TokenFilterFactory implements Res
   private static final String PARAM_AFFIX = "affix";
   private static final String PARAM_IGNORE_CASE = "ignoreCase";
   private static final String PARAM_STRICT_AFFIX_PARSING = "strictAffixParsing";
+  private static final String PARAM_RECURSION_CAP = "recursionCap";
 
   private final String dictionaryArg;
   private final String affixFile;
   private final boolean ignoreCase;
   private final boolean strictAffixParsing;
   private HunspellDictionary dictionary;
+  private int recursionCap;
   
   /** Creates a new HunspellStemFilterFactory */
   public HunspellStemFilterFactory(Map<String,String> args) {
@@ -69,6 +71,7 @@ public class HunspellStemFilterFactory extends TokenFilterFactory implements Res
     affixFile = get(args, PARAM_AFFIX);
     ignoreCase = getBoolean(args, PARAM_IGNORE_CASE, false);
     strictAffixParsing = getBoolean(args, PARAM_STRICT_AFFIX_PARSING, true);
+    recursionCap = getInt(args, PARAM_RECURSION_CAP, 2);
     if (!args.isEmpty()) {
       throw new IllegalArgumentException("Unknown parameters: " + args);
     }
@@ -111,6 +114,6 @@ public class HunspellStemFilterFactory extends TokenFilterFactory implements Res
    */
   @Override
   public TokenStream create(TokenStream tokenStream) {
-    return new HunspellStemFilter(tokenStream, dictionary);
+    return new HunspellStemFilter(tokenStream, dictionary, recursionCap);
   }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemmer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemmer.java
index 6a7bf89..b0ded28 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemmer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/HunspellStemmer.java
@@ -17,16 +17,10 @@ package org.apache.lucene.analysis.hunspell;
  * limitations under the License.
  */
 
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.charset.Charset;
-import java.text.ParseException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.List;
-import java.util.Scanner;
 
 import org.apache.lucene.analysis.util.CharArraySet;
 import org.apache.lucene.analysis.util.CharacterUtils;
@@ -37,23 +31,33 @@ import org.apache.lucene.util.Version;
  * conforms to the algorithm in the original hunspell algorithm, including recursive suffix stripping.
  */
 public class HunspellStemmer {
-
-  private static final int RECURSION_CAP = 2;
-  
+  private final int recursionCap;
   private final HunspellDictionary dictionary;
   private final StringBuilder segment = new StringBuilder();
   private CharacterUtils charUtils = CharacterUtils.getInstance(Version.LUCENE_40);
 
   /**
-   * Constructs a new HunspellStemmer which will use the provided HunspellDictionary to create its stems
+   * Constructs a new HunspellStemmer which will use the provided HunspellDictionary to create its stems. Uses the 
+   * default recursion cap of <code>2</code> (based on Hunspell documentation). 
    *
    * @param dictionary HunspellDictionary that will be used to create the stems
    */
   public HunspellStemmer(HunspellDictionary dictionary) {
-    this.dictionary = dictionary;
+    this(dictionary, 2);
   }
 
   /**
+   * Constructs a new HunspellStemmer which will use the provided HunspellDictionary to create its stems
+   *
+   * @param dictionary HunspellDictionary that will be used to create the stems
+   * @param recursionCap maximum level of recursion stemmer can go into
+   */
+  public HunspellStemmer(HunspellDictionary dictionary, int recursionCap) {
+    this.dictionary = dictionary;
+    this.recursionCap = recursionCap;
+  } 
+  
+  /**
    * Find the stem(s) of the provided word
    * 
    * @param word Word to find the stems for
@@ -194,7 +198,7 @@ public class HunspellStemmer {
       }
     }
 
-    if (affix.isCrossProduct() && recursionDepth < RECURSION_CAP) {
+    if (affix.isCrossProduct() && recursionDepth < recursionCap) {
       stems.addAll(stem(strippedWord, length, affix.getAppendFlags(), ++recursionDepth));
     }
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilter.java
old mode 100755
new mode 100644
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/HyphenatedWordsFilterFactory.java
old mode 100755
new mode 100644
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.java
index 891fbd6..60fc961 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.java
@@ -84,11 +84,6 @@ public final class PerFieldAnalyzerWrapper extends AnalyzerWrapper {
   }
 
   @Override
-  protected TokenStreamComponents wrapComponents(String fieldName, TokenStreamComponents components) {
-    return components;
-  }
-  
-  @Override
   public String toString() {
     return "PerFieldAnalyzerWrapper(" + fieldAnalyzers + ", default=" + defaultAnalyzer + ")";
   }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizerFactory.java
old mode 100755
new mode 100644
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizerFactory.java
old mode 100755
new mode 100644
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java
index 90c70f8..3ce5897 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/19/12 6:23 PM */
+/* The following code was generated by JFlex. */
 
 package org.apache.lucene.analysis.standard;
 
@@ -29,13 +29,10 @@ WARNING: if you change ClassicTokenizerImpl.jflex and need to regenerate
 import java.io.Reader;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
-
 /**
- * This class is a scanner generated by 
- * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 9/19/12 6:23 PM from the specification file
- * <tt>C:/svn/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex</tt>
+ * This class implements the classic lucene StandardTokenizer up until 3.0 
  */
+
 class ClassicTokenizerImpl implements StandardTokenizerInterface {
 
   /** This character denotes the end of file */
@@ -359,7 +356,6 @@ public static final int ACRONYM_DEP       = StandardTokenizer.ACRONYM_DEP;
 
 public static final String [] TOKEN_TYPES = StandardTokenizer.TOKEN_TYPES;
 
-@Override
 public final int yychar()
 {
     return yychar;
@@ -368,7 +364,6 @@ public final int yychar()
 /**
  * Fills CharTermAttribute with the current token text.
  */
-@Override
 public final void getText(CharTermAttribute t) {
   t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
 }
@@ -484,7 +479,6 @@ public final void getText(CharTermAttribute t) {
    *
    * @param reader   the new input stream 
    */
-  @Override
   public final void yyreset(java.io.Reader reader) {
     zzReader = reader;
     zzAtBOL  = true;
@@ -544,7 +538,6 @@ public final void getText(CharTermAttribute t) {
   /**
    * Returns the length of the matched text region.
    */
-  @Override
   public final int yylength() {
     return zzMarkedPos-zzStartRead;
   }
@@ -600,7 +593,6 @@ public final void getText(CharTermAttribute t) {
    * @return      the next token
    * @exception   java.io.IOException  if any I/O-Error occurs
    */
-  @Override
   public int getNextToken() throws java.io.IOException {
     int zzInput;
     int zzAction;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex
index 3d4a3a1..4d408b9 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex
@@ -27,6 +27,9 @@ WARNING: if you change ClassicTokenizerImpl.jflex and need to regenerate
 import java.io.Reader;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
+/**
+ * This class implements the classic lucene StandardTokenizer up until 3.0 
+ */
 %%
 
 %class ClassicTokenizerImpl
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro
index 60aab1a..efc0fe1 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/SUPPLEMENTARY.jflex-macro
@@ -14,7 +14,7 @@
  * limitations under the License.
  */
 
-// Generated using ICU4J 49.1.0.0 on Wednesday, September 19, 2012 10:23:34 PM UTC
+// Generated using ICU4J 49.1.0.0
 // by org.apache.lucene.analysis.icu.GenerateJFlexSupplementaryMacros
 
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java
index 20081ae..f5ff031 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/19/12 6:23 PM */
+/* The following code was generated by JFlex. */
 
 package org.apache.lucene.analysis.standard;
 
@@ -843,7 +843,6 @@ public final class StandardTokenizerImpl implements StandardTokenizerInterface {
   
   public static final int HANGUL_TYPE = StandardTokenizer.HANGUL;
 
-  @Override
   public final int yychar()
   {
     return yychar;
@@ -852,7 +851,6 @@ public final class StandardTokenizerImpl implements StandardTokenizerInterface {
   /**
    * Fills CharTermAttribute with the current token text.
    */
-  @Override
   public final void getText(CharTermAttribute t) {
     t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
   }
@@ -967,7 +965,6 @@ public final class StandardTokenizerImpl implements StandardTokenizerInterface {
    *
    * @param reader   the new input stream 
    */
-  @Override
   public final void yyreset(java.io.Reader reader) {
     zzReader = reader;
     zzAtBOL  = true;
@@ -1027,7 +1024,6 @@ public final class StandardTokenizerImpl implements StandardTokenizerInterface {
   /**
    * Returns the length of the matched text region.
    */
-  @Override
   public final int yylength() {
     return zzMarkedPos-zzStartRead;
   }
@@ -1083,7 +1079,6 @@ public final class StandardTokenizerImpl implements StandardTokenizerInterface {
    * @return      the next token
    * @exception   java.io.IOException  if any I/O-Error occurs
    */
-  @Override
   public int getNextToken() throws java.io.IOException {
     int zzInput;
     int zzAction;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailAnalyzer.java
old mode 100755
new mode 100644
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java
index 764c5f1..1619e45 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/19/12 6:23 PM */
+/* The following code was generated by JFlex. */
 
 package org.apache.lucene.analysis.standard;
 
@@ -4033,7 +4033,6 @@ public final class UAX29URLEmailTokenizerImpl implements StandardTokenizerInterf
   
   public static final int URL_TYPE = UAX29URLEmailTokenizer.URL;
 
-  @Override
   public final int yychar()
   {
     return yychar;
@@ -4042,7 +4041,6 @@ public final class UAX29URLEmailTokenizerImpl implements StandardTokenizerInterf
   /**
    * Fills CharTermAttribute with the current token text.
    */
-  @Override
   public final void getText(CharTermAttribute t) {
     t.copyBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
   }
@@ -4157,7 +4155,6 @@ public final class UAX29URLEmailTokenizerImpl implements StandardTokenizerInterf
    *
    * @param reader   the new input stream 
    */
-  @Override
   public final void yyreset(java.io.Reader reader) {
     zzReader = reader;
     zzAtBOL  = true;
@@ -4217,7 +4214,6 @@ public final class UAX29URLEmailTokenizerImpl implements StandardTokenizerInterf
   /**
    * Returns the length of the matched text region.
    */
-  @Override
   public final int yylength() {
     return zzMarkedPos-zzStartRead;
   }
@@ -4273,7 +4269,6 @@ public final class UAX29URLEmailTokenizerImpl implements StandardTokenizerInterf
    * @return      the next token
    * @exception   java.io.IOException  if any I/O-Error occurs
    */
-  @Override
   public int getNextToken() throws java.io.IOException {
     int zzInput;
     int zzAction;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java
index c06ba32..0344db4 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java
@@ -68,6 +68,7 @@ public class SynonymFilterFactory extends TokenFilterFactory implements Resource
   private final String synonyms;
   private final String format;
   private final boolean expand;
+  private final String analyzerName;
   private final Map<String, String> tokArgs = new HashMap<String, String>();
 
   private SynonymMap map;
@@ -79,7 +80,13 @@ public class SynonymFilterFactory extends TokenFilterFactory implements Resource
     format = get(args, "format");
     expand = getBoolean(args, "expand", true);
 
+    analyzerName = get(args, "analyzer");
     tokenizerFactory = get(args, "tokenizerFactory");
+    if (analyzerName != null && tokenizerFactory != null) {
+      throw new IllegalArgumentException("Analyzer and TokenizerFactory can't be specified both: " +
+                                         analyzerName + " and " + tokenizerFactory);
+    }
+
     if (tokenizerFactory != null) {
       assureMatchVersion();
       tokArgs.put("luceneMatchVersion", getLuceneMatchVersion().toString());
@@ -104,15 +111,20 @@ public class SynonymFilterFactory extends TokenFilterFactory implements Resource
   @Override
   public void inform(ResourceLoader loader) throws IOException {
     final TokenizerFactory factory = tokenizerFactory == null ? null : loadTokenizerFactory(loader, tokenizerFactory);
+    Analyzer analyzer;
     
-    Analyzer analyzer = new Analyzer() {
-      @Override
-      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-        Tokenizer tokenizer = factory == null ? new WhitespaceTokenizer(Version.LUCENE_50, reader) : factory.create(reader);
-        TokenStream stream = ignoreCase ? new LowerCaseFilter(Version.LUCENE_50, tokenizer) : tokenizer;
-        return new TokenStreamComponents(tokenizer, stream);
-      }
-    };
+    if (analyzerName != null) {
+      analyzer = loadAnalyzer(loader, analyzerName);
+    } else {
+      analyzer = new Analyzer() {
+        @Override
+        protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+          Tokenizer tokenizer = factory == null ? new WhitespaceTokenizer(Version.LUCENE_50, reader) : factory.create(reader);
+          TokenStream stream = ignoreCase ? new LowerCaseFilter(Version.LUCENE_50, tokenizer) : tokenizer;
+          return new TokenStreamComponents(tokenizer, stream);
+        }
+      };
+    }
 
     try {
       if (format == null || format.equals("solr")) {
@@ -188,4 +200,17 @@ public class SynonymFilterFactory extends TokenFilterFactory implements Resource
       throw new RuntimeException(e);
     }
   }
+
+  private Analyzer loadAnalyzer(ResourceLoader loader, String cname) throws IOException {
+    Class<? extends Analyzer> clazz = loader.findClass(cname, Analyzer.class);
+    try {
+      Analyzer analyzer = clazz.getConstructor(Version.class).newInstance(Version.LUCENE_50);
+      if (analyzer instanceof ResourceLoaderAware) {
+        ((ResourceLoaderAware) analyzer).inform(loader);
+      }
+      return analyzer;
+    } catch (Exception e) {
+      throw new RuntimeException(e);
+    }
+  }
 }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java
index af30284..f572318 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java
@@ -1,4 +1,4 @@
-/* The following code was generated by JFlex 1.5.0-SNAPSHOT on 9/19/12 6:23 PM */
+/* The following code was generated by JFlex. */
 
 package org.apache.lucene.analysis.wikipedia;
 
@@ -21,13 +21,10 @@ package org.apache.lucene.analysis.wikipedia;
 
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
-
 /**
- * This class is a scanner generated by 
- * <a href="http://www.jflex.de/">JFlex</a> 1.5.0-SNAPSHOT
- * on 9/19/12 6:23 PM from the specification file
- * <tt>C:/svn/lucene/dev/trunk/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex</tt>
+ * JFlex-generated tokenizer that is aware of Wikipedia syntax.
  */
+
 class WikipediaTokenizerImpl {
 
   /** This character denotes the end of file */
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
index 4a55847..3865ea0 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
@@ -19,6 +19,9 @@ package org.apache.lucene.analysis.wikipedia;
 
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
+/**
+ * JFlex-generated tokenizer that is aware of Wikipedia syntax.
+ */
 %%
 
 %class WikipediaTokenizerImpl
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUAX29URLEmailAnalyzer.java
old mode 100755
new mode 100644
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/hunspell/HunspellStemFilterTest.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/hunspell/HunspellStemFilterTest.java
index cef50a4..4679b45 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/hunspell/HunspellStemFilterTest.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/hunspell/HunspellStemFilterTest.java
@@ -30,6 +30,7 @@ import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
 import org.apache.lucene.analysis.miscellaneous.SetKeywordMarkerFilter;
 import org.apache.lucene.analysis.util.CharArraySet;
+import org.apache.lucene.util._TestUtil;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
@@ -57,13 +58,13 @@ public class HunspellStemFilterTest  extends BaseTokenStreamTestCase {
   public void testKeywordAttribute() throws IOException {
     MockTokenizer tokenizer = new MockTokenizer(new StringReader("lucene is awesome"), MockTokenizer.WHITESPACE, true);
     tokenizer.setEnableChecks(true);
-    HunspellStemFilter filter = new HunspellStemFilter(tokenizer, DICTIONARY);
+    HunspellStemFilter filter = new HunspellStemFilter(tokenizer, DICTIONARY, _TestUtil.nextInt(random(), 1, 3));
     assertTokenStreamContents(filter, new String[]{"lucene", "lucen", "is", "awesome"}, new int[] {1, 0, 1, 1});
     
     // assert with keywork marker
     tokenizer = new MockTokenizer(new StringReader("lucene is awesome"), MockTokenizer.WHITESPACE, true);
     CharArraySet set = new CharArraySet(TEST_VERSION_CURRENT, Arrays.asList("Lucene"), true);
-    filter = new HunspellStemFilter(new SetKeywordMarkerFilter(tokenizer, set), DICTIONARY);
+    filter = new HunspellStemFilter(new SetKeywordMarkerFilter(tokenizer, set), DICTIONARY, _TestUtil.nextInt(random(), 1, 3));
     assertTokenStreamContents(filter, new String[]{"lucene", "is", "awesome"}, new int[] {1, 1, 1});
   }
   
@@ -74,7 +75,7 @@ public class HunspellStemFilterTest  extends BaseTokenStreamTestCase {
       @Override
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
         Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
-        return new TokenStreamComponents(tokenizer, new HunspellStemFilter(tokenizer, DICTIONARY));
+        return new TokenStreamComponents(tokenizer, new HunspellStemFilter(tokenizer, DICTIONARY, _TestUtil.nextInt(random(), 1, 3)));
       }  
     };
     checkRandomData(random(), analyzer, 1000*RANDOM_MULTIPLIER);
@@ -85,7 +86,7 @@ public class HunspellStemFilterTest  extends BaseTokenStreamTestCase {
       @Override
       protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
         Tokenizer tokenizer = new KeywordTokenizer(reader);
-        return new TokenStreamComponents(tokenizer, new HunspellStemFilter(tokenizer, DICTIONARY));
+        return new TokenStreamComponents(tokenizer, new HunspellStemFilter(tokenizer, DICTIONARY, _TestUtil.nextInt(random(), 1, 3)));
       }
     };
     checkOneTermReuse(a, "", "");
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestHyphenatedWordsFilter.java
old mode 100755
new mode 100644
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilterFactory.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilterFactory.java
index 6cf3bc2..88ed6bb 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilterFactory.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/TestSynonymFilterFactory.java
@@ -29,6 +29,7 @@ import org.apache.lucene.analysis.util.TokenFilterFactory;
 import org.apache.lucene.analysis.util.BaseTokenStreamFactoryTestCase;
 import org.apache.lucene.analysis.util.ClasspathResourceLoader;
 import org.apache.lucene.analysis.util.StringMockResourceLoader;
+import org.apache.lucene.analysis.cjk.CJKAnalyzer;
 
 public class TestSynonymFilterFactory extends BaseTokenStreamFactoryTestCase {
   /** test that we can parse and use the solr syn file */
@@ -64,6 +65,28 @@ public class TestSynonymFilterFactory extends BaseTokenStreamFactoryTestCase {
     }
   }
 
+  /** Test that analyzer and tokenizerFactory is both specified */
+  public void testAnalyzer() throws Exception {
+    final String analyzer = CJKAnalyzer.class.getName();
+    final String tokenizerFactory = PatternTokenizerFactory.class.getName();
+    TokenFilterFactory factory = null;
+
+    factory = tokenFilterFactory("Synonym",
+        "synonyms", "synonyms2.txt",
+        "analyzer", analyzer);
+    assertNotNull(factory);
+
+    try {
+      tokenFilterFactory("Synonym",
+          "synonyms", "synonyms.txt",
+          "analyzer", analyzer,
+          "tokenizerFactory", tokenizerFactory);
+      fail();
+    } catch (IllegalArgumentException expected) {
+      assertTrue(expected.getMessage().contains("Analyzer and TokenizerFactory can't be specified both"));
+    }
+  }
+
   static final String TOK_SYN_ARG_VAL = "argument";
   static final String TOK_FOO_ARG_VAL = "foofoofoo";
 
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/synonyms2.txt b/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/synonyms2.txt
new file mode 100644
index 0000000..25c56c8
--- /dev/null
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/synonym/synonyms2.txt
@@ -0,0 +1,15 @@
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+#-----------------------------------------------------------------------
+
+?? => ?????
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArraySet.java
old mode 100755
new mode 100644
diff --git a/lucene/analysis/icu/build.xml b/lucene/analysis/icu/build.xml
index 5d3c76d..e768c08 100644
--- a/lucene/analysis/icu/build.xml
+++ b/lucene/analysis/icu/build.xml
@@ -150,4 +150,6 @@ are part of the ICU4C package. See http://site.icu-project.org/ </echo>
     </compile>
   </target>
 
+  <target name="regenerate" depends="gen-html-strip-charfilter-supp-macros,gen-uax29-supp-macros,gen-utr30-data-files,gennorm2,genrbbi"/>
+
 </project>
diff --git a/lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateHTMLStripCharFilterSupplementaryMacros.java b/lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateHTMLStripCharFilterSupplementaryMacros.java
index 18328cc..e50abbd 100644
--- a/lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateHTMLStripCharFilterSupplementaryMacros.java
+++ b/lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateHTMLStripCharFilterSupplementaryMacros.java
@@ -60,8 +60,7 @@ public class GenerateHTMLStripCharFilterSupplementaryMacros {
 
   static void outputHeader() {
     System.out.print(APACHE_LICENSE);
-    System.out.print("// Generated using ICU4J " + VersionInfo.ICU_VERSION.toString() + " on ");
-    System.out.println(DATE_FORMAT.format(new Date()));
+    System.out.println("// Generated using ICU4J " + VersionInfo.ICU_VERSION.toString());
     System.out.println("// by " + GenerateHTMLStripCharFilterSupplementaryMacros.class.getName());
     System.out.print(NL + NL);
   }
diff --git a/lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateJFlexSupplementaryMacros.java b/lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateJFlexSupplementaryMacros.java
index 3292418..23cc391 100644
--- a/lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateJFlexSupplementaryMacros.java
+++ b/lucene/analysis/icu/src/tools/java/org/apache/lucene/analysis/icu/GenerateJFlexSupplementaryMacros.java
@@ -74,8 +74,7 @@ public class GenerateJFlexSupplementaryMacros {
   
   static void outputHeader() {
     System.out.print(APACHE_LICENSE);
-    System.out.print("// Generated using ICU4J " + VersionInfo.ICU_VERSION.toString() + " on ");
-    System.out.println(DATE_FORMAT.format(new Date()));
+    System.out.println("// Generated using ICU4J " + VersionInfo.ICU_VERSION.toString());
     System.out.println("// by " + GenerateJFlexSupplementaryMacros.class.getName());
     System.out.print(NL + NL);
   }
diff --git a/lucene/analysis/kuromoji/build.xml b/lucene/analysis/kuromoji/build.xml
index 2428bac..fd901af 100644
--- a/lucene/analysis/kuromoji/build.xml
+++ b/lucene/analysis/kuromoji/build.xml
@@ -139,4 +139,6 @@
   <!-- TODO: not until we properly make 'test-tools' work with clover etc
   <target name="test" depends="module-build.test, test-tools"/> -->
 
+  <target name="regenerate" depends="build-dict"/>
+
 </project>
diff --git a/lucene/analysis/kuromoji/src/resources/org/apache/lucene/analysis/ja/dict/TokenInfoDictionary$fst.dat b/lucene/analysis/kuromoji/src/resources/org/apache/lucene/analysis/ja/dict/TokenInfoDictionary$fst.dat
index 538cd4c..ea5c43c 100644
Binary files a/lucene/analysis/kuromoji/src/resources/org/apache/lucene/analysis/ja/dict/TokenInfoDictionary$fst.dat and b/lucene/analysis/kuromoji/src/resources/org/apache/lucene/analysis/ja/dict/TokenInfoDictionary$fst.dat differ
diff --git a/lucene/analysis/morfologik/ivy.xml b/lucene/analysis/morfologik/ivy.xml
index 0c9c337..c4dd72f 100644
--- a/lucene/analysis/morfologik/ivy.xml
+++ b/lucene/analysis/morfologik/ivy.xml
@@ -19,9 +19,9 @@
 <ivy-module version="2.0">
     <info organisation="org.apache.lucene" module="analyzers-morfologik"/>
     <dependencies>
-      <dependency org="org.carrot2" name="morfologik-polish" rev="1.6.0" transitive="false"/>
-      <dependency org="org.carrot2" name="morfologik-fsa" rev="1.6.0" transitive="false"/>
-      <dependency org="org.carrot2" name="morfologik-stemming" rev="1.6.0" transitive="false"/>
+      <dependency org="org.carrot2" name="morfologik-polish" rev="1.7.1" transitive="false"/>
+      <dependency org="org.carrot2" name="morfologik-fsa" rev="1.7.1" transitive="false"/>
+      <dependency org="org.carrot2" name="morfologik-stemming" rev="1.7.1" transitive="false"/>
       <exclude org="*" ext="*" matcher="regexp" type="${ivy.exclude.types}"/> 
     </dependencies>
 </ivy-module>
diff --git a/lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorfologikFilter.java b/lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorfologikFilter.java
index 049dad1..5ac14cd 100644
--- a/lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorfologikFilter.java
+++ b/lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorfologikFilter.java
@@ -20,6 +20,7 @@ package org.apache.lucene.analysis.morfologik;
 
 import java.io.IOException;
 import java.util.*;
+import java.util.regex.Pattern;
 
 import morfologik.stemming.*;
 
@@ -82,71 +83,29 @@ public class MorfologikFilter extends TokenFilter {
   }
 
   /**
-   * The tag encoding format has been changing in Morfologik from version
-   * to version. Let's keep both variants and determine which one to run
-   * based on this flag.
+   * A pattern used to split lemma forms.
    */
-  private final static boolean multipleTagsPerLemma = true;
+  private final static Pattern lemmaSplitter = Pattern.compile("\\+|\\|");
 
   private void popNextLemma() {
-    if (multipleTagsPerLemma) {
-      // One tag (concatenated) per lemma.
-      final WordData lemma = lemmaList.get(lemmaListIndex++);
-      termAtt.setEmpty().append(lemma.getStem());
-      CharSequence tag = lemma.getTag();
-      if (tag != null) {
-        String[] tags = tag.toString().split("\\+|\\|");
-        for (int i = 0; i < tags.length; i++) {
-          if (tagsList.size() <= i) {
-            tagsList.add(new StringBuilder());
-          }
-          StringBuilder buffer = tagsList.get(i);
-          buffer.setLength(0);
-          buffer.append(tags[i]);
+    // One tag (concatenated) per lemma.
+    final WordData lemma = lemmaList.get(lemmaListIndex++);
+    termAtt.setEmpty().append(lemma.getStem());
+    CharSequence tag = lemma.getTag();
+    if (tag != null) {
+      String[] tags = lemmaSplitter.split(tag.toString());
+      for (int i = 0; i < tags.length; i++) {
+        if (tagsList.size() <= i) {
+          tagsList.add(new StringBuilder());
         }
-        tagsAtt.setTags(tagsList.subList(0, tags.length));
-      } else {
-        tagsAtt.setTags(Collections.<StringBuilder> emptyList());
+        StringBuilder buffer = tagsList.get(i);
+        buffer.setLength(0);
+        buffer.append(tags[i]);
       }
+      tagsAtt.setTags(tagsList.subList(0, tags.length));
     } else {
-      // One tag (concatenated) per stem (lemma repeated).
-      CharSequence currentStem;
-      int tags = 0;
-      do {
-        final WordData lemma = lemmaList.get(lemmaListIndex++);
-        currentStem = lemma.getStem();
-        final CharSequence tag = lemma.getTag();
-        if (tag != null) {
-          if (tagsList.size() <= tags) {
-            tagsList.add(new StringBuilder());
-          }
-  
-          final StringBuilder buffer = tagsList.get(tags++);  
-          buffer.setLength(0);
-          buffer.append(lemma.getTag());
-        }
-      } while (lemmaListIndex < lemmaList.size() &&
-               equalCharSequences(lemmaList.get(lemmaListIndex).getStem(), currentStem));
-
-      // Set the lemma's base form and tags as attributes.
-      termAtt.setEmpty().append(currentStem);
-      tagsAtt.setTags(tagsList.subList(0, tags));
-    }
-  }
-
-  /**
-   * Compare two char sequences for equality. Assumes non-null arguments. 
-   */
-  private static final boolean equalCharSequences(CharSequence s1, CharSequence s2) {
-    int len1 = s1.length();
-    int len2 = s2.length();
-    if (len1 != len2) return false;
-    for (int i = len1; --i >= 0;) {
-      if (s1.charAt(i) != s2.charAt(i)) { 
-        return false; 
-      }
+      tagsAtt.setTags(Collections.<StringBuilder> emptyList());
     }
-    return true;
   }
 
   /**
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DemoHTMLParser.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocData.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/DocData.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/HTMLParser.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/HTMLParser.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/NoMoreDataException.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/NoMoreDataException.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecGov2Parser.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/TrecGov2Parser.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddIndexesTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddIndexesTask.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/Judge.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/Judge.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQuery.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQuery.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/QualityQueryParser.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/trec/Trec1MQReader.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/DocNameExtractor.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/QualityQueriesFinder.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/quality/utils/SimpleQQParser.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksParse.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/AddIndexesTaskTest.java b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/AddIndexesTaskTest.java
old mode 100755
new mode 100644
diff --git a/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingSearchTestTask.java b/lucene/benchmark/src/test/org/apache/lucene/benchmark/byTask/tasks/CountingSearchTestTask.java
old mode 100755
new mode 100644
diff --git a/lucene/build.xml b/lucene/build.xml
index 4348450..5c419a4 100644
--- a/lucene/build.xml
+++ b/lucene/build.xml
@@ -610,4 +610,13 @@
     <jar-checksum-macro srcdir="${common.dir}" dstdir="${common.dir}/licenses"/>
   </target>
 
+  <target name="regenerate">
+    <subant target="regenerate" failonerror="true" inheritall="false">
+      <propertyset refid="uptodate.and.compiled.properties"/>
+      <fileset dir="core" includes="build.xml"/>
+      <fileset dir="test-framework" includes="build.xml"/>
+    </subant>
+    <modules-crawl target="regenerate"/>
+  </target>
+
 </project>
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java
index 2d1fb8f..7fa0e14 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java
@@ -69,9 +69,6 @@ public class BlockTermsReader extends FieldsProducer {
 
   private final TreeMap<String,FieldReader> fields = new TreeMap<String,FieldReader>();
 
-  // Caches the most recently looked-up field + terms:
-  private final DoubleBarrelLRUCache<FieldAndTerm,BlockTermState> termsCache;
-
   // Reads the terms index
   private TermsIndexReaderBase indexReader;
 
@@ -113,11 +110,10 @@ public class BlockTermsReader extends FieldsProducer {
   // private String segment;
   
   public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,
-                          int termsCacheSize, String segmentSuffix)
+                          String segmentSuffix)
     throws IOException {
     
     this.postingsReader = postingsReader;
-    termsCache = new DoubleBarrelLRUCache<FieldAndTerm,BlockTermState>(termsCacheSize);
 
     // this.segment = segment;
     in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),
@@ -317,11 +313,6 @@ public class BlockTermsReader extends FieldsProducer {
          calls next() (which is not "typical"), then we'll do the real seek */
       private boolean seekPending;
 
-      /* How many blocks we've read since last seek.  Once this
-         is >= indexEnum.getDivisor() we set indexIsCurrent to false (since
-         the index can no long bracket seek-within-block). */
-      private int blocksSinceSeek;
-
       private byte[] termSuffixes;
       private ByteArrayDataInput termSuffixesReader = new ByteArrayDataInput();
 
@@ -362,13 +353,13 @@ public class BlockTermsReader extends FieldsProducer {
       // return NOT_FOUND so it's a waste for us to fill in
       // the term that was actually NOT_FOUND
       @Override
-      public SeekStatus seekCeil(final BytesRef target, final boolean useCache) throws IOException {
+      public SeekStatus seekCeil(final BytesRef target) throws IOException {
 
         if (indexEnum == null) {
           throw new IllegalStateException("terms index was not loaded");
         }
    
-        //System.out.println("BTR.seek seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + term().utf8ToString() + " " + term() + " useCache=" + useCache + " indexIsCurrent=" + indexIsCurrent + " didIndexNext=" + didIndexNext + " seekPending=" + seekPending + " divisor=" + indexReader.getDivisor() + " this="  + this);
+        //System.out.println("BTR.seek seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + term().utf8ToString() + " " + term() + " indexIsCurrent=" + indexIsCurrent + " didIndexNext=" + didIndexNext + " seekPending=" + seekPending + " divisor=" + indexReader.getDivisor() + " this="  + this);
         if (didIndexNext) {
           if (nextIndexTerm == null) {
             //System.out.println("  nextIndexTerm=null");
@@ -377,23 +368,6 @@ public class BlockTermsReader extends FieldsProducer {
           }
         }
 
-        // Check cache
-        if (useCache) {
-          fieldTerm.term = target;
-          // TODO: should we differentiate "frozen"
-          // TermState (ie one that was cloned and
-          // cached/returned by termState()) from the
-          // malleable (primary) one?
-          final TermState cachedState = termsCache.get(fieldTerm);
-          if (cachedState != null) {
-            seekPending = true;
-            //System.out.println("  cached!");
-            seekExact(target, cachedState);
-            //System.out.println("  term=" + term.utf8ToString());
-            return SeekStatus.FOUND;
-          }
-        }
-
         boolean doSeek = true;
 
         // See if we can avoid seeking, because target term
@@ -441,8 +415,7 @@ public class BlockTermsReader extends FieldsProducer {
           assert result;
 
           indexIsCurrent = true;
-          didIndexNext = false;
-          blocksSinceSeek = 0;          
+          didIndexNext = false;      
 
           if (doOrd) {
             state.ord = indexEnum.ord()-1;
@@ -574,14 +547,6 @@ public class BlockTermsReader extends FieldsProducer {
                 // Done!  Exact match.  Stop here, fill in
                 // real term, return FOUND.
                 //System.out.println("  FOUND");
-
-                if (useCache) {
-                  // Store in cache
-                  decodeMetaData();
-                  //System.out.println("  cache! state=" + state);
-                  termsCache.put(new FieldAndTerm(fieldTerm), (BlockTermState) state.clone());
-                }
-
                 return SeekStatus.FOUND;
               } else {
                 //System.out.println("  NOT_FOUND");
@@ -758,7 +723,6 @@ public class BlockTermsReader extends FieldsProducer {
 
         indexIsCurrent = true;
         didIndexNext = false;
-        blocksSinceSeek = 0;
         seekPending = false;
 
         state.ord = indexEnum.ord()-1;
@@ -831,8 +795,7 @@ public class BlockTermsReader extends FieldsProducer {
 
         postingsReader.readTermsBlock(in, fieldInfo, state);
 
-        blocksSinceSeek++;
-        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());
+        indexIsCurrent = false;
         //System.out.println("  indexIsCurrent=" + indexIsCurrent);
 
         return true;
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
index 2066fcf..0d69e94 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
@@ -27,7 +27,7 @@ import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.PackedInts;
+import org.apache.lucene.util.packed.MonotonicBlockPackedReader;
 
 import java.util.HashMap;
 import java.util.Comparator;
@@ -43,21 +43,15 @@ import org.apache.lucene.index.IndexFileNames;
  */
 public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
 
-  // NOTE: long is overkill here, since this number is 128
-  // by default and only indexDivisor * 128 if you change
-  // the indexDivisor at search time.  But, we use this in a
+  // NOTE: long is overkill here, but we use this in a
   // number of places to multiply out the actual ord, and we
   // will overflow int during those multiplies.  So to avoid
   // having to upgrade each multiple to long in multiple
   // places (error prone), we use long here:
-  private long totalIndexInterval;
-
-  private int indexDivisor;
-  final private int indexInterval;
-
-  // Closed if indexLoaded is true:
-  private IndexInput in;
-  private volatile boolean indexLoaded;
+  private final long indexInterval;
+  
+  private final int packedIntsVersion;
+  private final int blocksize;
 
   private final Comparator<BytesRef> termComp;
 
@@ -72,35 +66,24 @@ public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
   // start of the field info data
   private long dirOffset;
   
-  private final int version;
-
-  public FixedGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, Comparator<BytesRef> termComp, String segmentSuffix, IOContext context)
+  public FixedGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, Comparator<BytesRef> termComp, String segmentSuffix, IOContext context)
     throws IOException {
 
     this.termComp = termComp;
-
-    assert indexDivisor == -1 || indexDivisor > 0;
-
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION), context);
+    
+    final IndexInput in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION), context);
     
     boolean success = false;
 
     try {
       
-      version = readHeader(in);
-      indexInterval = in.readInt();
+      readHeader(in);
+      indexInterval = in.readVInt();
       if (indexInterval < 1) {
         throw new CorruptIndexException("invalid indexInterval: " + indexInterval + " (resource=" + in + ")");
       }
-      this.indexDivisor = indexDivisor;
-
-      if (indexDivisor < 0) {
-        totalIndexInterval = indexInterval;
-      } else {
-        // In case terms index gets loaded, later, on demand
-        totalIndexInterval = indexInterval * indexDivisor;
-      }
-      assert totalIndexInterval > 0;
+      packedIntsVersion = in.readVInt();
+      blocksize = in.readVInt();
       
       seekDir(in, dirOffset);
 
@@ -112,7 +95,7 @@ public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
       //System.out.println("FGR: init seg=" + segment + " div=" + indexDivisor + " nF=" + numFields);
       for(int i=0;i<numFields;i++) {
         final int field = in.readVInt();
-        final int numIndexTerms = in.readVInt();
+        final long numIndexTerms = in.readVInt(); // TODO: change this to a vLong if we fix writer to support > 2B index terms
         if (numIndexTerms < 0) {
           throw new CorruptIndexException("invalid numIndexTerms: " + numIndexTerms + " (resource=" + in + ")");
         }
@@ -124,47 +107,33 @@ public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
           throw new CorruptIndexException("invalid packedIndexStart: " + packedIndexStart + " indexStart: " + indexStart + "numIndexTerms: " + numIndexTerms + " (resource=" + in + ")");
         }
         final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(fieldInfo, numIndexTerms, indexStart, termsStart, packedIndexStart, packedOffsetsStart));
+        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(in, indexStart, termsStart, packedIndexStart, packedOffsetsStart, numIndexTerms));
         if (previous != null) {
           throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
         }
       }
       success = true;
     } finally {
-      if (!success) {
+      if (success) {
+        IOUtils.close(in);
+      } else {
         IOUtils.closeWhileHandlingException(in);
       }
-      if (indexDivisor > 0) {
-        in.close();
-        in = null;
-        if (success) {
-          indexLoaded = true;
-        }
-        termBytesReader = termBytes.freeze(true);
-      }
+      termBytesReader = termBytes.freeze(true);
     }
   }
-  
-  @Override
-  public int getDivisor() {
-    return indexDivisor;
-  }
 
-  private int readHeader(IndexInput input) throws IOException {
-    int version = CodecUtil.checkHeader(input, FixedGapTermsIndexWriter.CODEC_NAME,
-      FixedGapTermsIndexWriter.VERSION_START, FixedGapTermsIndexWriter.VERSION_CURRENT);
-    if (version < FixedGapTermsIndexWriter.VERSION_APPEND_ONLY) {
-      dirOffset = input.readLong();
-    }
-    return version;
+  private void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, FixedGapTermsIndexWriter.CODEC_NAME,
+      FixedGapTermsIndexWriter.VERSION_CURRENT, FixedGapTermsIndexWriter.VERSION_CURRENT);
   }
 
   private class IndexEnum extends FieldIndexEnum {
-    private final FieldIndexData.CoreFieldIndex fieldIndex;
+    private final FieldIndexData fieldIndex;
     private final BytesRef term = new BytesRef();
     private long ord;
 
-    public IndexEnum(FieldIndexData.CoreFieldIndex fieldIndex) {
+    public IndexEnum(FieldIndexData fieldIndex) {
       this.fieldIndex = fieldIndex;
     }
 
@@ -175,12 +144,11 @@ public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
 
     @Override
     public long seek(BytesRef target) {
-      int lo = 0;          // binary search
-      int hi = fieldIndex.numIndexTerms - 1;
-      assert totalIndexInterval > 0 : "totalIndexInterval=" + totalIndexInterval;
+      long lo = 0;          // binary search
+      long hi = fieldIndex.numIndexTerms - 1;
 
       while (hi >= lo) {
-        int mid = (lo + hi) >>> 1;
+        long mid = (lo + hi) >>> 1;
 
         final long offset = fieldIndex.termOffsets.get(mid);
         final int length = (int) (fieldIndex.termOffsets.get(1+mid) - offset);
@@ -193,7 +161,7 @@ public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
           lo = mid + 1;
         } else {
           assert mid >= 0;
-          ord = mid*totalIndexInterval;
+          ord = mid*indexInterval;
           return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(mid);
         }
       }
@@ -207,17 +175,17 @@ public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
       final int length = (int) (fieldIndex.termOffsets.get(1+hi) - offset);
       termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
 
-      ord = hi*totalIndexInterval;
+      ord = hi*indexInterval;
       return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(hi);
     }
 
     @Override
     public long next() {
-      final int idx = 1 + (int) (ord / totalIndexInterval);
+      final long idx = 1 + (ord / indexInterval);
       if (idx >= fieldIndex.numIndexTerms) {
         return -1;
       }
-      ord += totalIndexInterval;
+      ord += indexInterval;
 
       final long offset = fieldIndex.termOffsets.get(idx);
       final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
@@ -232,13 +200,13 @@ public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
 
     @Override
     public long seek(long ord) {
-      int idx = (int) (ord / totalIndexInterval);
+      long idx = ord / indexInterval;
       // caller must ensure ord is in bounds
       assert idx < fieldIndex.numIndexTerms;
       final long offset = fieldIndex.termOffsets.get(idx);
       final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
       termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-      this.ord = idx * totalIndexInterval;
+      this.ord = idx * indexInterval;
       return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
     }
   }
@@ -249,176 +217,58 @@ public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
   }
 
   private final class FieldIndexData {
-
-    volatile CoreFieldIndex coreIndex;
-
-    private final long indexStart;
-    private final long termsStart;
-    private final long packedIndexStart;
-    private final long packedOffsetsStart;
-
-    private final int numIndexTerms;
-
-    public FieldIndexData(FieldInfo fieldInfo, int numIndexTerms, long indexStart, long termsStart, long packedIndexStart,
-                          long packedOffsetsStart) throws IOException {
-
+    // where this field's terms begin in the packed byte[]
+    // data
+    final long termBytesStart;
+    
+    // offset into index termBytes
+    final MonotonicBlockPackedReader termOffsets;
+    
+    // index pointers into main terms dict
+    final MonotonicBlockPackedReader termsDictOffsets;
+    
+    final long numIndexTerms;
+    final long termsStart;
+    
+    public FieldIndexData(IndexInput in, long indexStart, long termsStart, long packedIndexStart, long packedOffsetsStart, long numIndexTerms) throws IOException {
+      
       this.termsStart = termsStart;
-      this.indexStart = indexStart;
-      this.packedIndexStart = packedIndexStart;
-      this.packedOffsetsStart = packedOffsetsStart;
+      termBytesStart = termBytes.getPointer();
+      
+      IndexInput clone = in.clone();
+      clone.seek(indexStart);
+      
       this.numIndexTerms = numIndexTerms;
-
-      if (indexDivisor > 0) {
-        loadTermsIndex();
-      }
-    }
-
-    private void loadTermsIndex() throws IOException {
-      if (coreIndex == null) {
-        coreIndex = new CoreFieldIndex(indexStart, termsStart, packedIndexStart, packedOffsetsStart, numIndexTerms);
-      }
-    }
-
-    private final class CoreFieldIndex {
-
-      // where this field's terms begin in the packed byte[]
-      // data
-      final long termBytesStart;
-
-      // offset into index termBytes
-      final PackedInts.Reader termOffsets;
-
-      // index pointers into main terms dict
-      final PackedInts.Reader termsDictOffsets;
-
-      final int numIndexTerms;
-      final long termsStart;
-
-      public CoreFieldIndex(long indexStart, long termsStart, long packedIndexStart, long packedOffsetsStart, int numIndexTerms) throws IOException {
-
-        this.termsStart = termsStart;
-        termBytesStart = termBytes.getPointer();
-
-        IndexInput clone = in.clone();
-        clone.seek(indexStart);
-
-        // -1 is passed to mean "don't load term index", but
-        // if we are then later loaded it's overwritten with
-        // a real value
-        assert indexDivisor > 0;
-
-        this.numIndexTerms = 1+(numIndexTerms-1) / indexDivisor;
-
-        assert this.numIndexTerms  > 0: "numIndexTerms=" + numIndexTerms + " indexDivisor=" + indexDivisor;
-
-        if (indexDivisor == 1) {
-          // Default (load all index terms) is fast -- slurp in the images from disk:
-          
-          try {
-            final long numTermBytes = packedIndexStart - indexStart;
-            termBytes.copy(clone, numTermBytes);
-
-            // records offsets into main terms dict file
-            termsDictOffsets = PackedInts.getReader(clone);
-            assert termsDictOffsets.size() == numIndexTerms;
-
-            // records offsets into byte[] term data
-            termOffsets = PackedInts.getReader(clone);
-            assert termOffsets.size() == 1+numIndexTerms;
-          } finally {
-            clone.close();
-          }
-        } else {
-          // Get packed iterators
-          final IndexInput clone1 = in.clone();
-          final IndexInput clone2 = in.clone();
-
-          try {
-            // Subsample the index terms
-            clone1.seek(packedIndexStart);
-            final PackedInts.ReaderIterator termsDictOffsetsIter = PackedInts.getReaderIterator(clone1, PackedInts.DEFAULT_BUFFER_SIZE);
-
-            clone2.seek(packedOffsetsStart);
-            final PackedInts.ReaderIterator termOffsetsIter = PackedInts.getReaderIterator(clone2,  PackedInts.DEFAULT_BUFFER_SIZE);
-
-            // TODO: often we can get by w/ fewer bits per
-            // value, below.. .but this'd be more complex:
-            // we'd have to try @ fewer bits and then grow
-            // if we overflowed it.
-
-            PackedInts.Mutable termsDictOffsetsM = PackedInts.getMutable(this.numIndexTerms, termsDictOffsetsIter.getBitsPerValue(), PackedInts.DEFAULT);
-            PackedInts.Mutable termOffsetsM = PackedInts.getMutable(this.numIndexTerms+1, termOffsetsIter.getBitsPerValue(), PackedInts.DEFAULT);
-
-            termsDictOffsets = termsDictOffsetsM;
-            termOffsets = termOffsetsM;
-
-            int upto = 0;
-
-            long termOffsetUpto = 0;
-
-            while(upto < this.numIndexTerms) {
-              // main file offset copies straight over
-              termsDictOffsetsM.set(upto, termsDictOffsetsIter.next());
-
-              termOffsetsM.set(upto, termOffsetUpto);
-
-              long termOffset = termOffsetsIter.next();
-              long nextTermOffset = termOffsetsIter.next();
-              final int numTermBytes = (int) (nextTermOffset - termOffset);
-
-              clone.seek(indexStart + termOffset);
-              assert indexStart + termOffset < clone.length() : "indexStart=" + indexStart + " termOffset=" + termOffset + " len=" + clone.length();
-              assert indexStart + termOffset + numTermBytes < clone.length();
-
-              termBytes.copy(clone, numTermBytes);
-              termOffsetUpto += numTermBytes;
-
-              upto++;
-              if (upto == this.numIndexTerms) {
-                break;
-              }
-
-              // skip terms:
-              termsDictOffsetsIter.next();
-              for(int i=0;i<indexDivisor-2;i++) {
-                termOffsetsIter.next();
-                termsDictOffsetsIter.next();
-              }
-            }
-            termOffsetsM.set(upto, termOffsetUpto);
-
-          } finally {
-            clone1.close();
-            clone2.close();
-            clone.close();
-          }
-        }
+      assert this.numIndexTerms  > 0: "numIndexTerms=" + numIndexTerms;
+      
+      // slurp in the images from disk:
+      
+      try {
+        final long numTermBytes = packedIndexStart - indexStart;
+        termBytes.copy(clone, numTermBytes);
+        
+        // records offsets into main terms dict file
+        termsDictOffsets = new MonotonicBlockPackedReader(clone, packedIntsVersion, blocksize, numIndexTerms, false);
+        
+        // records offsets into byte[] term data
+        termOffsets = new MonotonicBlockPackedReader(clone, packedIntsVersion, blocksize, 1+numIndexTerms, false);
+      } finally {
+        clone.close();
       }
     }
   }
 
   @Override
   public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
-    final FieldIndexData fieldData = fields.get(fieldInfo);
-    if (fieldData.coreIndex == null) {
-      return null;
-    } else {
-      return new IndexEnum(fieldData.coreIndex);
-    }
+    return new IndexEnum(fields.get(fieldInfo));
   }
 
   @Override
-  public void close() throws IOException {
-    if (in != null && !indexLoaded) {
-      in.close();
-    }
-  }
+  public void close() throws IOException {}
 
   private void seekDir(IndexInput input, long dirOffset) throws IOException {
-    if (version >= FixedGapTermsIndexWriter.VERSION_APPEND_ONLY) {
-      input.seek(input.length() - 8);
-      dirOffset = input.readLong();
-    }
+    input.seek(input.length() - 8);
+    dirOffset = input.readLong();
     input.seek(dirOffset);
   }
 }
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java
index b270fcc..789300e 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java
@@ -18,15 +18,16 @@ package org.apache.lucene.codecs.blockterms;
  */
 
 import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.TermStats;
-import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.MonotonicAppendingLongBuffer;
+import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
 import org.apache.lucene.util.packed.PackedInts;
 
 import java.util.List;
@@ -50,23 +51,32 @@ public class FixedGapTermsIndexWriter extends TermsIndexWriterBase {
   final static String CODEC_NAME = "SIMPLE_STANDARD_TERMS_INDEX";
   final static int VERSION_START = 0;
   final static int VERSION_APPEND_ONLY = 1;
-  final static int VERSION_CURRENT = VERSION_APPEND_ONLY;
+  final static int VERSION_MONOTONIC_ADDRESSING = 2;
+  final static int VERSION_CURRENT = VERSION_MONOTONIC_ADDRESSING;
 
+  final static int BLOCKSIZE = 4096;
   final private int termIndexInterval;
+  public static final int DEFAULT_TERM_INDEX_INTERVAL = 32;
 
   private final List<SimpleFieldWriter> fields = new ArrayList<SimpleFieldWriter>();
   
-  @SuppressWarnings("unused") private final FieldInfos fieldInfos; // unread
-
   public FixedGapTermsIndexWriter(SegmentWriteState state) throws IOException {
+    this(state, DEFAULT_TERM_INDEX_INTERVAL);
+  }
+  
+  public FixedGapTermsIndexWriter(SegmentWriteState state, int termIndexInterval) throws IOException {
+    if (termIndexInterval <= 0) {
+      throw new IllegalArgumentException("invalid termIndexInterval: " + termIndexInterval);
+    }
+    this.termIndexInterval = termIndexInterval;
     final String indexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
-    termIndexInterval = state.termIndexInterval;
     out = state.directory.createOutput(indexFileName, state.context);
     boolean success = false;
     try {
-      fieldInfos = state.fieldInfos;
       writeHeader(out);
-      out.writeInt(termIndexInterval);
+      out.writeVInt(termIndexInterval);
+      out.writeVInt(PackedInts.VERSION_CURRENT);
+      out.writeVInt(BLOCKSIZE);
       success = true;
     } finally {
       if (!success) {
@@ -114,22 +124,25 @@ public class FixedGapTermsIndexWriter extends TermsIndexWriterBase {
     long packedOffsetsStart;
     private long numTerms;
 
-    // TODO: we could conceivably make a PackedInts wrapper
-    // that auto-grows... then we wouldn't force 6 bytes RAM
-    // per index term:
-    private short[] termLengths;
-    private int[] termsPointerDeltas;
-    private long lastTermsPointer;
-    private long totTermLength;
+    private RAMOutputStream offsetsBuffer = new RAMOutputStream();
+    private MonotonicBlockPackedWriter termOffsets = new MonotonicBlockPackedWriter(offsetsBuffer, BLOCKSIZE);
+    private long currentOffset;
+
+    private RAMOutputStream addressBuffer = new RAMOutputStream();
+    private MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCKSIZE);
 
     private final BytesRef lastTerm = new BytesRef();
 
     SimpleFieldWriter(FieldInfo fieldInfo, long termsFilePointer) {
       this.fieldInfo = fieldInfo;
       indexStart = out.getFilePointer();
-      termsStart = lastTermsPointer = termsFilePointer;
-      termLengths = new short[0];
-      termsPointerDeltas = new int[0];
+      termsStart = termsFilePointer;
+      // we write terms+1 offsets, term n's length is n+1 - n
+      try {
+        termOffsets.add(0L);
+      } catch (IOException bogus) {
+        throw new RuntimeException(bogus);
+      }
     }
 
     @Override
@@ -157,21 +170,13 @@ public class FixedGapTermsIndexWriter extends TermsIndexWriterBase {
       // against prior term
       out.writeBytes(text.bytes, text.offset, indexedTermLength);
 
-      if (termLengths.length == numIndexTerms) {
-        termLengths = ArrayUtil.grow(termLengths);
-      }
-      if (termsPointerDeltas.length == numIndexTerms) {
-        termsPointerDeltas = ArrayUtil.grow(termsPointerDeltas);
-      }
-
       // save delta terms pointer
-      termsPointerDeltas[numIndexTerms] = (int) (termsFilePointer - lastTermsPointer);
-      lastTermsPointer = termsFilePointer;
+      termAddresses.add(termsFilePointer - termsStart);
 
       // save term length (in bytes)
       assert indexedTermLength <= Short.MAX_VALUE;
-      termLengths[numIndexTerms] = (short) indexedTermLength;
-      totTermLength += indexedTermLength;
+      currentOffset += indexedTermLength;
+      termOffsets.add(currentOffset);
 
       lastTerm.copyBytes(text);
       numIndexTerms++;
@@ -183,32 +188,20 @@ public class FixedGapTermsIndexWriter extends TermsIndexWriterBase {
       // write primary terms dict offsets
       packedIndexStart = out.getFilePointer();
 
-      PackedInts.Writer w = PackedInts.getWriter(out, numIndexTerms, PackedInts.bitsRequired(termsFilePointer), PackedInts.DEFAULT);
-
       // relative to our indexStart
-      long upto = 0;
-      for(int i=0;i<numIndexTerms;i++) {
-        upto += termsPointerDeltas[i];
-        w.add(upto);
-      }
-      w.finish();
+      termAddresses.finish();
+      addressBuffer.writeTo(out);
 
       packedOffsetsStart = out.getFilePointer();
 
       // write offsets into the byte[] terms
-      w = PackedInts.getWriter(out, 1+numIndexTerms, PackedInts.bitsRequired(totTermLength), PackedInts.DEFAULT);
-      upto = 0;
-      for(int i=0;i<numIndexTerms;i++) {
-        w.add(upto);
-        upto += termLengths[i];
-      }
-      w.add(upto);
-      w.finish();
+      termOffsets.finish();
+      offsetsBuffer.writeTo(out);
 
       // our referrer holds onto us, while other fields are
       // being written, so don't tie up this RAM:
-      termLengths = null;
-      termsPointerDeltas = null;
+      termOffsets = termAddresses = null;
+      addressBuffer = offsetsBuffer = null;
     }
   }
 
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/TermsIndexReaderBase.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/TermsIndexReaderBase.java
index 0461bd4..4a8d96f 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/TermsIndexReaderBase.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/TermsIndexReaderBase.java
@@ -47,8 +47,6 @@ public abstract class TermsIndexReaderBase implements Closeable {
 
   public abstract boolean supportsOrd();
 
-  public abstract int getDivisor();
-
   /** 
    * Similar to TermsEnum, except, the only "metadata" it
    * reports for a given indexed term is the long fileOffset
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java
index 6975f26..10d2abb 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java
@@ -32,8 +32,7 @@ import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.fst.BytesRefFSTEnum;
 import org.apache.lucene.util.fst.FST;
 import org.apache.lucene.util.fst.PositiveIntOutputs;
@@ -45,11 +44,6 @@ import org.apache.lucene.util.fst.Util; // for toDot
 public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
 
   private final PositiveIntOutputs fstOutputs = PositiveIntOutputs.getSingleton();
-  private int indexDivisor;
-
-  // Closed if indexLoaded is true:
-  private IndexInput in;
-  private volatile boolean indexLoaded;
 
   final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
   
@@ -59,17 +53,15 @@ public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
   private final int version;
 
   final String segment;
-  public VariableGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, String segmentSuffix, IOContext context)
+  public VariableGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, String segmentSuffix, IOContext context)
     throws IOException {
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION), new IOContext(context, true));
+    final IndexInput in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION), new IOContext(context, true));
     this.segment = segment;
     boolean success = false;
-    assert indexDivisor == -1 || indexDivisor > 0;
 
     try {
       
       version = readHeader(in);
-      this.indexDivisor = indexDivisor;
 
       seekDir(in, dirOffset);
 
@@ -83,27 +75,20 @@ public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
         final int field = in.readVInt();
         final long indexStart = in.readVLong();
         final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(fieldInfo, indexStart));
+        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(in, fieldInfo, indexStart));
         if (previous != null) {
           throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
         }
       }
       success = true;
     } finally {
-      if (indexDivisor > 0) {
-        in.close();
-        in = null;
-        if (success) {
-          indexLoaded = true;
-        }
+      if (success) {
+        IOUtils.close(in);
+      } else {
+        IOUtils.closeWhileHandlingException(in);
       }
     }
   }
-
-  @Override
-  public int getDivisor() {
-    return indexDivisor;
-  }
   
   private int readHeader(IndexInput input) throws IOException {
     int version = CodecUtil.checkHeader(input, VariableGapTermsIndexWriter.CODEC_NAME,
@@ -168,52 +153,21 @@ public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
   }
 
   private final class FieldIndexData {
-
-    private final long indexStart;
-    // Set only if terms index is loaded:
-    private volatile FST<Long> fst;
-
-    public FieldIndexData(FieldInfo fieldInfo, long indexStart) throws IOException {
-      this.indexStart = indexStart;
-
-      if (indexDivisor > 0) {
-        loadTermsIndex();
-      }
-    }
-
-    private void loadTermsIndex() throws IOException {
-      if (fst == null) {
-        IndexInput clone = in.clone();
-        clone.seek(indexStart);
-        fst = new FST<Long>(clone, fstOutputs);
-        clone.close();
-
-        /*
-        final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
-        Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
-        Util.toDot(fst, w, false, false);
-        System.out.println("FST INDEX: SAVED to " + dotFileName);
-        w.close();
-        */
-
-        if (indexDivisor > 1) {
-          // subsample
-          final IntsRef scratchIntsRef = new IntsRef();
-          final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();
-          final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
-          final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst);
-          BytesRefFSTEnum.InputOutput<Long> result;
-          int count = indexDivisor;
-          while((result = fstEnum.next()) != null) {
-            if (count == indexDivisor) {
-              builder.add(Util.toIntsRef(result.input, scratchIntsRef), result.output);
-              count = 0;
-            }
-            count++;
-          }
-          fst = builder.finish();
-        }
-      }
+    private final FST<Long> fst;
+
+    public FieldIndexData(IndexInput in, FieldInfo fieldInfo, long indexStart) throws IOException {
+      IndexInput clone = in.clone();
+      clone.seek(indexStart);
+      fst = new FST<Long>(clone, fstOutputs);
+      clone.close();
+
+      /*
+      final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
+      Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
+      Util.toDot(fst, w, false, false);
+      System.out.println("FST INDEX: SAVED to " + dotFileName);
+      w.close();
+      */
     }
   }
 
@@ -228,11 +182,7 @@ public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
   }
 
   @Override
-  public void close() throws IOException {
-    if (in != null && !indexLoaded) {
-      in.close();
-    }
-  }
+  public void close() throws IOException {}
 
   private void seekDir(IndexInput input, long dirOffset) throws IOException {
     if (version >= VariableGapTermsIndexWriter.VERSION_APPEND_ONLY) {
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
index 2ff143e..982b72a 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
@@ -331,7 +331,7 @@ public final class BloomFilteringPostingsFormat extends PostingsFormat {
       }
       
       @Override
-      public final boolean seekExact(BytesRef text, boolean useCache)
+      public final boolean seekExact(BytesRef text)
           throws IOException {
         // The magical fail-fast speed up that is the entire point of all of
         // this code - save a disk seek if there is a match on an in-memory
@@ -341,13 +341,13 @@ public final class BloomFilteringPostingsFormat extends PostingsFormat {
         if (filter.contains(text) == ContainsResult.NO) {
           return false;
         }
-        return delegate().seekExact(text, useCache);
+        return delegate().seekExact(text);
       }
       
       @Override
-      public final SeekStatus seekCeil(BytesRef text, boolean useCache)
+      public final SeekStatus seekCeil(BytesRef text)
           throws IOException {
-        return delegate().seekCeil(text, useCache);
+        return delegate().seekCeil(text);
       }
       
       @Override
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java
index 05263d3..2d4853a 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesConsumer.java
@@ -27,9 +27,11 @@ import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.MathUtil;
+import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.packed.BlockPackedWriter;
 import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
 import org.apache.lucene.util.packed.PackedInts;
@@ -38,6 +40,7 @@ import org.apache.lucene.util.packed.PackedInts;
 public class DiskDocValuesConsumer extends DocValuesConsumer {
 
   static final int BLOCK_SIZE = 16384;
+  static final int ADDRESS_INTERVAL = 16;
 
   /** Compressed using packed blocks of ints. */
   public static final int DELTA_COMPRESSED = 0;
@@ -45,6 +48,13 @@ public class DiskDocValuesConsumer extends DocValuesConsumer {
   public static final int GCD_COMPRESSED = 1;
   /** Compressed by giving IDs to unique values. */
   public static final int TABLE_COMPRESSED = 2;
+  
+  /** Uncompressed binary, written directly (fixed length). */
+  public static final int BINARY_FIXED_UNCOMPRESSED = 0;
+  /** Uncompressed binary, written directly (variable length). */
+  public static final int BINARY_VARIABLE_UNCOMPRESSED = 1;
+  /** Compressed binary with shared prefixes */
+  public static final int BINARY_PREFIX_COMPRESSED = 2;
 
   final IndexOutput data, meta;
   final int maxDoc;
@@ -173,7 +183,7 @@ public class DiskDocValuesConsumer extends DocValuesConsumer {
   }
 
   @Override
-  public void addBinaryField(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
+  public void addBinaryField(FieldInfo field, Iterable<BytesRef> values) throws IOException {
     // write the byte[] data
     meta.writeVInt(field.number);
     meta.writeByte(DiskDocValuesFormat.BINARY);
@@ -187,6 +197,7 @@ public class DiskDocValuesConsumer extends DocValuesConsumer {
       data.writeBytes(v.bytes, v.offset, v.length);
       count++;
     }
+    meta.writeVInt(minLength == maxLength ? BINARY_FIXED_UNCOMPRESSED : BINARY_VARIABLE_UNCOMPRESSED);
     meta.writeVInt(minLength);
     meta.writeVInt(maxLength);
     meta.writeVLong(count);
@@ -208,12 +219,68 @@ public class DiskDocValuesConsumer extends DocValuesConsumer {
       writer.finish();
     }
   }
+  
+  protected void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
+    // first check if its a "fixed-length" terms dict
+    int minLength = Integer.MAX_VALUE;
+    int maxLength = Integer.MIN_VALUE;
+    for (BytesRef v : values) {
+      minLength = Math.min(minLength, v.length);
+      maxLength = Math.max(maxLength, v.length);
+    }
+    if (minLength == maxLength) {
+      // no index needed: direct addressing by mult
+      addBinaryField(field, values);
+    } else {
+      // header
+      meta.writeVInt(field.number);
+      meta.writeByte(DiskDocValuesFormat.BINARY);
+      meta.writeVInt(BINARY_PREFIX_COMPRESSED);
+      // now write the bytes: sharing prefixes within a block
+      final long startFP = data.getFilePointer();
+      // currently, we have to store the delta from expected for every 1/nth term
+      // we could avoid this, but its not much and less overall RAM than the previous approach!
+      RAMOutputStream addressBuffer = new RAMOutputStream();
+      MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);
+      BytesRef lastTerm = new BytesRef();
+      long count = 0;
+      for (BytesRef v : values) {
+        if (count % ADDRESS_INTERVAL == 0) {
+          termAddresses.add(data.getFilePointer() - startFP);
+          // force the first term in a block to be abs-encoded
+          lastTerm.length = 0;
+        }
+        
+        // prefix-code
+        int sharedPrefix = StringHelper.bytesDifference(lastTerm, v);
+        data.writeVInt(sharedPrefix);
+        data.writeVInt(v.length - sharedPrefix);
+        data.writeBytes(v.bytes, v.offset + sharedPrefix, v.length - sharedPrefix);
+        lastTerm.copyBytes(v);
+        count++;
+      }
+      final long indexStartFP = data.getFilePointer();
+      // write addresses of indexed terms
+      termAddresses.finish();
+      addressBuffer.writeTo(data);
+      addressBuffer = null;
+      termAddresses = null;
+      meta.writeVInt(minLength);
+      meta.writeVInt(maxLength);
+      meta.writeVLong(count);
+      meta.writeLong(startFP);
+      meta.writeVInt(ADDRESS_INTERVAL);
+      meta.writeLong(indexStartFP);
+      meta.writeVInt(PackedInts.VERSION_CURRENT);
+      meta.writeVInt(BLOCK_SIZE);
+    }
+  }
 
   @Override
   public void addSortedField(FieldInfo field, Iterable<BytesRef> values, Iterable<Number> docToOrd) throws IOException {
     meta.writeVInt(field.number);
     meta.writeByte(DiskDocValuesFormat.SORTED);
-    addBinaryField(field, values);
+    addTermsDict(field, values);
     addNumericField(field, docToOrd, false);
   }
   
@@ -222,7 +289,7 @@ public class DiskDocValuesConsumer extends DocValuesConsumer {
     meta.writeVInt(field.number);
     meta.writeByte(DiskDocValuesFormat.SORTED_SET);
     // write the ord -> byte[] as a binary field
-    addBinaryField(field, values);
+    addTermsDict(field, values);
     // write the stream of ords as a numeric field
     // NOTE: we could return an iterator that delta-encodes these within a doc
     addNumericField(field, ords, false);
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java
index b9d021a..43a7d57 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesFormat.java
@@ -28,8 +28,7 @@ import org.apache.lucene.index.SegmentWriteState;
 /**
  * DocValues format that keeps most things on disk.
  * <p>
- * Things like ordinals and disk offsets are loaded into ram,
- * for single-seek access to all the types.
+ * Only things like disk offsets are loaded into ram.
  * <p>
  * @lucene.experimental
  */
@@ -54,7 +53,8 @@ public final class DiskDocValuesFormat extends DocValuesFormat {
   public static final String META_CODEC = "DiskDocValuesMetadata";
   public static final String META_EXTENSION = "dvdm";
   public static final int VERSION_START = 0;
-  public static final int VERSION_CURRENT = VERSION_START;
+  public static final int VERSION_COMPRESSED_TERMS = 1;
+  public static final int VERSION_CURRENT = VERSION_COMPRESSED_TERMS;
   public static final byte NUMERIC = 0;
   public static final byte BINARY = 1;
   public static final byte SORTED = 2;
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java
index d8b6bad..c100b84 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/diskdv/DiskDocValuesProducer.java
@@ -21,7 +21,12 @@ import static org.apache.lucene.codecs.diskdv.DiskDocValuesConsumer.DELTA_COMPRE
 import static org.apache.lucene.codecs.diskdv.DiskDocValuesConsumer.GCD_COMPRESSED;
 import static org.apache.lucene.codecs.diskdv.DiskDocValuesConsumer.TABLE_COMPRESSED;
 
+import static org.apache.lucene.codecs.diskdv.DiskDocValuesConsumer.BINARY_FIXED_UNCOMPRESSED;
+import static org.apache.lucene.codecs.diskdv.DiskDocValuesConsumer.BINARY_VARIABLE_UNCOMPRESSED;
+import static org.apache.lucene.codecs.diskdv.DiskDocValuesConsumer.BINARY_PREFIX_COMPRESSED;
+
 import java.io.IOException;
+import java.util.Comparator;
 import java.util.HashMap;
 import java.util.Map;
 
@@ -29,6 +34,8 @@ import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.IndexFileNames;
@@ -36,7 +43,10 @@ import org.apache.lucene.index.NumericDocValues;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SortedDocValues;
 import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
 import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.packed.BlockPackedReader;
@@ -51,7 +61,6 @@ class DiskDocValuesProducer extends DocValuesProducer {
   private final IndexInput data;
 
   // memory-resident structures
-  private final Map<Integer,BlockPackedReader> ordinalInstances = new HashMap<Integer,BlockPackedReader>();
   private final Map<Integer,MonotonicBlockPackedReader> addressInstances = new HashMap<Integer,MonotonicBlockPackedReader>();
   private final Map<Integer,MonotonicBlockPackedReader> ordIndexInstances = new HashMap<Integer,MonotonicBlockPackedReader>();
   
@@ -63,7 +72,7 @@ class DiskDocValuesProducer extends DocValuesProducer {
     final int version;
     try {
       version = CodecUtil.checkHeader(in, metaCodec, 
-                                      DiskDocValuesFormat.VERSION_START,
+                                      DiskDocValuesFormat.VERSION_CURRENT,
                                       DiskDocValuesFormat.VERSION_CURRENT);
       numerics = new HashMap<Integer,NumericEntry>();
       ords = new HashMap<Integer,NumericEntry>();
@@ -85,7 +94,7 @@ class DiskDocValuesProducer extends DocValuesProducer {
       String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
       data = state.directory.openInput(dataName, state.context);
       final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 DiskDocValuesFormat.VERSION_START,
+                                                 DiskDocValuesFormat.VERSION_CURRENT,
                                                  DiskDocValuesFormat.VERSION_CURRENT);
       if (version != version2) {
         throw new CorruptIndexException("Format versions mismatch");
@@ -197,14 +206,27 @@ class DiskDocValuesProducer extends DocValuesProducer {
   
   static BinaryEntry readBinaryEntry(IndexInput meta) throws IOException {
     BinaryEntry entry = new BinaryEntry();
+    entry.format = meta.readVInt();
     entry.minLength = meta.readVInt();
     entry.maxLength = meta.readVInt();
     entry.count = meta.readVLong();
     entry.offset = meta.readLong();
-    if (entry.minLength != entry.maxLength) {
-      entry.addressesOffset = meta.readLong();
-      entry.packedIntsVersion = meta.readVInt();
-      entry.blockSize = meta.readVInt();
+    switch(entry.format) {
+      case BINARY_FIXED_UNCOMPRESSED:
+        break;
+      case BINARY_PREFIX_COMPRESSED:
+        entry.addressInterval = meta.readVInt();
+        entry.addressesOffset = meta.readLong();
+        entry.packedIntsVersion = meta.readVInt();
+        entry.blockSize = meta.readVInt();
+        break;
+      case BINARY_VARIABLE_UNCOMPRESSED:
+        entry.addressesOffset = meta.readLong();
+        entry.packedIntsVersion = meta.readVInt();
+        entry.blockSize = meta.readVInt();
+        break;
+      default:
+        throw new CorruptIndexException("Unknown format: " + entry.format + ", input=" + meta);
     }
     return entry;
   }
@@ -256,10 +278,15 @@ class DiskDocValuesProducer extends DocValuesProducer {
   @Override
   public BinaryDocValues getBinary(FieldInfo field) throws IOException {
     BinaryEntry bytes = binaries.get(field.number);
-    if (bytes.minLength == bytes.maxLength) {
-      return getFixedBinary(field, bytes);
-    } else {
-      return getVariableBinary(field, bytes);
+    switch(bytes.format) {
+      case BINARY_FIXED_UNCOMPRESSED:
+        return getFixedBinary(field, bytes);
+      case BINARY_VARIABLE_UNCOMPRESSED:
+        return getVariableBinary(field, bytes);
+      case BINARY_PREFIX_COMPRESSED:
+        return getCompressedBinary(field, bytes);
+      default:
+        throw new AssertionError();
     }
   }
   
@@ -322,22 +349,39 @@ class DiskDocValuesProducer extends DocValuesProducer {
     };
   }
 
+  private BinaryDocValues getCompressedBinary(FieldInfo field, final BinaryEntry bytes) throws IOException {
+    final IndexInput data = this.data.clone();
+    final long interval = bytes.addressInterval;
+
+    final MonotonicBlockPackedReader addresses;
+    synchronized (addressInstances) {
+      MonotonicBlockPackedReader addrInstance = addressInstances.get(field.number);
+      if (addrInstance == null) {
+        data.seek(bytes.addressesOffset);
+        final long size;
+        if (bytes.count % interval == 0) {
+          size = bytes.count / interval;
+        } else {
+          size = 1L + bytes.count / interval;
+        }
+        addrInstance = new MonotonicBlockPackedReader(data, bytes.packedIntsVersion, bytes.blockSize, size, false);
+        addressInstances.put(field.number, addrInstance);
+      }
+      addresses = addrInstance;
+    }
+    
+    return new CompressedBinaryDocValues(bytes, addresses, data);
+  }
+
   @Override
   public SortedDocValues getSorted(FieldInfo field) throws IOException {
     final int valueCount = (int) binaries.get(field.number).count;
     final BinaryDocValues binary = getBinary(field);
-    final BlockPackedReader ordinals;
-    synchronized (ordinalInstances) {
-      BlockPackedReader ordsInstance = ordinalInstances.get(field.number);
-      if (ordsInstance == null) {
-        NumericEntry entry = ords.get(field.number);
-        IndexInput data = this.data.clone();
-        data.seek(entry.offset);
-        ordsInstance = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, false);
-        ordinalInstances.put(field.number, ordsInstance);
-      }
-      ordinals = ordsInstance;
-    }
+    NumericEntry entry = ords.get(field.number);
+    IndexInput data = this.data.clone();
+    data.seek(entry.offset);
+    final BlockPackedReader ordinals = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);
+    
     return new SortedDocValues() {
 
       @Override
@@ -354,6 +398,24 @@ class DiskDocValuesProducer extends DocValuesProducer {
       public int getValueCount() {
         return valueCount;
       }
+
+      @Override
+      public int lookupTerm(BytesRef key) {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return (int) ((CompressedBinaryDocValues)binary).lookupTerm(key);
+        } else {
+        return super.lookupTerm(key);
+        }
+      }
+
+      @Override
+      public TermsEnum termsEnum() {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return ((CompressedBinaryDocValues)binary).getTermsEnum();
+        } else {
+          return super.termsEnum();
+        }
+      }
     };
   }
 
@@ -407,6 +469,24 @@ class DiskDocValuesProducer extends DocValuesProducer {
       public long getValueCount() {
         return valueCount;
       }
+      
+      @Override
+      public long lookupTerm(BytesRef key) {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return ((CompressedBinaryDocValues)binary).lookupTerm(key);
+        } else {
+          return super.lookupTerm(key);
+        }
+      }
+
+      @Override
+      public TermsEnum termsEnum() {
+        if (binary instanceof CompressedBinaryDocValues) {
+          return ((CompressedBinaryDocValues)binary).getTermsEnum();
+        } else {
+          return super.termsEnum();
+        }
+      }
     };
   }
 
@@ -431,10 +511,12 @@ class DiskDocValuesProducer extends DocValuesProducer {
   static class BinaryEntry {
     long offset;
 
+    int format;
     long count;
     int minLength;
     int maxLength;
     long addressesOffset;
+    long addressInterval;
     int packedIntsVersion;
     int blockSize;
   }
@@ -457,4 +539,204 @@ class DiskDocValuesProducer extends DocValuesProducer {
     
     abstract void get(long id, BytesRef Result);
   }
+  
+  // in the compressed case, we add a few additional operations for
+  // more efficient reverse lookup and enumeration
+  static class CompressedBinaryDocValues extends LongBinaryDocValues {
+    final BinaryEntry bytes;
+    final long interval;
+    final long numValues;
+    final long numIndexValues;
+    final MonotonicBlockPackedReader addresses;
+    final IndexInput data;
+    final TermsEnum termsEnum;
+    
+    public CompressedBinaryDocValues(BinaryEntry bytes, MonotonicBlockPackedReader addresses, IndexInput data) throws IOException {
+      this.bytes = bytes;
+      this.interval = bytes.addressInterval;
+      this.addresses = addresses;
+      this.data = data;
+      this.numValues = bytes.count;
+      this.numIndexValues = addresses.size();
+      this.termsEnum = getTermsEnum(data);
+    }
+    
+    @Override
+    public void get(long id, BytesRef result) {
+      try {
+        termsEnum.seekExact(id);
+        BytesRef term = termsEnum.term();
+        result.bytes = term.bytes;
+        result.offset = term.offset;
+        result.length = term.length;
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    
+    long lookupTerm(BytesRef key) {
+      try {
+        SeekStatus status = termsEnum.seekCeil(key);
+        if (status == SeekStatus.END) {
+          return -numValues-1;
+        } else if (status == SeekStatus.FOUND) {
+          return termsEnum.ord();
+        } else {
+          return -termsEnum.ord()-1;
+        }
+      } catch (IOException bogus) {
+        throw new RuntimeException(bogus);
+      }
+    }
+    
+    TermsEnum getTermsEnum() {
+      try {
+        return getTermsEnum(data.clone());
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    
+    private TermsEnum getTermsEnum(final IndexInput input) throws IOException {
+      input.seek(bytes.offset);
+      
+      return new TermsEnum() {
+        private long currentOrd = -1;
+        // TODO: maxLength is negative when all terms are merged away...
+        private final BytesRef termBuffer = new BytesRef(bytes.maxLength < 0 ? 0 : bytes.maxLength);
+        private final BytesRef term = new BytesRef(); // TODO: paranoia?
+
+        @Override
+        public BytesRef next() throws IOException {
+          if (doNext() == null) {
+            return null;
+          } else {
+            setTerm();
+            return term;
+          }
+        }
+        
+        private BytesRef doNext() throws IOException {
+          if (++currentOrd >= numValues) {
+            return null;
+          } else {
+            int start = input.readVInt();
+            int suffix = input.readVInt();
+            input.readBytes(termBuffer.bytes, start, suffix);
+            termBuffer.length = start + suffix;
+            return termBuffer;
+          }
+        }
+
+        @Override
+        public SeekStatus seekCeil(BytesRef text) throws IOException {
+          // binary-search just the index values to find the block,
+          // then scan within the block
+          long low = 0;
+          long high = numIndexValues-1;
+
+          while (low <= high) {
+            long mid = (low + high) >>> 1;
+            doSeek(mid * interval);
+            int cmp = termBuffer.compareTo(text);
+
+            if (cmp < 0) {
+              low = mid + 1;
+            } else if (cmp > 0) {
+              high = mid - 1;
+            } else {
+              // we got lucky, found an indexed term
+              setTerm();
+              return SeekStatus.FOUND;
+            }
+          }
+          
+          if (numIndexValues == 0) {
+            return SeekStatus.END;
+          }
+          
+          // block before insertion point
+          long block = low-1;
+          doSeek(block < 0 ? -1 : block * interval);
+          
+          while (doNext() != null) {
+            int cmp = termBuffer.compareTo(text);
+            if (cmp == 0) {
+              setTerm();
+              return SeekStatus.FOUND;
+            } else if (cmp > 0) {
+              setTerm();
+              return SeekStatus.NOT_FOUND;
+            }
+          }
+          
+          return SeekStatus.END;
+        }
+
+        @Override
+        public void seekExact(long ord) throws IOException {
+          doSeek(ord);
+          setTerm();
+        }
+        
+        private void doSeek(long ord) throws IOException {
+          long block = ord / interval;
+
+          if (ord >= currentOrd && block == currentOrd / interval) {
+            // seek within current block
+          } else {
+            // position before start of block
+            currentOrd = ord - ord % interval - 1;
+            input.seek(bytes.offset + addresses.get(block));
+          }
+          
+          while (currentOrd < ord) {
+            doNext();
+          }
+        }
+        
+        private void setTerm() {
+          // TODO: is there a cleaner way
+          term.bytes = new byte[termBuffer.length];
+          term.offset = 0;
+          term.copyBytes(termBuffer);
+        }
+
+        @Override
+        public BytesRef term() throws IOException {
+          return term;
+        }
+
+        @Override
+        public long ord() throws IOException {
+          return currentOrd;
+        }
+        
+        @Override
+        public Comparator<BytesRef> getComparator() {
+          return BytesRef.getUTF8SortedAsUnicodeComparator();
+        }
+
+        @Override
+        public int docFreq() throws IOException {
+          throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public long totalTermFreq() throws IOException {
+          return -1;
+        }
+
+        @Override
+        public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+          throw new UnsupportedOperationException();
+        }
+
+        @Override
+        public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+          throw new UnsupportedOperationException();
+        }
+      };
+    }
+  }
 }
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java
index 0594c88..b19b151 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java
@@ -684,7 +684,7 @@ public final class DirectPostingsFormat extends PostingsFormat {
       }
 
       @Override
-      public SeekStatus seekCeil(BytesRef term, boolean useCache) {
+      public SeekStatus seekCeil(BytesRef term) {
         // TODO: we should use the skip pointers; should be
         // faster than bin search; we should also hold
         // & reuse current state so seeking forwards is
@@ -707,7 +707,7 @@ public final class DirectPostingsFormat extends PostingsFormat {
       }
 
       @Override
-      public boolean seekExact(BytesRef term, boolean useCache) {
+      public boolean seekExact(BytesRef term) {
         // TODO: we should use the skip pointers; should be
         // faster than bin search; we should also hold
         // & reuse current state so seeking forwards is
@@ -1413,7 +1413,7 @@ public final class DirectPostingsFormat extends PostingsFormat {
       }
 
       @Override
-      public SeekStatus seekCeil(BytesRef term, boolean useCache) {
+      public SeekStatus seekCeil(BytesRef term) {
         throw new UnsupportedOperationException();
       }
 
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
index e16b7c8..37c4bd7 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
@@ -335,11 +335,11 @@ public final class MemoryPostingsFormat extends PostingsFormat {
     
     public FSTDocsEnum reset(BytesRef bufferIn, Bits liveDocs, int numDocs) {
       assert numDocs > 0;
-      if (buffer.length < bufferIn.length - bufferIn.offset) {
-        buffer = ArrayUtil.grow(buffer, bufferIn.length - bufferIn.offset);
+      if (buffer.length < bufferIn.length) {
+        buffer = ArrayUtil.grow(buffer, bufferIn.length);
       }
-      in.reset(buffer, 0, bufferIn.length - bufferIn.offset);
-      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length - bufferIn.offset);
+      in.reset(buffer, 0, bufferIn.length);
+      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length);
       this.liveDocs = liveDocs;
       docID = -1;
       accum = 0;
@@ -472,11 +472,11 @@ public final class MemoryPostingsFormat extends PostingsFormat {
       //   System.out.println("  " + Integer.toHexString(bufferIn.bytes[i]&0xFF));
       // }
 
-      if (buffer.length < bufferIn.length - bufferIn.offset) {
-        buffer = ArrayUtil.grow(buffer, bufferIn.length - bufferIn.offset);
+      if (buffer.length < bufferIn.length) {
+        buffer = ArrayUtil.grow(buffer, bufferIn.length);
       }
       in.reset(buffer, 0, bufferIn.length - bufferIn.offset);
-      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length - bufferIn.offset);
+      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length);
       this.liveDocs = liveDocs;
       docID = -1;
       accum = 0;
@@ -632,6 +632,7 @@ public final class MemoryPostingsFormat extends PostingsFormat {
     private int docFreq;
     private long totalTermFreq;
     private BytesRefFSTEnum.InputOutput<BytesRef> current;
+    private BytesRef postingsSpare = new BytesRef();
 
     public FSTTermsEnum(FieldInfo field, FST<BytesRef> fst) {
       this.field = field;
@@ -640,21 +641,23 @@ public final class MemoryPostingsFormat extends PostingsFormat {
 
     private void decodeMetaData() {
       if (!didDecode) {
-        buffer.reset(current.output.bytes, 0, current.output.length);
+        buffer.reset(current.output.bytes, current.output.offset, current.output.length);
         docFreq = buffer.readVInt();
         if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
           totalTermFreq = docFreq + buffer.readVLong();
         } else {
           totalTermFreq = -1;
         }
-        current.output.offset = buffer.getPosition();
+        postingsSpare.bytes = current.output.bytes;
+        postingsSpare.offset = buffer.getPosition();
+        postingsSpare.length = current.output.length - (buffer.getPosition() - current.output.offset);
         //System.out.println("  df=" + docFreq + " totTF=" + totalTermFreq + " offset=" + buffer.getPosition() + " len=" + current.output.length);
         didDecode = true;
       }
     }
 
     @Override
-    public boolean seekExact(BytesRef text, boolean useCache /* ignored */) throws IOException {
+    public boolean seekExact(BytesRef text) throws IOException {
       //System.out.println("te.seekExact text=" + field.name + ":" + text.utf8ToString() + " this=" + this);
       current = fstEnum.seekExact(text);
       didDecode = false;
@@ -662,7 +665,7 @@ public final class MemoryPostingsFormat extends PostingsFormat {
     }
 
     @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
+    public SeekStatus seekCeil(BytesRef text) throws IOException {
       //System.out.println("te.seek text=" + field.name + ":" + text.utf8ToString() + " this=" + this);
       current = fstEnum.seekCeil(text);
       if (current == null) {
@@ -699,7 +702,7 @@ public final class MemoryPostingsFormat extends PostingsFormat {
           docsEnum = new FSTDocsEnum(field.getIndexOptions(), field.hasPayloads());
         }
       }
-      return docsEnum.reset(current.output, liveDocs, docFreq);
+      return docsEnum.reset(this.postingsSpare, liveDocs, docFreq);
     }
 
     @Override
@@ -720,7 +723,7 @@ public final class MemoryPostingsFormat extends PostingsFormat {
         }
       }
       //System.out.println("D&P reset this=" + this);
-      return docsAndPositionsEnum.reset(current.output, liveDocs, docFreq);
+      return docsAndPositionsEnum.reset(postingsSpare, liveDocs, docFreq);
     }
 
     @Override
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java
index 2aeb5a3..735efdc 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java
@@ -103,8 +103,7 @@ public abstract class PulsingPostingsFormat extends PostingsFormat {
                                                     state.directory, state.fieldInfos, state.segmentInfo,
                                                     pulsingReader,
                                                     state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
+                                                    state.segmentSuffix);
       success = true;
       return ret;
     } finally {
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
index d576d3c..74b1d79 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
@@ -109,7 +109,7 @@ class SimpleTextFieldsReader extends FieldsProducer {
     }
 
     @Override
-    public boolean seekExact(BytesRef text, boolean useCache /* ignored */) throws IOException {
+    public boolean seekExact(BytesRef text) throws IOException {
 
       final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.seekExact(text);
       if (result != null) {
@@ -125,7 +125,7 @@ class SimpleTextFieldsReader extends FieldsProducer {
     }
 
     @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
+    public SeekStatus seekCeil(BytesRef text) throws IOException {
 
       //System.out.println("seek to text=" + text.utf8ToString());
       final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.seekCeil(text);
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
index 2fe6419..2c33a0f 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
@@ -331,7 +331,7 @@ public class SimpleTextTermVectorsReader extends TermVectorsReader {
     }
     
     @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException {
+    public SeekStatus seekCeil(BytesRef text) throws IOException {
       iterator = terms.tailMap(text).entrySet().iterator();
       if (!iterator.hasNext()) {
         return SeekStatus.END;
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestFixedGapPostingsFormat.java b/lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestFixedGapPostingsFormat.java
index b0586a0..1aaeeff 100644
--- a/lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestFixedGapPostingsFormat.java
+++ b/lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestFixedGapPostingsFormat.java
@@ -25,10 +25,8 @@ import org.apache.lucene.util._TestUtil;
 /**
  * Basic tests of a PF using FixedGap terms dictionary
  */
-// TODO: we should add an instantiation for VarGap too to TestFramework, and a test in this package
-// TODO: ensure both of these are also in rotation in RandomCodec
 public class TestFixedGapPostingsFormat extends BasePostingsFormatTestCase {
-  private final Codec codec = _TestUtil.alwaysPostingsFormat(new Lucene41WithOrds());
+  private final Codec codec = _TestUtil.alwaysPostingsFormat(new Lucene41WithOrds(_TestUtil.nextInt(random(), 1, 1000)));
 
   @Override
   protected Codec getCodec() {
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapDocFreqIntervalPostingsFormat.java b/lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapDocFreqIntervalPostingsFormat.java
new file mode 100644
index 0000000..16ae249
--- /dev/null
+++ b/lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapDocFreqIntervalPostingsFormat.java
@@ -0,0 +1,35 @@
+package org.apache.lucene.codecs.blockterms;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapFixedInterval;
+import org.apache.lucene.index.BasePostingsFormatTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Basic tests of a PF using VariableGap terms dictionary (fixed interval)
+ */
+public class TestVarGapDocFreqIntervalPostingsFormat extends BasePostingsFormatTestCase {
+  private final Codec codec = _TestUtil.alwaysPostingsFormat(new Lucene41VarGapFixedInterval(_TestUtil.nextInt(random(), 1, 1000)));
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+}
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapFixedIntervalPostingsFormat.java b/lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapFixedIntervalPostingsFormat.java
new file mode 100644
index 0000000..4ae298d
--- /dev/null
+++ b/lucene/codecs/src/test/org/apache/lucene/codecs/blockterms/TestVarGapFixedIntervalPostingsFormat.java
@@ -0,0 +1,35 @@
+package org.apache.lucene.codecs.blockterms;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapDocFreqInterval;
+import org.apache.lucene.index.BasePostingsFormatTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Basic tests of a PF using VariableGap terms dictionary (fixed interval, docFreq threshold)
+ */
+public class TestVarGapFixedIntervalPostingsFormat extends BasePostingsFormatTestCase {
+  private final Codec codec = _TestUtil.alwaysPostingsFormat(new Lucene41VarGapDocFreqInterval(_TestUtil.nextInt(random(), 1, 100), _TestUtil.nextInt(random(), 1, 1000)));
+
+  @Override
+  protected Codec getCodec() {
+    return codec;
+  }
+}
diff --git a/lucene/common-build.xml b/lucene/common-build.xml
index 411232c..da61603 100644
--- a/lucene/common-build.xml
+++ b/lucene/common-build.xml
@@ -230,9 +230,6 @@
   <property name="svn.exe" value="svn" />
   <property name="perl.exe" value="perl" />
   
-  <property name="hg.exe" value="hg" />
-  <property name="moman.url" value="https://bitbucket.org/jpbarrette/moman" />
-  <property name="moman.rev" value="120" />
   <property name="python.exe" value="python" />
   <property name="python32.exe" value="python3.2" />
 
@@ -445,20 +442,6 @@
     </sequential>
   </macrodef>
 
-  <target name="jflex-uptodate-check">
-    <uptodate property="jflex.files.uptodate">
-      <srcfiles dir="${src.dir}" includes="**/*.jflex" />
-      <mapper type="glob" from="*.jflex" to="*.java"/>
-    </uptodate>
-  </target>
- 
-  <target name="jflex-notice" depends="jflex-uptodate-check" unless="jflex.files.uptodate">
-    <echo>
-      One or more of the JFlex .jflex files is newer than its corresponding
-      .java file.  Run the "jflex" target to regenerate the artifacts.
-    </echo>
-  </target>
-
   <target name="jflex-check">
     <available property="jflex.present" classname="jflex.anttask.JFlexTask">
       <classpath refid="jflex.classpath"/>
@@ -471,7 +454,7 @@
       Please install the jFlex 1.5 version (currently not released)
       from its SVN repository:
 
-       svn co http://jflex.svn.sourceforge.net/svnroot/jflex/trunk jflex
+       svn co -r 623 http://jflex.svn.sourceforge.net/svnroot/jflex/trunk jflex
        cd jflex
        mvn install
 
@@ -2100,6 +2083,8 @@ ${tests-output}/junit4-*.suites     - per-JVM executed suites
     <property name="pegdown.loaded" value="true"/>
   </target>
   
+  <target name="regenerate"/>
+	
   <macrodef name="pegdown">
     <attribute name="todir"/>
     <attribute name="flatten" default="false"/>
diff --git a/lucene/core/build.xml b/lucene/core/build.xml
index 2e72431..105a183 100644
--- a/lucene/core/build.xml
+++ b/lucene/core/build.xml
@@ -24,6 +24,9 @@
 
   <import file="../common-build.xml"/>
 
+  <property name="moman.commit-hash" value="5c5c2a1e4dea" />
+  <property name="moman.url" value="https://bitbucket.org/jpbarrette/moman/get/${moman.commit-hash}.zip" />
+
   <path id="classpath"/>
   
   <path id="test.classpath">
@@ -109,36 +112,24 @@
     <fixcrlf srcdir="src/java/org/apache/lucene/util/packed" includes="BulkOperation*.java,Direct*.java,Packed64SingleBlock.java,Packed*ThreeBlocks.py" encoding="UTF-8"/>
   </target>
 
-  <target name="createLevAutomata" depends="check-moman,clone-moman,pull-moman">
+  <target name="createLevAutomata" depends="check-moman,download-moman">
     <createLevAutomaton n="1"/>
     <createLevAutomaton n="2"/>
   </target>
   
   <target name="check-moman">
-    <condition property="moman.cloned">
-      <available file="${build.dir}/moman"/>
-    </condition>
+    <available file="${build.dir}/moman" property="moman.downloaded"/>
   </target>
 
-  <target name="clone-moman" unless="moman.cloned">
-    <mkdir dir="${build.dir}"/>
-    <exec dir="${build.dir}" 
-          executable="${hg.exe}" failonerror="true">
-      <arg value="clone"/>
-      <arg value="-r"/>
-      <arg value="${moman.rev}"/>
-      <arg value="${moman.url}"/>
-      <arg value="moman"/>
-    </exec>
+  <target name="download-moman" unless="moman.downloaded">
+    <mkdir dir="${build.dir}/moman"/>
+    <get src="${moman.url}" dest="${build.dir}/moman.zip"/>
+    <unzip dest="${build.dir}/moman" src="${build.dir}/moman.zip">
+      <cutdirsmapper dirs="1"/>
+    </unzip>
+    <delete file="${build.dir}/moman.zip"/>
   </target>
 
-  <target name="pull-moman" if="moman.cloned">
-    <exec dir="${build.dir}/moman" 
-          executable="${hg.exe}" failonerror="true">
-      <arg value="pull"/>
-      <arg value="-f"/>
-      <arg value="-r"/>
-      <arg value="${moman.rev}"/>
-    </exec>
-  </target>
+  <target name="regenerate" depends="createLevAutomata,createPackedIntSources"/>
+
 </project>
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/AnalyzerWrapper.java b/lucene/core/src/java/org/apache/lucene/analysis/AnalyzerWrapper.java
index 0acac2c..e3140fa 100644
--- a/lucene/core/src/java/org/apache/lucene/analysis/AnalyzerWrapper.java
+++ b/lucene/core/src/java/org/apache/lucene/analysis/AnalyzerWrapper.java
@@ -51,16 +51,35 @@ public abstract class AnalyzerWrapper extends Analyzer {
 
   /**
    * Wraps / alters the given TokenStreamComponents, taken from the wrapped
-   * Analyzer, to form new components.  It is through this method that new
-   * TokenFilters can be added by AnalyzerWrappers.
-   *
-   *
-   * @param fieldName Name of the field which is to be analyzed
-   * @param components TokenStreamComponents taken from the wrapped Analyzer
+   * Analyzer, to form new components. It is through this method that new
+   * TokenFilters can be added by AnalyzerWrappers. By default, the given
+   * components are returned.
+   * 
+   * @param fieldName
+   *          Name of the field which is to be analyzed
+   * @param components
+   *          TokenStreamComponents taken from the wrapped Analyzer
    * @return Wrapped / altered TokenStreamComponents.
    */
-  protected abstract TokenStreamComponents wrapComponents(String fieldName, TokenStreamComponents components);
+  protected TokenStreamComponents wrapComponents(String fieldName, TokenStreamComponents components) {
+    return components;
+  }
 
+  /**
+   * Wraps / alters the given Reader. Through this method AnalyzerWrappers can
+   * implement {@link #initReader(String, Reader)}. By default, the given reader
+   * is returned.
+   * 
+   * @param fieldName
+   *          name of the field which is to be analyzed
+   * @param reader
+   *          the reader to wrap
+   * @return the wrapped reader
+   */
+  protected Reader wrapReader(String fieldName, Reader reader) {
+    return reader;
+  }
+  
   @Override
   protected final TokenStreamComponents createComponents(String fieldName, Reader aReader) {
     return wrapComponents(fieldName, getWrappedAnalyzer(fieldName).createComponents(fieldName, aReader));
@@ -78,6 +97,6 @@ public abstract class AnalyzerWrapper extends Analyzer {
 
   @Override
   public final Reader initReader(String fieldName, Reader reader) {
-    return getWrappedAnalyzer(fieldName).initReader(fieldName, reader);
+    return getWrappedAnalyzer(fieldName).initReader(fieldName, wrapReader(fieldName, reader));
   }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/TokenStreamToAutomaton.java b/lucene/core/src/java/org/apache/lucene/analysis/TokenStreamToAutomaton.java
index fdbcc40..2bb3aec 100644
--- a/lucene/core/src/java/org/apache/lucene/analysis/TokenStreamToAutomaton.java
+++ b/lucene/core/src/java/org/apache/lucene/analysis/TokenStreamToAutomaton.java
@@ -32,7 +32,8 @@ import org.apache.lucene.util.automaton.Transition;
 // TODO: maybe also toFST?  then we can translate atts into FST outputs/weights
 
 /** Consumes a TokenStream and creates an {@link Automaton}
- *  where the transition labels are UTF8 bytes from the {@link
+ *  where the transition labels are UTF8 bytes (or Unicode 
+ *  code points if unicodeArcs is true) from the {@link
  *  TermToBytesRefAttribute}.  Between tokens we insert
  *  POS_SEP and for holes we insert HOLE.
  *
@@ -40,6 +41,7 @@ import org.apache.lucene.util.automaton.Transition;
 public class TokenStreamToAutomaton {
 
   private boolean preservePositionIncrements;
+  private boolean unicodeArcs;
 
   /** Sole constructor. */
   public TokenStreamToAutomaton() {
@@ -51,6 +53,12 @@ public class TokenStreamToAutomaton {
     this.preservePositionIncrements = enablePositionIncrements;
   }
 
+  /** Whether to make transition labels Unicode code points instead of UTF8 bytes, 
+   *  <code>false</code> by default */
+  public void setUnicodeArcs(boolean unicodeArcs) {
+    this.unicodeArcs = unicodeArcs;
+  }
+
   private static class Position implements RollingBuffer.Resettable {
     // Any tokens that ended at our position arrive to this state:
     State arriving;
@@ -80,15 +88,16 @@ public class TokenStreamToAutomaton {
   }
 
   /** We create transition between two adjacent tokens. */
-  public static final int POS_SEP = 256;
+  public static final int POS_SEP = 0x001f;
 
   /** We add this arc to represent a hole. */
-  public static final int HOLE = 257;
+  public static final int HOLE = 0x001e;
 
   /** Pulls the graph (including {@link
    *  PositionLengthAttribute}) from the provided {@link
    *  TokenStream}, and creates the corresponding
-   *  automaton where arcs are bytes from each term. */
+   *  automaton where arcs are bytes (or Unicode code points 
+   *  if unicodeArcs = true) from each term. */
   public Automaton toAutomaton(TokenStream in) throws IOException {
     final Automaton a = new Automaton();
     boolean deterministic = true;
@@ -156,16 +165,34 @@ public class TokenStreamToAutomaton {
       final int endPos = pos + posLengthAtt.getPositionLength();
 
       termBytesAtt.fillBytesRef();
-      final BytesRef term2 = changeToken(term);
+      final BytesRef termUTF8 = changeToken(term);
+      int[] termUnicode = null;
       final Position endPosData = positions.get(endPos);
       if (endPosData.arriving == null) {
         endPosData.arriving = new State();
       }
 
       State state = posData.leaving;
-      for(int byteIDX=0;byteIDX<term2.length;byteIDX++) {
-        final State nextState = byteIDX == term2.length-1 ? endPosData.arriving : new State();
-        state.addTransition(new Transition(term2.bytes[term2.offset + byteIDX] & 0xff, nextState));
+      int termLen;
+      if (unicodeArcs) {
+        final String utf16 = termUTF8.utf8ToString();
+        termUnicode = new int[utf16.codePointCount(0, utf16.length())];
+        termLen = termUnicode.length;
+        for (int cp, i = 0, j = 0; i < utf16.length(); i += Character.charCount(cp))
+          termUnicode[j++] = cp = utf16.codePointAt(i);
+      } else {
+        termLen = termUTF8.length;
+      }
+
+      for(int byteIDX=0;byteIDX<termLen;byteIDX++) {
+        final State nextState = byteIDX == termLen-1 ? endPosData.arriving : new State();
+        int c;
+        if (unicodeArcs) {
+          c = termUnicode[byteIDX];
+        } else {
+          c = termUTF8.bytes[termUTF8.offset + byteIDX] & 0xff;
+        }
+        state.addTransition(new Transition(c, nextState));
         state = nextState;
       }
 
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java
index 32324bb..fbe3400 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java
@@ -67,9 +67,9 @@ import org.apache.lucene.util.fst.Util;
  *  does not support a pluggable terms index
  *  implementation).
  *
- *  <p><b>NOTE</b>: this terms dictionary does not support
- *  index divisor when opening an IndexReader.  Instead, you
- *  can change the min/maxItemsPerBlock during indexing.</p>
+ *  <p><b>NOTE</b>: this terms dictionary supports
+ *  min/maxItemsPerBlock during indexing to control how
+ *  much memory the terms index uses.</p>
  *
  *  <p>The data structure used by this implementation is very
  *  similar to a burst trie
@@ -112,7 +112,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
   /** Sole constructor. */
   public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,
                               PostingsReaderBase postingsReader, IOContext ioContext,
-                              String segmentSuffix, int indexDivisor)
+                              String segmentSuffix)
     throws IOException {
     
     this.postingsReader = postingsReader;
@@ -126,13 +126,11 @@ public class BlockTreeTermsReader extends FieldsProducer {
 
     try {
       version = readHeader(in);
-      if (indexDivisor != -1) {
-        indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),
+      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),
                                 ioContext);
-        int indexVersion = readIndexHeader(indexIn);
-        if (indexVersion != version) {
-          throw new CorruptIndexException("mixmatched version files: " + in + "=" + version + "," + indexIn + "=" + indexVersion);
-        }
+      int indexVersion = readIndexHeader(indexIn);
+      if (indexVersion != version) {
+        throw new CorruptIndexException("mixmatched version files: " + in + "=" + version + "," + indexIn + "=" + indexVersion);
       }
 
       // Have PostingsReader init itself
@@ -140,9 +138,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
 
       // Read per-field details
       seekDir(in, dirOffset);
-      if (indexDivisor != -1) {
-        seekDir(indexIn, indexDirOffset);
-      }
+      seekDir(indexIn, indexDirOffset);
 
       final int numFields = in.readVInt();
       if (numFields < 0) {
@@ -171,15 +167,13 @@ public class BlockTreeTermsReader extends FieldsProducer {
         if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings
           throw new CorruptIndexException("invalid sumTotalTermFreq: " + sumTotalTermFreq + " sumDocFreq: " + sumDocFreq + " (resource=" + in + ")");
         }
-        final long indexStartFP = indexDivisor != -1 ? indexIn.readVLong() : 0;
+        final long indexStartFP = indexIn.readVLong();
         FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, indexIn));
         if (previous != null) {
           throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
         }
       }
-      if (indexDivisor != -1) {
-        indexIn.close();
-      }
+      indexIn.close();
 
       success = true;
     } finally {
@@ -1222,7 +1216,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
       }
 
       @Override
-      public boolean seekExact(BytesRef text, boolean useCache) {
+      public boolean seekExact(BytesRef text) {
         throw new UnsupportedOperationException();
       }
 
@@ -1237,7 +1231,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
       }
 
       @Override
-      public SeekStatus seekCeil(BytesRef text, boolean useCache) {
+      public SeekStatus seekCeil(BytesRef text) {
         throw new UnsupportedOperationException();
       }
     }
@@ -1499,7 +1493,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
       }
 
       @Override
-      public boolean seekExact(final BytesRef target, final boolean useCache) throws IOException {
+      public boolean seekExact(final BytesRef target) throws IOException {
 
         if (index == null) {
           throw new IllegalStateException("terms index was not loaded");
@@ -1713,7 +1707,6 @@ public class BlockTreeTermsReader extends FieldsProducer {
             if (arc.output != NO_OUTPUT) {
               output = fstOutputs.add(output, arc.output);
             }
-
             // if (DEBUG) {
             //   System.out.println("    index: follow label=" + toHex(target.bytes[target.offset + targetUpto]&0xff) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextFinalOutput);
             // }
@@ -1760,7 +1753,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
       }
 
       @Override
-      public SeekStatus seekCeil(final BytesRef target, final boolean useCache) throws IOException {
+      public SeekStatus seekCeil(final BytesRef target) throws IOException {
         if (index == null) {
           throw new IllegalStateException("terms index was not loaded");
         }
@@ -2096,7 +2089,7 @@ public class BlockTreeTermsReader extends FieldsProducer {
           // this method catches up all internal state so next()
           // works properly:
           //if (DEBUG) System.out.println("  re-seek to pending term=" + term.utf8ToString() + " " + term);
-          final boolean result = seekExact(term, false);
+          final boolean result = seekExact(term);
           assert result;
         }
 
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java
index 48c8d5c..073e60c 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsReader.java
@@ -824,7 +824,7 @@ public final class CompressingTermVectorsReader extends TermVectorsReader implem
     }
 
     @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache)
+    public SeekStatus seekCeil(BytesRef text)
         throws IOException {
       if (ord < numTerms && ord >= 0) {
         final int cmp = term().compareTo(text);
@@ -851,16 +851,7 @@ public final class CompressingTermVectorsReader extends TermVectorsReader implem
 
     @Override
     public void seekExact(long ord) throws IOException {
-      if (ord < -1 || ord >= numTerms) {
-        throw new IOException("ord is out of range: ord=" + ord + ", numTerms=" + numTerms);
-      }
-      if (ord < this.ord) {
-        reset();
-      }
-      for (int i = this.ord; i < ord; ++i) {
-        next();
-      }
-      assert ord == this.ord();
+      throw new UnsupportedOperationException();
     }
 
     @Override
@@ -870,7 +861,7 @@ public final class CompressingTermVectorsReader extends TermVectorsReader implem
 
     @Override
     public long ord() throws IOException {
-      return ord;
+      throw new UnsupportedOperationException();
     }
 
     @Override
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
index 061f512..7d342e6 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
@@ -258,8 +258,7 @@ public class Lucene40PostingsFormat extends PostingsFormat {
                                                     state.segmentInfo,
                                                     postings,
                                                     state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
+                                                    state.segmentSuffix);
       success = true;
       return ret;
     } finally {
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
index 5faa143..11f6b3d 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
@@ -433,7 +433,7 @@ public class Lucene40TermVectorsReader extends TermVectorsReader implements Clos
 
     // NOTE: slow!  (linear scan)
     @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache)
+    public SeekStatus seekCeil(BytesRef text)
       throws IOException {
       if (nextTerm != 0) {
         final int cmp = text.compareTo(term);
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsFormat.java
index 4838f7a..b28d9b4 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41PostingsFormat.java
@@ -439,8 +439,7 @@ public final class Lucene41PostingsFormat extends PostingsFormat {
                                                     state.segmentInfo,
                                                     postingsReader,
                                                     state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
+                                                    state.segmentSuffix);
       success = true;
       return ret;
     } finally {
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
index c5ac3f1..16ecf18 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene42/Lucene42DocValuesProducer.java
@@ -490,7 +490,7 @@ class Lucene42DocValuesProducer extends DocValuesProducer {
     }
 
     @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException {
+    public SeekStatus seekCeil(BytesRef text) throws IOException {
       if (in.seekCeil(text) == null) {
         return SeekStatus.END;
       } else if (term().equals(text)) {
@@ -503,7 +503,7 @@ class Lucene42DocValuesProducer extends DocValuesProducer {
     }
 
     @Override
-    public boolean seekExact(BytesRef text, boolean useCache) throws IOException {
+    public boolean seekExact(BytesRef text) throws IOException {
       if (in.seekExact(text) == null) {
         return false;
       } else {
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockPostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockPostingsFormat.java
index 1bd61c7..2ecb3dd 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockPostingsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockPostingsFormat.java
@@ -437,8 +437,7 @@ public final class TempBlockPostingsFormat extends PostingsFormat {
                                                     state.segmentInfo,
                                                     postingsReader,
                                                     state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
+                                                    state.segmentSuffix);
       success = true;
       return ret;
     } finally {
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockTermsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockTermsReader.java
index daa3fe5..45e092a 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockTermsReader.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/temp/TempBlockTermsReader.java
@@ -117,7 +117,7 @@ public class TempBlockTermsReader extends FieldsProducer {
   /** Sole constructor. */
   public TempBlockTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,
                               TempPostingsReaderBase postingsReader, IOContext ioContext,
-                              String segmentSuffix, int indexDivisor)
+                              String segmentSuffix)
     throws IOException {
     
     this.postingsReader = postingsReader;
@@ -131,13 +131,11 @@ public class TempBlockTermsReader extends FieldsProducer {
 
     try {
       version = readHeader(in);
-      if (indexDivisor != -1) {
-        indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, TempBlockTermsWriter.TERMS_INDEX_EXTENSION),
+      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, TempBlockTermsWriter.TERMS_INDEX_EXTENSION),
                                 ioContext);
-        int indexVersion = readIndexHeader(indexIn);
-        if (indexVersion != version) {
-          throw new CorruptIndexException("mixmatched version files: " + in + "=" + version + "," + indexIn + "=" + indexVersion);
-        }
+      int indexVersion = readIndexHeader(indexIn);
+      if (indexVersion != version) {
+        throw new CorruptIndexException("mixmatched version files: " + in + "=" + version + "," + indexIn + "=" + indexVersion);
       }
 
       // Have PostingsReader init itself
@@ -145,9 +143,7 @@ public class TempBlockTermsReader extends FieldsProducer {
 
       // Read per-field details
       seekDir(in, dirOffset);
-      if (indexDivisor != -1) {
-        seekDir(indexIn, indexDirOffset);
-      }
+      seekDir(indexIn, indexDirOffset);
 
       final int numFields = in.readVInt();
       if (numFields < 0) {
@@ -177,15 +173,13 @@ public class TempBlockTermsReader extends FieldsProducer {
         if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings
           throw new CorruptIndexException("invalid sumTotalTermFreq: " + sumTotalTermFreq + " sumDocFreq: " + sumDocFreq + " (resource=" + in + ")");
         }
-        final long indexStartFP = indexDivisor != -1 ? indexIn.readVLong() : 0;
+        final long indexStartFP = indexIn.readVLong();
         FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, longsSize, indexIn));
         if (previous != null) {
           throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
         }
       }
-      if (indexDivisor != -1) {
-        indexIn.close();
-      }
+      indexIn.close();
 
       success = true;
     } finally {
@@ -1251,7 +1245,7 @@ public class TempBlockTermsReader extends FieldsProducer {
       }
 
       @Override
-      public boolean seekExact(BytesRef text, boolean useCache) {
+      public boolean seekExact(BytesRef text) {
         throw new UnsupportedOperationException();
       }
 
@@ -1266,7 +1260,7 @@ public class TempBlockTermsReader extends FieldsProducer {
       }
 
       @Override
-      public SeekStatus seekCeil(BytesRef text, boolean useCache) {
+      public SeekStatus seekCeil(BytesRef text) {
         throw new UnsupportedOperationException();
       }
     }
@@ -1528,7 +1522,7 @@ public class TempBlockTermsReader extends FieldsProducer {
       }
 
       @Override
-      public boolean seekExact(final BytesRef target, final boolean useCache) throws IOException {
+      public boolean seekExact(final BytesRef target) throws IOException {
 
         if (index == null) {
           throw new IllegalStateException("terms index was not loaded");
@@ -1789,7 +1783,7 @@ public class TempBlockTermsReader extends FieldsProducer {
       }
 
       @Override
-      public SeekStatus seekCeil(final BytesRef target, final boolean useCache) throws IOException {
+      public SeekStatus seekCeil(final BytesRef target) throws IOException {
         if (index == null) {
           throw new IllegalStateException("terms index was not loaded");
         }
@@ -2125,7 +2119,7 @@ public class TempBlockTermsReader extends FieldsProducer {
           // this method catches up all internal state so next()
           // works properly:
           //if (DEBUG) System.out.println("  re-seek to pending term=" + term.utf8ToString() + " " + term);
-          final boolean result = seekExact(term, false);
+          final boolean result = seekExact(term);
           assert result;
         }
 
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/temp/TempFSTOrdTermsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/temp/TempFSTOrdTermsReader.java
index e25ecab..1e97ea9 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/temp/TempFSTOrdTermsReader.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/temp/TempFSTOrdTermsReader.java
@@ -461,7 +461,7 @@ public class TempFSTOrdTermsReader extends FieldsProducer {
       public BytesRef next() throws IOException {
         if (seekPending) {  // previously positioned, but termOutputs not fetched
           seekPending = false;
-          SeekStatus status = seekCeil(term, false);
+          SeekStatus status = seekCeil(term);
           assert status == SeekStatus.FOUND;  // must positioned on valid term
         }
         updateEnum(fstEnum.next());
@@ -469,13 +469,13 @@ public class TempFSTOrdTermsReader extends FieldsProducer {
       }
 
       @Override
-      public boolean seekExact(BytesRef target, boolean useCache) throws IOException {
+      public boolean seekExact(BytesRef target) throws IOException {
         updateEnum(fstEnum.seekExact(target));
         return term != null;
       }
 
       @Override
-      public SeekStatus seekCeil(BytesRef target, boolean useCache) throws IOException {
+      public SeekStatus seekCeil(BytesRef target) throws IOException {
         updateEnum(fstEnum.seekCeil(target));
         if (term == null) {
           return SeekStatus.END;
@@ -587,17 +587,9 @@ public class TempFSTOrdTermsReader extends FieldsProducer {
         super.decodeStats();
       }
 
-      // nocommit: need testcase for this
       @Override
-      public SeekStatus seekCeil(BytesRef target, boolean useCache) throws IOException {
-        decoded = false;
-        term = doSeekCeil(target);
-        decodeStats();
-        if (term == null) {
-          return SeekStatus.END;
-        } else {
-          return term.equals(target) ? SeekStatus.FOUND : SeekStatus.NOT_FOUND;
-        }
+      public SeekStatus seekCeil(BytesRef target) throws IOException {
+        throw new UnsupportedOperationException();
       }
 
       @Override
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/temp/TempFSTTermsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/temp/TempFSTTermsReader.java
index 890d26a..0e57722 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/temp/TempFSTTermsReader.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/temp/TempFSTTermsReader.java
@@ -348,7 +348,7 @@ public class TempFSTTermsReader extends FieldsProducer {
       public BytesRef next() throws IOException {
         if (seekPending) {  // previously positioned, but termOutputs not fetched
           seekPending = false;
-          SeekStatus status = seekCeil(term, false);
+          SeekStatus status = seekCeil(term);
           assert status == SeekStatus.FOUND;  // must positioned on valid term
         }
         updateEnum(fstEnum.next());
@@ -356,13 +356,13 @@ public class TempFSTTermsReader extends FieldsProducer {
       }
 
       @Override
-      public boolean seekExact(BytesRef target, boolean useCache) throws IOException {
+      public boolean seekExact(BytesRef target) throws IOException {
         updateEnum(fstEnum.seekExact(target));
         return term != null;
       }
 
       @Override
-      public SeekStatus seekCeil(BytesRef target, boolean useCache) throws IOException {
+      public SeekStatus seekCeil(BytesRef target) throws IOException {
         updateEnum(fstEnum.seekCeil(target));
         if (term == null) {
           return SeekStatus.END;
@@ -497,7 +497,7 @@ public class TempFSTTermsReader extends FieldsProducer {
       }
 
       @Override
-      public SeekStatus seekCeil(BytesRef target, boolean useCache) throws IOException {
+      public SeekStatus seekCeil(BytesRef target) throws IOException {
         decoded = false;
         term = doSeekCeil(target);
         loadMetaData();
diff --git a/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java b/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
index f671a5b..1b0e416 100644
--- a/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/AtomicReader.java
@@ -78,7 +78,7 @@ public abstract class AtomicReader extends IndexReader {
       return 0;
     }
     final TermsEnum termsEnum = terms.iterator(null);
-    if (termsEnum.seekExact(term.bytes(), true)) {
+    if (termsEnum.seekExact(term.bytes())) {
       return termsEnum.docFreq();
     } else {
       return 0;
@@ -101,7 +101,7 @@ public abstract class AtomicReader extends IndexReader {
       return 0;
     }
     final TermsEnum termsEnum = terms.iterator(null);
-    if (termsEnum.seekExact(term.bytes(), true)) {
+    if (termsEnum.seekExact(term.bytes())) {
       return termsEnum.totalTermFreq();
     } else {
       return 0;
@@ -156,7 +156,7 @@ public abstract class AtomicReader extends IndexReader {
       final Terms terms = fields.terms(term.field());
       if (terms != null) {
         final TermsEnum termsEnum = terms.iterator(null);
-        if (termsEnum.seekExact(term.bytes(), true)) {
+        if (termsEnum.seekExact(term.bytes())) {
           return termsEnum.docs(getLiveDocs(), null);
         }
       }
@@ -176,7 +176,7 @@ public abstract class AtomicReader extends IndexReader {
       final Terms terms = fields.terms(term.field());
       if (terms != null) {
         final TermsEnum termsEnum = terms.iterator(null);
-        if (termsEnum.seekExact(term.bytes(), true)) {
+        if (termsEnum.seekExact(term.bytes())) {
           return termsEnum.docsAndPositions(getLiveDocs(), null);
         }
       }
diff --git a/lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java b/lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java
index e9cc2fa..643408f 100644
--- a/lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/BinaryDocValuesWriter.java
@@ -26,7 +26,8 @@ import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
 import org.apache.lucene.util.ByteBlockPool;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.packed.AppendingLongBuffer;
+import org.apache.lucene.util.packed.AppendingDeltaPackedLongBuffer;
+import org.apache.lucene.util.packed.PackedInts;
 
 import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
 
@@ -36,14 +37,14 @@ import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
 class BinaryDocValuesWriter extends DocValuesWriter {
 
   private final ByteBlockPool pool;
-  private final AppendingLongBuffer lengths;
+  private final AppendingDeltaPackedLongBuffer lengths;
   private final FieldInfo fieldInfo;
   private int addedValues = 0;
 
   public BinaryDocValuesWriter(FieldInfo fieldInfo, Counter iwBytesUsed) {
     this.fieldInfo = fieldInfo;
     this.pool = new ByteBlockPool(new DirectTrackingAllocator(iwBytesUsed));
-    this.lengths = new AppendingLongBuffer();
+    this.lengths = new AppendingDeltaPackedLongBuffer(PackedInts.COMPACT);
   }
 
   public void addValue(int docID, BytesRef value) {
@@ -90,7 +91,7 @@ class BinaryDocValuesWriter extends DocValuesWriter {
   // iterates over the values we have in ram
   private class BytesIterator implements Iterator<BytesRef> {
     final BytesRef value = new BytesRef();
-    final AppendingLongBuffer.Iterator lengthsIterator = lengths.iterator();
+    final AppendingDeltaPackedLongBuffer.Iterator lengthsIterator = lengths.iterator();
     final int size = (int) lengths.size();
     final int maxDoc;
     int upto;
diff --git a/lucene/core/src/java/org/apache/lucene/index/BufferedDeletes.java b/lucene/core/src/java/org/apache/lucene/index/BufferedDeletes.java
index afd1eef..a538dd1 100644
--- a/lucene/core/src/java/org/apache/lucene/index/BufferedDeletes.java
+++ b/lucene/core/src/java/org/apache/lucene/index/BufferedDeletes.java
@@ -33,9 +33,9 @@ import org.apache.lucene.util.RamUsageEstimator;
  * deletes are pushed (on flush in DocumentsWriter), these
  * deletes are converted to a FrozenDeletes instance. */
 
-// NOTE: we are sync'd by BufferedDeletes, ie, all access to
-// instances of this class is via sync'd methods on
-// BufferedDeletes
+// NOTE: instances of this class are accessed either via a private
+// instance on DocumentWriterPerThread, or via sync'd code by
+// DocumentsWriterDeleteQueue
 
 class BufferedDeletes {
 
@@ -136,6 +136,9 @@ class BufferedDeletes {
     }
 
     terms.put(term, Integer.valueOf(docIDUpto));
+    // note that if current != null then it means there's already a buffered
+    // delete on that term, therefore we seem to over-count. this over-counting
+    // is done to respect IndexWriterConfig.setMaxBufferedDeleteTerms.
     numTermDeletes.incrementAndGet();
     if (current == null) {
       bytesUsed.addAndGet(BYTES_PER_DEL_TERM + term.bytes.length + (RamUsageEstimator.NUM_BYTES_CHAR * term.field().length()));
diff --git a/lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream.java b/lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream.java
index 5ef2b9c..247626e 100644
--- a/lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream.java
+++ b/lucene/core/src/java/org/apache/lucene/index/BufferedDeletesStream.java
@@ -390,7 +390,7 @@ class BufferedDeletesStream {
 
       // System.out.println("  term=" + term);
 
-      if (termsEnum.seekExact(term.bytes(), false)) {
+      if (termsEnum.seekExact(term.bytes())) {
         // we don't need term frequencies for this
         DocsEnum docsEnum = termsEnum.docs(rld.getLiveDocs(), docs, DocsEnum.FLAG_NONE);
         //System.out.println("BDS: got docsEnum=" + docsEnum);
diff --git a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
index 142a67c..612fbc5 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
@@ -30,8 +30,8 @@ import java.util.Map;
 
 import org.apache.lucene.codecs.BlockTreeTermsReader;
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.PostingsFormat; // javadocs
-import org.apache.lucene.document.FieldType; // for javadocs
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.index.CheckIndex.Status.DocValuesStatus;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
@@ -159,19 +159,6 @@ public class CheckIndex {
        *  segment. */
       public double sizeMB;
 
-      /** Doc store offset, if this segment shares the doc
-       *  store files (stored fields and term vectors) with
-       *  other segments.  This is -1 if it does not share. */
-      public int docStoreOffset = -1;
-    
-      /** String of the shared doc store segment, or null if
-       *  this segment does not share the doc store files. */
-      public String docStoreSegment;
-
-      /** True if the shared doc store files are compound file
-       *  format. */
-      public boolean docStoreCompoundFile;
-
       /** True if this segment has pending deletions. */
       public boolean hasDeletions;
 
@@ -297,10 +284,21 @@ public class CheckIndex {
       DocValuesStatus() {
       }
 
-      /** Number of documents tested. */
-      public int docCount;
       /** Total number of docValues tested. */
       public long totalValueFields;
+      
+      /** Total number of numeric fields */
+      public long totalNumericFields;
+      
+      /** Total number of binary fields */
+      public long totalBinaryFields;
+      
+      /** Total number of sorted fields */
+      public long totalSortedFields;
+      
+      /** Total number of sortedset fields */
+      public long totalSortedSetFields;
+      
       /** Exception thrown during doc values test (null on success) */
       public Throwable error = null;
     }
@@ -535,7 +533,7 @@ public class CheckIndex {
         }
         if (infoStream != null)
           infoStream.print("    test: open reader.........");
-        reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, IOContext.DEFAULT);
+        reader = new SegmentReader(info, IOContext.DEFAULT);
 
         segInfoStat.openReaderPassed = true;
 
@@ -1117,7 +1115,7 @@ public class CheckIndex {
             long totDocCountNoDeletes = 0;
             long totDocFreq = 0;
             for(int i=0;i<seekCount;i++) {
-              if (!termsEnum.seekExact(seekTerms[i], true)) {
+              if (!termsEnum.seekExact(seekTerms[i])) {
                 throw new RuntimeException("seek to existing term " + seekTerms[i] + " failed");
               }
               
@@ -1272,7 +1270,7 @@ public class CheckIndex {
       for (FieldInfo fieldInfo : reader.getFieldInfos()) {
         if (fieldInfo.hasDocValues()) {
           status.totalValueFields++;
-          checkDocValues(fieldInfo, reader, infoStream);
+          checkDocValues(fieldInfo, reader, infoStream, status);
         } else {
           if (reader.getBinaryDocValues(fieldInfo.name) != null ||
               reader.getNumericDocValues(fieldInfo.name) != null ||
@@ -1283,7 +1281,11 @@ public class CheckIndex {
         }
       }
 
-      msg(infoStream, "OK [" + status.docCount + " total doc count; " + status.totalValueFields + " docvalues fields]");
+      msg(infoStream, "OK [" + status.totalValueFields + " docvalues fields; "
+                             + status.totalBinaryFields + " BINARY; " 
+                             + status.totalNumericFields + " NUMERIC; "
+                             + status.totalSortedFields + " SORTED; "
+                             + status.totalSortedSetFields + " SORTED_SET]");
     } catch (Throwable e) {
       msg(infoStream, "ERROR [" + String.valueOf(e.getMessage()) + "]");
       status.error = e;
@@ -1382,9 +1384,10 @@ public class CheckIndex {
     }
   }
   
-  private static void checkDocValues(FieldInfo fi, AtomicReader reader, PrintStream infoStream) throws Exception {
+  private static void checkDocValues(FieldInfo fi, AtomicReader reader, PrintStream infoStream, DocValuesStatus status) throws Exception {
     switch(fi.getDocValuesType()) {
       case SORTED:
+        status.totalSortedFields++;
         checkSortedDocValues(fi.name, reader, reader.getSortedDocValues(fi.name));
         if (reader.getBinaryDocValues(fi.name) != null ||
             reader.getNumericDocValues(fi.name) != null ||
@@ -1393,6 +1396,7 @@ public class CheckIndex {
         }
         break;
       case SORTED_SET:
+        status.totalSortedSetFields++;
         checkSortedSetDocValues(fi.name, reader, reader.getSortedSetDocValues(fi.name));
         if (reader.getBinaryDocValues(fi.name) != null ||
             reader.getNumericDocValues(fi.name) != null ||
@@ -1401,6 +1405,7 @@ public class CheckIndex {
         }
         break;
       case BINARY:
+        status.totalBinaryFields++;
         checkBinaryDocValues(fi.name, reader, reader.getBinaryDocValues(fi.name));
         if (reader.getNumericDocValues(fi.name) != null ||
             reader.getSortedDocValues(fi.name) != null ||
@@ -1409,6 +1414,7 @@ public class CheckIndex {
         }
         break;
       case NUMERIC:
+        status.totalNumericFields++;
         checkNumericDocValues(fi.name, reader, reader.getNumericDocValues(fi.name));
         if (reader.getBinaryDocValues(fi.name) != null ||
             reader.getSortedDocValues(fi.name) != null ||
@@ -1543,7 +1549,7 @@ public class CheckIndex {
                 }
 
                 final DocsEnum postingsDocs2;
-                if (!postingsTermsEnum.seekExact(term, true)) {
+                if (!postingsTermsEnum.seekExact(term)) {
                   throw new RuntimeException("vector term=" + term + " field=" + field + " does not exist in postings; doc=" + j);
                 }
                 postingsPostings = postingsTermsEnum.docsAndPositions(null, postingsPostings);
@@ -1677,7 +1683,7 @@ public class CheckIndex {
    *
    * <p><b>WARNING</b>: Make sure you only call this when the
    *  index is not opened  by any writer. */
-  public void fixIndex(Status result, Codec codec) throws IOException {
+  public void fixIndex(Status result) throws IOException {
     if (result.partial)
       throw new IllegalArgumentException("can only fix an index that was fully checked (this status checked a subset of segments)");
     result.newSegments.changed();
@@ -1732,7 +1738,6 @@ public class CheckIndex {
 
     boolean doFix = false;
     boolean doCrossCheckTermVectors = false;
-    Codec codec = Codec.getDefault(); // only used when fixing
     boolean verbose = false;
     List<String> onlySegments = new ArrayList<String>();
     String indexPath = null;
@@ -1744,13 +1749,6 @@ public class CheckIndex {
         doFix = true;
       } else if ("-crossCheckTermVectors".equals(arg)) {
         doCrossCheckTermVectors = true;
-      } else if ("-codec".equals(arg)) {
-        if (i == args.length-1) {
-          System.out.println("ERROR: missing name for -codec option");
-          System.exit(1);
-        }
-        i++;
-        codec = Codec.forName(args[i]);
       } else if (arg.equals("-verbose")) {
         verbose = true;
       } else if (arg.equals("-segment")) {
@@ -1851,7 +1849,7 @@ public class CheckIndex {
           System.out.println("  " + (5-s) + "...");
         }
         System.out.println("Writing...");
-        checker.fixIndex(result, codec);
+        checker.fixIndex(result);
         System.out.println("OK");
         System.out.println("Wrote new segments file \"" + result.newSegments.getSegmentsFileName() + "\"");
       }
diff --git a/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java b/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
index e2372a1..e2a6121 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
@@ -52,9 +52,6 @@ import org.apache.lucene.store.NoSuchDirectoryException;
 */
 public abstract class DirectoryReader extends BaseCompositeReader<AtomicReader> {
 
-  /** Default termInfosIndexDivisor. */
-  public static final int DEFAULT_TERMS_INDEX_DIVISOR = 1;
-
   /** The index directory. */
   protected final Directory directory;
   
@@ -64,29 +61,7 @@ public abstract class DirectoryReader extends BaseCompositeReader<AtomicReader>
    * @throws IOException if there is a low-level IO error
    */
   public static DirectoryReader open(final Directory directory) throws IOException {
-    return StandardDirectoryReader.open(directory, null, DEFAULT_TERMS_INDEX_DIVISOR);
-  }
-  
-  /** Expert: Returns a IndexReader reading the index in the given
-   *  Directory with the given termInfosIndexDivisor.
-   * @param directory the index directory
-   * @param termInfosIndexDivisor Subsamples which indexed
-   *  terms are loaded into RAM. This has the same effect as {@link
-   *  IndexWriterConfig#setTermIndexInterval} except that setting
-   *  must be done at indexing time while this setting can be
-   *  set per reader.  When set to N, then one in every
-   *  N*termIndexInterval terms in the index is loaded into
-   *  memory.  By setting this to a value > 1 you can reduce
-   *  memory usage, at the expense of higher latency when
-   *  loading a TermInfo.  The default value is 1.  Set this
-   *  to -1 to skip loading the terms index entirely.
-   *  <b>NOTE:</b> divisor settings &gt; 1 do not apply to all PostingsFormat
-   *  implementations, including the default one in this release. It only makes
-   *  sense for terms indexes that can efficiently re-sample terms at load time.
-   * @throws IOException if there is a low-level IO error
-   */
-  public static DirectoryReader open(final Directory directory, int termInfosIndexDivisor) throws IOException {
-    return StandardDirectoryReader.open(directory, null, termInfosIndexDivisor);
+    return StandardDirectoryReader.open(directory, null);
   }
   
   /**
@@ -118,29 +93,7 @@ public abstract class DirectoryReader extends BaseCompositeReader<AtomicReader>
    * @throws IOException if there is a low-level IO error
    */
   public static DirectoryReader open(final IndexCommit commit) throws IOException {
-    return StandardDirectoryReader.open(commit.getDirectory(), commit, DEFAULT_TERMS_INDEX_DIVISOR);
-  }
-
-  /** Expert: returns an IndexReader reading the index in the given
-   *  {@link IndexCommit} and termInfosIndexDivisor.
-   * @param commit the commit point to open
-   * @param termInfosIndexDivisor Subsamples which indexed
-   *  terms are loaded into RAM. This has the same effect as {@link
-   *  IndexWriterConfig#setTermIndexInterval} except that setting
-   *  must be done at indexing time while this setting can be
-   *  set per reader.  When set to N, then one in every
-   *  N*termIndexInterval terms in the index is loaded into
-   *  memory.  By setting this to a value > 1 you can reduce
-   *  memory usage, at the expense of higher latency when
-   *  loading a TermInfo.  The default value is 1.  Set this
-   *  to -1 to skip loading the terms index entirely.
-   *  <b>NOTE:</b> divisor settings &gt; 1 do not apply to all PostingsFormat
-   *  implementations, including the default one in this release. It only makes
-   *  sense for terms indexes that can efficiently re-sample terms at load time.
-   * @throws IOException if there is a low-level IO error
-   */
-  public static DirectoryReader open(final IndexCommit commit, int termInfosIndexDivisor) throws IOException {
-    return StandardDirectoryReader.open(commit.getDirectory(), commit, termInfosIndexDivisor);
+    return StandardDirectoryReader.open(commit.getDirectory(), commit);
   }
 
   /**
diff --git a/lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java b/lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java
index 4bd919f..56859ce 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DocTermOrds.java
@@ -659,7 +659,7 @@ public class DocTermOrds {
     }
 
     @Override
-    public SeekStatus seekCeil(BytesRef target, boolean useCache) throws IOException {
+    public SeekStatus seekCeil(BytesRef target) throws IOException {
 
       // already here
       if (term != null && term.equals(target)) {
@@ -729,7 +729,7 @@ public class DocTermOrds {
         //System.out.println("  do seek term=" + base.utf8ToString());
         ord = idx << indexIntervalBits;
         delta = (int) (targetOrd - ord);
-        final TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(base, true);
+        final TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(base);
         assert seekStatus == TermsEnum.SeekStatus.FOUND;
       } else {
         //System.out.println("seek w/in block");
diff --git a/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java b/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
index cdc5088..f9d3d1a 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
@@ -403,8 +403,8 @@ class DocumentsWriterPerThread {
     ++numDocsInRAM;
   }
 
-  // Buffer a specific docID for deletion.  Currently only
-  // used when we hit a exception when adding a document
+  // Buffer a specific docID for deletion. Currently only
+  // used when we hit an exception when adding a document
   void deleteDocID(int docIDUpto) {
     pendingDeletes.addDocID(docIDUpto);
     // NOTE: we do not trigger flush here.  This is
@@ -468,7 +468,6 @@ class DocumentsWriterPerThread {
     assert deleteSlice == null : "all deletes must be applied in prepareFlush";
     segmentInfo.setDocCount(numDocsInRAM);
     flushState = new SegmentWriteState(infoStream, directory, segmentInfo, fieldInfos.finish(),
-        writer.getConfig().getTermIndexInterval(),
         pendingDeletes, new IOContext(new FlushInfo(numDocsInRAM, bytesUsed())));
     final double startMBUsed = parent.flushControl.netBytes() / 1024. / 1024.;
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThreadPool.java b/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThreadPool.java
index 649a81c..2a16d32 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThreadPool.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterPerThreadPool.java
@@ -147,7 +147,10 @@ abstract class DocumentsWriterPerThreadPool implements Cloneable {
   @Override
   public DocumentsWriterPerThreadPool clone() {
     // We should only be cloned before being used:
-    assert numThreadStatesActive == 0;
+    if (numThreadStatesActive != 0) {
+      throw new IllegalStateException("clone this object before it is used!");
+    }
+    
     DocumentsWriterPerThreadPool clone;
     try {
       clone = (DocumentsWriterPerThreadPool) super.clone();
diff --git a/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java b/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java
index b86432c..93be7a6 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FilterAtomicReader.java
@@ -157,8 +157,8 @@ public class FilterAtomicReader extends AtomicReader {
     }
 
     @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException {
-      return in.seekCeil(text, useCache);
+    public SeekStatus seekCeil(BytesRef text) throws IOException {
+      return in.seekCeil(text);
     }
 
     @Override
diff --git a/lucene/core/src/java/org/apache/lucene/index/FilteredTermsEnum.java b/lucene/core/src/java/org/apache/lucene/index/FilteredTermsEnum.java
index 563ac85..4a704d3 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FilteredTermsEnum.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FilteredTermsEnum.java
@@ -154,7 +154,7 @@ public abstract class FilteredTermsEnum extends TermsEnum {
    *         support seeking.
    */
   @Override
-  public boolean seekExact(BytesRef term, boolean useCache) throws IOException {
+  public boolean seekExact(BytesRef term) throws IOException {
     throw new UnsupportedOperationException(getClass().getName()+" does not support seeking");
   }
 
@@ -163,7 +163,7 @@ public abstract class FilteredTermsEnum extends TermsEnum {
    *         support seeking.
    */
   @Override
-  public SeekStatus seekCeil(BytesRef term, boolean useCache) throws IOException {
+  public SeekStatus seekCeil(BytesRef term) throws IOException {
     throw new UnsupportedOperationException(getClass().getName()+" does not support seeking");
   }
 
@@ -222,7 +222,7 @@ public abstract class FilteredTermsEnum extends TermsEnum {
         //System.out.println("  seek to t=" + (t == null ? "null" : t.utf8ToString()) + " tenum=" + tenum);
         // Make sure we always seek forward:
         assert actualTerm == null || t == null || getComparator().compare(t, actualTerm) > 0: "curTerm=" + actualTerm + " seekTerm=" + t;
-        if (t == null || tenum.seekCeil(t, false) == SeekStatus.END) {
+        if (t == null || tenum.seekCeil(t) == SeekStatus.END) {
           // no more terms to seek to or enum exhausted
           //System.out.println("  return null");
           return null;
diff --git a/lucene/core/src/java/org/apache/lucene/index/FlushByRamOrCountsPolicy.java b/lucene/core/src/java/org/apache/lucene/index/FlushByRamOrCountsPolicy.java
index fbb87d8..4e2cc36 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FlushByRamOrCountsPolicy.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FlushByRamOrCountsPolicy.java
@@ -20,23 +20,32 @@ package org.apache.lucene.index;
 import org.apache.lucene.index.DocumentsWriterPerThreadPool.ThreadState;
 
 /**
- * Default {@link FlushPolicy} implementation that flushes based on RAM used,
- * document count and number of buffered deletes depending on the IndexWriter's
- * {@link IndexWriterConfig}.
+ * Default {@link FlushPolicy} implementation that flushes new segments based on
+ * RAM used and document count depending on the IndexWriter's
+ * {@link IndexWriterConfig}. It also applies pending deletes based on the
+ * number of buffered delete terms.
  * 
  * <ul>
- * <li>{@link #onDelete(DocumentsWriterFlushControl, DocumentsWriterPerThreadPool.ThreadState)} - flushes
- * based on the global number of buffered delete terms iff
- * {@link IndexWriterConfig#getMaxBufferedDeleteTerms()} is enabled</li>
- * <li>{@link #onInsert(DocumentsWriterFlushControl, DocumentsWriterPerThreadPool.ThreadState)} - flushes
- * either on the number of documents per {@link DocumentsWriterPerThread} (
+ * <li>
+ * {@link #onDelete(DocumentsWriterFlushControl, DocumentsWriterPerThreadPool.ThreadState)}
+ * - applies pending delete operations based on the global number of buffered
+ * delete terms iff {@link IndexWriterConfig#getMaxBufferedDeleteTerms()} is
+ * enabled</li>
+ * <li>
+ * {@link #onInsert(DocumentsWriterFlushControl, DocumentsWriterPerThreadPool.ThreadState)}
+ * - flushes either on the number of documents per
+ * {@link DocumentsWriterPerThread} (
  * {@link DocumentsWriterPerThread#getNumDocsInRAM()}) or on the global active
  * memory consumption in the current indexing session iff
  * {@link IndexWriterConfig#getMaxBufferedDocs()} or
  * {@link IndexWriterConfig#getRAMBufferSizeMB()} is enabled respectively</li>
- * <li>{@link #onUpdate(DocumentsWriterFlushControl, DocumentsWriterPerThreadPool.ThreadState)} - calls
- * {@link #onInsert(DocumentsWriterFlushControl, DocumentsWriterPerThreadPool.ThreadState)} and
- * {@link #onDelete(DocumentsWriterFlushControl, DocumentsWriterPerThreadPool.ThreadState)} in order</li>
+ * <li>
+ * {@link #onUpdate(DocumentsWriterFlushControl, DocumentsWriterPerThreadPool.ThreadState)}
+ * - calls
+ * {@link #onInsert(DocumentsWriterFlushControl, DocumentsWriterPerThreadPool.ThreadState)}
+ * and
+ * {@link #onDelete(DocumentsWriterFlushControl, DocumentsWriterPerThreadPool.ThreadState)}
+ * in order</li>
  * </ul>
  * All {@link IndexWriterConfig} settings are used to mark
  * {@link DocumentsWriterPerThread} as flush pending during indexing with
diff --git a/lucene/core/src/java/org/apache/lucene/index/FlushPolicy.java b/lucene/core/src/java/org/apache/lucene/index/FlushPolicy.java
index 9645479..03f6b9f 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FlushPolicy.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FlushPolicy.java
@@ -32,18 +32,19 @@ import org.apache.lucene.util.SetOnce;
  * {@link IndexWriterConfig#setRAMBufferSizeMB(double)}</li>
  * <li>Number of RAM resident documents - configured via
  * {@link IndexWriterConfig#setMaxBufferedDocs(int)}</li>
- * <li>Number of buffered delete terms/queries - configured via
- * {@link IndexWriterConfig#setMaxBufferedDeleteTerms(int)}</li>
  * </ul>
- * 
- * The {@link IndexWriter} consults a provided {@link FlushPolicy} to control the
- * flushing process. The policy is informed for each added or
- * updated document as well as for each delete term. Based on the
- * {@link FlushPolicy}, the information provided via {@link ThreadState} and
+ * The policy also applies pending delete operations (by term and/or query),
+ * given the threshold set in
+ * {@link IndexWriterConfig#setMaxBufferedDeleteTerms(int)}.
+ * <p>
+ * {@link IndexWriter} consults the provided {@link FlushPolicy} to control the
+ * flushing process. The policy is informed for each added or updated document
+ * as well as for each delete term. Based on the {@link FlushPolicy}, the
+ * information provided via {@link ThreadState} and
  * {@link DocumentsWriterFlushControl}, the {@link FlushPolicy} decides if a
- * {@link DocumentsWriterPerThread} needs flushing and mark it as
- * flush-pending via
- * {@link DocumentsWriterFlushControl#setFlushPending(DocumentsWriterPerThreadPool.ThreadState)}.
+ * {@link DocumentsWriterPerThread} needs flushing and mark it as flush-pending
+ * via {@link DocumentsWriterFlushControl#setFlushPending}, or if deletes need
+ * to be applied.
  * 
  * @see ThreadState
  * @see DocumentsWriterFlushControl
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
index d891562..93fd9ed 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
@@ -630,15 +630,13 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
 
   /**
    * Constructs a new IndexWriter per the settings given in <code>conf</code>.
-   * Note that the passed in {@link IndexWriterConfig} is
-   * privately cloned, which, in-turn, clones the
-   * {@link IndexWriterConfig#getFlushPolicy() flush policy},
-   * {@link IndexWriterConfig#getIndexDeletionPolicy() deletion policy},
-   * {@link IndexWriterConfig#getMergePolicy() merge policy},
-   * and {@link IndexWriterConfig#getMergeScheduler() merge scheduler}.
-   * If you need to make subsequent "live"
-   * changes to the configuration use {@link #getConfig}.
+   * If you want to make "live" changes to this writer instance, use
+   * {@link #getConfig()}.
+   * 
    * <p>
+   * <b>NOTE:</b> after ths writer is created, the given configuration instance
+   * cannot be passed to another writer. If you intend to do so, you should
+   * {@link IndexWriterConfig#clone() clone} it beforehand.
    * 
    * @param d
    *          the index directory. The index is either created or appended
@@ -653,7 +651,8 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
    *           IO error
    */
   public IndexWriter(Directory d, IndexWriterConfig conf) throws IOException {
-    config = new LiveIndexWriterConfig(conf.clone());
+    conf.setIndexWriter(this); // prevent reuse by other instances
+    config = new LiveIndexWriterConfig(conf);
     directory = d;
     analyzer = config.getAnalyzer();
     infoStream = config.getInfoStream();
@@ -2429,15 +2428,16 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
    * close the writer. See <a href="#OOME">above</a> for details.
    * 
    * <p>
+   * <b>NOTE:</b> empty segments are dropped by this method and not added to this
+   * index.
+   * 
+   * <p>
    * <b>NOTE:</b> this method merges all given {@link IndexReader}s in one
    * merge. If you intend to merge a large number of readers, it may be better
    * to call this method multiple times, each time with a small set of readers.
    * In principle, if you use a merge policy with a {@code mergeFactor} or
    * {@code maxMergeAtOnce} parameter, you should pass that many readers in one
-   * call. Also, if the given readers are {@link DirectoryReader}s, they can be
-   * opened with {@code termIndexInterval=-1} to save RAM, since during merge
-   * the in-memory structure is not used. See
-   * {@link DirectoryReader#open(Directory, int)}.
+   * call.
    * 
    * <p>
    * <b>NOTE</b>: if you call {@link #close(boolean)} with <tt>false</tt>, which
@@ -2462,11 +2462,20 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
       String mergedName = newSegmentName();
       final List<AtomicReader> mergeReaders = new ArrayList<AtomicReader>();
       for (IndexReader indexReader : readers) {
-        numDocs += indexReader.numDocs();
-        for (AtomicReaderContext ctx : indexReader.leaves()) {
-          mergeReaders.add(ctx.reader());
+        if (indexReader.numDocs() > 0) {
+          numDocs += indexReader.numDocs();
+          for (AtomicReaderContext ctx : indexReader.leaves()) {
+            if (ctx.reader().numDocs() > 0) { // drop empty (or all deleted) segments
+              mergeReaders.add(ctx.reader());
+            }
+          }
         }
       }
+      
+      if (mergeReaders.isEmpty()) { // no segments with documents to add
+        return;
+      }
+      
       final IOContext context = new IOContext(new MergeInfo(numDocs, -1, true, -1));
 
       // TODO: somehow we should fix this merge so it's
@@ -2476,7 +2485,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
       SegmentInfo info = new SegmentInfo(directory, Constants.LUCENE_MAIN_VERSION, mergedName, -1,
                                          false, codec, null, null);
 
-      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir, config.getTermIndexInterval(),
+      SegmentMerger merger = new SegmentMerger(mergeReaders, info, infoStream, trackingDir,
                                                MergeState.CheckAbort.NONE, globalFieldNumberMap, context);
 
       MergeState mergeState;
@@ -3658,7 +3667,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
         // Hold onto the "live" reader; we will use this to
         // commit merged deletes
         final ReadersAndLiveDocs rld = readerPool.get(info, true);
-        SegmentReader reader = rld.getMergeReader(context);
+        SegmentReader reader = rld.getReader(context);
         assert reader != null;
 
         // Carefully pull the most recent live docs:
@@ -3715,7 +3724,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit {
       // we pass merge.getMergeReaders() instead of merge.readers to allow the
       // OneMerge to return a view over the actual segments to merge
       final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),
-          merge.info.info, infoStream, dirWrapper, config.getTermIndexInterval(),
+          merge.info.info, infoStream, dirWrapper,
           checkAbort, globalFieldNumberMap, context);
 
       merge.checkAborted(directory);
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
index 43de381..8993d1b 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriterConfig.java
@@ -26,6 +26,8 @@ import org.apache.lucene.index.IndexWriter.IndexReaderWarmer;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.PrintStreamInfoStream;
+import org.apache.lucene.util.SetOnce;
+import org.apache.lucene.util.SetOnce.AlreadySetException;
 import org.apache.lucene.util.Version;
 
 /**
@@ -70,9 +72,6 @@ public final class IndexWriterConfig extends LiveIndexWriterConfig implements Cl
     CREATE_OR_APPEND 
   }
 
-  /** Default value is 32. Change using {@link #setTermIndexInterval(int)}. */
-  public static final int DEFAULT_TERM_INDEX_INTERVAL = 32; // TODO: this should be private to the codec, not settable here
-
   /** Denotes a flush trigger is disabled. */
   public final static int DISABLE_AUTO_FLUSH = -1;
 
@@ -98,9 +97,6 @@ public final class IndexWriterConfig extends LiveIndexWriterConfig implements Cl
   /** Default setting for {@link #setReaderPooling}. */
   public final static boolean DEFAULT_READER_POOLING = false;
 
-  /** Default value is 1. Change using {@link #setReaderTermsIndexDivisor(int)}. */
-  public static final int DEFAULT_READER_TERMS_INDEX_DIVISOR = DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR;
-
   /** Default value is 1945. Change using {@link #setRAMPerThreadHardLimitMB(int)} */
   public static final int DEFAULT_RAM_PER_THREAD_HARD_LIMIT_MB = 1945;
   
@@ -132,6 +128,21 @@ public final class IndexWriterConfig extends LiveIndexWriterConfig implements Cl
     return WRITE_LOCK_TIMEOUT;
   }
 
+  // indicates whether this config instance is already attached to a writer.
+  // not final so that it can be cloned properly.
+  private SetOnce<IndexWriter> writer = new SetOnce<IndexWriter>();
+  
+  /**
+   * Sets the {@link IndexWriter} this config is attached to.
+   * 
+   * @throws AlreadySetException
+   *           if this config is already attached to a writer.
+   */
+  IndexWriterConfig setIndexWriter(IndexWriter writer) {
+    this.writer.set(writer);
+    return this;
+  }
+  
   /**
    * Creates a new config that with defaults that match the specified
    * {@link Version} as well as the default {@link
@@ -152,6 +163,8 @@ public final class IndexWriterConfig extends LiveIndexWriterConfig implements Cl
     try {
       IndexWriterConfig clone = (IndexWriterConfig) super.clone();
       
+      clone.writer = writer.clone();
+      
       // Mostly shallow clone, but do a deepish clone of
       // certain objects that have state that cannot be shared
       // across IW instances:
@@ -484,16 +497,6 @@ public final class IndexWriterConfig extends LiveIndexWriterConfig implements Cl
     return super.getRAMBufferSizeMB();
   }
   
-  @Override
-  public int getReaderTermsIndexDivisor() {
-    return super.getReaderTermsIndexDivisor();
-  }
-  
-  @Override
-  public int getTermIndexInterval() {
-    return super.getTermIndexInterval();
-  }
-  
   /** If non-null, information about merges, deletes and a
    * message when maxFieldLength is reached will be printed
    * to this.
@@ -536,17 +539,15 @@ public final class IndexWriterConfig extends LiveIndexWriterConfig implements Cl
   }
   
   @Override
-  public IndexWriterConfig setReaderTermsIndexDivisor(int divisor) {
-    return (IndexWriterConfig) super.setReaderTermsIndexDivisor(divisor);
-  }
-  
-  @Override
-  public IndexWriterConfig setTermIndexInterval(int interval) {
-    return (IndexWriterConfig) super.setTermIndexInterval(interval);
-  }
-  
   public IndexWriterConfig setUseCompoundFile(boolean useCompoundFile) {
     return (IndexWriterConfig) super.setUseCompoundFile(useCompoundFile);
   }
 
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder(super.toString());
+    sb.append("writer=").append(writer).append("\n");
+    return sb.toString();
+  }
+  
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java b/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
old mode 100755
new mode 100644
index 8bb344e..23e1cf2
--- a/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
+++ b/lucene/core/src/java/org/apache/lucene/index/LiveIndexWriterConfig.java
@@ -19,7 +19,6 @@ package org.apache.lucene.index;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
 import org.apache.lucene.index.DocumentsWriterPerThread.IndexingChain;
 import org.apache.lucene.index.IndexWriter.IndexReaderWarmer;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
@@ -41,9 +40,7 @@ public class LiveIndexWriterConfig {
   private volatile int maxBufferedDocs;
   private volatile double ramBufferSizeMB;
   private volatile int maxBufferedDeleteTerms;
-  private volatile int readerTermsIndexDivisor;
   private volatile IndexReaderWarmer mergedSegmentWarmer;
-  private volatile int termIndexInterval; // TODO: this should be private to the codec, not settable here
 
   // modified by IndexWriterConfig
   /** {@link IndexDeletionPolicy} controlling when commit
@@ -108,9 +105,7 @@ public class LiveIndexWriterConfig {
     ramBufferSizeMB = IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB;
     maxBufferedDocs = IndexWriterConfig.DEFAULT_MAX_BUFFERED_DOCS;
     maxBufferedDeleteTerms = IndexWriterConfig.DEFAULT_MAX_BUFFERED_DELETE_TERMS;
-    readerTermsIndexDivisor = IndexWriterConfig.DEFAULT_READER_TERMS_INDEX_DIVISOR;
     mergedSegmentWarmer = null;
-    termIndexInterval = IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL; // TODO: this should be private to the codec, not settable here
     delPolicy = new KeepOnlyLastCommitDeletionPolicy();
     commit = null;
     useCompoundFile = IndexWriterConfig.DEFAULT_USE_COMPOUND_FILE_SYSTEM;
@@ -140,8 +135,6 @@ public class LiveIndexWriterConfig {
     maxBufferedDocs = config.getMaxBufferedDocs();
     mergedSegmentWarmer = config.getMergedSegmentWarmer();
     ramBufferSizeMB = config.getRAMBufferSizeMB();
-    readerTermsIndexDivisor = config.getReaderTermsIndexDivisor();
-    termIndexInterval = config.getTermIndexInterval();
     matchVersion = config.matchVersion;
     analyzer = config.getAnalyzer();
     delPolicy = config.getIndexDeletionPolicy();
@@ -165,73 +158,11 @@ public class LiveIndexWriterConfig {
   public Analyzer getAnalyzer() {
     return analyzer;
   }
-  
-  /**
-   * Expert: set the interval between indexed terms. Large values cause less
-   * memory to be used by IndexReader, but slow random-access to terms. Small
-   * values cause more memory to be used by an IndexReader, and speed
-   * random-access to terms.
-   * <p>
-   * This parameter determines the amount of computation required per query
-   * term, regardless of the number of documents that contain that term. In
-   * particular, it is the maximum number of other terms that must be scanned
-   * before a term is located and its frequency and position information may be
-   * processed. In a large index with user-entered query terms, query processing
-   * time is likely to be dominated not by term lookup but rather by the
-   * processing of frequency and positional data. In a small index or when many
-   * uncommon query terms are generated (e.g., by wildcard queries) term lookup
-   * may become a dominant cost.
-   * <p>
-   * In particular, <code>numUniqueTerms/interval</code> terms are read into
-   * memory by an IndexReader, and, on average, <code>interval/2</code> terms
-   * must be scanned for each random term access.
-   * 
-   * <p>
-   * Takes effect immediately, but only applies to newly flushed/merged
-   * segments.
-   * 
-   * <p>
-   * <b>NOTE:</b> This parameter does not apply to all PostingsFormat implementations,
-   * including the default one in this release. It only makes sense for term indexes
-   * that are implemented as a fixed gap between terms. For example, 
-   * {@link Lucene41PostingsFormat} implements the term index instead based upon how
-   * terms share prefixes. To configure its parameters (the minimum and maximum size
-   * for a block), you would instead use  {@link Lucene41PostingsFormat#Lucene41PostingsFormat(int, int)}.
-   * which can also be configured on a per-field basis:
-   * <pre class="prettyprint">
-   * //customize Lucene41PostingsFormat, passing minBlockSize=50, maxBlockSize=100
-   * final PostingsFormat tweakedPostings = new Lucene41PostingsFormat(50, 100);
-   * iwc.setCodec(new Lucene42Codec() {
-   *   &#64;Override
-   *   public PostingsFormat getPostingsFormatForField(String field) {
-   *     if (field.equals("fieldWithTonsOfTerms"))
-   *       return tweakedPostings;
-   *     else
-   *       return super.getPostingsFormatForField(field);
-   *   }
-   * });
-   * </pre>
-   * Note that other implementations may have their own parameters, or no parameters at all.
-   * 
-   * @see IndexWriterConfig#DEFAULT_TERM_INDEX_INTERVAL
-   */
-  public LiveIndexWriterConfig setTermIndexInterval(int interval) { // TODO: this should be private to the codec, not settable here
-    this.termIndexInterval = interval;
-    return this;
-  }
-
-  /**
-   * Returns the interval between indexed terms.
-   *
-   * @see #setTermIndexInterval(int)
-   */
-  public int getTermIndexInterval() { // TODO: this should be private to the codec, not settable here
-    return termIndexInterval;
-  }
 
   /**
-   * Determines the minimal number of delete terms required before the buffered
-   * in-memory delete terms and queries are applied and flushed.
+   * Determines the maximum number of delete-by-term operations that will be
+   * buffered before both the buffered in-memory delete terms and queries are
+   * applied and flushed.
    * <p>
    * Disabled by default (writer flushes by RAM usage).
    * <p>
@@ -239,7 +170,8 @@ public class LiveIndexWriterConfig {
    * 
    * <p>
    * Takes effect immediately, but only the next time a document is added,
-   * updated or deleted.
+   * updated or deleted. Also, if you only delete-by-query, this setting has no
+   * effect, i.e. delete queries are buffered until the next segment is flushed.
    * 
    * @throws IllegalArgumentException
    *           if maxBufferedDeleteTerms is enabled but smaller than 1
@@ -390,37 +322,6 @@ public class LiveIndexWriterConfig {
   public IndexReaderWarmer getMergedSegmentWarmer() {
     return mergedSegmentWarmer;
   }
-
-  /**
-   * Sets the termsIndexDivisor passed to any readers that IndexWriter opens,
-   * for example when applying deletes or creating a near-real-time reader in
-   * {@link DirectoryReader#open(IndexWriter, boolean)}. If you pass -1, the
-   * terms index won't be loaded by the readers. This is only useful in advanced
-   * situations when you will only .next() through all terms; attempts to seek
-   * will hit an exception.
-   * 
-   * <p>
-   * Takes effect immediately, but only applies to readers opened after this
-   * call
-   * <p>
-   * <b>NOTE:</b> divisor settings &gt; 1 do not apply to all PostingsFormat
-   * implementations, including the default one in this release. It only makes
-   * sense for terms indexes that can efficiently re-sample terms at load time.
-   */
-  public LiveIndexWriterConfig setReaderTermsIndexDivisor(int divisor) {
-    if (divisor <= 0 && divisor != -1) {
-      throw new IllegalArgumentException("divisor must be >= 1, or -1 (got " + divisor + ")");
-    }
-    readerTermsIndexDivisor = divisor;
-    return this;
-  }
-
-  /** Returns the {@code termInfosIndexDivisor}.
-   * 
-   * @see #setReaderTermsIndexDivisor(int) */
-  public int getReaderTermsIndexDivisor() {
-    return readerTermsIndexDivisor;
-  }
   
   /** Returns the {@link OpenMode} set by {@link IndexWriterConfig#setOpenMode(OpenMode)}. */
   public OpenMode getOpenMode() {
@@ -583,8 +484,6 @@ public class LiveIndexWriterConfig {
     sb.append("maxBufferedDocs=").append(getMaxBufferedDocs()).append("\n");
     sb.append("maxBufferedDeleteTerms=").append(getMaxBufferedDeleteTerms()).append("\n");
     sb.append("mergedSegmentWarmer=").append(getMergedSegmentWarmer()).append("\n");
-    sb.append("readerTermsIndexDivisor=").append(getReaderTermsIndexDivisor()).append("\n");
-    sb.append("termIndexInterval=").append(getTermIndexInterval()).append("\n"); // TODO: this should be private to the codec, not settable here
     sb.append("delPolicy=").append(getIndexDeletionPolicy().getClass().getName()).append("\n");
     IndexCommit commit = getIndexCommit();
     sb.append("commit=").append(commit == null ? "null" : commit).append("\n");
diff --git a/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java b/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
index f4aa8cd..8f262a8 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
@@ -23,8 +23,9 @@ import java.util.List;
 import org.apache.lucene.index.MultiTermsEnum.TermsEnumIndex;
 import org.apache.lucene.index.MultiTermsEnum.TermsEnumWithSlice;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.packed.AppendingLongBuffer;
+import org.apache.lucene.util.packed.AppendingPackedLongBuffer;
 import org.apache.lucene.util.packed.MonotonicAppendingLongBuffer;
+import org.apache.lucene.util.packed.PackedInts;
 
 /**
  * A wrapper for CompositeIndexReader providing access to DocValues.
@@ -277,7 +278,7 @@ public class MultiDocValues {
     // globalOrd -> (globalOrd - segmentOrd)
     final MonotonicAppendingLongBuffer globalOrdDeltas;
     // globalOrd -> sub index
-    final AppendingLongBuffer subIndexes;
+    final AppendingPackedLongBuffer subIndexes;
     // segmentOrd -> (globalOrd - segmentOrd)
     final MonotonicAppendingLongBuffer ordDeltas[];
     
@@ -293,8 +294,8 @@ public class MultiDocValues {
       // create the ordinal mappings by pulling a termsenum over each sub's 
       // unique terms, and walking a multitermsenum over those
       this.owner = owner;
-      globalOrdDeltas = new MonotonicAppendingLongBuffer();
-      subIndexes = new AppendingLongBuffer();
+      globalOrdDeltas = new MonotonicAppendingLongBuffer(PackedInts.COMPACT);
+      subIndexes = new AppendingPackedLongBuffer(PackedInts.COMPACT);
       ordDeltas = new MonotonicAppendingLongBuffer[subs.length];
       for (int i = 0; i < ordDeltas.length; i++) {
         ordDeltas[i] = new MonotonicAppendingLongBuffer();
diff --git a/lucene/core/src/java/org/apache/lucene/index/MultiFields.java b/lucene/core/src/java/org/apache/lucene/index/MultiFields.java
index b72d9f3..86956dd 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MultiFields.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MultiFields.java
@@ -150,7 +150,7 @@ public final class MultiFields extends Fields {
     final Terms terms = getTerms(r, field);
     if (terms != null) {
       final TermsEnum termsEnum = terms.iterator(null);
-      if (termsEnum.seekExact(term, true)) {
+      if (termsEnum.seekExact(term)) {
         return termsEnum.docs(liveDocs, null, flags);
       }
     }
@@ -178,7 +178,7 @@ public final class MultiFields extends Fields {
     final Terms terms = getTerms(r, field);
     if (terms != null) {
       final TermsEnum termsEnum = terms.iterator(null);
-      if (termsEnum.seekExact(term, true)) {
+      if (termsEnum.seekExact(term)) {
         return termsEnum.docsAndPositions(liveDocs, null, flags);
       }
     }
diff --git a/lucene/core/src/java/org/apache/lucene/index/MultiTermsEnum.java b/lucene/core/src/java/org/apache/lucene/index/MultiTermsEnum.java
index bcaac7c..937804e 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MultiTermsEnum.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MultiTermsEnum.java
@@ -144,7 +144,7 @@ public final class MultiTermsEnum extends TermsEnum {
   }
 
   @Override
-  public boolean seekExact(BytesRef term, boolean useCache) throws IOException {
+  public boolean seekExact(BytesRef term) throws IOException {
     queue.clear();
     numTop = 0;
 
@@ -173,13 +173,13 @@ public final class MultiTermsEnum extends TermsEnum {
           } else if (cmp < 0) {
             status = false;
           } else {
-            status = currentSubs[i].terms.seekExact(term, useCache);
+            status = currentSubs[i].terms.seekExact(term);
           }
         } else {
           status = false;
         }
       } else {
-        status = currentSubs[i].terms.seekExact(term, useCache);
+        status = currentSubs[i].terms.seekExact(term);
       }
 
       if (status) {
@@ -195,7 +195,7 @@ public final class MultiTermsEnum extends TermsEnum {
   }
 
   @Override
-  public SeekStatus seekCeil(BytesRef term, boolean useCache) throws IOException {
+  public SeekStatus seekCeil(BytesRef term) throws IOException {
     queue.clear();
     numTop = 0;
     lastSeekExact = false;
@@ -225,13 +225,13 @@ public final class MultiTermsEnum extends TermsEnum {
           } else if (cmp < 0) {
             status = SeekStatus.NOT_FOUND;
           } else {
-            status = currentSubs[i].terms.seekCeil(term, useCache);
+            status = currentSubs[i].terms.seekCeil(term);
           }
         } else {
           status = SeekStatus.END;
         }
       } else {
-        status = currentSubs[i].terms.seekCeil(term, useCache);
+        status = currentSubs[i].terms.seekCeil(term);
       }
 
       if (status == SeekStatus.FOUND) {
diff --git a/lucene/core/src/java/org/apache/lucene/index/NumericDocValuesWriter.java b/lucene/core/src/java/org/apache/lucene/index/NumericDocValuesWriter.java
index a77e8ea..cc07083 100644
--- a/lucene/core/src/java/org/apache/lucene/index/NumericDocValuesWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/NumericDocValuesWriter.java
@@ -23,7 +23,8 @@ import java.util.NoSuchElementException;
 
 import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.packed.AppendingLongBuffer;
+import org.apache.lucene.util.packed.AppendingDeltaPackedLongBuffer;
+import org.apache.lucene.util.packed.PackedInts;
 
 /** Buffers up pending long per doc, then flushes when
  *  segment flushes. */
@@ -31,13 +32,13 @@ class NumericDocValuesWriter extends DocValuesWriter {
 
   private final static long MISSING = 0L;
 
-  private AppendingLongBuffer pending;
+  private AppendingDeltaPackedLongBuffer pending;
   private final Counter iwBytesUsed;
   private long bytesUsed;
   private final FieldInfo fieldInfo;
 
   public NumericDocValuesWriter(FieldInfo fieldInfo, Counter iwBytesUsed) {
-    pending = new AppendingLongBuffer();
+    pending = new AppendingDeltaPackedLongBuffer(PackedInts.COMPACT);
     bytesUsed = pending.ramBytesUsed();
     this.fieldInfo = fieldInfo;
     this.iwBytesUsed = iwBytesUsed;
@@ -89,7 +90,7 @@ class NumericDocValuesWriter extends DocValuesWriter {
   
   // iterates over the values we have in ram
   private class NumericIterator implements Iterator<Number> {
-    final AppendingLongBuffer.Iterator iter = pending.iterator();
+    final AppendingDeltaPackedLongBuffer.Iterator iter = pending.iterator();
     final int size = (int)pending.size();
     final int maxDoc;
     int upto;
diff --git a/lucene/core/src/java/org/apache/lucene/index/ReaderManager.java b/lucene/core/src/java/org/apache/lucene/index/ReaderManager.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/index/ReadersAndLiveDocs.java b/lucene/core/src/java/org/apache/lucene/index/ReadersAndLiveDocs.java
index a80a7ce..a361583 100644
--- a/lucene/core/src/java/org/apache/lucene/index/ReadersAndLiveDocs.java
+++ b/lucene/core/src/java/org/apache/lucene/index/ReadersAndLiveDocs.java
@@ -43,16 +43,6 @@ class ReadersAndLiveDocs {
   // Set once (null, and then maybe set, and never set again):
   private SegmentReader reader;
 
-  // TODO: it's sometimes wasteful that we hold open two
-  // separate SRs (one for merging one for
-  // reading)... maybe just use a single SR?  The gains of
-  // not loading the terms index (for merging in the
-  // non-NRT case) are far less now... and if the app has
-  // any deletes it'll open real readers anyway.
-
-  // Set once (null, and then maybe set, and never set again):
-  private SegmentReader mergeReader;
-
   // Holds the current shared (readable and writable
   // liveDocs).  This is null when there are no deleted
   // docs, and it's copy-on-write (cloned whenever we need
@@ -118,7 +108,7 @@ class ReadersAndLiveDocs {
 
     if (reader == null) {
       // We steal returned ref:
-      reader = new SegmentReader(info, writer.getConfig().getReaderTermsIndexDivisor(), context);
+      reader = new SegmentReader(info, context);
       if (liveDocs == null) {
         liveDocs = reader.getLiveDocs();
       }
@@ -131,37 +121,6 @@ class ReadersAndLiveDocs {
     return reader;
   }
 
-  // Get reader for merging (does not load the terms
-  // index):
-  public synchronized SegmentReader getMergeReader(IOContext context) throws IOException {
-    //System.out.println("  livedocs=" + rld.liveDocs);
-
-    if (mergeReader == null) {
-
-      if (reader != null) {
-        // Just use the already opened non-merge reader
-        // for merging.  In the NRT case this saves us
-        // pointless double-open:
-        //System.out.println("PROMOTE non-merge reader seg=" + rld.info);
-        // Ref for us:
-        reader.incRef();
-        mergeReader = reader;
-        //System.out.println(Thread.currentThread().getName() + ": getMergeReader share seg=" + info.name);
-      } else {
-        //System.out.println(Thread.currentThread().getName() + ": getMergeReader seg=" + info.name);
-        // We steal returned ref:
-        mergeReader = new SegmentReader(info, -1, context);
-        if (liveDocs == null) {
-          liveDocs = mergeReader.getLiveDocs();
-        }
-      }
-    }
-
-    // Ref for caller
-    mergeReader.incRef();
-    return mergeReader;
-  }
-
   public synchronized void release(SegmentReader sr) throws IOException {
     assert info == sr.getSegmentInfo();
     sr.decRef();
@@ -185,23 +144,12 @@ class ReadersAndLiveDocs {
   public synchronized void dropReaders() throws IOException {
     // TODO: can we somehow use IOUtils here...?  problem is
     // we are calling .decRef not .close)...
-    try {
-      if (reader != null) {
-        //System.out.println("  pool.drop info=" + info + " rc=" + reader.getRefCount());
-        try {
-          reader.decRef();
-        } finally {
-          reader = null;
-        }
-      }
-    } finally {
-      if (mergeReader != null) {
-        //System.out.println("  pool.drop info=" + info + " merge rc=" + mergeReader.getRefCount());
-        try {
-          mergeReader.decRef();
-        } finally {
-          mergeReader = null;
-        }
+    if (reader != null) {
+      //System.out.println("  pool.drop info=" + info + " rc=" + reader.getRefCount());
+      try {
+        reader.decRef();
+      } finally {
+        reader = null;
       }
     }
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java b/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
index e5498a2..ab03482 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
@@ -57,8 +57,6 @@ final class SegmentCoreReaders {
   final DocValuesProducer dvProducer;
   final DocValuesProducer normsProducer;
 
-  final int termsIndexDivisor;
-  
   private final SegmentReader owner;
   
   final StoredFieldsReader fieldsReaderOrig;
@@ -100,11 +98,7 @@ final class SegmentCoreReaders {
   private final Set<CoreClosedListener> coreClosedListeners = 
       Collections.synchronizedSet(new LinkedHashSet<CoreClosedListener>());
   
-  SegmentCoreReaders(SegmentReader owner, Directory dir, SegmentInfoPerCommit si, IOContext context, int termsIndexDivisor) throws IOException {
-    
-    if (termsIndexDivisor == 0) {
-      throw new IllegalArgumentException("indexDivisor must be < 0 (don't load terms index) or greater than 0 (got 0)");
-    }
+  SegmentCoreReaders(SegmentReader owner, Directory dir, SegmentInfoPerCommit si, IOContext context) throws IOException {
     
     final Codec codec = si.info.getCodec();
     final Directory cfsDir; // confusing name: if (cfs) its the cfsdir, otherwise its the segment's directory.
@@ -120,9 +114,8 @@ final class SegmentCoreReaders {
       }
       fieldInfos = codec.fieldInfosFormat().getFieldInfosReader().read(cfsDir, si.info.name, IOContext.READONCE);
 
-      this.termsIndexDivisor = termsIndexDivisor;
       final PostingsFormat format = codec.postingsFormat();
-      final SegmentReadState segmentReadState = new SegmentReadState(cfsDir, si.info, fieldInfos, context, termsIndexDivisor);
+      final SegmentReadState segmentReadState = new SegmentReadState(cfsDir, si.info, fieldInfos, context);
       // Ask codec for its Fields
       fields = format.fieldsProducer(segmentReadState);
       assert fields != null;
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
index 6631020..4c1fdb9 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -42,7 +42,6 @@ import org.apache.lucene.util.InfoStream;
  */
 final class SegmentMerger {
   private final Directory directory;
-  private final int termIndexInterval;
 
   private final Codec codec;
   
@@ -52,11 +51,10 @@ final class SegmentMerger {
   private final FieldInfos.Builder fieldInfosBuilder;
 
   // note, just like in codec apis Directory 'dir' is NOT the same as segmentInfo.dir!!
-  SegmentMerger(List<AtomicReader> readers, SegmentInfo segmentInfo, InfoStream infoStream, Directory dir, int termIndexInterval,
+  SegmentMerger(List<AtomicReader> readers, SegmentInfo segmentInfo, InfoStream infoStream, Directory dir,
                 MergeState.CheckAbort checkAbort, FieldInfos.FieldNumbers fieldNumbers, IOContext context) {
     mergeState = new MergeState(readers, segmentInfo, infoStream, checkAbort);
     directory = dir;
-    this.termIndexInterval = termIndexInterval;
     this.codec = segmentInfo.getCodec();
     this.context = context;
     this.fieldInfosBuilder = new FieldInfos.Builder(fieldNumbers);
@@ -91,7 +89,7 @@ final class SegmentMerger {
     assert numMerged == mergeState.segmentInfo.getDocCount();
 
     final SegmentWriteState segmentWriteState = new SegmentWriteState(mergeState.infoStream, directory, mergeState.segmentInfo,
-                                                                      mergeState.fieldInfos, termIndexInterval, null, context);
+                                                                      mergeState.fieldInfos, null, context);
     if (mergeState.infoStream.isEnabled("SM")) {
       t0 = System.nanoTime();
     }
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentReadState.java b/lucene/core/src/java/org/apache/lucene/index/SegmentReadState.java
index fc1ea8f..b7eef06 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentReadState.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentReadState.java
@@ -41,17 +41,6 @@ public class SegmentReadState {
    *  Directory#openInput(String,IOContext)}. */
   public final IOContext context;
 
-  /** The {@code termInfosIndexDivisor} to use, if
-   *  appropriate (not all {@link PostingsFormat}s support
-   *  it; in particular the current default does not).
-   *
-   * <p>  NOTE: if this is &lt; 0, that means "defer terms index
-   *  load until needed".  But if the codec must load the
-   *  terms index on init (preflex is the only once currently
-   *  that must do so), then it should negate this value to
-   *  get the app's terms divisor */
-  public int termsIndexDivisor;
-
   /** Unique suffix for any postings files read for this
    *  segment.  {@link PerFieldPostingsFormat} sets this for
    *  each of the postings formats it wraps.  If you create
@@ -62,8 +51,8 @@ public class SegmentReadState {
 
   /** Create a {@code SegmentReadState}. */
   public SegmentReadState(Directory dir, SegmentInfo info,
-      FieldInfos fieldInfos, IOContext context, int termsIndexDivisor) {
-    this(dir, info, fieldInfos,  context, termsIndexDivisor, "");
+      FieldInfos fieldInfos, IOContext context) {
+    this(dir, info, fieldInfos,  context, "");
   }
   
   /** Create a {@code SegmentReadState}. */
@@ -71,13 +60,11 @@ public class SegmentReadState {
                           SegmentInfo info,
                           FieldInfos fieldInfos,
                           IOContext context,
-                          int termsIndexDivisor,
                           String segmentSuffix) {
     this.directory = dir;
     this.segmentInfo = info;
     this.fieldInfos = fieldInfos;
     this.context = context;
-    this.termsIndexDivisor = termsIndexDivisor;
     this.segmentSuffix = segmentSuffix;
   }
 
@@ -88,7 +75,6 @@ public class SegmentReadState {
     this.segmentInfo = other.segmentInfo;
     this.fieldInfos = other.fieldInfos;
     this.context = other.context;
-    this.termsIndexDivisor = other.termsIndexDivisor;
     this.segmentSuffix = newSegmentSuffix;
   }
 }
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java b/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
index a539de1..8214a98 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentReader.java
@@ -51,9 +51,9 @@ public final class SegmentReader extends AtomicReader {
    * @throws IOException if there is a low-level IO error
    */
   // TODO: why is this public?
-  public SegmentReader(SegmentInfoPerCommit si, int termInfosIndexDivisor, IOContext context) throws IOException {
+  public SegmentReader(SegmentInfoPerCommit si, IOContext context) throws IOException {
     this.si = si;
-    core = new SegmentCoreReaders(this, si.info.dir, si, context, termInfosIndexDivisor);
+    core = new SegmentCoreReaders(this, si.info.dir, si, context);
     boolean success = false;
     try {
       if (si.hasDeletions()) {
@@ -217,12 +217,6 @@ public final class SegmentReader extends AtomicReader {
     return this;
   }
 
-  /** Returns term infos index divisor originally passed to
-   *  {@link #SegmentReader(SegmentInfoPerCommit, int, IOContext)}. */
-  public int getTermInfosIndexDivisor() {
-    return core.termsIndexDivisor;
-  }
-
   @Override
   public NumericDocValues getNumericDocValues(String field) throws IOException {
     ensureOpen();
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java b/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java
index ed390c0..7caf012 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java
@@ -66,13 +66,6 @@ public class SegmentWriteState {
    *  write/read must be derived using this suffix (use
    *  {@link IndexFileNames#segmentFileName(String,String,String)}). */
   public final String segmentSuffix;
-
-  /** Expert: The fraction of terms in the "dictionary" which should be stored
-   * in RAM.  Smaller values use more memory, but make searching slightly
-   * faster, while larger values use less memory and make searching slightly
-   * slower.  Searching is typically not dominated by dictionary lookup, so
-   * tweaking this is rarely useful.*/
-  public int termIndexInterval;                   // TODO: this should be private to the codec, not settable here or in IWC
   
   /** {@link IOContext} for all writes; you should pass this
    *  to {@link Directory#createOutput(String,IOContext)}. */
@@ -80,26 +73,22 @@ public class SegmentWriteState {
 
   /** Sole constructor. */
   public SegmentWriteState(InfoStream infoStream, Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos,
-      int termIndexInterval, BufferedDeletes segDeletes, IOContext context) {
+      BufferedDeletes segDeletes, IOContext context) {
     this.infoStream = infoStream;
     this.segDeletes = segDeletes;
     this.directory = directory;
     this.segmentInfo = segmentInfo;
     this.fieldInfos = fieldInfos;
-    this.termIndexInterval = termIndexInterval;
     segmentSuffix = "";
     this.context = context;
   }
   
-  /**
-   * Create a shallow {@link SegmentWriteState} copy final a format ID
-   */
+  /** Create a shallow copy of {@link SegmentWriteState} with a new segment suffix. */
   public SegmentWriteState(SegmentWriteState state, String segmentSuffix) {
     infoStream = state.infoStream;
     directory = state.directory;
     segmentInfo = state.segmentInfo;
     fieldInfos = state.fieldInfos;
-    termIndexInterval = state.termIndexInterval;
     context = state.context;
     this.segmentSuffix = segmentSuffix;
     segDeletes = state.segDeletes;
diff --git a/lucene/core/src/java/org/apache/lucene/index/SortedDocValuesTermsEnum.java b/lucene/core/src/java/org/apache/lucene/index/SortedDocValuesTermsEnum.java
index c30ea86..e3d9c5c 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SortedDocValuesTermsEnum.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SortedDocValuesTermsEnum.java
@@ -37,7 +37,7 @@ class SortedDocValuesTermsEnum extends TermsEnum {
   }
 
   @Override
-  public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
+  public SeekStatus seekCeil(BytesRef text) throws IOException {
     int ord = values.lookupTerm(text);
     if (ord >= 0) {
       currentOrd = ord;
@@ -61,7 +61,7 @@ class SortedDocValuesTermsEnum extends TermsEnum {
   }
 
   @Override
-  public boolean seekExact(BytesRef text, boolean useCache) throws IOException {
+  public boolean seekExact(BytesRef text) throws IOException {
     int ord = values.lookupTerm(text);
     if (ord >= 0) {
       term.offset = 0;
diff --git a/lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java b/lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java
index 2d038e3..d337a0c 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SortedDocValuesWriter.java
@@ -30,13 +30,14 @@ import org.apache.lucene.util.BytesRefHash.DirectBytesStartArray;
 import org.apache.lucene.util.BytesRefHash;
 import org.apache.lucene.util.Counter;
 import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.AppendingLongBuffer;
+import org.apache.lucene.util.packed.AppendingPackedLongBuffer;
+import org.apache.lucene.util.packed.PackedInts;
 
 /** Buffers up pending byte[] per doc, deref and sorting via
  *  int ord, then flushes when segment flushes. */
 class SortedDocValuesWriter extends DocValuesWriter {
   final BytesRefHash hash;
-  private AppendingLongBuffer pending;
+  private AppendingPackedLongBuffer pending;
   private final Counter iwBytesUsed;
   private long bytesUsed; // this currently only tracks differences in 'pending'
   private final FieldInfo fieldInfo;
@@ -51,7 +52,7 @@ class SortedDocValuesWriter extends DocValuesWriter {
             new ByteBlockPool.DirectTrackingAllocator(iwBytesUsed)),
             BytesRefHash.DEFAULT_CAPACITY,
             new DirectBytesStartArray(BytesRefHash.DEFAULT_CAPACITY, iwBytesUsed));
-    pending = new AppendingLongBuffer();
+    pending = new AppendingPackedLongBuffer(PackedInts.COMPACT);
     bytesUsed = pending.ramBytesUsed();
     iwBytesUsed.addAndGet(bytesUsed);
   }
@@ -176,7 +177,7 @@ class SortedDocValuesWriter extends DocValuesWriter {
   
   // iterates over the ords for each doc we have in ram
   private class OrdsIterator implements Iterator<Number> {
-    final AppendingLongBuffer.Iterator iter = pending.iterator();
+    final AppendingPackedLongBuffer.Iterator iter = pending.iterator();
     final int ordMap[];
     final int maxDoc;
     int docUpto;
diff --git a/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesTermsEnum.java b/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesTermsEnum.java
index a9ceac9..3c04135 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesTermsEnum.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesTermsEnum.java
@@ -37,7 +37,7 @@ class SortedSetDocValuesTermsEnum extends TermsEnum {
   }
 
   @Override
-  public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
+  public SeekStatus seekCeil(BytesRef text) throws IOException {
     long ord = values.lookupTerm(text);
     if (ord >= 0) {
       currentOrd = ord;
@@ -61,7 +61,7 @@ class SortedSetDocValuesTermsEnum extends TermsEnum {
   }
 
   @Override
-  public boolean seekExact(BytesRef text, boolean useCache) throws IOException {
+  public boolean seekExact(BytesRef text) throws IOException {
     long ord = values.lookupTerm(text);
     if (ord >= 0) {
       term.offset = 0;
diff --git a/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java b/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java
index 43a5ae7..ee2494b 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java
@@ -32,14 +32,16 @@ import org.apache.lucene.util.BytesRefHash.DirectBytesStartArray;
 import org.apache.lucene.util.BytesRefHash;
 import org.apache.lucene.util.Counter;
 import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.AppendingLongBuffer;
+import org.apache.lucene.util.packed.AppendingDeltaPackedLongBuffer;
+import org.apache.lucene.util.packed.AppendingPackedLongBuffer;
+import org.apache.lucene.util.packed.PackedInts;
 
 /** Buffers up pending byte[]s per doc, deref and sorting via
  *  int ord, then flushes when segment flushes. */
 class SortedSetDocValuesWriter extends DocValuesWriter {
   final BytesRefHash hash;
-  private AppendingLongBuffer pending; // stream of all termIDs
-  private AppendingLongBuffer pendingCounts; // termIDs per doc
+  private AppendingPackedLongBuffer pending; // stream of all termIDs
+  private AppendingDeltaPackedLongBuffer pendingCounts; // termIDs per doc
   private final Counter iwBytesUsed;
   private long bytesUsed; // this only tracks differences in 'pending' and 'pendingCounts'
   private final FieldInfo fieldInfo;
@@ -56,8 +58,8 @@ class SortedSetDocValuesWriter extends DocValuesWriter {
             new ByteBlockPool.DirectTrackingAllocator(iwBytesUsed)),
             BytesRefHash.DEFAULT_CAPACITY,
             new DirectBytesStartArray(BytesRefHash.DEFAULT_CAPACITY, iwBytesUsed));
-    pending = new AppendingLongBuffer();
-    pendingCounts = new AppendingLongBuffer();
+    pending = new AppendingPackedLongBuffer(PackedInts.COMPACT);
+    pendingCounts = new AppendingDeltaPackedLongBuffer(PackedInts.COMPACT);
     bytesUsed = pending.ramBytesUsed() + pendingCounts.ramBytesUsed();
     iwBytesUsed.addAndGet(bytesUsed);
   }
@@ -224,8 +226,8 @@ class SortedSetDocValuesWriter extends DocValuesWriter {
   
   // iterates over the ords for each doc we have in ram
   private class OrdsIterator implements Iterator<Number> {
-    final AppendingLongBuffer.Iterator iter = pending.iterator();
-    final AppendingLongBuffer.Iterator counts = pendingCounts.iterator();
+    final AppendingPackedLongBuffer.Iterator iter = pending.iterator();
+    final AppendingDeltaPackedLongBuffer.Iterator counts = pendingCounts.iterator();
     final int ordMap[];
     final long numOrds;
     long ordUpto;
@@ -273,7 +275,7 @@ class SortedSetDocValuesWriter extends DocValuesWriter {
   }
   
   private class OrdCountIterator implements Iterator<Number> {
-    final AppendingLongBuffer.Iterator iter = pendingCounts.iterator();
+    final AppendingDeltaPackedLongBuffer.Iterator iter = pendingCounts.iterator();
     final int maxDoc;
     int docUpto;
     
diff --git a/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java b/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
index 18d8463..7e4316a 100644
--- a/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/StandardDirectoryReader.java
@@ -33,22 +33,19 @@ final class StandardDirectoryReader extends DirectoryReader {
 
   private final IndexWriter writer;
   private final SegmentInfos segmentInfos;
-  private final int termInfosIndexDivisor;
   private final boolean applyAllDeletes;
   
   /** called only from static open() methods */
   StandardDirectoryReader(Directory directory, AtomicReader[] readers, IndexWriter writer,
-    SegmentInfos sis, int termInfosIndexDivisor, boolean applyAllDeletes) {
+    SegmentInfos sis, boolean applyAllDeletes) {
     super(directory, readers);
     this.writer = writer;
     this.segmentInfos = sis;
-    this.termInfosIndexDivisor = termInfosIndexDivisor;
     this.applyAllDeletes = applyAllDeletes;
   }
 
   /** called from DirectoryReader.open(...) methods */
-  static DirectoryReader open(final Directory directory, final IndexCommit commit,
-                          final int termInfosIndexDivisor) throws IOException {
+  static DirectoryReader open(final Directory directory, final IndexCommit commit) throws IOException {
     return (DirectoryReader) new SegmentInfos.FindSegmentsFile(directory) {
       @Override
       protected Object doBody(String segmentFileName) throws IOException {
@@ -59,7 +56,7 @@ final class StandardDirectoryReader extends DirectoryReader {
           IOException prior = null;
           boolean success = false;
           try {
-            readers[i] = new SegmentReader(sis.info(i), termInfosIndexDivisor, IOContext.READ);
+            readers[i] = new SegmentReader(sis.info(i), IOContext.READ);
             success = true;
           } catch(IOException ex) {
             prior = ex;
@@ -68,7 +65,7 @@ final class StandardDirectoryReader extends DirectoryReader {
               IOUtils.closeWhileHandlingException(prior, readers);
           }
         }
-        return new StandardDirectoryReader(directory, readers, null, sis, termInfosIndexDivisor, false);
+        return new StandardDirectoryReader(directory, readers, null, sis, false);
       }
     }.run(commit);
   }
@@ -119,12 +116,11 @@ final class StandardDirectoryReader extends DirectoryReader {
       }
     }
     return new StandardDirectoryReader(dir, readers.toArray(new SegmentReader[readers.size()]),
-      writer, segmentInfos, writer.getConfig().getReaderTermsIndexDivisor(), applyAllDeletes);
+      writer, segmentInfos, applyAllDeletes);
   }
 
   /** This constructor is only used for {@link #doOpenIfChanged(SegmentInfos)} */
-  private static DirectoryReader open(Directory directory, SegmentInfos infos, List<? extends AtomicReader> oldReaders,
-    int termInfosIndexDivisor) throws IOException {
+  private static DirectoryReader open(Directory directory, SegmentInfos infos, List<? extends AtomicReader> oldReaders) throws IOException {
 
     // we put the old SegmentReaders in a map, that allows us
     // to lookup a reader using its segment name
@@ -162,7 +158,7 @@ final class StandardDirectoryReader extends DirectoryReader {
         if (newReaders[i] == null || infos.info(i).info.getUseCompoundFile() != newReaders[i].getSegmentInfo().info.getUseCompoundFile()) {
 
           // this is a new reader; in case we hit an exception we can close it safely
-          newReader = new SegmentReader(infos.info(i), termInfosIndexDivisor, IOContext.READ);
+          newReader = new SegmentReader(infos.info(i), IOContext.READ);
           readerShared[i] = false;
           newReaders[i] = newReader;
         } else {
@@ -212,7 +208,7 @@ final class StandardDirectoryReader extends DirectoryReader {
         }
       }
     }    
-    return new StandardDirectoryReader(directory, newReaders, null, infos, termInfosIndexDivisor, false);
+    return new StandardDirectoryReader(directory, newReaders, null, infos, false);
   }
 
   @Override
@@ -313,7 +309,7 @@ final class StandardDirectoryReader extends DirectoryReader {
   }
 
   DirectoryReader doOpenIfChanged(SegmentInfos infos) throws IOException {
-    return StandardDirectoryReader.open(directory, infos, getSequentialSubReaders(), termInfosIndexDivisor);
+    return StandardDirectoryReader.open(directory, infos, getSequentialSubReaders());
   }
 
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/index/TermContext.java b/lucene/core/src/java/org/apache/lucene/index/TermContext.java
index ec7ef8f..ac80a94 100644
--- a/lucene/core/src/java/org/apache/lucene/index/TermContext.java
+++ b/lucene/core/src/java/org/apache/lucene/index/TermContext.java
@@ -78,7 +78,7 @@ public final class TermContext {
    * <p>
    * Note: the given context must be a top-level context.
    */
-  public static TermContext build(IndexReaderContext context, Term term, boolean cache)
+  public static TermContext build(IndexReaderContext context, Term term)
       throws IOException {
     assert context != null && context.isTopLevel;
     final String field = term.field();
@@ -92,7 +92,7 @@ public final class TermContext {
         final Terms terms = fields.terms(field);
         if (terms != null) {
           final TermsEnum termsEnum = terms.iterator(null);
-          if (termsEnum.seekExact(bytes, cache)) { 
+          if (termsEnum.seekExact(bytes)) { 
             final TermState termState = termsEnum.termState();
             //if (DEBUG) System.out.println("    found");
             perReaderTermState.register(termState, ctx.ord, termsEnum.docFreq(), termsEnum.totalTermFreq());
diff --git a/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java b/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java
index c83b327..7dc13d5 100644
--- a/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java
+++ b/lucene/core/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java
@@ -76,31 +76,31 @@ final class TermVectorsConsumerPerField extends TermsHashConsumerPerField {
             doVectorPayloads |= field.fieldType().storeTermVectorPayloads();
           } else if (field.fieldType().storeTermVectorPayloads()) {
             // TODO: move this check somewhere else, and impl the other missing ones
-            throw new IllegalArgumentException("cannot index term vector payloads for field: " + field + " without term vector positions");
+            throw new IllegalArgumentException("cannot index term vector payloads without term vector positions (field=\"" + field.name() + "\")");
           }
         } else {
           if (field.fieldType().storeTermVectorOffsets()) {
-            throw new IllegalArgumentException("cannot index term vector offsets when term vectors are not indexed (field=\"" + field.name());
+            throw new IllegalArgumentException("cannot index term vector offsets when term vectors are not indexed (field=\"" + field.name() + "\")");
           }
           if (field.fieldType().storeTermVectorPositions()) {
-            throw new IllegalArgumentException("cannot index term vector positions when term vectors are not indexed (field=\"" + field.name());
+            throw new IllegalArgumentException("cannot index term vector positions when term vectors are not indexed (field=\"" + field.name() + "\")");
           }
           if (field.fieldType().storeTermVectorPayloads()) {
-            throw new IllegalArgumentException("cannot index term vector payloads when term vectors are not indexed (field=\"" + field.name());
+            throw new IllegalArgumentException("cannot index term vector payloads when term vectors are not indexed (field=\"" + field.name() + "\")");
           }
         }
       } else {
         if (field.fieldType().storeTermVectors()) {
-          throw new IllegalArgumentException("cannot index term vectors when field is not indexed (field=\"" + field.name());
+          throw new IllegalArgumentException("cannot index term vectors when field is not indexed (field=\"" + field.name() + "\")");
         }
         if (field.fieldType().storeTermVectorOffsets()) {
-          throw new IllegalArgumentException("cannot index term vector offsets when field is not indexed (field=\"" + field.name());
+          throw new IllegalArgumentException("cannot index term vector offsets when field is not indexed (field=\"" + field.name() + "\")");
         }
         if (field.fieldType().storeTermVectorPositions()) {
-          throw new IllegalArgumentException("cannot index term vector positions when field is not indexed (field=\"" + field.name());
+          throw new IllegalArgumentException("cannot index term vector positions when field is not indexed (field=\"" + field.name() + "\")");
         }
         if (field.fieldType().storeTermVectorPayloads()) {
-          throw new IllegalArgumentException("cannot index term vector payloads when field is not indexed (field=\"" + field.name());
+          throw new IllegalArgumentException("cannot index term vector payloads when field is not indexed (field=\"" + field.name() + "\")");
         }
       }
     }
diff --git a/lucene/core/src/java/org/apache/lucene/index/TermsEnum.java b/lucene/core/src/java/org/apache/lucene/index/TermsEnum.java
index 3814bc6..b54c19e 100644
--- a/lucene/core/src/java/org/apache/lucene/index/TermsEnum.java
+++ b/lucene/core/src/java/org/apache/lucene/index/TermsEnum.java
@@ -26,7 +26,7 @@ import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.BytesRefIterator;
 
 /** Iterator to seek ({@link #seekCeil(BytesRef)}, {@link
- * #seekExact(BytesRef,boolean)}) or step through ({@link
+ * #seekExact(BytesRef)}) or step through ({@link
  * #next} terms to obtain frequency information ({@link
  * #docFreq}), {@link DocsEnum} or {@link
  * DocsAndPositionsEnum} for the current term ({@link
@@ -70,24 +70,17 @@ public abstract class TermsEnum implements BytesRefIterator {
    *  true if the term is found.  If this returns false, the
    *  enum is unpositioned.  For some codecs, seekExact may
    *  be substantially faster than {@link #seekCeil}. */
-  public boolean seekExact(BytesRef text, boolean useCache) throws IOException {
-    return seekCeil(text, useCache) == SeekStatus.FOUND;
+  public boolean seekExact(BytesRef text) throws IOException {
+    return seekCeil(text) == SeekStatus.FOUND;
   }
 
-  /** Expert: just like {@link #seekCeil(BytesRef)} but allows
-   *  you to control whether the implementation should
-   *  attempt to use its term cache (if it uses one). */
-  public abstract SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException;
-
   /** Seeks to the specified term, if it exists, or to the
    *  next (ceiling) term.  Returns SeekStatus to
    *  indicate whether exact term was found, a different
    *  term was found, or EOF was hit.  The target term may
    *  be before or after the current term.  If this returns
    *  SeekStatus.END, the enum is unpositioned. */
-  public final SeekStatus seekCeil(BytesRef text) throws IOException {
-    return seekCeil(text, true);
-  }
+  public abstract SeekStatus seekCeil(BytesRef text) throws IOException;
 
   /** Seeks to the specified term by ordinal (position) as
    *  previously returned by {@link #ord}.  The target ord
@@ -117,7 +110,7 @@ public abstract class TermsEnum implements BytesRefIterator {
    * @param state the {@link TermState}
    * */
   public void seekExact(BytesRef term, TermState state) throws IOException {
-    if (!seekExact(term, true)) {
+    if (!seekExact(term)) {
       throw new IllegalArgumentException("term=" + term + " does not exist");
     }
   }
@@ -226,7 +219,7 @@ public abstract class TermsEnum implements BytesRefIterator {
    */
   public static final TermsEnum EMPTY = new TermsEnum() {    
     @Override
-    public SeekStatus seekCeil(BytesRef term, boolean useCache) { return SeekStatus.END; }
+    public SeekStatus seekCeil(BytesRef term) { return SeekStatus.END; }
     
     @Override
     public void seekExact(long ord) {}
diff --git a/lucene/core/src/java/org/apache/lucene/search/FuzzyTermsEnum.java b/lucene/core/src/java/org/apache/lucene/search/FuzzyTermsEnum.java
index 6d58749..85698fe 100644
--- a/lucene/core/src/java/org/apache/lucene/search/FuzzyTermsEnum.java
+++ b/lucene/core/src/java/org/apache/lucene/search/FuzzyTermsEnum.java
@@ -303,13 +303,13 @@ public class FuzzyTermsEnum extends TermsEnum {
   }
   
   @Override
-  public boolean seekExact(BytesRef text, boolean useCache) throws IOException {
-    return actualEnum.seekExact(text, useCache);
+  public boolean seekExact(BytesRef text) throws IOException {
+    return actualEnum.seekExact(text);
   }
 
   @Override
-  public SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException {
-    return actualEnum.seekCeil(text, useCache);
+  public SeekStatus seekCeil(BytesRef text) throws IOException {
+    return actualEnum.seekCeil(text);
   }
   
   @Override
diff --git a/lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java b/lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java
index b730e8f..4a2fa0a 100644
--- a/lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java
+++ b/lucene/core/src/java/org/apache/lucene/search/IndexSearcher.java
@@ -430,6 +430,10 @@ public class IndexSearcher {
     if (limit == 0) {
       limit = 1;
     }
+    if (after != null && after.doc >= limit) {
+      throw new IllegalArgumentException("after.doc exceeds the number of documents in that reader: after.doc="
+          + after.doc + " limit=" + limit);
+    }
     nDocs = Math.min(nDocs, limit);
     
     if (executor == null) {
@@ -440,8 +444,7 @@ public class IndexSearcher {
       final ExecutionHelper<TopDocs> runner = new ExecutionHelper<TopDocs>(executor);
     
       for (int i = 0; i < leafSlices.length; i++) { // search each sub
-        runner.submit(
-                      new SearcherCallableNoSort(lock, this, leafSlices[i], weight, after, nDocs, hq));
+        runner.submit(new SearcherCallableNoSort(lock, this, leafSlices[i], weight, after, nDocs, hq));
       }
 
       int totalHits = 0;
@@ -920,7 +923,7 @@ public class IndexSearcher {
    */
   public TermStatistics termStatistics(Term term, TermContext context) throws IOException {
     return new TermStatistics(term.bytes(), context.docFreq(), context.totalTermFreq());
-  };
+  }
   
   /**
    * Returns {@link CollectionStatistics} for a field.
diff --git a/lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java b/lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
index 92837ec..c46ec48 100644
--- a/lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/MultiPhraseQuery.java
@@ -149,7 +149,7 @@ public class MultiPhraseQuery extends Query {
         for (Term term: terms) {
           TermContext termContext = termContexts.get(term);
           if (termContext == null) {
-            termContext = TermContext.build(context, term, true);
+            termContext = TermContext.build(context, term);
             termContexts.put(term, termContext);
           }
           allTermStats.add(searcher.termStatistics(term, termContext));
diff --git a/lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java b/lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
index b48a1dc..6a32d51 100644
--- a/lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/PhraseQuery.java
@@ -37,7 +37,6 @@ import org.apache.lucene.search.similarities.Similarity.SimScorer;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.ToStringUtils;
 
 /** A Query that matches documents containing a particular sequence of terms.
@@ -218,7 +217,7 @@ public class PhraseQuery extends Query {
       TermStatistics termStats[] = new TermStatistics[terms.size()];
       for (int i = 0; i < terms.size(); i++) {
         final Term term = terms.get(i);
-        states[i] = TermContext.build(context, term, true);
+        states[i] = TermContext.build(context, term);
         termStats[i] = searcher.termStatistics(term, states[i]);
       }
       stats = similarity.computeWeight(getBoost(), searcher.collectionStatistics(field), termStats);
@@ -269,7 +268,7 @@ public class PhraseQuery extends Query {
         // PhraseQuery on a field that did not index
         // positions.
         if (postingsEnum == null) {
-          assert te.seekExact(t.bytes(), false) : "termstate found but no term exists in reader";
+          assert te.seekExact(t.bytes()) : "termstate found but no term exists in reader";
           // term does exist, but has no positions
           throw new IllegalStateException("field \"" + t.field() + "\" was indexed without position data; cannot run PhraseQuery (term=" + t.text() + ")");
         }
diff --git a/lucene/core/src/java/org/apache/lucene/search/PrefixFilter.java b/lucene/core/src/java/org/apache/lucene/search/PrefixFilter.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/search/ReferenceManager.java b/lucene/core/src/java/org/apache/lucene/search/ReferenceManager.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/search/TermQuery.java b/lucene/core/src/java/org/apache/lucene/search/TermQuery.java
index 099e90b..b6a1f23 100644
--- a/lucene/core/src/java/org/apache/lucene/search/TermQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/TermQuery.java
@@ -163,7 +163,7 @@ public class TermQuery extends Query {
     final TermContext termState;
     if (perReaderTermState == null || perReaderTermState.topReaderContext != context) {
       // make TermQuery single-pass if we don't have a PRTS or if the context differs!
-      termState = TermContext.build(context, term, true); // cache term lookups!
+      termState = TermContext.build(context, term);
     } else {
      // PRTS was pre-build for this IS
      termState = this.perReaderTermState;
diff --git a/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java b/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
index 3ae6ffd..a307598 100644
--- a/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
+++ b/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadSpanUtil.java
@@ -182,7 +182,7 @@ public class PayloadSpanUtil {
     TreeSet<Term> terms = new TreeSet<Term>();
     query.extractTerms(terms);
     for (Term term : terms) {
-      termContexts.put(term, TermContext.build(context, term, true));
+      termContexts.put(term, TermContext.build(context, term));
     }
     for (AtomicReaderContext atomicReaderContext : context.leaves()) {
       final Spans spans = query.getSpans(atomicReaderContext, atomicReaderContext.reader().getLiveDocs(), termContexts);
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java b/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
index 1ee44a5..95451f4 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/NearSpansOrdered.java
@@ -22,6 +22,7 @@ import org.apache.lucene.index.Term;
 import org.apache.lucene.index.TermContext;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.InPlaceMergeSorter;
 
 import java.io.IOException;
 import java.util.ArrayList;
@@ -72,13 +73,19 @@ public class NearSpansOrdered extends Spans {
   private List<byte[]> matchPayload;
 
   private final Spans[] subSpansByDoc;
-  private final Comparator<Spans> spanDocComparator = new Comparator<Spans>() {
+  // Even though the array is probably almost sorted, InPlaceMergeSorter will likely
+  // perform better since it has a lower overhead than TimSorter for small arrays
+  private final InPlaceMergeSorter sorter = new InPlaceMergeSorter() {
     @Override
-    public int compare(Spans o1, Spans o2) {
-      return o1.doc() - o2.doc();
+    protected void swap(int i, int j) {
+      ArrayUtil.swap(subSpansByDoc, i, j);
+    }
+    @Override
+    protected int compare(int i, int j) {
+      return subSpansByDoc[i].doc() - subSpansByDoc[j].doc();
     }
   };
-  
+
   private SpanNearQuery query;
   private boolean collectPayloads = true;
   
@@ -204,7 +211,7 @@ public class NearSpansOrdered extends Spans {
 
   /** Advance the subSpans to the same document */
   private boolean toSameDoc() throws IOException {
-    ArrayUtil.timSort(subSpansByDoc, spanDocComparator);
+    sorter.sort(0, subSpansByDoc.length);
     int firstIndex = 0;
     int maxDoc = subSpansByDoc[subSpansByDoc.length - 1].doc();
     while (subSpansByDoc[firstIndex].doc() != maxDoc) {
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java b/lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java
index 9bba5db..7a7fcf9 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/SpanNotQuery.java
@@ -31,16 +31,36 @@ import java.util.Collection;
 import java.util.Map;
 import java.util.Set;
 
-/** Removes matches which overlap with another SpanQuery. */
+/** Removes matches which overlap with another SpanQuery or 
+ * within a x tokens before or y tokens after another SpanQuery. */
 public class SpanNotQuery extends SpanQuery implements Cloneable {
   private SpanQuery include;
   private SpanQuery exclude;
+  private final int pre;
+  private final int post;
 
   /** Construct a SpanNotQuery matching spans from <code>include</code> which
    * have no overlap with spans from <code>exclude</code>.*/
   public SpanNotQuery(SpanQuery include, SpanQuery exclude) {
+     this(include, exclude, 0, 0);
+  }
+
+  
+  /** Construct a SpanNotQuery matching spans from <code>include</code> which
+   * have no overlap with spans from <code>exclude</code> within 
+   * <code>dist</code> tokens of <code>include</code>. */
+  public SpanNotQuery(SpanQuery include, SpanQuery exclude, int dist) {
+     this(include, exclude, dist, dist);
+  }
+  
+  /** Construct a SpanNotQuery matching spans from <code>include</code> which
+   * have no overlap with spans from <code>exclude</code> within 
+   * <code>pre</code> tokens before or <code>post</code> tokens of <code>include</code>. */
+  public SpanNotQuery(SpanQuery include, SpanQuery exclude, int pre, int post) {
     this.include = include;
     this.exclude = exclude;
+    this.pre = (pre >=0) ? pre : 0;
+    this.post = (post >= 0) ? post : 0;
 
     if (!include.getField().equals(exclude.getField()))
       throw new IllegalArgumentException("Clauses must have same field.");
@@ -65,6 +85,10 @@ public class SpanNotQuery extends SpanQuery implements Cloneable {
     buffer.append(include.toString(field));
     buffer.append(", ");
     buffer.append(exclude.toString(field));
+    buffer.append(", ");
+    buffer.append(Integer.toString(pre));
+    buffer.append(", ");
+    buffer.append(Integer.toString(post));
     buffer.append(")");
     buffer.append(ToStringUtils.boost(getBoost()));
     return buffer.toString();
@@ -72,7 +96,8 @@ public class SpanNotQuery extends SpanQuery implements Cloneable {
 
   @Override
   public SpanNotQuery clone() {
-    SpanNotQuery spanNotQuery = new SpanNotQuery((SpanQuery)include.clone(),(SpanQuery) exclude.clone());
+    SpanNotQuery spanNotQuery = new SpanNotQuery((SpanQuery)include.clone(),
+          (SpanQuery) exclude.clone(), pre, post);
     spanNotQuery.setBoost(getBoost());
     return  spanNotQuery;
   }
@@ -98,13 +123,13 @@ public class SpanNotQuery extends SpanQuery implements Cloneable {
 
             while (moreExclude                    // while exclude is before
                    && includeSpans.doc() == excludeSpans.doc()
-                   && excludeSpans.end() <= includeSpans.start()) {
+                   && excludeSpans.end() <= includeSpans.start() - pre) {
               moreExclude = excludeSpans.next();  // increment exclude
             }
 
             if (!moreExclude                      // if no intersection
                 || includeSpans.doc() != excludeSpans.doc()
-                || includeSpans.end() <= excludeSpans.start())
+                || includeSpans.end()+post <= excludeSpans.start())
               break;                              // we found a match
 
             moreInclude = includeSpans.next();    // intersected: keep scanning
@@ -126,13 +151,13 @@ public class SpanNotQuery extends SpanQuery implements Cloneable {
 
           while (moreExclude                      // while exclude is before
                  && includeSpans.doc() == excludeSpans.doc()
-                 && excludeSpans.end() <= includeSpans.start()) {
+                 && excludeSpans.end() <= includeSpans.start()-pre) {
             moreExclude = excludeSpans.next();    // increment exclude
           }
 
           if (!moreExclude                      // if no intersection
                 || includeSpans.doc() != excludeSpans.doc()
-                || includeSpans.end() <= excludeSpans.start())
+                || includeSpans.end()+post <= excludeSpans.start())
             return true;                          // we found a match
 
           return next();                          // scan to next match
@@ -199,23 +224,28 @@ public class SpanNotQuery extends SpanQuery implements Cloneable {
     /** Returns true iff <code>o</code> is equal to this. */
   @Override
   public boolean equals(Object o) {
-    if (this == o) return true;
-    if (!(o instanceof SpanNotQuery)) return false;
+    if (!super.equals(o))
+      return false;
 
     SpanNotQuery other = (SpanNotQuery)o;
     return this.include.equals(other.include)
             && this.exclude.equals(other.exclude)
-            && this.getBoost() == other.getBoost();
+            && this.pre == other.pre 
+            && this.post == other.post;
   }
 
   @Override
   public int hashCode() {
-    int h = include.hashCode();
-    h = (h<<1) | (h >>> 31);  // rotate left
+    int h = super.hashCode();
+    h = Integer.rotateLeft(h, 1);
+    h ^= include.hashCode();
+    h = Integer.rotateLeft(h, 1);
     h ^= exclude.hashCode();
-    h = (h<<1) | (h >>> 31);  // rotate left
-    h ^= Float.floatToRawIntBits(getBoost());
+    h = Integer.rotateLeft(h, 1);
+    h ^= pre;
+    h = Integer.rotateLeft(h, 1);
+    h ^= post;
     return h;
   }
 
-}
+}
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java b/lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java
index 7fcbfa0..f0a27c4 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java
@@ -98,7 +98,7 @@ public class SpanTermQuery extends SpanQuery {
         final Terms terms = fields.terms(term.field());
         if (terms != null) {
           final TermsEnum termsEnum = terms.iterator(null);
-          if (termsEnum.seekExact(term.bytes(), true)) { 
+          if (termsEnum.seekExact(term.bytes())) { 
             state = termsEnum.termState();
           } else {
             state = null;
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java b/lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java
index 8e428f1..fb73721 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/SpanWeight.java
@@ -52,7 +52,7 @@ public class SpanWeight extends Weight {
     final TermStatistics termStats[] = new TermStatistics[terms.size()];
     int i = 0;
     for (Term term : terms) {
-      TermContext state = TermContext.build(context, term, true);
+      TermContext state = TermContext.build(context, term);
       termStats[i] = searcher.termStatistics(term, state);
       termContexts.put(term, state);
       i++;
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/package.html b/lucene/core/src/java/org/apache/lucene/search/spans/package.html
index 3e43d3b..054336e 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/package.html
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/package.html
@@ -38,8 +38,8 @@ and inter-phrase proximity (when constructed from other {@link org.apache.lucene
 number of other {@link org.apache.lucene.search.spans.SpanQuery}s.</li>
 
 <li>A {@link org.apache.lucene.search.spans.SpanNotQuery SpanNotQuery} removes spans
-matching one {@link org.apache.lucene.search.spans.SpanQuery SpanQuery} which overlap
-another.  This can be used, e.g., to implement within-paragraph
+matching one {@link org.apache.lucene.search.spans.SpanQuery SpanQuery} which overlap (or comes
+near) another.  This can be used, e.g., to implement within-paragraph
 search.</li>
 
 <li>A {@link org.apache.lucene.search.spans.SpanFirstQuery SpanFirstQuery} matches spans
diff --git a/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java b/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java
index 56c88ce..063388f 100644
--- a/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java
+++ b/lucene/core/src/java/org/apache/lucene/store/BufferedIndexInput.java
@@ -23,7 +23,7 @@ import java.io.IOException;
 /** Base implementation class for buffered {@link IndexInput}. */
 public abstract class BufferedIndexInput extends IndexInput {
 
-  /** Default buffer size set to 1024*/
+  /** Default buffer size set to {@value #BUFFER_SIZE}. */
   public static final int BUFFER_SIZE = 1024;
   
   // The normal read buffer size defaults to 1024, but
@@ -33,7 +33,7 @@ public abstract class BufferedIndexInput extends IndexInput {
   // BufferedIndexInputs created during merging.  See
   // LUCENE-888 for details.
   /**
-   * A buffer size for merges set to 4096
+   * A buffer size for merges set to {@value #MERGE_BUFFER_SIZE}.
    */
   public static final int MERGE_BUFFER_SIZE = 4096;
 
@@ -115,15 +115,14 @@ public abstract class BufferedIndexInput extends IndexInput {
 
   @Override
   public final void readBytes(byte[] b, int offset, int len, boolean useBuffer) throws IOException {
-
-    if(len <= (bufferLength-bufferPosition)){
+    int available = bufferLength - bufferPosition;
+    if(len <= available){
       // the buffer contains enough data to satisfy this request
       if(len>0) // to allow b to be null if len is 0...
         System.arraycopy(buffer, bufferPosition, b, offset, len);
       bufferPosition+=len;
     } else {
       // the buffer does not have enough data. First serve all we've got.
-      int available = bufferLength - bufferPosition;
       if(available > 0){
         System.arraycopy(buffer, bufferPosition, b, offset, available);
         offset += available;
diff --git a/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java b/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
index 1c739bd..1dea78e 100644
--- a/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
+++ b/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
@@ -31,6 +31,7 @@ import java.util.concurrent.Future;
 
 import org.apache.lucene.util.ThreadInterruptedException;
 import org.apache.lucene.util.Constants;
+import org.apache.lucene.util.IOUtils;
 
 /**
  * Base class for Directory implementations that store index
@@ -111,17 +112,8 @@ import org.apache.lucene.util.Constants;
  */
 public abstract class FSDirectory extends Directory {
 
-  /**
-   * Default read chunk size.  This is a conditional default: on 32bit JVMs, it defaults to 100 MB.  On 64bit JVMs, it's
-   * <code>Integer.MAX_VALUE</code>.
-   *
-   * @see #setReadChunkSize
-   */
-  public static final int DEFAULT_READ_CHUNK_SIZE = Constants.JRE_IS_64BIT ? Integer.MAX_VALUE : 100 * 1024 * 1024;
-
   protected final File directory; // The underlying filesystem directory
   protected final Set<String> staleFiles = synchronizedSet(new HashSet<String>()); // Files written, but not yet sync'ed
-  private int chunkSize = DEFAULT_READ_CHUNK_SIZE; // LUCENE-1566
 
   // returns the canonical version of the directory, creating it if it doesn't exist.
   private static File getCanonicalPath(File file) throws IOException {
@@ -355,68 +347,38 @@ public abstract class FSDirectory extends Directory {
   }
 
   /**
-   * Sets the maximum number of bytes read at once from the
-   * underlying file during {@link IndexInput#readBytes}.
-   * The default value is {@link #DEFAULT_READ_CHUNK_SIZE};
-   *
-   * <p> This was introduced due to <a
-   * href="http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6478546">Sun
-   * JVM Bug 6478546</a>, which throws an incorrect
-   * OutOfMemoryError when attempting to read too many bytes
-   * at once.  It only happens on 32bit JVMs with a large
-   * maximum heap size.</p>
-   *
-   * <p>Changes to this value will not impact any
-   * already-opened {@link IndexInput}s.  You should call
-   * this before attempting to open an index on the
-   * directory.</p>
-   *
-   * <p> <b>NOTE</b>: This value should be as large as
-   * possible to reduce any possible performance impact.  If
-   * you still encounter an incorrect OutOfMemoryError,
-   * trying lowering the chunk size.</p>
-   */
-  public final void setReadChunkSize(int chunkSize) {
-    // LUCENE-1566
-    if (chunkSize <= 0) {
-      throw new IllegalArgumentException("chunkSize must be positive");
-    }
-    if (!Constants.JRE_IS_64BIT) {
-      this.chunkSize = chunkSize;
-    }
-  }
-
-  /**
-   * The maximum number of bytes to read at once from the
-   * underlying file during {@link IndexInput#readBytes}.
-   * @see #setReadChunkSize
-   */
-  public final int getReadChunkSize() {
-    // LUCENE-1566
-    return chunkSize;
-  }
-
-  /**
    * Writes output with {@link RandomAccessFile#write(byte[], int, int)}
    */
   protected static class FSIndexOutput extends BufferedIndexOutput {
+    /**
+     * The maximum chunk size is 8192 bytes, because {@link RandomAccessFile} mallocs
+     * a native buffer outside of stack if the write buffer size is larger.
+     */
+    private static final int CHUNK_SIZE = 8192;
+    
     private final FSDirectory parent;
     private final String name;
     private final RandomAccessFile file;
     private volatile boolean isOpen; // remember if the file is open, so that we don't try to close it more than once
     
     public FSIndexOutput(FSDirectory parent, String name) throws IOException {
+      super(CHUNK_SIZE);
       this.parent = parent;
       this.name = name;
       file = new RandomAccessFile(new File(parent.directory, name), "rw");
       isOpen = true;
     }
 
-    /** output methods: */
     @Override
-    public void flushBuffer(byte[] b, int offset, int size) throws IOException {
+    protected void flushBuffer(byte[] b, int offset, int size) throws IOException {
       assert isOpen;
-      file.write(b, offset, size);
+      while (size > 0) {
+        final int toWrite = Math.min(CHUNK_SIZE, size);
+        file.write(b, offset, toWrite);
+        offset += toWrite;
+        size -= toWrite;
+      }
+      assert size == 0;
     }
     
     @Override
@@ -424,21 +386,14 @@ public abstract class FSDirectory extends Directory {
       parent.onIndexOutputClosed(name);
       // only close the file if it has not been closed yet
       if (isOpen) {
-        boolean success = false;
+        IOException priorE = null;
         try {
           super.close();
-          success = true;
+        } catch (IOException ioe) {
+          priorE = ioe;
         } finally {
           isOpen = false;
-          if (!success) {
-            try {
-              file.close();
-            } catch (Throwable t) {
-              // Suppress so we don't mask original exception
-            }
-          } else {
-            file.close();
-          }
+          IOUtils.closeWhileHandlingException(priorE, file);
         }
       }
     }
diff --git a/lucene/core/src/java/org/apache/lucene/store/LockFactory.java b/lucene/core/src/java/org/apache/lucene/store/LockFactory.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.java b/lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.java
index ad5b129..1fa7693 100644
--- a/lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.java
+++ b/lucene/core/src/java/org/apache/lucene/store/NIOFSDirectory.java
@@ -20,6 +20,7 @@ package org.apache.lucene.store;
 import java.io.File;
 import java.io.EOFException;
 import java.io.IOException;
+import java.io.RandomAccessFile;
 import java.nio.ByteBuffer;
 import java.nio.channels.ClosedChannelException; // javadoc @link
 import java.nio.channels.FileChannel;
@@ -79,7 +80,7 @@ public class NIOFSDirectory extends FSDirectory {
     ensureOpen();
     File path = new File(getDirectory(), name);
     FileChannel fc = FileChannel.open(path.toPath(), StandardOpenOption.READ);
-    return new NIOFSIndexInput("NIOFSIndexInput(path=\"" + path + "\")", fc, context, getReadChunkSize());
+    return new NIOFSIndexInput("NIOFSIndexInput(path=\"" + path + "\")", fc, context);
   }
   
   @Override
@@ -98,7 +99,7 @@ public class NIOFSDirectory extends FSDirectory {
       @Override
       public IndexInput openSlice(String sliceDescription, long offset, long length) {
         return new NIOFSIndexInput("NIOFSIndexInput(" + sliceDescription + " in path=\"" + path + "\" slice=" + offset + ":" + (offset+length) + ")", descriptor, offset,
-            length, BufferedIndexInput.bufferSize(context), getReadChunkSize());
+            length, BufferedIndexInput.bufferSize(context));
       }
     };
   }
@@ -107,12 +108,15 @@ public class NIOFSDirectory extends FSDirectory {
    * Reads bytes with {@link FileChannel#read(ByteBuffer, long)}
    */
   protected static class NIOFSIndexInput extends BufferedIndexInput {
+    /**
+     * The maximum chunk size for reads of 16384 bytes.
+     */
+    private static final int CHUNK_SIZE = 16384;
+    
     /** the file channel we will read from */
     protected final FileChannel channel;
     /** is this instance a clone and hence does not own the file to close it */
     boolean isClone = false;
-    /** maximum read length on a 32bit JVM to prevent incorrect OOM, see LUCENE-1566 */ 
-    protected final int chunkSize;
     /** start offset: non-zero in the slice case */
     protected final long off;
     /** end offset (start+length) */
@@ -120,18 +124,16 @@ public class NIOFSDirectory extends FSDirectory {
     
     private ByteBuffer byteBuf; // wraps the buffer for NIO
 
-    public NIOFSIndexInput(String resourceDesc, FileChannel fc, IOContext context, int chunkSize) throws IOException {
+    public NIOFSIndexInput(String resourceDesc, FileChannel fc, IOContext context) throws IOException {
       super(resourceDesc, context);
       this.channel = fc; 
-      this.chunkSize = chunkSize;
       this.off = 0L;
       this.end = fc.size();
     }
     
-    public NIOFSIndexInput(String resourceDesc, FileChannel fc, long off, long length, int bufferSize, int chunkSize) {
+    public NIOFSIndexInput(String resourceDesc, FileChannel fc, long off, long length, int bufferSize) {
       super(resourceDesc, bufferSize);
       this.channel = fc;
-      this.chunkSize = chunkSize;
       this.off = off;
       this.end = off + length;
       this.isClone = true;
@@ -164,24 +166,18 @@ public class NIOFSDirectory extends FSDirectory {
 
     @Override
     protected void readInternal(byte[] b, int offset, int len) throws IOException {
-
       final ByteBuffer bb;
 
       // Determine the ByteBuffer we should use
-      if (b == buffer && 0 == offset) {
+      if (b == buffer) {
         // Use our own pre-wrapped byteBuf:
         assert byteBuf != null;
-        byteBuf.clear();
-        byteBuf.limit(len);
         bb = byteBuf;
+        byteBuf.clear().position(offset);
       } else {
         bb = ByteBuffer.wrap(b, offset, len);
       }
 
-      int readOffset = bb.position();
-      int readLength = bb.limit() - readOffset;
-      assert readLength == len;
-
       long pos = getFilePointer() + off;
       
       if (pos + len > end) {
@@ -189,30 +185,20 @@ public class NIOFSDirectory extends FSDirectory {
       }
 
       try {
+        int readLength = len;
         while (readLength > 0) {
-          final int limit;
-          if (readLength > chunkSize) {
-            // LUCENE-1566 - work around JVM Bug by breaking
-            // very large reads into chunks
-            limit = readOffset + chunkSize;
-          } else {
-            limit = readOffset + readLength;
+          final int toRead = Math.min(CHUNK_SIZE, readLength);
+          bb.limit(bb.position() + toRead);
+          assert bb.remaining() == toRead;
+          final int i = channel.read(bb, pos);
+          if (i < 0) { // be defensive here, even though we checked before hand, something could have changed
+            throw new EOFException("read past EOF: " + this + " off: " + offset + " len: " + len + " pos: " + pos + " chunkLen: " + toRead + " end: " + end);
           }
-          bb.limit(limit);
-          int i = channel.read(bb, pos);
+          assert i > 0 : "FileChannel.read with non zero-length bb.remaining() must always read at least one byte (FileChannel is in blocking mode, see spec of ReadableByteChannel)";
           pos += i;
-          readOffset += i;
           readLength -= i;
         }
-      } catch (OutOfMemoryError e) {
-        // propagate OOM up and add a hint for 32bit VM Users hitting the bug
-        // with a large chunk size in the fast path.
-        final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(
-              "OutOfMemoryError likely caused by the Sun VM Bug described in "
-              + "https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize "
-              + "with a value smaller than the current chunk size (" + chunkSize + ")");
-        outOfMemoryError.initCause(e);
-        throw outOfMemoryError;
+        assert readLength == 0;
       } catch (IOException ioe) {
         throw new IOException(ioe.getMessage() + ": " + this, ioe);
       }
diff --git a/lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java b/lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/store/NoLockFactory.java b/lucene/core/src/java/org/apache/lucene/store/NoLockFactory.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.java b/lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.java
index 786e077..d05c1fe 100644
--- a/lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.java
+++ b/lucene/core/src/java/org/apache/lucene/store/SimpleFSDirectory.java
@@ -56,7 +56,7 @@ public class SimpleFSDirectory extends FSDirectory {
     ensureOpen();
     final File path = new File(directory, name);
     RandomAccessFile raf = new RandomAccessFile(path, "r");
-    return new SimpleFSIndexInput("SimpleFSIndexInput(path=\"" + path.getPath() + "\")", raf, context, getReadChunkSize());
+    return new SimpleFSIndexInput("SimpleFSIndexInput(path=\"" + path.getPath() + "\")", raf, context);
   }
 
   @Override
@@ -75,7 +75,7 @@ public class SimpleFSDirectory extends FSDirectory {
       @Override
       public IndexInput openSlice(String sliceDescription, long offset, long length) {
         return new SimpleFSIndexInput("SimpleFSIndexInput(" + sliceDescription + " in path=\"" + file.getPath() + "\" slice=" + offset + ":" + (offset+length) + ")", descriptor, offset,
-            length, BufferedIndexInput.bufferSize(context), getReadChunkSize());
+            length, BufferedIndexInput.bufferSize(context));
       }
     };
   }
@@ -85,29 +85,31 @@ public class SimpleFSDirectory extends FSDirectory {
    * {@link RandomAccessFile#read(byte[], int, int)}.  
    */
   protected static class SimpleFSIndexInput extends BufferedIndexInput {
+    /**
+     * The maximum chunk size is 8192 bytes, because {@link RandomAccessFile} mallocs
+     * a native buffer outside of stack if the read buffer size is larger.
+     */
+    private static final int CHUNK_SIZE = 8192;
+    
     /** the file channel we will read from */
     protected final RandomAccessFile file;
     /** is this instance a clone and hence does not own the file to close it */
     boolean isClone = false;
-    /** maximum read length on a 32bit JVM to prevent incorrect OOM, see LUCENE-1566 */ 
-    protected final int chunkSize;
     /** start offset: non-zero in the slice case */
     protected final long off;
     /** end offset (start+length) */
     protected final long end;
     
-    public SimpleFSIndexInput(String resourceDesc, RandomAccessFile file, IOContext context, int chunkSize) throws IOException {
+    public SimpleFSIndexInput(String resourceDesc, RandomAccessFile file, IOContext context) throws IOException {
       super(resourceDesc, context);
       this.file = file; 
-      this.chunkSize = chunkSize;
       this.off = 0L;
       this.end = file.length();
     }
     
-    public SimpleFSIndexInput(String resourceDesc, RandomAccessFile file, long off, long length, int bufferSize, int chunkSize) {
+    public SimpleFSIndexInput(String resourceDesc, RandomAccessFile file, long off, long length, int bufferSize) {
       super(resourceDesc, bufferSize);
       this.file = file;
-      this.chunkSize = chunkSize;
       this.off = off;
       this.end = off + length;
       this.isClone = true;
@@ -146,26 +148,16 @@ public class SimpleFSDirectory extends FSDirectory {
         }
 
         try {
-          do {
-            final int readLength;
-            if (total + chunkSize > len) {
-              readLength = len - total;
-            } else {
-              // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks
-              readLength = chunkSize;
+          while (total < len) {
+            final int toRead = Math.min(CHUNK_SIZE, len - total);
+            final int i = file.read(b, offset + total, toRead);
+            if (i < 0) { // be defensive here, even though we checked before hand, something could have changed
+             throw new EOFException("read past EOF: " + this + " off: " + offset + " len: " + len + " total: " + total + " chunkLen: " + toRead + " end: " + end);
             }
-            final int i = file.read(b, offset + total, readLength);
+            assert i > 0 : "RandomAccessFile.read with non zero-length toRead must always read at least one byte";
             total += i;
-          } while (total < len);
-        } catch (OutOfMemoryError e) {
-          // propagate OOM up and add a hint for 32bit VM Users hitting the bug
-          // with a large chunk size in the fast path.
-          final OutOfMemoryError outOfMemoryError = new OutOfMemoryError(
-              "OutOfMemoryError likely caused by the Sun VM Bug described in "
-              + "https://issues.apache.org/jira/browse/LUCENE-1566; try calling FSDirectory.setReadChunkSize "
-              + "with a value smaller than the current chunk size (" + chunkSize + ")");
-          outOfMemoryError.initCause(e);
-          throw outOfMemoryError;
+          }
+          assert total == len;
         } catch (IOException ioe) {
           throw new IOException(ioe.getMessage() + ": " + this, ioe);
         }
diff --git a/lucene/core/src/java/org/apache/lucene/store/SimpleFSLockFactory.java b/lucene/core/src/java/org/apache/lucene/store/SimpleFSLockFactory.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/store/SingleInstanceLockFactory.java b/lucene/core/src/java/org/apache/lucene/store/SingleInstanceLockFactory.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/util/GrowableByteArrayDataOutput.java b/lucene/core/src/java/org/apache/lucene/util/GrowableByteArrayDataOutput.java
index 18583ba..3a3b702 100644
--- a/lucene/core/src/java/org/apache/lucene/util/GrowableByteArrayDataOutput.java
+++ b/lucene/core/src/java/org/apache/lucene/util/GrowableByteArrayDataOutput.java
@@ -47,9 +47,7 @@ public final class GrowableByteArrayDataOutput extends DataOutput {
   @Override
   public void writeBytes(byte[] b, int off, int len) {
     final int newLength = length + len;
-    if (newLength > bytes.length) {
-      bytes = ArrayUtil.grow(bytes, newLength);
-    }
+    bytes = ArrayUtil.grow(bytes, newLength);
     System.arraycopy(b, off, bytes, length, len);
     length = newLength;
   }
diff --git a/lucene/core/src/java/org/apache/lucene/util/SetOnce.java b/lucene/core/src/java/org/apache/lucene/util/SetOnce.java
index fd8b26a..74c2fa8 100644
--- a/lucene/core/src/java/org/apache/lucene/util/SetOnce.java
+++ b/lucene/core/src/java/org/apache/lucene/util/SetOnce.java
@@ -28,7 +28,7 @@ import java.util.concurrent.atomic.AtomicBoolean;
  *
  * @lucene.experimental
  */
-public final class SetOnce<T> {
+public final class SetOnce<T> implements Cloneable {
 
   /** Thrown when {@link SetOnce#set(Object)} is called more than once. */
   public static final class AlreadySetException extends IllegalStateException {
@@ -74,4 +74,10 @@ public final class SetOnce<T> {
   public final T get() {
     return obj;
   }
+  
+  @Override
+  public SetOnce<T> clone() {
+    return obj == null ? new SetOnce<T>() : new SetOnce<T>(obj);
+  }
+  
 }
diff --git a/lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.java b/lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.java
index be6baf1..c1db53f 100644
--- a/lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.java
+++ b/lucene/core/src/java/org/apache/lucene/util/WAH8DocIdSet.java
@@ -27,6 +27,7 @@ import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.ByteArrayDataInput;
 import org.apache.lucene.store.DataInput;
 import org.apache.lucene.util.packed.MonotonicAppendingLongBuffer;
+import org.apache.lucene.util.packed.PackedInts;
 
 /**
  * {@link DocIdSet} implementation based on word-aligned hybrid encoding on
@@ -34,31 +35,33 @@ import org.apache.lucene.util.packed.MonotonicAppendingLongBuffer;
  * <p>This implementation doesn't support random-access but has a fast
  * {@link DocIdSetIterator} which can advance in logarithmic time thanks to
  * an index.</p>
- * <p>The compression scheme is simplistic and should work well with sparse doc
- * id sets while being only slightly larger than a {@link FixedBitSet} for
- * incompressible sets (overhead&lt;2% in the worst case) in spite of the index.</p>
+ * <p>The compression scheme is simplistic and should work well with sparse and
+ * very dense doc id sets while being only slightly larger than a
+ * {@link FixedBitSet} for incompressible sets (overhead&lt;2% in the worst
+ * case) in spite of the index.</p>
  * <p><b>Format</b>: The format is byte-aligned. An 8-bits word is either clean,
- * meaning composed only of zeros, or dirty, meaning that it contains at least one
- * bit set. The idea is to encode sequences of clean words using run-length
- * encoding and to leave sequences of dirty words as-is.</p>
+ * meaning composed only of zeros or ones, or dirty, meaning that it contains
+ * between 1 and 7 bits set. The idea is to encode sequences of clean words
+ * using run-length encoding and to leave sequences of dirty words as-is.</p>
  * <table>
  *   <tr><th>Token</th><th>Clean length+</th><th>Dirty length+</th><th>Dirty words</th></tr>
  *   <tr><td>1 byte</td><td>0-n bytes</td><td>0-n bytes</td><td>0-n bytes</td></tr>
  * </table>
  * <ul>
- *   <li><b>Token</b> encodes the number of clean words minus 2 on the first 4
- * bits and the number of dirty words minus 1 on the last 4 bits. The
- * higher-order bit is a continuation bit, meaning that the number is incomplete
- * and needs additional bytes to be read.</li>
+ *   <li><b>Token</b> encodes whether clean means full of zeros or ones in the
+ * first bit, the number of clean words minus 2 on the next 3 bits and the
+ * number of dirty words on the last 4 bits. The higher-order bit is a
+ * continuation bit, meaning that the number is incomplete and needs additional
+ * bytes to be read.</li>
  *   <li><b>Clean length+</b>: If clean length has its higher-order bit set,
  * you need to read a {@link DataInput#readVInt() vint}, shift it by 3 bits on
  * the left side and add it to the 3 bits which have been read in the token.</li>
  *   <li><b>Dirty length+</b> works the same way as <b>Clean length+</b> but
- * for the length of dirty words.</li>
+ * on 4 bits and for the length of dirty words.</li>
  *   <li><b>Dirty words</b> are the dirty words, there are <b>Dirty length</b>
  * of them.</li>
  * </ul>
- * <p>This format cannot encode sequences of less than 2 clean words and 1 dirty
+ * <p>This format cannot encode sequences of less than 2 clean words and 0 dirty
  * word. The reason is that if you find a single clean word, you should rather
  * encode it as a dirty word. This takes the same space as starting a new
  * sequence (since you need one byte for the token) but will be lighter to
@@ -66,10 +69,9 @@ import org.apache.lucene.util.packed.MonotonicAppendingLongBuffer;
  * sequence may start directly with a dirty word, the clean length is encoded
  * directly, without subtracting 2.</p>
  * <p>There is an additional restriction on the format: the sequence of dirty
- * words must start and end with a non-null word and is not allowed to contain
- * two consecutive null words. This restriction exists to make sure no space is
- * wasted and to make sure iterators can read the next doc ID by reading at most
- * 2 dirty words.</p>
+ * words is not allowed to contain two consecutive clean words. This restriction
+ * exists to make sure no space is wasted and to make sure iterators can read
+ * the next doc ID by reading at most 2 dirty words.</p>
  * @lucene.experimental
  */
 public final class WAH8DocIdSet extends DocIdSet {
@@ -82,9 +84,9 @@ public final class WAH8DocIdSet extends DocIdSet {
   private static final int MIN_INDEX_INTERVAL = 8;
 
   /** Default index interval. */
-  public static final int DEFAULT_INDEX_INTERVAL = MIN_INDEX_INTERVAL;
+  public static final int DEFAULT_INDEX_INTERVAL = 24;
 
-  private static final MonotonicAppendingLongBuffer SINGLE_ZERO_BUFFER = new MonotonicAppendingLongBuffer();
+  private static final MonotonicAppendingLongBuffer SINGLE_ZERO_BUFFER = new MonotonicAppendingLongBuffer(1, 64, PackedInts.COMPACT);
   private static WAH8DocIdSet EMPTY = new WAH8DocIdSet(new byte[0], 0, 1, SINGLE_ZERO_BUFFER, SINGLE_ZERO_BUFFER);
 
   static {
@@ -229,6 +231,7 @@ public final class WAH8DocIdSet extends DocIdSet {
     int numSequences;
     int indexInterval;
     int cardinality;
+    boolean reverse;
 
     WordBuilder() {
       out = new GrowableByteArrayDataOutput(1024);
@@ -255,34 +258,45 @@ public final class WAH8DocIdSet extends DocIdSet {
       return this;
     }
 
-    void writeHeader(int cleanLength) throws IOException {
+    void writeHeader(boolean reverse, int cleanLength, int dirtyLength) throws IOException {
       final int cleanLengthMinus2 = cleanLength - 2;
-      final int dirtyLengthMinus1 = dirtyWords.length - 1;
       assert cleanLengthMinus2 >= 0;
-      assert dirtyLengthMinus1 >= 0;
-      int token = ((cleanLengthMinus2 & 0x07) << 4) | (dirtyLengthMinus1 & 0x07);
-      if (cleanLengthMinus2 > 0x07) {
+      assert dirtyLength >= 0;
+      int token = ((cleanLengthMinus2 & 0x03) << 4) | (dirtyLength & 0x07);
+      if (reverse) {
         token |= 1 << 7;
       }
-      if (dirtyLengthMinus1 > 0x07) {
+      if (cleanLengthMinus2 > 0x03) {
+        token |= 1 << 6;
+      }
+      if (dirtyLength > 0x07) {
         token |= 1 << 3;
       }
       out.writeByte((byte) token);
-      if (cleanLengthMinus2 > 0x07) {
-        out.writeVInt(cleanLengthMinus2 >>> 3);
+      if (cleanLengthMinus2 > 0x03) {
+        out.writeVInt(cleanLengthMinus2 >>> 2);
+      }
+      if (dirtyLength > 0x07) {
+        out.writeVInt(dirtyLength >>> 3);
       }
-      if (dirtyLengthMinus1 > 0x07) {
-        out.writeVInt(dirtyLengthMinus1 >>> 3);
+    }
+
+    private boolean sequenceIsConsistent() {
+      for (int i = 1; i < dirtyWords.length; ++i) {
+        assert dirtyWords.bytes[i-1] != 0 || dirtyWords.bytes[i] != 0;
+        assert dirtyWords.bytes[i-1] != (byte) 0xFF || dirtyWords.bytes[i] != (byte) 0xFF;
       }
+      return true;
     }
 
-    void writeSequence(int cleanLength) {
+    void writeSequence() {
+      assert sequenceIsConsistent();
       try {
-        writeHeader(cleanLength);
-        out.writeBytes(dirtyWords.bytes, dirtyWords.length);
+        writeHeader(reverse, clean, dirtyWords.length);
       } catch (IOException cannotHappen) {
         throw new AssertionError(cannotHappen);
       }
+      out.writeBytes(dirtyWords.bytes, 0, dirtyWords.length);
       dirtyWords.length = 0;
       ++numSequences;
     }
@@ -291,20 +305,57 @@ public final class WAH8DocIdSet extends DocIdSet {
       assert wordNum > lastWordNum;
       assert word != 0;
 
-      if (lastWordNum == -1) {
-        clean = 2 + wordNum; // special case for the 1st sequence
-        dirtyWords.writeByte(word);
+      if (!reverse) {
+        if (lastWordNum == -1) {
+          clean = 2 + wordNum; // special case for the 1st sequence
+          dirtyWords.writeByte(word);
+        } else {
+          switch (wordNum - lastWordNum) {
+            case 1:
+              if (word == (byte) 0xFF && dirtyWords.bytes[dirtyWords.length-1] == (byte) 0xFF) {
+                --dirtyWords.length;
+                writeSequence();
+                reverse = true;
+                clean = 2;
+              } else {
+                dirtyWords.writeByte(word);
+              }
+              break;
+            case 2:
+              dirtyWords.writeByte((byte) 0);
+              dirtyWords.writeByte(word);
+              break;
+            default:
+              writeSequence();
+              clean = wordNum - lastWordNum - 1;
+              dirtyWords.writeByte(word);
+          }
+        }
       } else {
+        assert lastWordNum >= 0;
         switch (wordNum - lastWordNum) {
           case 1:
-            dirtyWords.writeByte(word);
+            if (word == (byte) 0xFF) {
+              if (dirtyWords.length == 0) {
+                ++clean;
+              } else if (dirtyWords.bytes[dirtyWords.length - 1] == (byte) 0xFF) {
+                --dirtyWords.length;
+                writeSequence();
+                clean = 2;
+              } else {
+                dirtyWords.writeByte(word);
+              }
+            } else {
+              dirtyWords.writeByte(word);
+            }
             break;
           case 2:
             dirtyWords.writeByte((byte) 0);
             dirtyWords.writeByte(word);
             break;
           default:
-            writeSequence(clean);
+            writeSequence();
+            reverse = false;
             clean = wordNum - lastWordNum - 1;
             dirtyWords.writeByte(word);
         }
@@ -319,7 +370,7 @@ public final class WAH8DocIdSet extends DocIdSet {
         assert lastWordNum == -1;
         return EMPTY;
       }
-      writeSequence(clean);
+      writeSequence();
       final byte[] data = Arrays.copyOf(out.bytes, out.length);
 
       // Now build the index
@@ -330,9 +381,9 @@ public final class WAH8DocIdSet extends DocIdSet {
       } else {
         final int pageSize = 128;
         final int initialPageCount = (valueCount + pageSize - 1) / pageSize;
-        final MonotonicAppendingLongBuffer positions = new MonotonicAppendingLongBuffer(initialPageCount, pageSize);
-        final MonotonicAppendingLongBuffer wordNums = new MonotonicAppendingLongBuffer(initialPageCount, pageSize);
- 
+        final MonotonicAppendingLongBuffer positions = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);
+        final MonotonicAppendingLongBuffer wordNums = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, PackedInts.COMPACT);
+
         positions.add(0L);
         wordNums.add(0L);
         final Iterator it = new Iterator(data, cardinality, Integer.MAX_VALUE, SINGLE_ZERO_BUFFER, SINGLE_ZERO_BUFFER);
@@ -443,20 +494,43 @@ public final class WAH8DocIdSet extends DocIdSet {
     return new Iterator(data, cardinality, indexInterval, positions, wordNums);
   }
 
-  static int readLength(ByteArrayDataInput in, int len) {
-    if ((len & 0x08) == 0) {
-      // no continuation bit
-      return len;
+  static int readCleanLength(ByteArrayDataInput in, int token) {
+    int len = (token >>> 4) & 0x07;
+    final int startPosition = in.getPosition();
+    if ((len & 0x04) != 0) {
+      len = (len & 0x03) | (in.readVInt() << 2);
+    }
+    if (startPosition != 1) {
+      len += 2;
+    }
+    return len;
+  }
+
+  static int readDirtyLength(ByteArrayDataInput in, int token) {
+    int len = token & 0x0F;
+    if ((len & 0x08) != 0) {
+      len = (len & 0x07) | (in.readVInt() << 3);
     }
-    return (len & 0x07) | (in.readVInt() << 3);
+    return len;
   }
 
   static class Iterator extends DocIdSetIterator {
 
+    /* Using the index can be costly for close targets. */
+    static int indexThreshold(int cardinality, int indexInterval) {
+      // Short sequences encode for 3 words (2 clean words and 1 dirty byte),
+      // don't advance if we are going to read less than 3 x indexInterval
+      // sequences
+      long indexThreshold = 3L * 3 * indexInterval;
+      return (int) Math.min(Integer.MAX_VALUE, indexThreshold);
+    }
+
     final ByteArrayDataInput in;
     final int cardinality;
     final int indexInterval;
     final MonotonicAppendingLongBuffer positions, wordNums;
+    final int indexThreshold;
+    int allOnesLength;
     int dirtyLength;
 
     int wordNum; // byte offset
@@ -477,6 +551,7 @@ public final class WAH8DocIdSet extends DocIdSet {
       bitList = 0;
       sequenceNum = -1;
       docID = -1;
+      indexThreshold = indexThreshold(cardinality, indexInterval);
     }
 
     boolean readSequence() {
@@ -485,40 +560,64 @@ public final class WAH8DocIdSet extends DocIdSet {
         return false;
       }
       final int token = in.readByte() & 0xFF;
-      final int cleanLength = (in.getPosition() == 1 ? 0 : 2) + readLength(in, token >>> 4);
-      wordNum += cleanLength;
-      dirtyLength = 1 + readLength(in, token & 0x0F);
+      if ((token & (1 << 7)) == 0) {
+        final int cleanLength = readCleanLength(in, token);
+        wordNum += cleanLength;
+      } else {
+        allOnesLength = readCleanLength(in, token);
+      }
+      dirtyLength = readDirtyLength(in, token);
+      assert in.length() - in.getPosition() >= dirtyLength : in.getPosition() + " " + in.length() + " " + dirtyLength;
       ++sequenceNum;
       return true;
     }
 
     void skipDirtyBytes(int count) {
       assert count >= 0;
-      assert count <= dirtyLength;
-      in.skipBytes(count);
+      assert count <= allOnesLength + dirtyLength;
       wordNum += count;
-      dirtyLength -= count;
+      if (count <= allOnesLength) {
+        allOnesLength -= count;
+      } else {
+        count -= allOnesLength;
+        allOnesLength = 0;
+        in.skipBytes(count);
+        dirtyLength -= count;
+      }
     }
 
     void skipDirtyBytes() {
+      wordNum += allOnesLength + dirtyLength;
       in.skipBytes(dirtyLength);
-      wordNum += dirtyLength;
+      allOnesLength = 0;
       dirtyLength = 0;
     }
 
     void nextWord() {
-      if (dirtyLength == 0 && !readSequence()) {
+      if (allOnesLength > 0) {
+        word = (byte) 0xFF;
+        ++wordNum;
+        --allOnesLength;
         return;
       }
-      word = in.readByte();
-      if (word == 0) {
+      if (dirtyLength > 0) {
         word = in.readByte();
-        assert word != 0; // there can never be two consecutive null dirty words
         ++wordNum;
         --dirtyLength;
+        if (word != 0) {
+          return;
+        }
+        if (dirtyLength > 0) {
+          word = in.readByte();
+          ++wordNum;
+          --dirtyLength;
+          assert word != 0; // never more than one consecutive 0
+          return;
+        }
+      }
+      if (readSequence()) {
+        nextWord();
       }
-      ++wordNum;
-      --dirtyLength;
     }
 
     int forwardBinarySearch(int targetWordNum) {
@@ -557,20 +656,20 @@ public final class WAH8DocIdSet extends DocIdSet {
     void advanceWord(int targetWordNum) {
       assert targetWordNum > wordNum;
       int delta = targetWordNum - wordNum;
-      if (delta <= dirtyLength + 1) {
-        if (delta > 1) {
-          skipDirtyBytes(delta - 1);
-        }
+      if (delta <= allOnesLength + dirtyLength + 1) {
+        skipDirtyBytes(delta - 1);
       } else {
         skipDirtyBytes();
         assert dirtyLength == 0;
-        // use the index
-        final int i = forwardBinarySearch(targetWordNum);
-        final int position = (int) positions.get(i);
-        if (position > in.getPosition()) { // if the binary search returned a backward offset, don't move
-          wordNum = (int) wordNums.get(i) - 1;
-          in.setPosition(position);
-          sequenceNum = i * indexInterval - 1;
+        if (delta > indexThreshold) {
+          // use the index
+          final int i = forwardBinarySearch(targetWordNum);
+          final int position = (int) positions.get(i);
+          if (position > in.getPosition()) { // if the binary search returned a backward offset, don't move
+            wordNum = (int) wordNums.get(i) - 1;
+            in.setPosition(position);
+            sequenceNum = i * indexInterval - 1;
+          }
         }
 
         while (true) {
@@ -578,7 +677,7 @@ public final class WAH8DocIdSet extends DocIdSet {
             return;
           }
           delta = targetWordNum - wordNum;
-          if (delta <= dirtyLength + 1) {
+          if (delta <= allOnesLength + dirtyLength + 1) {
             if (delta > 1) {
               skipDirtyBytes(delta - 1);
             }
diff --git a/lucene/core/src/java/org/apache/lucene/util/fst/FST.java b/lucene/core/src/java/org/apache/lucene/util/fst/FST.java
index b2cbd66..133a2da 100644
--- a/lucene/core/src/java/org/apache/lucene/util/fst/FST.java
+++ b/lucene/core/src/java/org/apache/lucene/util/fst/FST.java
@@ -171,6 +171,8 @@ public final class FST<T> {
   private final boolean allowArrayArcs;
 
   private Arc<T> cachedRootArcs[];
+  private Arc<T> assertingCachedRootArcs[]; // only set wit assert
+
 
   /** Represents a single arc. */
   public final static class Arc<T> {
@@ -213,7 +215,7 @@ public final class FST<T> {
       }
       return this;
     }
-
+    
     boolean flag(int flag) {
       return FST.flag(flags, flag);
     }
@@ -423,11 +425,18 @@ public final class FST<T> {
       return node;
     }
   }
-
+  
   // Caches first 128 labels
   @SuppressWarnings({"rawtypes","unchecked"})
   private void cacheRootArcs() throws IOException {
     cachedRootArcs = (Arc<T>[]) new Arc[0x80];
+    readRootArcs(cachedRootArcs);
+    
+    assert setAssertingRootArcs(cachedRootArcs);
+    assert assertRootArcs();
+  }
+  
+  public void readRootArcs(Arc<T>[] arcs) throws IOException {
     final Arc<T> arc = new Arc<T>();
     getFirstArc(arc);
     final BytesReader in = getBytesReader();
@@ -436,7 +445,7 @@ public final class FST<T> {
       while(true) {
         assert arc.label != END_LABEL;
         if (arc.label < cachedRootArcs.length) {
-          cachedRootArcs[arc.label] = new Arc<T>().copyFrom(arc);
+          arcs[arc.label] = new Arc<T>().copyFrom(arc);
         } else {
           break;
         }
@@ -447,6 +456,38 @@ public final class FST<T> {
       }
     }
   }
+  
+  @SuppressWarnings({"rawtypes","unchecked"})
+  private boolean setAssertingRootArcs(Arc<T>[] arcs) throws IOException {
+    assertingCachedRootArcs = (Arc<T>[]) new Arc[arcs.length];
+    readRootArcs(assertingCachedRootArcs);
+    return true;
+  }
+  
+  private boolean assertRootArcs() {
+    assert cachedRootArcs != null;
+    assert assertingCachedRootArcs != null;
+    for (int i = 0; i < cachedRootArcs.length; i++) {
+      final Arc<T> root = cachedRootArcs[i];
+      final Arc<T> asserting = assertingCachedRootArcs[i];
+      if (root != null) { 
+        assert root.arcIdx == asserting.arcIdx;
+        assert root.bytesPerArc == asserting.bytesPerArc;
+        assert root.flags == asserting.flags;
+        assert root.label == asserting.label;
+        assert root.nextArc == asserting.nextArc;
+        assert root.nextFinalOutput.equals(asserting.nextFinalOutput);
+        assert root.node == asserting.node;
+        assert root.numArcs == asserting.numArcs;
+        assert root.output.equals(asserting.output);
+        assert root.posArcsStart == asserting.posArcsStart;
+        assert root.target == asserting.target;
+      } else {
+        assert root == null && asserting == null;
+      } 
+    }
+    return true;
+  }
 
   public T getEmptyOutput() {
     return emptyOutput;
@@ -1105,7 +1146,7 @@ public final class FST<T> {
   /** Finds an arc leaving the incoming arc, replacing the arc in place.
    *  This returns null if the arc was not found, else the incoming arc. */
   public Arc<T> findTargetArc(int labelToMatch, Arc<T> follow, Arc<T> arc, BytesReader in) throws IOException {
-    assert cachedRootArcs != null;
+    assert assertRootArcs();
 
     if (labelToMatch == END_LABEL) {
       if (follow.isFinal()) {
@@ -1129,7 +1170,7 @@ public final class FST<T> {
     if (follow.target == startNode && labelToMatch < cachedRootArcs.length) {
       final Arc<T> result = cachedRootArcs[labelToMatch];
       if (result == null) {
-        return result;
+        return null;
       } else {
         arc.copyFrom(result);
         return arc;
diff --git a/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValue.java b/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValue.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValueDate.java b/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValueDate.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValueDouble.java b/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValueDouble.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValueFloat.java b/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValueFloat.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValueInt.java b/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValueInt.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValueStr.java b/lucene/core/src/java/org/apache/lucene/util/mutable/MutableValueStr.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/AbstractAppendingLongBuffer.java b/lucene/core/src/java/org/apache/lucene/util/packed/AbstractAppendingLongBuffer.java
index 4fab936..c0da058 100644
--- a/lucene/core/src/java/org/apache/lucene/util/packed/AbstractAppendingLongBuffer.java
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/AbstractAppendingLongBuffer.java
@@ -17,14 +17,14 @@ package org.apache.lucene.util.packed;
  * limitations under the License.
  */
 
-import static org.apache.lucene.util.packed.PackedInts.checkBlockSize;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
 
 import java.util.Arrays;
 
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.RamUsageEstimator;
+import static org.apache.lucene.util.packed.PackedInts.checkBlockSize;
 
-/** Common functionality shared by {@link AppendingLongBuffer} and {@link MonotonicAppendingLongBuffer}. */
+/** Common functionality shared by {@link AppendingDeltaPackedLongBuffer} and {@link MonotonicAppendingLongBuffer}. */
 abstract class AbstractAppendingLongBuffer {
 
   static final int MIN_PAGE_SIZE = 64;
@@ -33,21 +33,21 @@ abstract class AbstractAppendingLongBuffer {
   static final int MAX_PAGE_SIZE = 1 << 20;
 
   final int pageShift, pageMask;
-  long[] minValues;
-  PackedInts.Reader[] deltas;
-  private long deltasBytes;
+  PackedInts.Reader[] values;
+  private long valuesBytes;
   int valuesOff;
   long[] pending;
   int pendingOff;
+  float acceptableOverheadRatio;
 
-  AbstractAppendingLongBuffer(int initialBlockCount, int pageSize) {
-    minValues = new long[initialBlockCount];
-    deltas = new PackedInts.Reader[initialBlockCount];
+  AbstractAppendingLongBuffer(int initialBlockCount, int pageSize, float acceptableOverheadRatio) {
+    values = new PackedInts.Reader[initialBlockCount];
     pending = new long[pageSize];
     pageShift = checkBlockSize(pageSize, MIN_PAGE_SIZE, MAX_PAGE_SIZE);
     pageMask = pageSize - 1;
     valuesOff = 0;
     pendingOff = 0;
+    this.acceptableOverheadRatio = acceptableOverheadRatio;
   }
 
   final int pageSize() {
@@ -58,7 +58,7 @@ abstract class AbstractAppendingLongBuffer {
   public final long size() {
     long size = pendingOff;
     if (valuesOff > 0) {
-      size += deltas[valuesOff - 1].size();
+      size += values[valuesOff - 1].size();
     }
     if (valuesOff > 1) {
       size += (long) (valuesOff - 1) * pageSize();
@@ -73,12 +73,12 @@ abstract class AbstractAppendingLongBuffer {
     }
     if (pendingOff == pending.length) {
       // check size
-      if (deltas.length == valuesOff) {
+      if (values.length == valuesOff) {
         final int newLength = ArrayUtil.oversize(valuesOff + 1, 8);
         grow(newLength);
       }
       packPendingValues();
-      deltasBytes += deltas[valuesOff].ramBytesUsed();
+      valuesBytes += values[valuesOff].ramBytesUsed();
       ++valuesOff;
       // reset pending buffer
       pendingOff = 0;
@@ -87,8 +87,7 @@ abstract class AbstractAppendingLongBuffer {
   }
 
   void grow(int newBlockCount) {
-    minValues = Arrays.copyOf(minValues, newBlockCount);
-    deltas = Arrays.copyOf(deltas, newBlockCount);
+    values = Arrays.copyOf(values, newBlockCount);
   }
 
   abstract void packPendingValues();
@@ -101,11 +100,33 @@ abstract class AbstractAppendingLongBuffer {
     return get(block, element);
   }
 
+  /**
+   * Bulk get: read at least one and at most <code>len</code> longs starting
+   * from <code>index</code> into <code>arr[off:off+len]</code> and return
+   * the actual number of values that have been read.
+   */
+  public final int get(long index, long[] arr, int off, int len) {
+    assert len > 0 : "len must be > 0 (got " + len + ")";
+    assert index >= 0 && index < size();
+    assert off + len <= arr.length;
+
+    int block = (int) (index >> pageShift);
+    int element = (int) (index & pageMask);
+    return get(block, element, arr, off, len);
+  }
+
+
   abstract long get(int block, int element);
 
-  abstract Iterator iterator();
+  abstract int get(int block, int element, long[] arr, int off, int len);
+
+
+  /** Return an iterator over the values of this buffer. */
+  public Iterator iterator() {
+    return new Iterator();
+  }
 
-  abstract class Iterator {
+  final public class Iterator {
 
     long[] currentValues;
     int vOff, pOff;
@@ -117,12 +138,22 @@ abstract class AbstractAppendingLongBuffer {
         currentValues = pending;
         currentCount = pendingOff;
       } else {
-        currentValues = new long[deltas[0].size()];
+        currentValues = new long[values[0].size()];
         fillValues();
       }
     }
 
-    abstract void fillValues();
+    void fillValues() {
+      if (vOff == valuesOff) {
+        currentValues = pending;
+        currentCount = pendingOff;
+      } else {
+        currentCount = values[vOff].size();
+        for (int k = 0; k < currentCount; ) {
+          k += get(vOff, k, currentValues, k, currentCount - k);
+        }
+      }
+    }
 
     /** Whether or not there are remaining values. */
     public final boolean hasNext() {
@@ -149,33 +180,31 @@ abstract class AbstractAppendingLongBuffer {
 
   long baseRamBytesUsed() {
     return RamUsageEstimator.NUM_BYTES_OBJECT_HEADER
-        + 3 * RamUsageEstimator.NUM_BYTES_OBJECT_REF // the 3 arrays
+        + 2 * RamUsageEstimator.NUM_BYTES_OBJECT_REF // the 2 arrays
         + 2 * RamUsageEstimator.NUM_BYTES_INT // the 2 offsets
         + 2 * RamUsageEstimator.NUM_BYTES_INT // pageShift, pageMask
-        + RamUsageEstimator.NUM_BYTES_LONG; // deltasBytes
+        + RamUsageEstimator.NUM_BYTES_FLOAT   // acceptable overhead
+        + RamUsageEstimator.NUM_BYTES_LONG; // valuesBytes
   }
 
-  /**
-   * Return the number of bytes used by this instance.
-   */
+  /** Return the number of bytes used by this instance. */
   public long ramBytesUsed() {
     // TODO: this is called per-doc-per-norms/dv-field, can we optimize this?
     long bytesUsed = RamUsageEstimator.alignObjectSize(baseRamBytesUsed())
         + (pending != null ? RamUsageEstimator.sizeOf(pending) : 0L)
-        + RamUsageEstimator.sizeOf(minValues)
-        + RamUsageEstimator.alignObjectSize(RamUsageEstimator.NUM_BYTES_ARRAY_HEADER + (long) RamUsageEstimator.NUM_BYTES_OBJECT_REF * deltas.length); // values
+        + RamUsageEstimator.alignObjectSize(RamUsageEstimator.NUM_BYTES_ARRAY_HEADER + (long) RamUsageEstimator.NUM_BYTES_OBJECT_REF * values.length); // values
 
-    return bytesUsed + deltasBytes;
+    return bytesUsed + valuesBytes;
   }
 
   /** Pack all pending values in this buffer. Subsequent calls to {@link #add(long)} will fail. */
   public void freeze() {
     if (pendingOff > 0) {
-      if (deltas.length == valuesOff) {
+      if (values.length == valuesOff) {
         grow(valuesOff + 1); // don't oversize!
       }
       packPendingValues();
-      deltasBytes += deltas[valuesOff].ramBytesUsed();
+      valuesBytes += values[valuesOff].ramBytesUsed();
       ++valuesOff;
       pendingOff = 0;
     }
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/AppendingDeltaPackedLongBuffer.java b/lucene/core/src/java/org/apache/lucene/util/packed/AppendingDeltaPackedLongBuffer.java
new file mode 100644
index 0000000..f5ea192
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/AppendingDeltaPackedLongBuffer.java
@@ -0,0 +1,136 @@
+package org.apache.lucene.util.packed;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.util.RamUsageEstimator;
+
+import java.util.Arrays;
+
+/**
+ * Utility class to buffer a list of signed longs in memory. This class only
+ * supports appending and is optimized for the case where values are close to
+ * each other.
+ *
+ * @lucene.internal
+ */
+public final class AppendingDeltaPackedLongBuffer extends AbstractAppendingLongBuffer {
+
+  long[] minValues;
+
+
+  /** Create {@link AppendingDeltaPackedLongBuffer}
+   * @param initialPageCount        the initial number of pages
+   * @param pageSize                the size of a single page
+   * @param acceptableOverheadRatio an acceptable overhead ratio per value
+   */
+  public AppendingDeltaPackedLongBuffer(int initialPageCount, int pageSize, float acceptableOverheadRatio) {
+    super(initialPageCount, pageSize, acceptableOverheadRatio);
+    minValues = new long[values.length];
+  }
+
+  /**
+   * Create an {@link AppendingDeltaPackedLongBuffer} with initialPageCount=16,
+   * pageSize=1024 and acceptableOverheadRatio={@link PackedInts#DEFAULT}
+   */
+  public AppendingDeltaPackedLongBuffer() {
+    this(16, 1024, PackedInts.DEFAULT);
+  }
+
+  /**
+   * Create an {@link AppendingDeltaPackedLongBuffer} with initialPageCount=16,
+   * pageSize=1024
+   */
+  public AppendingDeltaPackedLongBuffer(float acceptableOverheadRatio) {
+    this(16, 1024, acceptableOverheadRatio);
+  }
+
+  @Override
+  long get(int block, int element) {
+    if (block == valuesOff) {
+      return pending[element];
+    } else if (values[block] == null) {
+      return minValues[block];
+    } else {
+      return minValues[block] + values[block].get(element);
+    }
+  }
+
+  @Override
+  int get(int block, int element, long[] arr, int off, int len) {
+    if (block == valuesOff) {
+      int sysCopyToRead = Math.min(len, pendingOff - element);
+      System.arraycopy(pending, element, arr, off, sysCopyToRead);
+      return sysCopyToRead;
+    } else {
+      /* packed block */
+      int read = values[block].get(element, arr, off, len);
+      long d = minValues[block];
+      for (int r = 0; r < read; r++, off++) {
+        arr[off] += d;
+      }
+      return read;
+    }
+  }
+
+  @Override
+  void packPendingValues() {
+    // compute max delta
+    long minValue = pending[0];
+    long maxValue = pending[0];
+    for (int i = 1; i < pendingOff; ++i) {
+      minValue = Math.min(minValue, pending[i]);
+      maxValue = Math.max(maxValue, pending[i]);
+    }
+    final long delta = maxValue - minValue;
+
+    minValues[valuesOff] = minValue;
+    if (delta == 0) {
+      values[valuesOff] = new PackedInts.NullReader(pendingOff);
+    } else {
+      // build a new packed reader
+      final int bitsRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);
+      for (int i = 0; i < pendingOff; ++i) {
+        pending[i] -= minValue;
+      }
+      final PackedInts.Mutable mutable = PackedInts.getMutable(pendingOff, bitsRequired, acceptableOverheadRatio);
+      for (int i = 0; i < pendingOff; ) {
+        i += mutable.set(i, pending, i, pendingOff - i);
+      }
+      values[valuesOff] = mutable;
+    }
+  }
+
+  @Override
+  void grow(int newBlockCount) {
+    super.grow(newBlockCount);
+    this.minValues = Arrays.copyOf(minValues, newBlockCount);
+  }
+
+  @Override
+  long baseRamBytesUsed() {
+    return super.baseRamBytesUsed()
+        + RamUsageEstimator.NUM_BYTES_OBJECT_REF; // additional array
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return super.ramBytesUsed() + RamUsageEstimator.sizeOf(minValues);
+  }
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/AppendingLongBuffer.java b/lucene/core/src/java/org/apache/lucene/util/packed/AppendingLongBuffer.java
deleted file mode 100644
index 86784ab..0000000
--- a/lucene/core/src/java/org/apache/lucene/util/packed/AppendingLongBuffer.java
+++ /dev/null
@@ -1,111 +0,0 @@
-package org.apache.lucene.util.packed;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/**
- * Utility class to buffer a list of signed longs in memory. This class only
- * supports appending and is optimized for the case where values are close to
- * each other.
- * @lucene.internal
- */
-public final class AppendingLongBuffer extends AbstractAppendingLongBuffer {
-
-  /** @param initialPageCount the initial number of pages
-   *  @param pageSize         the size of a single page */
-  public AppendingLongBuffer(int initialPageCount, int pageSize) {
-    super(initialPageCount, pageSize);
-  }
-
-  /** Create an {@link AppendingLongBuffer} with initialPageCount=16 and
-   *  pageSize=1024. */
-  public AppendingLongBuffer() {
-    this(16, 1024);
-  }
-
-  @Override
-  long get(int block, int element) {
-    if (block == valuesOff) {
-      return pending[element];
-    } else if (deltas[block] == null) {
-      return minValues[block];
-    } else {
-      return minValues[block] + deltas[block].get(element);
-    }
-  }
-
-  @Override
-  void packPendingValues() {
-    // compute max delta
-    long minValue = pending[0];
-    long maxValue = pending[0];
-    for (int i = 1; i < pendingOff; ++i) {
-      minValue = Math.min(minValue, pending[i]);
-      maxValue = Math.max(maxValue, pending[i]);
-    }
-    final long delta = maxValue - minValue;
-
-    minValues[valuesOff] = minValue;
-    if (delta == 0) {
-      deltas[valuesOff] = new PackedInts.NullReader(pendingOff);
-    } else {
-      // build a new packed reader
-      final int bitsRequired = delta < 0 ? 64 : PackedInts.bitsRequired(delta);
-      for (int i = 0; i < pendingOff; ++i) {
-        pending[i] -= minValue;
-      }
-      final PackedInts.Mutable mutable = PackedInts.getMutable(pendingOff, bitsRequired, PackedInts.COMPACT);
-      for (int i = 0; i < pendingOff; ) {
-        i += mutable.set(i, pending, i, pendingOff - i);
-      }
-      deltas[valuesOff] = mutable;
-    }
-  }
-
-  /** Return an iterator over the values of this buffer. */
-  @Override
-  public Iterator iterator() {
-    return new Iterator();
-  }
-
-  /** A long iterator. */
-  public final class Iterator extends AbstractAppendingLongBuffer.Iterator {
-
-    Iterator() {
-      super();
-    }
-
-    @Override
-    void fillValues() {
-      if (vOff == valuesOff) {
-        currentValues = pending;
-        currentCount = pendingOff;
-      } else {
-        currentCount = deltas[vOff].size();
-        for (int k = 0; k < currentCount; ) {
-          k += deltas[vOff].get(k, currentValues, k, currentCount - k);
-        }
-        for (int k = 0; k < currentCount; ++k) {
-          currentValues[k] += minValues[vOff];
-        }
-      }
-    }
-
-  }
-
-}
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/AppendingPackedLongBuffer.java b/lucene/core/src/java/org/apache/lucene/util/packed/AppendingPackedLongBuffer.java
new file mode 100644
index 0000000..e2229d1
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/AppendingPackedLongBuffer.java
@@ -0,0 +1,96 @@
+package org.apache.lucene.util.packed;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/**
+ * Utility class to buffer a list of signed longs in memory. This class only
+ * supports appending and is optimized for non-negative numbers with a uniform distribution over a fixed (limited) range
+ *
+ * @lucene.internal
+ */
+public final class AppendingPackedLongBuffer extends AbstractAppendingLongBuffer {
+
+  /**{@link AppendingPackedLongBuffer}
+   * @param initialPageCount        the initial number of pages
+   * @param pageSize                the size of a single page
+   * @param acceptableOverheadRatio an acceptable overhead ratio per value
+   */
+  public AppendingPackedLongBuffer(int initialPageCount, int pageSize, float acceptableOverheadRatio) {
+    super(initialPageCount, pageSize, acceptableOverheadRatio);
+  }
+
+  /**
+   * Create an {@link AppendingPackedLongBuffer} with initialPageCount=16,
+   * pageSize=1024 and acceptableOverheadRatio={@link PackedInts#DEFAULT}
+   */
+  public AppendingPackedLongBuffer() {
+    this(16, 1024, PackedInts.DEFAULT);
+  }
+
+  /**
+   * Create an {@link AppendingPackedLongBuffer} with initialPageCount=16,
+   * pageSize=1024
+   */
+  public AppendingPackedLongBuffer(float acceptableOverheadRatio) {
+    this(16, 1024, acceptableOverheadRatio);
+  }
+
+  @Override
+  long get(int block, int element) {
+    if (block == valuesOff) {
+      return pending[element];
+    } else {
+      return values[block].get(element);
+    }
+  }
+
+  @Override
+  int get(int block, int element, long[] arr, int off, int len) {
+    if (block == valuesOff) {
+      int sysCopyToRead = Math.min(len, pendingOff - element);
+      System.arraycopy(pending, element, arr, off, sysCopyToRead);
+      return sysCopyToRead;
+    } else {
+      /* packed block */
+      return values[block].get(element, arr, off, len);
+    }
+  }
+
+  @Override
+  void packPendingValues() {
+    // compute max delta
+    long minValue = pending[0];
+    long maxValue = pending[0];
+    for (int i = 1; i < pendingOff; ++i) {
+      minValue = Math.min(minValue, pending[i]);
+      maxValue = Math.max(maxValue, pending[i]);
+    }
+
+
+    // build a new packed reader
+    final int bitsRequired = minValue < 0 ? 64 : PackedInts.bitsRequired(maxValue);
+    final PackedInts.Mutable mutable = PackedInts.getMutable(pendingOff, bitsRequired, acceptableOverheadRatio);
+    for (int i = 0; i < pendingOff; ) {
+      i += mutable.set(i, pending, i, pendingOff - i);
+    }
+    values[valuesOff] = mutable;
+
+  }
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/MonotonicAppendingLongBuffer.java b/lucene/core/src/java/org/apache/lucene/util/packed/MonotonicAppendingLongBuffer.java
index 43e0d20..bf04d16 100644
--- a/lucene/core/src/java/org/apache/lucene/util/packed/MonotonicAppendingLongBuffer.java
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/MonotonicAppendingLongBuffer.java
@@ -17,14 +17,15 @@ package org.apache.lucene.util.packed;
  * limitations under the License.
  */
 
-import java.util.Arrays;
-
 import org.apache.lucene.util.RamUsageEstimator;
 
+import java.util.Arrays;
+
 /**
  * Utility class to buffer signed longs in memory, which is optimized for the
  * case where the sequence is monotonic, although it can encode any sequence of
  * arbitrary longs. It only supports appending.
+ *
  * @lucene.internal
  */
 public final class MonotonicAppendingLongBuffer extends AbstractAppendingLongBuffer {
@@ -32,36 +33,77 @@ public final class MonotonicAppendingLongBuffer extends AbstractAppendingLongBuf
   static long zigZagDecode(long n) {
     return ((n >>> 1) ^ -(n & 1));
   }
-  
+
   static long zigZagEncode(long n) {
     return (n >> 63) ^ (n << 1);
   }
 
   float[] averages;
-
-  /** @param initialPageCount the initial number of pages
-   *  @param pageSize         the size of a single page */
-  public MonotonicAppendingLongBuffer(int initialPageCount, int pageSize) {
-    super(initialPageCount, pageSize);
-    averages = new float[pageSize];
+  long[] minValues;
+
+  /**
+   * @param initialPageCount        the initial number of pages
+   * @param pageSize                the size of a single page
+   * @param acceptableOverheadRatio an acceptable overhead ratio per value
+   */
+  public MonotonicAppendingLongBuffer(int initialPageCount, int pageSize, float acceptableOverheadRatio) {
+    super(initialPageCount, pageSize, acceptableOverheadRatio);
+    averages = new float[values.length];
+    minValues = new long[values.length];
   }
 
-  /** Create an {@link MonotonicAppendingLongBuffer} with initialPageCount=16
-   *  and pageSize=1024. */
+  /**
+   * Create an {@link MonotonicAppendingLongBuffer} with initialPageCount=16,
+   * pageSize=1024 and acceptableOverheadRatio={@link PackedInts#DEFAULT}
+   */
   public MonotonicAppendingLongBuffer() {
-    this(16, 1024);
+    this(16, 1024, PackedInts.DEFAULT);
+  }
+
+  /**
+   * Create an {@link AppendingDeltaPackedLongBuffer} with initialPageCount=16,
+   * pageSize=1024
+   */
+  public MonotonicAppendingLongBuffer(float acceptableOverheadRatio) {
+    this(16, 1024, acceptableOverheadRatio);
   }
 
+
   @Override
   long get(int block, int element) {
     if (block == valuesOff) {
       return pending[element];
     } else {
       final long base = minValues[block] + (long) (averages[block] * (long) element);
-      if (deltas[block] == null) {
+      if (values[block] == null) {
         return base;
       } else {
-        return base + zigZagDecode(deltas[block].get(element));
+        return base + zigZagDecode(values[block].get(element));
+      }
+    }
+  }
+
+  @Override
+  int get(int block, int element, long[] arr, int off, int len) {
+    if (block == valuesOff) {
+      int sysCopyToRead = Math.min(len, pendingOff - element);
+      System.arraycopy(pending, element, arr, off, sysCopyToRead);
+      return sysCopyToRead;
+    } else {
+      if (values[block] == null) {
+        int toFill = Math.min(len, pending.length - element);
+        for (int r = 0; r < toFill; r++, off++, element++) {
+          arr[off] = minValues[block] + (long) (averages[block] * (long) element);
+        }
+        return toFill;
+      } else {
+
+    /* packed block */
+        int read = values[block].get(element, arr, off, len);
+        for (int r = 0; r < read; r++, off++, element++) {
+          arr[off] = minValues[block] + (long) (averages[block] * (long) element) + zigZagDecode(arr[off]);
+        }
+        return read;
       }
     }
   }
@@ -70,6 +112,7 @@ public final class MonotonicAppendingLongBuffer extends AbstractAppendingLongBuf
   void grow(int newBlockCount) {
     super.grow(newBlockCount);
     this.averages = Arrays.copyOf(averages, newBlockCount);
+    this.minValues = Arrays.copyOf(minValues, newBlockCount);
   }
 
   @Override
@@ -91,58 +134,27 @@ public final class MonotonicAppendingLongBuffer extends AbstractAppendingLongBuf
       }
     }
     if (maxDelta == 0) {
-      deltas[valuesOff] = new  PackedInts.NullReader(pendingOff);
+      values[valuesOff] = new PackedInts.NullReader(pendingOff);
     } else {
       final int bitsRequired = maxDelta < 0 ? 64 : PackedInts.bitsRequired(maxDelta);
-      final PackedInts.Mutable mutable = PackedInts.getMutable(pendingOff, bitsRequired, PackedInts.COMPACT);
+      final PackedInts.Mutable mutable = PackedInts.getMutable(pendingOff, bitsRequired, acceptableOverheadRatio);
       for (int i = 0; i < pendingOff; ) {
         i += mutable.set(i, pending, i, pendingOff - i);
       }
-      deltas[valuesOff] = mutable;
+      values[valuesOff] = mutable;
     }
   }
 
-  /** Return an iterator over the values of this buffer. */
-  @Override
-  public Iterator iterator() {
-    return new Iterator();
-  }
-
-  /** A long iterator. */
-  public final class Iterator extends AbstractAppendingLongBuffer.Iterator {
-
-    Iterator() {
-      super();
-    }
-
-    @Override
-    void fillValues() {
-      if (vOff == valuesOff) {
-        currentValues = pending;
-        currentCount = pendingOff;
-      } else {
-        currentCount = deltas[vOff].size();
-        for (int k = 0; k < currentCount; ) {
-          k += deltas[vOff].get(k, currentValues, k, currentCount - k);
-        }
-        for (int k = 0; k < currentCount; ++k) {
-          currentValues[k] = minValues[vOff] + (long) (averages[vOff] * (long) k) + zigZagDecode(currentValues[k]);
-        }
-      }
-    }
-
-  }
-
   @Override
   long baseRamBytesUsed() {
     return super.baseRamBytesUsed()
-        + RamUsageEstimator.NUM_BYTES_OBJECT_REF; // the additional array
+        + 2 * RamUsageEstimator.NUM_BYTES_OBJECT_REF; // 2 additional arrays
   }
 
   @Override
   public long ramBytesUsed() {
     return super.ramBytesUsed()
-        + RamUsageEstimator.sizeOf(averages);
+        + RamUsageEstimator.sizeOf(averages) + RamUsageEstimator.sizeOf(minValues);
   }
 
 }
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedReader.java b/lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedReader.java
index f7f6e44..7eec87b 100644
--- a/lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedReader.java
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/MonotonicBlockPackedReader.java
@@ -78,5 +78,10 @@ public final class MonotonicBlockPackedReader {
     final int idx = (int) (index & blockMask);
     return minValues[block] + (long) (idx * averages[block]) + zigZagDecode(subReaders[block].get(idx));
   }
+  
+  /** Returns the number of values */
+  public long size() {
+    return valueCount;
+  }
 
 }
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/Packed64SingleBlock.java b/lucene/core/src/java/org/apache/lucene/util/packed/Packed64SingleBlock.java
index 5275124..3482ca7 100644
--- a/lucene/core/src/java/org/apache/lucene/util/packed/Packed64SingleBlock.java
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/Packed64SingleBlock.java
@@ -586,4 +586,4 @@ abstract class Packed64SingleBlock extends PackedInts.MutableImpl {
 
   }
 
-}
\ No newline at end of file
+}
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/PackedInts.java b/lucene/core/src/java/org/apache/lucene/util/packed/PackedInts.java
index 3634dc2..1695403 100644
--- a/lucene/core/src/java/org/apache/lucene/util/packed/PackedInts.java
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/PackedInts.java
@@ -705,6 +705,9 @@ public class PackedInts {
 
     @Override
     public int get(int index, long[] arr, int off, int len) {
+      assert len > 0 : "len must be > 0 (got " + len + ")";
+      assert index >= 0 && index < valueCount;
+      len = Math.min(len, valueCount - index);
       Arrays.fill(arr, off, off + len, 0);
       return len;
     }
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/PagedGrowableWriter.java b/lucene/core/src/java/org/apache/lucene/util/packed/PagedGrowableWriter.java
index 0563846..531cd6d 100644
--- a/lucene/core/src/java/org/apache/lucene/util/packed/PagedGrowableWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/PagedGrowableWriter.java
@@ -23,7 +23,7 @@ import org.apache.lucene.util.packed.PackedInts.Mutable;
 /**
  * A {@link PagedGrowableWriter}. This class slices data into fixed-size blocks
  * which have independent numbers of bits per value and grow on-demand.
- * <p>You should use this class instead of {@link AppendingLongBuffer} only when
+ * <p>You should use this class instead of the {@link AbstractAppendingLongBuffer} related ones only when
  * you need random write-access. Otherwise this class will likely be slower and
  * less memory-efficient.
  * @lucene.internal
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/gen_Direct.py b/lucene/core/src/java/org/apache/lucene/util/packed/gen_Direct.py
index 3cb3c47..0031a5c 100644
--- a/lucene/core/src/java/org/apache/lucene/util/packed/gen_Direct.py
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/gen_Direct.py
@@ -87,6 +87,7 @@ if __name__ == '__main__':
     return values[index]%s;
   }
 
+  @Override
   public void set(final int index, final long value) {
     values[index] = %s(value);
   }
@@ -100,6 +101,7 @@ if __name__ == '__main__':
         + RamUsageEstimator.sizeOf(values);
   }
 
+  @Override
   public void clear() {
     Arrays.fill(values, %s0L);
   }
@@ -128,6 +130,7 @@ if __name__ == '__main__':
     return gets;
   }
 
+  @Override
   public int set(int index, long[] arr, int off, int len) {
     assert len > 0 : "len must be > 0 (got " + len + ")";
     assert index >= 0 && index < valueCount;
@@ -158,6 +161,7 @@ if __name__ == '__main__':
     return gets;
   }
 
+  @Override
   public int set(int index, long[] arr, int off, int len) {
     assert len > 0 : "len must be > 0 (got " + len + ")";
     assert index >= 0 && index < valueCount;
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/gen_Packed64SingleBlock.py b/lucene/core/src/java/org/apache/lucene/util/packed/gen_Packed64SingleBlock.py
index f925762..c21cb36 100644
--- a/lucene/core/src/java/org/apache/lucene/util/packed/gen_Packed64SingleBlock.py
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/gen_Packed64SingleBlock.py
@@ -77,8 +77,13 @@ abstract class Packed64SingleBlock extends PackedInts.MutableImpl {
     Arrays.fill(blocks, 0L);
   }
 
+  @Override
   public long ramBytesUsed() {
-    return RamUsageEstimator.sizeOf(blocks);
+    return RamUsageEstimator.alignObjectSize(
+        RamUsageEstimator.NUM_BYTES_OBJECT_HEADER
+        + 2 * RamUsageEstimator.NUM_BYTES_INT     // valueCount,bitsPerValue
+        + RamUsageEstimator.NUM_BYTES_OBJECT_REF) // blocks ref
+        + RamUsageEstimator.sizeOf(blocks);
   }
 
   @Override
@@ -106,8 +111,8 @@ abstract class Packed64SingleBlock extends PackedInts.MutableImpl {
     // bulk get
     assert index %% valuesPerBlock == 0;
     final PackedInts.Decoder decoder = BulkOperation.of(PackedInts.Format.PACKED_SINGLE_BLOCK, bitsPerValue);
-    assert decoder.blockCount() == 1;
-    assert decoder.valueCount() == valuesPerBlock;
+    assert decoder.longBlockCount() == 1;
+    assert decoder.longValueCount() == valuesPerBlock;
     final int blockIndex = index / valuesPerBlock;
     final int nblocks = (index + len) / valuesPerBlock - blockIndex;
     decoder.decode(blocks, blockIndex, arr, off, nblocks);
@@ -150,8 +155,8 @@ abstract class Packed64SingleBlock extends PackedInts.MutableImpl {
     // bulk set
     assert index %% valuesPerBlock == 0;
     final BulkOperation op = BulkOperation.of(PackedInts.Format.PACKED_SINGLE_BLOCK, bitsPerValue);
-    assert op.blockCount() == 1;
-    assert op.valueCount() == valuesPerBlock;
+    assert op.longBlockCount() == 1;
+    assert op.longValueCount() == valuesPerBlock;
     final int blockIndex = index / valuesPerBlock;
     final int nblocks = (index + len) / valuesPerBlock - blockIndex;
     op.encode(arr, off, blocks, blockIndex, nblocks);
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/package.html b/lucene/core/src/java/org/apache/lucene/util/packed/package.html
index 50470dd..9833f80 100644
--- a/lucene/core/src/java/org/apache/lucene/util/packed/package.html
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/package.html
@@ -50,16 +50,19 @@
     <li><b>{@link org.apache.lucene.util.packed.PagedGrowableWriter}</b><ul>
         <li>Slices data into fixed-size blocks stored in GrowableWriters.</li>
         <li>Supports more than 2B values.</li>
-        <li>You should use AppendingLongBuffer instead if you don't need random write access.</li>
+        <li>You should use Appending(Delta)PackedLongBuffer instead if you don't need random write access.</li>
     </ul></li>
-    <li><b>{@link org.apache.lucene.util.packed.AppendingLongBuffer}</b><ul>
+    <li><b>{@link org.apache.lucene.util.packed.AppendingDeltaPackedLongBuffer}</b><ul>
         <li>Can store any sequence of longs.</li>
         <li>Compression is good when values are close to each other.</li>
         <li>Supports random reads, but only sequential writes.</li>
         <li>Can address up to 2^42 values.</li>
     </ul></li>
+    <li><b>{@link org.apache.lucene.util.packed.AppendingPackedLongBuffer}</b><ul>
+        <li>Same as AppendingDeltaPackedLongBuffer but assumes values are 0-based.</li>
+    </ul></li>
     <li><b>{@link org.apache.lucene.util.packed.MonotonicAppendingLongBuffer}</b><ul>
-        <li>Same as AppendingLongBuffer except that compression is good when the stream is a succession of affine functions.</li>
+        <li>Same as AppendingDeltaPackedLongBuffer except that compression is good when the stream is a succession of affine functions.</li>
     </ul></li>
 </ul>
 
diff --git a/lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java b/lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java
index 1bac429..1aacfa6 100644
--- a/lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java
+++ b/lucene/core/src/test/org/apache/lucene/analysis/TestMockAnalyzer.java
@@ -1,7 +1,9 @@
 package org.apache.lucene.analysis;
 
+import java.io.Reader;
 import java.io.StringReader;
 import java.util.Arrays;
+import java.util.Random;
 
 import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.util.automaton.Automaton;
@@ -128,4 +130,29 @@ public class TestMockAnalyzer extends BaseTokenStreamTestCase {
       ts.close();
     }
   }
+  
+  public void testWrapReader() throws Exception {
+    // LUCENE-5153: test that wrapping an analyzer's reader is allowed
+    final Random random = random();
+    
+    Analyzer a = new AnalyzerWrapper() {
+      
+      @Override
+      protected Reader wrapReader(String fieldName, Reader reader) {
+        return new MockCharFilter(reader, 7);
+      }
+      
+      @Override
+      protected TokenStreamComponents wrapComponents(String fieldName, TokenStreamComponents components) {
+        return components;
+      }
+      
+      @Override
+      protected Analyzer getWrappedAnalyzer(String fieldName) {
+        return new MockAnalyzer(random);
+      }
+    };
+    
+    checkOneTerm(a, "abc", "aabc");
+  }
 }
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat.java
index 7a95362..7294315 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/compressing/TestCompressingTermVectorsFormat.java
@@ -1,7 +1,18 @@
 package org.apache.lucene.codecs.compressing;
 
 import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.BaseTermVectorsFormatTestCase;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -27,4 +38,35 @@ public class TestCompressingTermVectorsFormat extends BaseTermVectorsFormatTestC
     return CompressingCodec.randomInstance(random());
   }
   
+  // https://issues.apache.org/jira/browse/LUCENE-5156
+  public void testNoOrds() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
+    Document doc = new Document();
+    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
+    ft.setStoreTermVectors(true);
+    doc.add(new Field("foo", "this is a test", ft));
+    iw.addDocument(doc);
+    AtomicReader ir = getOnlySegmentReader(iw.getReader());
+    Terms terms = ir.getTermVector(0, "foo");
+    assertNotNull(terms);
+    TermsEnum termsEnum = terms.iterator(null);
+    assertEquals(SeekStatus.FOUND, termsEnum.seekCeil(new BytesRef("this")));
+    try {
+      termsEnum.ord();
+      fail();
+    } catch (UnsupportedOperationException expected) {
+      // expected exception
+    }
+    
+    try {
+      termsEnum.seekExact(0);
+      fail();
+    } catch (UnsupportedOperationException expected) {
+      // expected exception
+    }
+    ir.close();
+    iw.close();
+    dir.close();
+  }
 }
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java
index 3995049..199690f 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestLucene40PostingsFormat.java
@@ -24,7 +24,7 @@ import org.apache.lucene.index.BasePostingsFormatTestCase;
  * Tests Lucene40PostingsFormat
  */
 public class TestLucene40PostingsFormat extends BasePostingsFormatTestCase {
-  private final Codec codec = new Lucene40Codec();
+  private final Codec codec = new Lucene40RWCodec();
 
   @Override
   protected Codec getCodec() {
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
index c5aa644..848b7e6 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
@@ -162,7 +162,7 @@ public class TestReuseDocsEnum extends LuceneTestCase {
       return null;
     }
     TermsEnum iterator = terms.iterator(null);
-    if (iterator.seekExact(term, true)) {
+    if (iterator.seekExact(term)) {
       return iterator.docs(bits, null, random().nextBoolean() ? DocsEnum.FLAG_FREQS : DocsEnum.FLAG_NONE);
     }
     return null;
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat2.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat2.java
index 00bbcf3..7f8287b 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat2.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat2.java
@@ -46,7 +46,7 @@ public class TestBlockPostingsFormat2 extends LuceneTestCase {
     dir = newFSDirectory(_TestUtil.getTempDir("testDFBlockSize"));
     iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene41PostingsFormat()));
-    iw = new RandomIndexWriter(random(), dir, iwc);
+    iw = new RandomIndexWriter(random(), dir, iwc.clone());
     iw.setDoRandomForceMerge(false); // we will ourselves
   }
   
@@ -55,7 +55,7 @@ public class TestBlockPostingsFormat2 extends LuceneTestCase {
     iw.close();
     _TestUtil.checkIndex(dir); // for some extra coverage, checkIndex before we forceMerge
     iwc.setOpenMode(OpenMode.APPEND);
-    IndexWriter iw = new IndexWriter(dir, iwc);
+    IndexWriter iw = new IndexWriter(dir, iwc.clone());
     iw.forceMerge(1);
     iw.close();
     dir.close(); // just force a checkindex for now
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java
index 19d8a1a..5b0e2d7 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/lucene41/TestBlockPostingsFormat3.java
@@ -86,7 +86,7 @@ public class TestBlockPostingsFormat3 extends LuceneTestCase {
     iwc.setCodec(_TestUtil.alwaysPostingsFormat(new Lucene41PostingsFormat())); 
     // TODO we could actually add more fields implemented with different PFs
     // or, just put this test into the usual rotation?
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc.clone());
     Document doc = new Document();
     FieldType docsOnlyType = new FieldType(TextField.TYPE_NOT_STORED);
     // turn this on for a cross-check
@@ -138,7 +138,7 @@ public class TestBlockPostingsFormat3 extends LuceneTestCase {
     verify(dir);
     _TestUtil.checkIndex(dir); // for some extra coverage, checkIndex before we forceMerge
     iwc.setOpenMode(OpenMode.APPEND);
-    IndexWriter iw2 = new IndexWriter(dir, iwc);
+    IndexWriter iw2 = new IndexWriter(dir, iwc.clone());
     iw2.forceMerge(1);
     iw2.close();
     verify(dir);
@@ -235,21 +235,21 @@ public class TestBlockPostingsFormat3 extends LuceneTestCase {
       leftEnum = leftTerms.iterator(leftEnum);
       rightEnum = rightTerms.iterator(rightEnum);
       
-      assertEquals(leftEnum.seekExact(b, false), rightEnum.seekExact(b, false));
-      assertEquals(leftEnum.seekExact(b, true), rightEnum.seekExact(b, true));
+      assertEquals(leftEnum.seekExact(b), rightEnum.seekExact(b));
+      assertEquals(leftEnum.seekExact(b), rightEnum.seekExact(b));
       
       SeekStatus leftStatus;
       SeekStatus rightStatus;
       
-      leftStatus = leftEnum.seekCeil(b, false);
-      rightStatus = rightEnum.seekCeil(b, false);
+      leftStatus = leftEnum.seekCeil(b);
+      rightStatus = rightEnum.seekCeil(b);
       assertEquals(leftStatus, rightStatus);
       if (leftStatus != SeekStatus.END) {
         assertEquals(leftEnum.term(), rightEnum.term());
       }
       
-      leftStatus = leftEnum.seekCeil(b, true);
-      rightStatus = rightEnum.seekCeil(b, true);
+      leftStatus = leftEnum.seekCeil(b);
+      rightStatus = rightEnum.seekCeil(b);
       assertEquals(leftStatus, rightStatus);
       if (leftStatus != SeekStatus.END) {
         assertEquals(leftEnum.term(), rightEnum.term());
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat2.java b/lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat2.java
index d1eed42..8d9f883 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat2.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat2.java
@@ -192,7 +192,7 @@ public class TestPerFieldPostingsFormat2 extends LuceneTestCase {
     if (VERBOSE) {
       System.out.println("\nTEST: assertQuery " + t);
     }
-    IndexReader reader = DirectoryReader.open(dir, 1);
+    IndexReader reader = DirectoryReader.open(dir);
     IndexSearcher searcher = newSearcher(reader);
     TopDocs search = searcher.search(new TermQuery(t), num + 10);
     assertEquals(num, search.totalHits);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java b/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
old mode 100755
new mode 100644
index defc848..2d34a42
--- a/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestAddIndexes.java
@@ -43,6 +43,7 @@ import org.apache.lucene.store.BaseDirectoryWrapper;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.MockDirectoryWrapper;
 import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
 
@@ -1205,4 +1206,53 @@ public class TestAddIndexes extends LuceneTestCase {
     r3.close();
     d3.close();
   }
+  
+  public void testAddEmpty() throws Exception {
+    Directory d1 = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d1);
+    MultiReader empty = new MultiReader();
+    w.addIndexes(empty);
+    w.close();
+    DirectoryReader dr = DirectoryReader.open(d1);
+    for (AtomicReaderContext ctx : dr.leaves()) {
+      assertTrue("empty segments should be dropped by addIndexes", ctx.reader().maxDoc() > 0);
+    }
+    dr.close();
+    d1.close();
+  }
+
+  // Currently it's impossible to end up with a segment with all documents
+  // deleted, as such segments are dropped. Still, to validate that addIndexes
+  // works with such segments, or readers that end up in such state, we fake an
+  // all deleted segment.
+  public void testFakeAllDeleted() throws Exception {
+    Directory src = newDirectory(), dest = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), src);
+    w.addDocument(new Document());
+    IndexReader allDeletedReader = new FilterAtomicReader(w.getReader().leaves().get(0).reader()) {
+      @Override
+      public Bits getLiveDocs() {
+        return new Bits() {
+          @Override public int length() { return 1; }
+          @Override public boolean get(int index) { return false; }
+        };
+      }
+      @Override public boolean hasDeletions() { return true; }
+      @Override public int numDocs() { return 0; }
+    };
+    w.close();
+    
+    w = new RandomIndexWriter(random(), dest);
+    w.addIndexes(allDeletedReader);
+    w.close();
+    DirectoryReader dr = DirectoryReader.open(src);
+    for (AtomicReaderContext ctx : dr.leaves()) {
+      assertTrue("empty segments should be dropped by addIndexes", ctx.reader().maxDoc() > 0);
+    }
+    dr.close();
+    allDeletedReader.close();
+    src.close();
+    dest.close();
+  }
+
 }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java b/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
index 1610761..8e56129 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestCodecs.java
@@ -296,7 +296,7 @@ public class TestCodecs extends LuceneTestCase {
     Codec codec = Codec.getDefault();
     final SegmentInfo si = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, SEGMENT, 10000, false, codec, null, null);
 
-    final FieldsProducer reader = codec.postingsFormat().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, newIOContext(random()), DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR));
+    final FieldsProducer reader = codec.postingsFormat().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, newIOContext(random())));
 
     final Iterator<String> fieldsEnum = reader.iterator();
     String fieldName = fieldsEnum.next();
@@ -357,7 +357,7 @@ public class TestCodecs extends LuceneTestCase {
     if (VERBOSE) {
       System.out.println("TEST: now read postings");
     }
-    final FieldsProducer terms = codec.postingsFormat().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, newIOContext(random()), DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR));
+    final FieldsProducer terms = codec.postingsFormat().fieldsProducer(new SegmentReadState(dir, si, fieldInfos, newIOContext(random())));
 
     final Verify[] threads = new Verify[NUM_TEST_THREADS-1];
     for(int i=0;i<NUM_TEST_THREADS-1;i++) {
@@ -655,10 +655,9 @@ public class TestCodecs extends LuceneTestCase {
 
   private void write(final FieldInfos fieldInfos, final Directory dir, final FieldData[] fields) throws Throwable {
 
-    final int termIndexInterval = _TestUtil.nextInt(random(), 13, 27);
     final Codec codec = Codec.getDefault();
     final SegmentInfo si = new SegmentInfo(dir, Constants.LUCENE_MAIN_VERSION, SEGMENT, 10000, false, codec, null, null);
-    final SegmentWriteState state = new SegmentWriteState(InfoStream.getDefault(), dir, si, fieldInfos, termIndexInterval, null, newIOContext(random()));
+    final SegmentWriteState state = new SegmentWriteState(InfoStream.getDefault(), dir, si, fieldInfos, null, newIOContext(random()));
 
     final FieldsConsumer consumer = codec.postingsFormat().fieldsConsumer(state);
     Arrays.sort(fields);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java b/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
index 102b7c9..4f7ca8f 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
@@ -822,54 +822,6 @@ public void testFilesOpenClose() throws IOException {
     dir.close();
   }
   
-  // LUCENE-1609: don't load terms index
-  public void testNoTermsIndex() throws Throwable {
-    Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(_TestUtil.alwaysPostingsFormat(new Lucene41PostingsFormat())));
-    Document doc = new Document();
-    doc.add(newTextField("field", "a b c d e f g h i j k l m n o p q r s t u v w x y z", Field.Store.NO));
-    doc.add(newTextField("number", "0 1 2 3 4 5 6 7 8 9", Field.Store.NO));
-    writer.addDocument(doc);
-    writer.addDocument(doc);
-    writer.close();
-  
-    DirectoryReader r = DirectoryReader.open(dir, -1);
-    try {
-      r.docFreq(new Term("field", "f"));
-      fail("did not hit expected exception");
-    } catch (IllegalStateException ise) {
-      // expected
-    }
-  
-    assertEquals(-1, ((SegmentReader) r.leaves().get(0).reader()).getTermInfosIndexDivisor());
-    writer = new IndexWriter(
-        dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).
-            setCodec(_TestUtil.alwaysPostingsFormat(new Lucene41PostingsFormat())).
-            setMergePolicy(newLogMergePolicy(10))
-    );
-    writer.addDocument(doc);
-    writer.close();
-  
-    // LUCENE-1718: ensure re-open carries over no terms index:
-    DirectoryReader r2 = DirectoryReader.openIfChanged(r);
-    assertNotNull(r2);
-    assertNull(DirectoryReader.openIfChanged(r2));
-    r.close();
-    List<AtomicReaderContext> leaves = r2.leaves();
-    assertEquals(2, leaves.size());
-    for(AtomicReaderContext ctx : leaves) {
-      try {
-        ctx.reader().docFreq(new Term("field", "f"));
-        fail("did not hit expected exception");
-      } catch (IllegalStateException ise) {
-        // expected
-      }
-    }
-    r2.close();
-    dir.close();
-  }
-  
   // LUCENE-2046
   public void testPrepareCommitIsCurrent() throws Throwable {
     Directory dir = newDirectory();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDoc.java b/lucene/core/src/test/org/apache/lucene/index/TestDoc.java
index 04b397b..7557b29 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDoc.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDoc.java
@@ -212,15 +212,15 @@ public class TestDoc extends LuceneTestCase {
    private SegmentInfoPerCommit merge(Directory dir, SegmentInfoPerCommit si1, SegmentInfoPerCommit si2, String merged, boolean useCompoundFile)
    throws Exception {
       IOContext context = newIOContext(random());
-      SegmentReader r1 = new SegmentReader(si1, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, context);
-      SegmentReader r2 = new SegmentReader(si2, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, context);
+      SegmentReader r1 = new SegmentReader(si1, context);
+      SegmentReader r2 = new SegmentReader(si2, context);
 
       final Codec codec = Codec.getDefault();
       TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(si1.info.dir);
       final SegmentInfo si = new SegmentInfo(si1.info.dir, Constants.LUCENE_MAIN_VERSION, merged, -1, false, codec, null, null);
 
       SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(r1, r2),
-          si, InfoStream.getDefault(), trackingDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,
+          si, InfoStream.getDefault(), trackingDir,
           MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), context);
 
       MergeState mergeState = merger.merge();
@@ -245,7 +245,7 @@ public class TestDoc extends LuceneTestCase {
 
    private void printSegment(PrintWriter out, SegmentInfoPerCommit si)
    throws Exception {
-      SegmentReader reader = new SegmentReader(si, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
+      SegmentReader reader = new SegmentReader(si, newIOContext(random()));
 
       for (int i = 0; i < reader.numDocs(); i++)
         out.println(reader.document(i));
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java b/lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java
index d6bccca..51af43e 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDocTermOrds.java
@@ -332,7 +332,7 @@ public class TestDocTermOrds extends LuceneTestCase {
         Terms terms = MultiFields.getTerms(r, "field");
         if (terms != null) {
           TermsEnum termsEnum = terms.iterator(null);
-          TermsEnum.SeekStatus result = termsEnum.seekCeil(prefixRef, false);
+          TermsEnum.SeekStatus result = termsEnum.seekCeil(prefixRef);
           if (result != TermsEnum.SeekStatus.END) {
             assertFalse("term=" + termsEnum.term().utf8ToString() + " matches prefix=" + prefixRef.utf8ToString(), StringHelper.startsWith(termsEnum.term(), prefixRef));
           } else {
@@ -454,16 +454,16 @@ public class TestDocTermOrds extends LuceneTestCase {
     assertEquals(SeekStatus.END, termsEnum.seekCeil(new BytesRef("zzz")));
     
     // seekExact()
-    assertTrue(termsEnum.seekExact(new BytesRef("beer"), true));
+    assertTrue(termsEnum.seekExact(new BytesRef("beer")));
     assertEquals("beer", termsEnum.term().utf8ToString());
     assertEquals(0, termsEnum.ord());
-    assertTrue(termsEnum.seekExact(new BytesRef("hello"), true));
+    assertTrue(termsEnum.seekExact(new BytesRef("hello")));
     assertEquals("hello", termsEnum.term().utf8ToString());
     assertEquals(1, termsEnum.ord());
-    assertTrue(termsEnum.seekExact(new BytesRef("world"), true));
+    assertTrue(termsEnum.seekExact(new BytesRef("world")));
     assertEquals("world", termsEnum.term().utf8ToString());
     assertEquals(2, termsEnum.ord());
-    assertFalse(termsEnum.seekExact(new BytesRef("bogus"), true));
+    assertFalse(termsEnum.seekExact(new BytesRef("bogus")));
     
     // seek(ord)
     termsEnum.seekExact(0);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java b/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
index d79a081..a458182 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
@@ -558,13 +558,13 @@ public class TestDocValuesIndexing extends LuceneTestCase {
   public void testIllegalTypeChangeAcrossSegments() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter writer = new IndexWriter(dir, conf);
+    IndexWriter writer = new IndexWriter(dir, conf.clone());
     Document doc = new Document();
     doc.add(new NumericDocValuesField("dv", 0L));
     writer.addDocument(doc);
     writer.close();
 
-    writer = new IndexWriter(dir, conf);
+    writer = new IndexWriter(dir, conf.clone());
     doc = new Document();
     doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
     try {
@@ -580,13 +580,13 @@ public class TestDocValuesIndexing extends LuceneTestCase {
   public void testTypeChangeAfterCloseAndDeleteAll() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter writer = new IndexWriter(dir, conf);
+    IndexWriter writer = new IndexWriter(dir, conf.clone());
     Document doc = new Document();
     doc.add(new NumericDocValuesField("dv", 0L));
     writer.addDocument(doc);
     writer.close();
 
-    writer = new IndexWriter(dir, conf);
+    writer = new IndexWriter(dir, conf.clone());
     writer.deleteAll();
     doc = new Document();
     doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
@@ -629,13 +629,13 @@ public class TestDocValuesIndexing extends LuceneTestCase {
   public void testTypeChangeAfterOpenCreate() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter writer = new IndexWriter(dir, conf);
+    IndexWriter writer = new IndexWriter(dir, conf.clone());
     Document doc = new Document();
     doc.add(new NumericDocValuesField("dv", 0L));
     writer.addDocument(doc);
     writer.close();
     conf.setOpenMode(IndexWriterConfig.OpenMode.CREATE);
-    writer = new IndexWriter(dir, conf);
+    writer = new IndexWriter(dir, conf.clone());
     doc = new Document();
     doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
     writer.addDocument(doc);
@@ -646,14 +646,14 @@ public class TestDocValuesIndexing extends LuceneTestCase {
   public void testTypeChangeViaAddIndexes() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter writer = new IndexWriter(dir, conf);
+    IndexWriter writer = new IndexWriter(dir, conf.clone());
     Document doc = new Document();
     doc.add(new NumericDocValuesField("dv", 0L));
     writer.addDocument(doc);
     writer.close();
 
     Directory dir2 = newDirectory();
-    writer = new IndexWriter(dir2, conf);
+    writer = new IndexWriter(dir2, conf.clone());
     doc = new Document();
     doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
     writer.addDocument(doc);
@@ -672,14 +672,14 @@ public class TestDocValuesIndexing extends LuceneTestCase {
   public void testTypeChangeViaAddIndexesIR() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter writer = new IndexWriter(dir, conf);
+    IndexWriter writer = new IndexWriter(dir, conf.clone());
     Document doc = new Document();
     doc.add(new NumericDocValuesField("dv", 0L));
     writer.addDocument(doc);
     writer.close();
 
     Directory dir2 = newDirectory();
-    writer = new IndexWriter(dir2, conf);
+    writer = new IndexWriter(dir2, conf.clone());
     doc = new Document();
     doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
     writer.addDocument(doc);
@@ -700,14 +700,14 @@ public class TestDocValuesIndexing extends LuceneTestCase {
   public void testTypeChangeViaAddIndexes2() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter writer = new IndexWriter(dir, conf);
+    IndexWriter writer = new IndexWriter(dir, conf.clone());
     Document doc = new Document();
     doc.add(new NumericDocValuesField("dv", 0L));
     writer.addDocument(doc);
     writer.close();
 
     Directory dir2 = newDirectory();
-    writer = new IndexWriter(dir2, conf);
+    writer = new IndexWriter(dir2, conf.clone());
     writer.addIndexes(dir);
     doc = new Document();
     doc.add(new SortedDocValuesField("dv", new BytesRef("foo")));
@@ -725,14 +725,14 @@ public class TestDocValuesIndexing extends LuceneTestCase {
   public void testTypeChangeViaAddIndexesIR2() throws Exception {
     Directory dir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter writer = new IndexWriter(dir, conf);
+    IndexWriter writer = new IndexWriter(dir, conf.clone());
     Document doc = new Document();
     doc.add(new NumericDocValuesField("dv", 0L));
     writer.addDocument(doc);
     writer.close();
 
     Directory dir2 = newDirectory();
-    writer = new IndexWriter(dir2, conf);
+    writer = new IndexWriter(dir2, conf.clone());
     IndexReader[] readers = new IndexReader[] {DirectoryReader.open(dir)};
     writer.addIndexes(readers);
     readers[0].close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java b/lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java
index 80d0d73..c4423bb 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDocsAndPositions.java
@@ -95,7 +95,7 @@ public class TestDocsAndPositions extends LuceneTestCase {
     Terms terms = reader.terms(fieldName);
     if (terms != null) {
       TermsEnum te = terms.iterator(null);
-      if (te.seekExact(bytes, true)) {
+      if (te.seekExact(bytes)) {
         return te.docsAndPositions(liveDocs, null);
       }
     }
@@ -341,7 +341,7 @@ public class TestDocsAndPositions extends LuceneTestCase {
     
     // now reuse and check again
     TermsEnum te = r.terms("foo").iterator(null);
-    assertTrue(te.seekExact(new BytesRef("bar"), true));
+    assertTrue(te.seekExact(new BytesRef("bar")));
     disi = _TestUtil.docs(random(), te, null, disi, DocsEnum.FLAG_NONE);
     docid = disi.docID();
     assertEquals(-1, docid);
@@ -366,7 +366,7 @@ public class TestDocsAndPositions extends LuceneTestCase {
     
     // now reuse and check again
     TermsEnum te = r.terms("foo").iterator(null);
-    assertTrue(te.seekExact(new BytesRef("bar"), true));
+    assertTrue(te.seekExact(new BytesRef("bar")));
     disi = te.docsAndPositions(null, disi);
     docid = disi.docID();
     assertEquals(-1, docid);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java b/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java
index e36479e..1984258 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDocumentWriter.java
@@ -65,7 +65,7 @@ public class TestDocumentWriter extends LuceneTestCase {
     SegmentInfoPerCommit info = writer.newestSegment();
     writer.close();
     //After adding the document, we should be able to read it back in
-    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
+    SegmentReader reader = new SegmentReader(info, newIOContext(random()));
     assertTrue(reader != null);
     StoredDocument doc = reader.document(0);
     assertTrue(doc != null);
@@ -126,7 +126,7 @@ public class TestDocumentWriter extends LuceneTestCase {
     writer.commit();
     SegmentInfoPerCommit info = writer.newestSegment();
     writer.close();
-    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
+    SegmentReader reader = new SegmentReader(info, newIOContext(random()));
 
     DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, MultiFields.getLiveDocs(reader),
                                                                           "repeated", new BytesRef("repeated"));
@@ -198,7 +198,7 @@ public class TestDocumentWriter extends LuceneTestCase {
     writer.commit();
     SegmentInfoPerCommit info = writer.newestSegment();
     writer.close();
-    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
+    SegmentReader reader = new SegmentReader(info, newIOContext(random()));
 
     DocsAndPositionsEnum termPositions = MultiFields.getTermPositionsEnum(reader, reader.getLiveDocs(), "f1", new BytesRef("a"));
     assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
@@ -241,7 +241,7 @@ public class TestDocumentWriter extends LuceneTestCase {
     writer.commit();
     SegmentInfoPerCommit info = writer.newestSegment();
     writer.close();
-    SegmentReader reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
+    SegmentReader reader = new SegmentReader(info, newIOContext(random()));
 
     DocsAndPositionsEnum termPositions = reader.termPositionsEnum(new Term("preanalyzed", "term1"));
     assertTrue(termPositions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
index 8caafd6..1db3d81 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -1305,36 +1305,6 @@ public class TestIndexWriter extends LuceneTestCase {
     dir.close();
   }
 
-  public void testIndexDivisor() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig config = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    config.setTermIndexInterval(2);
-    IndexWriter w = new IndexWriter(dir, config);
-    StringBuilder s = new StringBuilder();
-    // must be > 256
-    for(int i=0;i<300;i++) {
-      s.append(' ').append(i);
-    }
-    Document d = new Document();
-    Field f = newTextField("field", s.toString(), Field.Store.NO);
-    d.add(f);
-    w.addDocument(d);
-
-    AtomicReader r = getOnlySegmentReader(w.getReader());
-    TermsEnum t = r.fields().terms("field").iterator(null);
-    int count = 0;
-    while(t.next() != null) {
-      final DocsEnum docs = _TestUtil.docs(random(), t, null, null, DocsEnum.FLAG_NONE);
-      assertEquals(0, docs.nextDoc());
-      assertEquals(DocIdSetIterator.NO_MORE_DOCS, docs.nextDoc());
-      count++;
-    }
-    assertEquals(300, count);
-    r.close();
-    w.close();
-    dir.close();
-  }
-
   public void testDeleteUnusedFiles() throws Exception {
     for(int iter=0;iter<2;iter++) {
       Directory dir = newMockDirectory(); // relies on windows semantics
@@ -1716,20 +1686,6 @@ public class TestIndexWriter extends LuceneTestCase {
     dir.close();
   }
 
-  // LUCENE-3183
-  public void testEmptyFieldNameTIIOne() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setTermIndexInterval(1);
-    iwc.setReaderTermsIndexDivisor(1);
-    IndexWriter writer = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(newTextField("", "a b c", Field.Store.NO));
-    writer.addDocument(doc);
-    writer.close();
-    dir.close();
-  }
-
   public void testDeleteAllNRTLeftoverFiles() throws Exception {
 
     Directory d = new MockDirectoryWrapper(random(), new RAMDirectory());
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
index 9bc0802..bdd83a7 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
@@ -37,6 +37,7 @@ import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.SetOnce.AlreadySetException;
 import org.junit.Test;
 
 public class TestIndexWriterConfig extends LuceneTestCase {
@@ -65,7 +66,6 @@ public class TestIndexWriterConfig extends LuceneTestCase {
     assertEquals(OpenMode.CREATE_OR_APPEND, conf.getOpenMode());
     // we don't need to assert this, it should be unspecified
     assertTrue(IndexSearcher.getDefaultSimilarity() == conf.getSimilarity());
-    assertEquals(IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL, conf.getTermIndexInterval());
     assertEquals(IndexWriterConfig.getDefaultWriteLockTimeout(), conf.getWriteLockTimeout());
     assertEquals(IndexWriterConfig.WRITE_LOCK_TIMEOUT, IndexWriterConfig.getDefaultWriteLockTimeout());
     assertEquals(IndexWriterConfig.DEFAULT_MAX_BUFFERED_DELETE_TERMS, conf.getMaxBufferedDeleteTerms());
@@ -74,7 +74,6 @@ public class TestIndexWriterConfig extends LuceneTestCase {
     assertEquals(IndexWriterConfig.DEFAULT_READER_POOLING, conf.getReaderPooling());
     assertTrue(DocumentsWriterPerThread.defaultIndexingChain == conf.getIndexingChain());
     assertNull(conf.getMergedSegmentWarmer());
-    assertEquals(IndexWriterConfig.DEFAULT_READER_TERMS_INDEX_DIVISOR, conf.getReaderTermsIndexDivisor());
     assertEquals(TieredMergePolicy.class, conf.getMergePolicy().getClass());
     assertEquals(ThreadAffinityDocumentsWriterThreadPool.class, conf.getIndexerThreadPool().getClass());
     assertEquals(FlushByRamOrCountsPolicy.class, conf.getFlushPolicy().getClass());
@@ -91,7 +90,6 @@ public class TestIndexWriterConfig extends LuceneTestCase {
     getters.add("getMergeScheduler");
     getters.add("getOpenMode");
     getters.add("getSimilarity");
-    getters.add("getTermIndexInterval");
     getters.add("getWriteLockTimeout");
     getters.add("getDefaultWriteLockTimeout");
     getters.add("getMaxBufferedDeleteTerms");
@@ -103,7 +101,6 @@ public class TestIndexWriterConfig extends LuceneTestCase {
     getters.add("getMaxThreadStates");
     getters.add("getReaderPooling");
     getters.add("getIndexerThreadPool");
-    getters.add("getReaderTermsIndexDivisor");
     getters.add("getFlushPolicy");
     getters.add("getRAMPerThreadHardLimitMB");
     getters.add("getCodec");
@@ -145,18 +142,30 @@ public class TestIndexWriterConfig extends LuceneTestCase {
   @Test
   public void testReuse() throws Exception {
     Directory dir = newDirectory();
-    // test that if the same IWC is reused across two IWs, it is cloned by each.
+    // test that IWC cannot be reused across two IWs
     IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, conf);
-    LiveIndexWriterConfig liveConf1 = iw.w.getConfig();
-    iw.close();
-    
-    iw = new RandomIndexWriter(random(), dir, conf);
-    LiveIndexWriterConfig liveConf2 = iw.w.getConfig();
-    iw.close();
+    new RandomIndexWriter(random(), dir, conf).close();
+
+    // this should fail
+    try {
+      assertNotNull(new RandomIndexWriter(random(), dir, conf));
+      fail("should have hit AlreadySetException");
+    } catch (AlreadySetException e) {
+      // expected
+    }
+
+    // also cloning it won't help, after it has been used already
+    try {
+      assertNotNull(new RandomIndexWriter(random(), dir, conf.clone()));
+      fail("should have hit AlreadySetException");
+    } catch (AlreadySetException e) {
+      // expected
+    }
     
-    // LiveIndexWriterConfig's "copy" constructor doesn't clone objects.
-    assertNotSame("IndexWriterConfig should have been cloned", liveConf1.getMergePolicy(), liveConf2.getMergePolicy());
+    // if it's cloned in advance, it should be ok
+    conf = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
+    new RandomIndexWriter(random(), dir, conf.clone()).close();
+    new RandomIndexWriter(random(), dir, conf.clone()).close();
     
     dir.close();
   }
@@ -187,14 +196,12 @@ public class TestIndexWriterConfig extends LuceneTestCase {
   public void testConstants() throws Exception {
     // Tests that the values of the constants does not change
     assertEquals(1000, IndexWriterConfig.WRITE_LOCK_TIMEOUT);
-    assertEquals(32, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL);
     assertEquals(-1, IndexWriterConfig.DISABLE_AUTO_FLUSH);
     assertEquals(IndexWriterConfig.DISABLE_AUTO_FLUSH, IndexWriterConfig.DEFAULT_MAX_BUFFERED_DELETE_TERMS);
     assertEquals(IndexWriterConfig.DISABLE_AUTO_FLUSH, IndexWriterConfig.DEFAULT_MAX_BUFFERED_DOCS);
     assertEquals(16.0, IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB, 0.0);
     assertEquals(false, IndexWriterConfig.DEFAULT_READER_POOLING);
     assertEquals(true, IndexWriterConfig.DEFAULT_USE_COMPOUND_FILE_SYSTEM);
-    assertEquals(DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, IndexWriterConfig.DEFAULT_READER_TERMS_INDEX_DIVISOR);
   }
 
   @Test
@@ -328,23 +335,6 @@ public class TestIndexWriterConfig extends LuceneTestCase {
     } catch (IllegalArgumentException e) {
       // this is expected
     }
-
-    // Test setReaderTermsIndexDivisor
-    try {
-      conf.setReaderTermsIndexDivisor(0);
-      fail("should not have succeeded to set termsIndexDivisor to 0");
-    } catch (IllegalArgumentException e) {
-      // this is expected
-    }
-    
-    // Setting to -1 is ok
-    conf.setReaderTermsIndexDivisor(-1);
-    try {
-      conf.setReaderTermsIndexDivisor(-2);
-      fail("should not have succeeded to set termsIndexDivisor to < -1");
-    } catch (IllegalArgumentException e) {
-      // this is expected
-    }
     
     try {
       conf.setRAMPerThreadHardLimitMB(2048);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
index db94cb9..dd8aacc 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
@@ -1152,7 +1152,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
     Directory dir = newDirectory();
     IndexWriterConfig iwc = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     iwc.setMaxBufferedDocs(2);
-    IndexWriter w = new IndexWriter(dir, iwc);
+    IndexWriter w = new IndexWriter(dir, iwc.clone());
     Document doc = new Document();
     doc.add(newField("field", "0", StringField.TYPE_NOT_STORED));
     w.addDocument(doc);
@@ -1177,7 +1177,7 @@ public class TestIndexWriterDelete extends LuceneTestCase {
 
     // Segment should have deletions:
     assertTrue(s.contains("has deletions"));
-    w = new IndexWriter(dir, iwc);
+    w = new IndexWriter(dir, iwc.clone());
     w.forceMerge(1);
     w.close();
 
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java
index e3042cb..433e49c 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterForceMerge.java
@@ -130,7 +130,6 @@ public class TestIndexWriterForceMerge extends LuceneTestCase {
     for(int j=0;j<500;j++) {
       TestIndexWriter.addDocWithIndex(writer, j);
     }
-    final int termIndexInterval = writer.getConfig().getTermIndexInterval();
     // force one extra segment w/ different doc store so
     // we see the doc stores get merged
     writer.commit();
@@ -152,10 +151,7 @@ public class TestIndexWriterForceMerge extends LuceneTestCase {
     dir.resetMaxUsedSizeInBytes();
     dir.setTrackDiskUsage(true);
 
-    // Import to use same term index interval else a
-    // smaller one here could increase the disk usage and
-    // cause a false failure:
-    writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setTermIndexInterval(termIndexInterval).setMergePolicy(newLogMergePolicy()));
+    writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setOpenMode(OpenMode.APPEND).setMergePolicy(newLogMergePolicy()));
     writer.forceMerge(1);
     writer.close();
     long maxDiskUsage = dir.getMaxUsedSizeInBytes();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
index 859c7af..64841cd 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterReader.java
@@ -314,7 +314,7 @@ public class TestIndexWriterReader extends LuceneTestCase {
     boolean doFullMerge = true;
 
     Directory dir1 = newDirectory();
-    IndexWriter writer = new IndexWriter(dir1, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setReaderTermsIndexDivisor(2));
+    IndexWriter writer = new IndexWriter(dir1, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())));
     // create the index
     createIndexNoClose(!doFullMerge, "index1", writer);
     writer.flush(false, true);
@@ -1006,36 +1006,6 @@ public class TestIndexWriterReader extends LuceneTestCase {
     assertTrue(didWarm.get());
   }
   
-  public void testNoTermsIndex() throws Exception {
-    // Some Codecs don't honor the ReaderTermsIndexDivisor, so skip the test if
-    // they're picked.
-    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT,
-        new MockAnalyzer(random())).setReaderTermsIndexDivisor(-1);
-    
-    // Don't proceed if picked Codec is in the list of illegal ones.
-    final String format = _TestUtil.getPostingsFormat("f");
-    assumeFalse("Format: " + format + " does not support ReaderTermsIndexDivisor!",
-                (format.equals("SimpleText") || format.equals("Memory") || format.equals("Direct") || 
-                 format.equals("TempFST") || format.equals("TempFSTOrd")));
-
-    Directory dir = newDirectory();
-    IndexWriter w = new IndexWriter(dir, conf);
-    Document doc = new Document();
-    doc.add(new TextField("f", "val", Field.Store.NO));
-    w.addDocument(doc);
-    SegmentReader r = getOnlySegmentReader(DirectoryReader.open(w, true));
-    try {
-      _TestUtil.docs(random(), r, "f", new BytesRef("val"), null, null, DocsEnum.FLAG_NONE);
-      fail("should have failed to seek since terms index was not loaded.");
-    } catch (IllegalStateException e) {
-      // expected - we didn't load the term index
-    } finally {
-      r.close();
-      w.close();
-      dir.close();
-    }
-  }
-  
   public void testReopenAfterNoRealChange() throws Exception {
     Directory d = newDirectory();
     IndexWriter w = new IndexWriter(
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestLazyProxSkipping.java b/lucene/core/src/test/org/apache/lucene/index/TestLazyProxSkipping.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestNorms.java b/lucene/core/src/test/org/apache/lucene/index/TestNorms.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestParallelTermEnum.java b/lucene/core/src/test/org/apache/lucene/index/TestParallelTermEnum.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors.java b/lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors.java
index d1bbcca..2cc02e7 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestPayloadsOnVectors.java
@@ -70,7 +70,7 @@ public class TestPayloadsOnVectors extends LuceneTestCase {
     Terms terms = reader.getTermVector(1, "field");
     assert terms != null;
     TermsEnum termsEnum = terms.iterator(null);
-    assertTrue(termsEnum.seekExact(new BytesRef("withPayload"), true));
+    assertTrue(termsEnum.seekExact(new BytesRef("withPayload")));
     DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);
     assertEquals(0, de.nextDoc());
     assertEquals(0, de.nextPosition());
@@ -112,7 +112,7 @@ public class TestPayloadsOnVectors extends LuceneTestCase {
     Terms terms = reader.getTermVector(0, "field");
     assert terms != null;
     TermsEnum termsEnum = terms.iterator(null);
-    assertTrue(termsEnum.seekExact(new BytesRef("withPayload"), true));
+    assertTrue(termsEnum.seekExact(new BytesRef("withPayload")));
     DocsAndPositionsEnum de = termsEnum.docsAndPositions(null, null);
     assertEquals(0, de.nextDoc());
     assertEquals(3, de.nextPosition());
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java b/lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
index 9718e10..e56e6ef 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
@@ -227,7 +227,7 @@ public class TestPerSegmentDeletes extends LuceneTestCase {
     Fields fields = MultiFields.getFields(reader);
     Terms cterms = fields.terms(term.field);
     TermsEnum ctermsEnum = cterms.iterator(null);
-    if (ctermsEnum.seekExact(new BytesRef(term.text()), false)) {
+    if (ctermsEnum.seekExact(new BytesRef(term.text()))) {
       DocsEnum docsEnum = _TestUtil.docs(random(), ctermsEnum, bits, null, DocsEnum.FLAG_NONE);
       return toArray(docsEnum);
     }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java b/lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java
index a3e58a9..f25e80b 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets.java
@@ -301,7 +301,7 @@ public class TestPostingsOffsets extends LuceneTestCase {
       final FieldCache.Ints docIDToID = FieldCache.DEFAULT.getInts(sub, "id", false);
       for(String term : terms) {
         //System.out.println("  term=" + term);
-        if (termsEnum.seekExact(new BytesRef(term), random().nextBoolean())) {
+        if (termsEnum.seekExact(new BytesRef(term))) {
           docs = termsEnum.docs(null, docs);
           assertNotNull(docs);
           int doc;
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java b/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
index a950497..9ac95eb 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestSegmentMerger.java
@@ -54,8 +54,8 @@ public class TestSegmentMerger extends LuceneTestCase {
     SegmentInfoPerCommit info1 = DocHelper.writeDoc(random(), merge1Dir, doc1);
     DocHelper.setupDoc(doc2);
     SegmentInfoPerCommit info2 = DocHelper.writeDoc(random(), merge2Dir, doc2);
-    reader1 = new SegmentReader(info1, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
-    reader2 = new SegmentReader(info2, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
+    reader1 = new SegmentReader(info1, newIOContext(random()));
+    reader2 = new SegmentReader(info2, newIOContext(random()));
   }
 
   @Override
@@ -81,7 +81,7 @@ public class TestSegmentMerger extends LuceneTestCase {
     final SegmentInfo si = new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, -1, false, codec, null, null);
 
     SegmentMerger merger = new SegmentMerger(Arrays.<AtomicReader>asList(reader1, reader2),
-        si, InfoStream.getDefault(), mergedDir, IndexWriterConfig.DEFAULT_TERM_INDEX_INTERVAL,
+        si, InfoStream.getDefault(), mergedDir,
         MergeState.CheckAbort.NONE, new FieldInfos.FieldNumbers(), newIOContext(random()));
     MergeState mergeState = merger.merge();
     int docsMerged = mergeState.segmentInfo.getDocCount();
@@ -91,7 +91,7 @@ public class TestSegmentMerger extends LuceneTestCase {
                                                          new SegmentInfo(mergedDir, Constants.LUCENE_MAIN_VERSION, mergedSegment, docsMerged,
                                                                          false, codec, null, null),
                                                          0, -1L),
-                                                   DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, newIOContext(random()));
+                                                   newIOContext(random()));
     assertTrue(mergedReader != null);
     assertTrue(mergedReader.numDocs() == 2);
     StoredDocument newDoc1 = mergedReader.document(0);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java b/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
index c216126..239af28 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestSegmentReader.java
@@ -43,7 +43,7 @@ public class TestSegmentReader extends LuceneTestCase {
     dir = newDirectory();
     DocHelper.setupDoc(testDoc);
     SegmentInfoPerCommit info = DocHelper.writeDoc(random(), dir, testDoc);
-    reader = new SegmentReader(info, DirectoryReader.DEFAULT_TERMS_INDEX_DIVISOR, IOContext.READ);
+    reader = new SegmentReader(info, IOContext.READ);
   }
   
   @Override
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java b/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
index 7366a92..b3507da 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestSegmentTermDocs.java
@@ -50,16 +50,11 @@ public class TestSegmentTermDocs extends LuceneTestCase {
   public void test() {
     assertTrue(dir != null);
   }
-  
-  public void testTermDocs() throws IOException {
-    testTermDocs(1);
-  }
 
-  public void testTermDocs(int indexDivisor) throws IOException {
+  public void testTermDocs() throws IOException {
     //After adding the document, we should be able to read it back in
-    SegmentReader reader = new SegmentReader(info, indexDivisor, newIOContext(random()));
+    SegmentReader reader = new SegmentReader(info, newIOContext(random()));
     assertTrue(reader != null);
-    assertEquals(indexDivisor, reader.getTermInfosIndexDivisor());
 
     TermsEnum terms = reader.fields().terms(DocHelper.TEXT_FIELD_2_KEY).iterator(null);
     terms.seekCeil(new BytesRef("field"));
@@ -74,13 +69,9 @@ public class TestSegmentTermDocs extends LuceneTestCase {
   }  
   
   public void testBadSeek() throws IOException {
-    testBadSeek(1);
-  }
-
-  public void testBadSeek(int indexDivisor) throws IOException {
     {
       //After adding the document, we should be able to read it back in
-      SegmentReader reader = new SegmentReader(info, indexDivisor, newIOContext(random()));
+      SegmentReader reader = new SegmentReader(info, newIOContext(random()));
       assertTrue(reader != null);
       DocsEnum termDocs = _TestUtil.docs(random(), reader,
                                          "textField2",
@@ -94,7 +85,7 @@ public class TestSegmentTermDocs extends LuceneTestCase {
     }
     {
       //After adding the document, we should be able to read it back in
-      SegmentReader reader = new SegmentReader(info, indexDivisor, newIOContext(random()));
+      SegmentReader reader = new SegmentReader(info, newIOContext(random()));
       assertTrue(reader != null);
       DocsEnum termDocs = _TestUtil.docs(random(), reader,
                                          "junk",
@@ -108,10 +99,6 @@ public class TestSegmentTermDocs extends LuceneTestCase {
   }
   
   public void testSkipTo() throws IOException {
-    testSkipTo(1);
-  }
-
-  public void testSkipTo(int indexDivisor) throws IOException {
     Directory dir = newDirectory();
     IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random())).setMergePolicy(newLogMergePolicy()));
     
@@ -131,7 +118,7 @@ public class TestSegmentTermDocs extends LuceneTestCase {
     writer.forceMerge(1);
     writer.close();
     
-    IndexReader reader = DirectoryReader.open(dir, indexDivisor);
+    IndexReader reader = DirectoryReader.open(dir);
 
     DocsEnum tdocs = _TestUtil.docs(random(), reader,
                                     ta.field(),
@@ -267,15 +254,7 @@ public class TestSegmentTermDocs extends LuceneTestCase {
     reader.close();
     dir.close();
   }
-  
-  public void testIndexDivisor() throws IOException {
-    testDoc = new Document();
-    DocHelper.setupDoc(testDoc);
-    DocHelper.writeDoc(random(), dir, testDoc);
-    testTermDocs(2);
-    testBadSeek(2);
-    testSkipTo(2);
-  }
+
 
   private void addDoc(IndexWriter writer, String value) throws IOException
   {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java b/lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java
index 2d8ae61..84fd32e 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestSnapshotDeletionPolicy.java
@@ -42,7 +42,7 @@ public class TestSnapshotDeletionPolicy extends LuceneTestCase {
   public static final String INDEX_PATH = "test.snapshots";
   
   protected IndexWriterConfig getConfig(Random random, IndexDeletionPolicy dp) {
-    IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random));
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));
     if (dp != null) {
       conf.setIndexDeletionPolicy(dp);
     }
@@ -323,8 +323,8 @@ public class TestSnapshotDeletionPolicy extends LuceneTestCase {
     int numSnapshots = 2;
     Directory dir = newDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, getConfig(random(), getDeletionPolicy()));
-    SnapshotDeletionPolicy sdp = (SnapshotDeletionPolicy) writer.getConfig().getIndexDeletionPolicy();
+    SnapshotDeletionPolicy sdp = getDeletionPolicy();
+    IndexWriter writer = new IndexWriter(dir, getConfig(random(), sdp));
     prepareIndexAndSnapshots(sdp, writer, numSnapshots);
     writer.close();
 
@@ -333,8 +333,7 @@ public class TestSnapshotDeletionPolicy extends LuceneTestCase {
     // this does the actual rollback
     writer.commit();
     writer.deleteUnusedFiles();
-    //sdp = (SnapshotDeletionPolicy) writer.getConfig().getIndexDeletionPolicy();
-    assertSnapshotExists(dir, sdp, numSnapshots - 1, true);
+    assertSnapshotExists(dir, sdp, numSnapshots - 1, false);
     writer.close();
 
     // but 'snapshot1' files will still exist (need to release snapshot before they can be deleted).
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java b/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
index bc78249..9ce0b06 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestStressIndexing2.java
@@ -354,7 +354,7 @@ public class TestStressIndexing2 extends LuceneTestCase {
       }
 
       termDocs1 = _TestUtil.docs(random(), termsEnum, liveDocs1, termDocs1, DocsEnum.FLAG_NONE);
-      if (termsEnum2.seekExact(term, false)) {
+      if (termsEnum2.seekExact(term)) {
         termDocs2 = _TestUtil.docs(random(), termsEnum2, liveDocs2, termDocs2, DocsEnum.FLAG_NONE);
       } else {
         termDocs2 = null;
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java b/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java
index 63018a1..9fe274b 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTermVectorsReader.java
@@ -331,4 +331,64 @@ public class TestTermVectorsReader extends LuceneTestCase {
     }
     reader.close();
   }
+
+  public void testIllegalIndexableField() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
+    ft.setStoreTermVectors(true);
+    ft.setStoreTermVectorPayloads(true);
+    Document doc = new Document();
+    doc.add(new Field("field", "value", ft));
+    try {
+      w.addDocument(doc);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // Expected
+      assertEquals("cannot index term vector payloads without term vector positions (field=\"field\")", iae.getMessage());
+    }
+
+    ft = new FieldType(TextField.TYPE_NOT_STORED);
+    ft.setStoreTermVectors(false);
+    ft.setStoreTermVectorOffsets(true);
+    doc = new Document();
+    doc.add(new Field("field", "value", ft));
+    try {
+      w.addDocument(doc);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // Expected
+      assertEquals("cannot index term vector offsets when term vectors are not indexed (field=\"field\")", iae.getMessage());
+    }
+
+    ft = new FieldType(TextField.TYPE_NOT_STORED);
+    ft.setStoreTermVectors(false);
+    ft.setStoreTermVectorPositions(true);
+    doc = new Document();
+    doc.add(new Field("field", "value", ft));
+    try {
+      w.addDocument(doc);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // Expected
+      assertEquals("cannot index term vector positions when term vectors are not indexed (field=\"field\")", iae.getMessage());
+    }
+
+    ft = new FieldType(TextField.TYPE_NOT_STORED);
+    ft.setStoreTermVectors(false);
+    ft.setStoreTermVectorPayloads(true);
+    doc = new Document();
+    doc.add(new Field("field", "value", ft));
+    try {
+      w.addDocument(doc);
+      fail("did not hit exception");
+    } catch (IllegalArgumentException iae) {
+      // Expected
+      assertEquals("cannot index term vector payloads when term vectors are not indexed (field=\"field\")", iae.getMessage());
+    }
+
+    w.close();
+    
+    dir.close();
+  }
 }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java b/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
index 6249983..d3843fe 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum.java
@@ -111,7 +111,7 @@ public class TestTermsEnum extends LuceneTestCase {
             System.out.println("TEST: iter seekCeil target=" + target.utf8ToString() + " exists=" + exists);
           }
           // seekCeil
-          final TermsEnum.SeekStatus status = termsEnum.seekCeil(target, random().nextBoolean());
+          final TermsEnum.SeekStatus status = termsEnum.seekCeil(target);
           if (VERBOSE) {
             System.out.println("  got " + status);
           }
@@ -134,7 +134,7 @@ public class TestTermsEnum extends LuceneTestCase {
             System.out.println("TEST: iter seekExact target=" + target.utf8ToString() + " exists=" + exists);
           }
           // seekExact
-          final boolean result = termsEnum.seekExact(target, false);
+          final boolean result = termsEnum.seekExact(target);
           if (VERBOSE) {
             System.out.println("  got " + result);
           }
@@ -570,7 +570,7 @@ public class TestTermsEnum extends LuceneTestCase {
 
   // sugar
   private boolean seekExact(TermsEnum te, String term) throws IOException {
-    return te.seekExact(new BytesRef(term), random().nextBoolean());
+    return te.seekExact(new BytesRef(term));
   }
 
   // sugar
@@ -665,13 +665,13 @@ public class TestTermsEnum extends LuceneTestCase {
         if (VERBOSE) {
           System.out.println("  seekExact");
         }
-        assertEquals(loc >= 0, te.seekExact(t, random().nextBoolean()));
+        assertEquals(loc >= 0, te.seekExact(t));
       } else {
         if (VERBOSE) {
           System.out.println("  seekCeil");
         }
 
-        final TermsEnum.SeekStatus result = te.seekCeil(t, random().nextBoolean());
+        final TermsEnum.SeekStatus result = te.seekCeil(t);
         if (VERBOSE) {
           System.out.println("  got " + result);
         }
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2.java b/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2.java
index 7ef612e..813771a 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestTermsEnum2.java
@@ -117,10 +117,10 @@ public class TestTermsEnum2 extends LuceneTestCase {
           // term is accepted
           if (random().nextBoolean()) {
             // seek exact
-            assertTrue(te.seekExact(term, random().nextBoolean()));
+            assertTrue(te.seekExact(term));
           } else {
             // seek ceil
-            assertEquals(SeekStatus.FOUND, te.seekCeil(term, random().nextBoolean()));
+            assertEquals(SeekStatus.FOUND, te.seekCeil(term));
             assertEquals(term, te.term());
           }
         }
@@ -138,10 +138,10 @@ public class TestTermsEnum2 extends LuceneTestCase {
         if (c == 0) {
           assertEquals(term, te.next());
         } else if (c == 1) {
-          assertEquals(SeekStatus.FOUND, te.seekCeil(term, random().nextBoolean()));
+          assertEquals(SeekStatus.FOUND, te.seekCeil(term));
           assertEquals(term, te.term());
         } else {
-          assertTrue(te.seekExact(term, random().nextBoolean()));
+          assertTrue(te.seekExact(term));
         }
       }
     }
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestCachingCollector.java b/lucene/core/src/test/org/apache/lucene/search/TestCachingCollector.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java b/lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java
index bbfa622..2c498ae 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestFieldCache.java
@@ -735,7 +735,7 @@ public class TestFieldCache extends LuceneTestCase {
     Directory dir = newDirectory();
     IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     cfg.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir);
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, cfg);
     Document doc = new Document();
     IntField field = new IntField("f", 0, Store.YES);
     doc.add(field);
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestIndexSearcher.java b/lucene/core/src/test/org/apache/lucene/search/TestIndexSearcher.java
index c2e16b1..5c24a46 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestIndexSearcher.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestIndexSearcher.java
@@ -29,9 +29,11 @@ import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.NamedThreadFactory;
 import org.apache.lucene.util._TestUtil;
+import org.junit.Test;
 
 public class TestIndexSearcher extends LuceneTestCase {
   Directory dir;
@@ -116,4 +118,25 @@ public class TestIndexSearcher extends LuceneTestCase {
     
     _TestUtil.shutdownExecutorService(service);
   }
+  
+  @Test
+  public void testSearchAfterPassedMaxDoc() throws Exception {
+    // LUCENE-5128: ensure we get a meaningful message if searchAfter exceeds maxDoc
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+    w.addDocument(new Document());
+    IndexReader r = w.getReader();
+    w.close();
+    
+    IndexSearcher s = new IndexSearcher(r);
+    try {
+      s.searchAfter(new ScoreDoc(r.maxDoc(), 0.54f), new MatchAllDocsQuery(), 10);
+      fail("should have hit IllegalArgumentException when searchAfter exceeds maxDoc");
+    } catch (IllegalArgumentException e) {
+      // ok
+    } finally {
+      IOUtils.close(r, dir);
+    }
+  }
+  
 }
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java b/lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java
index 57db46a..c8e963c 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestMinShouldMatch2.java
@@ -294,7 +294,7 @@ public class TestMinShouldMatch2 extends LuceneTestCase {
         if (ord >= 0) {
           boolean success = ords.add(ord);
           assert success; // no dups
-          TermContext context = TermContext.build(reader.getContext(), term, true);
+          TermContext context = TermContext.build(reader.getContext(), term);
           SimWeight w = weight.similarity.computeWeight(1f, 
                         searcher.collectionStatistics("field"),
                         searcher.termStatistics(term, context));
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestScorerPerf.java b/lucene/core/src/test/org/apache/lucene/search/TestScorerPerf.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java b/lucene/core/src/test/org/apache/lucene/search/TestSloppyPhraseQuery.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java b/lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java
index 6d38820..fcab7ae 100644
--- a/lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java
+++ b/lucene/core/src/test/org/apache/lucene/search/spans/MultiSpansWrapper.java
@@ -60,7 +60,7 @@ public class MultiSpansWrapper extends Spans { // can't be package private due t
     TreeSet<Term> terms = new TreeSet<Term>();
     query.extractTerms(terms);
     for (Term term : terms) {
-      termContexts.put(term, TermContext.build(topLevelReaderContext, term, true));
+      termContexts.put(term, TermContext.build(topLevelReaderContext, term));
     }
     final List<AtomicReaderContext> leaves = topLevelReaderContext.leaves();
     if(leaves.size() == 1) {
diff --git a/lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java b/lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java
index 82ec34e..1b00341 100644
--- a/lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java
+++ b/lucene/core/src/test/org/apache/lucene/search/spans/TestBasics.java
@@ -364,6 +364,77 @@ public class TestBasics extends LuceneTestCase {
   }
   
   @Test
+  public void testSpanNotWindowOne() throws Exception {
+    SpanTermQuery term1 = new SpanTermQuery(new Term("field", "eight"));
+    SpanTermQuery term2 = new SpanTermQuery(new Term("field", "forty"));
+    SpanNearQuery near = new SpanNearQuery(new SpanQuery[] {term1, term2},
+                                           4, true);
+    SpanTermQuery term3 = new SpanTermQuery(new Term("field", "one"));
+    SpanNotQuery query = new SpanNotQuery(near, term3, 1, 1);
+
+    checkHits(query, new int[]
+      {840, 842, 843, 844, 845, 846, 847, 848, 849,
+          1840, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849});
+
+    assertTrue(searcher.explain(query, 840).getValue() > 0.0f);
+    assertTrue(searcher.explain(query, 1842).getValue() > 0.0f);
+  }
+  
+  @Test
+  public void testSpanNotWindowTwoBefore() throws Exception {
+    SpanTermQuery term1 = new SpanTermQuery(new Term("field", "eight"));
+    SpanTermQuery term2 = new SpanTermQuery(new Term("field", "forty"));
+    SpanNearQuery near = new SpanNearQuery(new SpanQuery[] {term1, term2},
+                                           4, true);
+    SpanTermQuery term3 = new SpanTermQuery(new Term("field", "one"));
+    SpanNotQuery query = new SpanNotQuery(near, term3, 2, 0);
+
+    checkHits(query, new int[]
+      {840, 841, 842, 843, 844, 845, 846, 847, 848, 849});
+
+    assertTrue(searcher.explain(query, 840).getValue() > 0.0f);
+    assertTrue(searcher.explain(query, 849).getValue() > 0.0f);
+  }
+
+  @Test
+  public void testSpanNotWindowNeg() throws Exception {
+     //test handling of invalid window < 0
+     SpanTermQuery term1 = new SpanTermQuery(new Term("field", "eight"));
+     SpanTermQuery term2 = new SpanTermQuery(new Term("field", "one"));
+     SpanNearQuery near = new SpanNearQuery(new SpanQuery[] {term1, term2},
+                                            4, true);
+     SpanTermQuery term3 = new SpanTermQuery(new Term("field", "forty"));
+
+     SpanOrQuery or = new SpanOrQuery(term3);
+
+     SpanNotQuery query = new SpanNotQuery(near, or);
+
+     checkHits(query, new int[]
+       {801, 821, 831, 851, 861, 871, 881, 891,
+               1801, 1821, 1831, 1851, 1861, 1871, 1881, 1891});
+
+     assertTrue(searcher.explain(query, 801).getValue() > 0.0f);
+     assertTrue(searcher.explain(query, 891).getValue() > 0.0f);
+  }
+  
+  @Test
+  public void testSpanNotWindowDoubleExcludesBefore() throws Exception {
+     //test hitting two excludes before an include
+     SpanTermQuery term1 = new SpanTermQuery(new Term("field", "forty"));
+     SpanTermQuery term2 = new SpanTermQuery(new Term("field", "two"));
+     SpanNearQuery near = new SpanNearQuery(new SpanTermQuery[]{term1, term2}, 2, true);
+     SpanTermQuery exclude = new SpanTermQuery(new Term("field", "one"));
+
+     SpanNotQuery query = new SpanNotQuery(near, exclude, 4, 1);
+
+     checkHits(query, new int[]
+       {42, 242, 342, 442, 542, 642, 742, 842, 942});
+
+     assertTrue(searcher.explain(query, 242).getValue() > 0.0f);
+     assertTrue(searcher.explain(query, 942).getValue() > 0.0f);
+  }
+  
+  @Test
   public void testSpanFirst() throws Exception {
     SpanTermQuery term1 = new SpanTermQuery(new Term("field", "five"));
     SpanFirstQuery query = new SpanFirstQuery(term1, 1);
diff --git a/lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java b/lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java
index 61495de..30e8a9e 100644
--- a/lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java
+++ b/lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java
@@ -84,7 +84,8 @@ public class TestSpans extends LuceneTestCase {
     "u2 xx u1 u2",
     "u2 u1 xx u2",
     "u1 u2 xx u2",
-    "t1 t2 t1 t3 t2 t3"
+    "t1 t2 t1 t3 t2 t3",
+    "s2 s1 s1 xx xx s2 xx s2 xx s1 xx xx xx xx xx s2 xx"
   };
 
   public SpanTermQuery makeSpanTermQuery(String text) {
@@ -502,4 +503,52 @@ public class TestSpans extends LuceneTestCase {
     reader.close();
     dir.close();
   }
+  
+  
+  public void testSpanNots() throws Throwable{
+     assertEquals("SpanNotIncludeExcludeSame1", 0, spanCount("s2", "s2", 0, 0), 0);
+     assertEquals("SpanNotIncludeExcludeSame2", 0, spanCount("s2", "s2", 10, 10), 0);
+     
+     //focus on behind
+     assertEquals("SpanNotS2NotS1_6_0", 1, spanCount("s2", "s1", 6, 0));
+     assertEquals("SpanNotS2NotS1_5_0", 2, spanCount("s2", "s1", 5, 0));
+     assertEquals("SpanNotS2NotS1_3_0", 3, spanCount("s2", "s1", 3, 0));
+     assertEquals("SpanNotS2NotS1_2_0", 4, spanCount("s2", "s1", 2, 0));
+     assertEquals("SpanNotS2NotS1_0_0", 4, spanCount("s2", "s1", 0, 0));
+     
+     //focus on both
+     assertEquals("SpanNotS2NotS1_3_1", 2, spanCount("s2", "s1", 3, 1));
+     assertEquals("SpanNotS2NotS1_2_1", 3, spanCount("s2", "s1", 2, 1));
+     assertEquals("SpanNotS2NotS1_1_1", 3, spanCount("s2", "s1", 1, 1));
+     assertEquals("SpanNotS2NotS1_10_10", 0, spanCount("s2", "s1", 10, 10));
+     
+     //focus on ahead
+     assertEquals("SpanNotS1NotS2_10_10", 0, spanCount("s1", "s2", 10, 10));  
+     assertEquals("SpanNotS1NotS2_0_1", 3, spanCount("s1", "s2", 0, 1));  
+     assertEquals("SpanNotS1NotS2_0_2", 3, spanCount("s1", "s2", 0, 2));  
+     assertEquals("SpanNotS1NotS2_0_3", 2, spanCount("s1", "s2", 0, 3));  
+     assertEquals("SpanNotS1NotS2_0_4", 1, spanCount("s1", "s2", 0, 4));
+     assertEquals("SpanNotS1NotS2_0_8", 0, spanCount("s1", "s2", 0, 8));
+     
+     //exclude doesn't exist
+     assertEquals("SpanNotS1NotS3_8_8", 3, spanCount("s1", "s3", 8, 8));
+
+     //include doesn't exist
+     assertEquals("SpanNotS3NotS1_8_8", 0, spanCount("s3", "s1", 8, 8));
+
+  }
+  
+  private int spanCount(String include, String exclude, int pre, int post) throws IOException{
+     SpanTermQuery iq = new SpanTermQuery(new Term(field, include));
+     SpanTermQuery eq = new SpanTermQuery(new Term(field, exclude));
+     SpanNotQuery snq = new SpanNotQuery(iq, eq, pre, post);
+     Spans spans = MultiSpansWrapper.wrap(searcher.getTopReaderContext(), snq);
+
+     int i = 0;
+     while (spans.next()){
+        i++;
+     }
+     return i;
+  }
+  
 }
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java b/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
old mode 100755
new mode 100644
index ec83b57..13fb48d
--- a/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
+++ b/lucene/core/src/test/org/apache/lucene/store/TestBufferedIndexInput.java
@@ -84,25 +84,6 @@ public class TestBufferedIndexInput extends LuceneTestCase {
   public void testReadBytes() throws Exception {
     MyBufferedIndexInput input = new MyBufferedIndexInput();
     runReadBytes(input, BufferedIndexInput.BUFFER_SIZE, random());
-
-    // This tests the workaround code for LUCENE-1566 where readBytesInternal
-    // provides a workaround for a JVM Bug that incorrectly raises a OOM Error
-    // when a large byte buffer is passed to a file read.
-    // NOTE: this does only test the chunked reads and NOT if the Bug is triggered.
-    //final int tmpFileSize = 1024 * 1024 * 5;
-    final int inputBufferSize = 128;
-    File tmpInputFile = _TestUtil.createTempFile("IndexInput", "tmpFile", TEMP_DIR);
-    tmpInputFile.deleteOnExit();
-    writeBytes(tmpInputFile, TEST_FILE_LENGTH);
-
-    // run test with chunk size of 10 bytes
-    runReadBytesAndClose(new SimpleFSIndexInput("SimpleFSIndexInput(path=\"" + tmpInputFile + "\")", 
-        new RandomAccessFile(tmpInputFile, "r"), newIOContext(random()), 10), inputBufferSize, random());
-
-    // run test with chunk size of 10 bytes
-    runReadBytesAndClose(new NIOFSIndexInput("NIOFSIndexInput(path=\"" + tmpInputFile + "\")", 
-        FileChannel.open(tmpInputFile.toPath(), StandardOpenOption.READ), newIOContext(random()), 10), 
-        inputBufferSize, random());
   }
 
   private void runReadBytesAndClose(IndexInput input, int bufferSize, Random r) throws IOException {
@@ -211,6 +192,7 @@ public class TestBufferedIndexInput extends LuceneTestCase {
     private static byte byten(long n){
       return (byte)(n*n%256);
     }
+    
     private static class MyBufferedIndexInput extends BufferedIndexInput {
       private long pos;
       private long len;
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestDirectory.java b/lucene/core/src/test/org/apache/lucene/store/TestDirectory.java
index 9e779f2..d8d3309 100644
--- a/lucene/core/src/test/org/apache/lucene/store/TestDirectory.java
+++ b/lucene/core/src/test/org/apache/lucene/store/TestDirectory.java
@@ -134,52 +134,66 @@ public class TestDirectory extends LuceneTestCase {
   // Test that different instances of FSDirectory can coexist on the same
   // path, can read, write, and lock files.
   public void testDirectInstantiation() throws Exception {
-    File path = _TestUtil.getTempDir("testDirectInstantiation");
-
-    int sz = 3;
-    Directory[] dirs = new Directory[sz];
+    final File path = _TestUtil.getTempDir("testDirectInstantiation");
+    
+    final byte[] largeBuffer = new byte[random().nextInt(256*1024)], largeReadBuffer = new byte[largeBuffer.length];
+    for (int i = 0; i < largeBuffer.length; i++) {
+      largeBuffer[i] = (byte) i; // automatically loops with modulo
+    }
 
-    dirs[0] = new SimpleFSDirectory(path, null);
-    dirs[1] = new NIOFSDirectory(path, null);
-    dirs[2] = new MMapDirectory(path, null);
+    final FSDirectory[] dirs = new FSDirectory[] {
+      new SimpleFSDirectory(path, null),
+      new NIOFSDirectory(path, null),
+      new MMapDirectory(path, null)
+    };
 
-    for (int i=0; i<sz; i++) {
-      Directory dir = dirs[i];
+    for (int i=0; i<dirs.length; i++) {
+      FSDirectory dir = dirs[i];
       dir.ensureOpen();
       String fname = "foo." + i;
       String lockname = "foo" + i + ".lck";
       IndexOutput out = dir.createOutput(fname, newIOContext(random()));
       out.writeByte((byte)i);
+      out.writeBytes(largeBuffer, largeBuffer.length);
       out.close();
 
-      for (int j=0; j<sz; j++) {
-        Directory d2 = dirs[j];
+      for (int j=0; j<dirs.length; j++) {
+        FSDirectory d2 = dirs[j];
         d2.ensureOpen();
         assertTrue(d2.fileExists(fname));
-        assertEquals(1, d2.fileLength(fname));
+        assertEquals(1 + largeBuffer.length, d2.fileLength(fname));
 
-        // don't test read on MMapDirectory, since it can't really be
-        // closed and will cause a failure to delete the file.
-        if (d2 instanceof MMapDirectory) continue;
+        // don't do read tests if unmapping is not supported!
+        if (d2 instanceof MMapDirectory && !((MMapDirectory) d2).getUseUnmap())
+          continue;
         
         IndexInput input = d2.openInput(fname, newIOContext(random()));
         assertEquals((byte)i, input.readByte());
+        // read array with buffering enabled
+        Arrays.fill(largeReadBuffer, (byte)0);
+        input.readBytes(largeReadBuffer, 0, largeReadBuffer.length, true);
+        assertArrayEquals(largeBuffer, largeReadBuffer);
+        // read again without using buffer
+        input.seek(1L);
+        Arrays.fill(largeReadBuffer, (byte)0);
+        input.readBytes(largeReadBuffer, 0, largeReadBuffer.length, false);
+        assertArrayEquals(largeBuffer, largeReadBuffer);        
         input.close();
       }
 
       // delete with a different dir
-      dirs[(i+1)%sz].deleteFile(fname);
+      dirs[(i+1)%dirs.length].deleteFile(fname);
 
-      for (int j=0; j<sz; j++) {
-        Directory d2 = dirs[j];
+      for (int j=0; j<dirs.length; j++) {
+        FSDirectory d2 = dirs[j];
         assertFalse(d2.fileExists(fname));
       }
 
       Lock lock = dir.makeLock(lockname);
       assertTrue(lock.obtain());
 
-      for (int j=0; j<sz; j++) {
-        Directory d2 = dirs[j];
+      for (int j=0; j<dirs.length; j++) {
+        FSDirectory d2 = dirs[j];
         Lock lock2 = d2.makeLock(lockname);
         try {
           assertFalse(lock2.obtain(1));
@@ -191,13 +205,13 @@ public class TestDirectory extends LuceneTestCase {
       lock.release();
       
       // now lock with different dir
-      lock = dirs[(i+1)%sz].makeLock(lockname);
+      lock = dirs[(i+1)%dirs.length].makeLock(lockname);
       assertTrue(lock.obtain());
       lock.release();
     }
 
-    for (int i=0; i<sz; i++) {
-      Directory dir = dirs[i];
+    for (int i=0; i<dirs.length; i++) {
+      FSDirectory dir = dirs[i];
       dir.ensureOpen();
       dir.close();
       assertFalse(dir.isOpen);
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestHugeRamFile.java b/lucene/core/src/test/org/apache/lucene/store/TestHugeRamFile.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestLockFactory.java b/lucene/core/src/test/org/apache/lucene/store/TestLockFactory.java
old mode 100755
new mode 100644
diff --git a/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java b/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
index fe21e0a..f647a71 100644
--- a/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
+++ b/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
@@ -932,20 +932,15 @@ public class TestFSTs extends LuceneTestCase {
           }
         }
 
-        final boolean useCache = random().nextBoolean();
-        if (VERBOSE) {
-          System.out.println("  useCache=" + useCache);
-        }
-
         final TermsEnum.SeekStatus status;
         if (nextID == null) {
-          if (termsEnum.seekExact(new BytesRef(id), useCache)) {
+          if (termsEnum.seekExact(new BytesRef(id))) {
             status = TermsEnum.SeekStatus.FOUND;
           } else {
             status = TermsEnum.SeekStatus.NOT_FOUND;
           }
         } else {
-          status = termsEnum.seekCeil(new BytesRef(id), useCache);
+          status = termsEnum.seekCeil(new BytesRef(id));
         }
 
         if (nextID != null) {
diff --git a/lucene/core/src/test/org/apache/lucene/util/packed/TestPackedInts.java b/lucene/core/src/test/org/apache/lucene/util/packed/TestPackedInts.java
index 93eb04f..6a84818 100644
--- a/lucene/core/src/test/org/apache/lucene/util/packed/TestPackedInts.java
+++ b/lucene/core/src/test/org/apache/lucene/util/packed/TestPackedInts.java
@@ -540,6 +540,28 @@ public class TestPackedInts extends LuceneTestCase {
     }
   }
 
+  public void testPackedIntsNull() {
+    // must be > 10 for the bulk reads below
+    int size = _TestUtil.nextInt(random(), 11, 256);
+    Reader packedInts = new PackedInts.NullReader(size);
+    assertEquals(0, packedInts.get(_TestUtil.nextInt(random(), 0, size - 1)));
+    long[] arr = new long[size + 10];
+    int r;
+    Arrays.fill(arr, 1);
+    r = packedInts.get(0, arr, 0, size - 1);
+    assertEquals(size - 1, r);
+    for (r--; r >= 0; r--) {
+      assertEquals(0, arr[r]);
+    }
+    Arrays.fill(arr, 1);
+    r = packedInts.get(10, arr, 0, size + 10);
+    assertEquals(size - 10, r);
+    for (int i = 0; i < size - 10; i++) {
+      assertEquals(0, arr[i]);
+    }
+
+  }
+
   public void testBulkGet() {
     final int valueCount = 1111;
     final int index = random().nextInt(valueCount);
@@ -669,8 +691,8 @@ public class TestPackedInts extends LuceneTestCase {
     PagedGrowableWriter writer = new PagedGrowableWriter(0, pageSize, _TestUtil.nextInt(random(), 1, 64), random().nextFloat());
     assertEquals(0, writer.size());
 
-    // compare against AppendingLongBuffer
-    AppendingLongBuffer buf = new AppendingLongBuffer();
+    // compare against AppendingDeltaPackedLongBuffer
+    AppendingDeltaPackedLongBuffer buf = new AppendingDeltaPackedLongBuffer();
     int size = random().nextInt(1000000);
     long max = 5;
     for (int i = 0; i < size; ++i) {
@@ -720,8 +742,8 @@ public class TestPackedInts extends LuceneTestCase {
     PagedMutable writer = new PagedMutable(0, pageSize, bitsPerValue, random().nextFloat() / 2);
     assertEquals(0, writer.size());
 
-    // compare against AppendingLongBuffer
-    AppendingLongBuffer buf = new AppendingLongBuffer();
+    // compare against AppendingDeltaPackedLongBuffer
+    AppendingDeltaPackedLongBuffer buf = new AppendingDeltaPackedLongBuffer();
     int size = random().nextInt(1000000);
     
     for (int i = 0; i < size; ++i) {
@@ -924,25 +946,46 @@ public class TestPackedInts extends LuceneTestCase {
     return true;
   }
 
+  enum DataType {
+    PACKED,
+    DELTA_PACKED,
+    MONOTONIC
+  }
+
+
   public void testAppendingLongBuffer() {
+
     final long[] arr = new long[RandomInts.randomIntBetween(random(), 1, 1000000)];
-    for (int bpv : new int[] {0, 1, 63, 64, RandomInts.randomIntBetween(random(), 2, 62)}) {
-      for (boolean monotonic : new boolean[] {true, false}) {
+    float[] ratioOptions = new float[]{PackedInts.DEFAULT, PackedInts.COMPACT, PackedInts.FAST};
+    for (int bpv : new int[]{0, 1, 63, 64, RandomInts.randomIntBetween(random(), 2, 62)}) {
+      for (DataType dataType : DataType.values()) {
         final int pageSize = 1 << _TestUtil.nextInt(random(), 6, 20);
         final int initialPageCount = _TestUtil.nextInt(random(), 0, 16);
+        float acceptableOverheadRatio = ratioOptions[_TestUtil.nextInt(random(), 0, ratioOptions.length - 1)];
         AbstractAppendingLongBuffer buf;
         final int inc;
-        if (monotonic) {
-          buf = new MonotonicAppendingLongBuffer(initialPageCount, pageSize);
-          inc = _TestUtil.nextInt(random(), -1000, 1000);
-        } else {
-          buf = new AppendingLongBuffer(initialPageCount, pageSize);
-          inc = 0;
+        switch (dataType) {
+          case PACKED:
+            buf = new AppendingPackedLongBuffer(initialPageCount, pageSize, acceptableOverheadRatio);
+            inc = 0;
+            break;
+          case DELTA_PACKED:
+            buf = new AppendingDeltaPackedLongBuffer(initialPageCount, pageSize, acceptableOverheadRatio);
+            inc = 0;
+            break;
+          case MONOTONIC:
+            buf = new MonotonicAppendingLongBuffer(initialPageCount, pageSize, acceptableOverheadRatio);
+            inc = _TestUtil.nextInt(random(), -1000, 1000);
+            break;
+          default:
+            throw new RuntimeException("added a type and forgot to add it here?");
+
         }
+
         if (bpv == 0) {
           arr[0] = random().nextLong();
           for (int i = 1; i < arr.length; ++i) {
-            arr[i] = arr[i-1] + inc;
+            arr[i] = arr[i - 1] + inc;
           }
         } else if (bpv == 64) {
           for (int i = 0; i < arr.length; ++i) {
@@ -954,6 +997,7 @@ public class TestPackedInts extends LuceneTestCase {
             arr[i] = minValue + inc * i + random().nextLong() & PackedInts.maxValue(bpv); // _TestUtil.nextLong is too slow
           }
         }
+
         for (int i = 0; i < arr.length; ++i) {
           buf.add(arr[i]);
         }
@@ -966,6 +1010,11 @@ public class TestPackedInts extends LuceneTestCase {
           }
         }
         assertEquals(arr.length, buf.size());
+
+        for (int i = 0; i < arr.length; ++i) {
+          assertEquals(arr[i], buf.get(i));
+        }
+
         final AbstractAppendingLongBuffer.Iterator it = buf.iterator();
         for (int i = 0; i < arr.length; ++i) {
           if (random().nextBoolean()) {
@@ -974,11 +1023,27 @@ public class TestPackedInts extends LuceneTestCase {
           assertEquals(arr[i], it.next());
         }
         assertFalse(it.hasNext());
-        
-        for (int i = 0; i < arr.length; ++i) {
-          assertEquals(arr[i], buf.get(i));
+
+
+        long[] target = new long[arr.length + 1024]; // check the request for more is OK.
+        for (int i = 0; i < arr.length; i += _TestUtil.nextInt(random(), 0, 10000)) {
+          int lenToRead = random().nextInt(buf.pageSize() * 2) + 1;
+          lenToRead = Math.min(lenToRead, target.length - i);
+          int lenToCheck = Math.min(lenToRead, arr.length - i);
+          int off = i;
+          while (off < arr.length && lenToRead > 0) {
+            int read = buf.get(off, target, off, lenToRead);
+            assertTrue(read > 0);
+            assertTrue(read <= lenToRead);
+            lenToRead -= read;
+            off += read;
+          }
+
+          for (int j = 0; j < lenToCheck; j++) {
+            assertEquals(arr[j + i], target[j + i]);
+          }
         }
-  
+
         final long expectedBytesUsed = RamUsageEstimator.sizeOf(buf);
         final long computedBytesUsed = buf.ramBytesUsed();
         assertEquals(expectedBytesUsed, computedBytesUsed);
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
index 78e7adc..fdfd5fd 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
@@ -1,27 +1,20 @@
 package org.apache.lucene.demo.facet;
 
 import java.io.IOException;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.associations.AssociationFloatSumFacetRequest;
-import org.apache.lucene.facet.associations.AssociationIntSumFacetRequest;
 import org.apache.lucene.facet.associations.AssociationsFacetFields;
 import org.apache.lucene.facet.associations.CategoryAssociation;
 import org.apache.lucene.facet.associations.CategoryAssociationsContainer;
 import org.apache.lucene.facet.associations.CategoryFloatAssociation;
 import org.apache.lucene.facet.associations.CategoryIntAssociation;
-import org.apache.lucene.facet.associations.MultiAssociationsFacetsAggregator;
-import org.apache.lucene.facet.associations.SumFloatAssociationFacetsAggregator;
-import org.apache.lucene.facet.associations.SumIntAssociationFacetsAggregator;
+import org.apache.lucene.facet.associations.SumFloatAssociationFacetRequest;
+import org.apache.lucene.facet.associations.SumIntAssociationFacetRequest;
 import org.apache.lucene.facet.index.FacetFields;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.search.FacetsAggregator;
 import org.apache.lucene.facet.search.FacetsCollector;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
@@ -135,22 +128,9 @@ public class AssociationsFacetsExample {
     
     CategoryPath tags = new CategoryPath("tags");
     CategoryPath genre = new CategoryPath("genre");
-    FacetSearchParams fsp = new FacetSearchParams(
-        new AssociationIntSumFacetRequest(tags, 10), 
-        new AssociationFloatSumFacetRequest(genre, 10));
-  
-    // every category has a different type of association, so use chain their
-    // respective aggregators.
-    final Map<CategoryPath,FacetsAggregator> aggregators = new HashMap<CategoryPath,FacetsAggregator>();
-    aggregators.put(tags, new SumIntAssociationFacetsAggregator());
-    aggregators.put(genre, new SumFloatAssociationFacetsAggregator());
-    FacetsAccumulator fa = new FacetsAccumulator(fsp, indexReader, taxoReader) {
-      @Override
-      public FacetsAggregator getAggregator() {
-        return new MultiAssociationsFacetsAggregator(aggregators);
-      }
-    };
-    FacetsCollector fc = FacetsCollector.create(fa);
+    FacetSearchParams fsp = new FacetSearchParams(new SumIntAssociationFacetRequest(tags, 10), 
+        new SumFloatAssociationFacetRequest(genre, 10));
+    FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
     
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
index 183828f..021429f 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
@@ -27,7 +27,6 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.LongField;
 import org.apache.lucene.document.NumericDocValuesField;
 import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.range.LongRange;
 import org.apache.lucene.facet.range.RangeAccumulator;
 import org.apache.lucene.facet.range.RangeFacetRequest;
@@ -80,13 +79,12 @@ public class RangeFacetsExample implements Closeable {
   /** User runs a query and counts facets. */
   public List<FacetResult> search() throws IOException {
 
-    FacetSearchParams fsp = new FacetSearchParams(
-                                new RangeFacetRequest<LongRange>("timestamp",
-                                                                 new LongRange("Past hour", nowSec-3600, true, nowSec, true),
-                                                                 new LongRange("Past six hours", nowSec-6*3600, true, nowSec, true),
-                                                                 new LongRange("Past day", nowSec-24*3600, true, nowSec, true)));
+    RangeFacetRequest<LongRange> rangeFacetRequest = new RangeFacetRequest<LongRange>("timestamp",
+                                     new LongRange("Past hour", nowSec-3600, true, nowSec, true),
+                                     new LongRange("Past six hours", nowSec-6*3600, true, nowSec, true),
+                                     new LongRange("Past day", nowSec-24*3600, true, nowSec, true));
     // Aggregatses the facet counts
-    FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(fsp, searcher.getIndexReader()));
+    FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(rangeFacetRequest));
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
@@ -112,6 +110,7 @@ public class RangeFacetsExample implements Closeable {
     return searcher.search(q, 10);
   }
 
+  @Override
   public void close() throws IOException {
     searcher.getIndexReader().close();
     indexDir.close();
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
index 351dc6c..3cd7c15 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
@@ -91,7 +91,7 @@ public class SimpleSortedSetFacetsExample {
         new CountFacetRequest(new CategoryPath("Author"), 10));
 
     // Aggregatses the facet counts
-    FacetsCollector fc = FacetsCollector.create(new SortedSetDocValuesAccumulator(fsp, state));
+    FacetsCollector fc = FacetsCollector.create(new SortedSetDocValuesAccumulator(state, fsp));
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
@@ -117,7 +117,7 @@ public class SimpleSortedSetFacetsExample {
     FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath("Author"), 10));
     DrillDownQuery q = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
     q.add(new CategoryPath("Publish Year/2010", '/'));
-    FacetsCollector fc = FacetsCollector.create(new SortedSetDocValuesAccumulator(fsp, state));
+    FacetsCollector fc = FacetsCollector.create(new SortedSetDocValuesAccumulator(state, fsp));
     searcher.search(q, fc);
 
     // Retrieve results
diff --git a/lucene/facet/build.xml b/lucene/facet/build.xml
index 324125b..f4349e0 100644
--- a/lucene/facet/build.xml
+++ b/lucene/facet/build.xml
@@ -23,9 +23,6 @@
     Faceted indexing and search capabilities
   </description>
 
-  <!-- prettify.css/js -->
-  <property name="rat.excludes" value="**/prettify.css,**/prettify.js"/>
-  
   <import file="../module-build.xml"/>
 
   <target name="run-encoding-benchmark" depends="compile-test">
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FacetPackage.java b/lucene/facet/src/java/org/apache/lucene/facet/FacetPackage.java
index efb9e33..861b577 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/FacetPackage.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/FacetPackage.java
@@ -17,11 +17,7 @@ package org.apache.lucene.facet;
  * limitations under the License.
  */
 
-/**
- * Required so that userguide files are copied as part of javadocs generation.
- * Otherwise, if the root facet package contains no classes, doc-files aren't
- * copied.
- */
+/** Required for javadocs generation. */
 public final class FacetPackage {
   
   private FacetPackage() {}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationFloatSumFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationFloatSumFacetRequest.java
deleted file mode 100644
index 8255b0d..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationFloatSumFacetRequest.java
+++ /dev/null
@@ -1,50 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetRequest} for weighting facets according to their float
- * association by summing the association values.
- * 
- * @lucene.experimental
- */
-public class AssociationFloatSumFacetRequest extends FacetRequest {
-
-  /**
-   * Create a float association facet request for a given node in the
-   * taxonomy.
-   */
-  public AssociationFloatSumFacetRequest(CategoryPath path, int num) {
-    super(path, num);
-  }
-
-  @Override
-  public double getValueOf(FacetArrays arrays, int ordinal) {
-    return arrays.getFloatArray()[ordinal];
-  }
-
-  @Override
-  public FacetArraysSource getFacetArraysSource() {
-    return FacetArraysSource.FLOAT;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumFacetRequest.java
deleted file mode 100644
index 2c94c3c..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumFacetRequest.java
+++ /dev/null
@@ -1,50 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetRequest} for weighting facets according to their integer
- * association by summing the association values.
- * 
- * @lucene.experimental
- */
-public class AssociationIntSumFacetRequest extends FacetRequest {
-
-  /**
-   * Create an integer association facet request for a given node in the
-   * taxonomy.
-   */
-  public AssociationIntSumFacetRequest(CategoryPath path, int num) {
-    super(path, num);
-  }
-
-  @Override
-  public FacetArraysSource getFacetArraysSource() {
-    return FacetArraysSource.INT;
-  }
-
-  @Override
-  public double getValueOf(FacetArrays arrays, int ordinal) {
-    return arrays.getIntArray()[ordinal];
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsListBuilder.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsListBuilder.java
index 42a4218..3c9db45 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsListBuilder.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsListBuilder.java
@@ -54,13 +54,6 @@ public class AssociationsListBuilder implements CategoryListBuilder {
       // build per-association key BytesRef
       CategoryAssociation association = associations.getAssociation(cp);
       
-      if (association == null) {
-        // it is ok to set a null association for a category - it's treated as a
-        // regular category in that case.
-        ++idx;
-        continue;
-      }
-
       BytesRef bytes = res.get(association.getCategoryListID());
       if (bytes == null) {
         bytes = new BytesRef(32);
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociationsContainer.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociationsContainer.java
index 67e6c4c8..d40a0c1 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociationsContainer.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/CategoryAssociationsContainer.java
@@ -30,11 +30,12 @@ public class CategoryAssociationsContainer implements Iterable<CategoryPath> {
   
   /**
    * Adds the {@link CategoryAssociation} for the given {@link CategoryPath
-   * category}. Overrides any assocation that was previously set. It is ok to
-   * pass {@code null}, in which case the category will be treated as a regular
-   * one (i.e. without association value).
+   * category}. Overrides any assocation that was previously set.
    */
   public void setAssociation(CategoryPath category, CategoryAssociation association) {
+    if (association == null) {
+      throw new IllegalArgumentException("cannot set a null association to a category");
+    }
     categoryAssociations.put(category, association);
   }
   
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/MultiAssociationsFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/MultiAssociationsFacetsAggregator.java
deleted file mode 100644
index 3004ad6..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/MultiAssociationsFacetsAggregator.java
+++ /dev/null
@@ -1,92 +0,0 @@
-package org.apache.lucene.facet.associations;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetsAggregator;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAggregator} which chains multiple aggregators for aggregating
- * the association values of categories that belong to the same category list.
- * While nothing prevents you from chaining general purpose aggregators, it is
- * only useful for aggregating association values, as each association type is
- * written in its own list.
- * 
- * @lucene.experimental
- */
-public class MultiAssociationsFacetsAggregator implements FacetsAggregator {
-  
-  private final Map<CategoryPath,FacetsAggregator> categoryAggregators;
-  private final List<FacetsAggregator> aggregators;
-  
-  /**
-   * Creates a new {@link MultiAssociationsFacetsAggregator} over the given
-   * aggregators. The mapping is used by
-   * {@link #rollupValues(FacetRequest, int, int[], int[], FacetArrays)} to
-   * rollup the values of the specific category by the corresponding
-   * {@link FacetsAggregator}. However, since each {@link FacetsAggregator}
-   * handles the associations of a specific type, which could cover multiple
-   * categories, the aggregation is done on the unique set of aggregators, which
-   * are identified by their class.
-   */
-  public MultiAssociationsFacetsAggregator(Map<CategoryPath,FacetsAggregator> aggregators) {
-    this.categoryAggregators = aggregators;
-    
-    // make sure that each FacetsAggregator class is invoked only once, or
-    // otherwise categories may be aggregated multiple times.
-    Map<Class<? extends FacetsAggregator>, FacetsAggregator> aggsClasses = 
-        new HashMap<Class<? extends FacetsAggregator>,FacetsAggregator>();
-    for (FacetsAggregator fa : aggregators.values()) {
-      aggsClasses.put(fa.getClass(), fa);
-    }
-    this.aggregators = new ArrayList<FacetsAggregator>(aggsClasses.values());
-  }
-  
-  @Override
-  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
-    for (FacetsAggregator fa : aggregators) {
-      fa.aggregate(matchingDocs, clp, facetArrays);
-    }
-  }
-  
-  @Override
-  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
-    categoryAggregators.get(fr.categoryPath).rollupValues(fr, ordinal, children, siblings, facetArrays);
-  }
-  
-  @Override
-  public boolean requiresDocScores() {
-    for (FacetsAggregator fa : aggregators) {
-      if (fa.requiresDocScores()) {
-        return true;
-      }
-    }
-    return false;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetRequest.java
new file mode 100644
index 0000000..2a03659
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetRequest.java
@@ -0,0 +1,46 @@
+package org.apache.lucene.facet.associations;
+
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetsAggregator;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetRequest} for weighting facets according to their float
+ * association by summing the association values.
+ * 
+ * @lucene.experimental
+ */
+public class SumFloatAssociationFacetRequest extends FacetRequest {
+
+  /**
+   * Create a float association facet request for a given node in the
+   * taxonomy.
+   */
+  public SumFloatAssociationFacetRequest(CategoryPath path, int num) {
+    super(path, num);
+  }
+
+  @Override
+  public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
+    return new SumFloatAssociationFacetsAggregator();
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetsAggregator.java
index ce52702..741a5cc 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetsAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/SumFloatAssociationFacetsAggregator.java
@@ -7,6 +7,8 @@ import org.apache.lucene.facet.search.FacetArrays;
 import org.apache.lucene.facet.search.FacetRequest;
 import org.apache.lucene.facet.search.FacetsAggregator;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.search.OrdinalValueResolver;
+import org.apache.lucene.facet.search.OrdinalValueResolver.FloatValueResolver;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.util.BytesRef;
 
@@ -54,23 +56,20 @@ public class SumFloatAssociationFacetsAggregator implements FacetsAggregator {
     int doc = 0;
     while (doc < length && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
       dv.get(doc, bytes);
-      if (bytes.length == 0) {
-        continue; // no associations for this document
+      if (bytes.length > 0) {
+        // aggreate float association values for ordinals
+        int bytesUpto = bytes.offset + bytes.length;
+        int pos = bytes.offset;
+        while (pos < bytesUpto) {
+          int ordinal = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
+              | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
+          
+          int value = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
+              | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
+          
+          values[ordinal] += Float.intBitsToFloat(value);
+        }
       }
-
-      // aggreate float association values for ordinals
-      int bytesUpto = bytes.offset + bytes.length;
-      int pos = bytes.offset;
-      while (pos < bytesUpto) {
-        int ordinal = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
-            | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
-        
-        int value = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
-            | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
-
-        values[ordinal] += Float.intBitsToFloat(value);
-      }
-      
       ++doc;
     }
   }
@@ -84,5 +83,10 @@ public class SumFloatAssociationFacetsAggregator implements FacetsAggregator {
   public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
     // NO-OP: this aggregator does no rollup values to the parents.
   }
+
+  @Override
+  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
+    return new FloatValueResolver(arrays);
+  }
   
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetRequest.java
new file mode 100644
index 0000000..6ec06c9
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetRequest.java
@@ -0,0 +1,46 @@
+package org.apache.lucene.facet.associations;
+
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetsAggregator;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetRequest} for weighting facets according to their integer
+ * association by summing the association values.
+ * 
+ * @lucene.experimental
+ */
+public class SumIntAssociationFacetRequest extends FacetRequest {
+
+  /**
+   * Create an integer association facet request for a given node in the
+   * taxonomy.
+   */
+  public SumIntAssociationFacetRequest(CategoryPath path, int num) {
+    super(path, num);
+  }
+
+  @Override
+  public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
+    return new SumIntAssociationFacetsAggregator();
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetsAggregator.java
index 03d035e..1510c4c 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetsAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/SumIntAssociationFacetsAggregator.java
@@ -7,6 +7,8 @@ import org.apache.lucene.facet.search.FacetArrays;
 import org.apache.lucene.facet.search.FacetRequest;
 import org.apache.lucene.facet.search.FacetsAggregator;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.search.OrdinalValueResolver;
+import org.apache.lucene.facet.search.OrdinalValueResolver.IntValueResolver;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.util.BytesRef;
 
@@ -53,23 +55,20 @@ public class SumIntAssociationFacetsAggregator implements FacetsAggregator {
     int doc = 0;
     while (doc < length && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
       dv.get(doc, bytes);
-      if (bytes.length == 0) {
-        continue; // no associations for this document
+      if (bytes.length > 0) {
+        // aggreate association values for ordinals
+        int bytesUpto = bytes.offset + bytes.length;
+        int pos = bytes.offset;
+        while (pos < bytesUpto) {
+          int ordinal = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
+              | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
+          
+          int value = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
+              | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
+          
+          values[ordinal] += value;
+        }
       }
-
-      // aggreate association values for ordinals
-      int bytesUpto = bytes.offset + bytes.length;
-      int pos = bytes.offset;
-      while (pos < bytesUpto) {
-        int ordinal = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
-            | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
-        
-        int value = ((bytes.bytes[pos++] & 0xFF) << 24) | ((bytes.bytes[pos++] & 0xFF) << 16)
-            | ((bytes.bytes[pos++] & 0xFF) <<  8) | (bytes.bytes[pos++] & 0xFF);
-
-        values[ordinal] += value;
-      }
-      
       ++doc;
     }
   }
@@ -84,4 +83,9 @@ public class SumIntAssociationFacetsAggregator implements FacetsAggregator {
     // NO-OP: this aggregator does no rollup values to the parents.
   }
 
+  @Override
+  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
+    return new IntValueResolver(arrays);
+  }
+
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/codecs/facet42/package.html b/lucene/facet/src/java/org/apache/lucene/facet/codecs/facet42/package.html
index 56f7f3f..c752b96 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/codecs/facet42/package.html
+++ b/lucene/facet/src/java/org/apache/lucene/facet/codecs/facet42/package.html
@@ -16,9 +16,6 @@
  limitations under the License.
 -->
 <html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
 <body>
 Codec + DocValuesFormat that are optimized for facets.
 </body>
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/complements/ComplementCountingAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/complements/ComplementCountingAggregator.java
deleted file mode 100644
index e12dd6f..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/complements/ComplementCountingAggregator.java
+++ /dev/null
@@ -1,45 +0,0 @@
-package org.apache.lucene.facet.complements;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.search.CountingAggregator;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link CountingAggregator} used during complement counting.
- * 
- * @lucene.experimental
- */
-public class ComplementCountingAggregator extends CountingAggregator {
-
-  public ComplementCountingAggregator(int[] counterArray) {
-    super(counterArray);
-  }
-
-  @Override
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
-    for (int i = 0; i < ordinals.length; i++) {
-      int ord = ordinals.ints[i];
-      assert counterArray[ord] != 0 : "complement aggregation: count is about to become negative for ordinal " + ord;
-      --counterArray[ord];
-    }
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCounts.java
index b177864..c83e1d0 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCounts.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCounts.java
@@ -11,20 +11,20 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.concurrent.atomic.AtomicInteger;
 
+import org.apache.lucene.facet.old.Aggregator;
+import org.apache.lucene.facet.old.CountingAggregator;
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
+import org.apache.lucene.facet.old.ScoredDocIdsUtils;
 import org.apache.lucene.facet.params.CategoryListParams;
 import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.Aggregator;
 import org.apache.lucene.facet.search.CategoryListIterator;
 import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.CountingAggregator;
 import org.apache.lucene.facet.search.FacetArrays;
 import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.facet.util.ScoredDocIdsUtils;
 import org.apache.lucene.index.IndexReader;
 
 /*
@@ -159,7 +159,7 @@ public class TotalFacetCounts {
     final int[][] counts = new int[(int) Math.ceil(taxonomy.getSize()  /(float) partitionSize)][partitionSize];
     FacetSearchParams newSearchParams = new FacetSearchParams(facetIndexingParams, DUMMY_REQ); 
       //createAllListsSearchParams(facetIndexingParams,  this.totalCounts);
-    StandardFacetsAccumulator sfa = new StandardFacetsAccumulator(newSearchParams, indexReader, taxonomy) {
+    OldFacetsAccumulator sfa = new OldFacetsAccumulator(newSearchParams, indexReader, taxonomy) {
       @Override
       protected HashMap<CategoryListIterator, Aggregator> getCategoryListMap(
           FacetArrays facetArrays, int partition) throws IOException {
@@ -172,7 +172,7 @@ public class TotalFacetCounts {
         return map;
       }
     };
-    sfa.setComplementThreshold(StandardFacetsAccumulator.DISABLE_COMPLEMENT);
+    sfa.setComplementThreshold(OldFacetsAccumulator.DISABLE_COMPLEMENT);
     sfa.accumulate(ScoredDocIdsUtils.createAllDocsScoredDocIDs(indexReader));
     return new TotalFacetCounts(taxonomy, facetIndexingParams, counts, CreationType.Computed);
   }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/doc-files/prettify.css b/lucene/facet/src/java/org/apache/lucene/facet/doc-files/prettify.css
deleted file mode 100755
index d44b3a2..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/doc-files/prettify.css
+++ /dev/null
@@ -1 +0,0 @@
-.pln{color:#000}@media screen{.str{color:#080}.kwd{color:#008}.com{color:#800}.typ{color:#606}.lit{color:#066}.pun,.opn,.clo{color:#660}.tag{color:#008}.atn{color:#606}.atv{color:#080}.dec,.var{color:#606}.fun{color:red}}@media print,projection{.str{color:#060}.kwd{color:#006;font-weight:bold}.com{color:#600;font-style:italic}.typ{color:#404;font-weight:bold}.lit{color:#044}.pun,.opn,.clo{color:#440}.tag{color:#006;font-weight:bold}.atn{color:#404}.atv{color:#060}}pre.prettyprint{padding:2px;border:1px solid #888}ol.linenums{margin-top:0;margin-bottom:0}li.L0,li.L1,li.L2,li.L3,li.L5,li.L6,li.L7,li.L8{list-style-type:none}li.L1,li.L3,li.L5,li.L7,li.L9{background:#eee}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/doc-files/prettify.js b/lucene/facet/src/java/org/apache/lucene/facet/doc-files/prettify.js
deleted file mode 100755
index eef5ad7..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/doc-files/prettify.js
+++ /dev/null
@@ -1,28 +0,0 @@
-var q=null;window.PR_SHOULD_USE_CONTINUATION=!0;
-(function(){function L(a){function m(a){var f=a.charCodeAt(0);if(f!==92)return f;var b=a.charAt(1);return(f=r[b])?f:"0"<=b&&b<="7"?parseInt(a.substring(1),8):b==="u"||b==="x"?parseInt(a.substring(2),16):a.charCodeAt(1)}function e(a){if(a<32)return(a<16?"\\x0":"\\x")+a.toString(16);a=String.fromCharCode(a);if(a==="\\"||a==="-"||a==="["||a==="]")a="\\"+a;return a}function h(a){for(var f=a.substring(1,a.length-1).match(/\\u[\dA-Fa-f]{4}|\\x[\dA-Fa-f]{2}|\\[0-3][0-7]{0,2}|\\[0-7]{1,2}|\\[\S\s]|[^\\]/g),a=
-[],b=[],o=f[0]==="^",c=o?1:0,i=f.length;c<i;++c){var j=f[c];if(/\\[bdsw]/i.test(j))a.push(j);else{var j=m(j),d;c+2<i&&"-"===f[c+1]?(d=m(f[c+2]),c+=2):d=j;b.push([j,d]);d<65||j>122||(d<65||j>90||b.push([Math.max(65,j)|32,Math.min(d,90)|32]),d<97||j>122||b.push([Math.max(97,j)&-33,Math.min(d,122)&-33]))}}b.sort(function(a,f){return a[0]-f[0]||f[1]-a[1]});f=[];j=[NaN,NaN];for(c=0;c<b.length;++c)i=b[c],i[0]<=j[1]+1?j[1]=Math.max(j[1],i[1]):f.push(j=i);b=["["];o&&b.push("^");b.push.apply(b,a);for(c=0;c<
-f.length;++c)i=f[c],b.push(e(i[0])),i[1]>i[0]&&(i[1]+1>i[0]&&b.push("-"),b.push(e(i[1])));b.push("]");return b.join("")}function y(a){for(var f=a.source.match(/\[(?:[^\\\]]|\\[\S\s])*]|\\u[\dA-Fa-f]{4}|\\x[\dA-Fa-f]{2}|\\\d+|\\[^\dux]|\(\?[!:=]|[()^]|[^()[\\^]+/g),b=f.length,d=[],c=0,i=0;c<b;++c){var j=f[c];j==="("?++i:"\\"===j.charAt(0)&&(j=+j.substring(1))&&j<=i&&(d[j]=-1)}for(c=1;c<d.length;++c)-1===d[c]&&(d[c]=++t);for(i=c=0;c<b;++c)j=f[c],j==="("?(++i,d[i]===void 0&&(f[c]="(?:")):"\\"===j.charAt(0)&&
-(j=+j.substring(1))&&j<=i&&(f[c]="\\"+d[i]);for(i=c=0;c<b;++c)"^"===f[c]&&"^"!==f[c+1]&&(f[c]="");if(a.ignoreCase&&s)for(c=0;c<b;++c)j=f[c],a=j.charAt(0),j.length>=2&&a==="["?f[c]=h(j):a!=="\\"&&(f[c]=j.replace(/[A-Za-z]/g,function(a){a=a.charCodeAt(0);return"["+String.fromCharCode(a&-33,a|32)+"]"}));return f.join("")}for(var t=0,s=!1,l=!1,p=0,d=a.length;p<d;++p){var g=a[p];if(g.ignoreCase)l=!0;else if(/[a-z]/i.test(g.source.replace(/\\u[\da-f]{4}|\\x[\da-f]{2}|\\[^UXux]/gi,""))){s=!0;l=!1;break}}for(var r=
-{b:8,t:9,n:10,v:11,f:12,r:13},n=[],p=0,d=a.length;p<d;++p){g=a[p];if(g.global||g.multiline)throw Error(""+g);n.push("(?:"+y(g)+")")}return RegExp(n.join("|"),l?"gi":"g")}function M(a){function m(a){switch(a.nodeType){case 1:if(e.test(a.className))break;for(var g=a.firstChild;g;g=g.nextSibling)m(g);g=a.nodeName;if("BR"===g||"LI"===g)h[s]="\n",t[s<<1]=y++,t[s++<<1|1]=a;break;case 3:case 4:g=a.nodeValue,g.length&&(g=p?g.replace(/\r\n?/g,"\n"):g.replace(/[\t\n\r ]+/g," "),h[s]=g,t[s<<1]=y,y+=g.length,
-t[s++<<1|1]=a)}}var e=/(?:^|\s)nocode(?:\s|$)/,h=[],y=0,t=[],s=0,l;a.currentStyle?l=a.currentStyle.whiteSpace:window.getComputedStyle&&(l=document.defaultView.getComputedStyle(a,q).getPropertyValue("white-space"));var p=l&&"pre"===l.substring(0,3);m(a);return{a:h.join("").replace(/\n$/,""),c:t}}function B(a,m,e,h){m&&(a={a:m,d:a},e(a),h.push.apply(h,a.e))}function x(a,m){function e(a){for(var l=a.d,p=[l,"pln"],d=0,g=a.a.match(y)||[],r={},n=0,z=g.length;n<z;++n){var f=g[n],b=r[f],o=void 0,c;if(typeof b===
-"string")c=!1;else{var i=h[f.charAt(0)];if(i)o=f.match(i[1]),b=i[0];else{for(c=0;c<t;++c)if(i=m[c],o=f.match(i[1])){b=i[0];break}o||(b="pln")}if((c=b.length>=5&&"lang-"===b.substring(0,5))&&!(o&&typeof o[1]==="string"))c=!1,b="src";c||(r[f]=b)}i=d;d+=f.length;if(c){c=o[1];var j=f.indexOf(c),k=j+c.length;o[2]&&(k=f.length-o[2].length,j=k-c.length);b=b.substring(5);B(l+i,f.substring(0,j),e,p);B(l+i+j,c,C(b,c),p);B(l+i+k,f.substring(k),e,p)}else p.push(l+i,b)}a.e=p}var h={},y;(function(){for(var e=a.concat(m),
-l=[],p={},d=0,g=e.length;d<g;++d){var r=e[d],n=r[3];if(n)for(var k=n.length;--k>=0;)h[n.charAt(k)]=r;r=r[1];n=""+r;p.hasOwnProperty(n)||(l.push(r),p[n]=q)}l.push(/[\S\s]/);y=L(l)})();var t=m.length;return e}function u(a){var m=[],e=[];a.tripleQuotedStrings?m.push(["str",/^(?:'''(?:[^'\\]|\\[\S\s]|''?(?=[^']))*(?:'''|$)|"""(?:[^"\\]|\\[\S\s]|""?(?=[^"]))*(?:"""|$)|'(?:[^'\\]|\\[\S\s])*(?:'|$)|"(?:[^"\\]|\\[\S\s])*(?:"|$))/,q,"'\""]):a.multiLineStrings?m.push(["str",/^(?:'(?:[^'\\]|\\[\S\s])*(?:'|$)|"(?:[^"\\]|\\[\S\s])*(?:"|$)|`(?:[^\\`]|\\[\S\s])*(?:`|$))/,
-q,"'\"`"]):m.push(["str",/^(?:'(?:[^\n\r'\\]|\\.)*(?:'|$)|"(?:[^\n\r"\\]|\\.)*(?:"|$))/,q,"\"'"]);a.verbatimStrings&&e.push(["str",/^@"(?:[^"]|"")*(?:"|$)/,q]);var h=a.hashComments;h&&(a.cStyleComments?(h>1?m.push(["com",/^#(?:##(?:[^#]|#(?!##))*(?:###|$)|.*)/,q,"#"]):m.push(["com",/^#(?:(?:define|elif|else|endif|error|ifdef|include|ifndef|line|pragma|undef|warning)\b|[^\n\r]*)/,q,"#"]),e.push(["str",/^<(?:(?:(?:\.\.\/)*|\/?)(?:[\w-]+(?:\/[\w-]+)+)?[\w-]+\.h|[a-z]\w*)>/,q])):m.push(["com",/^#[^\n\r]*/,
-q,"#"]));a.cStyleComments&&(e.push(["com",/^\/\/[^\n\r]*/,q]),e.push(["com",/^\/\*[\S\s]*?(?:\*\/|$)/,q]));a.regexLiterals&&e.push(["lang-regex",/^(?:^^\.?|[!+-]|!=|!==|#|%|%=|&|&&|&&=|&=|\(|\*|\*=|\+=|,|-=|->|\/|\/=|:|::|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|[?@[^]|\^=|\^\^|\^\^=|{|\||\|=|\|\||\|\|=|~|break|case|continue|delete|do|else|finally|instanceof|return|throw|try|typeof)\s*(\/(?=[^*/])(?:[^/[\\]|\\[\S\s]|\[(?:[^\\\]]|\\[\S\s])*(?:]|$))+\/)/]);(h=a.types)&&e.push(["typ",h]);a=(""+a.keywords).replace(/^ | $/g,
-"");a.length&&e.push(["kwd",RegExp("^(?:"+a.replace(/[\s,]+/g,"|")+")\\b"),q]);m.push(["pln",/^\s+/,q," \r\n\t\xa0"]);e.push(["lit",/^@[$_a-z][\w$@]*/i,q],["typ",/^(?:[@_]?[A-Z]+[a-z][\w$@]*|\w+_t\b)/,q],["pln",/^[$_a-z][\w$@]*/i,q],["lit",/^(?:0x[\da-f]+|(?:\d(?:_\d+)*\d*(?:\.\d*)?|\.\d\+)(?:e[+-]?\d+)?)[a-z]*/i,q,"0123456789"],["pln",/^\\[\S\s]?/,q],["pun",/^.[^\s\w"-$'./@\\`]*/,q]);return x(m,e)}function D(a,m){function e(a){switch(a.nodeType){case 1:if(k.test(a.className))break;if("BR"===a.nodeName)h(a),
-a.parentNode&&a.parentNode.removeChild(a);else for(a=a.firstChild;a;a=a.nextSibling)e(a);break;case 3:case 4:if(p){var b=a.nodeValue,d=b.match(t);if(d){var c=b.substring(0,d.index);a.nodeValue=c;(b=b.substring(d.index+d[0].length))&&a.parentNode.insertBefore(s.createTextNode(b),a.nextSibling);h(a);c||a.parentNode.removeChild(a)}}}}function h(a){function b(a,d){var e=d?a.cloneNode(!1):a,f=a.parentNode;if(f){var f=b(f,1),g=a.nextSibling;f.appendChild(e);for(var h=g;h;h=g)g=h.nextSibling,f.appendChild(h)}return e}
-for(;!a.nextSibling;)if(a=a.parentNode,!a)return;for(var a=b(a.nextSibling,0),e;(e=a.parentNode)&&e.nodeType===1;)a=e;d.push(a)}var k=/(?:^|\s)nocode(?:\s|$)/,t=/\r\n?|\n/,s=a.ownerDocument,l;a.currentStyle?l=a.currentStyle.whiteSpace:window.getComputedStyle&&(l=s.defaultView.getComputedStyle(a,q).getPropertyValue("white-space"));var p=l&&"pre"===l.substring(0,3);for(l=s.createElement("LI");a.firstChild;)l.appendChild(a.firstChild);for(var d=[l],g=0;g<d.length;++g)e(d[g]);m===(m|0)&&d[0].setAttribute("value",
-m);var r=s.createElement("OL");r.className="linenums";for(var n=Math.max(0,m-1|0)||0,g=0,z=d.length;g<z;++g)l=d[g],l.className="L"+(g+n)%10,l.firstChild||l.appendChild(s.createTextNode("\xa0")),r.appendChild(l);a.appendChild(r)}function k(a,m){for(var e=m.length;--e>=0;){var h=m[e];A.hasOwnProperty(h)?window.console&&console.warn("cannot override language handler %s",h):A[h]=a}}function C(a,m){if(!a||!A.hasOwnProperty(a))a=/^\s*</.test(m)?"default-markup":"default-code";return A[a]}function E(a){var m=
-a.g;try{var e=M(a.h),h=e.a;a.a=h;a.c=e.c;a.d=0;C(m,h)(a);var k=/\bMSIE\b/.test(navigator.userAgent),m=/\n/g,t=a.a,s=t.length,e=0,l=a.c,p=l.length,h=0,d=a.e,g=d.length,a=0;d[g]=s;var r,n;for(n=r=0;n<g;)d[n]!==d[n+2]?(d[r++]=d[n++],d[r++]=d[n++]):n+=2;g=r;for(n=r=0;n<g;){for(var z=d[n],f=d[n+1],b=n+2;b+2<=g&&d[b+1]===f;)b+=2;d[r++]=z;d[r++]=f;n=b}for(d.length=r;h<p;){var o=l[h+2]||s,c=d[a+2]||s,b=Math.min(o,c),i=l[h+1],j;if(i.nodeType!==1&&(j=t.substring(e,b))){k&&(j=j.replace(m,"\r"));i.nodeValue=
-j;var u=i.ownerDocument,v=u.createElement("SPAN");v.className=d[a+1];var x=i.parentNode;x.replaceChild(v,i);v.appendChild(i);e<o&&(l[h+1]=i=u.createTextNode(t.substring(b,o)),x.insertBefore(i,v.nextSibling))}e=b;e>=o&&(h+=2);e>=c&&(a+=2)}}catch(w){"console"in window&&console.log(w&&w.stack?w.stack:w)}}var v=["break,continue,do,else,for,if,return,while"],w=[[v,"auto,case,char,const,default,double,enum,extern,float,goto,int,long,register,short,signed,sizeof,static,struct,switch,typedef,union,unsigned,void,volatile"],
-"catch,class,delete,false,import,new,operator,private,protected,public,this,throw,true,try,typeof"],F=[w,"alignof,align_union,asm,axiom,bool,concept,concept_map,const_cast,constexpr,decltype,dynamic_cast,explicit,export,friend,inline,late_check,mutable,namespace,nullptr,reinterpret_cast,static_assert,static_cast,template,typeid,typename,using,virtual,where"],G=[w,"abstract,boolean,byte,extends,final,finally,implements,import,instanceof,null,native,package,strictfp,super,synchronized,throws,transient"],
-H=[G,"as,base,by,checked,decimal,delegate,descending,dynamic,event,fixed,foreach,from,group,implicit,in,interface,internal,into,is,lock,object,out,override,orderby,params,partial,readonly,ref,sbyte,sealed,stackalloc,string,select,uint,ulong,unchecked,unsafe,ushort,var"],w=[w,"debugger,eval,export,function,get,null,set,undefined,var,with,Infinity,NaN"],I=[v,"and,as,assert,class,def,del,elif,except,exec,finally,from,global,import,in,is,lambda,nonlocal,not,or,pass,print,raise,try,with,yield,False,True,None"],
-J=[v,"alias,and,begin,case,class,def,defined,elsif,end,ensure,false,in,module,next,nil,not,or,redo,rescue,retry,self,super,then,true,undef,unless,until,when,yield,BEGIN,END"],v=[v,"case,done,elif,esac,eval,fi,function,in,local,set,then,until"],K=/^(DIR|FILE|vector|(de|priority_)?queue|list|stack|(const_)?iterator|(multi)?(set|map)|bitset|u?(int|float)\d*)/,N=/\S/,O=u({keywords:[F,H,w,"caller,delete,die,do,dump,elsif,eval,exit,foreach,for,goto,if,import,last,local,my,next,no,our,print,package,redo,require,sub,undef,unless,until,use,wantarray,while,BEGIN,END"+
-I,J,v],hashComments:!0,cStyleComments:!0,multiLineStrings:!0,regexLiterals:!0}),A={};k(O,["default-code"]);k(x([],[["pln",/^[^<?]+/],["dec",/^<!\w[^>]*(?:>|$)/],["com",/^<\!--[\S\s]*?(?:--\>|$)/],["lang-",/^<\?([\S\s]+?)(?:\?>|$)/],["lang-",/^<%([\S\s]+?)(?:%>|$)/],["pun",/^(?:<[%?]|[%?]>)/],["lang-",/^<xmp\b[^>]*>([\S\s]+?)<\/xmp\b[^>]*>/i],["lang-js",/^<script\b[^>]*>([\S\s]*?)(<\/script\b[^>]*>)/i],["lang-css",/^<style\b[^>]*>([\S\s]*?)(<\/style\b[^>]*>)/i],["lang-in.tag",/^(<\/?[a-z][^<>]*>)/i]]),
-["default-markup","htm","html","mxml","xhtml","xml","xsl"]);k(x([["pln",/^\s+/,q," \t\r\n"],["atv",/^(?:"[^"]*"?|'[^']*'?)/,q,"\"'"]],[["tag",/^^<\/?[a-z](?:[\w-.:]*\w)?|\/?>$/i],["atn",/^(?!style[\s=]|on)[a-z](?:[\w:-]*\w)?/i],["lang-uq.val",/^=\s*([^\s"'>]*(?:[^\s"'/>]|\/(?=\s)))/],["pun",/^[/<->]+/],["lang-js",/^on\w+\s*=\s*"([^"]+)"/i],["lang-js",/^on\w+\s*=\s*'([^']+)'/i],["lang-js",/^on\w+\s*=\s*([^\s"'>]+)/i],["lang-css",/^style\s*=\s*"([^"]+)"/i],["lang-css",/^style\s*=\s*'([^']+)'/i],["lang-css",
-/^style\s*=\s*([^\s"'>]+)/i]]),["in.tag"]);k(x([],[["atv",/^[\S\s]+/]]),["uq.val"]);k(u({keywords:F,hashComments:!0,cStyleComments:!0,types:K}),["c","cc","cpp","cxx","cyc","m"]);k(u({keywords:"null,true,false"}),["json"]);k(u({keywords:H,hashComments:!0,cStyleComments:!0,verbatimStrings:!0,types:K}),["cs"]);k(u({keywords:G,cStyleComments:!0}),["java"]);k(u({keywords:v,hashComments:!0,multiLineStrings:!0}),["bsh","csh","sh"]);k(u({keywords:I,hashComments:!0,multiLineStrings:!0,tripleQuotedStrings:!0}),
-["cv","py"]);k(u({keywords:"caller,delete,die,do,dump,elsif,eval,exit,foreach,for,goto,if,import,last,local,my,next,no,our,print,package,redo,require,sub,undef,unless,until,use,wantarray,while,BEGIN,END",hashComments:!0,multiLineStrings:!0,regexLiterals:!0}),["perl","pl","pm"]);k(u({keywords:J,hashComments:!0,multiLineStrings:!0,regexLiterals:!0}),["rb"]);k(u({keywords:w,cStyleComments:!0,regexLiterals:!0}),["js"]);k(u({keywords:"all,and,by,catch,class,else,extends,false,finally,for,if,in,is,isnt,loop,new,no,not,null,of,off,on,or,return,super,then,true,try,unless,until,when,while,yes",
-hashComments:3,cStyleComments:!0,multilineStrings:!0,tripleQuotedStrings:!0,regexLiterals:!0}),["coffee"]);k(x([],[["str",/^[\S\s]+/]]),["regex"]);window.prettyPrintOne=function(a,m,e){var h=document.createElement("PRE");h.innerHTML=a;e&&D(h,e);E({g:m,i:e,h:h});return h.innerHTML};window.prettyPrint=function(a){function m(){for(var e=window.PR_SHOULD_USE_CONTINUATION?l.now()+250:Infinity;p<h.length&&l.now()<e;p++){var n=h[p],k=n.className;if(k.indexOf("prettyprint")>=0){var k=k.match(g),f,b;if(b=
-!k){b=n;for(var o=void 0,c=b.firstChild;c;c=c.nextSibling)var i=c.nodeType,o=i===1?o?b:c:i===3?N.test(c.nodeValue)?b:o:o;b=(f=o===b?void 0:o)&&"CODE"===f.tagName}b&&(k=f.className.match(g));k&&(k=k[1]);b=!1;for(o=n.parentNode;o;o=o.parentNode)if((o.tagName==="pre"||o.tagName==="code"||o.tagName==="xmp")&&o.className&&o.className.indexOf("prettyprint")>=0){b=!0;break}b||((b=(b=n.className.match(/\blinenums\b(?::(\d+))?/))?b[1]&&b[1].length?+b[1]:!0:!1)&&D(n,b),d={g:k,h:n,i:b},E(d))}}p<h.length?setTimeout(m,
-250):a&&a()}for(var e=[document.getElementsByTagName("pre"),document.getElementsByTagName("code"),document.getElementsByTagName("xmp")],h=[],k=0;k<e.length;++k)for(var t=0,s=e[k].length;t<s;++t)h.push(e[k][t]);var e=q,l=Date;l.now||(l={now:function(){return+new Date}});var p=0,d,g=/\blang(?:uage)?-([\w.]+)(?!\S)/;m()};window.PR={createSimpleLexer:x,registerLangHandler:k,sourceDecorator:u,PR_ATTRIB_NAME:"atn",PR_ATTRIB_VALUE:"atv",PR_COMMENT:"com",PR_DECLARATION:"dec",PR_KEYWORD:"kwd",PR_LITERAL:"lit",
-PR_NOCODE:"nocode",PR_PLAIN:"pln",PR_PUNCTUATION:"pun",PR_SOURCE:"src",PR_STRING:"str",PR_TAG:"tag",PR_TYPE:"typ"}})();
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/doc-files/userguide.html b/lucene/facet/src/java/org/apache/lucene/facet/doc-files/userguide.html
deleted file mode 100755
index 04c9fa5..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/doc-files/userguide.html
+++ /dev/null
@@ -1,788 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<title>Facet Userguide</title>
-
-<!-- load stylesheet + javascript in distribution mode -->
-<link rel="stylesheet" type="text/css" href="prettify.css">
-<script src="prettify.js" type="text/javascript"></script>
-
-<script language="javascript">
-	window.onload=function() {
-		prettyPrint();
-	}
-</script>
-
-<style>
-body {
-  margin-left: 20%;
-  width: 60%;
-  counter-reset: section;
-  text-align: left;
-}
-
-h1.title {
-  text-align: center;
-  margin-top: 30px;
-  font-size: 5em;
-  line-height: 150%;
-}
-
-h1.section {
-  margin-top: 50px;
-  font-size: 2.5em;
-  counter-reset: subsection;
-  border: 1px solid black;
-  background-color: #D8D8D8;
-  padding-left: 5px;
-}
-
-h2.subsection {
-  font-size: 2em;
-  border: 1px solid black;
-  background-color: #D8D8D8;
-  padding-left: 5px;
-}
-
-/* auto-generated heading numbers */
-h1.section:before {
-counter-increment: section;
-content: counter(section) ". ";
-}
-
-h2.subsection:before  {
-counter-increment: subsection;
-content: counter(section) "." counter(subsection) " ";
-}
-
-/* override from prettify.css - add shadow, padding etc. */
-pre.prettyprint {
-  margin-left: 2%;
-  width: 80%;
-  padding: 5px 3px 5px 3px;
-  /* shadow */
-  -moz-box-shadow: 5px 5px 2px #888;
-  -webkit-box-shadow: 5px 5px 2px #888;
-  box-shadow: 5px 5px 2px #888;
-}
-
-/* override from prettify.css - make keywords appear in bold */
-span.kwd {
-  font-weight: bold;
-}
-
-ol.toc a {
-  text-decoration: none;
-  color: blue;
-}
-
-li.toc_first {
-  // margin-top: 10px;
-  font-size: 16px;
-  color: blue;
-}
-
-li.toc_second {
-  font-size: 14px;
-  margin-left: 15px;
-  color: blue;
-}
-
-/* reset style from prettify.css, so that line numbers appear in each line */
-li.L0,li.L1,li.L2,li.L3,li.L5,li.L6,li.L7,li.L8 {
-  list-style-type:decimal
-}
-
-table.code_description td {
-  vertical-align: top;
-}
-
-</style>
-
-<body>
-<h1 class="title">
-	Apache Lucene<br>
-	Faceted Search<br>
-	User's Guide</h1>
-
-<div class="toc">
-<h1 class="toc">Table of Contents</h1>
-<ol class="toc">
-<li class="toc_first"><a href="#intro">Introduction</a></li>
-<li class="toc_first"><a href="#facet_features">Facet Features</a></li>
-<li class="toc_first"><a href="#facet_indexing">Indexing Categories Illustrated</a></li>
-<li class="toc_first"><a href="#facet_accumulation">Accumulating Facets Illustrated</a></li>
-<li class="toc_first"><a href="#indexed_facet_info">Indexed Facet Information</a></li>
-<li class="toc_first"><a href="#taxonomy_index">Taxonomy Index</a></li>
-<li class="toc_first"><a href="#facet_params">Facet Parameters</a></li>
-<li class="toc_first"><a href="#advanced">Advanced Faceted Examples</a></li>
-<li class="toc_first"><a href="#optimizations">Optimizations</a></li>
-<li class="toc_first"><a href="#concurrent_indexing_search">Concurrent Indexing and Search</a></li>
-</ol>
-</div>
-
-<h1 class="section"><a name="intro">Introduction</a></h1>
-<p>
-A category is an aspect of indexed documents which can be used to classify the
-documents. For example, in a collection of books at an online bookstore, categories of
-a book can be its price, author, publication date, binding type, and so on.
-<p>
-In faceted search, in addition to the standard set of search results, we also get facet
-results, which are lists of subcategories for certain categories. For example, for the
-price facet, we get a list of relevant price ranges; for the author facet, we get a list of
-relevant authors; and so on. In most UIs, when users click one of these subcategories,
-the search is narrowed, or drilled down, and a new search limited to this subcategory
-(e.g., to a specific price range or author) is performed.
-<p>
-Note that faceted search is more than just the ordinary fielded search. In fielded
-search, users can add search keywords like price:10 or author:"Mark
-Twain" to the query to narrow the search, but this requires knowledge of which
-fields are available, and which values are worth trying. This is where faceted search
-comes in: it provides a list of useful subcategories, which ensures that the user only
-drills down into useful subcategories and never into a category for which there are no
-results. In essence, faceted search makes it easy to navigate through the search results.
-The list of subcategories provided for each facet is also useful to the user in itself,
-even when the user never drills down. This list allows the user to see at one glance
-some statistics on the search results, e.g., what price ranges and which authors are
-most relevant to the given query.
-<p>
-In recent years, faceted search has become a very common UI feature in search
-engines, especially in e-commerce websites. Faceted search makes it easy for
-untrained users to find the specific item they are interested in, whereas manually
-adding search keywords (as in the examples above) proved too cumbersome for
-ordinary users, and required too much guesswork, trial-and-error, or the reading of
-lengthy help pages.
-<p>
-See <a href="http://en.wikipedia.org/wiki/Faceted_search">http://en.wikipedia.org/wiki/Faceted_search</a> for more information on faceted
-search.
-
-<h1 class="section"><a name="facet_features">Facet Features</a></h1>
-First and main faceted search capability that comes to mind is counting, but in fact
-faceted search is more than facet counting. We now briefly discuss the available
-faceted search features.
-
-<h2 class="subsection">Facet Counting</h2>
-<p>
-Which of the available subcategories of a facet should a UI display? A query in a
-book store might yield books by a hundred different authors, but normally we'd want
-do display only, say, ten of those.
-<p>
-Most available faceted search implementations use counts to determine the
-importance of each subcategory. These implementations go over all search results for
-the given query, and count how many results are in each subcategory. Finally, the
-subcategories with the most results can be displayed. So the user sees the price ranges,
-authors, and so on, for which there are most results. Often, the count is displayed next
-to the subcategory name, in parentheses, telling the user how many results he can
-expect to see if he drills down into this subcategory.
-<p>
-The main API for obtaining facet counting is <code>CountFacetRequest</code>, as in the
-following code snippet:
-<pre class="prettyprint lang-java">
-new CountFacetRequest(new CategoryPath("author"), 10));
-</pre>
-A detailed code example using count facet requests is shown below - see
-<a href="#facet_accumulation">Accumulating Facets</a>.
-
-<h2 class="subsection"><a name="facet_association">Facet Associations</a></h2>
-<p>
-So far we've discussed categories as binary features, where a document either belongs
-to a category, or not.
-<p>
-While counts are useful in most situations, they are sometimes not sufficiently
-informative for the user, with respect to deciding which subcategory is more
-important to display.
-<p>
-For this, the facets package allows to associate a value with a category. The search
-time interpretation of the associated value is application dependent. For example, a
-possible interpretation is as a <i>match level</i> (e.g., confidence level). This value can
-then be used so that a document that is very weakly associated with a certain category
-will only contribute little to this category's aggregated weight.
-
-<h2 class="subsection"><a name="multiple_requests">Multiple Facet Requests</a></h2>
-<p>
-A single faceted accumulation is capable of servicing multiple facet requests.
-Programmatic, this is quite simple - wrap all the facet requests of interest into the
-facet-search-parameters which are passed to a facets accumulator/collector (more on
-these objects below). The results would be comprised of as many facet results as there
-were facet requests.
-<p>
-However there is a delicate <b>limitation</b>: all facets maintained in the same location in
-the index are required to be treated the same. See the section on <a href="#indexing_params">Indexing Parameters</a>
-for an explanation on maintaining certain facets at certain locations.
-
-<h2 class="subsection"><a name="facet_labels">Facet Labels at Search Time</a></h2>
-<p>
-Facets results always contain the facet (internal) ID and (accumulated) value. Some of
-the results also contain the facet label, AKA the category name. We mention this here
-since computing the label is a time consuming task, and hence applications can
-specify with a facet request to return top 1000 facets but to compute the label only for
-the top 10 facets. In order to compute labels for more of the facet results it is not
-required to perform accumulation again.
-<p>
-See <code>FacetRequest.getNumResults()</code>, <code>FacetRequest.getNumLabel()</code> and
-<code>FacetResultNode.getLabel(TaxonomyReader)</code>.
-
-<h1 class="section"><a name="facet_indexing">Indexing Categories Illustrated</a></h1>
-<p>
-In order to find facets at search time they must first be added to the index at indexing
-time. Recall that Lucene documents are made of fields for textual search. The addition
-of categories is performed by an appropriate <code>DocumentBuilder</code> - or
-<code>CategoryDocumentBuilder</code> in our case.
-<p>
-Indexing therefore usually goes like this:
-<ul>
-<li>For each input document:
-<ul>
-<li>Create a fresh (empty) Lucene Document</li>
-<li>Parse input text and add appropriate text search fields</li>
-<li><b>Gather all input categories associated with the document and create
-a CategoryDocumentBuilder with the list of categories</b></li>
-<li><b>Build the document - this actually adds the categories to the
-Lucene document.</b></li>
-<li>Add the document to the index</li>
-</ul></li>
-</ul>
-Following is a code snippet for indexing categories. The complete example can be
-found in package <code>org.apache.lucene.facet.example.simple.SimpleIndexer</code>.
-<pre class="prettyprint lang-java linenums">
-IndexWriter writer = ...
-TaxonomyWriter taxo = new DirectoryTaxonomyWriter(taxoDir);
-...
-Document doc = new Document();
-doc.add(new Field("title", titleText, Store.YES, Index.ANALYZED));
-...
-List&lt;CategoryPath&gt; categories = new ArrayList&lt;CategoryPath&gt;();
-categories.add(new CategoryPath("author", "Mark Twain"));
-categories.add(new CategoryPath("year", "2010"));
-...
-DocumentBuilder categoryDocBuilder = new CategoryDocumentBuilder(taxo);
-categoryDocBuilder.setCategoryPaths(categories);
-categoryDocBuilder.build(doc);
-writer.addDocument(doc);
-</pre>
-<p>
-We now explain the steps above, following the code line numbers:
-<table class="code_description">
-<tr>
-	<td>(4)</td>
-	<td>Document contains not only text search fields but also facet search
-information.</td>
-</tr>
-<tr>
-	<td>(7)</td>
-	<td>Prepare a container for document categories.</td>
-</tr>
-<tr>
-	<td>(8)</td>
-	<td>Categories that should be added to the document are accumulated in the
-categories list.</td>
-</tr>
-<tr>
-	<td>(11)</td>
-	<td>A <code>CategoryDocumentBuilder</code> is created, set with the appropriate list
-of categories, and invoked to build - that is, to populate the document
-with categories. It is in this step that the taxonomy is updated to contain the
-newly added categories (if not already there) - see more on this in the
-section about the <a href="#taxonomy_index">Taxonomy Index</a> below. This line could be made more
-compact: one can create a single <code>CategoryDocumentBuilder cBuilder</code> and reuse it like this:
-<pre class="prettyprint lang-java linenums">
-DocumentBuilder cBuilder = new CategoryDocumentBuilder(taxo);
-cBuilder.setCategoryPaths(categories).build(doc);
-</pre>
-	</td>
-</tr>
-<tr>
-	<td>(14)</td>
-	<td>Add the document to the index. As a result, category info is saved also in
-the regular search index, for supporting facet aggregation at search time
-(e.g. facet counting) as well as facet drill-down. For more information on
-indexed facet information see below the section <a href="#indexed_facet_info">Indexed Facet Information</a>.</td>
-</tr>
-</table>
-
-<h1 class="section"><a name="facet_accumulation">Accumulating Facets Illustrated</a></h1>
-<p>
-Facets accumulation reflects a set of documents over some facet requests:
-<ul>
-<li><code>Document set</code> - a subset of the index documents, usually documents
-matching a user query.</li>
-<li><code>Facet requests</code> - facet accumulation specification, e.g. count a certain facet
-<i>dimension</i>.</li>
-</ul>
-<p>
-<code>FacetRequest</code> is a basic component in faceted search - it describes the facet
-information need. Every facet <b>request</b> is made of at least two fields:
-<ul>
-<li><code>CategoryPath</code> - root category of the facet request. The categories that
-are returned as a result of the request will all be descendants of this root</li>
-<li><code>Number of Results</code> - number of sub-categories to return (at most).</li>
-</ul>
-<p>
-There are other parameters to a facet request, such as -how many facet results to
-label-, -how <b>deep</b> to go from the request root when serving the facet request- and
-more - see the API Javadocs for <code>FacetRequest</code> and its subclasses for more
-information on these parameters. For labels in particular, see the section <a href="#facet_labels">Facet Labels
-at Search Time</a>.
-<p>
-<code>FacetRequest</code> in an abstract class, open for extensions, and users may add their
-own requests. The most often used request is <code>CountFacetRequest</code> - used for
-counting facets.
-<p>
-Facets accumulation is - not surprisingly - driven by a <code>FacetsAccumulator</code>. The
-most used one is <code>StandardFacetsAccumulator</code>, however there are also accumulators
-that support sampling - to be used in huge collections, and there's an adaptive facets
-accumulator which applies sampling conditionally on the statistics of the data. While
-facets accumulators are very extendible and powerful, they might be too
-overwhelming for beginners. For this reason, the code offers a higher level interface
-for facets accumulating: the <code>FacetsCollector</code>. It extends <code>Collector</code>, and as such
-can be passed to the search() method of Lucene's <code>IndexSearcher</code>. In case the
-application also needs to collect documents (in addition to accumulating/collecting
-facets), it can wrap multiple collectors with <code>MultiCollector</code>. Most code samples
-below use <code>FacetsCollector</code> due to its simple interface. It is quite likely that
-<code>FacetsCollector</code> should suffice the needs of most applications, therefore we
-recommend to start with it, and only when needing more flexibility turn to directly
-use facets accumulators.
-<p>
-Following is a code snippet from the example code - the complete example can be
-found under <code>org.apache.lucene.facet.example.simple.Searcher</code>:
-<pre class="prettyprint lang-java linenums">
-IndexReader indexReader = DirectoryReader.open(indexDir);
-IndexSearcher searcher = new IndexSearcher(indexReader);
-TaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-...
-Query q = new TermQuery(new Term(SimpleUtils.TEXT, "white"));
-TopScoreDocCollector tdc = TopScoreDocCollector.create(10, true);
-...
-FacetSearchParams facetSearchParams = new FacetSearchParams();
-facetSearchParams.addFacetRequest(new CountFacetRequest(
-    new CategoryPath("author"), 10));
-...
-FacetsCollector facetsCollector = new FacetsCollector(facetSearchParams, indexReader, taxo);
-searcher.search(q, MultiCollector.wrap(topDocsCollector, facetsCollector));
-List&lt;FacetResult&gt; res = facetsCollector.getFacetResults();
-</pre>
-<p>
-We now explain the steps above, following the code line numbers:
-<table class="code_description">
-<tr>
-	<td>(1)</td>
-	<td>Index reader and Searcher are initialized as usual.</td>
-</tr>
-<tr>
-	<td>(3)</td>
-	<td>A taxonomy reader is opened - it provides access to the facet information
-which was stored by the Taxonomy Writer at indexing time.</td>
-</tr>
-<tr>
-	<td>(5)</td>
-	<td>Regular text query is created to find the documents matching user need, and
-a collector for collecting the top matching documents is created.</td>
-</tr>
-<tr>
-	<td>(8)</td>
-	<td>Facet-search-params is a container for facet requests.</td>
-</tr>
-<tr>
-	<td>(10)</td>
-	<td>A single facet-request - namely a count facet request - is created and added
-to the facet search params. The request should return top 10 Author
-subcategory counts.</td>
-</tr>
-<tr>
-	<td>(12)</td>
-	<td>Facets-Collector is the simplest interface for facets accumulation (counting
-in this example).</td>
-</tr>
-<tr>
-	<td>(13)</td>
-	<td>Lucene search takes both collectors - facets-collector and top-doccollector,
-both wrapped by a multi-collector. This way, a single search
-operation finds both top documents and top facets. Note however that facets
-aggregation takes place not only over the top documents, but rather over all
-documents matching the query.</td>
-</tr>
-<tr>
-	<td>(14)</td>
-	<td>Once search completes, facet-results can be obtained from the facetscollector.</td>
-</tr>
-</table>
-
-<p>
-Returned facet results are organized in a list, conveniently ordered the same as the
-facet-requests in the facet-search-params. Each result however contains the request
-for which it was created.</li>
-<p>
-Here is the (recursive) structure of the facet result:
-<ul>
-<li><b>Facet Result</b>
-<ul>
-<li><b>Facet Request</b> - the request for which this result was obtained.</li>
-<li><b>Valid Descendants</b> - how many valid descendants were encountered
-over the set of matching documents (some of which might have been
-filtered out because e.g. only top 10 results were requested).</li>
-<li><b>Root Result Node</b> - root facet result for the request
-<ul>
-<li><b>Ordinal</b> - unique internal ID of the facet</li>
-<li><b>Label</b> - full label of the facet (possibly null)</li>
-<li><b>Value</b> - facet value, e.g. count</li>
-<li><b>Sub-results-nodes</b> - child result nodes (possibly null)</li>
-</ul></li>
-</ul></li>
-</ul>
-<p>
-Note that not always there would be sub result nodes - this depends on the
-requested result mode:
-<ul>
-<li><b>PER_NODE_IN_TREE</b> - a tree, and so there may be sub results.</li>
-<li><b>GLOBAL_FLAT</b> - here the results tree would be rather flat, with only (at
-most) leaves below the root result node.</li>
-</ul>
-
-<h1 class="section"><a name="indexed_facet_info">Indexed Facet Information</a></h1>
-<p>
-When indexing a document to which categories were added, information on these
-categories is added to the search index, in two locations:
-<ul>
-<li><i>Category Tokens</i> are added to the document for each category attached to
-that document. These categories can be used at search time for drill-down.</li>
-<li>A special <i>Category List Token</i> is added to each document containing
-information on all the categories that were added to this document. This can
-be used at search time for facet accumulation, e.g. facet counting.</li>
-</ul>
-<p>
-When a category is added to the index (that is, when a document containing a
-category is indexed), all its parent categories are added as well. For example, indexing
-a document with the category <code>&lt;<span style="color: blue">"author"</span>, 
-<span style="color: blue">"American-</span>, <span style="color: blue">"Mark Twain"</span>&gt;</code> results in
-creating three tokens: <code>"/author"</code>, <code>"/author/American"</code>, and
-<code>"/author/American/Mark Twain"</code> (the character <code>'/'</code> here is just a human
-readable separator - there's no such element in the actual index). This allows drilling down
-and counting any category in the taxonomy, and not just leaf nodes, enabling a
-UI application to show either how many books have authors, or how many books
-have American authors, or how many books have Mark Twain as their (American)
-author.
-<p>
-Similarly, Drill-down capabilities are this way possible also for node categories.
-<p>
-In order to keep the counting list compact, it is built using category ordinal - an
-ordinal is an integer number attached to a category when it is added for the first time
-into the taxonomy.
-<p>
-For ways to further alter facet index see the section below on <a href="#indexing_params">Facet Indexing
-Parameters</a>.
-
-<h1 class="section"><a name="taxonomy_index">Taxonomy Index</a></h1>
-<p>
-The taxonomy is an auxiliary data-structure maintained side-by-side with the regular
-index to support faceted search operations. It contains information about all the
-categories that ever existed in any document in the index. Its API is open and allows
-simple usage, or more advanced for the interested users.
-<p>
-When a category is added to a document, a corresponding node is added to the
-taxonomy (unless already there). In fact, sometimes more than one node is added -
-each parent category is added as well, so that the taxonomy is maintained as a Tree,
-with a virtual root.
-<p>
-So, for the above example, adding the category the category <code>&lt;<span style="color: blue">"author"</span>, 
-<span style="color: blue">"American-</span>, <span style="color: blue">"Mark Twain"</span>&gt;</code> 
-actually added three nodes: one for <code>"/author"</code>, one for <code>"/author/American"</code> and one for 
-<code>"/author/American/Mark Twain"</code>.
-<p>
-An integer number - called ordinal is attached to each category the first time the
-category is added to the taxonomy. This allows for a compact representation of
-category list tokens in the index, for facets accumulation.
-<p>
-One interesting fact about the taxonomy index is worth knowing: once a category
-is added to the taxonomy, it is never removed, even if all related documents are
-removed. This differs from a regular index, where if all documents containing a
-certain term are removed, and their segments are merged, the term will also be
-removed. This might cause a performance issue: large taxonomy means large ordinal
-numbers for categories, and hence large categories values arrays would be maintained
-during accumulation. It is probably not a real problem for most applications, but be
-aware of this. If, for example, an application at a certain point in time removes an
-index entirely in order to recreate it, or, if it removed all the documents from the index
-in order to re-populate it, it also makes sense in this opportunity to remove the
-taxonomy index and create a new, fresh one, without the unused categories.
-
-<h1 class="section"><a name="facet_params">Facet Parameters</a></h1>
-<p>
-Facet parameters control how categories and facets are indexed and searched. Apart
-from specifying facet requests within facet search parameters, under default settings it
-is not required to provide any parameters, as there are ready to use working defaults
-for everything.
-<p>
-However many aspects are configurable and can be modified by providing altered
-facet parameters for either search or indexing.
-
-<h2 class="subsection"><a name="indexing_params">Facet Indexing Parameters</a></h2>
-<p>
-Facet Indexing Parameters are consulted with during indexing. Among several
-parameters it defines, the following two are likely to interest many applications:
-<ul>
-<li><b>Category list definitions</b> - in the index, facets are maintained in two
-forms: category-tokens (for drill-down) and category-list-tokens (for
-accumulation). This parameter allows to specify, for each category, the
-Lucene term used for maintaining the category-list-tokens for that category.
-The default implementation in <code>FacetIndexingParams</code> maintains
-this information for all categories under the same special dedicated term.
-One case where it is needed to maintain two categories in separate category
-lists, is when it is known that at search time it would be required to use
-different types of accumulation logic for each, but at the same accumulation
-call.</li>
-<li><b>Partition size</b> - category lists can be maintained in a partitioned way. If,
-for example, the partition size is set to 1000, a distinct sub-term is used for
-maintaining each 1000 categories, e.g. term1 for categories 0 to 999, term2
-for categories 1000 to 1999, etc. The default implementation in
-<code>FacetIndexingParams</code> maintains category lists in a single
-partition, hence it defines the partition size as <code>Integer.MAX_VALUE</code>. The
-importance of this parameter is on allowing to handle very large
-taxonomies without exhausting RAM resources. This is because at facet
-accumulation time, facet values arrays are maintained in the size of the
-partition. With a single partition, the size of these arrays is as the size of the
-taxonomy, which might be OK for most applications. Limited partition
-sizes allow to perform the accumulation with less RAM, but with some
-runtime overhead, as the matching documents are processed for each of the
-partitions.</li>
-</ul>
-<p>
-See the API Javadocs of <code>FacetIndexingParams</code> for additional configuration
-capabilities which were not discussed here.
-
-<h2 class="subsection"><a name="search_params">Facet Search Parameters</a></h2>
-<p>
-Facet Search Parameters, consulted at search time (during facets accumulation) are
-rather plain, providing the following:
-<ul>
-<li><b>Facet indexing parameters</b> - which were in effect at indexing time -
-allowing facets accumulation to understand how facets are maintained in
-the index.</li>
-<li><b>Container of facet requests</b> - the requests which should be accumulated.</li>
-</ul>
-
-<h2 class="subsection"><a name="category_lists_multiple_dimensions">Category Lists, Multiple Dimensions</a></h2>
-<p>
-Category list parameters which are accessible through the facet indexing parameters
-provide the information about:
-<ul>
-<li>Lucene Term under which category information is maintained in the index.</li>
-<li>Encoding (and decoding) used for writing and reading the categories
-information in the index.</li>
-</ul>
-<p>
-For cases when certain categories should be maintained in different location than
-others, use <code>PerDimensionIndexingParams</code>, which returns a different
-<code>CategoryListParams</code> object for each <i>dimension</i>. This is a good opportunity to
-explain about dimensions. This is just a notion: the top element - or first element - in
-a category path is denoted as the dimension of that category. Indeed, the dimension
-stands out as a top important part of the category path, such as <code>"Location"</code> for the
-category <code>"Location/Europe/France/Paris"</code>.
-
-<h1 class="section"><a name="advanced">Advanced Faceted Examples</a></h1>
-<p>
-We now provide examples for more advanced facet indexing and search, such as
-drilling-down on facet values and multiple category lists.
-
-<h2 class="subsection"><a name="drill_down">Drill-Down with Regular Facets</a></h2>
-<p>
-Drill-down allows users to focus on part of the results. Assume a commercial sport
-equipment site where a user is searching for a tennis racquet. The user issues the
-query <i>tennis racquet</i> and as result is shown a page with 10 tennis racquets, by
-various providers, of various types and prices. In addition, the site UI shows to the
-user a break down of all available racquets by price and make. The user now decides
-to focus on racquets made by <i>Head</i>, and will now be shown a new page, with 10
-Head racquets, and new break down of the results into racquet types and prices.
-Additionally, the application can choose to display a new breakdown, by racquet
-weights. This step of moving from results (and facet statistics) of the entire (or larger)
-data set into a portion of it by specifying a certain category, is what we call <i>Drilldown</i>.
-We now show the required code lines for implementing such a drill-down.
-<pre class="prettyprint lang-java linenums">
-Query baseQuery = queryParser.parse("tennis racquet");
-DrillDownQuery q2 = new DrillDownQuery(indexingParams, baseQuery);
-q2.add(new CategoryPath("make", "head"), 10));
-</pre>
-<p>
-In line 1 the original user query is created and then used to obtain information on
-all tennis racquets.
-<p>
-In line 2, a specific category from within the facet results was selected by the user,
-and is hence used for creating the drill-down query.
-<p>
-Please refer to <code>SimpleSearcher.searchWithDrillDown()</code> for a more detailed
-code example performing drill-down.
-
-<h2 class="subsection"><a name="multi-category_list">Multiple Category Lists</a></h2>
-<p>
-The default is to maintain all categories information in a single list. While this will
-suit most applications, in some situations an application may wish to use multiple
-category lists, for example, when the distribution of some category values is different
-than that of other categories and calls for using a different encoding, more efficient
-for the specific distribution. Another example is when most facets are rarely used
-while some facets are used very heavily, so an application may opt to maintain the
-latter in memory - and in order to keep memory footprint lower it is useful to
-maintain only those heavily used facets in a separate category list.
-<p>
-First we define indexing parameters with multiple category lists:
-<pre class="prettyprint lang-java linenums">
-PerDimensionIndexingParams iParams = new PerDimensionIndexingParams();
-iParams.addCategoryListParams(new CategoryPath("Author"), 
-    new CategoryListParams(new Term("$RarelyUsed", "Facets")));
-iParams.addCategoryListParams(new CategoryPath("Language"),
-    new CategoryListParams(new Term("$HeavilyUsed", "Ones")));
-</pre>
-<p>
-This will cause the Language categories to be maintained in one category list, and
-Author facets to be maintained in a another category list. Note that any other category,
-if encountered, will still be maintained in the default category list.
-<p>
-These non-default indexing parameters should now be used both at indexing and
-search time. As depicted below, at indexing time this is done when creating the
-category document builder, while at search time this is done when creating the search
-parameters. Other than that the faceted search code is unmodified.
-<pre class="prettyprint lang-java linenums">
-DocumentBuilder categoryDocBuilder = new CategoryDocumentBuilder(taxo, iParams);
-...
-FacetSearchParams facetSearchParams = new FacetSearchParams(iParams);
-</pre>
-<p>
-A complete simple example can be found in package <code>org.apache.lucene.facet.example.multiCL</code> 
-under the example code.
-
-<h1 class="section"><a name="optimizations">Optimizations</a></h1>
-<p>
-Faceted search through a large collection of documents with large numbers of facets
-altogether and/or large numbers of facets per document is challenging performance
-wise, either in CPU, RAM, or both. A few ready to use optimizations exist to tackle
-these challenges.
-
-<h2 class="subsection"><a name="sampling">Sampling</a></h2>
-<p>
-Facet sampling allows to accumulate facets over a sample of the matching
-documents set. In many cases, once top facets are found over the sample set, exact
-accumulations are computed for those facets only, this time over the entire matching
-document set.
-<p>
-Two kinds of sampling exist: complete support and wrapping support. The
-complete support is through <code>SamplingAccumulator</code> and is tied to an extension of the
-<code>StandardFacetsAccumulator</code> and has the benefit of automatically applying other
-optimizations, such as <a href="#complements">Complements</a>. The wrapping support is through
-<code>SamplingWrapper</code> and can wrap any accumulator, and as such, provides more
-freedom for applications.
-
-<h2 class="subsection"><a name="complements">Complements</a></h2>
-<p>
-When accumulating facets over a very large matching documents set, possibly
-almost as large as the entire collection, it is possible to speed up accumulation by
-looking at the complement set of documents, and then obtaining the actual results by
-subtracting from the total results. It should be noted that this is available only for
-count requests, and that the first invocation that involves this optimization might take
-longer because the total counts have to be computed.
-<p>
-This optimization is applied automatically by <code>StandardFacetsAccumulator</code>.
-
-<h2 class="subsection"><a name="partitions">Partitions</a></h2>
-<p>
-Partitions are also discussed in the section about <a href="#indexing_params">Facet Indexing parameters.</a>
-<p>
-Facets are internally accumulated by first accumulating all facets and later on
-extracting the results for the requested facets. During this process, accumulation
-arrays are maintained in the size of the taxonomy. For a very large taxonomy, with
-multiple simultaneous faceted search operations, this might lead to excessive memory
-footprint. Partitioning the faceted information allows to relax the memory usage, by
-maintaining the category lists in several partitions, and by processing one partition at
-a time. This is automatically done by <code>StandardFacetsAccumulator</code>. However the
-default partition size is <code>Integer.MAX_VALUE</code>, practically setting to a single partition,
-i.e. no partitions at all.
-<p>
-Decision to override this behavior and use multiple partitions must be taken at
-indexing time. Once the index is created and already contains category lists it is too
-late to modify this.
-<p>
-See <code>FacetIndexingParams.getPartitionSize()</code> for API to alter this default
-behavior.
-
-<h1 class="section"><a name="concurrent_indexing_search">Concurrent Indexing and Search</a></h1>
-<p>
-Sometimes, indexing is done once, and when the index is fully prepared, searching
-starts. However, in most real applications indexing is <i>incremental</i> (new data comes in
-once in a while, and needs to be indexed), and indexing often needs to happen while
-searching is continuing at full steam.
-<p>
-Luckily, Lucene supports multiprocessing - one process writing to an index while
-another is reading from it. One of the key insights behind how Lucene allows multiprocessing 
-is <i>Point In Time</i> semantics. The idea is that when an <code>IndexReader</code> is opened, 
-it gets a view of the index at the <i>point in time</i> it was opened. If an <code>IndexWriter</code> 
-in a different process or thread modifies the index, the reader does not know about it until a new 
-<code>IndexReader</code> is opened (or the reopen() method of an existing <code>IndexReader</code> is called).
-<p>
-In faceted search, we complicate things somewhat by adding a second index - the
-taxonomy index. The taxonomy API also follows point-in-time semantics, but this is
-not quite enough. Some attention must be paid by the user to keep those two indexes
-consistently in sync:
-<p>
-The main index refers to category numbers defined in the taxonomy index.
-Therefore, it is important that we open the <code>TaxonomyReader</code> <i>after</i> opening the
-IndexReader. Moreover, every time an IndexReader is reopen()ed, the
-TaxonomyReader needs to be refresh()'ed as well.
-<p>
-But there is one extra caution: whenever the application deems it has written
-enough information worthy a commit, it must <b>first</b> call commit() for the
-<code>TaxonomyWriter</code> and only <b>after</b> that call commit() for the <code>IndexWriter</code>. 
-Closing the indices should also be done in this order - <b>first</b> close the taxonomy, and only <b>after</b>
-that close the index.
-<p>
-To summarize, if you're writing a faceted search application where searching and
-indexing happens concurrently, please follow these guidelines (in addition to the usual
-guidelines on how to use Lucene correctly in the concurrent case):
-<ul>
-<li>In the indexing process:
-<ol>
-<li>Before a writer commit()s the IndexWriter, it must commit() the
-TaxonomyWriter. Nothing should be added to the index between these
-two commit()s.</li>
-<li>Similarly, before a writer close()s the IndexWriter, it must close() the
-TaxonomyWriter.</li>
-</ol></li>
-<li>In the searching process:
-<ol>
-<li>Open the IndexReader first, and then the TaxonomyReader.</li>
-<li>After a reopen() on the IndexReader, refresh() the TaxonomyReader.
-No search should be performed on the new IndexReader until refresh()
-has finished.</li>
-</ol></li>
-</ul>
-<p>
-Note that the above discussion assumes that the underlying file-system on which
-the index and the taxonomy are stored respects ordering: if index A is written before
-index B, then any reader finding a modified index B will also see a modified index A.
-<p>
-<b>Note:</b> <code>TaxonomyReader</code>'s refresh() is simpler than <code>IndexReader</code>'s reopen(). 
-While the latter keeps both the old and new reader open, the former keeps only the new reader. The reason 
-is that a new <code>IndexReader</code> might have modified old information (old documents deleted, for 
-example) so a thread which is in the middle of a search needs to continue using the old information. With 
-<code>TaxonomyReader</code>, however, we are guaranteed that existing categories are never deleted or modified - 
-the only thing that can happen is that new categories are added. Since search threads do not care if new categories 
-are added in the middle of a search, there is no reason to keep around the old object, and the new one suffices.
-<br><b>However</b>, if the taxonomy index was recreated since the <code>TaxonomyReader</code> was opened or
-refreshed, this assumption (that categories are forevr) no longer holds, and <code>refresh()</code> will 
-throw an <code>InconsistentTaxonomyException</code>, guiding the application to open 
-a new <code>TaxonomyReader</code> for up-to-date taxonomy data. (Old one can
-be closed as soon as it is no more used.)
-
-
-</body>
-</html>
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/package.html b/lucene/facet/src/java/org/apache/lucene/facet/index/package.html
index 1b94556..bc9b2ee 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/package.html
+++ b/lucene/facet/src/java/org/apache/lucene/facet/index/package.html
@@ -20,6 +20,5 @@
 </head>
 <body>
 Facets indexing code.
-
 </body>
 </html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/old/AdaptiveFacetsAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/old/AdaptiveFacetsAccumulator.java
new file mode 100644
index 0000000..fb9b377
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/old/AdaptiveFacetsAccumulator.java
@@ -0,0 +1,116 @@
+package org.apache.lucene.facet.old;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.sampling.RandomSampler;
+import org.apache.lucene.facet.sampling.Sampler;
+import org.apache.lucene.facet.sampling.SamplingAccumulator;
+import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetsAccumulator;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * {@link FacetsAccumulator} whose behavior regarding complements, sampling,
+ * etc. is not set up front but rather is determined at accumulation time
+ * according to the statistics of the accumulated set of documents and the
+ * index.
+ * <p>
+ * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
+ * does not guarantee accurate values for
+ * {@link FacetResult#getNumValidDescendants()}.
+ * 
+ * @lucene.experimental
+ */
+public final class AdaptiveFacetsAccumulator extends OldFacetsAccumulator {
+  
+  private Sampler sampler = new RandomSampler();
+
+  /**
+   * Create an {@link AdaptiveFacetsAccumulator} 
+   * @see OldFacetsAccumulator#OldFacetsAccumulator(FacetSearchParams, IndexReader, TaxonomyReader)
+   */
+  public AdaptiveFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader, 
+      TaxonomyReader taxonomyReader) {
+    super(searchParams, indexReader, taxonomyReader);
+  }
+
+  /**
+   * Create an {@link AdaptiveFacetsAccumulator}
+   * 
+   * @see OldFacetsAccumulator#OldFacetsAccumulator(FacetSearchParams,
+   *      IndexReader, TaxonomyReader, FacetArrays)
+   */
+  public AdaptiveFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader,
+      TaxonomyReader taxonomyReader, FacetArrays facetArrays) {
+    super(searchParams, indexReader, taxonomyReader, facetArrays);
+  }
+
+  /**
+   * Set the sampler.
+   * @param sampler sampler to set
+   */
+  public void setSampler(Sampler sampler) {
+    this.sampler = sampler;
+  }
+  
+  @Override
+  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
+    OldFacetsAccumulator delegee = appropriateFacetCountingAccumulator(docids);
+
+    if (delegee == this) {
+      return super.accumulate(docids);
+    }
+
+    return delegee.accumulate(docids);
+  }
+
+  /**
+   * Compute the appropriate facet accumulator to use.
+   * If no special/clever adaptation is possible/needed return this (self).
+   */
+  private OldFacetsAccumulator appropriateFacetCountingAccumulator(ScoredDocIDs docids) {
+    // Verify that searchPareams permit sampling/complement/etc... otherwise do default
+    if (!mayComplement()) {
+      return this;
+    }
+    
+    // Now we're sure we can use the sampling methods as we're in a counting only mode
+    
+    // Verify that sampling is enabled and required ... otherwise do default
+    if (sampler == null || !sampler.shouldSample(docids)) {
+      return this;
+    }
+    
+    SamplingAccumulator samplingAccumulator = new SamplingAccumulator(sampler, searchParams, indexReader, taxonomyReader);
+    samplingAccumulator.setComplementThreshold(getComplementThreshold());
+    return samplingAccumulator;
+  }
+
+  /**
+   * @return the sampler in effect
+   */
+  public final Sampler getSampler() {
+    return sampler;
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/old/Aggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/old/Aggregator.java
new file mode 100644
index 0000000..5ac80bf
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/old/Aggregator.java
@@ -0,0 +1,48 @@
+package org.apache.lucene.facet.old;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Aggregates the categories of documents given to
+ * {@link #aggregate(int, float, IntsRef)}. Note that the document IDs are local
+ * to the reader given to {@link #setNextReader(AtomicReaderContext)}.
+ * 
+ * @lucene.experimental
+ */
+public interface Aggregator {
+
+  /**
+   * Sets the {@link AtomicReaderContext} for which
+   * {@link #aggregate(int, float, IntsRef)} calls will be made. If this method
+   * returns false, {@link #aggregate(int, float, IntsRef)} should not be called
+   * for this reader.
+   */
+  public boolean setNextReader(AtomicReaderContext context) throws IOException;
+  
+  /**
+   * Aggregate the ordinals of the given document ID (and its score). The given
+   * ordinals offset is always zero.
+   */
+  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException;
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/old/ComplementCountingAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/old/ComplementCountingAggregator.java
new file mode 100644
index 0000000..d5db7d4
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/old/ComplementCountingAggregator.java
@@ -0,0 +1,44 @@
+package org.apache.lucene.facet.old;
+
+import java.io.IOException;
+
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link CountingAggregator} used during complement counting.
+ * 
+ * @lucene.experimental
+ */
+public class ComplementCountingAggregator extends CountingAggregator {
+
+  public ComplementCountingAggregator(int[] counterArray) {
+    super(counterArray);
+  }
+
+  @Override
+  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
+    for (int i = 0; i < ordinals.length; i++) {
+      int ord = ordinals.ints[i];
+      assert counterArray[ord] != 0 : "complement aggregation: count is about to become negative for ordinal " + ord;
+      --counterArray[ord];
+    }
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/old/CountingAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/old/CountingAggregator.java
new file mode 100644
index 0000000..90be8be
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/old/CountingAggregator.java
@@ -0,0 +1,66 @@
+package org.apache.lucene.facet.old;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link Aggregator} which updates a counter array with the size of the
+ * whole taxonomy, counting the number of times each category appears in the
+ * given set of documents.
+ * 
+ * @lucene.experimental
+ */
+public class CountingAggregator implements Aggregator {
+
+  protected int[] counterArray;
+  
+  public CountingAggregator(int[] counterArray) {
+    this.counterArray = counterArray;
+  }
+  
+  @Override
+  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
+    for (int i = 0; i < ordinals.length; i++) {
+      counterArray[ordinals.ints[i]]++;
+    }
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (obj == null || obj.getClass() != this.getClass()) {
+      return false;
+    }
+    CountingAggregator that = (CountingAggregator) obj;
+    return that.counterArray == this.counterArray;
+  }
+
+  @Override
+  public int hashCode() {
+    return counterArray == null ? 0 : counterArray.hashCode();
+  }
+  
+  @Override
+  public boolean setNextReader(AtomicReaderContext context) throws IOException {
+    return true;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/old/MatchingDocsAsScoredDocIDs.java b/lucene/facet/src/java/org/apache/lucene/facet/old/MatchingDocsAsScoredDocIDs.java
new file mode 100644
index 0000000..505ac65
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/old/MatchingDocsAsScoredDocIDs.java
@@ -0,0 +1,174 @@
+package org.apache.lucene.facet.old;
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** 
+ * Represents {@link MatchingDocs} as {@link ScoredDocIDs}.
+ * 
+ * @lucene.experimental
+ */
+public class MatchingDocsAsScoredDocIDs implements ScoredDocIDs {
+
+  // TODO remove this class once we get rid of ScoredDocIDs 
+
+  final List<MatchingDocs> matchingDocs;
+  final int size;
+  
+  public MatchingDocsAsScoredDocIDs(List<MatchingDocs> matchingDocs) {
+    this.matchingDocs = matchingDocs;
+    int totalSize = 0;
+    for (MatchingDocs md : matchingDocs) {
+      totalSize += md.totalHits;
+    }
+    this.size = totalSize;
+  }
+  
+  @Override
+  public ScoredDocIDsIterator iterator() throws IOException {
+    return new ScoredDocIDsIterator() {
+      
+      final Iterator<MatchingDocs> mdIter = matchingDocs.iterator();
+      
+      int scoresIdx = 0;
+      int doc = 0;
+      MatchingDocs current;
+      int currentLength;
+      boolean done = false;
+      
+      @Override
+      public boolean next() {
+        if (done) {
+          return false;
+        }
+        
+        while (current == null) {
+          if (!mdIter.hasNext()) {
+            done = true;
+            return false;
+          }
+          current = mdIter.next();
+          currentLength = current.bits.length();
+          doc = 0;
+          scoresIdx = 0;
+          
+          if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
+            current = null;
+          } else {
+            doc = -1; // we're calling nextSetBit later on
+          }
+        }
+        
+        ++doc;
+        if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
+          current = null;
+          return next();
+        }
+        
+        return true;
+      }
+      
+      @Override
+      public float getScore() {
+        return current.scores == null ? ScoredDocIDsIterator.DEFAULT_SCORE : current.scores[scoresIdx++];
+      }
+      
+      @Override
+      public int getDocID() {
+        return done ? DocIdSetIterator.NO_MORE_DOCS : doc + current.context.docBase;
+      }
+    };
+  }
+
+  @Override
+  public DocIdSet getDocIDs() {
+    return new DocIdSet() {
+      
+      final Iterator<MatchingDocs> mdIter = matchingDocs.iterator();
+      int doc = 0;
+      MatchingDocs current;
+      int currentLength;
+      boolean done = false;
+      
+      @Override
+      public DocIdSetIterator iterator() throws IOException {
+        return new DocIdSetIterator() {
+          
+          @Override
+          public int nextDoc() throws IOException {
+            if (done) {
+              return DocIdSetIterator.NO_MORE_DOCS;
+            }
+            
+            while (current == null) {
+              if (!mdIter.hasNext()) {
+                done = true;
+                return DocIdSetIterator.NO_MORE_DOCS;
+              }
+              current = mdIter.next();
+              currentLength = current.bits.length();
+              doc = 0;
+              
+              if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
+                current = null;
+              } else {
+                doc = -1; // we're calling nextSetBit later on
+              }
+            }
+            
+            ++doc;
+            if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
+              current = null;
+              return nextDoc();
+            }
+            
+            return doc + current.context.docBase;
+          }
+          
+          @Override
+          public int docID() {
+            return doc + current.context.docBase;
+          }
+          
+          @Override
+          public long cost() {
+            return size;
+          }
+
+          @Override
+          public int advance(int target) throws IOException {
+            throw new UnsupportedOperationException("not supported");
+          }
+        };
+      }
+    };
+  }
+
+  @Override
+  public int size() {
+    return size;
+  }
+  
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator.java
new file mode 100644
index 0000000..a102b7c
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/old/OldFacetsAccumulator.java
@@ -0,0 +1,457 @@
+package org.apache.lucene.facet.old;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map.Entry;
+
+import org.apache.lucene.facet.complements.TotalFacetCounts;
+import org.apache.lucene.facet.complements.TotalFacetCountsCache;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.partitions.IntermediateFacetResult;
+import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
+import org.apache.lucene.facet.sampling.Sampler.OverSampledFacetRequest;
+import org.apache.lucene.facet.search.CategoryListIterator;
+import org.apache.lucene.facet.search.CountFacetRequest;
+import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetsAccumulator;
+import org.apache.lucene.facet.search.FacetsAggregator;
+import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.search.OrdinalValueResolver;
+import org.apache.lucene.facet.search.OrdinalValueResolver.FloatValueResolver;
+import org.apache.lucene.facet.search.OrdinalValueResolver.IntValueResolver;
+import org.apache.lucene.facet.search.SumScoreFacetRequest;
+import org.apache.lucene.facet.search.TaxonomyFacetsAccumulator;
+import org.apache.lucene.facet.search.TopKFacetResultsHandler;
+import org.apache.lucene.facet.search.TopKInEachNodeHandler;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.util.PartitionsUtils;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetsAccumulator} which supports partitions, sampling and
+ * complement counting.
+ * <p>
+ * <b>NOTE:</b> this accumulator still uses the old API and will be removed
+ * eventually in favor of dedicated accumulators which support the above
+ * features ovee the new {@link FacetsAggregator} API. It provides
+ * {@link Aggregator} implementations for {@link CountFacetRequest},
+ * {@link SumScoreFacetRequest} and {@link OverSampledFacetRequest}. If you need
+ * to use it in conjunction with other facet requests, you should override
+ * {@link #createAggregator(FacetRequest, FacetArrays)}.
+ * 
+ * @lucene.experimental
+ */
+public class OldFacetsAccumulator extends TaxonomyFacetsAccumulator {
+
+  /**
+   * Default threshold for using the complements optimization.
+   * If accumulating facets for a document set larger than this ratio of the index size than 
+   * perform the complement optimization.
+   * @see #setComplementThreshold(double) for more info on the complements optimization.  
+   */
+  public static final double DEFAULT_COMPLEMENT_THRESHOLD = 0.6;
+
+  /**
+   * Passing this to {@link #setComplementThreshold(double)} will disable using complement optimization.
+   */
+  public static final double DISABLE_COMPLEMENT = Double.POSITIVE_INFINITY; // > 1 actually
+
+  /**
+   * Passing this to {@link #setComplementThreshold(double)} will force using complement optimization.
+   */
+  public static final double FORCE_COMPLEMENT = 0; // <=0  
+
+  protected int partitionSize;
+  protected int maxPartitions;
+  protected boolean isUsingComplements;
+
+  private TotalFacetCounts totalFacetCounts;
+
+  private Object accumulateGuard;
+
+  private double complementThreshold = DEFAULT_COMPLEMENT_THRESHOLD;
+  
+  private static FacetArrays createFacetArrays(FacetSearchParams searchParams, TaxonomyReader taxoReader) {
+    return new FacetArrays(PartitionsUtils.partitionSize(searchParams.indexingParams, taxoReader)); 
+  }
+  
+  public OldFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader,  
+      TaxonomyReader taxonomyReader) {
+    this(searchParams, indexReader, taxonomyReader, null);
+  }
+
+  public OldFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader,
+      TaxonomyReader taxonomyReader, FacetArrays facetArrays) {
+    super(searchParams, indexReader, taxonomyReader, facetArrays == null ? createFacetArrays(searchParams, taxonomyReader) : facetArrays);
+    
+    // can only be computed later when docids size is known
+    isUsingComplements = false;
+    partitionSize = PartitionsUtils.partitionSize(searchParams.indexingParams, taxonomyReader);
+    maxPartitions = (int) Math.ceil(this.taxonomyReader.getSize() / (double) partitionSize);
+    accumulateGuard = new Object();
+  }
+
+  // TODO: this should be removed once we clean the API
+  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
+
+    // synchronize to prevent calling two accumulate()'s at the same time.
+    // We decided not to synchronize the method because that might mislead
+    // users to feel encouraged to call this method simultaneously.
+    synchronized (accumulateGuard) {
+
+      // only now we can compute this
+      isUsingComplements = shouldComplement(docids);
+
+      if (isUsingComplements) {
+        try {
+          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, searchParams.indexingParams);
+          if (totalFacetCounts != null) {
+            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);
+          } else {
+            isUsingComplements = false;
+          }
+        } catch (UnsupportedOperationException e) {
+          // TODO (Facet): this exception is thrown from TotalCountsKey if the
+          // IndexReader used does not support getVersion(). We should re-think
+          // this: is this tiny detail worth disabling total counts completely
+          // for such readers? Currently, it's not supported by Parallel and
+          // MultiReader, which might be problematic for several applications.
+          // We could, for example, base our "isCurrent" logic on something else
+          // than the reader's version. Need to think more deeply about it.
+          isUsingComplements = false;
+        } catch (IOException e) {
+          // silently fail if for some reason failed to load/save from/to dir 
+          isUsingComplements = false;
+        } catch (Exception e) {
+          // give up: this should not happen!
+          throw new IOException("PANIC: Got unexpected exception while trying to get/calculate total counts", e);
+        }
+      }
+
+      docids = actualDocsToAccumulate(docids);
+
+      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();
+
+      try {
+        for (int part = 0; part < maxPartitions; part++) {
+
+          // fill arrays from category lists
+          fillArraysForPartition(docids, facetArrays, part);
+
+          int offset = part * partitionSize;
+
+          // for each partition we go over all requests and handle
+          // each, where the request maintains the merged result.
+          // In this implementation merges happen after each partition,
+          // but other impl could merge only at the end.
+          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();
+          for (FacetRequest fr : searchParams.facetRequests) {
+            // Handle and merge only facet requests which were not already handled.  
+            if (handledRequests.add(fr)) {
+              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));
+              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);
+              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);
+              if (oldRes != null) {
+                res4fr = frHndlr.mergeResults(oldRes, res4fr);
+              }
+              fr2tmpRes.put(fr, res4fr);
+            } 
+          }
+        }
+      } finally {
+        facetArrays.free();
+      }
+
+      // gather results from all requests into a list for returning them
+      List<FacetResult> res = new ArrayList<FacetResult>();
+      for (FacetRequest fr : searchParams.facetRequests) {
+        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));
+        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);
+        if (tmpResult == null) {
+          // Add empty FacetResult:
+          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));
+          continue;
+        }
+        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);
+        // final labeling if allowed (because labeling is a costly operation)
+        frHndlr.labelResult(facetRes);
+        res.add(facetRes);
+      }
+
+      return res;
+    }
+  }
+
+  /** check if all requests are complementable */
+  protected boolean mayComplement() {
+    for (FacetRequest freq : searchParams.facetRequests) {
+      if (!(freq instanceof CountFacetRequest)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  @Override
+  public PartitionsFacetResultsHandler createFacetResultsHandler(FacetRequest fr, OrdinalValueResolver resolver) {
+    if (fr.getResultMode() == ResultMode.PER_NODE_IN_TREE) {
+      return new TopKInEachNodeHandler(taxonomyReader, fr, resolver, facetArrays);
+    } else {
+      return new TopKFacetResultsHandler(taxonomyReader, fr, resolver, facetArrays);
+    }
+  }
+  
+  /**
+   * Set the actual set of documents over which accumulation should take place.
+   * <p>
+   * Allows to override the set of documents to accumulate for. Invoked just
+   * before actual accumulating starts. From this point that set of documents
+   * remains unmodified. Default implementation just returns the input
+   * unchanged.
+   * 
+   * @param docids
+   *          candidate documents to accumulate for
+   * @return actual documents to accumulate for
+   */
+  protected ScoredDocIDs actualDocsToAccumulate(ScoredDocIDs docids) throws IOException {
+    return docids;
+  }
+
+  /** Check if it is worth to use complements */
+  protected boolean shouldComplement(ScoredDocIDs docids) {
+    return mayComplement() && (docids.size() > indexReader.numDocs() * getComplementThreshold()) ;
+  }
+
+  /**
+   * Creates an {@link OrdinalValueResolver} for the given {@link FacetRequest}.
+   * By default this method supports {@link CountFacetRequest} and
+   * {@link SumScoreFacetRequest}. You should override if you are using other
+   * requests with this accumulator.
+   */
+  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest fr) {
+    if (fr instanceof CountFacetRequest) {
+      return new IntValueResolver(facetArrays);
+    } else if (fr instanceof SumScoreFacetRequest) {
+      return new FloatValueResolver(facetArrays);
+    } else if (fr instanceof OverSampledFacetRequest) {
+      return createOrdinalValueResolver(((OverSampledFacetRequest) fr).orig);
+    } else {
+      throw new IllegalArgumentException("unrecognized FacetRequest " + fr.getClass());
+    }
+  }
+  
+  /**
+   * Iterate over the documents for this partition and fill the facet arrays with the correct
+   * count/complement count/value.
+   */
+  private final void fillArraysForPartition(ScoredDocIDs docids, FacetArrays facetArrays, int partition) 
+      throws IOException {
+    
+    if (isUsingComplements) {
+      initArraysByTotalCounts(facetArrays, partition, docids.size());
+    } else {
+      facetArrays.free(); // to get a cleared array for this partition
+    }
+
+    HashMap<CategoryListIterator, Aggregator> categoryLists = getCategoryListMap(facetArrays, partition);
+
+    IntsRef ordinals = new IntsRef(32); // a reasonable start capacity for most common apps
+    for (Entry<CategoryListIterator, Aggregator> entry : categoryLists.entrySet()) {
+      final ScoredDocIDsIterator iterator = docids.iterator();
+      final CategoryListIterator categoryListIter = entry.getKey();
+      final Aggregator aggregator = entry.getValue();
+      Iterator<AtomicReaderContext> contexts = indexReader.leaves().iterator();
+      AtomicReaderContext current = null;
+      int maxDoc = -1;
+      while (iterator.next()) {
+        int docID = iterator.getDocID();
+        if (docID >= maxDoc) {
+          boolean iteratorDone = false;
+          do { // find the segment which contains this document
+            if (!contexts.hasNext()) {
+              throw new RuntimeException("ScoredDocIDs contains documents outside this reader's segments !?");
+            }
+            current = contexts.next();
+            maxDoc = current.docBase + current.reader().maxDoc();
+            if (docID < maxDoc) { // segment has docs, check if it has categories
+              boolean validSegment = categoryListIter.setNextReader(current);
+              validSegment &= aggregator.setNextReader(current);
+              if (!validSegment) { // if categoryList or aggregtor say it's an invalid segment, skip all docs
+                while (docID < maxDoc && iterator.next()) {
+                  docID = iterator.getDocID();
+                }
+                if (docID < maxDoc) {
+                  iteratorDone = true;
+                }
+              }
+            }
+          } while (docID >= maxDoc);
+          if (iteratorDone) { // iterator finished, terminate the loop
+            break;
+          }
+        }
+        docID -= current.docBase;
+        categoryListIter.getOrdinals(docID, ordinals);
+        if (ordinals.length == 0) {
+          continue; // document does not have category ordinals
+        }
+        aggregator.aggregate(docID, iterator.getScore(), ordinals);
+      }
+    }
+  }
+
+  /** Init arrays for partition by total counts, optionally applying a factor */
+  private final void initArraysByTotalCounts(FacetArrays facetArrays, int partition, int nAccumulatedDocs) {
+    int[] intArray = facetArrays.getIntArray();
+    totalFacetCounts.fillTotalCountsForPartition(intArray, partition);
+    double totalCountsFactor = getTotalCountsFactor();
+    // fix total counts, but only if the effect of this would be meaningful. 
+    if (totalCountsFactor < 0.99999) {
+      int delta = nAccumulatedDocs + 1;
+      for (int i = 0; i < intArray.length; i++) {
+        intArray[i] *= totalCountsFactor;
+        // also translate to prevent loss of non-positive values
+        // due to complement sampling (ie if sampled docs all decremented a certain category). 
+        intArray[i] += delta; 
+      }
+    }
+  }
+
+  /**
+   * Expert: factor by which counts should be multiplied when initializing
+   * the count arrays from total counts.
+   * Default implementation for this returns 1, which is a no op.  
+   * @return a factor by which total counts should be multiplied
+   */
+  protected double getTotalCountsFactor() {
+    return 1;
+  }
+
+  protected Aggregator createAggregator(FacetRequest fr, FacetArrays facetArrays) {
+    if (fr instanceof CountFacetRequest) {
+      // we rely on that, if needed, result is cleared by arrays!
+      int[] a = facetArrays.getIntArray();
+      if (isUsingComplements) {
+        return new ComplementCountingAggregator(a);
+      } else {
+        return new CountingAggregator(a);
+      }
+    } else if (fr instanceof SumScoreFacetRequest) {
+      if (isUsingComplements) {
+        throw new IllegalArgumentException("complements are not supported by SumScoreFacetRequest");
+      } else {
+        return new ScoringAggregator(facetArrays.getFloatArray());
+      }
+    } else if (fr instanceof OverSampledFacetRequest) {
+      return createAggregator(((OverSampledFacetRequest) fr).orig, facetArrays);
+    } else {
+      throw new IllegalArgumentException("unknown Aggregator implementation for request " + fr.getClass());
+    }
+  }
+  
+  /**
+   * Create an {@link Aggregator} and a {@link CategoryListIterator} for each
+   * and every {@link FacetRequest}. Generating a map, matching each
+   * categoryListIterator to its matching aggregator.
+   * <p>
+   * If two CategoryListIterators are served by the same aggregator, a single
+   * aggregator is returned for both.
+   * 
+   * <b>NOTE: </b>If a given category list iterator is needed with two different
+   * aggregators (e.g counting and association) - an exception is thrown as this
+   * functionality is not supported at this time.
+   */
+  protected HashMap<CategoryListIterator, Aggregator> getCategoryListMap(FacetArrays facetArrays,
+      int partition) throws IOException {
+    
+    HashMap<CategoryListIterator, Aggregator> categoryLists = new HashMap<CategoryListIterator, Aggregator>();
+
+    FacetIndexingParams indexingParams = searchParams.indexingParams;
+    for (FacetRequest facetRequest : searchParams.facetRequests) {
+      Aggregator categoryAggregator = createAggregator(facetRequest, facetArrays);
+
+      CategoryListIterator cli = indexingParams.getCategoryListParams(facetRequest.categoryPath).createCategoryListIterator(partition);
+      
+      // get the aggregator
+      Aggregator old = categoryLists.put(cli, categoryAggregator);
+
+      if (old != null && !old.equals(categoryAggregator)) {
+        throw new RuntimeException("Overriding existing category list with different aggregator");
+      }
+      // if the aggregator is the same we're covered
+    }
+
+    return categoryLists;
+  }
+  
+  @Override
+  public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
+    return accumulate(new MatchingDocsAsScoredDocIDs(matchingDocs));
+  }
+
+  /**
+   * Returns the complement threshold.
+   * @see #setComplementThreshold(double)
+   */
+  public double getComplementThreshold() {
+    return complementThreshold;
+  }
+
+  /**
+   * Set the complement threshold.
+   * This threshold will dictate whether the complements optimization is applied.
+   * The optimization is to count for less documents. It is useful when the same 
+   * FacetSearchParams are used for varying sets of documents. The first time 
+   * complements is used the "total counts" are computed - counting for all the 
+   * documents in the collection. Then, only the complementing set of documents
+   * is considered, and used to decrement from the overall counts, thereby 
+   * walking through less documents, which is faster.
+   * <p>
+   * For the default settings see {@link #DEFAULT_COMPLEMENT_THRESHOLD}.
+   * <p>
+   * To forcing complements in all cases pass {@link #FORCE_COMPLEMENT}.
+   * This is mostly useful for testing purposes, as forcing complements when only 
+   * tiny fraction of available documents match the query does not make sense and 
+   * would incur performance degradations.
+   * <p>
+   * To disable complements pass {@link #DISABLE_COMPLEMENT}.
+   * @param complementThreshold the complement threshold to set
+   * @see #getComplementThreshold()
+   */
+  public void setComplementThreshold(double complementThreshold) {
+    this.complementThreshold = complementThreshold;
+  }
+
+  /** Returns true if complements are enabled. */
+  public boolean isUsingComplements() {
+    return isUsingComplements;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDs.java b/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDs.java
new file mode 100644
index 0000000..06f5cee
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDs.java
@@ -0,0 +1,42 @@
+package org.apache.lucene.facet.old;
+
+import java.io.IOException;
+
+import org.apache.lucene.search.DocIdSet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Document IDs with scores for each, driving facets accumulation. Document
+ * scores are optionally used in the process of facets scoring.
+ * 
+ * @see OldFacetsAccumulator#accumulate(ScoredDocIDs)
+ * @lucene.experimental
+ */
+public interface ScoredDocIDs {
+
+  /** Returns an iterator over the document IDs and their scores. */
+  public ScoredDocIDsIterator iterator() throws IOException;
+
+  /** Returns the set of doc IDs. */
+  public DocIdSet getDocIDs();
+
+  /** Returns the number of scored documents. */
+  public int size();
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDsIterator.java b/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDsIterator.java
new file mode 100644
index 0000000..fe09058
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIDsIterator.java
@@ -0,0 +1,43 @@
+package org.apache.lucene.facet.old;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Iterator over document IDs and their scores. Each {@link #next()} retrieves
+ * the next docID and its score which can be later be retrieved by
+ * {@link #getDocID()} and {@link #getScore()}. <b>NOTE:</b> you must call
+ * {@link #next()} before {@link #getDocID()} and/or {@link #getScore()}, or
+ * otherwise the returned values are unexpected.
+ * 
+ * @lucene.experimental
+ */
+public interface ScoredDocIDsIterator {
+
+  /** Default score used in case scoring is disabled. */
+  public static final float DEFAULT_SCORE = 1.0f;
+
+  /** Iterate to the next document/score pair. Returns true iff there is such a pair. */
+  public abstract boolean next();
+
+  /** Returns the ID of the current document. */
+  public abstract int getDocID();
+
+  /** Returns the score of the current document. */
+  public abstract float getScore();
+
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIdsUtils.java b/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIdsUtils.java
new file mode 100644
index 0000000..7983c1f
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/old/ScoredDocIdsUtils.java
@@ -0,0 +1,446 @@
+package org.apache.lucene.facet.old;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.facet.old.ScoredDocIDs;
+import org.apache.lucene.facet.old.ScoredDocIDsIterator;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.OpenBitSetDISI;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Utility methods for Scored Doc IDs.
+ * 
+ * @lucene.experimental
+ */
+public class ScoredDocIdsUtils {
+
+  /**
+   * Create a complement of the input set. The returned {@link ScoredDocIDs}
+   * does not contain any scores, which makes sense given that the complementing
+   * documents were not scored.
+   * 
+   * Note: the complement set does NOT contain doc ids which are noted as deleted by the given reader
+   * 
+   * @param docids to be complemented.
+   * @param reader holding the number of documents & information about deletions.
+   */
+  public final static ScoredDocIDs getComplementSet(final ScoredDocIDs docids, final IndexReader reader)
+      throws IOException {
+    final int maxDoc = reader.maxDoc();
+
+    DocIdSet docIdSet = docids.getDocIDs();
+    final FixedBitSet complement;
+    if (docIdSet instanceof FixedBitSet) {
+      // That is the most common case, if ScoredDocIdsCollector was used.
+      complement = ((FixedBitSet) docIdSet).clone();
+    } else {
+      complement = new FixedBitSet(maxDoc);
+      DocIdSetIterator iter = docIdSet.iterator();
+      int doc;
+      while ((doc = iter.nextDoc()) < maxDoc) {
+        complement.set(doc);
+      }
+    }
+    complement.flip(0, maxDoc);
+    clearDeleted(reader, complement);
+
+    return createScoredDocIds(complement, maxDoc);
+  }
+  
+  /** Clear all deleted documents from a given open-bit-set according to a given reader */
+  private static void clearDeleted(final IndexReader reader, final FixedBitSet set) throws IOException {
+    // TODO use BitsFilteredDocIdSet?
+    
+    // If there are no deleted docs
+    if (!reader.hasDeletions()) {
+      return; // return immediately
+    }
+    
+    DocIdSetIterator it = set.iterator();
+    int doc = it.nextDoc(); 
+    for (AtomicReaderContext context : reader.leaves()) {
+      AtomicReader r = context.reader();
+      final int maxDoc = r.maxDoc() + context.docBase;
+      if (doc >= maxDoc) { // skip this segment
+        continue;
+      }
+      if (!r.hasDeletions()) { // skip all docs that belong to this reader as it has no deletions
+        while ((doc = it.nextDoc()) < maxDoc) {}
+        continue;
+      }
+      Bits liveDocs = r.getLiveDocs();
+      do {
+        if (!liveDocs.get(doc - context.docBase)) {
+          set.clear(doc);
+        }
+      } while ((doc = it.nextDoc()) < maxDoc);
+    }
+  }
+  
+  /**
+   * Create a subset of an existing ScoredDocIDs object.
+   * 
+   * @param allDocIds orginal set
+   * @param sampleSet Doc Ids of the subset.
+   */
+  public static final ScoredDocIDs createScoredDocIDsSubset(final ScoredDocIDs allDocIds,
+      final int[] sampleSet) throws IOException {
+
+    // sort so that we can scan docs in order
+    final int[] docids = sampleSet;
+    Arrays.sort(docids);
+    final float[] scores = new float[docids.length];
+    // fetch scores and compute size
+    ScoredDocIDsIterator it = allDocIds.iterator();
+    int n = 0;
+    while (it.next() && n < docids.length) {
+      int doc = it.getDocID();
+      if (doc == docids[n]) {
+        scores[n] = it.getScore();
+        ++n;
+      }
+    }
+    final int size = n;
+
+    return new ScoredDocIDs() {
+
+      @Override
+      public DocIdSet getDocIDs() {
+        return new DocIdSet() {
+
+          @Override
+          public boolean isCacheable() { return true; }
+
+          @Override
+          public DocIdSetIterator iterator() {
+            return new DocIdSetIterator() {
+
+              private int next = -1;
+
+              @Override
+              public int advance(int target) {
+                while (next < size && docids[next++] < target) {
+                }
+                return next == size ? NO_MORE_DOCS : docids[next];
+              }
+
+              @Override
+              public int docID() {
+                return docids[next];
+              }
+
+              @Override
+              public int nextDoc() {
+                if (++next >= size) {
+                  return NO_MORE_DOCS;
+                }
+                return docids[next];
+              }
+
+              @Override
+              public long cost() {
+                return size;
+              }
+            };
+          }
+        };
+      }
+
+      @Override
+      public ScoredDocIDsIterator iterator() {
+        return new ScoredDocIDsIterator() {
+
+          int next = -1;
+
+          @Override
+          public boolean next() { return ++next < size; }
+
+          @Override
+          public float getScore() { return scores[next]; }
+
+          @Override
+          public int getDocID() { return docids[next]; }
+        };
+      }
+
+      @Override
+      public int size() { return size; }
+
+    };
+  }
+
+  /**
+   * Creates a {@link ScoredDocIDs} which returns document IDs all non-deleted doc ids 
+   * according to the given reader. 
+   * The returned set contains the range of [0 .. reader.maxDoc ) doc ids
+   */
+  public static final ScoredDocIDs createAllDocsScoredDocIDs (final IndexReader reader) {
+    if (reader.hasDeletions()) {
+      return new AllLiveDocsScoredDocIDs(reader);
+    }
+    return new AllDocsScoredDocIDs(reader);
+  }
+
+  /**
+   * Create a ScoredDocIDs out of a given docIdSet and the total number of documents in an index  
+   */
+  public static final ScoredDocIDs createScoredDocIds(final DocIdSet docIdSet, final int maxDoc) {
+    return new ScoredDocIDs() {
+      private int size = -1;
+      @Override
+      public DocIdSet getDocIDs() { return docIdSet; }
+
+      @Override
+      public ScoredDocIDsIterator iterator() throws IOException {
+        final DocIdSetIterator docIterator = docIdSet.iterator();
+        return new ScoredDocIDsIterator() {
+          @Override
+          public boolean next() {
+            try {
+              return docIterator.nextDoc() != DocIdSetIterator.NO_MORE_DOCS;
+            } catch (IOException e) {
+              throw new RuntimeException(e);
+            }
+          }
+
+          @Override
+          public float getScore() { return DEFAULT_SCORE; }
+
+          @Override
+          public int getDocID() { return docIterator.docID(); }
+        };
+      }
+
+      @Override
+      public int size() {
+        // lazy size computation
+        if (size < 0) {
+          OpenBitSetDISI openBitSetDISI;
+          try {
+            openBitSetDISI = new OpenBitSetDISI(docIdSet.iterator(), maxDoc);
+          } catch (IOException e) {
+            throw new RuntimeException(e);
+          }
+          size = (int) openBitSetDISI.cardinality();
+        }
+        return size;
+      }
+    };
+  }
+
+  /**
+   * All docs ScoredDocsIDs - this one is simply an 'all 1' bitset. Used when
+   * there are no deletions in the index and we wish to go through each and
+   * every document
+   */
+  private static class AllDocsScoredDocIDs implements ScoredDocIDs {
+    final int maxDoc;
+
+    public AllDocsScoredDocIDs(IndexReader reader) {
+      this.maxDoc = reader.maxDoc();
+    }
+
+    @Override
+    public int size() {  
+      return maxDoc;
+    }
+
+    @Override
+    public DocIdSet getDocIDs() {
+      return new DocIdSet() {
+
+        @Override
+        public boolean isCacheable() {
+          return true;
+        }
+
+        @Override
+        public DocIdSetIterator iterator() {
+          return new DocIdSetIterator() {
+            private int next = -1;
+
+            @Override
+            public int advance(int target) {
+              if (target <= next) {
+                target = next + 1;
+              }
+              return next = target >= maxDoc ? NO_MORE_DOCS : target;
+            }
+
+            @Override
+            public int docID() {
+              return next;
+            }
+
+            @Override
+            public int nextDoc() {
+              return ++next < maxDoc ? next : NO_MORE_DOCS;
+            }
+
+            @Override
+            public long cost() {
+              return maxDoc;
+            }
+          };
+        }
+      };
+    }
+
+    @Override
+    public ScoredDocIDsIterator iterator() {
+      try {
+        final DocIdSetIterator iter = getDocIDs().iterator();
+        return new ScoredDocIDsIterator() {
+          @Override
+          public boolean next() {
+            try {
+              return iter.nextDoc() != DocIdSetIterator.NO_MORE_DOCS;
+            } catch (IOException e) {
+              // cannot happen
+              return false;
+            }
+          }
+
+          @Override
+          public float getScore() {
+            return DEFAULT_SCORE;
+          }
+
+          @Override
+          public int getDocID() {
+            return iter.docID();
+          }
+        };
+      } catch (IOException e) {
+        // cannot happen
+        throw new RuntimeException(e);
+      }
+    }
+  }
+
+  /**
+   * An All-docs bitset which has '0' for deleted documents and '1' for the
+   * rest. Useful for iterating over all 'live' documents in a given index.
+   * <p>
+   * NOTE: this class would work for indexes with no deletions at all,
+   * although it is recommended to use {@link AllDocsScoredDocIDs} to ease
+   * the performance cost of validating isDeleted() on each and every docId
+   */
+  private static final class AllLiveDocsScoredDocIDs implements ScoredDocIDs {
+    final int maxDoc;
+    final IndexReader reader;
+
+    AllLiveDocsScoredDocIDs(IndexReader reader) {
+      this.maxDoc = reader.maxDoc();
+      this.reader = reader;
+    }
+
+    @Override
+    public int size() {
+      return reader.numDocs();
+    }
+
+    @Override
+    public DocIdSet getDocIDs() {
+      return new DocIdSet() {
+
+        @Override
+        public boolean isCacheable() {
+          return true;
+        }
+
+        @Override
+        public DocIdSetIterator iterator() {
+          return new DocIdSetIterator() {
+            final Bits liveDocs = MultiFields.getLiveDocs(reader);
+            private int next = -1;
+
+            @Override
+            public int advance(int target) {
+              if (target > next) {
+                next = target - 1;
+              }
+              return nextDoc();
+            }
+
+            @Override
+            public int docID() {
+              return next;
+            }
+
+            @Override
+            public int nextDoc() {
+              do {
+                ++next;
+              } while (next < maxDoc && liveDocs != null && !liveDocs.get(next));
+
+              return next < maxDoc ? next : NO_MORE_DOCS;
+            }
+
+            @Override
+            public long cost() {
+              return maxDoc;
+            }
+          };
+        }
+      };
+    }
+
+    @Override
+    public ScoredDocIDsIterator iterator() {
+      try {
+        final DocIdSetIterator iter = getDocIDs().iterator();
+        return new ScoredDocIDsIterator() {
+          @Override
+          public boolean next() {
+            try {
+              return iter.nextDoc() != DocIdSetIterator.NO_MORE_DOCS;
+            } catch (IOException e) {
+              // cannot happen
+              return false;
+            }
+          }
+
+          @Override
+          public float getScore() {
+            return DEFAULT_SCORE;
+          }
+
+          @Override
+          public int getDocID() {
+            return iter.docID();
+          }
+        };
+      } catch (IOException e) {
+        // cannot happen
+        throw new RuntimeException(e);
+      }
+    }
+  }
+  
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/old/ScoringAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/old/ScoringAggregator.java
new file mode 100644
index 0000000..4c065f9
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/old/ScoringAggregator.java
@@ -0,0 +1,67 @@
+package org.apache.lucene.facet.old;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link Aggregator} which updates the weight of a category according to the
+ * scores of the documents it was found in.
+ * 
+ * @lucene.experimental
+ */
+public class ScoringAggregator implements Aggregator {
+
+  private final float[] scoreArray;
+  private final int hashCode;
+  
+  public ScoringAggregator(float[] counterArray) {
+    this.scoreArray = counterArray;
+    this.hashCode = scoreArray == null ? 0 : scoreArray.hashCode();
+  }
+
+  @Override
+  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
+    for (int i = 0; i < ordinals.length; i++) {
+      scoreArray[ordinals.ints[i]] += score;
+    }
+  }
+  
+  @Override
+  public boolean equals(Object obj) {
+    if (obj == null || obj.getClass() != this.getClass()) {
+      return false;
+    }
+    ScoringAggregator that = (ScoringAggregator) obj;
+    return that.scoreArray == this.scoreArray;
+  }
+
+  @Override
+  public int hashCode() {
+    return hashCode;
+  }
+
+  @Override
+  public boolean setNextReader(AtomicReaderContext context) throws IOException {
+    return true;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/old/package.html b/lucene/facet/src/java/org/apache/lucene/facet/old/package.html
new file mode 100644
index 0000000..a2b28be
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/old/package.html
@@ -0,0 +1,24 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+<title>Old Faceted Search API</title>
+</head>
+<body>
+Old faceted search API, kept until complements, sampling and partitions are migrated to the new API.
+</body>
+</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/params/FacetSearchParams.java b/lucene/facet/src/java/org/apache/lucene/facet/params/FacetSearchParams.java
index f0a9805..c4bfc22 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/params/FacetSearchParams.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/params/FacetSearchParams.java
@@ -23,13 +23,9 @@ import org.apache.lucene.facet.search.FacetRequest;
  */
 
 /**
- * Defines parameters that are needed for faceted search. The list of
- * {@link FacetRequest facet requests} denotes the facets for which aggregated
- * should be done.
- * <p>
- * One can pass {@link FacetIndexingParams} in order to tell the search code how
- * to read the facets information. Note that you must use the same
- * {@link FacetIndexingParams} that were used for indexing.
+ * Defines parameters that are needed for faceted search: the list of facet
+ * {@link FacetRequest facet requests} which should be aggregated as well as the
+ * {@link FacetIndexingParams indexing params} that were used to index them.
  * 
  * @lucene.experimental
  */
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/partitions/PartitionsFacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/partitions/PartitionsFacetResultsHandler.java
index c5ec23f..13ed1b2 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/partitions/PartitionsFacetResultsHandler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/partitions/PartitionsFacetResultsHandler.java
@@ -2,13 +2,14 @@ package org.apache.lucene.facet.partitions;
 
 import java.io.IOException;
 
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
+import org.apache.lucene.facet.old.ScoredDocIDs;
 import org.apache.lucene.facet.search.FacetArrays;
 import org.apache.lucene.facet.search.FacetRequest;
 import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetResultNode;
 import org.apache.lucene.facet.search.FacetResultsHandler;
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
+import org.apache.lucene.facet.search.OrdinalValueResolver;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 
 /*
@@ -36,11 +37,10 @@ import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 public abstract class PartitionsFacetResultsHandler extends FacetResultsHandler {
   
   public PartitionsFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, 
-      FacetArrays facetArrays) {
-    super(taxonomyReader, facetRequest, facetArrays);
+      OrdinalValueResolver resolver, FacetArrays facetArrays) {
+    super(taxonomyReader, facetRequest, resolver, facetArrays);
   }
 
-
   /**
    * Fetch results of a single partition, given facet arrays for that partition,
    * and based on the matching documents and faceted search parameters.
@@ -103,7 +103,7 @@ public abstract class PartitionsFacetResultsHandler extends FacetResultsHandler
   /**
    * Label results according to settings in {@link FacetRequest}, such as
    * {@link FacetRequest#getNumLabel()}. Usually invoked by
-   * {@link StandardFacetsAccumulator#accumulate(ScoredDocIDs)}
+   * {@link OldFacetsAccumulator#accumulate(ScoredDocIDs)}
    * 
    * @param facetResult
    *          facet result to be labeled.
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/RangeAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/range/RangeAccumulator.java
index fffcb77..0a07e92 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/range/RangeAccumulator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/range/RangeAccumulator.java
@@ -19,6 +19,7 @@ package org.apache.lucene.facet.range;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.List;
 
 import org.apache.lucene.facet.params.FacetSearchParams;
@@ -26,10 +27,8 @@ import org.apache.lucene.facet.search.FacetRequest;
 import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetResultNode;
 import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.search.FacetsAggregator;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.NumericDocValues;
 
 /** Uses a {@link NumericDocValues} and accumulates
@@ -51,38 +50,34 @@ public class RangeAccumulator extends FacetsAccumulator {
 
   final List<RangeSet> requests = new ArrayList<RangeSet>();
 
-  public RangeAccumulator(FacetSearchParams fsp, IndexReader reader) {
-    super(fsp, reader, null, null);
-
-    for(FacetRequest fr : fsp.facetRequests) {
-
+  public RangeAccumulator(FacetRequest... facetRequests) {
+    this(Arrays.asList(facetRequests));
+  }
+  
+  public RangeAccumulator(List<FacetRequest> facetRequests) {
+    super(new FacetSearchParams(facetRequests));
+    for (FacetRequest fr : facetRequests) {
       if (!(fr instanceof RangeFacetRequest)) {
-        throw new IllegalArgumentException("only RangeFacetRequest is supported; got " + fsp.facetRequests.get(0).getClass());
+        throw new IllegalArgumentException("this accumulator only supports RangeFacetRequest; got " + fr);
       }
 
       if (fr.categoryPath.length != 1) {
         throw new IllegalArgumentException("only flat (dimension only) CategoryPath is allowed");
       }
-
+      
       RangeFacetRequest<?> rfr = (RangeFacetRequest<?>) fr;
-
-      requests.add(new RangeSet(rfr.ranges, rfr.categoryPath.components[0]));
+      requests.add(new RangeSet(rfr.ranges, fr.categoryPath.components[0]));
     }
   }
 
   @Override
-  public FacetsAggregator getAggregator() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
   public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
 
     // TODO: test if this is faster (in the past it was
     // faster to do MachingDocs on the inside) ... see
     // patches on LUCENE-4965):
     List<FacetResult> results = new ArrayList<FacetResult>();
-    for(int i=0;i<requests.size();i++) {
+    for (int i = 0; i < requests.size(); i++) {
       RangeSet ranges = requests.get(i);
 
       int[] counts = new int[ranges.ranges.length];
@@ -100,7 +95,7 @@ public class RangeAccumulator extends FacetsAccumulator {
           // (really, a specialized case of the interval
           // tree)
           // TODO: use interval tree instead of linear search:
-          for(int j=0;j<ranges.ranges.length;j++) {
+          for (int j = 0; j < ranges.ranges.length; j++) {
             if (ranges.ranges[j].accept(v)) {
               counts[j]++;
             }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetRequest.java
index b548624..a7376f7 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetRequest.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetRequest.java
@@ -19,11 +19,10 @@ package org.apache.lucene.facet.range;
 
 import java.util.List;
 
-import org.apache.lucene.facet.search.Aggregator;
-import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetsAggregator;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 
 /**
  * Facet request for dynamic ranges based on a
@@ -36,6 +35,7 @@ public class RangeFacetRequest<T extends Range> extends FacetRequest {
 
   public final Range[] ranges;
 
+  @SuppressWarnings("unchecked")
   public RangeFacetRequest(String field, T...ranges) {
     super(new CategoryPath(field), 1);
     this.ranges = ranges;
@@ -47,18 +47,8 @@ public class RangeFacetRequest<T extends Range> extends FacetRequest {
   }
 
   @Override
-  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public double getValueOf(FacetArrays arrays, int ordinal) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public FacetArraysSource getFacetArraysSource() {
-    throw new UnsupportedOperationException();
+  public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
+    return null;
   }
   
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetsAccumulatorWrapper.java b/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetsAccumulatorWrapper.java
deleted file mode 100644
index ef108f3..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/range/RangeFacetsAccumulatorWrapper.java
+++ /dev/null
@@ -1,117 +0,0 @@
-package org.apache.lucene.facet.range;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetRequest;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetResultsHandler;
-import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.search.FacetsAggregator;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-
-/** Takes multiple facet requests and if necessary splits
- *  them between the normal {@link FacetsAccumulator} and a
- *  {@link RangeAccumulator} */
-public class RangeFacetsAccumulatorWrapper extends FacetsAccumulator {
-  // TODO: somehow handle SortedSetDVAccumulator as
-  // well... but it's tricky because SSDV just uses an
-  // "ordinary" flat CountFacetRequest so we can't switch
-  // based on that.
-  private final FacetsAccumulator accumulator;
-  private final RangeAccumulator rangeAccumulator;
-
-  public static FacetsAccumulator create(FacetSearchParams fsp, IndexReader indexReader, TaxonomyReader taxoReader) {
-    return create(fsp, indexReader, taxoReader, new FacetArrays(taxoReader.getSize()));
-  }
-
-  public static FacetsAccumulator create(FacetSearchParams fsp, IndexReader indexReader, TaxonomyReader taxoReader, FacetArrays arrays) {
-    List<FacetRequest> rangeRequests = new ArrayList<FacetRequest>();
-    List<FacetRequest> nonRangeRequests = new ArrayList<FacetRequest>();
-    for(FacetRequest fr : fsp.facetRequests) {
-      if (fr instanceof RangeFacetRequest) {
-        rangeRequests.add(fr);
-      } else {
-        nonRangeRequests.add(fr);
-      }
-    }
-
-    if (rangeRequests.isEmpty()) {
-      return new FacetsAccumulator(fsp, indexReader, taxoReader, arrays);
-    } else if (nonRangeRequests.isEmpty()) {
-      return new RangeAccumulator(fsp, indexReader);
-    } else {
-      FacetsAccumulator accumulator = new FacetsAccumulator(new FacetSearchParams(fsp.indexingParams, nonRangeRequests), indexReader, taxoReader, arrays);
-      RangeAccumulator rangeAccumulator = new RangeAccumulator(new FacetSearchParams(fsp.indexingParams, rangeRequests), indexReader);
-      return new RangeFacetsAccumulatorWrapper(accumulator, rangeAccumulator, fsp);
-    }
-  }
-
-  private RangeFacetsAccumulatorWrapper(FacetsAccumulator accumulator, RangeAccumulator rangeAccumulator, FacetSearchParams fsp) {
-    super(fsp, accumulator.indexReader, accumulator.taxonomyReader);
-    this.accumulator = accumulator;
-    this.rangeAccumulator = rangeAccumulator;
-  }
-
-  @Override
-  public FacetsAggregator getAggregator() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  protected FacetResultsHandler createFacetResultsHandler(FacetRequest fr) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  protected Set<CategoryListParams> getCategoryLists() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public boolean requiresDocScores() {
-    return accumulator.requiresDocScores();
-  }
-
-  public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
-    List<FacetResult> results = accumulator.accumulate(matchingDocs);
-    List<FacetResult> rangeResults = rangeAccumulator.accumulate(matchingDocs);
-
-    int aUpto = 0;
-    int raUpto = 0;
-    List<FacetResult> merged = new ArrayList<FacetResult>();
-    for(FacetRequest fr : searchParams.facetRequests) {
-      if (fr instanceof RangeFacetRequest) {
-        merged.add(rangeResults.get(raUpto++));
-      } else {
-        merged.add(results.get(aUpto++));
-      }
-    }
-
-    return merged;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/RandomSampler.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/RandomSampler.java
index 7dae2db..1ce68dc 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/sampling/RandomSampler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/RandomSampler.java
@@ -3,9 +3,9 @@ package org.apache.lucene.facet.sampling;
 import java.io.IOException;
 import java.util.Random;
 
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.ScoredDocIDsIterator;
-import org.apache.lucene.facet.util.ScoredDocIdsUtils;
+import org.apache.lucene.facet.old.ScoredDocIDs;
+import org.apache.lucene.facet.old.ScoredDocIDsIterator;
+import org.apache.lucene.facet.old.ScoredDocIdsUtils;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler.java
index f1ae6b7..4e5a736 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler.java
@@ -5,12 +5,11 @@ import java.util.Arrays;
 import java.util.logging.Level;
 import java.util.logging.Logger;
 
+import org.apache.lucene.facet.old.ScoredDocIDs;
+import org.apache.lucene.facet.old.ScoredDocIDsIterator;
+import org.apache.lucene.facet.old.ScoredDocIdsUtils;
 import org.apache.lucene.util.PriorityQueue;
 
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.ScoredDocIDsIterator;
-import org.apache.lucene.facet.util.ScoredDocIdsUtils;
-
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
  * contributor license agreements.  See the NOTICE file distributed with
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SampleFixer.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SampleFixer.java
index d72752c..e04427c 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SampleFixer.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SampleFixer.java
@@ -2,9 +2,9 @@ package org.apache.lucene.facet.sampling;
 
 import java.io.IOException;
 
+import org.apache.lucene.facet.old.ScoredDocIDs;
 import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.ScoredDocIDs;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/Sampler.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/Sampler.java
index ec39ef7..4fe7785 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/sampling/Sampler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/Sampler.java
@@ -4,14 +4,13 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
+import org.apache.lucene.facet.old.ScoredDocIDs;
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.Aggregator;
-import org.apache.lucene.facet.search.FacetArrays;
 import org.apache.lucene.facet.search.FacetRequest;
 import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.search.FacetsAggregator;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -196,15 +195,9 @@ public abstract class Sampler {
     return res;
   }
   
-  /**
-   * Wrapping a facet request for over sampling.
-   * Implementation detail: even if the original request is a count request, no 
-   * statistics will be computed for it as the wrapping is not a count request.
-   * This is ok, as the sampling accumulator is later computing the statistics
-   * over the original requests.
-   */
-  private static class OverSampledFacetRequest extends FacetRequest {
-    final FacetRequest orig;
+  /** Wrapping a facet request for over sampling. */
+  public static class OverSampledFacetRequest extends FacetRequest {
+    public final FacetRequest orig;
     public OverSampledFacetRequest(FacetRequest orig, int num) {
       super(orig.categoryPath, num);
       this.orig = orig;
@@ -215,19 +208,8 @@ public abstract class Sampler {
     }
     
     @Override
-    public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) 
-        throws IOException {
-      return orig.createAggregator(useComplements, arrays, taxonomy);
-    }
-
-    @Override
-    public FacetArraysSource getFacetArraysSource() {
-      return orig.getFacetArraysSource();
-    }
-
-    @Override
-    public double getValueOf(FacetArrays arrays, int idx) {
-      return orig.getValueOf(arrays, idx);
+    public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
+      return orig.createFacetsAggregator(fip);
     }
   }
 
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingAccumulator.java
index 2a04394..b48554c 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingAccumulator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingAccumulator.java
@@ -4,14 +4,15 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
+import org.apache.lucene.facet.old.ScoredDocIDs;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
 import org.apache.lucene.facet.sampling.Sampler.SampleResult;
 import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.search.FacetRequest;
 import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.IndexReader;
 
@@ -38,10 +39,10 @@ import org.apache.lucene.index.IndexReader;
  * Note two major differences between this class and {@link SamplingWrapper}:
  * <ol>
  * <li>Latter can wrap any other {@link FacetsAccumulator} while this class
- * directly extends {@link StandardFacetsAccumulator}.</li>
+ * directly extends {@link OldFacetsAccumulator}.</li>
  * <li>This class can effectively apply sampling on the complement set of
  * matching document, thereby working efficiently with the complement
- * optimization - see {@link StandardFacetsAccumulator#getComplementThreshold()}
+ * optimization - see {@link OldFacetsAccumulator#getComplementThreshold()}
  * .</li>
  * </ol>
  * <p>
@@ -52,7 +53,7 @@ import org.apache.lucene.index.IndexReader;
  * @see Sampler
  * @lucene.experimental
  */
-public class SamplingAccumulator extends StandardFacetsAccumulator {
+public class SamplingAccumulator extends OldFacetsAccumulator {
   
   private double samplingRatio = -1d;
   private final Sampler sampler;
@@ -90,7 +91,8 @@ public class SamplingAccumulator extends StandardFacetsAccumulator {
     List<FacetResult> results = new ArrayList<FacetResult>();
     for (FacetResult fres : sampleRes) {
       // for sure fres is not null because this is guaranteed by the delegee.
-      PartitionsFacetResultsHandler frh = createFacetResultsHandler(fres.getFacetRequest());
+      FacetRequest fr = fres.getFacetRequest();
+      PartitionsFacetResultsHandler frh = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));
       if (samplerFixer != null) {
         // fix the result of current request
         samplerFixer.fixResult(docids, fres, samplingRatio);
@@ -106,7 +108,7 @@ public class SamplingAccumulator extends StandardFacetsAccumulator {
       // final labeling if allowed (because labeling is a costly operation)
       if (fres.getFacetResultNode().ordinal == TaxonomyReader.INVALID_ORDINAL) {
         // category does not exist, add an empty result
-        results.add(emptyResult(fres.getFacetResultNode().ordinal, fres.getFacetRequest()));
+        results.add(emptyResult(fres.getFacetResultNode().ordinal, fr));
       } else {
         frh.labelResult(fres);
         results.add(fres);
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingParams.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingParams.java
index 464b593..a24b8b7 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingParams.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingParams.java
@@ -32,19 +32,19 @@ public class SamplingParams {
   
   /**
    * Default ratio between size of sample to original size of document set.
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
    */
   public static final double DEFAULT_SAMPLE_RATIO = 0.01;
   
   /**
    * Default maximum size of sample.
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
    */
   public static final int DEFAULT_MAX_SAMPLE_SIZE = 10000;
   
   /**
    * Default minimum size of sample.
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
    */
   public static final int DEFAULT_MIN_SAMPLE_SIZE = 100;
   
@@ -65,7 +65,7 @@ public class SamplingParams {
   /**
    * Return the maxSampleSize.
    * In no case should the resulting sample size exceed this value.  
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
    */
   public final int getMaxSampleSize() {
     return maxSampleSize;
@@ -74,7 +74,7 @@ public class SamplingParams {
   /**
    * Return the minSampleSize.
    * In no case should the resulting sample size be smaller than this value.  
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
    */
   public final int getMinSampleSize() {
     return minSampleSize;
@@ -82,7 +82,7 @@ public class SamplingParams {
 
   /**
    * @return the sampleRatio
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.old.ScoredDocIDs)
    */
   public final double getSampleRatio() {
     return sampleRatio;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingWrapper.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingWrapper.java
index a6cdeeb..88734da 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingWrapper.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingWrapper.java
@@ -4,12 +4,13 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
+import org.apache.lucene.facet.old.ScoredDocIDs;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
 import org.apache.lucene.facet.sampling.Sampler.SampleResult;
+import org.apache.lucene.facet.search.FacetRequest;
 import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 
 /*
@@ -38,12 +39,12 @@ import org.apache.lucene.facet.taxonomy.TaxonomyReader;
  * 
  * @lucene.experimental
  */
-public class SamplingWrapper extends StandardFacetsAccumulator {
+public class SamplingWrapper extends OldFacetsAccumulator {
 
-  private StandardFacetsAccumulator delegee;
+  private OldFacetsAccumulator delegee;
   private Sampler sampler;
 
-  public SamplingWrapper(StandardFacetsAccumulator delegee, Sampler sampler) {
+  public SamplingWrapper(OldFacetsAccumulator delegee, Sampler sampler) {
     super(delegee.searchParams, delegee.indexReader, delegee.taxonomyReader);
     this.delegee = delegee;
     this.sampler = sampler;
@@ -68,7 +69,8 @@ public class SamplingWrapper extends StandardFacetsAccumulator {
     
     for (FacetResult fres : sampleRes) {
       // for sure fres is not null because this is guaranteed by the delegee.
-      PartitionsFacetResultsHandler frh = createFacetResultsHandler(fres.getFacetRequest());
+      FacetRequest fr = fres.getFacetRequest();
+      PartitionsFacetResultsHandler frh = createFacetResultsHandler(fr, createOrdinalValueResolver(fr));
       if (sampleFixer != null) {
         // fix the result of current request
         sampleFixer.fixResult(docids, fres, sampleSet.actualSampleRatio); 
@@ -83,7 +85,7 @@ public class SamplingWrapper extends StandardFacetsAccumulator {
       // final labeling if allowed (because labeling is a costly operation)
       if (fres.getFacetResultNode().ordinal == TaxonomyReader.INVALID_ORDINAL) {
         // category does not exist, add an empty result
-        results.add(emptyResult(fres.getFacetResultNode().ordinal, fres.getFacetRequest()));
+        results.add(emptyResult(fres.getFacetResultNode().ordinal, fr));
       } else {
         frh.labelResult(fres);
         results.add(fres);
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/TakmiSampleFixer.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/TakmiSampleFixer.java
index ade148c..2ec3613 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/sampling/TakmiSampleFixer.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/TakmiSampleFixer.java
@@ -2,11 +2,11 @@ package org.apache.lucene.facet.sampling;
 
 import java.io.IOException;
 
+import org.apache.lucene.facet.old.ScoredDocIDs;
+import org.apache.lucene.facet.old.ScoredDocIDsIterator;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.DrillDownQuery;
 import org.apache.lucene.facet.search.FacetResultNode;
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.ScoredDocIDsIterator;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.DocsEnum;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/AdaptiveFacetsAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/AdaptiveFacetsAccumulator.java
deleted file mode 100644
index 8d34158..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/AdaptiveFacetsAccumulator.java
+++ /dev/null
@@ -1,113 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.sampling.RandomSampler;
-import org.apache.lucene.facet.sampling.Sampler;
-import org.apache.lucene.facet.sampling.SamplingAccumulator;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * {@link FacetsAccumulator} whose behavior regarding complements, sampling,
- * etc. is not set up front but rather is determined at accumulation time
- * according to the statistics of the accumulated set of documents and the
- * index.
- * <p>
- * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
- * does not guarantee accurate values for
- * {@link FacetResult#getNumValidDescendants()}.
- * 
- * @lucene.experimental
- */
-public final class AdaptiveFacetsAccumulator extends StandardFacetsAccumulator {
-  
-  private Sampler sampler = new RandomSampler();
-
-  /**
-   * Create an {@link AdaptiveFacetsAccumulator} 
-   * @see StandardFacetsAccumulator#StandardFacetsAccumulator(FacetSearchParams, IndexReader, TaxonomyReader)
-   */
-  public AdaptiveFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader, 
-      TaxonomyReader taxonomyReader) {
-    super(searchParams, indexReader, taxonomyReader);
-  }
-
-  /**
-   * Create an {@link AdaptiveFacetsAccumulator}
-   * 
-   * @see StandardFacetsAccumulator#StandardFacetsAccumulator(FacetSearchParams,
-   *      IndexReader, TaxonomyReader, FacetArrays)
-   */
-  public AdaptiveFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader,
-      TaxonomyReader taxonomyReader, FacetArrays facetArrays) {
-    super(searchParams, indexReader, taxonomyReader, facetArrays);
-  }
-
-  /**
-   * Set the sampler.
-   * @param sampler sampler to set
-   */
-  public void setSampler(Sampler sampler) {
-    this.sampler = sampler;
-  }
-  
-  @Override
-  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
-    StandardFacetsAccumulator delegee = appropriateFacetCountingAccumulator(docids);
-
-    if (delegee == this) {
-      return super.accumulate(docids);
-    }
-
-    return delegee.accumulate(docids);
-  }
-
-  /**
-   * Compute the appropriate facet accumulator to use.
-   * If no special/clever adaptation is possible/needed return this (self).
-   */
-  private StandardFacetsAccumulator appropriateFacetCountingAccumulator(ScoredDocIDs docids) {
-    // Verify that searchPareams permit sampling/complement/etc... otherwise do default
-    if (!mayComplement()) {
-      return this;
-    }
-    
-    // Now we're sure we can use the sampling methods as we're in a counting only mode
-    
-    // Verify that sampling is enabled and required ... otherwise do default
-    if (sampler == null || !sampler.shouldSample(docids)) {
-      return this;
-    }
-    
-    SamplingAccumulator samplingAccumulator = new SamplingAccumulator(sampler, searchParams, indexReader, taxonomyReader);
-    samplingAccumulator.setComplementThreshold(getComplementThreshold());
-    return samplingAccumulator;
-  }
-
-  /**
-   * @return the sampler in effect
-   */
-  public final Sampler getSampler() {
-    return sampler;
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/Aggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/Aggregator.java
deleted file mode 100644
index e95af40..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/Aggregator.java
+++ /dev/null
@@ -1,48 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Aggregates the categories of documents given to
- * {@link #aggregate(int, float, IntsRef)}. Note that the document IDs are local
- * to the reader given to {@link #setNextReader(AtomicReaderContext)}.
- * 
- * @lucene.experimental
- */
-public interface Aggregator {
-
-  /**
-   * Sets the {@link AtomicReaderContext} for which
-   * {@link #aggregate(int, float, IntsRef)} calls will be made. If this method
-   * returns false, {@link #aggregate(int, float, IntsRef)} should not be called
-   * for this reader.
-   */
-  public boolean setNextReader(AtomicReaderContext context) throws IOException;
-  
-  /**
-   * Aggregate the ordinals of the given document ID (and its score). The given
-   * ordinals offset is always zero.
-   */
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException;
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/CountFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/search/CountFacetRequest.java
index 9519230..24bb95e 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/CountFacetRequest.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/CountFacetRequest.java
@@ -1,8 +1,7 @@
 package org.apache.lucene.facet.search;
 
-import org.apache.lucene.facet.complements.ComplementCountingAggregator;
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -33,23 +32,8 @@ public class CountFacetRequest extends FacetRequest {
   }
 
   @Override
-  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) {
-    // we rely on that, if needed, result is cleared by arrays!
-    int[] a = arrays.getIntArray();
-    if (useComplements) {
-      return new ComplementCountingAggregator(a);
-    }
-    return new CountingAggregator(a);
-  }
-
-  @Override
-  public double getValueOf(FacetArrays arrays, int ordinal) {
-    return arrays.getIntArray()[ordinal];
-  }
-
-  @Override
-  public FacetArraysSource getFacetArraysSource() {
-    return FacetArraysSource.INT;
+  public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
+    return CountingFacetsAggregator.create(fip.getCategoryListParams(categoryPath));
   }
   
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/CountingAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/CountingAggregator.java
deleted file mode 100644
index 395581b..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/CountingAggregator.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A CountingAggregator updates a counter array with the size of the whole
- * taxonomy, counting the number of times each category appears in the given set
- * of documents.
- * 
- * @lucene.experimental
- */
-public class CountingAggregator implements Aggregator {
-
-  protected int[] counterArray;
-  
-  public CountingAggregator(int[] counterArray) {
-    this.counterArray = counterArray;
-  }
-  
-  @Override
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
-    for (int i = 0; i < ordinals.length; i++) {
-      counterArray[ordinals.ints[i]]++;
-    }
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null || obj.getClass() != this.getClass()) {
-      return false;
-    }
-    CountingAggregator that = (CountingAggregator) obj;
-    return that.counterArray == this.counterArray;
-  }
-
-  @Override
-  public int hashCode() {
-    return counterArray == null ? 0 : counterArray.hashCode();
-  }
-  
-  @Override
-  public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    return true;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java
index 277f32d..26c3fe1 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java
@@ -2,6 +2,7 @@ package org.apache.lucene.facet.search;
 
 import java.io.IOException;
 
+import org.apache.lucene.facet.encoding.DGapVInt8IntDecoder;
 import org.apache.lucene.facet.params.CategoryListParams;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
 import org.apache.lucene.util.IntsRef;
@@ -34,6 +35,18 @@ import org.apache.lucene.util.IntsRef;
  */
 public class CountingFacetsAggregator extends IntRollupFacetsAggregator {
   
+  /**
+   * Returns a {@link FacetsAggregator} suitable for counting categories given
+   * the {@link CategoryListParams}.
+   */
+  public static FacetsAggregator create(CategoryListParams clp) {
+    if (clp.createEncoder().createMatchingDecoder().getClass() == DGapVInt8IntDecoder.class) {
+      return new FastCountingFacetsAggregator();
+    } else {
+      return new CountingFacetsAggregator();
+    }
+  }
+
   private final IntsRef ordinals = new IntsRef(32);
   
   @Override
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java
index 90bb3d6..6a51b98 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java
@@ -9,6 +9,7 @@ import java.util.Comparator;
 import org.apache.lucene.facet.search.FacetRequest.SortOrder;
 import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.util.CollectionUtil;
 import org.apache.lucene.util.PriorityQueue;
 
 /*
@@ -31,12 +32,11 @@ import org.apache.lucene.util.PriorityQueue;
 /**
  * A {@link FacetResultsHandler} which counts the top-K facets at depth 1 only
  * and always labels all result categories. The results are always sorted by
- * value, in descending order. Sub-classes are responsible to pull the values
- * from the corresponding {@link FacetArrays}.
+ * value, in descending order.
  * 
  * @lucene.experimental
  */
-public abstract class DepthOneFacetResultsHandler extends FacetResultsHandler {
+public class DepthOneFacetResultsHandler extends FacetResultsHandler {
   
   private static class FacetResultNodeQueue extends PriorityQueue<FacetResultNode> {
     
@@ -51,40 +51,19 @@ public abstract class DepthOneFacetResultsHandler extends FacetResultsHandler {
     
     @Override
     protected boolean lessThan(FacetResultNode a, FacetResultNode b) {
-      if (a.value < b.value) return true;
-      if (a.value > b.value) return false;
-      // both have the same value, break tie by ordinal
-      return a.ordinal < b.ordinal;
+      return a.compareTo(b)  < 0;
     }
     
   }
 
-  public DepthOneFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, FacetArrays facetArrays) {
-    super(taxonomyReader, facetRequest, facetArrays);
+  public DepthOneFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, FacetArrays facetArrays, 
+      OrdinalValueResolver resolver) {
+    super(taxonomyReader, facetRequest, resolver, facetArrays);
     assert facetRequest.getDepth() == 1 : "this handler only computes the top-K facets at depth 1";
     assert facetRequest.numResults == facetRequest.getNumLabel() : "this handler always labels all top-K results";
     assert facetRequest.getSortOrder() == SortOrder.DESCENDING : "this handler always sorts results in descending order";
   }
 
-  /** Returnt the value of the requested ordinal. Called once for the result root. */
-  protected abstract double valueOf(int ordinal);
-  
-  /**
-   * Add the siblings of {@code ordinal} to the given list. This is called
-   * whenever the number of results is too high (&gt; taxonomy size), instead of
-   * adding them to a {@link PriorityQueue}.
-   */
-  protected abstract void addSiblings(int ordinal, int[] siblings, ArrayList<FacetResultNode> nodes) throws IOException;
-  
-  /**
-   * Add the siblings of {@code ordinal} to the given {@link PriorityQueue}. The
-   * given {@link PriorityQueue} is already filled with sentinel objects, so
-   * implementations are encouraged to use {@link PriorityQueue#top()} and
-   * {@link PriorityQueue#updateTop()} for best performance.  Returns the total
-   * number of siblings.
-   */
-  protected abstract int addSiblings(int ordinal, int[] siblings, PriorityQueue<FacetResultNode> pq);
-  
   @Override
   public final FacetResult compute() throws IOException {
     ParallelTaxonomyArrays arrays = taxonomyReader.getParallelTaxonomyArrays();
@@ -93,23 +72,28 @@ public abstract class DepthOneFacetResultsHandler extends FacetResultsHandler {
     
     int rootOrd = taxonomyReader.getOrdinal(facetRequest.categoryPath);
         
-    FacetResultNode root = new FacetResultNode(rootOrd, valueOf(rootOrd));
+    FacetResultNode root = new FacetResultNode(rootOrd, resolver.valueOf(rootOrd));
     root.label = facetRequest.categoryPath;
     if (facetRequest.numResults > taxonomyReader.getSize()) {
       // specialize this case, user is interested in all available results
       ArrayList<FacetResultNode> nodes = new ArrayList<FacetResultNode>();
-      int child = children[rootOrd];
-      addSiblings(child, siblings, nodes);
-      Collections.sort(nodes, new Comparator<FacetResultNode>() {
+      int ordinal = children[rootOrd];
+      while (ordinal != TaxonomyReader.INVALID_ORDINAL) {
+        double value = resolver.valueOf(ordinal);
+        if (value > 0) {
+          FacetResultNode node = new FacetResultNode(ordinal, value);
+          node.label = taxonomyReader.getPath(ordinal);
+          nodes.add(node);
+        }
+        ordinal = siblings[ordinal];
+      }
+
+      CollectionUtil.introSort(nodes, Collections.reverseOrder(new Comparator<FacetResultNode>() {
         @Override
         public int compare(FacetResultNode o1, FacetResultNode o2) {
-          int value = (int) (o2.value - o1.value);
-          if (value == 0) {
-            value = o2.ordinal - o1.ordinal;
-          }
-          return value;
+          return o1.compareTo(o2);
         }
-      });
+      }));
       
       root.subResults = nodes;
       return new FacetResult(facetRequest, root, nodes.size());
@@ -117,7 +101,21 @@ public abstract class DepthOneFacetResultsHandler extends FacetResultsHandler {
     
     // since we use sentinel objects, we cannot reuse PQ. but that's ok because it's not big
     PriorityQueue<FacetResultNode> pq = new FacetResultNodeQueue(facetRequest.numResults, true);
-    int numSiblings = addSiblings(children[rootOrd], siblings, pq);
+    int ordinal = children[rootOrd];
+    FacetResultNode top = pq.top();
+    int numSiblings = 0;
+    while (ordinal != TaxonomyReader.INVALID_ORDINAL) {
+      double value = resolver.valueOf(ordinal);
+      if (value > 0) {
+        ++numSiblings;
+        if (value > top.value) {
+          top.value = value;
+          top.ordinal = ordinal;
+          top = pq.updateTop();
+        }
+      }
+      ordinal = siblings[ordinal];
+    }
 
     // pop() the least (sentinel) elements
     int pqsize = pq.size();
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java b/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java
index f3318ed..b2654ce 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java
@@ -25,7 +25,10 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.lucene.facet.index.FacetFields;
 import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetFields;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
@@ -70,11 +73,26 @@ public class DrillSideways {
 
   protected final IndexSearcher searcher;
   protected final TaxonomyReader taxoReader;
-
-  /** Create a new {@code DrillSideways} instance. */
+  protected final SortedSetDocValuesReaderState state;
+  
+  /**
+   * Create a new {@code DrillSideways} instance, assuming the categories were
+   * indexed with {@link FacetFields}.
+   */
   public DrillSideways(IndexSearcher searcher, TaxonomyReader taxoReader) {
     this.searcher = searcher;
     this.taxoReader = taxoReader;
+    this.state = null;
+  }
+  
+  /**
+   * Create a new {@code DrillSideways} instance, assuming the categories were
+   * indexed with {@link SortedSetDocValuesFacetFields}.
+   */
+  public DrillSideways(IndexSearcher searcher, SortedSetDocValuesReaderState state) {
+    this.searcher = searcher;
+    this.taxoReader = null;
+    this.state = state;
   }
 
   /** Moves any drill-downs that don't have a corresponding
@@ -440,13 +458,21 @@ public class DrillSideways {
   /** Override this to use a custom drill-down {@link
    *  FacetsAccumulator}. */
   protected FacetsAccumulator getDrillDownAccumulator(FacetSearchParams fsp) throws IOException {
-    return FacetsAccumulator.create(fsp, searcher.getIndexReader(), taxoReader);
+    if (taxoReader != null) {
+      return FacetsAccumulator.create(fsp, searcher.getIndexReader(), taxoReader, null);
+    } else {
+      return FacetsAccumulator.create(fsp, state, null);
+    }
   }
 
   /** Override this to use a custom drill-sideways {@link
    *  FacetsAccumulator}. */
   protected FacetsAccumulator getDrillSidewaysAccumulator(String dim, FacetSearchParams fsp) throws IOException {
-    return FacetsAccumulator.create(fsp, searcher.getIndexReader(), taxoReader);
+    if (taxoReader != null) {
+      return FacetsAccumulator.create(fsp, searcher.getIndexReader(), taxoReader, null);
+    } else {
+      return FacetsAccumulator.create(fsp, state, null);
+    }
   }
 
   /** Override this and return true if your collector
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysQuery.java b/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysQuery.java
index fd5d160..eee0431 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysQuery.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSidewaysQuery.java
@@ -130,7 +130,7 @@ class DrillSidewaysQuery extends Query {
             continue;
           }
           for(int i=0;i<drillDownTerms[dim].length;i++) {
-            if (termsEnum.seekExact(drillDownTerms[dim][i].bytes(), false)) {
+            if (termsEnum.seekExact(drillDownTerms[dim][i].bytes())) {
               DocsEnum docsEnum = termsEnum.docs(null, null, 0);
               if (docsEnum != null) {
                 dims[dim].docsEnums[i] = docsEnum;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetRequest.java
index fcdada0..3bfa56e 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetRequest.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetRequest.java
@@ -1,9 +1,9 @@
 package org.apache.lucene.facet.search;
 
-import java.io.IOException;
-
+import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.range.RangeFacetRequest;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -23,90 +23,70 @@ import org.apache.lucene.facet.taxonomy.TaxonomyReader;
  */
 
 /**
- * Request to accumulate facet information for a specified facet and possibly 
- * also some of its descendants, upto a specified depth.
+ * Defines an aggregation request for a category. Allows specifying the
+ * {@link #numResults number of child categories} to return as well as
+ * {@link #getSortOrder() which} categories to consider the "top" (highest or
+ * lowest ranking ones).
  * <p>
- * The facet request additionally defines what information should 
- * be computed within the facet results, if and how should results
- * be ordered, etc.
- * <P>
- * An example facet request is to look at all sub-categories of "Author", and
- * return the 10 with the highest counts (sorted by decreasing count). 
+ * If the category being aggregated is hierarchical, you can also specify the
+ * {@link #setDepth(int) depth} up which to aggregate child categories as well
+ * as how the result should be {@link #setResultMode(ResultMode) constructed}.
  * 
  * @lucene.experimental
  */
 public abstract class FacetRequest {
   
   /**
-   * Result structure manner of applying request's limits such as
-   * {@link FacetRequest#getNumLabel()} and {@link FacetRequest#numResults}.
-   * Only relevant when {@link FacetRequest#getDepth()} is &gt; 1.
+   * When {@link FacetRequest#getDepth()} is greater than 1, defines the
+   * structure of the result as well as how constraints such as
+   * {@link FacetRequest#numResults} and {@link FacetRequest#getNumLabel()} are
+   * applied.
    */
   public enum ResultMode { 
-    /** Limits are applied per node, and the result has a full tree structure. */
+    /**
+     * Constraints are applied per node, and the result has a full tree
+     * structure. Default result mode.
+     */
     PER_NODE_IN_TREE, 
     
-    /** Limits are applied globally, on total number of results, and the result has a flat structure. */
+    /**
+     * Constraints are applied globally, on total number of results, and the
+     * result has a flat structure.
+     */
     GLOBAL_FLAT
   }
   
   /**
-   * Specifies which array of {@link FacetArrays} should be used to resolve
-   * values. When set to {@link #INT} or {@link #FLOAT}, allows creating an
-   * optimized {@link FacetResultsHandler}, which does not call
-   * {@link FacetRequest#getValueOf(FacetArrays, int)} for every ordinals.
-   * <p>
-   * If set to {@link #BOTH}, the {@link FacetResultsHandler} will use
-   * {@link FacetRequest#getValueOf(FacetArrays, int)} to resolve ordinal
-   * values, although it is recommended that you consider writing a specialized
-   * {@link FacetResultsHandler}.
+   * Defines which categories to return. If {@link #DESCENDING} (the default),
+   * the highest {@link FacetRequest#numResults} weighted categories will be
+   * returned, otherwise the lowest ones.
    */
-  public enum FacetArraysSource { INT, FLOAT, BOTH }
-  
-  /** Requested sort order for the results. */
   public enum SortOrder { ASCENDING, DESCENDING }
-  
-  /**
-   * Default depth for facets accumulation.
-   * @see #getDepth()
-   */
-  public static final int DEFAULT_DEPTH = 1;
-  
-  /**
-   * Default result mode
-   * @see #getResultMode()
-   */
-  public static final ResultMode DEFAULT_RESULT_MODE = ResultMode.PER_NODE_IN_TREE;
-  
+
+  /** The category being aggregated in this facet request. */
   public final CategoryPath categoryPath;
+  
+  /** The number of child categories to return for {@link #categoryPath}. */
   public final int numResults;
   
   private int numLabel;
-  private int depth;
-  private SortOrder sortOrder;
+  private int depth = 1;
+  private SortOrder sortOrder = SortOrder.DESCENDING;
+  private ResultMode resultMode = ResultMode.PER_NODE_IN_TREE;
   
-  /**
-   * Computed at construction, this hashCode is based on two final members
-   * {@link CategoryPath} and <code>numResults</code>
-   */
+  // Computed at construction; based on categoryPath and numResults.
   private final int hashCode;
   
-  private ResultMode resultMode = DEFAULT_RESULT_MODE;
-  
   /**
-   * Initialize the request with a given path, and a requested number of facets
-   * results. By default, all returned results would be labeled - to alter this
-   * default see {@link #setNumLabel(int)}.
-   * <p>
-   * <b>NOTE:</b> if <code>numResults</code> is given as
-   * <code>Integer.MAX_VALUE</code> than all the facet results would be
-   * returned, without any limit.
-   * <p>
-   * <b>NOTE:</b> it is assumed that the given {@link CategoryPath} is not
-   * modified after construction of this object. Otherwise, some things may not
-   * function properly, e.g. {@link #hashCode()}.
+   * Constructor with the given category to aggregate and the number of child
+   * categories to return.
    * 
-   * @throws IllegalArgumentException if numResults is &le; 0
+   * @param path
+   *          the category to aggregate. Cannot be {@code null}.
+   * @param numResults
+   *          the number of child categories to return. If set to
+   *          {@code Integer.MAX_VALUE}, all immediate child categories will be
+   *          returned. Must be greater than 0.
    */
   public FacetRequest(CategoryPath path, int numResults) {
     if (numResults <= 0) {
@@ -118,152 +98,116 @@ public abstract class FacetRequest {
     categoryPath = path;
     this.numResults = numResults;
     numLabel = numResults;
-    depth = DEFAULT_DEPTH;
-    sortOrder = SortOrder.DESCENDING;
-    
     hashCode = categoryPath.hashCode() ^ this.numResults;
   }
   
   /**
-   * Create an aggregator for this facet request. Aggregator action depends on
-   * request definition. For a count request, it will usually increment the
-   * count for that facet.
-   * 
-   * @param useComplements
-   *          whether the complements optimization is being used for current
-   *          computation.
-   * @param arrays
-   *          provider for facet arrays in use for current computation.
-   * @param taxonomy
-   *          reader of taxonomy in effect.
-   * @throws IOException If there is a low-level I/O error.
+   * Returns the {@link FacetsAggregator} which can aggregate the categories of
+   * this facet request. The aggregator is expected to aggregate category values
+   * into {@link FacetArrays}. If the facet request does not support that, e.g.
+   * {@link RangeFacetRequest}, it can return {@code null}. Note though that
+   * such requests require a dedicated {@link FacetsAccumulator}.
    */
-  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) 
-      throws IOException {
-    throw new UnsupportedOperationException("this FacetRequest does not support this type of Aggregator anymore; " +
-        "you should override FacetsAccumulator to return the proper FacetsAggregator");
-  }
+  public abstract FacetsAggregator createFacetsAggregator(FacetIndexingParams fip);
   
   @Override
   public boolean equals(Object o) {
     if (o instanceof FacetRequest) {
-      FacetRequest that = (FacetRequest)o;
-      return that.hashCode == this.hashCode &&
+      FacetRequest that = (FacetRequest) o;
+     return that.hashCode == this.hashCode &&
           that.categoryPath.equals(this.categoryPath) &&
           that.numResults == this.numResults &&
           that.depth == this.depth &&
           that.resultMode == this.resultMode &&
-          that.numLabel == this.numLabel;
+          that.numLabel == this.numLabel &&
+          that.sortOrder == this.sortOrder;
     }
     return false;
   }
   
   /**
-   * How deeply to look under the given category. If the depth is 0,
-   * only the category itself is counted. If the depth is 1, its immediate
-   * children are also counted, and so on. If the depth is Integer.MAX_VALUE,
-   * all the category's descendants are counted.<br>
+   * How deeply to look under {@link #categoryPath}. By default, only its
+   * immediate children are aggregated (depth=1). If set to
+   * {@code Integer.MAX_VALUE}, the entire sub-tree of the category will be
+   * aggregated.
+   * <p>
+   * <b>NOTE:</b> setting depth to 0 means that only the category itself should
+   * be aggregated. In that case, make sure to index the category with
+   * {@link OrdinalPolicy#ALL_PARENTS}, unless it is not the root category (the
+   * dimension), in which case {@link OrdinalPolicy#ALL_BUT_DIMENSION} is fine
+   * too.
    */
   public final int getDepth() {
-    // TODO add AUTO_EXPAND option  
+    // TODO an AUTO_EXPAND option could be useful  
     return depth;
   }
   
   /**
-   * Returns the {@link FacetArraysSource} this {@link FacetRequest} uses in
-   * {@link #getValueOf(FacetArrays, int)}.
-   */
-  public abstract FacetArraysSource getFacetArraysSource();
-  
-  /**
-   * If getNumLabel() &lt; getNumResults(), only the first getNumLabel() results
-   * will have their category paths calculated, and the rest will only be
-   * available as ordinals (category numbers) and will have null paths.
-   * <P>
-   * If Integer.MAX_VALUE is specified, all results are labled.
-   * <P>
-   * The purpose of this parameter is to avoid having to run the whole faceted
-   * search again when the user asks for more values for the facet; The
-   * application can ask (getNumResults()) for more values than it needs to
-   * show, but keep getNumLabel() only the number it wants to immediately show.
-   * The slow-down caused by finding more values is negligible, because the
-   * slowest part - finding the categories' paths, is avoided.
+   * Allows to specify the number of categories to label. By default all
+   * returned categories are labeled.
    * <p>
-   * Depending on the {@link #getResultMode() LimitsMode}, this limit is applied
-   * globally or per results node. In the global mode, if this limit is 3, only
-   * 3 top results would be labeled. In the per-node mode, if this limit is 3, 3
-   * top children of {@link #categoryPath the target category} would be labeled,
-   * as well as 3 top children of each of them, and so forth, until the depth
-   * defined by {@link #getDepth()}.
-   * 
-   * @see #getResultMode()
+   * This allows an app to request a large number of results to return, while
+   * labeling them on-demand (e.g. when the UI requests to show more
+   * categories).
    */
   public final int getNumLabel() {
     return numLabel;
   }
   
-  /** Return the requested result mode. */
+  /** Return the requested result mode (defaults to {@link ResultMode#PER_NODE_IN_TREE}. */
   public final ResultMode getResultMode() {
     return resultMode;
   }
   
-  /** Return the requested order of results. */
+  /** Return the requested order of results (defaults to {@link SortOrder#DESCENDING}. */
   public final SortOrder getSortOrder() {
     return sortOrder;
   }
   
-  /**
-   * Return the value of a category used for facets computations for this
-   * request. For a count request this would be the count for that facet, i.e.
-   * an integer number. but for other requests this can be the result of a more
-   * complex operation, and the result can be any double precision number.
-   * Having this method with a general name <b>value</b> which is double
-   * precision allows to have more compact API and code for handling counts and
-   * perhaps other requests (such as for associations) very similarly, and by
-   * the same code and API, avoiding code duplication.
-   * 
-   * @param arrays
-   *          provider for facet arrays in use for current computation.
-   * @param idx
-   *          an index into the count arrays now in effect in
-   *          <code>arrays</code>. E.g., for ordinal number <i>n</i>, with
-   *          partition, of size <i>partitionSize</i>, now covering <i>n</i>,
-   *          <code>getValueOf</code> would be invoked with <code>idx</code>
-   *          being <i>n</i> % <i>partitionSize</i>.
-   */
-  // TODO perhaps instead of getValueOf we can have a postProcess(FacetArrays)
-  // That, together with getFacetArraysSource should allow ResultHandlers to
-  // efficiently obtain the values from the arrays directly
-  public abstract double getValueOf(FacetArrays arrays, int idx);
-  
   @Override
   public int hashCode() {
     return hashCode; 
   }
   
+  /**
+   * Sets the depth up to which to aggregate facets.
+   * 
+   * @see #getDepth()
+   */
   public void setDepth(int depth) {
     this.depth = depth;
   }
   
+  /**
+   * Sets the number of categories to label.
+   * 
+   * @see #getNumLabel()
+   */
   public void setNumLabel(int numLabel) {
     this.numLabel = numLabel;
   }
   
   /**
-   * @param resultMode the resultMode to set
+   * Sets the {@link ResultMode} for this request.
+   * 
    * @see #getResultMode()
    */
   public void setResultMode(ResultMode resultMode) {
     this.resultMode = resultMode;
   }
-  
+
+  /**
+   * Sets the {@link SortOrder} for this request.
+   * 
+   * @see #getSortOrder()
+   */
   public void setSortOrder(SortOrder sortOrder) {
     this.sortOrder = sortOrder;
   }
   
   @Override
   public String toString() {
-    return categoryPath.toString()+" nRes="+numResults+" nLbl="+numLabel;
+    return categoryPath.toString() + " nRes=" + numResults + " nLbl=" + numLabel;
   }
   
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResult.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResult.java
index d21fbf6..aa2c239 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResult.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResult.java
@@ -7,6 +7,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.util.CollectionUtil;
@@ -96,6 +97,11 @@ public class FacetResult {
         Map<CategoryPath, FacetResultNode> mergedNodes = new HashMap<CategoryPath,FacetResultNode>();
         FacetArrays arrays = dimArrays != null ? dimArrays.get(frs.get(0).getFacetRequest().categoryPath.components[0]) : null;
         for (FacetResult fr : frs) {
+          FacetRequest freq = fr.getFacetRequest();
+          OrdinalValueResolver resolver = null;
+          if (arrays != null) {
+            resolver = freq.createFacetsAggregator(FacetIndexingParams.DEFAULT).createOrdinalValueResolver(freq, arrays);
+          }
           FacetResultNode frn = fr.getFacetResultNode();
           FacetResultNode merged = mergedNodes.get(frn.label);
           if (merged == null) {
@@ -104,7 +110,10 @@ public class FacetResult {
             FacetResultNode parentNode = null;
             while (parent.length > 0 && (parentNode = mergedNodes.get(parent)) == null) {
               int parentOrd = taxoReader.getOrdinal(parent);
-              double parentValue = arrays != null ? fr.getFacetRequest().getValueOf(arrays, parentOrd) : -1;
+              double parentValue = -1;
+              if (arrays != null) {
+                parentValue = resolver.valueOf(parentOrd);
+              }
               parentNode = new FacetResultNode(parentOrd, parentValue);
               parentNode.label = parent;
               parentNode.subResults = new ArrayList<FacetResultNode>();
@@ -153,12 +162,7 @@ public class FacetResult {
         }
         FacetRequest dummy = new FacetRequest(min, frs.get(0).getFacetRequest().numResults) {
           @Override
-          public double getValueOf(FacetArrays arrays, int idx) {
-            throw new UnsupportedOperationException("not supported by this request");
-          }
-          
-          @Override
-          public FacetArraysSource getFacetArraysSource() {
+          public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
             throw new UnsupportedOperationException("not supported by this request");
           }
         };
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultNode.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultNode.java
index c6110b3..bf49306 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultNode.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultNode.java
@@ -28,10 +28,13 @@ import org.apache.lucene.facet.taxonomy.TaxonomyReader;
  * Result of faceted search for a certain taxonomy node. This class serves as a
  * bin of different attributes of the result node, such as its {@link #ordinal}
  * as well as {@link #label}. You are not expected to modify those values.
+ * <p>
+ * This class implements {@link Comparable} for easy comparisons of result
+ * nodes, e.g. when sorting or computing top-K nodes.
  * 
  * @lucene.experimental
  */
-public class FacetResultNode {
+public class FacetResultNode implements Comparable<FacetResultNode> {
 
   public static final List<FacetResultNode> EMPTY_SUB_RESULTS = Collections.emptyList();
   
@@ -71,6 +74,15 @@ public class FacetResultNode {
     this.ordinal = ordinal;
     this.value = value;
   }
+
+  @Override
+  public int compareTo(FacetResultNode o) {
+    int res = Double.compare(value, o.value);
+    if (res == 0) {
+      res = ordinal - o.ordinal;
+    }
+    return res;
+  }
   
   @Override
   public String toString() {
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java
index 3123c4d..9992d2d 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java
@@ -29,15 +29,17 @@ import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 public abstract class FacetResultsHandler {
 
   public final TaxonomyReader taxonomyReader;
-
   public final FacetRequest facetRequest;
   
+  protected final OrdinalValueResolver resolver;
   protected final FacetArrays facetArrays;
 
-  public FacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, FacetArrays facetArrays) {
+  public FacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, OrdinalValueResolver resolver, 
+      FacetArrays facetArrays) {
     this.taxonomyReader = taxonomyReader;
     this.facetRequest = facetRequest;
     this.facetArrays = facetArrays;
+    this.resolver = resolver;
   }
 
   /** Computes the {@link FacetResult} for the given {@link FacetArrays}. */
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java
index ce0dd33..1488cde 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java
@@ -2,20 +2,16 @@ package org.apache.lucene.facet.search;
 
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
 import java.util.List;
-import java.util.Set;
 
-import org.apache.lucene.facet.encoding.DGapVInt8IntDecoder;
-import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.search.FacetRequest.FacetArraysSource;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.FacetRequest.SortOrder;
+import org.apache.lucene.facet.range.RangeAccumulator;
+import org.apache.lucene.facet.range.RangeFacetRequest;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesAccumulator;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.IndexReader;
 
@@ -37,120 +33,126 @@ import org.apache.lucene.index.IndexReader;
  */
 
 /**
- * Driver for Accumulating facets of faceted search requests over given
- * documents.
+ * Accumulates the facets defined in the {@link FacetSearchParams}.
  * 
  * @lucene.experimental
  */
-public class FacetsAccumulator {
+public abstract class FacetsAccumulator {
 
-  public final TaxonomyReader taxonomyReader;
-  public final IndexReader indexReader;
-  public final FacetArrays facetArrays;
-  public FacetSearchParams searchParams;
+  // TODO this should be final, but currently SamplingAccumulator modifies the params.
+  // need to review the class and if it's resolved, make it final
+  public /*final*/ FacetSearchParams searchParams;
 
-  /**
-   * Initializes the accumulator with the given search params, index reader and
-   * taxonomy reader. This constructor creates the default {@link FacetArrays},
-   * which do not support reuse. If you want to use {@link ReusingFacetArrays},
-   * you should use the
-   * {@link #FacetsAccumulator(FacetSearchParams, IndexReader, TaxonomyReader, FacetArrays)}
-   * constructor.
-   */
-  public FacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader, TaxonomyReader taxonomyReader) {
-    this(searchParams, indexReader, taxonomyReader, new FacetArrays(taxonomyReader.getSize()));
+  /** Constructor with the given search params. */
+  protected FacetsAccumulator(FacetSearchParams fsp) {
+    this.searchParams = fsp;
   }
 
   /**
-   * Creates an appropriate {@link FacetsAccumulator},
-   * returning {@link FacetsAccumulator} when all requests
-   * are {@link CountFacetRequest} and only one partition is
-   * in use, otherwise {@link StandardFacetsAccumulator}.
+   * Creates a {@link FacetsAccumulator} for the given facet requests. This
+   * method supports {@link RangeAccumulator} and
+   * {@link TaxonomyFacetsAccumulator} by dividing the facet requests into
+   * {@link RangeFacetRequest} and the rest.
+   * <p>
+   * If both types of facet requests are used, it returns a
+   * {@link MultiFacetsAccumulator} and the facet results returned from
+   * {@link #accumulate(List)} may not be in the same order as the given facet
+   * requests.
+   * 
+   * @param fsp
+   *          the search params define the facet requests and the
+   *          {@link FacetIndexingParams}
+   * @param indexReader
+   *          the {@link IndexReader} used for search
+   * @param taxoReader
+   *          the {@link TaxonomyReader} used for search
+   * @param arrays
+   *          the {@link FacetArrays} which the accumulator should use to store
+   *          the categories weights in. Can be {@code null}.
    */
-  public static FacetsAccumulator create(FacetSearchParams fsp, IndexReader indexReader, TaxonomyReader taxoReader) {
+  public static FacetsAccumulator create(FacetSearchParams fsp, IndexReader indexReader, TaxonomyReader taxoReader, 
+      FacetArrays arrays) {
     if (fsp.indexingParams.getPartitionSize() != Integer.MAX_VALUE) {
-      return new StandardFacetsAccumulator(fsp, indexReader, taxoReader);
+      return new OldFacetsAccumulator(fsp, indexReader, taxoReader, arrays);
     }
     
+    List<FacetRequest> rangeRequests = new ArrayList<FacetRequest>();
+    List<FacetRequest> nonRangeRequests = new ArrayList<FacetRequest>();
     for (FacetRequest fr : fsp.facetRequests) {
-      if (!(fr instanceof CountFacetRequest)) {
-        return new StandardFacetsAccumulator(fsp, indexReader, taxoReader);
+      if (fr instanceof RangeFacetRequest) {
+        rangeRequests.add(fr);
+      } else {
+        nonRangeRequests.add(fr);
       }
     }
-    
-    return new FacetsAccumulator(fsp, indexReader, taxoReader);
-  }
-  
-  /** Returns an empty {@link FacetResult}. */
-  protected static FacetResult emptyResult(int ordinal, FacetRequest fr) {
-    FacetResultNode root = new FacetResultNode(ordinal, 0);
-    root.label = fr.categoryPath;
-    return new FacetResult(fr, root, 0);
-  }
-  
-  /**
-   * Initializes the accumulator with the given parameters as well as
-   * {@link FacetArrays}. Note that the accumulator doesn't call
-   * {@link FacetArrays#free()}. If you require that (only makes sense if you
-   * use {@link ReusingFacetArrays}, you should do it after you've finished with
-   * the accumulator.
-   */
-  public FacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader, TaxonomyReader taxonomyReader, 
-      FacetArrays facetArrays) {
-    this.facetArrays = facetArrays;
-    this.indexReader = indexReader;
-    this.taxonomyReader = taxonomyReader;
-    this.searchParams = searchParams;
-  }
-  
-  /**
-   * Returns the {@link FacetsAggregator} to use for aggregating the categories
-   * found in the result documents. The default implementation returns
-   * {@link CountingFacetsAggregator}, or {@link FastCountingFacetsAggregator}
-   * if all categories can be decoded with {@link DGapVInt8IntDecoder}.
-   */
-  public FacetsAggregator getAggregator() {
-    if (FastCountingFacetsAggregator.verifySearchParams(searchParams)) {
-      return new FastCountingFacetsAggregator();
+
+    if (rangeRequests.isEmpty()) {
+      return new TaxonomyFacetsAccumulator(fsp, indexReader, taxoReader, arrays);
+    } else if (nonRangeRequests.isEmpty()) {
+      return new RangeAccumulator(rangeRequests);
     } else {
-      return new CountingFacetsAggregator();
+      FacetSearchParams searchParams = new FacetSearchParams(fsp.indexingParams, nonRangeRequests);
+      FacetsAccumulator accumulator = new TaxonomyFacetsAccumulator(searchParams, indexReader, taxoReader, arrays);
+      RangeAccumulator rangeAccumulator = new RangeAccumulator(rangeRequests);
+      return MultiFacetsAccumulator.wrap(accumulator, rangeAccumulator);
     }
   }
   
   /**
-   * Creates a {@link FacetResultsHandler} that matches the given
-   * {@link FacetRequest}.
+   * Creates a {@link FacetsAccumulator} for the given facet requests. This
+   * method supports {@link RangeAccumulator} and
+   * {@link SortedSetDocValuesAccumulator} by dividing the facet requests into
+   * {@link RangeFacetRequest} and the rest.
+   * <p>
+   * If both types of facet requests are used, it returns a
+   * {@link MultiFacetsAccumulator} and the facet results returned from
+   * {@link #accumulate(List)} may not be in the same order as the given facet
+   * requests.
+   * 
+   * @param fsp
+   *          the search params define the facet requests and the
+   *          {@link FacetIndexingParams}
+   * @param state
+   *          the {@link SortedSetDocValuesReaderState} needed for accumulating
+   *          the categories
+   * @param arrays
+   *          the {@link FacetArrays} which the accumulator should use to
+   *          store the categories weights in. Can be {@code null}.
    */
-  protected FacetResultsHandler createFacetResultsHandler(FacetRequest fr) {
-    if (fr.getDepth() == 1 && fr.getSortOrder() == SortOrder.DESCENDING) {
-      FacetArraysSource fas = fr.getFacetArraysSource();
-      if (fas == FacetArraysSource.INT) {
-        return new IntFacetResultsHandler(taxonomyReader, fr, facetArrays);
-      }
-      
-      if (fas == FacetArraysSource.FLOAT) {
-        return new FloatFacetResultsHandler(taxonomyReader, fr, facetArrays);
-      }
+  public static FacetsAccumulator create(FacetSearchParams fsp, SortedSetDocValuesReaderState state, FacetArrays arrays) throws IOException {
+    if (fsp.indexingParams.getPartitionSize() != Integer.MAX_VALUE) {
+      throw new IllegalArgumentException("only default partition size is supported by this method: " + fsp.indexingParams.getPartitionSize());
     }
-
-    if (fr.getResultMode() == ResultMode.PER_NODE_IN_TREE) {
-      return new TopKInEachNodeHandler(taxonomyReader, fr, facetArrays);
-    } 
-    return new TopKFacetResultsHandler(taxonomyReader, fr, facetArrays);
-  }
-
-  protected Set<CategoryListParams> getCategoryLists() {
-    if (searchParams.indexingParams.getAllCategoryListParams().size() == 1) {
-      return Collections.singleton(searchParams.indexingParams.getCategoryListParams(null));
+    
+    List<FacetRequest> rangeRequests = new ArrayList<FacetRequest>();
+    List<FacetRequest> nonRangeRequests = new ArrayList<FacetRequest>();
+    for (FacetRequest fr : fsp.facetRequests) {
+      if (fr instanceof RangeFacetRequest) {
+        rangeRequests.add(fr);
+      } else {
+        nonRangeRequests.add(fr);
+      }
     }
     
-    HashSet<CategoryListParams> clps = new HashSet<CategoryListParams>();
-    for (FacetRequest fr : searchParams.facetRequests) {
-      clps.add(searchParams.indexingParams.getCategoryListParams(fr.categoryPath));
+    if (rangeRequests.isEmpty()) {
+      return new SortedSetDocValuesAccumulator(state, fsp, arrays);
+    } else if (nonRangeRequests.isEmpty()) {
+      return new RangeAccumulator(rangeRequests);
+    } else {
+      FacetSearchParams searchParams = new FacetSearchParams(fsp.indexingParams, nonRangeRequests);
+      FacetsAccumulator accumulator = new SortedSetDocValuesAccumulator(state, searchParams, arrays);
+      RangeAccumulator rangeAccumulator = new RangeAccumulator(rangeRequests);
+      return MultiFacetsAccumulator.wrap(accumulator, rangeAccumulator);
     }
-    return clps;
   }
-
+  
+  /** Returns an empty {@link FacetResult}. */
+  protected static FacetResult emptyResult(int ordinal, FacetRequest fr) {
+    FacetResultNode root = new FacetResultNode(ordinal, 0);
+    root.label = fr.categoryPath;
+    return new FacetResult(fr, root, 0);
+  }
+  
   /**
    * Used by {@link FacetsCollector} to build the list of {@link FacetResult
    * facet results} that match the {@link FacetRequest facet requests} that were
@@ -159,44 +161,12 @@ public class FacetsAccumulator {
    * @param matchingDocs
    *          the documents that matched the query, per-segment.
    */
-  public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
-    // aggregate facets per category list (usually onle one category list)
-    FacetsAggregator aggregator = getAggregator();
-    for (CategoryListParams clp : getCategoryLists()) {
-      for (MatchingDocs md : matchingDocs) {
-        aggregator.aggregate(md, clp, facetArrays);
-      }
-    }
-    
-    ParallelTaxonomyArrays arrays = taxonomyReader.getParallelTaxonomyArrays();
-    
-    // compute top-K
-    final int[] children = arrays.children();
-    final int[] siblings = arrays.siblings();
-    List<FacetResult> res = new ArrayList<FacetResult>();
-    for (FacetRequest fr : searchParams.facetRequests) {
-      int rootOrd = taxonomyReader.getOrdinal(fr.categoryPath);
-      if (rootOrd == TaxonomyReader.INVALID_ORDINAL) { // category does not exist
-        // Add empty FacetResult
-        res.add(emptyResult(rootOrd, fr));
-        continue;
-      }
-      CategoryListParams clp = searchParams.indexingParams.getCategoryListParams(fr.categoryPath);
-      if (fr.categoryPath.length > 0) { // someone might ask to aggregate the ROOT category
-        OrdinalPolicy ordinalPolicy = clp.getOrdinalPolicy(fr.categoryPath.components[0]);
-        if (ordinalPolicy == OrdinalPolicy.NO_PARENTS) {
-          // rollup values
-          aggregator.rollupValues(fr, rootOrd, children, siblings, facetArrays);
-        }
-      }
-      
-      FacetResultsHandler frh = createFacetResultsHandler(fr);
-      res.add(frh.compute());
-    }
-    return res;
-  }
+  public abstract List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException;
 
-  public boolean requiresDocScores() {
-    return getAggregator().requiresDocScores();
-  }
+  /**
+   * Used by {@link FacetsCollector} to determine if document scores need to be
+   * collected in addition to matching documents.
+   */
+  public abstract boolean requiresDocScores();
+  
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java
index befb195..16cc9f4 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java
@@ -46,4 +46,11 @@ public interface FacetsAggregator {
   /** Returns {@code true} if this aggregator requires document scores. */
   public boolean requiresDocScores();
   
+  /**
+   * Creates the appropriate {@link OrdinalValueResolver} for this aggregator
+   * and the given {@link FacetRequest}. The request is passed so that compound
+   * aggregators can return the correct {@link OrdinalValueResolver}.
+   */
+  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays);
+  
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java
index c7ba611..f27d3e2 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java
@@ -167,7 +167,7 @@ public abstract class FacetsCollector extends Collector {
    * FacetsAccumulator} from {@link FacetsAccumulator#create}.
    */
   public static FacetsCollector create(FacetSearchParams fsp, IndexReader indexReader, TaxonomyReader taxoReader) {
-    return create(FacetsAccumulator.create(fsp, indexReader, taxoReader));
+    return create(FacetsAccumulator.create(fsp, indexReader, taxoReader, null));
   }
 
   /**
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java
index 252eb4c..fdb81a6 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java
@@ -5,7 +5,6 @@ import java.io.IOException;
 import org.apache.lucene.facet.encoding.DGapVInt8IntDecoder;
 import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
 import org.apache.lucene.facet.params.CategoryListParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.util.BytesRef;
@@ -40,23 +39,6 @@ public final class FastCountingFacetsAggregator extends IntRollupFacetsAggregato
   
   private final BytesRef buf = new BytesRef(32);
   
-  /**
-   * Asserts that this {@link FacetsCollector} can handle the given
-   * {@link FacetSearchParams}. Returns {@code null} if true, otherwise an error
-   * message.
-   */
-  final static boolean verifySearchParams(FacetSearchParams fsp) {
-    // verify that all category lists were encoded with DGapVInt
-    for (FacetRequest fr : fsp.facetRequests) {
-      CategoryListParams clp = fsp.indexingParams.getCategoryListParams(fr.categoryPath);
-      if (clp.createEncoder().createMatchingDecoder().getClass() != DGapVInt8IntDecoder.class) {
-        return false;
-      }
-    }
-    
-    return true;
-  }
-
   @Override
   public final void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) 
       throws IOException {
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FloatFacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FloatFacetResultsHandler.java
deleted file mode 100644
index 004bedf..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FloatFacetResultsHandler.java
+++ /dev/null
@@ -1,78 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.util.PriorityQueue;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link DepthOneFacetResultsHandler} which fills the categories values from
- * {@link FacetArrays#getFloatArray()}.
- * 
- * @lucene.experimental
- */
-public final class FloatFacetResultsHandler extends DepthOneFacetResultsHandler {
-
-  private final float[] values;
-  
-  public FloatFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, FacetArrays facetArrays) {
-    super(taxonomyReader, facetRequest, facetArrays);
-    this.values = facetArrays.getFloatArray();
-  }
-
-  @Override
-  protected final double valueOf(int ordinal) {
-    return values[ordinal];
-  }
-
-  @Override
-  protected final int addSiblings(int ordinal, int[] siblings, PriorityQueue<FacetResultNode> pq) {
-    FacetResultNode top = pq.top();
-    int numResults = 0;
-    while (ordinal != TaxonomyReader.INVALID_ORDINAL) {
-      float value = values[ordinal];
-      if (value > 0.0f) {
-        ++numResults;
-        if (value > top.value) {
-          top.value = value;
-          top.ordinal = ordinal;
-          top = pq.updateTop();
-        }
-      }
-      ordinal = siblings[ordinal];
-    }
-    return numResults;
-  }
-  
-  @Override
-  protected final void addSiblings(int ordinal, int[] siblings, ArrayList<FacetResultNode> nodes) throws IOException {
-    while (ordinal != TaxonomyReader.INVALID_ORDINAL) {
-      float value = values[ordinal];
-      if (value > 0) {
-        FacetResultNode node = new FacetResultNode(ordinal, value);
-        node.label = taxonomyReader.getPath(ordinal);
-        nodes.add(node);
-      }
-      ordinal = siblings[ordinal];
-    }
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/IntFacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/IntFacetResultsHandler.java
deleted file mode 100644
index 51cda6a..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/IntFacetResultsHandler.java
+++ /dev/null
@@ -1,78 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.util.PriorityQueue;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link DepthOneFacetResultsHandler} which fills the categories values from
- * {@link FacetArrays#getIntArray()}.
- * 
- * @lucene.experimental
- */
-public final class IntFacetResultsHandler extends DepthOneFacetResultsHandler {
-  
-  private final int[] values;
-  
-  public IntFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, FacetArrays facetArrays) {
-    super(taxonomyReader, facetRequest, facetArrays);
-    this.values = facetArrays.getIntArray();
-  }
-  
-  @Override
-  protected final double valueOf(int ordinal) {
-    return values[ordinal];
-  }
-  
-  @Override
-  protected final int addSiblings(int ordinal, int[] siblings, PriorityQueue<FacetResultNode> pq) {
-    FacetResultNode top = pq.top();
-    int numResults = 0;
-    while (ordinal != TaxonomyReader.INVALID_ORDINAL) {
-      int value = values[ordinal];
-      if (value > 0) {
-        ++numResults;
-        if (value > top.value) {
-          top.value = value;
-          top.ordinal = ordinal;
-          top = pq.updateTop();
-        }
-      }
-      ordinal = siblings[ordinal];
-    }
-    return numResults;
-  }
-  
-  @Override
-  protected final void addSiblings(int ordinal, int[] siblings, ArrayList<FacetResultNode> nodes) throws IOException {
-    while (ordinal != TaxonomyReader.INVALID_ORDINAL) {
-      int value = values[ordinal];
-      if (value > 0) {
-        FacetResultNode node = new FacetResultNode(ordinal, value);
-        node.label = taxonomyReader.getPath(ordinal);
-        nodes.add(node);
-      }
-      ordinal = siblings[ordinal];
-    }
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/IntRollupFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/IntRollupFacetsAggregator.java
index 82b6b17..4947f68 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/IntRollupFacetsAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/IntRollupFacetsAggregator.java
@@ -4,6 +4,7 @@ import java.io.IOException;
 
 import org.apache.lucene.facet.params.CategoryListParams;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.search.OrdinalValueResolver.IntValueResolver;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 
 /*
@@ -60,4 +61,9 @@ public abstract class IntRollupFacetsAggregator implements FacetsAggregator {
     return false;
   }
   
+  @Override
+  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
+    return new IntValueResolver(arrays);
+  }
+  
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/MatchingDocsAsScoredDocIDs.java b/lucene/facet/src/java/org/apache/lucene/facet/search/MatchingDocsAsScoredDocIDs.java
deleted file mode 100644
index 5d1014e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/MatchingDocsAsScoredDocIDs.java
+++ /dev/null
@@ -1,174 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.DocIdSetIterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** 
- * Represents {@link MatchingDocs} as {@link ScoredDocIDs}.
- * 
- * @lucene.experimental
- */
-public class MatchingDocsAsScoredDocIDs implements ScoredDocIDs {
-
-  // TODO remove this class once we get rid of ScoredDocIDs 
-
-  final List<MatchingDocs> matchingDocs;
-  final int size;
-  
-  public MatchingDocsAsScoredDocIDs(List<MatchingDocs> matchingDocs) {
-    this.matchingDocs = matchingDocs;
-    int totalSize = 0;
-    for (MatchingDocs md : matchingDocs) {
-      totalSize += md.totalHits;
-    }
-    this.size = totalSize;
-  }
-  
-  @Override
-  public ScoredDocIDsIterator iterator() throws IOException {
-    return new ScoredDocIDsIterator() {
-      
-      final Iterator<MatchingDocs> mdIter = matchingDocs.iterator();
-      
-      int scoresIdx = 0;
-      int doc = 0;
-      MatchingDocs current;
-      int currentLength;
-      boolean done = false;
-      
-      @Override
-      public boolean next() {
-        if (done) {
-          return false;
-        }
-        
-        while (current == null) {
-          if (!mdIter.hasNext()) {
-            done = true;
-            return false;
-          }
-          current = mdIter.next();
-          currentLength = current.bits.length();
-          doc = 0;
-          scoresIdx = 0;
-          
-          if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
-            current = null;
-          } else {
-            doc = -1; // we're calling nextSetBit later on
-          }
-        }
-        
-        ++doc;
-        if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
-          current = null;
-          return next();
-        }
-        
-        return true;
-      }
-      
-      @Override
-      public float getScore() {
-        return current.scores == null ? ScoredDocIDsIterator.DEFAULT_SCORE : current.scores[scoresIdx++];
-      }
-      
-      @Override
-      public int getDocID() {
-        return done ? DocIdSetIterator.NO_MORE_DOCS : doc + current.context.docBase;
-      }
-    };
-  }
-
-  @Override
-  public DocIdSet getDocIDs() {
-    return new DocIdSet() {
-      
-      final Iterator<MatchingDocs> mdIter = matchingDocs.iterator();
-      int doc = 0;
-      MatchingDocs current;
-      int currentLength;
-      boolean done = false;
-      
-      @Override
-      public DocIdSetIterator iterator() throws IOException {
-        return new DocIdSetIterator() {
-          
-          @Override
-          public int nextDoc() throws IOException {
-            if (done) {
-              return DocIdSetIterator.NO_MORE_DOCS;
-            }
-            
-            while (current == null) {
-              if (!mdIter.hasNext()) {
-                done = true;
-                return DocIdSetIterator.NO_MORE_DOCS;
-              }
-              current = mdIter.next();
-              currentLength = current.bits.length();
-              doc = 0;
-              
-              if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
-                current = null;
-              } else {
-                doc = -1; // we're calling nextSetBit later on
-              }
-            }
-            
-            ++doc;
-            if (doc >= currentLength || (doc = current.bits.nextSetBit(doc)) == -1) {
-              current = null;
-              return nextDoc();
-            }
-            
-            return doc + current.context.docBase;
-          }
-          
-          @Override
-          public int docID() {
-            return doc + current.context.docBase;
-          }
-          
-          @Override
-          public long cost() {
-            return size;
-          }
-
-          @Override
-          public int advance(int target) throws IOException {
-            throw new UnsupportedOperationException("not supported");
-          }
-        };
-      }
-    };
-  }
-
-  @Override
-  public int size() {
-    return size;
-  }
-  
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAccumulator.java
new file mode 100644
index 0000000..e82f76b
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAccumulator.java
@@ -0,0 +1,69 @@
+package org.apache.lucene.facet.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetsAccumulator;
+import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+
+/**
+ * Wraps multiple {@link FacetsAccumulator} and returns a merged list of
+ * {@link FacetResult}, in the order the accumulators were given.
+ */
+public class MultiFacetsAccumulator extends FacetsAccumulator {
+  
+  private final FacetsAccumulator[] accumulators;
+  
+  /** Wraps the given {@link FacetsAccumulator accumulators}. */
+  public static FacetsAccumulator wrap(FacetsAccumulator... accumulators) {
+    if (accumulators.length == 0) {
+      return accumulators[0];
+    } else {
+      return new MultiFacetsAccumulator(accumulators);
+    }
+  }
+
+  private MultiFacetsAccumulator(FacetsAccumulator... accumulators) {
+    super((FacetSearchParams) null);
+    this.accumulators = accumulators;
+  }
+
+  @Override
+  public boolean requiresDocScores() {
+    for (FacetsAccumulator fa : accumulators) {
+      if (fa.requiresDocScores()) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  @Override
+  public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
+    List<FacetResult> merged = new ArrayList<FacetResult>();
+    for (FacetsAccumulator fa : accumulators) {
+      merged.addAll(fa.accumulate(matchingDocs));
+    }
+    return merged;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAggregator.java
new file mode 100644
index 0000000..1935ecb
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/MultiFacetsAggregator.java
@@ -0,0 +1,96 @@
+package org.apache.lucene.facet.search;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetsAggregator;
+import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetsAggregator} which chains multiple aggregators for aggregating
+ * the association values of categories that belong to the same category list.
+ * While nothing prevents you from chaining general purpose aggregators, it is
+ * only useful for aggregating association values, as each association type is
+ * written in its own list.
+ * 
+ * @lucene.experimental
+ */
+public class MultiFacetsAggregator implements FacetsAggregator {
+  
+  private final Map<CategoryPath,FacetsAggregator> categoryAggregators;
+  private final List<FacetsAggregator> aggregators;
+  
+  /**
+   * Constructor.
+   * <p>
+   * The mapping is used to rollup the values of the specific category by the
+   * corresponding {@link FacetsAggregator}. It is ok to pass differnet
+   * {@link FacetsAggregator} instances for each {@link CategoryPath} - the
+   * constructor ensures that each aggregator <u>type</u> (determined by its
+   * class) is invoked only once.
+   */
+  public MultiFacetsAggregator(Map<CategoryPath,FacetsAggregator> aggregators) {
+    this.categoryAggregators = aggregators;
+    
+    // make sure that each FacetsAggregator class is invoked only once, or
+    // otherwise categories may be aggregated multiple times.
+    Map<Class<? extends FacetsAggregator>, FacetsAggregator> aggsClasses = 
+        new HashMap<Class<? extends FacetsAggregator>,FacetsAggregator>();
+    for (FacetsAggregator fa : aggregators.values()) {
+      aggsClasses.put(fa.getClass(), fa);
+    }
+    this.aggregators = new ArrayList<FacetsAggregator>(aggsClasses.values());
+  }
+  
+  @Override
+  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
+    for (FacetsAggregator fa : aggregators) {
+      fa.aggregate(matchingDocs, clp, facetArrays);
+    }
+  }
+  
+  @Override
+  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
+    categoryAggregators.get(fr.categoryPath).rollupValues(fr, ordinal, children, siblings, facetArrays);
+  }
+  
+  @Override
+  public boolean requiresDocScores() {
+    for (FacetsAggregator fa : aggregators) {
+      if (fa.requiresDocScores()) {
+        return true;
+      }
+    }
+    return false;
+  }
+  
+  @Override
+  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
+    return categoryAggregators.get(facetRequest.categoryPath).createOrdinalValueResolver(facetRequest, arrays);
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/OrdinalValueResolver.java b/lucene/facet/src/java/org/apache/lucene/facet/search/OrdinalValueResolver.java
new file mode 100644
index 0000000..e228501
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/OrdinalValueResolver.java
@@ -0,0 +1,76 @@
+package org.apache.lucene.facet.search;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Resolves an ordinal's value to given the {@link FacetArrays}.
+ * Implementations of this class are encouraged to initialize the needed array
+ * from {@link FacetArrays} in the constructor.
+ */
+public abstract class OrdinalValueResolver {
+
+  /**
+   * An {@link OrdinalValueResolver} which resolves ordinals value from
+   * {@link FacetArrays#getIntArray()}, by returning the value in the array.
+   */
+  public static final class IntValueResolver extends OrdinalValueResolver {
+
+    private final int[] values;
+    
+    public IntValueResolver(FacetArrays arrays) {
+      super(arrays);
+      this.values = arrays.getIntArray();
+    }
+
+    @Override
+    public final double valueOf(int ordinal) {
+      return values[ordinal];
+    }
+    
+  }
+  
+  /**
+   * An {@link OrdinalValueResolver} which resolves ordinals value from
+   * {@link FacetArrays#getFloatArray()}, by returning the value in the array.
+   */
+  public static final class FloatValueResolver extends OrdinalValueResolver {
+    
+    private final float[] values;
+    
+    public FloatValueResolver(FacetArrays arrays) {
+      super(arrays);
+      this.values = arrays.getFloatArray();
+    }
+    
+    @Override
+    public final double valueOf(int ordinal) {
+      return values[ordinal];
+    }
+    
+  }
+  
+  protected final FacetArrays arrays;
+  
+  protected OrdinalValueResolver(FacetArrays arrays) {
+    this.arrays = arrays;
+  }
+
+  /** Returns the value of the given ordinal. */
+  public abstract double valueOf(int ordinal);
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/PerCategoryListAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/PerCategoryListAggregator.java
index 790dba6..8107409 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/PerCategoryListAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/PerCategoryListAggregator.java
@@ -27,6 +27,9 @@ import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
 /**
  * A {@link FacetsAggregator} which invokes the proper aggregator per
  * {@link CategoryListParams}.
+ * {@link #rollupValues(FacetRequest, int, int[], int[], FacetArrays)} is
+ * delegated to the proper aggregator which handles the
+ * {@link CategoryListParams} the given {@link FacetRequest} belongs to.
  */
 public class PerCategoryListAggregator implements FacetsAggregator {
   
@@ -58,5 +61,11 @@ public class PerCategoryListAggregator implements FacetsAggregator {
     }
     return false;
   }
+
+  @Override
+  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
+    CategoryListParams clp = fip.getCategoryListParams(facetRequest.categoryPath);
+    return aggregators.get(clp).createOrdinalValueResolver(facetRequest, arrays);
+  }
   
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/ScoredDocIDs.java b/lucene/facet/src/java/org/apache/lucene/facet/search/ScoredDocIDs.java
deleted file mode 100644
index 60dc415..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/ScoredDocIDs.java
+++ /dev/null
@@ -1,42 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.search.DocIdSet;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Document IDs with scores for each, driving facets accumulation. Document
- * scores are optionally used in the process of facets scoring.
- * 
- * @see StandardFacetsAccumulator#accumulate(ScoredDocIDs)
- * @lucene.experimental
- */
-public interface ScoredDocIDs {
-
-  /** Returns an iterator over the document IDs and their scores. */
-  public ScoredDocIDsIterator iterator() throws IOException;
-
-  /** Returns the set of doc IDs. */
-  public DocIdSet getDocIDs();
-
-  /** Returns the number of scored documents. */
-  public int size();
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/ScoredDocIDsIterator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/ScoredDocIDsIterator.java
deleted file mode 100644
index 82363d9..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/ScoredDocIDsIterator.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.lucene.facet.search;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Iterator over document IDs and their scores. Each {@link #next()} retrieves
- * the next docID and its score which can be later be retrieved by
- * {@link #getDocID()} and {@link #getScore()}. <b>NOTE:</b> you must call
- * {@link #next()} before {@link #getDocID()} and/or {@link #getScore()}, or
- * otherwise the returned values are unexpected.
- * 
- * @lucene.experimental
- */
-public interface ScoredDocIDsIterator {
-
-  /** Default score used in case scoring is disabled. */
-  public static final float DEFAULT_SCORE = 1.0f;
-
-  /** Iterate to the next document/score pair. Returns true iff there is such a pair. */
-  public abstract boolean next();
-
-  /** Returns the ID of the current document. */
-  public abstract int getDocID();
-
-  /** Returns the score of the current document. */
-  public abstract float getScore();
-
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/ScoringAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/ScoringAggregator.java
deleted file mode 100644
index 2ecf0b6..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/ScoringAggregator.java
+++ /dev/null
@@ -1,67 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link Aggregator} which updates the weight of a category according to the
- * scores of the documents it was found in.
- * 
- * @lucene.experimental
- */
-public class ScoringAggregator implements Aggregator {
-
-  private final float[] scoreArray;
-  private final int hashCode;
-  
-  public ScoringAggregator(float[] counterArray) {
-    this.scoreArray = counterArray;
-    this.hashCode = scoreArray == null ? 0 : scoreArray.hashCode();
-  }
-
-  @Override
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
-    for (int i = 0; i < ordinals.length; i++) {
-      scoreArray[ordinals.ints[i]] += score;
-    }
-  }
-  
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null || obj.getClass() != this.getClass()) {
-      return false;
-    }
-    ScoringAggregator that = (ScoringAggregator) obj;
-    return that.scoreArray == this.scoreArray;
-  }
-
-  @Override
-  public int hashCode() {
-    return hashCode;
-  }
-
-  @Override
-  public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    return true;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator.java
deleted file mode 100644
index 97e57cb..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator.java
+++ /dev/null
@@ -1,419 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map.Entry;
-import java.util.logging.Level;
-import java.util.logging.Logger;
-
-import org.apache.lucene.facet.complements.TotalFacetCounts;
-import org.apache.lucene.facet.complements.TotalFacetCountsCache;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.partitions.IntermediateFacetResult;
-import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.facet.util.ScoredDocIdsUtils;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Standard implementation for {@link FacetsAccumulator}, utilizing partitions to save on memory.
- * <p>
- * Why partitions? Because if there are say 100M categories out of which 
- * only top K are required, we must first compute value for all 100M categories
- * (going over all documents) and only then could we select top K. 
- * This is made easier on memory by working in partitions of distinct categories: 
- * Once a values for a partition are found, we take the top K for that 
- * partition and work on the next partition, them merge the top K of both, 
- * and so forth, thereby computing top K with RAM needs for the size of 
- * a single partition rather than for the size of all the 100M categories.
- * <p>
- * Decision on partitions size is done at indexing time, and the facet information
- * for each partition is maintained separately.
- * <p>
- * <u>Implementation detail:</u> Since facets information of each partition is 
- * maintained in a separate "category list", we can be more efficient
- * at search time, because only the facet info for a single partition 
- * need to be read while processing that partition. 
- * 
- * @lucene.experimental
- */
-public class StandardFacetsAccumulator extends FacetsAccumulator {
-
-  private static final Logger logger = Logger.getLogger(StandardFacetsAccumulator.class.getName());
-
-  /**
-   * Default threshold for using the complements optimization.
-   * If accumulating facets for a document set larger than this ratio of the index size than 
-   * perform the complement optimization.
-   * @see #setComplementThreshold(double) for more info on the complements optimization.  
-   */
-  public static final double DEFAULT_COMPLEMENT_THRESHOLD = 0.6;
-
-  /**
-   * Passing this to {@link #setComplementThreshold(double)} will disable using complement optimization.
-   */
-  public static final double DISABLE_COMPLEMENT = Double.POSITIVE_INFINITY; // > 1 actually
-
-  /**
-   * Passing this to {@link #setComplementThreshold(double)} will force using complement optimization.
-   */
-  public static final double FORCE_COMPLEMENT = 0; // <=0  
-
-  protected int partitionSize;
-  protected int maxPartitions;
-  protected boolean isUsingComplements;
-
-  private TotalFacetCounts totalFacetCounts;
-
-  private Object accumulateGuard;
-
-  private double complementThreshold = DEFAULT_COMPLEMENT_THRESHOLD;
-  
-  public StandardFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader, 
-      TaxonomyReader taxonomyReader) {
-    this(searchParams, indexReader, taxonomyReader, new FacetArrays(
-        PartitionsUtils.partitionSize(searchParams.indexingParams, taxonomyReader)));
-  }
-
-  public StandardFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader,
-      TaxonomyReader taxonomyReader, FacetArrays facetArrays) {
-    super(searchParams, indexReader, taxonomyReader, facetArrays);
-    
-    // can only be computed later when docids size is known
-    isUsingComplements = false;
-    partitionSize = PartitionsUtils.partitionSize(searchParams.indexingParams, taxonomyReader);
-    maxPartitions = (int) Math.ceil(this.taxonomyReader.getSize() / (double) partitionSize);
-    accumulateGuard = new Object();
-  }
-
-  // TODO: this should be removed once we clean the API
-  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
-
-    // synchronize to prevent calling two accumulate()'s at the same time.
-    // We decided not to synchronize the method because that might mislead
-    // users to feel encouraged to call this method simultaneously.
-    synchronized (accumulateGuard) {
-
-      // only now we can compute this
-      isUsingComplements = shouldComplement(docids);
-
-      if (isUsingComplements) {
-        try {
-          totalFacetCounts = TotalFacetCountsCache.getSingleton().getTotalCounts(indexReader, taxonomyReader, 
-              searchParams.indexingParams);
-          if (totalFacetCounts != null) {
-            docids = ScoredDocIdsUtils.getComplementSet(docids, indexReader);
-          } else {
-            isUsingComplements = false;
-          }
-        } catch (UnsupportedOperationException e) {
-          // TODO (Facet): this exception is thrown from TotalCountsKey if the
-          // IndexReader used does not support getVersion(). We should re-think
-          // this: is this tiny detail worth disabling total counts completely
-          // for such readers? Currently, it's not supported by Parallel and
-          // MultiReader, which might be problematic for several applications.
-          // We could, for example, base our "isCurrent" logic on something else
-          // than the reader's version. Need to think more deeply about it.
-          if (logger.isLoggable(Level.FINEST)) {
-            logger.log(Level.FINEST, "IndexReader used does not support completents: ", e);
-          }
-          isUsingComplements = false;
-        } catch (IOException e) {
-          if (logger.isLoggable(Level.FINEST)) {
-            logger.log(Level.FINEST, "Failed to load/calculate total counts (complement counting disabled): ", e);
-          }
-          // silently fail if for some reason failed to load/save from/to dir 
-          isUsingComplements = false;
-        } catch (Exception e) {
-          // give up: this should not happen!
-          throw new IOException("PANIC: Got unexpected exception while trying to get/calculate total counts", e);
-        }
-      }
-
-      docids = actualDocsToAccumulate(docids);
-
-      HashMap<FacetRequest, IntermediateFacetResult> fr2tmpRes = new HashMap<FacetRequest, IntermediateFacetResult>();
-
-      try {
-        for (int part = 0; part < maxPartitions; part++) {
-
-          // fill arrays from category lists
-          fillArraysForPartition(docids, facetArrays, part);
-
-          int offset = part * partitionSize;
-
-          // for each partition we go over all requests and handle
-          // each, where the request maintains the merged result.
-          // In this implementation merges happen after each partition,
-          // but other impl could merge only at the end.
-          final HashSet<FacetRequest> handledRequests = new HashSet<FacetRequest>();
-          for (FacetRequest fr : searchParams.facetRequests) {
-            // Handle and merge only facet requests which were not already handled.  
-            if (handledRequests.add(fr)) {
-              PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);
-              IntermediateFacetResult res4fr = frHndlr.fetchPartitionResult(offset);
-              IntermediateFacetResult oldRes = fr2tmpRes.get(fr);
-              if (oldRes != null) {
-                res4fr = frHndlr.mergeResults(oldRes, res4fr);
-              }
-              fr2tmpRes.put(fr, res4fr);
-            } 
-          }
-        }
-      } finally {
-        facetArrays.free();
-      }
-
-      // gather results from all requests into a list for returning them
-      List<FacetResult> res = new ArrayList<FacetResult>();
-      for (FacetRequest fr : searchParams.facetRequests) {
-        PartitionsFacetResultsHandler frHndlr = createFacetResultsHandler(fr);
-        IntermediateFacetResult tmpResult = fr2tmpRes.get(fr);
-        if (tmpResult == null) {
-          // Add empty FacetResult:
-          res.add(emptyResult(taxonomyReader.getOrdinal(fr.categoryPath), fr));
-          continue;
-        }
-        FacetResult facetRes = frHndlr.renderFacetResult(tmpResult);
-        // final labeling if allowed (because labeling is a costly operation)
-        frHndlr.labelResult(facetRes);
-        res.add(facetRes);
-      }
-
-      return res;
-    }
-  }
-
-  /** check if all requests are complementable */
-  protected boolean mayComplement() {
-    for (FacetRequest freq : searchParams.facetRequests) {
-      if (!(freq instanceof CountFacetRequest)) {
-        return false;
-      }
-    }
-    return true;
-  }
-
-  @Override
-  protected PartitionsFacetResultsHandler createFacetResultsHandler(FacetRequest fr) {
-    if (fr.getResultMode() == ResultMode.PER_NODE_IN_TREE) {
-      return new TopKInEachNodeHandler(taxonomyReader, fr, facetArrays);
-    } else {
-      return new TopKFacetResultsHandler(taxonomyReader, fr, facetArrays);
-    }
-  }
-  
-  /**
-   * Set the actual set of documents over which accumulation should take place.
-   * <p>
-   * Allows to override the set of documents to accumulate for. Invoked just
-   * before actual accumulating starts. From this point that set of documents
-   * remains unmodified. Default implementation just returns the input
-   * unchanged.
-   * 
-   * @param docids
-   *          candidate documents to accumulate for
-   * @return actual documents to accumulate for
-   */
-  protected ScoredDocIDs actualDocsToAccumulate(ScoredDocIDs docids) throws IOException {
-    return docids;
-  }
-
-  /** Check if it is worth to use complements */
-  protected boolean shouldComplement(ScoredDocIDs docids) {
-    return mayComplement() && (docids.size() > indexReader.numDocs() * getComplementThreshold()) ;
-  }
-
-  /**
-   * Iterate over the documents for this partition and fill the facet arrays with the correct
-   * count/complement count/value.
-   */
-  private final void fillArraysForPartition(ScoredDocIDs docids, FacetArrays facetArrays, int partition) 
-      throws IOException {
-    
-    if (isUsingComplements) {
-      initArraysByTotalCounts(facetArrays, partition, docids.size());
-    } else {
-      facetArrays.free(); // to get a cleared array for this partition
-    }
-
-    HashMap<CategoryListIterator, Aggregator> categoryLists = getCategoryListMap(facetArrays, partition);
-
-    IntsRef ordinals = new IntsRef(32); // a reasonable start capacity for most common apps
-    for (Entry<CategoryListIterator, Aggregator> entry : categoryLists.entrySet()) {
-      final ScoredDocIDsIterator iterator = docids.iterator();
-      final CategoryListIterator categoryListIter = entry.getKey();
-      final Aggregator aggregator = entry.getValue();
-      Iterator<AtomicReaderContext> contexts = indexReader.leaves().iterator();
-      AtomicReaderContext current = null;
-      int maxDoc = -1;
-      while (iterator.next()) {
-        int docID = iterator.getDocID();
-        if (docID >= maxDoc) {
-          boolean iteratorDone = false;
-          do { // find the segment which contains this document
-            if (!contexts.hasNext()) {
-              throw new RuntimeException("ScoredDocIDs contains documents outside this reader's segments !?");
-            }
-            current = contexts.next();
-            maxDoc = current.docBase + current.reader().maxDoc();
-            if (docID < maxDoc) { // segment has docs, check if it has categories
-              boolean validSegment = categoryListIter.setNextReader(current);
-              validSegment &= aggregator.setNextReader(current);
-              if (!validSegment) { // if categoryList or aggregtor say it's an invalid segment, skip all docs
-                while (docID < maxDoc && iterator.next()) {
-                  docID = iterator.getDocID();
-                }
-                if (docID < maxDoc) {
-                  iteratorDone = true;
-                }
-              }
-            }
-          } while (docID >= maxDoc);
-          if (iteratorDone) { // iterator finished, terminate the loop
-            break;
-          }
-        }
-        docID -= current.docBase;
-        categoryListIter.getOrdinals(docID, ordinals);
-        if (ordinals.length == 0) {
-          continue; // document does not have category ordinals
-        }
-        aggregator.aggregate(docID, iterator.getScore(), ordinals);
-      }
-    }
-  }
-
-  /** Init arrays for partition by total counts, optionally applying a factor */
-  private final void initArraysByTotalCounts(FacetArrays facetArrays, int partition, int nAccumulatedDocs) {
-    int[] intArray = facetArrays.getIntArray();
-    totalFacetCounts.fillTotalCountsForPartition(intArray, partition);
-    double totalCountsFactor = getTotalCountsFactor();
-    // fix total counts, but only if the effect of this would be meaningful. 
-    if (totalCountsFactor < 0.99999) {
-      int delta = nAccumulatedDocs + 1;
-      for (int i = 0; i < intArray.length; i++) {
-        intArray[i] *= totalCountsFactor;
-        // also translate to prevent loss of non-positive values
-        // due to complement sampling (ie if sampled docs all decremented a certain category). 
-        intArray[i] += delta; 
-      }
-    }
-  }
-
-  /**
-   * Expert: factor by which counts should be multiplied when initializing
-   * the count arrays from total counts.
-   * Default implementation for this returns 1, which is a no op.  
-   * @return a factor by which total counts should be multiplied
-   */
-  protected double getTotalCountsFactor() {
-    return 1;
-  }
-
-  /**
-   * Create an {@link Aggregator} and a {@link CategoryListIterator} for each
-   * and every {@link FacetRequest}. Generating a map, matching each
-   * categoryListIterator to its matching aggregator.
-   * <p>
-   * If two CategoryListIterators are served by the same aggregator, a single
-   * aggregator is returned for both.
-   * 
-   * <b>NOTE: </b>If a given category list iterator is needed with two different
-   * aggregators (e.g counting and association) - an exception is thrown as this
-   * functionality is not supported at this time.
-   */
-  protected HashMap<CategoryListIterator, Aggregator> getCategoryListMap(FacetArrays facetArrays,
-      int partition) throws IOException {
-    
-    HashMap<CategoryListIterator, Aggregator> categoryLists = new HashMap<CategoryListIterator, Aggregator>();
-
-    FacetIndexingParams indexingParams = searchParams.indexingParams;
-    for (FacetRequest facetRequest : searchParams.facetRequests) {
-      Aggregator categoryAggregator = facetRequest.createAggregator(isUsingComplements, facetArrays, taxonomyReader);
-
-      CategoryListIterator cli = indexingParams.getCategoryListParams(facetRequest.categoryPath).createCategoryListIterator(partition);
-      
-      // get the aggregator
-      Aggregator old = categoryLists.put(cli, categoryAggregator);
-
-      if (old != null && !old.equals(categoryAggregator)) {
-        throw new RuntimeException("Overriding existing category list with different aggregator");
-      }
-      // if the aggregator is the same we're covered
-    }
-
-    return categoryLists;
-  }
-  
-  @Override
-  public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
-    return accumulate(new MatchingDocsAsScoredDocIDs(matchingDocs));
-  }
-
-  /**
-   * Returns the complement threshold.
-   * @see #setComplementThreshold(double)
-   */
-  public double getComplementThreshold() {
-    return complementThreshold;
-  }
-
-  /**
-   * Set the complement threshold.
-   * This threshold will dictate whether the complements optimization is applied.
-   * The optimization is to count for less documents. It is useful when the same 
-   * FacetSearchParams are used for varying sets of documents. The first time 
-   * complements is used the "total counts" are computed - counting for all the 
-   * documents in the collection. Then, only the complementing set of documents
-   * is considered, and used to decrement from the overall counts, thereby 
-   * walking through less documents, which is faster.
-   * <p>
-   * For the default settings see {@link #DEFAULT_COMPLEMENT_THRESHOLD}.
-   * <p>
-   * To forcing complements in all cases pass {@link #FORCE_COMPLEMENT}.
-   * This is mostly useful for testing purposes, as forcing complements when only 
-   * tiny fraction of available documents match the query does not make sense and 
-   * would incur performance degradations.
-   * <p>
-   * To disable complements pass {@link #DISABLE_COMPLEMENT}.
-   * @param complementThreshold the complement threshold to set
-   * @see #getComplementThreshold()
-   */
-  public void setComplementThreshold(double complementThreshold) {
-    this.complementThreshold = complementThreshold;
-  }
-
-  /** Returns true if complements are enabled. */
-  public boolean isUsingComplements() {
-    return isUsingComplements;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetRequest.java
index 7a08a12..09eb68c 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetRequest.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetRequest.java
@@ -1,7 +1,7 @@
 package org.apache.lucene.facet.search;
 
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -34,19 +34,8 @@ public class SumScoreFacetRequest extends FacetRequest {
   }
 
   @Override
-  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) {
-    assert !useComplements : "complements are not supported by this FacetRequest";
-    return new ScoringAggregator(arrays.getFloatArray());
+  public FacetsAggregator createFacetsAggregator(FacetIndexingParams fip) {
+    return new SumScoreFacetsAggregator();
   }
-
-  @Override
-  public double getValueOf(FacetArrays arrays, int ordinal) {
-    return arrays.getFloatArray()[ordinal];
-  }
-
-  @Override
-  public FacetArraysSource getFacetArraysSource() {
-    return FacetArraysSource.FLOAT;
-  }
-
+  
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java
index 1663958..e89fbd3 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java
@@ -4,6 +4,7 @@ import java.io.IOException;
 
 import org.apache.lucene.facet.params.CategoryListParams;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.search.OrdinalValueResolver.FloatValueResolver;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.util.IntsRef;
 
@@ -76,5 +77,10 @@ public class SumScoreFacetsAggregator implements FacetsAggregator {
   public boolean requiresDocScores() {
     return true;
   }
+
+  @Override
+  public OrdinalValueResolver createOrdinalValueResolver(FacetRequest facetRequest, FacetArrays arrays) {
+    return new FloatValueResolver(arrays);
+  }
   
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/TaxonomyFacetsAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/TaxonomyFacetsAccumulator.java
new file mode 100644
index 0000000..a19ef68
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/TaxonomyFacetsAccumulator.java
@@ -0,0 +1,218 @@
+package org.apache.lucene.facet.search;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
+import org.apache.lucene.facet.search.FacetRequest.SortOrder;
+import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetsAccumulator} suitable for accumulating categories that were
+ * indexed into a taxonomy index.
+ * 
+ * @lucene.experimental
+ */
+public class TaxonomyFacetsAccumulator extends FacetsAccumulator {
+
+  public final TaxonomyReader taxonomyReader;
+  public final IndexReader indexReader;
+  public final FacetArrays facetArrays;
+  
+  /**
+   * Initializes the accumulator with the given search params, index reader and
+   * taxonomy reader. This constructor creates the default {@link FacetArrays},
+   * which do not support reuse. If you want to use {@link ReusingFacetArrays},
+   * you should use the
+   * {@link #TaxonomyFacetsAccumulator(FacetSearchParams, IndexReader, TaxonomyReader)}
+   * constructor.
+   */
+  public TaxonomyFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader, 
+      TaxonomyReader taxonomyReader) {
+    this(searchParams, indexReader, taxonomyReader, null);
+  }
+
+  /**
+   * Initializes the accumulator with the given parameters as well as
+   * {@link FacetArrays}. Note that the accumulator doesn't call
+   * {@link FacetArrays#free()}. If you require that (only makes sense if you
+   * use {@link ReusingFacetArrays}, you should do it after you've finished with
+   * the accumulator.
+   */
+  public TaxonomyFacetsAccumulator(FacetSearchParams searchParams, IndexReader indexReader, 
+      TaxonomyReader taxonomyReader, FacetArrays facetArrays) {
+    super(searchParams);
+    this.facetArrays = facetArrays == null ? new FacetArrays(taxonomyReader.getSize()) : facetArrays;
+    this.indexReader = indexReader;
+    this.taxonomyReader = taxonomyReader;
+  }
+
+  /** Group all requests that belong to the same {@link CategoryListParams}. */
+  protected Map<CategoryListParams,List<FacetRequest>> groupRequests() {
+    if (searchParams.indexingParams.getAllCategoryListParams().size() == 1) {
+      return Collections.singletonMap(searchParams.indexingParams.getCategoryListParams(null), searchParams.facetRequests);
+    }
+    
+    HashMap<CategoryListParams,List<FacetRequest>> requestsPerCLP = new HashMap<CategoryListParams,List<FacetRequest>>();
+    for (FacetRequest fr : searchParams.facetRequests) {
+      CategoryListParams clp = searchParams.indexingParams.getCategoryListParams(fr.categoryPath);
+      List<FacetRequest> requests = requestsPerCLP.get(clp);
+      if (requests == null) {
+        requests = new ArrayList<FacetRequest>();
+        requestsPerCLP.put(clp, requests);
+      }
+      requests.add(fr);
+    }
+    return requestsPerCLP;
+  }
+
+  /**
+   * Returns the {@link FacetsAggregator} to use for aggregating the categories
+   * found in the result documents.
+   */
+  public FacetsAggregator getAggregator() {
+    Map<CategoryListParams,List<FacetRequest>> requestsPerCLP = groupRequests();
+
+    // optimize for all-CountFacetRequest and single category list (common case)
+    if (requestsPerCLP.size() == 1) {
+      boolean allCount = true;
+      for (FacetRequest fr : searchParams.facetRequests) {
+        if (!(fr instanceof CountFacetRequest)) {
+          allCount = false;
+          break;
+        }
+      }
+      if (allCount) {
+        return requestsPerCLP.values().iterator().next().get(0).createFacetsAggregator(searchParams.indexingParams);
+      }
+    }
+    
+    // If we're here it means the facet requests are spread across multiple
+    // category lists, or there are multiple types of facet requests, or both.
+    // Therefore create a per-CategoryList mapping of FacetsAggregators.
+    Map<CategoryListParams,FacetsAggregator> perCLPAggregator = new HashMap<CategoryListParams,FacetsAggregator>();
+    for (Entry<CategoryListParams,List<FacetRequest>> e : requestsPerCLP.entrySet()) {
+      CategoryListParams clp = e.getKey();
+      List<FacetRequest> requests = e.getValue();
+      Map<Class<? extends FacetsAggregator>,FacetsAggregator> aggClasses = new HashMap<Class<? extends FacetsAggregator>,FacetsAggregator>();
+      Map<CategoryPath,FacetsAggregator> perCategoryAggregator = new HashMap<CategoryPath,FacetsAggregator>();
+      for (FacetRequest fr : requests) {
+        FacetsAggregator fa = fr.createFacetsAggregator(searchParams.indexingParams);
+        if (fa == null) {
+          throw new IllegalArgumentException("this accumulator only supports requests that create a FacetsAggregator: " + fr);
+        }
+        Class<? extends FacetsAggregator> faClass = fa.getClass();
+        if (!aggClasses.containsKey(faClass)) {
+          aggClasses.put(faClass, fa);
+        } else {
+          fa = aggClasses.get(faClass);
+        }
+        perCategoryAggregator.put(fr.categoryPath, fa);
+      }
+      
+      if (aggClasses.size() == 1) { // only one type of facet request
+        perCLPAggregator.put(clp, aggClasses.values().iterator().next());
+      } else {
+        perCLPAggregator.put(clp, new MultiFacetsAggregator(perCategoryAggregator));
+      }
+    }
+
+    return new PerCategoryListAggregator(perCLPAggregator, searchParams.indexingParams);
+  }
+  
+  /**
+   * Creates a {@link FacetResultsHandler} that matches the given
+   * {@link FacetRequest}, using the {@link OrdinalValueResolver}.
+   */
+  protected FacetResultsHandler createFacetResultsHandler(FacetRequest fr, OrdinalValueResolver resolver) {
+    if (fr.getDepth() == 1 && fr.getSortOrder() == SortOrder.DESCENDING) {
+      return new DepthOneFacetResultsHandler(taxonomyReader, fr, facetArrays, resolver);
+    }
+
+    if (fr.getResultMode() == ResultMode.PER_NODE_IN_TREE) {
+      return new TopKInEachNodeHandler(taxonomyReader, fr, resolver, facetArrays);
+    } else {
+      return new TopKFacetResultsHandler(taxonomyReader, fr, resolver, facetArrays);
+    }
+  }
+
+  /**
+   * Used by {@link FacetsCollector} to build the list of {@link FacetResult
+   * facet results} that match the {@link FacetRequest facet requests} that were
+   * given in the constructor.
+   * 
+   * @param matchingDocs
+   *          the documents that matched the query, per-segment.
+   */
+  @Override
+  public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
+    // aggregate facets per category list (usually onle one category list)
+    FacetsAggregator aggregator = getAggregator();
+    for (CategoryListParams clp : groupRequests().keySet()) {
+      for (MatchingDocs md : matchingDocs) {
+        aggregator.aggregate(md, clp, facetArrays);
+      }
+    }
+    
+    ParallelTaxonomyArrays arrays = taxonomyReader.getParallelTaxonomyArrays();
+    
+    // compute top-K
+    final int[] children = arrays.children();
+    final int[] siblings = arrays.siblings();
+    List<FacetResult> res = new ArrayList<FacetResult>();
+    for (FacetRequest fr : searchParams.facetRequests) {
+      int rootOrd = taxonomyReader.getOrdinal(fr.categoryPath);
+      if (rootOrd == TaxonomyReader.INVALID_ORDINAL) { // category does not exist
+        // Add empty FacetResult
+        res.add(emptyResult(rootOrd, fr));
+        continue;
+      }
+      CategoryListParams clp = searchParams.indexingParams.getCategoryListParams(fr.categoryPath);
+      if (fr.categoryPath.length > 0) { // someone might ask to aggregate the ROOT category
+        OrdinalPolicy ordinalPolicy = clp.getOrdinalPolicy(fr.categoryPath.components[0]);
+        if (ordinalPolicy == OrdinalPolicy.NO_PARENTS) {
+          // rollup values
+          aggregator.rollupValues(fr, rootOrd, children, siblings, facetArrays);
+        }
+      }
+      
+      FacetResultsHandler frh = createFacetResultsHandler(fr, aggregator.createOrdinalValueResolver(fr, facetArrays));
+      res.add(frh.compute());
+    }
+    return res;
+  }
+
+  @Override
+  public boolean requiresDocScores() {
+    return getAggregator().requiresDocScores();
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java
index 7b2816b..81eb1fb 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java
@@ -34,16 +34,10 @@ import org.apache.lucene.facet.util.ResultSortUtils;
  */
 public class TopKFacetResultsHandler extends PartitionsFacetResultsHandler {
   
-  /**
-   * Construct top-K results handler.
-   * 
-   * @param taxonomyReader
-   *          taxonomy reader
-   * @param facetRequest
-   *          facet request being served
-   */
-  public TopKFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, FacetArrays facetArrays) {
-    super(taxonomyReader, facetRequest, facetArrays);
+  /** Construct top-K results handler. */
+  public TopKFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, 
+      OrdinalValueResolver resolver, FacetArrays facetArrays) {
+    super(taxonomyReader, facetRequest, resolver, facetArrays);
   }
   
   // fetch top K for specific partition. 
@@ -56,7 +50,7 @@ public class TopKFacetResultsHandler extends PartitionsFacetResultsHandler {
       double value = 0;  
       if (isSelfPartition(ordinal, facetArrays, offset)) {
         int partitionSize = facetArrays.arrayLength;
-        value = facetRequest.getValueOf(facetArrays, ordinal % partitionSize);
+        value = resolver.valueOf(ordinal % partitionSize);
       }
       
       FacetResultNode parentResultNode = new FacetResultNode(ordinal, value);
@@ -158,7 +152,7 @@ public class TopKFacetResultsHandler extends PartitionsFacetResultsHandler {
       // collect it, if belongs to current partition, and then push its kids on itself, if applicable
       if (tosOrdinal >= offset) { // tosOrdinal resides in current partition
         int relativeOrdinal = tosOrdinal % partitionSize;
-        double value = facetRequest.getValueOf(facetArrays, relativeOrdinal);
+        double value = resolver.valueOf(relativeOrdinal);
         if (value != 0 && !Double.isNaN(value)) {
           // Count current ordinal -- the TOS
           if (reusable == null) {
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java
index 936edb9..9952321 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java
@@ -62,8 +62,9 @@ import org.apache.lucene.util.PriorityQueue;
  */
 public class TopKInEachNodeHandler extends PartitionsFacetResultsHandler {
 
-  public TopKInEachNodeHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, FacetArrays facetArrays) {
-    super(taxonomyReader, facetRequest, facetArrays);
+  public TopKInEachNodeHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, OrdinalValueResolver resolver, 
+      FacetArrays facetArrays) {
+    super(taxonomyReader, facetRequest, resolver, facetArrays);
   }
 
   /**
@@ -112,8 +113,8 @@ public class TopKInEachNodeHandler extends PartitionsFacetResultsHandler {
     // this will grow into the returned IntermediateFacetResult
     IntToObjectMap<AACO> AACOsOfOnePartition = new IntToObjectMap<AACO>();
 
-    int partitionSize = facetArrays.arrayLength; // all partitions, except, possibly, the last,
-    // have the same length. Hence modulo is OK.
+    // all partitions, except, possibly, the last, have the same length. Hence modulo is OK.
+    int partitionSize = facetArrays.arrayLength;
 
     int depth = facetRequest.getDepth();
 
@@ -123,7 +124,7 @@ public class TopKInEachNodeHandler extends PartitionsFacetResultsHandler {
           facetRequest, AACOsOfOnePartition);
       if (isSelfPartition(rootNode, facetArrays, offset)) {
         tempFRWH.isRootNodeIncluded = true;
-        tempFRWH.rootNodeValue = this.facetRequest.getValueOf(facetArrays, rootNode % partitionSize);
+        tempFRWH.rootNodeValue = resolver.valueOf(rootNode % partitionSize);
       }
       return tempFRWH;
     }
@@ -267,7 +268,7 @@ public class TopKInEachNodeHandler extends PartitionsFacetResultsHandler {
 
         while (tosOrdinal >= offset) { // while tosOrdinal belongs to the given partition; here, too, we use the fact
           // that TaxonomyReader.INVALID_ORDINAL == -1 < offset
-          double value = facetRequest.getValueOf(facetArrays, tosOrdinal % partitionSize);
+          double value = resolver.valueOf(tosOrdinal % partitionSize);
           if (value != 0) { // the value of yc is not 0, it is to be considered.  
             totalNumOfDescendantsConsidered++;
 
@@ -338,7 +339,7 @@ public class TopKInEachNodeHandler extends PartitionsFacetResultsHandler {
         facetRequest, AACOsOfOnePartition);
     if (isSelfPartition(rootNode, facetArrays, offset)) {
       tempFRWH.isRootNodeIncluded = true;
-      tempFRWH.rootNodeValue = this.facetRequest.getValueOf(facetArrays, rootNode % partitionSize);
+      tempFRWH.rootNodeValue = resolver.valueOf(rootNode % partitionSize);
     }
     tempFRWH.totalNumOfFacetsConsidered = totalNumOfDescendantsConsidered;
     return tempFRWH;
@@ -374,7 +375,7 @@ public class TopKInEachNodeHandler extends PartitionsFacetResultsHandler {
     int ret = 0;
     if (offset <= ordinal) {
       // ordinal belongs to the current partition
-      if (0 != facetRequest.getValueOf(facetArrays, ordinal % partitionSize)) {
+      if (0 != resolver.valueOf(ordinal % partitionSize)) {
         ret++;
       }
     }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesAccumulator.java
index 108be03..52bcec8 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesAccumulator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sortedset/SortedSetDocValuesAccumulator.java
@@ -32,19 +32,19 @@ import org.apache.lucene.facet.search.FacetRequest;
 import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetResultNode;
 import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.search.FacetsAggregator;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.search.TaxonomyFacetsAccumulator;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
 import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
 import org.apache.lucene.index.ReaderUtil;
 import org.apache.lucene.index.SortedSetDocValues;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.PriorityQueue;
 
-/** A {@link FacetsAccumulator} that uses previously
+/** A {@link TaxonomyFacetsAccumulator} that uses previously
  *  indexed {@link SortedSetDocValuesFacetFields} to perform faceting,
  *  without require a separate taxonomy index.  Faceting is
  *  a bit slower (~25%), and there is added cost on every
@@ -57,25 +57,34 @@ public class SortedSetDocValuesAccumulator extends FacetsAccumulator {
   final SortedSetDocValuesReaderState state;
   final SortedSetDocValues dv;
   final String field;
-
-  public SortedSetDocValuesAccumulator(FacetSearchParams fsp, SortedSetDocValuesReaderState state) throws IOException {
-    super(fsp, null, null, new FacetArrays(state.getSize()));
+  final FacetArrays facetArrays;
+  
+  /** Constructor with the given facet search params. */
+  public SortedSetDocValuesAccumulator(SortedSetDocValuesReaderState state, FacetSearchParams fsp) 
+      throws IOException {
+    this(state, fsp, null);
+  }
+  
+  public SortedSetDocValuesAccumulator(SortedSetDocValuesReaderState state, FacetSearchParams fsp, FacetArrays arrays) 
+      throws IOException {
+    super(fsp);
     this.state = state;
     this.field = state.getField();
+    this.facetArrays = arrays == null ? new FacetArrays(state.getSize()) : arrays;
     dv = state.getDocValues();
 
     // Check params:
-    for(FacetRequest request : fsp.facetRequests) {
-      if (!(request instanceof CountFacetRequest)) {
-        throw new IllegalArgumentException("this collector only supports CountFacetRequest; got " + request);
+    for (FacetRequest fr : fsp.facetRequests) {
+      if (!(fr instanceof CountFacetRequest)) {
+        throw new IllegalArgumentException("this accumulator only supports CountFacetRequest; got " + fr);
       }
-      if (request.categoryPath.length != 1) {
-        throw new IllegalArgumentException("this collector only supports depth 1 CategoryPath; got " + request.categoryPath);
+      if (fr.categoryPath.length != 1) {
+        throw new IllegalArgumentException("this accumulator only supports 1-level CategoryPath; got " + fr.categoryPath);
       }
-      if (request.getDepth() != 1) {
-        throw new IllegalArgumentException("this collector only supports depth=1; got " + request.getDepth());
+      if (fr.getDepth() != 1) {
+        throw new IllegalArgumentException("this accumulator only supports depth=1; got " + fr.getDepth());
       }
-      String dim = request.categoryPath.components[0];
+      String dim = fr.categoryPath.components[0];
 
       SortedSetDocValuesReaderState.OrdRange ordRange = state.getOrdRange(dim);
       if (ordRange == null) {
@@ -84,131 +93,123 @@ public class SortedSetDocValuesAccumulator extends FacetsAccumulator {
     }
   }
 
-  @Override
-  public FacetsAggregator getAggregator() {
+  /** Keeps highest count results. */
+  static class TopCountPQ extends PriorityQueue<FacetResultNode> {
+    public TopCountPQ(int topN) {
+      super(topN, false);
+    }
 
-    return new FacetsAggregator() {
+    @Override
+    protected boolean lessThan(FacetResultNode a, FacetResultNode b) {
+      if (a.value < b.value) {
+        return true;
+      } else if (a.value > b.value) {
+        return false;
+      } else {
+        return a.ordinal > b.ordinal;
+      }
+    }
+  }
 
-      @Override
-      public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
+  static class SortedSetAggregator {
 
-        AtomicReader reader = matchingDocs.context.reader();
+    private final SortedSetDocValuesReaderState state;
+    private final String field;
+    private final SortedSetDocValues dv;
+    
+    public SortedSetAggregator(String field, SortedSetDocValuesReaderState state, SortedSetDocValues dv) {
+      this.field = field;
+      this.state = state;
+      this.dv = dv;
+    }
+    
+    public void aggregate(MatchingDocs matchingDocs, FacetArrays facetArrays) throws IOException {
 
-        // LUCENE-5090: make sure the provided reader context "matches"
-        // the top-level reader passed to the
-        // SortedSetDocValuesReaderState, else cryptic
-        // AIOOBE can happen:
-        if (ReaderUtil.getTopLevelContext(matchingDocs.context).reader() != state.origReader) {
-          throw new IllegalStateException("the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader");
-        }
-        
-        SortedSetDocValues segValues = reader.getSortedSetDocValues(field);
-        if (segValues == null) {
-          return;
-        }
+      AtomicReader reader = matchingDocs.context.reader();
 
-        final int[] counts = facetArrays.getIntArray();
-        final int maxDoc = reader.maxDoc();
-        assert maxDoc == matchingDocs.bits.length();
-
-        if (dv instanceof MultiSortedSetDocValues) {
-          MultiDocValues.OrdinalMap ordinalMap = ((MultiSortedSetDocValues) dv).mapping;
-          int segOrd = matchingDocs.context.ord;
-
-          int numSegOrds = (int) segValues.getValueCount();
-
-          if (matchingDocs.totalHits < numSegOrds/10) {
-            // Remap every ord to global ord as we iterate:
-            int doc = 0;
-            while (doc < maxDoc && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-              segValues.setDocument(doc);
-              int term = (int) segValues.nextOrd();
-              while (term != SortedSetDocValues.NO_MORE_ORDS) {
-                counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;
-                term = (int) segValues.nextOrd();
-              }
-              ++doc;
-            }
-          } else {
-
-            // First count in seg-ord space:
-            final int[] segCounts = new int[numSegOrds];
-            int doc = 0;
-            while (doc < maxDoc && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
-              segValues.setDocument(doc);
-              int term = (int) segValues.nextOrd();
-              while (term != SortedSetDocValues.NO_MORE_ORDS) {
-                segCounts[term]++;
-                term = (int) segValues.nextOrd();
-              }
-              ++doc;
-            }
+      // LUCENE-5090: make sure the provided reader context "matches"
+      // the top-level reader passed to the
+      // SortedSetDocValuesReaderState, else cryptic
+      // AIOOBE can happen:
+      if (ReaderUtil.getTopLevelContext(matchingDocs.context).reader() != state.origReader) {
+        throw new IllegalStateException("the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader");
+      }
+      
+      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);
+      if (segValues == null) {
+        return;
+      }
 
-            // Then, migrate to global ords:
-            for(int ord=0;ord<numSegOrds;ord++) {
-              int count = segCounts[ord];
-              if (count != 0) {
-                counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;
-              }
+      final int[] counts = facetArrays.getIntArray();
+      final int maxDoc = reader.maxDoc();
+      assert maxDoc == matchingDocs.bits.length();
+
+      if (dv instanceof MultiSortedSetDocValues) {
+        MultiDocValues.OrdinalMap ordinalMap = ((MultiSortedSetDocValues) dv).mapping;
+        int segOrd = matchingDocs.context.ord;
+
+        int numSegOrds = (int) segValues.getValueCount();
+
+        if (matchingDocs.totalHits < numSegOrds/10) {
+          // Remap every ord to global ord as we iterate:
+          int doc = 0;
+          while (doc < maxDoc && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
+            segValues.setDocument(doc);
+            int term = (int) segValues.nextOrd();
+            while (term != SortedSetDocValues.NO_MORE_ORDS) {
+              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;
+              term = (int) segValues.nextOrd();
             }
+            ++doc;
           }
         } else {
-          // No ord mapping (e.g., single segment index):
-          // just aggregate directly into counts:
 
+          // First count in seg-ord space:
+          final int[] segCounts = new int[numSegOrds];
           int doc = 0;
           while (doc < maxDoc && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
             segValues.setDocument(doc);
             int term = (int) segValues.nextOrd();
             while (term != SortedSetDocValues.NO_MORE_ORDS) {
-              counts[term]++;
+              segCounts[term]++;
               term = (int) segValues.nextOrd();
             }
             ++doc;
           }
-        }
-      }
-
-      @Override
-      public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
-        // Nothing to do here: we only support flat (dim +
-        // label) facets, and in accumulate we sum up the
-        // count for the dimension.
-      }
 
-      @Override
-      public boolean requiresDocScores() {
-        return false;
-      }
-    };
-  }
-
-  /** Keeps highest count results. */
-  static class TopCountPQ extends PriorityQueue<FacetResultNode> {
-    public TopCountPQ(int topN) {
-      super(topN, false);
-    }
-
-    @Override
-    protected boolean lessThan(FacetResultNode a, FacetResultNode b) {
-      if (a.value < b.value) {
-        return true;
-      } else if (a.value > b.value) {
-        return false;
+          // Then, migrate to global ords:
+          for(int ord=0;ord<numSegOrds;ord++) {
+            int count = segCounts[ord];
+            if (count != 0) {
+              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;
+            }
+          }
+        }
       } else {
-        return a.ordinal > b.ordinal;
+        // No ord mapping (e.g., single segment index):
+        // just aggregate directly into counts:
+
+        int doc = 0;
+        while (doc < maxDoc && (doc = matchingDocs.bits.nextSetBit(doc)) != -1) {
+          segValues.setDocument(doc);
+          int term = (int) segValues.nextOrd();
+          while (term != SortedSetDocValues.NO_MORE_ORDS) {
+            counts[term]++;
+            term = (int) segValues.nextOrd();
+          }
+          ++doc;
+        }
       }
     }
-  }
 
+  }
+  
   @Override
   public List<FacetResult> accumulate(List<MatchingDocs> matchingDocs) throws IOException {
 
-    FacetsAggregator aggregator = getAggregator();
-    for (CategoryListParams clp : getCategoryLists()) {
-      for (MatchingDocs md : matchingDocs) {
-        aggregator.aggregate(md, clp, facetArrays);
-      }
+    SortedSetAggregator aggregator = new SortedSetAggregator(field, state, dv);
+    for (MatchingDocs md : matchingDocs) {
+      aggregator.aggregate(md, facetArrays);
     }
 
     // compute top-K
@@ -218,7 +219,7 @@ public class SortedSetDocValuesAccumulator extends FacetsAccumulator {
 
     BytesRef scratch = new BytesRef();
 
-    for(FacetRequest request : searchParams.facetRequests) {
+    for (FacetRequest request : searchParams.facetRequests) {
       String dim = request.categoryPath.components[0];
       SortedSetDocValuesReaderState.OrdRange ordRange = state.getOrdRange(dim);
       // checked in ctor:
@@ -315,4 +316,10 @@ public class SortedSetDocValuesAccumulator extends FacetsAccumulator {
 
     return results;
   }
+  
+  @Override
+  public boolean requiresDocScores() {
+    return false;
+  }
+  
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
index f56ade3..1e106cf 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
@@ -428,7 +428,7 @@ public class DirectoryTaxonomyWriter implements TaxonomyWriter {
         Terms terms = ctx.reader().terms(Consts.FULL);
         if (terms != null) {
           termsEnum = terms.iterator(termsEnum);
-          if (termsEnum.seekExact(catTerm, true)) {
+          if (termsEnum.seekExact(catTerm)) {
             // liveDocs=null because the taxonomy has no deletes
             docs = termsEnum.docs(null, docs, 0 /* freqs not required */);
             // if the term was found, we know it has exactly one document.
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/package.html b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/package.html
index 36f13e8..8713c67 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/package.html
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/package.html
@@ -21,28 +21,33 @@
 <body>
 	<h1>Taxonomy of Categories</h1>
 	
-	Facets are defined using a hierarchy of categories, known as a
-	<i>Taxonomy</i>.
-	
-	<br>
-	For example, in a book store application, a Taxonomy could have the
-	following hierarchy:
-	<p>
+	Facets are defined using a hierarchy of categories, known as a <i>Taxonomy</i>.
+	For example, the taxonomy of a book store application might have the following structure:
 	<ul>
-		<li>Author</li>
-		<ul>
-		<li>Mark Twain</li>
-		<li>J. K. Rowling</li>
-		</ul>
+		<li>Author
+			<ul>
+			<li>Mark Twain</li>
+			<li>J. K. Rowling</li>
+			</ul>
+		</li>
 	</ul>
 	<ul>
-		<li>Date</li>
-		<ul>
-		<li>2010</li>
-		<li>2009</li>
-		</ul>
+		<li>Date
+			<ul>
+			<li>2010
+				<ul>
+				<li>March</li>
+				<li>April</li>
+				</ul>
+			</li>
+			<li>2009</li>
+			</ul>
+		</li>
 	</ul>
 	
-	The <i>Taxonomy</i> translates category-paths into category-ordinal and vice versa.
+	The <i>Taxonomy</i> translates category-paths into interger identifiers (often termed <i>ordinals</i>) and vice versa.
+	The category <code>Author/Mark Twain</code> adds two nodes to the taxonomy: <code>Author</code> and 
+	<code>Author/Mark Twain</code>, each is assigned a different ordinal. The taxonomy maintains the invariant that a 
+	node always has an ordinal that is &lt; all its children.
 </body>
 </html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/util/FacetsPayloadMigrationReader.java b/lucene/facet/src/java/org/apache/lucene/facet/util/FacetsPayloadMigrationReader.java
index 4ce34b1..ec4d2c9 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/util/FacetsPayloadMigrationReader.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/util/FacetsPayloadMigrationReader.java
@@ -90,7 +90,7 @@ public class FacetsPayloadMigrationReader extends FilterAtomicReader {
           Terms terms = fields.terms(term.field());
           if (terms != null) {
             TermsEnum te = terms.iterator(null); // no use for reusing
-            if (te.seekExact(term.bytes(), true)) {
+            if (te.seekExact(term.bytes())) {
               // we're not expected to be called for deleted documents
               dpe = te.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_PAYLOADS);
             }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/util/ScoredDocIdsUtils.java b/lucene/facet/src/java/org/apache/lucene/facet/util/ScoredDocIdsUtils.java
deleted file mode 100644
index ad780ad..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/util/ScoredDocIdsUtils.java
+++ /dev/null
@@ -1,446 +0,0 @@
-package org.apache.lucene.facet.util;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.ScoredDocIDsIterator;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.OpenBitSetDISI;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Utility methods for Scored Doc IDs.
- * 
- * @lucene.experimental
- */
-public class ScoredDocIdsUtils {
-
-  /**
-   * Create a complement of the input set. The returned {@link ScoredDocIDs}
-   * does not contain any scores, which makes sense given that the complementing
-   * documents were not scored.
-   * 
-   * Note: the complement set does NOT contain doc ids which are noted as deleted by the given reader
-   * 
-   * @param docids to be complemented.
-   * @param reader holding the number of documents & information about deletions.
-   */
-  public final static ScoredDocIDs getComplementSet(final ScoredDocIDs docids, final IndexReader reader)
-      throws IOException {
-    final int maxDoc = reader.maxDoc();
-
-    DocIdSet docIdSet = docids.getDocIDs();
-    final FixedBitSet complement;
-    if (docIdSet instanceof FixedBitSet) {
-      // That is the most common case, if ScoredDocIdsCollector was used.
-      complement = ((FixedBitSet) docIdSet).clone();
-    } else {
-      complement = new FixedBitSet(maxDoc);
-      DocIdSetIterator iter = docIdSet.iterator();
-      int doc;
-      while ((doc = iter.nextDoc()) < maxDoc) {
-        complement.set(doc);
-      }
-    }
-    complement.flip(0, maxDoc);
-    clearDeleted(reader, complement);
-
-    return createScoredDocIds(complement, maxDoc);
-  }
-  
-  /** Clear all deleted documents from a given open-bit-set according to a given reader */
-  private static void clearDeleted(final IndexReader reader, final FixedBitSet set) throws IOException {
-    // TODO use BitsFilteredDocIdSet?
-    
-    // If there are no deleted docs
-    if (!reader.hasDeletions()) {
-      return; // return immediately
-    }
-    
-    DocIdSetIterator it = set.iterator();
-    int doc = it.nextDoc(); 
-    for (AtomicReaderContext context : reader.leaves()) {
-      AtomicReader r = context.reader();
-      final int maxDoc = r.maxDoc() + context.docBase;
-      if (doc >= maxDoc) { // skip this segment
-        continue;
-      }
-      if (!r.hasDeletions()) { // skip all docs that belong to this reader as it has no deletions
-        while ((doc = it.nextDoc()) < maxDoc) {}
-        continue;
-      }
-      Bits liveDocs = r.getLiveDocs();
-      do {
-        if (!liveDocs.get(doc - context.docBase)) {
-          set.clear(doc);
-        }
-      } while ((doc = it.nextDoc()) < maxDoc);
-    }
-  }
-  
-  /**
-   * Create a subset of an existing ScoredDocIDs object.
-   * 
-   * @param allDocIds orginal set
-   * @param sampleSet Doc Ids of the subset.
-   */
-  public static final ScoredDocIDs createScoredDocIDsSubset(final ScoredDocIDs allDocIds,
-      final int[] sampleSet) throws IOException {
-
-    // sort so that we can scan docs in order
-    final int[] docids = sampleSet;
-    Arrays.sort(docids);
-    final float[] scores = new float[docids.length];
-    // fetch scores and compute size
-    ScoredDocIDsIterator it = allDocIds.iterator();
-    int n = 0;
-    while (it.next() && n < docids.length) {
-      int doc = it.getDocID();
-      if (doc == docids[n]) {
-        scores[n] = it.getScore();
-        ++n;
-      }
-    }
-    final int size = n;
-
-    return new ScoredDocIDs() {
-
-      @Override
-      public DocIdSet getDocIDs() {
-        return new DocIdSet() {
-
-          @Override
-          public boolean isCacheable() { return true; }
-
-          @Override
-          public DocIdSetIterator iterator() {
-            return new DocIdSetIterator() {
-
-              private int next = -1;
-
-              @Override
-              public int advance(int target) {
-                while (next < size && docids[next++] < target) {
-                }
-                return next == size ? NO_MORE_DOCS : docids[next];
-              }
-
-              @Override
-              public int docID() {
-                return docids[next];
-              }
-
-              @Override
-              public int nextDoc() {
-                if (++next >= size) {
-                  return NO_MORE_DOCS;
-                }
-                return docids[next];
-              }
-
-              @Override
-              public long cost() {
-                return size;
-              }
-            };
-          }
-        };
-      }
-
-      @Override
-      public ScoredDocIDsIterator iterator() {
-        return new ScoredDocIDsIterator() {
-
-          int next = -1;
-
-          @Override
-          public boolean next() { return ++next < size; }
-
-          @Override
-          public float getScore() { return scores[next]; }
-
-          @Override
-          public int getDocID() { return docids[next]; }
-        };
-      }
-
-      @Override
-      public int size() { return size; }
-
-    };
-  }
-
-  /**
-   * Creates a {@link ScoredDocIDs} which returns document IDs all non-deleted doc ids 
-   * according to the given reader. 
-   * The returned set contains the range of [0 .. reader.maxDoc ) doc ids
-   */
-  public static final ScoredDocIDs createAllDocsScoredDocIDs (final IndexReader reader) {
-    if (reader.hasDeletions()) {
-      return new AllLiveDocsScoredDocIDs(reader);
-    }
-    return new AllDocsScoredDocIDs(reader);
-  }
-
-  /**
-   * Create a ScoredDocIDs out of a given docIdSet and the total number of documents in an index  
-   */
-  public static final ScoredDocIDs createScoredDocIds(final DocIdSet docIdSet, final int maxDoc) {
-    return new ScoredDocIDs() {
-      private int size = -1;
-      @Override
-      public DocIdSet getDocIDs() { return docIdSet; }
-
-      @Override
-      public ScoredDocIDsIterator iterator() throws IOException {
-        final DocIdSetIterator docIterator = docIdSet.iterator();
-        return new ScoredDocIDsIterator() {
-          @Override
-          public boolean next() {
-            try {
-              return docIterator.nextDoc() != DocIdSetIterator.NO_MORE_DOCS;
-            } catch (IOException e) {
-              throw new RuntimeException(e);
-            }
-          }
-
-          @Override
-          public float getScore() { return DEFAULT_SCORE; }
-
-          @Override
-          public int getDocID() { return docIterator.docID(); }
-        };
-      }
-
-      @Override
-      public int size() {
-        // lazy size computation
-        if (size < 0) {
-          OpenBitSetDISI openBitSetDISI;
-          try {
-            openBitSetDISI = new OpenBitSetDISI(docIdSet.iterator(), maxDoc);
-          } catch (IOException e) {
-            throw new RuntimeException(e);
-          }
-          size = (int) openBitSetDISI.cardinality();
-        }
-        return size;
-      }
-    };
-  }
-
-  /**
-   * All docs ScoredDocsIDs - this one is simply an 'all 1' bitset. Used when
-   * there are no deletions in the index and we wish to go through each and
-   * every document
-   */
-  private static class AllDocsScoredDocIDs implements ScoredDocIDs {
-    final int maxDoc;
-
-    public AllDocsScoredDocIDs(IndexReader reader) {
-      this.maxDoc = reader.maxDoc();
-    }
-
-    @Override
-    public int size() {  
-      return maxDoc;
-    }
-
-    @Override
-    public DocIdSet getDocIDs() {
-      return new DocIdSet() {
-
-        @Override
-        public boolean isCacheable() {
-          return true;
-        }
-
-        @Override
-        public DocIdSetIterator iterator() {
-          return new DocIdSetIterator() {
-            private int next = -1;
-
-            @Override
-            public int advance(int target) {
-              if (target <= next) {
-                target = next + 1;
-              }
-              return next = target >= maxDoc ? NO_MORE_DOCS : target;
-            }
-
-            @Override
-            public int docID() {
-              return next;
-            }
-
-            @Override
-            public int nextDoc() {
-              return ++next < maxDoc ? next : NO_MORE_DOCS;
-            }
-
-            @Override
-            public long cost() {
-              return maxDoc;
-            }
-          };
-        }
-      };
-    }
-
-    @Override
-    public ScoredDocIDsIterator iterator() {
-      try {
-        final DocIdSetIterator iter = getDocIDs().iterator();
-        return new ScoredDocIDsIterator() {
-          @Override
-          public boolean next() {
-            try {
-              return iter.nextDoc() != DocIdSetIterator.NO_MORE_DOCS;
-            } catch (IOException e) {
-              // cannot happen
-              return false;
-            }
-          }
-
-          @Override
-          public float getScore() {
-            return DEFAULT_SCORE;
-          }
-
-          @Override
-          public int getDocID() {
-            return iter.docID();
-          }
-        };
-      } catch (IOException e) {
-        // cannot happen
-        throw new RuntimeException(e);
-      }
-    }
-  }
-
-  /**
-   * An All-docs bitset which has '0' for deleted documents and '1' for the
-   * rest. Useful for iterating over all 'live' documents in a given index.
-   * <p>
-   * NOTE: this class would work for indexes with no deletions at all,
-   * although it is recommended to use {@link AllDocsScoredDocIDs} to ease
-   * the performance cost of validating isDeleted() on each and every docId
-   */
-  private static final class AllLiveDocsScoredDocIDs implements ScoredDocIDs {
-    final int maxDoc;
-    final IndexReader reader;
-
-    AllLiveDocsScoredDocIDs(IndexReader reader) {
-      this.maxDoc = reader.maxDoc();
-      this.reader = reader;
-    }
-
-    @Override
-    public int size() {
-      return reader.numDocs();
-    }
-
-    @Override
-    public DocIdSet getDocIDs() {
-      return new DocIdSet() {
-
-        @Override
-        public boolean isCacheable() {
-          return true;
-        }
-
-        @Override
-        public DocIdSetIterator iterator() {
-          return new DocIdSetIterator() {
-            final Bits liveDocs = MultiFields.getLiveDocs(reader);
-            private int next = -1;
-
-            @Override
-            public int advance(int target) {
-              if (target > next) {
-                next = target - 1;
-              }
-              return nextDoc();
-            }
-
-            @Override
-            public int docID() {
-              return next;
-            }
-
-            @Override
-            public int nextDoc() {
-              do {
-                ++next;
-              } while (next < maxDoc && liveDocs != null && !liveDocs.get(next));
-
-              return next < maxDoc ? next : NO_MORE_DOCS;
-            }
-
-            @Override
-            public long cost() {
-              return maxDoc;
-            }
-          };
-        }
-      };
-    }
-
-    @Override
-    public ScoredDocIDsIterator iterator() {
-      try {
-        final DocIdSetIterator iter = getDocIDs().iterator();
-        return new ScoredDocIDsIterator() {
-          @Override
-          public boolean next() {
-            try {
-              return iter.nextDoc() != DocIdSetIterator.NO_MORE_DOCS;
-            } catch (IOException e) {
-              // cannot happen
-              return false;
-            }
-          }
-
-          @Override
-          public float getScore() {
-            return DEFAULT_SCORE;
-          }
-
-          @Override
-          public int getDocID() {
-            return iter.docID();
-          }
-        };
-      } catch (IOException e) {
-        // cannot happen
-        throw new RuntimeException(e);
-      }
-    }
-  }
-  
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java b/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java
index 3a6c75b..2763171 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java
@@ -45,7 +45,7 @@ public class TaxonomyMergeUtils {
     // merge the taxonomies
     destTaxWriter.addTaxonomy(srcTaxDir, map);
     int ordinalMap[] = map.getMap();
-    DirectoryReader reader = DirectoryReader.open(srcIndexDir, -1);
+    DirectoryReader reader = DirectoryReader.open(srcIndexDir);
     List<AtomicReaderContext> leaves = reader.leaves();
     int numReaders = leaves.size();
     AtomicReader wrappedLeaves[] = new AtomicReader[numReaders];
diff --git a/lucene/facet/src/java/overview.html b/lucene/facet/src/java/overview.html
index 93ce4b6..0d7378f 100644
--- a/lucene/facet/src/java/overview.html
+++ b/lucene/facet/src/java/overview.html
@@ -15,12 +15,10 @@
  limitations under the License.
 -->
 <html>
-  <head>
-    <title>
-      facet
-    </title>
-  </head>
-  <body>
-  Provides faceted indexing and search capabilities (checkout the <a href="org/apache/lucene/facet/doc-files/userguide.html">userguide</a>).
-  </body>
+<head><title>facet</title></head>
+<body>
+Provides faceted indexing and search capabilities. Checkout <a href="http://shaierera.blogspot.com/2012/11/lucene-facets-part-1.html">this</a>
+and <a href="http://shaierera.blogspot.com/2012/11/lucene-facets-part-2.html">this</a> blog posts for some overview on the facets module
+as well as source code examples <a href="../demo">here</a>.
+</body>
 </html>
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/associations/AssociationsFacetRequestTest.java b/lucene/facet/src/test/org/apache/lucene/facet/associations/AssociationsFacetRequestTest.java
index 6d8f608..29b2764 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/associations/AssociationsFacetRequestTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/associations/AssociationsFacetRequestTest.java
@@ -1,8 +1,6 @@
 package org.apache.lucene.facet.associations;
 
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
@@ -10,9 +8,9 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsAccumulator;
 import org.apache.lucene.facet.search.FacetsAggregator;
 import org.apache.lucene.facet.search.FacetsCollector;
+import org.apache.lucene.facet.search.TaxonomyFacetsAccumulator;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
@@ -69,14 +67,18 @@ public class AssociationsFacetRequestTest extends FacetTestCase {
     AssociationsFacetFields assocFacetFields = new AssociationsFacetFields(taxoWriter);
     
     // index documents, 50% have only 'b' and all have 'a'
-    for (int i = 0; i < 100; i++) {
+    for (int i = 0; i < 110; i++) {
       Document doc = new Document();
       CategoryAssociationsContainer associations = new CategoryAssociationsContainer();
-      associations.setAssociation(aint, new CategoryIntAssociation(2));
-      associations.setAssociation(afloat, new CategoryFloatAssociation(0.5f));
-      if (i % 2 == 0) { // 50
-        associations.setAssociation(bint, new CategoryIntAssociation(3));
-        associations.setAssociation(bfloat, new CategoryFloatAssociation(0.2f));
+      // every 11th document is added empty, this used to cause the association
+      // aggregators to go into an infinite loop
+      if (i % 11 != 0) {
+        associations.setAssociation(aint, new CategoryIntAssociation(2));
+        associations.setAssociation(afloat, new CategoryFloatAssociation(0.5f));
+        if (i % 2 == 0) { // 50
+          associations.setAssociation(bint, new CategoryIntAssociation(3));
+          associations.setAssociation(bfloat, new CategoryFloatAssociation(0.2f));
+        }
       }
       assocFacetFields.addFields(doc, associations);
       writer.addDocument(doc);
@@ -103,12 +105,12 @@ public class AssociationsFacetRequestTest extends FacetTestCase {
     
     // facet requests for two facets
     FacetSearchParams fsp = new FacetSearchParams(
-        new AssociationIntSumFacetRequest(aint, 10),
-        new AssociationIntSumFacetRequest(bint, 10));
+        new SumIntAssociationFacetRequest(aint, 10),
+        new SumIntAssociationFacetRequest(bint, 10));
     
     Query q = new MatchAllDocsQuery();
     
-    FacetsAccumulator fa = new FacetsAccumulator(fsp, reader, taxo) {
+    TaxonomyFacetsAccumulator fa = new TaxonomyFacetsAccumulator(fsp, reader, taxo) {
       @Override
       public FacetsAggregator getAggregator() {
         return new SumIntAssociationFacetsAggregator();
@@ -135,12 +137,12 @@ public class AssociationsFacetRequestTest extends FacetTestCase {
     
     // facet requests for two facets
     FacetSearchParams fsp = new FacetSearchParams(
-        new AssociationFloatSumFacetRequest(afloat, 10),
-        new AssociationFloatSumFacetRequest(bfloat, 10));
+        new SumFloatAssociationFacetRequest(afloat, 10),
+        new SumFloatAssociationFacetRequest(bfloat, 10));
     
     Query q = new MatchAllDocsQuery();
     
-    FacetsAccumulator fa = new FacetsAccumulator(fsp, reader, taxo) {
+    TaxonomyFacetsAccumulator fa = new TaxonomyFacetsAccumulator(fsp, reader, taxo) {
       @Override
       public FacetsAggregator getAggregator() {
         return new SumFloatAssociationFacetsAggregator();
@@ -167,27 +169,14 @@ public class AssociationsFacetRequestTest extends FacetTestCase {
     
     // facet requests for two facets
     FacetSearchParams fsp = new FacetSearchParams(
-        new AssociationIntSumFacetRequest(aint, 10),
-        new AssociationIntSumFacetRequest(bint, 10),
-        new AssociationFloatSumFacetRequest(afloat, 10),
-        new AssociationFloatSumFacetRequest(bfloat, 10));
+        new SumIntAssociationFacetRequest(aint, 10),
+        new SumIntAssociationFacetRequest(bint, 10),
+        new SumFloatAssociationFacetRequest(afloat, 10),
+        new SumFloatAssociationFacetRequest(bfloat, 10));
     
     Query q = new MatchAllDocsQuery();
     
-    final SumIntAssociationFacetsAggregator sumInt = new SumIntAssociationFacetsAggregator();
-    final SumFloatAssociationFacetsAggregator sumFloat = new SumFloatAssociationFacetsAggregator();
-    final Map<CategoryPath,FacetsAggregator> aggregators = new HashMap<CategoryPath,FacetsAggregator>();
-    aggregators.put(aint, sumInt);
-    aggregators.put(bint, sumInt);
-    aggregators.put(afloat, sumFloat);
-    aggregators.put(bfloat, sumFloat);
-    FacetsAccumulator fa = new FacetsAccumulator(fsp, reader, taxo) {
-      @Override
-      public FacetsAggregator getAggregator() {
-        return new MultiAssociationsFacetsAggregator(aggregators);
-      }
-    };
-    FacetsCollector fc = FacetsCollector.create(fa);
+    FacetsCollector fc = FacetsCollector.create(fsp, reader, taxo);
     
     IndexSearcher searcher = newSearcher(reader);
     searcher.search(q, fc);
@@ -200,6 +189,6 @@ public class AssociationsFacetRequestTest extends FacetTestCase {
     assertEquals("Wrong count for category 'b'!",10f, (float) res.get(3).getFacetResultNode().value, 0.00001);
     
     taxo.close();
-  }  
+  }
   
 }
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/complements/TestFacetsAccumulatorWithComplement.java b/lucene/facet/src/test/org/apache/lucene/facet/complements/TestFacetsAccumulatorWithComplement.java
index f3de1ca..a821f3a 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/complements/TestFacetsAccumulatorWithComplement.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/complements/TestFacetsAccumulatorWithComplement.java
@@ -4,13 +4,13 @@ import java.io.IOException;
 import java.util.List;
 
 import org.apache.lucene.facet.FacetTestBase;
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
 import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.CountFacetRequest;
 import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetResultNode;
 import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.MultiReader;
@@ -112,8 +112,8 @@ public class TestFacetsAccumulatorWithComplement extends FacetTestBase {
   /** compute facets with certain facet requests and docs */
   private List<FacetResult> findFacets(boolean withComplement) throws IOException {
     FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(new CategoryPath("root","a"), 10));
-    StandardFacetsAccumulator sfa = new StandardFacetsAccumulator(fsp, indexReader, taxoReader);
-    sfa.setComplementThreshold(withComplement ? StandardFacetsAccumulator.FORCE_COMPLEMENT : StandardFacetsAccumulator.DISABLE_COMPLEMENT);
+    OldFacetsAccumulator sfa = new OldFacetsAccumulator(fsp, indexReader, taxoReader);
+    sfa.setComplementThreshold(withComplement ? OldFacetsAccumulator.FORCE_COMPLEMENT : OldFacetsAccumulator.DISABLE_COMPLEMENT);
     FacetsCollector fc = FacetsCollector.create(sfa);
     searcher.search(new MatchAllDocsQuery(), fc);
     
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/old/AdaptiveAccumulatorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/old/AdaptiveAccumulatorTest.java
new file mode 100644
index 0000000..299191b
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/old/AdaptiveAccumulatorTest.java
@@ -0,0 +1,38 @@
+package org.apache.lucene.facet.old;
+
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.sampling.BaseSampleTestTopK;
+import org.apache.lucene.facet.sampling.Sampler;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.util.LuceneTestCase.Slow;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+@Slow
+public class AdaptiveAccumulatorTest extends BaseSampleTestTopK {
+
+  @Override
+  protected OldFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
+      IndexReader indexReader, FacetSearchParams searchParams) {
+    AdaptiveFacetsAccumulator res = new AdaptiveFacetsAccumulator(searchParams, indexReader, taxoReader);
+    res.setSampler(sampler);
+    return res;
+  }
+  
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/old/TestScoredDocIDsUtils.java b/lucene/facet/src/test/org/apache/lucene/facet/old/TestScoredDocIDsUtils.java
new file mode 100644
index 0000000..c86b092
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/old/TestScoredDocIDsUtils.java
@@ -0,0 +1,154 @@
+package org.apache.lucene.facet.old;
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.FixedBitSet;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestScoredDocIDsUtils extends FacetTestCase {
+
+  @Test
+  public void testComplementIterator() throws Exception {
+    final int n = atLeast(10000);
+    final FixedBitSet bits = new FixedBitSet(n);
+    Random random = random();
+    for (int i = 0; i < n; i++) {
+      int idx = random.nextInt(n);
+      bits.flip(idx, idx + 1);
+    }
+    
+    FixedBitSet verify = new FixedBitSet(bits);
+
+    ScoredDocIDs scoredDocIDs = ScoredDocIdsUtils.createScoredDocIds(bits, n); 
+
+    Directory dir = newDirectory();
+    IndexReader reader = createReaderWithNDocs(random, n, dir);
+    try { 
+      assertEquals(n - verify.cardinality(), ScoredDocIdsUtils.getComplementSet(scoredDocIDs, reader).size());
+    } finally {
+      reader.close();
+      dir.close();
+    }
+  }
+
+  @Test
+  public void testAllDocs() throws Exception {
+    int maxDoc = 3;
+    Directory dir = newDirectory();
+    IndexReader reader = createReaderWithNDocs(random(), maxDoc, dir);
+    try {
+      ScoredDocIDs all = ScoredDocIdsUtils.createAllDocsScoredDocIDs(reader);
+      assertEquals("invalid size", maxDoc, all.size());
+      ScoredDocIDsIterator iter = all.iterator();
+      int doc = 0;
+      while (iter.next()) {
+        assertEquals("invalid doc ID: " + iter.getDocID(), doc++, iter.getDocID());
+        assertEquals("invalid score: " + iter.getScore(), ScoredDocIDsIterator.DEFAULT_SCORE, iter.getScore(), 0.0f);
+      }
+      assertEquals("invalid maxDoc: " + doc, maxDoc, doc);
+      
+      DocIdSet docIDs = all.getDocIDs();
+      assertTrue("should be cacheable", docIDs.isCacheable());
+      DocIdSetIterator docIDsIter = docIDs.iterator();
+      assertEquals("nextDoc() hasn't been called yet", -1, docIDsIter.docID());
+      assertEquals(0, docIDsIter.nextDoc());
+      assertEquals(1, docIDsIter.advance(1));
+      // if advance is smaller than current doc, advance to cur+1.
+      assertEquals(2, docIDsIter.advance(0));
+    } finally {
+      reader.close();
+      dir.close();
+    }
+  }
+  
+  /**
+   * Creates an index with n documents, this method is meant for testing purposes ONLY
+   */
+  static IndexReader createReaderWithNDocs(Random random, int nDocs, Directory directory) throws IOException {
+    return createReaderWithNDocs(random, nDocs, new DocumentFactory(nDocs), directory);
+  }
+
+  private static class DocumentFactory {
+    protected final static String field = "content";
+    protected final static String delTxt = "delete";
+    protected final static String alphaTxt = "alpha";
+    
+    private final static Field deletionMark = new StringField(field, delTxt, Field.Store.NO);
+    private final static Field alphaContent = new StringField(field, alphaTxt, Field.Store.NO);
+    
+    public DocumentFactory(int totalNumDocs) {
+    }
+    
+    public boolean markedDeleted(int docNum) {
+      return false;
+    }
+
+    public Document getDoc(int docNum) {
+      Document doc = new Document();
+      if (markedDeleted(docNum)) {
+        doc.add(deletionMark);
+        // Add a special field for docs that are marked for deletion. Later we
+        // assert that those docs are not returned by all-scored-doc-IDs.
+        FieldType ft = new FieldType();
+        ft.setStored(true);
+        doc.add(new Field("del", Integer.toString(docNum), ft));
+      }
+
+      if (haveAlpha(docNum)) {
+        doc.add(alphaContent);
+      }
+      return doc;
+    }
+
+    public boolean haveAlpha(int docNum) {
+      return false;
+    }
+  }
+
+  static IndexReader createReaderWithNDocs(Random random, int nDocs, DocumentFactory docFactory, Directory dir) throws IOException {
+    RandomIndexWriter writer = new RandomIndexWriter(random, dir,
+        newIndexWriterConfig(random, TEST_VERSION_CURRENT,
+            new MockAnalyzer(random, MockTokenizer.KEYWORD, false)));
+    for (int docNum = 0; docNum < nDocs; docNum++) {
+      writer.addDocument(docFactory.getDoc(docNum));
+    }
+    // Delete documents marked for deletion
+    writer.deleteDocuments(new Term(DocumentFactory.field, DocumentFactory.delTxt));
+    writer.close();
+
+    // Open a fresh read-only reader with the deletions in place
+    return DirectoryReader.open(dir);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeAccumulator.java b/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeAccumulator.java
index 48738c2..92ecb17 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeAccumulator.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/range/TestRangeAccumulator.java
@@ -17,6 +17,7 @@ package org.apache.lucene.facet.range;
  * limitations under the License.
  */
 
+import java.io.IOException;
 import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
@@ -37,13 +38,15 @@ import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.CountFacetRequest;
 import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.search.DrillSideways.DrillSidewaysResult;
 import org.apache.lucene.facet.search.DrillSideways;
+import org.apache.lucene.facet.search.DrillSideways.DrillSidewaysResult;
 import org.apache.lucene.facet.search.FacetRequest;
 import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetResultNode;
 import org.apache.lucene.facet.search.FacetsAccumulator;
 import org.apache.lucene.facet.search.FacetsCollector;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetFields;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
@@ -74,15 +77,12 @@ public class TestRangeAccumulator extends FacetTestCase {
     IndexReader r = w.getReader();
     w.close();
 
-    FacetSearchParams fsp = new FacetSearchParams(
-                                new RangeFacetRequest<LongRange>("field",
-                                                      new LongRange("less than 10", 0L, true, 10L, false),
-                                                      new LongRange("less than or equal to 10", 0L, true, 10L, true),
-                                                      new LongRange("over 90", 90L, false, 100L, false),
-                                                      new LongRange("90 or above", 90L, true, 100L, false),
-                                                      new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false)));
-
-    RangeAccumulator a = new RangeAccumulator(fsp, r);
+    RangeAccumulator a = new RangeAccumulator(new RangeFacetRequest<LongRange>("field",
+        new LongRange("less than 10", 0L, true, 10L, false),
+        new LongRange("less than or equal to 10", 0L, true, 10L, true),
+        new LongRange("over 90", 90L, false, 100L, false),
+        new LongRange("90 or above", 90L, true, 100L, false),
+        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false)));
     
     FacetsCollector fc = FacetsCollector.create(a);
 
@@ -97,15 +97,15 @@ public class TestRangeAccumulator extends FacetTestCase {
   }
 
   /** Tests single request that mixes Range and non-Range
-   *  faceting, with DrillSideways. */
-  public void testMixedRangeAndNonRange() throws Exception {
+   *  faceting, with DrillSideways and taxonomy. */
+  public void testMixedRangeAndNonRangeTaxonomy() throws Exception {
     Directory d = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random(), d);
     Directory td = newDirectory();
     DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
     FacetFields ff = new FacetFields(tw);
 
-    for(long l=0;l<100;l++) {
+    for (long l = 0; l < 100; l++) {
       Document doc = new Document();
       // For computing range facet counts:
       doc.add(new NumericDocValuesField("field", l));
@@ -122,7 +122,7 @@ public class TestRangeAccumulator extends FacetTestCase {
       w.addDocument(doc);
     }
 
-    IndexReader r = w.getReader();
+    final IndexReader r = w.getReader();
     w.close();
 
     final TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
@@ -130,32 +130,32 @@ public class TestRangeAccumulator extends FacetTestCase {
 
     IndexSearcher s = newSearcher(r);
 
-    final FacetSearchParams fsp = new FacetSearchParams(
-                                new CountFacetRequest(new CategoryPath("dim"), 2),
-                                new RangeFacetRequest<LongRange>("field",
-                                                      new LongRange("less than 10", 0L, true, 10L, false),
-                                                      new LongRange("less than or equal to 10", 0L, true, 10L, true),
-                                                      new LongRange("over 90", 90L, false, 100L, false),
-                                                      new LongRange("90 or above", 90L, true, 100L, false),
-                                                      new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false)));
-
+    final CountFacetRequest countRequest = new CountFacetRequest(new CategoryPath("dim"), 2);
+    final RangeFacetRequest<LongRange> rangeRequest = new RangeFacetRequest<LongRange>("field",
+                          new LongRange("less than 10", 0L, true, 10L, false),
+                          new LongRange("less than or equal to 10", 0L, true, 10L, true),
+                          new LongRange("over 90", 90L, false, 100L, false),
+                          new LongRange("90 or above", 90L, true, 100L, false),
+                          new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false));
+    FacetSearchParams fsp = new FacetSearchParams(countRequest, rangeRequest);
+    
     final Set<String> dimSeen = new HashSet<String>();
 
     DrillSideways ds = new DrillSideways(s, tr) {
         @Override
         protected FacetsAccumulator getDrillDownAccumulator(FacetSearchParams fsp) {
           checkSeen(fsp);
-          return RangeFacetsAccumulatorWrapper.create(fsp, searcher.getIndexReader(), tr);
+          return FacetsAccumulator.create(fsp, r, tr, null);
         }
 
         @Override
         protected FacetsAccumulator getDrillSidewaysAccumulator(String dim, FacetSearchParams fsp) {
           checkSeen(fsp);
-          return RangeFacetsAccumulatorWrapper.create(fsp, searcher.getIndexReader(), tr);
+          return FacetsAccumulator.create(fsp, r, tr, null);
         }
 
         private void checkSeen(FacetSearchParams fsp) {
-          // Each dim should should up only once, across
+          // Each dim should up only once, across
           // both drillDown and drillSideways requests:
           for(FacetRequest fr : fsp.facetRequests) {
             String dim = fr.categoryPath.components[0];
@@ -204,6 +204,111 @@ public class TestRangeAccumulator extends FacetTestCase {
     IOUtils.close(tr, td, r, d);
   }
 
+  /** Tests single request that mixes Range and non-Range
+   *  faceting, with DrillSideways and SortedSet. */
+  public void testMixedRangeAndNonRangeSortedSet() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    SortedSetDocValuesFacetFields ff = new SortedSetDocValuesFacetFields();
+
+    for (long l = 0; l < 100; l++) {
+      Document doc = new Document();
+      // For computing range facet counts:
+      doc.add(new NumericDocValuesField("field", l));
+      // For drill down by numeric range:
+      doc.add(new LongField("field", l, Field.Store.NO));
+
+      CategoryPath cp;
+      if ((l&3) == 0) {
+        cp = new CategoryPath("dim", "a");
+      } else {
+        cp = new CategoryPath("dim", "b");
+      }
+      ff.addFields(doc, Collections.singletonList(cp));
+      w.addDocument(doc);
+    }
+
+    final IndexReader r = w.getReader();
+    w.close();
+
+    IndexSearcher s = newSearcher(r);
+    final SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(s.getIndexReader());
+
+    final CountFacetRequest countRequest = new CountFacetRequest(new CategoryPath("dim"), 2);
+    final RangeFacetRequest<LongRange> rangeRequest = new RangeFacetRequest<LongRange>("field",
+                          new LongRange("less than 10", 0L, true, 10L, false),
+                          new LongRange("less than or equal to 10", 0L, true, 10L, true),
+                          new LongRange("over 90", 90L, false, 100L, false),
+                          new LongRange("90 or above", 90L, true, 100L, false),
+                          new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false));
+    FacetSearchParams fsp = new FacetSearchParams(countRequest, rangeRequest);
+    
+    final Set<String> dimSeen = new HashSet<String>();
+
+    DrillSideways ds = new DrillSideways(s, state) {
+        @Override
+        protected FacetsAccumulator getDrillDownAccumulator(FacetSearchParams fsp) throws IOException {
+          checkSeen(fsp);
+          return FacetsAccumulator.create(fsp, state, null);
+        }
+
+        @Override
+        protected FacetsAccumulator getDrillSidewaysAccumulator(String dim, FacetSearchParams fsp) throws IOException {
+          checkSeen(fsp);
+          return FacetsAccumulator.create(fsp, state, null);
+        }
+
+        private void checkSeen(FacetSearchParams fsp) {
+          // Each dim should up only once, across
+          // both drillDown and drillSideways requests:
+          for(FacetRequest fr : fsp.facetRequests) {
+            String dim = fr.categoryPath.components[0];
+            assertFalse("dim " + dim + " already seen", dimSeen.contains(dim));
+            dimSeen.add(dim);
+          }
+        }
+
+        @Override
+        protected boolean scoreSubDocsAtOnce() {
+          return random().nextBoolean();
+        }
+      };
+
+    // First search, no drill downs:
+    DrillDownQuery ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT, new MatchAllDocsQuery());
+    DrillSidewaysResult dsr = ds.search(null, ddq, 10, fsp);
+
+    assertEquals(100, dsr.hits.totalHits);
+    assertEquals(2, dsr.facetResults.size());
+    assertEquals("dim (0)\n  b (75)\n  a (25)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(0)));
+    assertEquals("field (0)\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(1)));
+
+    // Second search, drill down on dim=b:
+    ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT, new MatchAllDocsQuery());
+    ddq.add(new CategoryPath("dim", "b"));
+    dimSeen.clear();
+    dsr = ds.search(null, ddq, 10, fsp);
+
+    assertEquals(75, dsr.hits.totalHits);
+    assertEquals(2, dsr.facetResults.size());
+    assertEquals("dim (0)\n  b (75)\n  a (25)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(0)));
+    assertEquals("field (0)\n  less than 10 (7)\n  less than or equal to 10 (8)\n  over 90 (7)\n  90 or above (8)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(1)));
+
+    // Third search, drill down on "less than or equal to 10":
+    ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT, new MatchAllDocsQuery());
+    ddq.add("field", NumericRangeQuery.newLongRange("field", 0L, 10L, true, true));
+    dimSeen.clear();
+    dsr = ds.search(null, ddq, 10, fsp);
+
+    assertEquals(11, dsr.hits.totalHits);
+    assertEquals(2, dsr.facetResults.size());
+    assertEquals("dim (0)\n  b (8)\n  a (3)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(0)));
+    assertEquals("field (0)\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n", FacetTestUtils.toSimpleString(dsr.facetResults.get(1)));
+
+    IOUtils.close(r, d);
+  }
+
   public void testBasicDouble() throws Exception {
     Directory d = newDirectory();
     RandomIndexWriter w = new RandomIndexWriter(random(), d);
@@ -211,22 +316,19 @@ public class TestRangeAccumulator extends FacetTestCase {
     DoubleDocValuesField field = new DoubleDocValuesField("field", 0.0);
     doc.add(field);
     for(long l=0;l<100;l++) {
-      field.setDoubleValue((double) l);
+      field.setDoubleValue(l);
       w.addDocument(doc);
     }
 
     IndexReader r = w.getReader();
     w.close();
 
-    FacetSearchParams fsp = new FacetSearchParams(
-                                new RangeFacetRequest<DoubleRange>("field",
-                                                      new DoubleRange("less than 10", 0.0, true, 10.0, false),
-                                                      new DoubleRange("less than or equal to 10", 0.0, true, 10.0, true),
-                                                      new DoubleRange("over 90", 90.0, false, 100.0, false),
-                                                      new DoubleRange("90 or above", 90.0, true, 100.0, false),
-                                                      new DoubleRange("over 1000", 1000.0, false, Double.POSITIVE_INFINITY, false)));
-
-    RangeAccumulator a = new RangeAccumulator(fsp, r);
+    RangeAccumulator a = new RangeAccumulator(new RangeFacetRequest<DoubleRange>("field",
+        new DoubleRange("less than 10", 0.0, true, 10.0, false),
+        new DoubleRange("less than or equal to 10", 0.0, true, 10.0, true),
+        new DoubleRange("over 90", 90.0, false, 100.0, false),
+        new DoubleRange("90 or above", 90.0, true, 100.0, false),
+        new DoubleRange("over 1000", 1000.0, false, Double.POSITIVE_INFINITY, false)));
     
     FacetsCollector fc = FacetsCollector.create(a);
 
@@ -247,22 +349,19 @@ public class TestRangeAccumulator extends FacetTestCase {
     FloatDocValuesField field = new FloatDocValuesField("field", 0.0f);
     doc.add(field);
     for(long l=0;l<100;l++) {
-      field.setFloatValue((float) l);
+      field.setFloatValue(l);
       w.addDocument(doc);
     }
 
     IndexReader r = w.getReader();
     w.close();
 
-    FacetSearchParams fsp = new FacetSearchParams(
-                                new RangeFacetRequest<FloatRange>("field",
-                                                      new FloatRange("less than 10", 0.0f, true, 10.0f, false),
-                                                      new FloatRange("less than or equal to 10", 0.0f, true, 10.0f, true),
-                                                      new FloatRange("over 90", 90.0f, false, 100.0f, false),
-                                                      new FloatRange("90 or above", 90.0f, true, 100.0f, false),
-                                                      new FloatRange("over 1000", 1000.0f, false, Float.POSITIVE_INFINITY, false)));
-
-    RangeAccumulator a = new RangeAccumulator(fsp, r);
+    RangeAccumulator a = new RangeAccumulator(new RangeFacetRequest<FloatRange>("field",
+        new FloatRange("less than 10", 0.0f, true, 10.0f, false),
+        new FloatRange("less than or equal to 10", 0.0f, true, 10.0f, true),
+        new FloatRange("over 90", 90.0f, false, 100.0f, false),
+        new FloatRange("90 or above", 90.0f, true, 100.0f, false),
+        new FloatRange("over 1000", 1000.0f, false, Float.POSITIVE_INFINITY, false)));
     
     FacetsCollector fc = FacetsCollector.create(a);
 
@@ -335,8 +434,7 @@ public class TestRangeAccumulator extends FacetTestCase {
         }
       }
 
-      FacetSearchParams fsp = new FacetSearchParams(new RangeFacetRequest<LongRange>("field", ranges));
-      FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(fsp, r));
+      FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(new RangeFacetRequest<LongRange>("field", ranges)));
       s.search(new MatchAllDocsQuery(), fc);
       List<FacetResult> results = fc.getFacetResults();
       assertEquals(1, results.size());
@@ -350,7 +448,7 @@ public class TestRangeAccumulator extends FacetTestCase {
         assertEquals("field/r" + rangeID, subNode.label.toString('/'));
         assertEquals(expectedCounts[rangeID], (int) subNode.value);
 
-        LongRange range = (LongRange) ((RangeFacetRequest) results.get(0).getFacetRequest()).ranges[rangeID];
+        LongRange range = (LongRange) ((RangeFacetRequest<?>) results.get(0).getFacetRequest()).ranges[rangeID];
 
         // Test drill-down:
         DrillDownQuery ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT);
@@ -422,8 +520,7 @@ public class TestRangeAccumulator extends FacetTestCase {
         }
       }
 
-      FacetSearchParams fsp = new FacetSearchParams(new RangeFacetRequest<FloatRange>("field", ranges));
-      FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(fsp, r));
+      FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(new RangeFacetRequest<FloatRange>("field", ranges)));
       s.search(new MatchAllDocsQuery(), fc);
       List<FacetResult> results = fc.getFacetResults();
       assertEquals(1, results.size());
@@ -437,7 +534,7 @@ public class TestRangeAccumulator extends FacetTestCase {
         assertEquals("field/r" + rangeID, subNode.label.toString('/'));
         assertEquals(expectedCounts[rangeID], (int) subNode.value);
 
-        FloatRange range = (FloatRange) ((RangeFacetRequest) results.get(0).getFacetRequest()).ranges[rangeID];
+        FloatRange range = (FloatRange) ((RangeFacetRequest<?>) results.get(0).getFacetRequest()).ranges[rangeID];
 
         // Test drill-down:
         DrillDownQuery ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT);
@@ -509,8 +606,7 @@ public class TestRangeAccumulator extends FacetTestCase {
         }
       }
 
-      FacetSearchParams fsp = new FacetSearchParams(new RangeFacetRequest<DoubleRange>("field", ranges));
-      FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(fsp, r));
+      FacetsCollector fc = FacetsCollector.create(new RangeAccumulator(new RangeFacetRequest<DoubleRange>("field", ranges)));
       s.search(new MatchAllDocsQuery(), fc);
       List<FacetResult> results = fc.getFacetResults();
       assertEquals(1, results.size());
@@ -524,7 +620,7 @@ public class TestRangeAccumulator extends FacetTestCase {
         assertEquals("field/r" + rangeID, subNode.label.toString('/'));
         assertEquals(expectedCounts[rangeID], (int) subNode.value);
 
-        DoubleRange range = (DoubleRange) ((RangeFacetRequest) results.get(0).getFacetRequest()).ranges[rangeID];
+        DoubleRange range = (DoubleRange) ((RangeFacetRequest<?>) results.get(0).getFacetRequest()).ranges[rangeID];
 
         // Test drill-down:
         DrillDownQuery ddq = new DrillDownQuery(FacetIndexingParams.DEFAULT);
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/sampling/BaseSampleTestTopK.java b/lucene/facet/src/test/org/apache/lucene/facet/sampling/BaseSampleTestTopK.java
index 152a40d..85d4548 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/sampling/BaseSampleTestTopK.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/sampling/BaseSampleTestTopK.java
@@ -3,18 +3,14 @@ package org.apache.lucene.facet.sampling;
 import java.util.List;
 import java.util.Random;
 
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
 import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.sampling.RandomSampler;
-import org.apache.lucene.facet.sampling.RepeatableSampler;
-import org.apache.lucene.facet.sampling.Sampler;
-import org.apache.lucene.facet.sampling.SamplingParams;
 import org.apache.lucene.facet.search.BaseTestTopK;
 import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
 import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.Term;
@@ -60,7 +56,7 @@ public abstract class BaseSampleTestTopK extends BaseTestTopK {
     return res;
   }
   
-  protected abstract StandardFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
+  protected abstract OldFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
       IndexReader indexReader, FacetSearchParams searchParams);
   
   /**
@@ -123,8 +119,8 @@ public abstract class BaseSampleTestTopK extends BaseTestTopK {
   
   private FacetsCollector samplingCollector(final boolean complement, final Sampler sampler,
       FacetSearchParams samplingSearchParams) {
-    StandardFacetsAccumulator sfa = getSamplingAccumulator(sampler, taxoReader, indexReader, samplingSearchParams);
-    sfa.setComplementThreshold(complement ? StandardFacetsAccumulator.FORCE_COMPLEMENT : StandardFacetsAccumulator.DISABLE_COMPLEMENT);
+    OldFacetsAccumulator sfa = getSamplingAccumulator(sampler, taxoReader, indexReader, samplingSearchParams);
+    sfa.setComplementThreshold(complement ? OldFacetsAccumulator.FORCE_COMPLEMENT : OldFacetsAccumulator.DISABLE_COMPLEMENT);
     return FacetsCollector.create(sfa);
   }
   
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/sampling/OversampleWithDepthTest.java b/lucene/facet/src/test/org/apache/lucene/facet/sampling/OversampleWithDepthTest.java
index 347e378..08bc4a7 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/sampling/OversampleWithDepthTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/sampling/OversampleWithDepthTest.java
@@ -6,19 +6,15 @@ import java.util.Collections;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
 import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.sampling.RandomSampler;
-import org.apache.lucene.facet.sampling.Sampler;
-import org.apache.lucene.facet.sampling.SamplingAccumulator;
-import org.apache.lucene.facet.sampling.SamplingParams;
 import org.apache.lucene.facet.search.CountFacetRequest;
 import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
 import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetResultNode;
 import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
-import org.apache.lucene.facet.search.FacetRequest.ResultMode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
@@ -116,7 +112,7 @@ public class OversampleWithDepthTest extends FacetTestCase {
       final SamplingParams params) throws IOException {
     // a FacetsCollector with a sampling accumulator
     Sampler sampler = new RandomSampler(params, random());
-    StandardFacetsAccumulator sfa = new SamplingAccumulator(sampler, fsp, r, tr);
+    OldFacetsAccumulator sfa = new SamplingAccumulator(sampler, fsp, r, tr);
     FacetsCollector fcWithSampling = FacetsCollector.create(sfa);
     
     IndexSearcher s = newSearcher(r);
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplerTest.java b/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplerTest.java
index 93648ac..027aaf1 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplerTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplerTest.java
@@ -4,12 +4,12 @@ import java.util.ArrayList;
 import java.util.List;
 
 import org.apache.lucene.facet.FacetTestBase;
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
 import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.CountFacetRequest;
 import org.apache.lucene.facet.search.FacetResultNode;
 import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.junit.After;
@@ -99,7 +99,7 @@ public class SamplerTest extends FacetTestBase {
     
     // Make sure no complements are in action
     accumulator
-        .setComplementThreshold(StandardFacetsAccumulator.DISABLE_COMPLEMENT);
+        .setComplementThreshold(OldFacetsAccumulator.DISABLE_COMPLEMENT);
     
     FacetsCollector fc = FacetsCollector.create(accumulator);
     
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingAccumulatorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingAccumulatorTest.java
index 9950904..2cbfd1d 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingAccumulatorTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingAccumulatorTest.java
@@ -1,9 +1,7 @@
 package org.apache.lucene.facet.sampling;
 
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
 import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.sampling.Sampler;
-import org.apache.lucene.facet.sampling.SamplingAccumulator;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.util.LuceneTestCase.Slow;
@@ -29,7 +27,7 @@ import org.apache.lucene.util.LuceneTestCase.Slow;
 public class SamplingAccumulatorTest extends BaseSampleTestTopK {
 
   @Override
-  protected StandardFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
+  protected OldFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
       IndexReader indexReader, FacetSearchParams searchParams) {
     return new SamplingAccumulator(sampler, searchParams, indexReader, taxoReader);
   }
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingWrapperTest.java b/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingWrapperTest.java
index 4eddb07..a6074fa 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingWrapperTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingWrapperTest.java
@@ -1,13 +1,10 @@
 package org.apache.lucene.facet.sampling;
 
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.LuceneTestCase.Slow;
-
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
 import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.sampling.Sampler;
-import org.apache.lucene.facet.sampling.SamplingWrapper;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.util.LuceneTestCase.Slow;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -30,9 +27,9 @@ import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 public class SamplingWrapperTest extends BaseSampleTestTopK {
 
   @Override
-  protected StandardFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
+  protected OldFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
       IndexReader indexReader, FacetSearchParams searchParams) {
-    return new SamplingWrapper(new StandardFacetsAccumulator(searchParams, indexReader, taxoReader), sampler);
+    return new SamplingWrapper(new OldFacetsAccumulator(searchParams, indexReader, taxoReader), sampler);
   }
   
 }
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/AdaptiveAccumulatorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/AdaptiveAccumulatorTest.java
deleted file mode 100644
index 15bce9c..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/AdaptiveAccumulatorTest.java
+++ /dev/null
@@ -1,39 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.LuceneTestCase.Slow;
-
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.sampling.BaseSampleTestTopK;
-import org.apache.lucene.facet.sampling.Sampler;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-@Slow
-public class AdaptiveAccumulatorTest extends BaseSampleTestTopK {
-
-  @Override
-  protected StandardFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
-      IndexReader indexReader, FacetSearchParams searchParams) {
-    AdaptiveFacetsAccumulator res = new AdaptiveFacetsAccumulator(searchParams, indexReader, taxoReader);
-    res.setSampler(sampler);
-    return res;
-  }
-  
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java
index cb0cbea..9110c6c 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java
@@ -269,7 +269,7 @@ public class CountingFacetsAggregatorTest extends FacetTestCase {
     IOUtils.close(indexWriter, taxoWriter);
   }
   
-  private FacetsAccumulator randomAccumulator(FacetSearchParams fsp, IndexReader indexReader, TaxonomyReader taxoReader) {
+  private TaxonomyFacetsAccumulator randomAccumulator(FacetSearchParams fsp, IndexReader indexReader, TaxonomyReader taxoReader) {
     final FacetsAggregator aggregator;
     double val = random().nextDouble();
     if (val < 0.6) {
@@ -279,7 +279,7 @@ public class CountingFacetsAggregatorTest extends FacetTestCase {
     } else {
       aggregator = new CachedOrdsCountingFacetsAggregator();
     }
-    return new FacetsAccumulator(fsp, indexReader, taxoReader) {
+    return new TaxonomyFacetsAccumulator(fsp, indexReader, taxoReader) {
       @Override
       public FacetsAggregator getAggregator() {
         return aggregator;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/FacetResultTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/FacetResultTest.java
index 994c75a..5d38db2 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/FacetResultTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/FacetResultTest.java
@@ -116,7 +116,7 @@ public class FacetResultTest extends FacetTestCase {
         @Override
         protected FacetsAccumulator getDrillSidewaysAccumulator(String dim, FacetSearchParams fsp) throws IOException {
           FacetsAccumulator fa = super.getDrillSidewaysAccumulator(dim, fsp);
-          dimArrays.put(dim, fa.facetArrays);
+          dimArrays.put(dim, ((TaxonomyFacetsAccumulator) fa).facetArrays);
           return fa;
         }
       };
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestDrillSideways.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestDrillSideways.java
index 5f6d6b0..25b8e4c 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestDrillSideways.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestDrillSideways.java
@@ -41,7 +41,6 @@ import org.apache.lucene.facet.index.FacetFields;
 import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.DrillSideways.DrillSidewaysResult;
-import org.apache.lucene.facet.sortedset.SortedSetDocValuesAccumulator;
 import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetFields;
 import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
@@ -62,8 +61,8 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.Scorer;
 import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField.Type;
 import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.SortField.Type;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
@@ -336,6 +335,8 @@ public class TestDrillSideways extends FacetTestCase {
     String id;
     String contentToken;
 
+    public Doc() {}
+    
     // -1 if the doc is missing this dim, else the index
     // -into the values for this dim:
     int[] dims;
@@ -790,17 +791,7 @@ public class TestDrillSideways extends FacetTestCase {
       Sort sort = new Sort(new SortField("id", SortField.Type.STRING));
       DrillSideways ds;
       if (doUseDV) {
-        ds = new DrillSideways(s, null) {
-            @Override
-            protected FacetsAccumulator getDrillDownAccumulator(FacetSearchParams fsp) throws IOException {
-              return new SortedSetDocValuesAccumulator(fsp, sortedSetDVState);
-            }
-
-            @Override
-            protected FacetsAccumulator getDrillSidewaysAccumulator(String dim, FacetSearchParams fsp) throws IOException {
-              return new SortedSetDocValuesAccumulator(fsp, sortedSetDVState);
-            }
-          };
+        ds = new DrillSideways(s, sortedSetDVState);
       } else {
         ds = new DrillSideways(s, tr);
       }
@@ -881,6 +872,7 @@ public class TestDrillSideways extends FacetTestCase {
     List<Doc> hits;
     int[][] counts;
     int[] uniqueCounts;
+    public SimpleFacetResult() {}
   }
   
   private int[] getTopNOrds(final int[] counts, final String[] values, int topN) {
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java
index bf011b4..e6cb9b5 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java
@@ -3,9 +3,7 @@ package org.apache.lucene.facet.search;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.List;
-import java.util.Map;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
@@ -13,6 +11,8 @@ import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.old.AdaptiveFacetsAccumulator;
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
 import org.apache.lucene.facet.params.CategoryListParams;
 import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
@@ -90,7 +90,7 @@ public class TestFacetsCollector extends FacetTestCase {
     DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
     
     FacetSearchParams sParams = new FacetSearchParams(new SumScoreFacetRequest(new CategoryPath("a"), 10));
-    FacetsAccumulator fa = new FacetsAccumulator(sParams, r, taxo) {
+    TaxonomyFacetsAccumulator fa = new TaxonomyFacetsAccumulator(sParams, r, taxo) {
       @Override
       public FacetsAggregator getAggregator() {
         return new SumScoreFacetsAggregator();
@@ -181,18 +181,7 @@ public class TestFacetsCollector extends FacetTestCase {
         new CountFacetRequest(new CategoryPath("a"), 10), 
         new SumScoreFacetRequest(new CategoryPath("b"), 10));
     
-    Map<CategoryListParams,FacetsAggregator> aggregators = new HashMap<CategoryListParams,FacetsAggregator>();
-    aggregators.put(fip.getCategoryListParams(new CategoryPath("a")), new FastCountingFacetsAggregator());
-    aggregators.put(fip.getCategoryListParams(new CategoryPath("b")), new SumScoreFacetsAggregator());
-    final FacetsAggregator aggregator = new PerCategoryListAggregator(aggregators, fip);
-    FacetsAccumulator fa = new FacetsAccumulator(sParams, r, taxo) {
-      @Override
-      public FacetsAggregator getAggregator() {
-        return aggregator;
-      }
-    };
-    
-    FacetsCollector fc = FacetsCollector.create(fa);
+    FacetsCollector fc = FacetsCollector.create(sParams, r, taxo);
     TopScoreDocCollector topDocs = TopScoreDocCollector.create(10, false);
     newSearcher(r).search(new MatchAllDocsQuery(), MultiCollector.wrap(fc, topDocs));
     
@@ -231,7 +220,7 @@ public class TestFacetsCollector extends FacetTestCase {
     
     FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(CategoryPath.EMPTY, 10));
     
-    final FacetsAccumulator fa = random().nextBoolean() ? new FacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);
+    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);
     FacetsCollector fc = FacetsCollector.create(fa);
     newSearcher(r).search(new MatchAllDocsQuery(), fc);
     
@@ -265,7 +254,7 @@ public class TestFacetsCollector extends FacetTestCase {
     FacetSearchParams fsp = new FacetSearchParams(
         new CountFacetRequest(new CategoryPath("a"), 10), 
         new CountFacetRequest(new CategoryPath("b"), 10));
-    final FacetsAccumulator fa = random().nextBoolean() ? new FacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);
+    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);
     final FacetsCollector fc = FacetsCollector.create(fa);
     newSearcher(r).search(new MatchAllDocsQuery(), fc);
     
@@ -297,7 +286,7 @@ public class TestFacetsCollector extends FacetTestCase {
     FacetSearchParams fsp = new FacetSearchParams(
         new CountFacetRequest(new CategoryPath("a"), 10), 
         new CountFacetRequest(new CategoryPath("b"), 10));
-    final FacetsAccumulator fa = random().nextBoolean() ? new FacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);
+    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);
     final FacetsCollector fc = FacetsCollector.create(fa);
     // this should populate the cached results, but doing search should clear the cache
     fc.getFacetResults();
@@ -338,7 +327,7 @@ public class TestFacetsCollector extends FacetTestCase {
 
     // assert IntFacetResultHandler
     FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath("a"), 10));
-    FacetsAccumulator fa = random().nextBoolean() ? new FacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);
+    TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);
     FacetsCollector fc = FacetsCollector.create(fa);
     newSearcher(r).search(new MatchAllDocsQuery(), fc);
     assertTrue("invalid ordinal for child node: 0", 0 != fc.getFacetResults().get(0).getFacetResultNode().subResults.get(0).ordinal);
@@ -346,14 +335,14 @@ public class TestFacetsCollector extends FacetTestCase {
     // assert IntFacetResultHandler
     fsp = new FacetSearchParams(new SumScoreFacetRequest(new CategoryPath("a"), 10));
     if (random().nextBoolean()) {
-      fa = new FacetsAccumulator(fsp, r, taxo) {
+      fa = new TaxonomyFacetsAccumulator(fsp, r, taxo) {
         @Override
         public FacetsAggregator getAggregator() {
           return new SumScoreFacetsAggregator();
         }
       };
     } else {
-      fa = new StandardFacetsAccumulator(fsp, r, taxo);
+      fa = new OldFacetsAccumulator(fsp, r, taxo);
     }
     fc = FacetsCollector.create(fa);
     newSearcher(r).search(new MatchAllDocsQuery(), fc);
@@ -387,7 +376,7 @@ public class TestFacetsCollector extends FacetTestCase {
     CountFacetRequest cfr = new CountFacetRequest(new CategoryPath("a"), 2);
     cfr.setResultMode(random().nextBoolean() ? ResultMode.GLOBAL_FLAT : ResultMode.PER_NODE_IN_TREE);
     FacetSearchParams fsp = new FacetSearchParams(cfr);
-    final FacetsAccumulator fa = random().nextBoolean() ? new FacetsAccumulator(fsp, r, taxo) : new StandardFacetsAccumulator(fsp, r, taxo);
+    final TaxonomyFacetsAccumulator fa = random().nextBoolean() ? new TaxonomyFacetsAccumulator(fsp, r, taxo) : new OldFacetsAccumulator(fsp, r, taxo);
     FacetsCollector fc = FacetsCollector.create(fa);
     newSearcher(r).search(new MatchAllDocsQuery(), fc);
     
@@ -426,15 +415,15 @@ public class TestFacetsCollector extends FacetTestCase {
     }
     final Sampler sampler = new RandomSampler(sampleParams, random());
     
-    FacetsAccumulator[] accumulators = new FacetsAccumulator[] {
-      new FacetsAccumulator(fsp, indexReader, taxoReader),
-      new StandardFacetsAccumulator(fsp, indexReader, taxoReader),
+    TaxonomyFacetsAccumulator[] accumulators = new TaxonomyFacetsAccumulator[] {
+      new TaxonomyFacetsAccumulator(fsp, indexReader, taxoReader),
+      new OldFacetsAccumulator(fsp, indexReader, taxoReader),
       new SamplingAccumulator(sampler, fsp, indexReader, taxoReader),
       new AdaptiveFacetsAccumulator(fsp, indexReader, taxoReader),
-      new SamplingWrapper(new StandardFacetsAccumulator(fsp, indexReader, taxoReader), sampler)
+      new SamplingWrapper(new OldFacetsAccumulator(fsp, indexReader, taxoReader), sampler)
     };
     
-    for (FacetsAccumulator fa : accumulators) {
+    for (TaxonomyFacetsAccumulator fa : accumulators) {
       FacetsCollector fc = FacetsCollector.create(fa);
       searcher.search(new MatchAllDocsQuery(), fc);
       List<FacetResult> facetResults = fc.getFacetResults();
@@ -444,20 +433,19 @@ public class TestFacetsCollector extends FacetTestCase {
     
     try {
       // SortedSetDocValuesAccumulator cannot even be created in such state
-      assertNull(new SortedSetDocValuesAccumulator(fsp, new SortedSetDocValuesReaderState(indexReader)));
+      assertNull(new SortedSetDocValuesAccumulator(new SortedSetDocValuesReaderState(indexReader), fsp));
       // if this ever changes, make sure FacetResultNode is labeled correctly 
       fail("should not have succeeded to execute a request over a category which wasn't indexed as SortedSetDVField");
     } catch (IllegalArgumentException e) {
       // expected
     }
 
-    fsp = new FacetSearchParams(new RangeFacetRequest<LongRange>("f", new LongRange("grr", 0, true, 1, true)));
-    RangeAccumulator ra = new RangeAccumulator(fsp, indexReader);
+    RangeAccumulator ra = new RangeAccumulator(new RangeFacetRequest<LongRange>("f", new LongRange("grr", 0, true, 1, true)));
     FacetsCollector fc = FacetsCollector.create(ra);
     searcher.search(new MatchAllDocsQuery(), fc);
     List<FacetResult> facetResults = fc.getFacetResults();
     assertNotNull(facetResults);
-    assertEquals("incorrect label returned for RangeAccumulator", fsp.facetRequests.get(0).categoryPath, facetResults.get(0).getFacetResultNode().label);
+    assertEquals("incorrect label returned for RangeAccumulator", new CategoryPath("f"), facetResults.get(0).getFacetResultNode().label);
 
     IOUtils.close(indexReader, taxoReader);
 
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
index e5a235a..e183043 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
@@ -11,6 +11,7 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
 import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.FacetRequest.ResultMode;
@@ -150,8 +151,8 @@ public class TestTopKInEachNodeResultHandler extends FacetTestCase {
       FacetSearchParams facetSearchParams = new FacetSearchParams(iParams, facetRequests);
       
       FacetArrays facetArrays = new FacetArrays(PartitionsUtils.partitionSize(facetSearchParams.indexingParams, tr));
-      StandardFacetsAccumulator sfa = new StandardFacetsAccumulator(facetSearchParams, is.getIndexReader(), tr, facetArrays);
-      sfa.setComplementThreshold(StandardFacetsAccumulator.DISABLE_COMPLEMENT);
+      OldFacetsAccumulator sfa = new OldFacetsAccumulator(facetSearchParams, is.getIndexReader(), tr, facetArrays);
+      sfa.setComplementThreshold(OldFacetsAccumulator.DISABLE_COMPLEMENT);
       FacetsCollector fc = FacetsCollector.create(sfa);
       
       is.search(q, fc);
@@ -183,7 +184,7 @@ public class TestTopKInEachNodeResultHandler extends FacetTestCase {
       }
       // now rearrange
       double [] expectedValues00 = { 6.0, 1.0, 5.0, 3.0, 2.0 };
-      fr = sfa.createFacetResultsHandler(cfra23).rearrangeFacetResult(fr);
+      fr = sfa.createFacetResultsHandler(cfra23, sfa.createOrdinalValueResolver(cfra23)).rearrangeFacetResult(fr);
       i = 0;
       for (FacetResultNode node : parentRes.subResults) {
         assertEquals(expectedValues00[i++], node.value, Double.MIN_VALUE);
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java
index 51e673a..cd9a3ca 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java
@@ -4,6 +4,7 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.List;
 
+import org.apache.lucene.facet.old.OldFacetsAccumulator;
 import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.search.MatchAllDocsQuery;
@@ -33,8 +34,8 @@ public class TestTopKResultsHandlerRandom extends BaseTestTopK {
       throws IOException {
     Query q = new MatchAllDocsQuery();
     FacetSearchParams facetSearchParams = searchParamsWithRequests(numResults, fip);
-    StandardFacetsAccumulator sfa = new StandardFacetsAccumulator(facetSearchParams, indexReader, taxoReader);
-    sfa.setComplementThreshold(doComplement ? StandardFacetsAccumulator.FORCE_COMPLEMENT : StandardFacetsAccumulator.DISABLE_COMPLEMENT);
+    OldFacetsAccumulator sfa = new OldFacetsAccumulator(facetSearchParams, indexReader, taxoReader);
+    sfa.setComplementThreshold(doComplement ? OldFacetsAccumulator.FORCE_COMPLEMENT : OldFacetsAccumulator.DISABLE_COMPLEMENT);
     FacetsCollector fc = FacetsCollector.create(sfa);
     searcher.search(q, fc);
     List<FacetResult> facetResults = fc.getFacetResults();
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java
index 55db5ad..008a007 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/sortedset/TestSortedSetDocValuesFacets.java
@@ -112,7 +112,7 @@ public class TestSortedSetDocValuesFacets extends FacetTestCase {
     //SortedSetDocValuesCollector c = new SortedSetDocValuesCollector(state);
     //SortedSetDocValuesCollectorMergeBySeg c = new SortedSetDocValuesCollectorMergeBySeg(state);
 
-    FacetsCollector c = FacetsCollector.create(new SortedSetDocValuesAccumulator(fsp, state));
+    FacetsCollector c = FacetsCollector.create(new SortedSetDocValuesAccumulator(state, fsp));
 
     searcher.search(new MatchAllDocsQuery(), c);
 
@@ -177,7 +177,7 @@ public class TestSortedSetDocValuesFacets extends FacetTestCase {
 
     FacetSearchParams fsp = new FacetSearchParams(requests);
     
-    FacetsCollector c = FacetsCollector.create(new SortedSetDocValuesAccumulator(fsp, state));
+    FacetsCollector c = FacetsCollector.create(new SortedSetDocValuesAccumulator(state, fsp));
 
     searcher.search(new MatchAllDocsQuery(), c);
 
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCharBlockArray.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCharBlockArray.java
index 747acfe..76f8c16 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCharBlockArray.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCharBlockArray.java
@@ -11,6 +11,7 @@ import java.nio.charset.CodingErrorAction;
 
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
 import org.junit.Test;
 
 /*
@@ -83,7 +84,8 @@ public class TestCharBlockArray extends FacetTestCase {
 
     assertEqualsInternal("GrowingCharArray<->StringBuilder mismatch.", builder, array);
 
-    File f = new File("GrowingCharArrayTest.tmp");
+    File tempDir = _TestUtil.getTempDir("growingchararray");
+    File f = new File(tempDir, "GrowingCharArrayTest.tmp");
     BufferedOutputStream out = new BufferedOutputStream(new FileOutputStream(f));
     array.flush(out);
     out.flush();
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/util/TestScoredDocIDsUtils.java b/lucene/facet/src/test/org/apache/lucene/facet/util/TestScoredDocIDsUtils.java
deleted file mode 100644
index 252e75c..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/util/TestScoredDocIDsUtils.java
+++ /dev/null
@@ -1,156 +0,0 @@
-package org.apache.lucene.facet.util;
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.ScoredDocIDsIterator;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.FixedBitSet;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestScoredDocIDsUtils extends FacetTestCase {
-
-  @Test
-  public void testComplementIterator() throws Exception {
-    final int n = atLeast(10000);
-    final FixedBitSet bits = new FixedBitSet(n);
-    Random random = random();
-    for (int i = 0; i < n; i++) {
-      int idx = random.nextInt(n);
-      bits.flip(idx, idx + 1);
-    }
-    
-    FixedBitSet verify = new FixedBitSet(bits);
-
-    ScoredDocIDs scoredDocIDs = ScoredDocIdsUtils.createScoredDocIds(bits, n); 
-
-    Directory dir = newDirectory();
-    IndexReader reader = createReaderWithNDocs(random, n, dir);
-    try { 
-      assertEquals(n - verify.cardinality(), ScoredDocIdsUtils.getComplementSet(scoredDocIDs, reader).size());
-    } finally {
-      reader.close();
-      dir.close();
-    }
-  }
-
-  @Test
-  public void testAllDocs() throws Exception {
-    int maxDoc = 3;
-    Directory dir = newDirectory();
-    IndexReader reader = createReaderWithNDocs(random(), maxDoc, dir);
-    try {
-      ScoredDocIDs all = ScoredDocIdsUtils.createAllDocsScoredDocIDs(reader);
-      assertEquals("invalid size", maxDoc, all.size());
-      ScoredDocIDsIterator iter = all.iterator();
-      int doc = 0;
-      while (iter.next()) {
-        assertEquals("invalid doc ID: " + iter.getDocID(), doc++, iter.getDocID());
-        assertEquals("invalid score: " + iter.getScore(), ScoredDocIDsIterator.DEFAULT_SCORE, iter.getScore(), 0.0f);
-      }
-      assertEquals("invalid maxDoc: " + doc, maxDoc, doc);
-      
-      DocIdSet docIDs = all.getDocIDs();
-      assertTrue("should be cacheable", docIDs.isCacheable());
-      DocIdSetIterator docIDsIter = docIDs.iterator();
-      assertEquals("nextDoc() hasn't been called yet", -1, docIDsIter.docID());
-      assertEquals(0, docIDsIter.nextDoc());
-      assertEquals(1, docIDsIter.advance(1));
-      // if advance is smaller than current doc, advance to cur+1.
-      assertEquals(2, docIDsIter.advance(0));
-    } finally {
-      reader.close();
-      dir.close();
-    }
-  }
-  
-  /**
-   * Creates an index with n documents, this method is meant for testing purposes ONLY
-   */
-  static IndexReader createReaderWithNDocs(Random random, int nDocs, Directory directory) throws IOException {
-    return createReaderWithNDocs(random, nDocs, new DocumentFactory(nDocs), directory);
-  }
-
-  private static class DocumentFactory {
-    protected final static String field = "content";
-    protected final static String delTxt = "delete";
-    protected final static String alphaTxt = "alpha";
-    
-    private final static Field deletionMark = new StringField(field, delTxt, Field.Store.NO);
-    private final static Field alphaContent = new StringField(field, alphaTxt, Field.Store.NO);
-    
-    public DocumentFactory(int totalNumDocs) {
-    }
-    
-    public boolean markedDeleted(int docNum) {
-      return false;
-    }
-
-    public Document getDoc(int docNum) {
-      Document doc = new Document();
-      if (markedDeleted(docNum)) {
-        doc.add(deletionMark);
-        // Add a special field for docs that are marked for deletion. Later we
-        // assert that those docs are not returned by all-scored-doc-IDs.
-        FieldType ft = new FieldType();
-        ft.setStored(true);
-        doc.add(new Field("del", Integer.toString(docNum), ft));
-      }
-
-      if (haveAlpha(docNum)) {
-        doc.add(alphaContent);
-      }
-      return doc;
-    }
-
-    public boolean haveAlpha(int docNum) {
-      return false;
-    }
-  }
-
-  static IndexReader createReaderWithNDocs(Random random, int nDocs, DocumentFactory docFactory, Directory dir) throws IOException {
-    RandomIndexWriter writer = new RandomIndexWriter(random, dir,
-        newIndexWriterConfig(random, TEST_VERSION_CURRENT,
-            new MockAnalyzer(random, MockTokenizer.KEYWORD, false)));
-    for (int docNum = 0; docNum < nDocs; docNum++) {
-      writer.addDocument(docFactory.getDoc(docNum));
-    }
-    // Delete documents marked for deletion
-    writer.deleteDocuments(new Term(DocumentFactory.field, DocumentFactory.delTxt));
-    writer.close();
-
-    // Open a fresh read-only reader with the deletions in place
-    return DirectoryReader.open(dir);
-  }
-}
diff --git a/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java b/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java
index 75753f6..075214a 100644
--- a/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java
+++ b/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermGroupFacetCollector.java
@@ -303,7 +303,7 @@ public abstract class TermGroupFacetCollector extends AbstractGroupFacetCollecto
 
         int facetOrd;
         if (groupedFacetHit.facetValue != null) {
-          if (facetOrdTermsEnum == null || !facetOrdTermsEnum.seekExact(groupedFacetHit.facetValue, true)) {
+          if (facetOrdTermsEnum == null || !facetOrdTermsEnum.seekExact(groupedFacetHit.facetValue)) {
             continue;
           }
           facetOrd = (int) facetOrdTermsEnum.ord();
@@ -319,7 +319,7 @@ public abstract class TermGroupFacetCollector extends AbstractGroupFacetCollecto
       if (facetPrefix != null) {
         TermsEnum.SeekStatus seekStatus;
         if (facetOrdTermsEnum != null) {
-          seekStatus = facetOrdTermsEnum.seekCeil(facetPrefix, true);
+          seekStatus = facetOrdTermsEnum.seekCeil(facetPrefix);
         } else {
           seekStatus = TermsEnum.SeekStatus.END;
         }
@@ -334,7 +334,7 @@ public abstract class TermGroupFacetCollector extends AbstractGroupFacetCollecto
 
         BytesRef facetEndPrefix = BytesRef.deepCopyOf(facetPrefix);
         facetEndPrefix.append(UnicodeUtil.BIG_TERM);
-        seekStatus = facetOrdTermsEnum.seekCeil(facetEndPrefix, true);
+        seekStatus = facetOrdTermsEnum.seekCeil(facetEndPrefix);
         if (seekStatus != TermsEnum.SeekStatus.END) {
           endFacetOrd = (int) facetOrdTermsEnum.ord();
         } else {
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SpanGradientFormatter.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/SpanGradientFormatter.java
old mode 100755
new mode 100644
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
index 88446b3..a207b97 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
@@ -282,7 +282,7 @@ public class WeightedSpanTermExtractor {
       TreeSet<Term> extractedTerms = new TreeSet<Term>();
       q.extractTerms(extractedTerms);
       for (Term term : extractedTerms) {
-        termContexts.put(term, TermContext.build(context, term, true));
+        termContexts.put(term, TermContext.build(context, term));
       }
       Bits acceptDocs = context.reader().getLiveDocs();
       final Spans spans = q.getSpans(context, acceptDocs, termContexts);
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
index 715efeb..3d7b95d 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
@@ -459,7 +459,7 @@ public class PostingsHighlighter {
         continue;
       } else if (de == null) {
         postings[i] = EMPTY; // initially
-        if (!termsEnum.seekExact(terms[i], true)) {
+        if (!termsEnum.seekExact(terms[i])) {
           continue; // term not found
         }
         de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);
@@ -506,6 +506,13 @@ public class PostingsHighlighter {
         throw new IllegalArgumentException("field '" + field + "' was indexed without offsets, cannot highlight");
       }
       int end = dp.endOffset();
+      // LUCENE-5166: this hit would span the content limit... however more valid 
+      // hits may exist (they are sorted by start). so we pretend like we never 
+      // saw this term, it won't cause a passage to be added to passageQueue or anything.
+      assert EMPTY.startOffset() == Integer.MAX_VALUE;
+      if (start < contentLength && end > contentLength) {
+        continue;
+      }
       if (start >= current.endOffset) {
         if (current.startOffset >= 0) {
           // finalize current
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java b/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java
index 74cceb4..0168bbe 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java
@@ -60,50 +60,47 @@ public class FieldPhraseList {
   public FieldPhraseList( FieldTermStack fieldTermStack, FieldQuery fieldQuery, int phraseLimit ){
     final String field = fieldTermStack.getFieldName();
 
-    LinkedList<TermInfo> phraseCandidate = new LinkedList<TermInfo>();
-    QueryPhraseMap currMap = null;
-    QueryPhraseMap nextMap = null;
-    while( !fieldTermStack.isEmpty() && (phraseList.size() < phraseLimit) )
-    {      
-      phraseCandidate.clear();
+    QueryPhraseMap qpm = fieldQuery.getRootMap(field);
+    if (qpm != null) {
+      LinkedList<TermInfo> phraseCandidate = new LinkedList<TermInfo>();
+      extractPhrases(fieldTermStack.termList, qpm, phraseCandidate, 0);
+      assert phraseCandidate.size() == 0;
+    }
+  }
 
-      TermInfo ti = fieldTermStack.pop();
-      currMap = fieldQuery.getFieldTermMap( field, ti.getText() );
+  void extractPhrases(LinkedList<TermInfo> terms, QueryPhraseMap currMap, LinkedList<TermInfo> phraseCandidate, int longest) {
+    if (terms.isEmpty()) {
+      if (longest > 0) {
+        addIfNoOverlap( new WeightedPhraseInfo( phraseCandidate.subList(0, longest), currMap.getBoost(), currMap.getTermOrPhraseNumber() ) );
+      }
+      return;
+    }
+    ArrayList<TermInfo> samePositionTerms = new ArrayList<TermInfo>();
+    do {
+      samePositionTerms.add(terms.pop());
+    } while (!terms.isEmpty() && terms.get(0).getPosition() == samePositionTerms.get(0).getPosition());
 
-      // if not found, discard top TermInfo from stack, then try next element
-      if( currMap == null ) continue;
-      
-      // if found, search the longest phrase
-      phraseCandidate.add( ti );
-      while( true ){
-        ti = fieldTermStack.pop();
-        nextMap = null;
-        if( ti != null )
-          nextMap = currMap.getTermMap( ti.getText() );
-        if( ti == null || nextMap == null ){
-          if( ti != null ) 
-            fieldTermStack.push( ti );
-          if( currMap.isValidTermOrPhrase( phraseCandidate ) ){
-            addIfNoOverlap( new WeightedPhraseInfo( phraseCandidate, currMap.getBoost(), currMap.getTermOrPhraseNumber() ) );
-          }
-          else{
-            while( phraseCandidate.size() > 1 ){
-              fieldTermStack.push( phraseCandidate.removeLast() );
-              currMap = fieldQuery.searchPhrase( field, phraseCandidate );
-              if( currMap != null ){
-                addIfNoOverlap( new WeightedPhraseInfo( phraseCandidate, currMap.getBoost(), currMap.getTermOrPhraseNumber() ) );
-                break;
-              }
-            }
-          }
-          break;
-        }
-        else{
-          phraseCandidate.add( ti );
-          currMap = nextMap;
+    // try all next terms at the same position
+    for (TermInfo nextTerm : samePositionTerms) {
+      QueryPhraseMap nextMap = currMap.getTermMap(nextTerm.getText());
+      if (nextMap != null) {
+        phraseCandidate.add(nextTerm);
+        int l = longest;
+        if(nextMap.isValidTermOrPhrase( phraseCandidate ) ){
+          l = phraseCandidate.size();
         }
+        extractPhrases(terms, nextMap, phraseCandidate, l);
+        phraseCandidate.removeLast();
       }
     }
+
+    // ignore the next term
+    extractPhrases(terms, currMap, phraseCandidate, longest);
+
+    // add terms back
+    for (TermInfo nextTerm : samePositionTerms) {
+      terms.push(nextTerm);
+    }
   }
 
   public void addIfNoOverlap( WeightedPhraseInfo wpi ){
@@ -159,11 +156,11 @@ public class FieldPhraseList {
       return termsInfos;
     }
 
-    public WeightedPhraseInfo( LinkedList<TermInfo> terms, float boost ){
+    public WeightedPhraseInfo( List<TermInfo> terms, float boost ){
       this( terms, boost, 0 );
     }
     
-    public WeightedPhraseInfo( LinkedList<TermInfo> terms, float boost, int seqnum ){
+    public WeightedPhraseInfo( List<TermInfo> terms, float boost, int seqnum ){
       this.boost = boost;
       this.seqnum = seqnum;
       
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java b/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java
index c028479..5333862 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java
@@ -17,6 +17,8 @@ package org.apache.lucene.search.vectorhighlight;
  */
 
 import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.Collection;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -39,6 +41,7 @@ import org.apache.lucene.search.PhraseQuery;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.vectorhighlight.FieldTermStack.TermInfo;
+import org.apache.lucene.util.InPlaceMergeSorter;
 
 /**
  * FieldQuery breaks down query object into terms/phrases and keeps
@@ -330,7 +333,8 @@ public class FieldQuery {
     return root.searchPhrase( phraseCandidate );
   }
   
-  private QueryPhraseMap getRootMap( String fieldName ){
+  /** Get the root map for the given field name. */
+  public QueryPhraseMap getRootMap( String fieldName ){
     return rootMaps.get( fieldMatch ? fieldName : null );
   }
   
@@ -347,6 +351,7 @@ public class FieldQuery {
     boolean terminal;
     int slop;   // valid if terminal == true and phraseHighlight == true
     float boost;  // valid if terminal == true
+    int[] positions; // valid if terminal == true
     int termOrPhraseNumber;   // valid if terminal == true
     FieldQuery fieldQuery;
     Map<String, QueryPhraseMap> subMap = new HashMap<String, QueryPhraseMap>();
@@ -369,38 +374,107 @@ public class FieldQuery {
       return map;
     }
 
-      void add( Query query, IndexReader reader ) {
+    void add( Query query, IndexReader reader ) {
       if( query instanceof TermQuery ){
         addTerm( ((TermQuery)query).getTerm(), query.getBoost() );
       }
       else if( query instanceof PhraseQuery ){
         PhraseQuery pq = (PhraseQuery)query;
-        Term[] terms = pq.getTerms();
-        Map<String, QueryPhraseMap> map = subMap;
-        QueryPhraseMap qpm = null;
-        for( Term term : terms ){
-          qpm = getOrNewMap( map, term.text() );
-          map = qpm.subMap;
-        }
-        qpm.markTerminal( pq.getSlop(), pq.getBoost() );
+        final Term[] terms = pq.getTerms();
+        final int[] positions = pq.getPositions();
+        new InPlaceMergeSorter() {
+
+          @Override
+          protected void swap(int i, int j) {
+            Term tmpTerm = terms[i];
+            terms[i] = terms[j];
+            terms[j] = tmpTerm;
+
+            int tmpPos = positions[i];
+            positions[i] = positions[j];
+            positions[j] = tmpPos;
+          }
+
+          @Override
+          protected int compare(int i, int j) {
+            return positions[i] - positions[j];
+          }
+        }.sort(0, terms.length);
+
+        addToMap(pq, terms, positions, 0, subMap, pq.getSlop());
       }
       else
         throw new RuntimeException( "query \"" + query.toString() + "\" must be flatten first." );
     }
-    
+
+    private int numTermsAtSamePosition(int[] positions, int i) {
+      int numTermsAtSamePosition = 1;
+      for (int j = i + 1; j < positions.length; ++j) {
+        if (positions[j] == positions[i]) {
+          ++numTermsAtSamePosition;
+        }
+      }
+      return numTermsAtSamePosition;
+    }
+
+    private void addToMap(PhraseQuery pq, Term[] terms, int[] positions, int i, Map<String, QueryPhraseMap> map, int slop) {
+      int numTermsAtSamePosition = numTermsAtSamePosition(positions, i);
+      for (int j = 0; j < numTermsAtSamePosition; ++j) {
+        QueryPhraseMap qpm = getOrNewMap(map, terms[i + j].text());
+        if (i + numTermsAtSamePosition == terms.length) {
+          qpm.markTerminal(pq.getSlop(), pq.getBoost(), uniquePositions(positions));
+        } else {
+          addToMap(pq, terms, positions, i + numTermsAtSamePosition, qpm.subMap, slop);
+        }
+      }
+      if (slop > 2 && i + numTermsAtSamePosition < terms.length) {
+        Term[] otherTerms = Arrays.copyOf(terms, terms.length);
+        int[] otherPositions = Arrays.copyOf(positions, positions.length);
+        final int nextTermAtSamePosition = numTermsAtSamePosition(positions, i + numTermsAtSamePosition);
+        System.arraycopy(terms, i + numTermsAtSamePosition, otherTerms, i, nextTermAtSamePosition);
+        System.arraycopy(positions, i + numTermsAtSamePosition, otherPositions, i, nextTermAtSamePosition);
+        System.arraycopy(terms, i, otherTerms, i + nextTermAtSamePosition, numTermsAtSamePosition);
+        System.arraycopy(positions, i, otherPositions, i + nextTermAtSamePosition, numTermsAtSamePosition);
+        addToMap(pq, otherTerms, otherPositions, i, map, slop - 2);
+      }
+    }
+
+    private int[] uniquePositions(int[] positions) {
+      int uniqueCount = 1;
+      for (int i = 1; i < positions.length; ++i) {
+        if (positions[i] != positions[i - 1]) {
+          ++uniqueCount;
+        }
+      }
+      if (uniqueCount == positions.length) {
+        return positions;
+      }
+      int[] result = new int[uniqueCount];
+      result[0] = positions[0];
+      for (int i = 1, j = 1; i < positions.length; ++i) {
+        if (positions[i] != positions[i - 1]) {
+          result[j++] = positions[i];
+        }
+      }
+      return result;
+    }
+
     public QueryPhraseMap getTermMap( String term ){
       return subMap.get( term );
     }
     
     private void markTerminal( float boost ){
-      markTerminal( 0, boost );
+      markTerminal( 0, boost, null );
     }
     
-    private void markTerminal( int slop, float boost ){
-      this.terminal = true;
-      this.slop = slop;
-      this.boost = boost;
-      this.termOrPhraseNumber = fieldQuery.nextTermOrPhraseNumber();
+    private void markTerminal( int slop, float boost, int[] positions ){
+      if (slop > this.slop || (slop == this.slop && boost > this.boost)) {
+        this.terminal = true;
+        this.slop = slop;
+        this.boost = boost;
+        this.termOrPhraseNumber = fieldQuery.nextTermOrPhraseNumber();
+        this.positions = positions;
+      }
     }
     
     public boolean isTerminal(){
@@ -435,15 +509,20 @@ public class FieldQuery {
       // if the candidate is a term, it is valid
       if( phraseCandidate.size() == 1 ) return true;
 
+      
+      assert phraseCandidate.size() == positions.length;
       // else check whether the candidate is valid phrase
       // compare position-gaps between terms to slop
       int pos = phraseCandidate.get( 0 ).getPosition();
+      int totalDistance = 0;
       for( int i = 1; i < phraseCandidate.size(); i++ ){
         int nextPos = phraseCandidate.get( i ).getPosition();
-        if( Math.abs( nextPos - pos - 1 ) > slop ) return false;
+        final int expectedDelta = positions[i] - positions[i - 1];
+        final int actualDelta = nextPos - pos;
+        totalDistance += Math.abs(expectedDelta - actualDelta);
         pos = nextPos;
       }
-      return true;
+      return totalDistance <= slop;
     }
   }
 }
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java b/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
index 1375cfd..51998aa 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
@@ -145,6 +145,13 @@ public class FieldTermStack {
   }
 
   /**
+   * Return the top TermInfo object of the stack without removing it.
+   */
+  public TermInfo peek() {
+    return termList.peek();
+  }
+
+  /**
    * @param termInfo the TermInfo object to be put on the top of the stack
    */
   public void push( TermInfo termInfo ){
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighter.java b/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighter.java
index f72c59c..10e2c5f 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighter.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighter.java
@@ -87,6 +87,81 @@ public class TestPostingsHighlighter extends LuceneTestCase {
     dir.close();
   }
   
+  public void testFormatWithMatchExceedingContentLength() throws Exception {
+          
+    int maxLength = 17;
+    String bodyText = "123 5678 01234 TEST";
+    
+    final Analyzer analyzer = new MockAnalyzer(random());
+    
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer);
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    
+    final FieldType fieldType = new FieldType(TextField.TYPE_STORED);
+    fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    final Field body = new Field("body", bodyText, fieldType);
+    
+    Document doc = new Document();
+    doc.add(body);
+    
+    iw.addDocument(doc);
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    
+    Query query = new TermQuery(new Term("body", "test"));
+    
+    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
+    assertEquals(1, topDocs.totalHits);
+    
+    PostingsHighlighter highlighter = new PostingsHighlighter(maxLength);
+    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
+    
+    
+    assertEquals(1, snippets.length);
+    // LUCENE-5166: no snippet
+    assertEquals("123 5678 01234 TE", snippets[0]);
+    
+    ir.close();
+    dir.close();
+  }
+  
+  // simple test highlighting last word.
+  public void testHighlightLastWord() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    
+    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
+    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    Field body = new Field("body", "", offsetsType);
+    Document doc = new Document();
+    doc.add(body);
+    
+    body.setStringValue("This is a test");
+    iw.addDocument(doc);
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    PostingsHighlighter highlighter = new PostingsHighlighter();
+    Query query = new TermQuery(new Term("body", "test"));
+    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
+    assertEquals(1, topDocs.totalHits);
+    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
+    assertEquals(1, snippets.length);
+    assertEquals("This is a <b>test</b>", snippets[0]);
+    
+    ir.close();
+    dir.close();
+  }
+  
   // simple test with one sentence documents.
   public void testOneSentence() throws Exception {
     Directory dir = newDirectory();
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.java b/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.java
index 32fbf50..0050e73 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.java
@@ -173,6 +173,8 @@ public class TestPostingsHighlighterRanking extends LuceneTestCase {
         assertTrue(p.getNumMatches() > 0);
         assertTrue(p.getStartOffset() >= 0);
         assertTrue(p.getStartOffset() <= content.length());
+        assertTrue(p.getEndOffset() >= p.getStartOffset());
+        assertTrue(p.getEndOffset() <= content.length());
         // we use a very simple analyzer. so we can assert the matches are correct
         int lastMatchStart = -1;
         for (int i = 0; i < p.getNumMatches(); i++) {
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest.java b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest.java
index 718c7ed..137effb 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FastVectorHighlighterTest.java
@@ -16,10 +16,18 @@ package org.apache.lucene.search.vectorhighlight;
  * limitations under the License.
  */
 import java.io.IOException;
+import java.io.Reader;
+import java.util.Arrays;
 
+import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -27,12 +35,14 @@ import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.queries.CommonTermsQuery;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.store.Directory;
@@ -40,7 +50,8 @@ import org.apache.lucene.util.LuceneTestCase;
 
 
 public class FastVectorHighlighterTest extends LuceneTestCase {
-  
+
+  private static final String FIELD = "text";
   
   public void testSimpleHighlightTest() throws IOException {
     Directory dir = newDirectory();
@@ -287,4 +298,128 @@ public class FastVectorHighlighterTest extends LuceneTestCase {
     writer.close();
     dir.close();
   }
+
+  public void testOverlappingPhrases() throws IOException {
+    final Analyzer analyzer = new Analyzer() {
+
+      @Override
+      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
+        final Tokenizer source = new MockTokenizer(reader);
+        TokenStream sink = source;
+        sink = new SynonymFilter(sink);
+        return new TokenStreamComponents(source, sink);
+      }
+
+    };
+    final Directory directory = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random(), directory, analyzer);
+    Document doc = new Document();
+    FieldType withVectors = new FieldType(TextField.TYPE_STORED);
+    withVectors.setStoreTermVectors(true);
+    withVectors.setStoreTermVectorPositions(true);
+    withVectors.setStoreTermVectorOffsets(true);
+    doc.add(new Field(FIELD, "a b c", withVectors));
+    iw.addDocument(doc);
+    DirectoryReader ir = iw.getReader();
+
+    // Disjunction of two overlapping phrase queries
+    final PhraseQuery pq1 = new PhraseQuery();
+    pq1.add(new Term(FIELD, "a"), 0);
+    pq1.add(new Term(FIELD, "b"), 1);
+    pq1.add(new Term(FIELD, "c"), 2);
+
+    final PhraseQuery pq2 = new PhraseQuery();
+    pq2.add(new Term(FIELD, "a"), 0);
+    pq2.add(new Term(FIELD, "B"), 1);
+    pq2.add(new Term(FIELD, "c"), 2);
+
+    final BooleanQuery bq = new BooleanQuery();
+    bq.add(pq1, Occur.SHOULD);
+    bq.add(pq2, Occur.SHOULD);
+
+    // Single phrase query with two terms at the same position
+    final PhraseQuery pq = new PhraseQuery();
+    pq.add(new Term(FIELD, "a"), 0);
+    pq.add(new Term(FIELD, "b"), 1);
+    pq.add(new Term(FIELD, "B"), 1);
+    pq.add(new Term(FIELD, "c"), 2);
+
+    for (Query query : Arrays.asList(pq1, pq2, bq, pq)) {
+      assertEquals(1, new IndexSearcher(ir).search(bq, 1).totalHits);
+
+      FastVectorHighlighter highlighter = new FastVectorHighlighter();
+      FieldQuery fieldQuery  = highlighter.getFieldQuery(query, ir);
+      String[] bestFragments = highlighter.getBestFragments(fieldQuery, ir, 0, FIELD, 1000, 1);
+      assertEquals("<b>a b c</b>", bestFragments[0]);
+    }
+
+    ir.close();
+    iw.close();
+    directory.close();
+  }
+
+  public void testPhraseWithGap() throws IOException {
+    final Directory directory = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random(), directory, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false));
+    Document doc = new Document();
+    FieldType withVectors = new FieldType(TextField.TYPE_STORED);
+    withVectors.setStoreTermVectors(true);
+    withVectors.setStoreTermVectorPositions(true);
+    withVectors.setStoreTermVectorOffsets(true);
+    doc.add(new Field(FIELD, "a b c", withVectors));
+    iw.addDocument(doc);
+    DirectoryReader ir = iw.getReader();
+
+    final PhraseQuery pq = new PhraseQuery();
+    pq.add(new Term(FIELD, "c"), 2);
+    pq.add(new Term(FIELD, "a"), 0);
+
+    assertEquals(1, new IndexSearcher(ir).search(pq, 1).totalHits);
+
+    FastVectorHighlighter highlighter = new FastVectorHighlighter();
+    FieldQuery fieldQuery  = highlighter.getFieldQuery(pq, ir);
+    String[] bestFragments = highlighter.getBestFragments(fieldQuery, ir, 0, FIELD, 1000, 1);
+    assertEquals("<b>a</b> b <b>c</b>", bestFragments[0]);
+
+    ir.close();
+    iw.close();
+    directory.close();
+  }
+
+  // Simple token filter that adds 'B' as a synonym of 'b'
+  private static class SynonymFilter extends TokenFilter {
+
+    final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+    final PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
+
+    State pending;
+
+    protected SynonymFilter(TokenStream input) {
+      super(input);
+    }
+
+    @Override
+    public boolean incrementToken() throws IOException {
+      if (pending != null) {
+        restoreState(pending);
+        termAtt.setEmpty().append('B');
+        posIncAtt.setPositionIncrement(0);
+        pending = null;
+        return true;
+      }
+      if (!input.incrementToken()) {
+        return false;
+      }
+      if (termAtt.toString().equals("b")) {
+        pending = captureState();
+      }
+      return true;
+    }
+
+    @Override
+    public void reset() throws IOException {
+      super.reset();
+      pending = null;
+    }
+  }
 }
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java
index 45054e6..879b6b5 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java
@@ -120,7 +120,31 @@ public class FieldPhraseListTest extends AbstractTestCase {
     assertEquals( 4, fpl.phraseList.get( 0 ).getStartOffset() );
     assertEquals( 9, fpl.phraseList.get( 0 ).getEndOffset() );
   }
-  
+
+  public void testProximityPhraseReverse() throws Exception {
+    make1d1fIndex( "z a a b c" );
+    
+    FieldQuery fq = new FieldQuery( pqF( 2F, 3, "c", "a" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "ac(2.0)((4,5)(8,9))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( 4, fpl.phraseList.get( 0 ).getStartOffset() );
+    assertEquals( 9, fpl.phraseList.get( 0 ).getEndOffset() );
+  }
+
+  public void testProximityPhraseWithRepeatedTerms() throws Exception {
+    make1d1fIndex( "z a a b b z d" );
+    
+    FieldQuery fq = new FieldQuery( pqF( 2F, 2, "a", "b", "d" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "abd(2.0)((4,7)(12,13))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( 4, fpl.phraseList.get( 0 ).getStartOffset() );
+    assertEquals( 13, fpl.phraseList.get( 0 ).getEndOffset() );
+  }
+
   public void test2PhrasesOverlap() throws Exception {
     make1d1fIndex( "d a b c d" );
 
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
index f381fa7..e82cb36 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
@@ -863,8 +863,8 @@ public class FieldQueryTest extends AbstractTestCase {
     phraseCandidate.add( new TermInfo( "c", 4, 5, 4, 1 ) );
     assertNull( fq.searchPhrase( F, phraseCandidate ) );
 
-    // "a b c"~1
-    query = pqF( 1F, 1, "a", "b", "c" );
+    // "a b c"~2
+    query = pqF( 1F, 2, "a", "b", "c" );
 
     // phraseHighlight = true, fieldMatch = true
     fq = new FieldQuery( query, true, true );
diff --git a/lucene/join/src/java/org/apache/lucene/search/join/TermsIncludingScoreQuery.java b/lucene/join/src/java/org/apache/lucene/search/join/TermsIncludingScoreQuery.java
index 31eba6e..c239ce4 100644
--- a/lucene/join/src/java/org/apache/lucene/search/join/TermsIncludingScoreQuery.java
+++ b/lucene/join/src/java/org/apache/lucene/search/join/TermsIncludingScoreQuery.java
@@ -241,7 +241,7 @@ class TermsIncludingScoreQuery extends Query {
         }
 
         scoreUpto = upto;
-        if (termsEnum.seekExact(terms.get(ords[upto++], spare), true)) {
+        if (termsEnum.seekExact(terms.get(ords[upto++], spare))) {
           docsEnum = reuse = termsEnum.docs(acceptDocs, reuse, DocsEnum.FLAG_NONE);
         }
       }
@@ -337,7 +337,7 @@ class TermsIncludingScoreQuery extends Query {
       BytesRef spare = new BytesRef();
       DocsEnum docsEnum = null;
       for (int i = 0; i < terms.size(); i++) {
-        if (termsEnum.seekExact(terms.get(ords[i], spare), true)) {
+        if (termsEnum.seekExact(terms.get(ords[i], spare))) {
           docsEnum = termsEnum.docs(acceptDocs, docsEnum, DocsEnum.FLAG_NONE);
           float score = TermsIncludingScoreQuery.this.scores[ords[i]];
           for (int doc = docsEnum.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = docsEnum.nextDoc()) {
@@ -393,7 +393,7 @@ class TermsIncludingScoreQuery extends Query {
       BytesRef spare = new BytesRef();
       DocsEnum docsEnum = null;
       for (int i = 0; i < terms.size(); i++) {
-        if (termsEnum.seekExact(terms.get(ords[i], spare), true)) {
+        if (termsEnum.seekExact(terms.get(ords[i], spare))) {
           docsEnum = termsEnum.docs(acceptDocs, docsEnum, DocsEnum.FLAG_NONE);
           float score = TermsIncludingScoreQuery.this.scores[ords[i]];
           for (int doc = docsEnum.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = docsEnum.nextDoc()) {
diff --git a/lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java b/lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java
index dbbf7ba..e067f0b 100644
--- a/lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java
+++ b/lucene/join/src/test/org/apache/lucene/search/join/TestJoinUtil.java
@@ -557,7 +557,7 @@ public class TestJoinUtil extends LuceneTestCase {
             joinValues.addAll(joinValueToJoinScores.keySet());
             for (BytesRef joinValue : joinValues) {
               termsEnum = terms.iterator(termsEnum);
-              if (termsEnum.seekExact(joinValue, true)) {
+              if (termsEnum.seekExact(joinValue)) {
                 docsEnum = termsEnum.docs(slowCompositeReader.getLiveDocs(), docsEnum, DocsEnum.FLAG_NONE);
                 JoinScore joinScore = joinValueToJoinScores.get(joinValue);
 
diff --git a/lucene/licenses/morfologik-fsa-1.6.0.jar.sha1 b/lucene/licenses/morfologik-fsa-1.6.0.jar.sha1
deleted file mode 100644
index 8041cb4..0000000
--- a/lucene/licenses/morfologik-fsa-1.6.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-397a99307020797e6790f2faf8cf865983b52559
diff --git a/lucene/licenses/morfologik-fsa-1.7.1.jar.sha1 b/lucene/licenses/morfologik-fsa-1.7.1.jar.sha1
new file mode 100644
index 0000000..b71174e
--- /dev/null
+++ b/lucene/licenses/morfologik-fsa-1.7.1.jar.sha1
@@ -0,0 +1 @@
+fdf556c88d66f65440bd24024f55a52c227c0e3f
diff --git a/lucene/licenses/morfologik-polish-1.6.0.jar.sha1 b/lucene/licenses/morfologik-polish-1.6.0.jar.sha1
deleted file mode 100644
index b44ead1..0000000
--- a/lucene/licenses/morfologik-polish-1.6.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-ca0663530971b54420fc1cea00a6338f68428232
diff --git a/lucene/licenses/morfologik-polish-1.7.1.jar.sha1 b/lucene/licenses/morfologik-polish-1.7.1.jar.sha1
new file mode 100644
index 0000000..3bd0d88
--- /dev/null
+++ b/lucene/licenses/morfologik-polish-1.7.1.jar.sha1
@@ -0,0 +1 @@
+e03b9feb39f6e2c0ac7c37e220d01cdae66d3a28
diff --git a/lucene/licenses/morfologik-stemming-1.6.0.jar.sha1 b/lucene/licenses/morfologik-stemming-1.6.0.jar.sha1
deleted file mode 100644
index 4ba5467..0000000
--- a/lucene/licenses/morfologik-stemming-1.6.0.jar.sha1
+++ /dev/null
@@ -1 +0,0 @@
-8a284571bea2cdd305cd86fbac9bab6deef31c7f
diff --git a/lucene/licenses/morfologik-stemming-1.7.1.jar.sha1 b/lucene/licenses/morfologik-stemming-1.7.1.jar.sha1
new file mode 100644
index 0000000..3b53503
--- /dev/null
+++ b/lucene/licenses/morfologik-stemming-1.7.1.jar.sha1
@@ -0,0 +1 @@
+c81d6c63e22e97819063cad7f1ecd20269cba720
diff --git a/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java b/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
index c891852..8107aab 100644
--- a/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
+++ b/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
@@ -885,13 +885,13 @@ public class MemoryIndex {
     
 
       @Override
-      public boolean seekExact(BytesRef text, boolean useCache) {
+      public boolean seekExact(BytesRef text) {
         termUpto = binarySearch(text, br, 0, info.terms.size()-1, info.terms, info.sortedTerms, BytesRef.getUTF8SortedAsUnicodeComparator());
         return termUpto >= 0;
       }
 
       @Override
-      public SeekStatus seekCeil(BytesRef text, boolean useCache) {
+      public SeekStatus seekCeil(BytesRef text) {
         termUpto = binarySearch(text, br, 0, info.terms.size()-1, info.terms, info.sortedTerms, BytesRef.getUTF8SortedAsUnicodeComparator());
         if (termUpto < 0) { // not found; choose successor
           termUpto = -termUpto-1;
diff --git a/lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java b/lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
index 5d9b1d6..350e9e9 100644
--- a/lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
+++ b/lucene/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
@@ -327,7 +327,7 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
     
     // now reuse and check again
     TermsEnum te = reader.terms("foo").iterator(null);
-    assertTrue(te.seekExact(new BytesRef("bar"), true));
+    assertTrue(te.seekExact(new BytesRef("bar")));
     disi = te.docs(null, disi, DocsEnum.FLAG_NONE);
     docid = disi.docID();
     assertEquals(-1, docid);
@@ -361,7 +361,7 @@ public class MemoryIndexTest extends BaseTokenStreamTestCase {
       
       // now reuse and check again
       TermsEnum te = reader.terms("foo").iterator(null);
-      assertTrue(te.seekExact(new BytesRef("bar"), true));
+      assertTrue(te.seekExact(new BytesRef("bar")));
       disi = te.docsAndPositions(null, disi);
       docid = disi.docID();
       assertEquals(-1, docid);
diff --git a/lucene/module-build.xml b/lucene/module-build.xml
index 572de31..6c6d97a 100644
--- a/lucene/module-build.xml
+++ b/lucene/module-build.xml
@@ -132,6 +132,17 @@
     </ant>
     <property name="queryparser-javadocs.uptodate" value="true"/>
   </target>
+	
+  <property name="join.jar" value="${common.dir}/build/join/lucene-join-${version}.jar"/>
+  <target name="check-join-uptodate" unless="join.uptodate">
+    <module-uptodate name="join" jarfile="${join.jar}" property="join.uptodate"/>
+  </target>
+  <target name="jar-join" unless="join.uptodate" depends="check-join-uptodate">
+    <ant dir="${common.dir}/join" target="jar-core" inheritAll="false">
+      <propertyset refid="uptodate.and.compiled.properties"/>
+	</ant>
+	<property name="join.uptodate" value="true"/>
+  </target>	
   
   <property name="analyzers-common.jar" value="${common.dir}/build/analysis/common/lucene-analyzers-common-${version}.jar"/>
   <target name="check-analyzers-common-uptodate" unless="analyzers-common.uptodate">
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java b/lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java
index 5bdbada..c0389a7 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/CommonTermsQuery.java
@@ -74,7 +74,8 @@ public class CommonTermsQuery extends Query {
   protected final Occur highFreqOccur;
   protected float lowFreqBoost = 1.0f;
   protected float highFreqBoost = 1.0f;
-  protected float minNrShouldMatch = 0;
+  protected float lowFreqMinNrShouldMatch = 0;
+  protected float highFreqMinNrShouldMatch = 0;
   
   /**
    * Creates a new {@link CommonTermsQuery}
@@ -161,10 +162,18 @@ public class CommonTermsQuery extends Query {
   }
   
   protected int calcLowFreqMinimumNumberShouldMatch(int numOptional) {
-      if (minNrShouldMatch >= 1.0f || minNrShouldMatch == 0.0f) {
-          return (int) minNrShouldMatch;
-      }
-      return (int) (Math.round(minNrShouldMatch * numOptional));
+    return minNrShouldMatch(lowFreqMinNrShouldMatch, numOptional);
+  }
+  
+  protected int calcHighFreqMinimumNumberShouldMatch(int numOptional) {
+    return minNrShouldMatch(highFreqMinNrShouldMatch, numOptional);
+  }
+  
+  private final int minNrShouldMatch(float minNrShouldMatch, int numOptional) {
+    if (minNrShouldMatch >= 1.0f || minNrShouldMatch == 0.0f) {
+      return (int) minNrShouldMatch;
+    }
+    return (int) (Math.round(minNrShouldMatch * numOptional));
   }
   
   protected Query buildQuery(final int maxDoc,
@@ -190,11 +199,16 @@ public class CommonTermsQuery extends Query {
       }
       
     }
-    final int numLowFreqClauses = lowFreq.clauses().size(); 
+    final int numLowFreqClauses = lowFreq.clauses().size();
+    final int numHighFreqClauses = highFreq.clauses().size();
     if (lowFreqOccur == Occur.SHOULD && numLowFreqClauses > 0) {
       int minMustMatch = calcLowFreqMinimumNumberShouldMatch(numLowFreqClauses);
       lowFreq.setMinimumNumberShouldMatch(minMustMatch);
     }
+    if (highFreqOccur == Occur.SHOULD && numHighFreqClauses > 0) {
+      int minMustMatch = calcHighFreqMinimumNumberShouldMatch(numHighFreqClauses);
+      highFreq.setMinimumNumberShouldMatch(minMustMatch);
+    }
     if (lowFreq.clauses().isEmpty()) {
       /*
        * if lowFreq is empty we rewrite the high freq terms in a conjunction to
@@ -246,7 +260,7 @@ public class CommonTermsQuery extends Query {
         assert termsEnum != null;
         
         if (termsEnum == TermsEnum.EMPTY) continue;
-        if (termsEnum.seekExact(term.bytes(), false)) {
+        if (termsEnum.seekExact(term.bytes())) {
           if (termContext == null) {
             contextArray[i] = new TermContext(reader.getContext(),
                 termsEnum.termState(), context.ord, termsEnum.docFreq(),
@@ -272,7 +286,7 @@ public class CommonTermsQuery extends Query {
   }
   
   /**
-   * Specifies a minimum number of the optional BooleanClauses which must be
+   * Specifies a minimum number of the low frequent optional BooleanClauses which must be
    * satisfied in order to produce a match on the low frequency terms query
    * part. This method accepts a float value in the range [0..1) as a fraction
    * of the actual query terms in the low frequent clause or a number
@@ -287,16 +301,44 @@ public class CommonTermsQuery extends Query {
    * @param min
    *          the number of optional clauses that must match
    */
-  public void setMinimumNumberShouldMatch(float min) {
-    this.minNrShouldMatch = min;
+  public void setLowFreqMinimumNumberShouldMatch(float min) {
+    this.lowFreqMinNrShouldMatch = min;
   }
   
   /**
-   * Gets the minimum number of the optional BooleanClauses which must be
+   * Gets the minimum number of the optional low frequent BooleanClauses which must be
    * satisfied.
    */
-  public float getMinimumNumberShouldMatch() {
-    return minNrShouldMatch;
+  public float getLowFreqMinimumNumberShouldMatch() {
+    return lowFreqMinNrShouldMatch;
+  }
+  
+  /**
+   * Specifies a minimum number of the high frequent optional BooleanClauses which must be
+   * satisfied in order to produce a match on the low frequency terms query
+   * part. This method accepts a float value in the range [0..1) as a fraction
+   * of the actual query terms in the low frequent clause or a number
+   * <tt>&gt;=1</tt> as an absolut number of clauses that need to match.
+   * 
+   * <p>
+   * By default no optional clauses are necessary for a match (unless there are
+   * no required clauses). If this method is used, then the specified number of
+   * clauses is required.
+   * </p>
+   * 
+   * @param min
+   *          the number of optional clauses that must match
+   */
+  public void setHighFreqMinimumNumberShouldMatch(float min) {
+    this.highFreqMinNrShouldMatch = min;
+  }
+  
+  /**
+   * Gets the minimum number of the optional high frequent BooleanClauses which must be
+   * satisfied.
+   */
+  public float getHighFreqMinimumNumberShouldMatch() {
+    return highFreqMinNrShouldMatch;
   }
   
   @Override
@@ -308,7 +350,7 @@ public class CommonTermsQuery extends Query {
   public String toString(String field) {
     StringBuilder buffer = new StringBuilder();
     boolean needParens = (getBoost() != 1.0)
-        || (getMinimumNumberShouldMatch() > 0);
+        || (getLowFreqMinimumNumberShouldMatch() > 0);
     if (needParens) {
       buffer.append("(");
     }
@@ -321,9 +363,12 @@ public class CommonTermsQuery extends Query {
     if (needParens) {
       buffer.append(")");
     }
-    if (getMinimumNumberShouldMatch() > 0) {
+    if (getLowFreqMinimumNumberShouldMatch() > 0 || getHighFreqMinimumNumberShouldMatch() > 0) {
       buffer.append('~');
-      buffer.append(getMinimumNumberShouldMatch());
+      buffer.append("(");
+      buffer.append(getLowFreqMinimumNumberShouldMatch());
+      buffer.append(getHighFreqMinimumNumberShouldMatch());
+      buffer.append(")");
     }
     if (getBoost() != 1.0f) {
       buffer.append(ToStringUtils.boost(getBoost()));
@@ -343,7 +388,8 @@ public class CommonTermsQuery extends Query {
     result = prime * result
         + ((lowFreqOccur == null) ? 0 : lowFreqOccur.hashCode());
     result = prime * result + Float.floatToIntBits(maxTermFrequency);
-    result = prime * result + Float.floatToIntBits(minNrShouldMatch);
+    result = prime * result + Float.floatToIntBits(lowFreqMinNrShouldMatch);
+    result = prime * result + Float.floatToIntBits(highFreqMinNrShouldMatch);
     result = prime * result + ((terms == null) ? 0 : terms.hashCode());
     return result;
   }
@@ -363,7 +409,8 @@ public class CommonTermsQuery extends Query {
     if (lowFreqOccur != other.lowFreqOccur) return false;
     if (Float.floatToIntBits(maxTermFrequency) != Float
         .floatToIntBits(other.maxTermFrequency)) return false;
-    if (minNrShouldMatch != other.minNrShouldMatch) return false;
+    if (lowFreqMinNrShouldMatch != other.lowFreqMinNrShouldMatch) return false;
+    if (highFreqMinNrShouldMatch != other.highFreqMinNrShouldMatch) return false;
     if (terms == null) {
       if (other.terms != null) return false;
     } else if (!terms.equals(other.terms)) return false;
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java b/lucene/queries/src/java/org/apache/lucene/queries/CustomScoreQuery.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/TermFilter.java b/lucene/queries/src/java/org/apache/lucene/queries/TermFilter.java
new file mode 100644
index 0000000..0a61b97
--- /dev/null
+++ b/lucene/queries/src/java/org/apache/lucene/queries/TermFilter.java
@@ -0,0 +1,100 @@
+package org.apache.lucene.queries;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.util.Bits;
+
+import java.io.IOException;
+
+/**
+ * A filter that includes documents that match with a specific term.
+ */
+final public class TermFilter extends Filter {
+
+  private final Term term;
+
+  /**
+   * @param term The term documents need to have in order to be a match for this filter.
+   */
+  public TermFilter(Term term) {
+    if (term == null) {
+      throw new IllegalArgumentException("Term must not be null");
+    } else if (term.field() == null) {
+      throw new IllegalArgumentException("Field must not be null");
+    }
+    this.term = term;
+  }
+
+  /**
+   * @return The term this filter includes documents with.
+   */
+  public Term getTerm() {
+    return term;
+  }
+
+  @Override
+  public DocIdSet getDocIdSet(AtomicReaderContext context, final Bits acceptDocs) throws IOException {
+    Terms terms = context.reader().terms(term.field());
+    if (terms == null) {
+      return null;
+    }
+
+    final TermsEnum termsEnum = terms.iterator(null);
+    if (!termsEnum.seekExact(term.bytes())) {
+      return null;
+    }
+    return new DocIdSet() {
+      @Override
+      public DocIdSetIterator iterator() throws IOException {
+        return termsEnum.docs(acceptDocs, null, DocsEnum.FLAG_NONE);
+      }
+
+    };
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    TermFilter that = (TermFilter) o;
+
+    if (term != null ? !term.equals(that.term) : that.term != null) return false;
+
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    return term != null ? term.hashCode() : 0;
+  }
+
+  @Override
+  public String toString() {
+    return term.field() + ":" + term.text();
+  }
+
+}
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java b/lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java
index 6480c40..4dc8822 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/TermsFilter.java
@@ -193,7 +193,7 @@ public final class TermsFilter extends Filter {
         for (int i = termsAndField.start; i < termsAndField.end; i++) {
           spare.offset = offsets[i];
           spare.length = offsets[i+1] - offsets[i];
-          if (termsEnum.seekExact(spare, false)) { // don't use cache since we could pollute the cache here easily
+          if (termsEnum.seekExact(spare)) {
             docs = termsEnum.docs(acceptDocs, docs, DocsEnum.FLAG_NONE); // no freq since we don't need them
             if (result == null) {
               if (docs.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java b/lucene/queries/src/java/org/apache/lucene/queries/function/BoostedQuery.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java b/lucene/queries/src/java/org/apache/lucene/queries/function/docvalues/DocTermsIndexDocValues.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstNumberSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstNumberSource.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ConstValueSource.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DivFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DivFloatFunction.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DocFreqValueSource.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DoubleConstValueSource.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/DualFloatFunction.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/IDFValueSource.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java
index 1936a49..c59ef21 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/JoinDocFreqValueSource.java
@@ -69,7 +69,7 @@ public class JoinDocFreqValueSource extends FieldCacheSource {
       {
         try {
           terms.get(doc, ref);
-          if (termsEnum.seekExact(ref, true)) {
+          if (termsEnum.seekExact(ref)) {
             return termsEnum.docFreq();
           } else {
             return 0;
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxDocValueSource.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NormValueSource.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/NumDocsValueSource.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/PowFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/PowFloatFunction.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/QueryValueSource.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/RangeMapFloatFunction.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ScaleFloatFunction.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SimpleFloatFunction.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SingleFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SingleFunction.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java
old mode 100755
new mode 100644
index 12554be..5fe1af4
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TFValueSource.java
@@ -67,7 +67,7 @@ public class TFValueSource extends TermFreqValueSource {
         
         if (terms != null) {
           final TermsEnum termsEnum = terms.iterator(null);
-          if (termsEnum.seekExact(indexedBytes, false)) {
+          if (termsEnum.seekExact(indexedBytes)) {
             docs = termsEnum.docs(null, null);
           } else {
             docs = null;
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java
old mode 100755
new mode 100644
index ee6800c..3fba5f8
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/TermFreqValueSource.java
@@ -60,7 +60,7 @@ public class TermFreqValueSource extends DocFreqValueSource {
         
         if (terms != null) {
           final TermsEnum termsEnum = terms.iterator(null);
-          if (termsEnum.seekExact(indexedBytes, false)) {
+          if (termsEnum.seekExact(indexedBytes)) {
             docs = termsEnum.docs(null, null);
           } else {
             docs = null;
diff --git a/lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java b/lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
index 0e94d9d..60be3e8 100644
--- a/lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
+++ b/lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
@@ -19,6 +19,7 @@ package org.apache.lucene.queries;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Random;
@@ -146,6 +147,8 @@ public class CommonTermsQueryTest extends LuceneTestCase {
         left.add(new Term(_TestUtil.randomRealisticUnicodeString(r), _TestUtil
             .randomRealisticUnicodeString(r)));
       }
+      left.setHighFreqMinimumNumberShouldMatch(r.nextInt(4));
+      left.setLowFreqMinimumNumberShouldMatch(r.nextInt(4));
       
       r = new Random(seed);
       CommonTermsQuery right = new CommonTermsQuery(randomOccur(r),
@@ -155,6 +158,8 @@ public class CommonTermsQueryTest extends LuceneTestCase {
         right.add(new Term(_TestUtil.randomRealisticUnicodeString(r), _TestUtil
             .randomRealisticUnicodeString(r)));
       }
+      right.setHighFreqMinimumNumberShouldMatch(r.nextInt(4));
+      right.setLowFreqMinimumNumberShouldMatch(r.nextInt(4));
       QueryUtils.checkEqual(left, right);
     }
   }
@@ -200,7 +205,7 @@ public class CommonTermsQueryTest extends LuceneTestCase {
       query.add(new Term("field", "world"));
       query.add(new Term("field", "universe"));
       query.add(new Term("field", "right"));
-      query.setMinimumNumberShouldMatch(0.5f);
+      query.setLowFreqMinimumNumberShouldMatch(0.5f);
       TopDocs search = s.search(query, 10);
       assertEquals(search.totalHits, 1);
       assertEquals("0", r.document(search.scoreDocs[0].doc).get("id"));
@@ -214,7 +219,7 @@ public class CommonTermsQueryTest extends LuceneTestCase {
       query.add(new Term("field", "world"));
       query.add(new Term("field", "universe"));
       query.add(new Term("field", "right"));
-      query.setMinimumNumberShouldMatch(2.0f);
+      query.setLowFreqMinimumNumberShouldMatch(2.0f);
       TopDocs search = s.search(query, 10);
       assertEquals(search.totalHits, 1);
       assertEquals("0", r.document(search.scoreDocs[0].doc).get("id"));
@@ -229,7 +234,7 @@ public class CommonTermsQueryTest extends LuceneTestCase {
       query.add(new Term("field", "world"));
       query.add(new Term("field", "universe"));
       query.add(new Term("field", "right"));
-      query.setMinimumNumberShouldMatch(0.49f);
+      query.setLowFreqMinimumNumberShouldMatch(0.49f);
       TopDocs search = s.search(query, 10);
       assertEquals(search.totalHits, 3);
       assertEquals("0", r.document(search.scoreDocs[0].doc).get("id"));
@@ -246,14 +251,37 @@ public class CommonTermsQueryTest extends LuceneTestCase {
       query.add(new Term("field", "world"));
       query.add(new Term("field", "universe"));
       query.add(new Term("field", "right"));
-      query.setMinimumNumberShouldMatch(1.0f);
+      query.setLowFreqMinimumNumberShouldMatch(1.0f);
       TopDocs search = s.search(query, 10);
       assertEquals(search.totalHits, 3);
       assertEquals("0", r.document(search.scoreDocs[0].doc).get("id"));
       assertEquals("2", r.document(search.scoreDocs[1].doc).get("id"));
       assertEquals("3", r.document(search.scoreDocs[2].doc).get("id"));
+      assertTrue(search.scoreDocs[1].score > search.scoreDocs[2].score);
+    }
+    
+    {
+      CommonTermsQuery query = new CommonTermsQuery(Occur.SHOULD, Occur.SHOULD,
+          random().nextBoolean() ? 2.0f : 0.5f);
+      query.add(new Term("field", "is"));
+      query.add(new Term("field", "this"));
+      query.add(new Term("field", "end"));
+      query.add(new Term("field", "world"));
+      query.add(new Term("field", "universe"));
+      query.add(new Term("field", "right"));
+      query.setLowFreqMinimumNumberShouldMatch(1.0f);
+      query.setHighFreqMinimumNumberShouldMatch(4.0f);
+      TopDocs search = s.search(query, 10);
+      assertEquals(search.totalHits, 3);
+      assertEquals(search.scoreDocs[1].score, search.scoreDocs[2].score, 0.0f);
+      assertEquals("0", r.document(search.scoreDocs[0].doc).get("id"));
+      // doc 2 and 3 only get a score from low freq terms
+      assertEquals(
+          new HashSet<>(Arrays.asList("2", "3")),
+          new HashSet<>(Arrays.asList(
+              r.document(search.scoreDocs[1].doc).get("id"),
+              r.document(search.scoreDocs[2].doc).get("id"))));
     }
-   
     r.close();
     w.close();
     dir.close();
diff --git a/lucene/queries/src/test/org/apache/lucene/queries/TermFilterTest.java b/lucene/queries/src/test/org/apache/lucene/queries/TermFilterTest.java
new file mode 100644
index 0000000..e6b5277
--- /dev/null
+++ b/lucene/queries/src/test/org/apache/lucene/queries/TermFilterTest.java
@@ -0,0 +1,182 @@
+package org.apache.lucene.queries;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+
+public class TermFilterTest extends LuceneTestCase {
+
+  public void testCachability() throws Exception {
+    TermFilter a = termFilter("field1", "a");
+    HashSet<Filter> cachedFilters = new HashSet<Filter>();
+    cachedFilters.add(a);
+    assertTrue("Must be cached", cachedFilters.contains(termFilter("field1", "a")));
+    assertFalse("Must not be cached", cachedFilters.contains(termFilter("field1", "b")));
+    assertFalse("Must not be cached", cachedFilters.contains(termFilter("field2", "a")));
+  }
+
+  public void testMissingTermAndField() throws Exception {
+    String fieldName = "field1";
+    Directory rd = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), rd);
+    Document doc = new Document();
+    doc.add(newStringField(fieldName, "value1", Field.Store.NO));
+    w.addDocument(doc);
+    IndexReader reader = new SlowCompositeReaderWrapper(w.getReader());
+    assertTrue(reader.getContext() instanceof AtomicReaderContext);
+    AtomicReaderContext context = (AtomicReaderContext) reader.getContext();
+    w.close();
+
+    DocIdSet idSet = termFilter(fieldName, "value1").getDocIdSet(context, context.reader().getLiveDocs());
+    assertNotNull("must not be null", idSet);
+    DocIdSetIterator iter = idSet.iterator();
+    assertEquals(iter.nextDoc(), 0);
+    assertEquals(iter.nextDoc(), DocIdSetIterator.NO_MORE_DOCS);
+
+    idSet = termFilter(fieldName, "value2").getDocIdSet(context, context.reader().getLiveDocs());
+    assertNull("must be null", idSet);
+
+    idSet = termFilter("field2", "value1").getDocIdSet(context, context.reader().getLiveDocs());
+    assertNull("must be null", idSet);
+
+    reader.close();
+    rd.close();
+  }
+  
+  public void testRandom() throws IOException {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+    int num = atLeast(100);
+    List<Term> terms = new ArrayList<Term>();
+    for (int i = 0; i < num; i++) {
+      String field = "field" + i;
+      String string = _TestUtil.randomRealisticUnicodeString(random());
+      terms.add(new Term(field, string));
+      Document doc = new Document();
+      doc.add(newStringField(field, string, Field.Store.NO));
+      w.addDocument(doc);
+    }
+    IndexReader reader = w.getReader();
+    w.close();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    
+    int numQueries = atLeast(10);
+    for (int i = 0; i < numQueries; i++) {
+      Term term = terms.get(random().nextInt(num));
+      TopDocs queryResult = searcher.search(new TermQuery(term), reader.maxDoc());
+      
+      MatchAllDocsQuery matchAll = new MatchAllDocsQuery();
+      final TermFilter filter = termFilter(term);
+      TopDocs filterResult = searcher.search(matchAll, filter, reader.maxDoc());
+      assertEquals(filterResult.totalHits, queryResult.totalHits);
+      ScoreDoc[] scoreDocs = filterResult.scoreDocs;
+      for (int j = 0; j < scoreDocs.length; j++) {
+        assertEquals(scoreDocs[j].doc, queryResult.scoreDocs[j].doc);
+      }
+    }
+    
+    reader.close();
+    dir.close();
+  }
+  
+  public void testHashCodeAndEquals() {
+    int num = atLeast(100);
+    for (int i = 0; i < num; i++) {
+      String field1 = "field" + i;
+      String field2 = "field" + i + num;
+      String value1 = _TestUtil.randomRealisticUnicodeString(random());
+      String value2 = value1 + "x"; // this must be not equal to value1
+
+      TermFilter filter1 = termFilter(field1, value1);
+      TermFilter filter2 = termFilter(field1, value2);
+      TermFilter filter3 = termFilter(field2, value1);
+      TermFilter filter4 = termFilter(field2, value2);
+      TermFilter[] filters = new TermFilter[]{filter1, filter2, filter3, filter4};
+      for (int j = 0; j < filters.length; j++) {
+        TermFilter termFilter = filters[j];
+        for (int k = 0; k < filters.length; k++) {
+          TermFilter otherTermFilter = filters[k];
+          if (j == k) {
+            assertEquals(termFilter, otherTermFilter);
+            assertEquals(termFilter.hashCode(), otherTermFilter.hashCode());
+            assertTrue(termFilter.equals(otherTermFilter));
+          } else {
+            assertFalse(termFilter.equals(otherTermFilter));
+          }
+        }
+      }
+
+      TermFilter filter5 = termFilter(field2, value2);
+      assertEquals(filter5, filter4);
+      assertEquals(filter5.hashCode(), filter4.hashCode());
+      assertTrue(filter5.equals(filter4));
+
+      assertEquals(filter5, filter4);
+      assertTrue(filter5.equals(filter4));
+    }
+  }
+  
+  public void testNoTerms() {
+    try {
+      new TermFilter(null);
+      fail("must fail - no term!");
+    } catch (IllegalArgumentException e) {}
+    
+    try {
+      new TermFilter(new Term(null));
+      fail("must fail - no field!");
+    } catch (IllegalArgumentException e) {}
+  }
+
+  public void testToString() {
+    TermFilter termsFilter = new TermFilter(new Term("field1", "a"));
+    assertEquals("field1:a", termsFilter.toString());
+  }
+
+  private TermFilter termFilter(String field, String value) {
+    return termFilter(new Term(field, value));
+  }
+
+  private TermFilter termFilter(Term term) {
+    return new TermFilter(term);
+  }
+
+}
diff --git a/lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java b/lucene/queries/src/test/org/apache/lucene/queries/TestCustomScoreQuery.java
old mode 100755
new mode 100644
diff --git a/lucene/queries/src/test/org/apache/lucene/queries/function/TestFieldScoreQuery.java b/lucene/queries/src/test/org/apache/lucene/queries/function/TestFieldScoreQuery.java
old mode 100755
new mode 100644
diff --git a/lucene/queryparser/build.xml b/lucene/queryparser/build.xml
index d91b34f..4bd4711 100644
--- a/lucene/queryparser/build.xml
+++ b/lucene/queryparser/build.xml
@@ -152,4 +152,6 @@ import org.apache.lucene.queryparser.flexible.core.messages.*;"
     </sequential>
   </macrodef>
 
+  <target name="regenerate" depends="javacc"/>
+
 </project>
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/CharStream.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/CharStream.java
index 85b1461..2c5fcba 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/CharStream.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/CharStream.java
@@ -112,4 +112,4 @@ interface CharStream {
   void Done();
 
 }
-/* JavaCC - OriginalChecksum=c847dd1920bf7901125a7244125682ad (do not edit this line) */
+/* JavaCC - OriginalChecksum=30b94cad7b10d0d81e3a59a1083939d0 (do not edit this line) */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/ParseException.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/ParseException.java
index 750f95a..7ba0d3c 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/ParseException.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/ParseException.java
@@ -184,4 +184,4 @@ public class ParseException extends Exception {
    }
 
 }
-/* JavaCC - OriginalChecksum=61602edcb3a15810cbc58f5593eba40d (do not edit this line) */
+/* JavaCC - OriginalChecksum=b187d97d5bb75c3fc63d642c1c26ac6e (do not edit this line) */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/Token.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/Token.java
index aa57487..0e52ec2 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/Token.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/Token.java
@@ -128,4 +128,4 @@ public class Token implements java.io.Serializable {
   }
 
 }
-/* JavaCC - OriginalChecksum=c1e1418b35aa9e47ef8dc98b87423d70 (do not edit this line) */
+/* JavaCC - OriginalChecksum=405bb5d2fcd84e94ac1c8f0b12c1f914 (do not edit this line) */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/TokenMgrError.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/TokenMgrError.java
index 8a2eb7b..51712a7 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/TokenMgrError.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/TokenMgrError.java
@@ -144,4 +144,4 @@ public class TokenMgrError extends Error
     this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
   }
 }
-/* JavaCC - OriginalChecksum=0c275864a1972d9a01601ab81426872d (do not edit this line) */
+/* JavaCC - OriginalChecksum=f433e1a52b8eadbf12f3fbbbf87fd140 (do not edit this line) */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/CharStream.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/CharStream.java
index ab149a6..15b8245 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/CharStream.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/CharStream.java
@@ -112,4 +112,4 @@ interface CharStream {
   void Done();
 
 }
-/* JavaCC - OriginalChecksum=c95f1720d9b38046dc5d294b741c44cb (do not edit this line) */
+/* JavaCC - OriginalChecksum=53b2ec7502d50e2290e86187a6c01270 (do not edit this line) */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/ParseException.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/ParseException.java
index fadb9eb..4d87b83 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/ParseException.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/ParseException.java
@@ -187,4 +187,4 @@ public class ParseException extends QueryNodeParseException {
    }
 
 }
-/* JavaCC - OriginalChecksum=81401c29cf6f9909761c636b4778ccc0 (do not edit this line) */
+/* JavaCC - OriginalChecksum=4263a02db9988d7a863aa97ad2f6dc67 (do not edit this line) */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/Token.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/Token.java
index fd33402..95e66bb 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/Token.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/Token.java
@@ -128,4 +128,4 @@ public class Token implements java.io.Serializable {
   }
 
 }
-/* JavaCC - OriginalChecksum=30bbd23e0dec26f141130dc62a4f6e9d (do not edit this line) */
+/* JavaCC - OriginalChecksum=ea8b1e55950603be28e2f63dcd544ab4 (do not edit this line) */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/TokenMgrError.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/TokenMgrError.java
index c98b0d5..9207c0f 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/TokenMgrError.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/parser/TokenMgrError.java
@@ -144,4 +144,4 @@ public class TokenMgrError extends Error
     this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
   }
 }
-/* JavaCC - OriginalChecksum=3ca7fbf7de9f2424b131a5499b0a78d0 (do not edit this line) */
+/* JavaCC - OriginalChecksum=be88283d82a985d82a34dda46bcf42d5 (do not edit this line) */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/CharStream.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/CharStream.java
index 1756f86..31f3ad7 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/CharStream.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/CharStream.java
@@ -112,4 +112,4 @@ interface CharStream {
   void Done();
 
 }
-/* JavaCC - OriginalChecksum=5ca20c9145f29a0f8909470a7f949fe4 (do not edit this line) */
+/* JavaCC - OriginalChecksum=242ae59b965491e225a44534cbc73b42 (do not edit this line) */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/ParseException.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/ParseException.java
index 9060dc5..a163111 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/ParseException.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/ParseException.java
@@ -184,4 +184,4 @@ public class ParseException extends Exception {
    }
 
 }
-/* JavaCC - OriginalChecksum=be6f55e3bf157e8c96b4c06cca5ec81b (do not edit this line) */
+/* JavaCC - OriginalChecksum=bd8163f41bf2fd1bb00f025fce3dcaaf (do not edit this line) */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/Token.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/Token.java
index fd71550..d6736f8 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/Token.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/Token.java
@@ -128,4 +128,4 @@ public class Token implements java.io.Serializable {
   }
 
 }
-/* JavaCC - OriginalChecksum=db38f23b3674db52ff034369707a0ac3 (do not edit this line) */
+/* JavaCC - OriginalChecksum=f2df701e24da1cf2d025118ce6efdd2f (do not edit this line) */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/TokenMgrError.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/TokenMgrError.java
index 6834d2d..c79fc21 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/TokenMgrError.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/parser/TokenMgrError.java
@@ -144,4 +144,4 @@ public class TokenMgrError extends Error
     this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
   }
 }
-/* JavaCC - OriginalChecksum=dcdd5ccde13b91bcd8f76a86ca618852 (do not edit this line) */
+/* JavaCC - OriginalChecksum=8c69a370d9a9893140562c8bb911678c (do not edit this line) */
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/IndexAndTaxonomyReplicationHandler.java b/lucene/replicator/src/java/org/apache/lucene/replicator/IndexAndTaxonomyReplicationHandler.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/IndexAndTaxonomyRevision.java b/lucene/replicator/src/java/org/apache/lucene/replicator/IndexAndTaxonomyRevision.java
old mode 100755
new mode 100644
index dbad317..125a66c
--- a/lucene/replicator/src/java/org/apache/lucene/replicator/IndexAndTaxonomyRevision.java
+++ b/lucene/replicator/src/java/org/apache/lucene/replicator/IndexAndTaxonomyRevision.java
@@ -78,15 +78,14 @@ public class IndexAndTaxonomyRevision implements Revision {
     @Override
     protected IndexWriterConfig createIndexWriterConfig(OpenMode openMode) {
       IndexWriterConfig conf = super.createIndexWriterConfig(openMode);
-      conf.setIndexDeletionPolicy(new SnapshotDeletionPolicy(conf.getIndexDeletionPolicy()));
+      sdp = new SnapshotDeletionPolicy(conf.getIndexDeletionPolicy());
+      conf.setIndexDeletionPolicy(sdp);
       return conf;
     }
     
     @Override
     protected IndexWriter openIndexWriter(Directory directory, IndexWriterConfig config) throws IOException {
       writer = super.openIndexWriter(directory, config);
-      // must set it here because IndexWriter clones the config
-      sdp = (SnapshotDeletionPolicy) writer.getConfig().getIndexDeletionPolicy();
       return writer;
     }
     
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/IndexInputInputStream.java b/lucene/replicator/src/java/org/apache/lucene/replicator/IndexInputInputStream.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/IndexReplicationHandler.java b/lucene/replicator/src/java/org/apache/lucene/replicator/IndexReplicationHandler.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/IndexRevision.java b/lucene/replicator/src/java/org/apache/lucene/replicator/IndexRevision.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/LocalReplicator.java b/lucene/replicator/src/java/org/apache/lucene/replicator/LocalReplicator.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/PerSessionDirectoryFactory.java b/lucene/replicator/src/java/org/apache/lucene/replicator/PerSessionDirectoryFactory.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java b/lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/Replicator.java b/lucene/replicator/src/java/org/apache/lucene/replicator/Replicator.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/Revision.java b/lucene/replicator/src/java/org/apache/lucene/replicator/Revision.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/RevisionFile.java b/lucene/replicator/src/java/org/apache/lucene/replicator/RevisionFile.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/SessionExpiredException.java b/lucene/replicator/src/java/org/apache/lucene/replicator/SessionExpiredException.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/SessionToken.java b/lucene/replicator/src/java/org/apache/lucene/replicator/SessionToken.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/http/HttpClientBase.java b/lucene/replicator/src/java/org/apache/lucene/replicator/http/HttpClientBase.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/http/HttpReplicator.java b/lucene/replicator/src/java/org/apache/lucene/replicator/http/HttpReplicator.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/java/org/apache/lucene/replicator/http/ReplicationService.java b/lucene/replicator/src/java/org/apache/lucene/replicator/http/ReplicationService.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java b/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyRevisionTest.java b/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyRevisionTest.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/IndexReplicationClientTest.java b/lucene/replicator/src/test/org/apache/lucene/replicator/IndexReplicationClientTest.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/IndexRevisionTest.java b/lucene/replicator/src/test/org/apache/lucene/replicator/IndexRevisionTest.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/LocalReplicatorTest.java b/lucene/replicator/src/test/org/apache/lucene/replicator/LocalReplicatorTest.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/ReplicatorTestCase.java b/lucene/replicator/src/test/org/apache/lucene/replicator/ReplicatorTestCase.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/SessionTokenTest.java b/lucene/replicator/src/test/org/apache/lucene/replicator/SessionTokenTest.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/http/HttpReplicatorTest.java b/lucene/replicator/src/test/org/apache/lucene/replicator/http/HttpReplicatorTest.java
old mode 100755
new mode 100644
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/http/ReplicationServlet.java b/lucene/replicator/src/test/org/apache/lucene/replicator/http/ReplicationServlet.java
old mode 100755
new mode 100644
diff --git a/lucene/site/xsl/index.xsl b/lucene/site/xsl/index.xsl
index a5e0ec4..9a7235b 100644
--- a/lucene/site/xsl/index.xsl
+++ b/lucene/site/xsl/index.xsl
@@ -36,7 +36,15 @@
       </head>
       <body>
         <div><a href="http://lucene.apache.org/core/"><img src="lucene_green_300.gif" title="Apache Lucene Logo" alt="Lucene" border="0"/></a></div>
-        <h1><xsl:text>Apache Lucene </xsl:text><xsl:value-of select="$version"/><xsl:text> Documentation</xsl:text></h1>
+        <h1>
+          <xsl:text>Apache Lucene</xsl:text>
+          <span style="vertical-align: top; font-size: x-small">
+            <xsl:text>TM</xsl:text>
+          </span>
+          <xsl:text> </xsl:text>
+          <xsl:value-of select="$version"/>
+          <xsl:text> Documentation</xsl:text>
+        </h1>
         <p>Lucene is a Java full-text search engine. Lucene is not a complete application, 
         but rather a code library and API that can easily be used to add search capabilities
         to applications.</p>
@@ -71,7 +79,6 @@
             <li><a href="core/org/apache/lucene/search/package-summary.html#package_description">Search and Scoring in Lucene</a>: Introduction to how Lucene scores documents.</li>
             <li><a href="core/org/apache/lucene/search/similarities/TFIDFSimilarity.html">Classic Scoring Formula</a>: Formula of Lucene's classic <a href="http://en.wikipedia.org/wiki/Vector_Space_Model">Vector Space</a> implementation. (look <a href="core/org/apache/lucene/search/similarities/package-summary.html#package_description">here</a> for other models)</li>
             <li><a href="queryparser/org/apache/lucene/queryparser/classic/package-summary.html#package_description">Classic QueryParser Syntax</a>: Overview of the Classic QueryParser's syntax and features.</li>
-            <li><a href="facet/org/apache/lucene/facet/doc-files/userguide.html">Facet User Guide</a>: User's Guide to implementing <a href="http://en.wikipedia.org/wiki/Faceted_search">Faceted search</a>.</li>
           </ul>
         <h2>API Javadocs</h2>
         <xsl:call-template name="modules"/>
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java b/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java
index de09406..f229c87 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java
@@ -102,11 +102,20 @@ public abstract class SpatialStrategy {
   public abstract Field[] createIndexableFields(Shape shape);
 
   /**
+   * See {@link #makeDistanceValueSource(com.spatial4j.core.shape.Point, double)} called with
+   * a multiplier of 1.0 (i.e. units of degrees).
+   */
+  public ValueSource makeDistanceValueSource(Point queryPoint) {
+    return makeDistanceValueSource(queryPoint, 1.0);
+  }
+
+  /**
    * Make a ValueSource returning the distance between the center of the
    * indexed shape and {@code queryPoint}.  If there are multiple indexed shapes
-   * then the closest one is chosen.
+   * then the closest one is chosen. The result is multiplied by {@code multiplier}, which
+   * conveniently is used to get the desired units.
    */
-  public abstract ValueSource makeDistanceValueSource(Point queryPoint);
+  public abstract ValueSource makeDistanceValueSource(Point queryPoint, double multiplier);
 
   /**
    * Make a Query based principally on {@link org.apache.lucene.spatial.query.SpatialOperation}
@@ -139,7 +148,7 @@ public abstract class SpatialStrategy {
 
   /**
    * Returns a ValueSource with values ranging from 1 to 0, depending inversely
-   * on the distance from {@link #makeDistanceValueSource(com.spatial4j.core.shape.Point)}.
+   * on the distance from {@link #makeDistanceValueSource(com.spatial4j.core.shape.Point,double)}.
    * The formula is {@code c/(d + c)} where 'd' is the distance and 'c' is
    * one tenth the distance to the farthest edge from the center. Thus the
    * scores will be 1 for indexed points at the center of the query shape and as
@@ -151,7 +160,7 @@ public abstract class SpatialStrategy {
         ctx.makePoint(bbox.getMinX(), bbox.getMinY()), bbox.getMaxX(), bbox.getMaxY());
     double distToEdge = diagonalDist * 0.5;
     float c = (float)distToEdge * 0.1f;//one tenth
-    return new ReciprocalFloatFunction(makeDistanceValueSource(queryShape.getCenter()), 1f, c, c);
+    return new ReciprocalFloatFunction(makeDistanceValueSource(queryShape.getCenter(), 1.0), 1f, c, c);
   }
 
   @Override
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java
index 7b2204f..ced3f65 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java
@@ -135,9 +135,9 @@ public class BBoxStrategy extends SpatialStrategy {
   //---------------------------------
 
   @Override
-  public ValueSource makeDistanceValueSource(Point queryPoint) {
+  public ValueSource makeDistanceValueSource(Point queryPoint, double multiplier) {
     return new BBoxSimilarityValueSource(
-        this, new DistanceSimilarity(this.getSpatialContext(), queryPoint));
+        this, new DistanceSimilarity(this.getSpatialContext(), queryPoint, multiplier));
   }
 
   public ValueSource makeBBoxAreaSimilarityValueSource(Rectangle queryBox) {
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/DistanceSimilarity.java b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/DistanceSimilarity.java
index 81af9b0..f2a4402 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/DistanceSimilarity.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/DistanceSimilarity.java
@@ -30,13 +30,15 @@ import org.apache.lucene.search.Explanation;
  */
 public class DistanceSimilarity implements BBoxSimilarity {
   private final Point queryPoint;
+  private final double multiplier;
   private final DistanceCalculator distCalc;
   private final double nullValue;
 
-  public DistanceSimilarity(SpatialContext ctx, Point queryPoint) {
+  public DistanceSimilarity(SpatialContext ctx, Point queryPoint, double multiplier) {
     this.queryPoint = queryPoint;
+    this.multiplier = multiplier;
     this.distCalc = ctx.getDistCalc();
-    this.nullValue = (ctx.isGeo() ? 180 : Double.MAX_VALUE);
+    this.nullValue = (ctx.isGeo() ? 180 * multiplier : Double.MAX_VALUE);
   }
 
   @Override
@@ -45,14 +47,43 @@ public class DistanceSimilarity implements BBoxSimilarity {
     if (indexRect == null) {
       score = nullValue;
     } else {
-      score = distCalc.distance(queryPoint, indexRect.getCenter());
+      score = distCalc.distance(queryPoint, indexRect.getCenter()) * multiplier;
     }
     if (exp != null) {
       exp.setValue((float)score);
       exp.setDescription(this.getClass().getSimpleName());
-      exp.addDetail(new Explanation(-1f,""+queryPoint));
+      exp.addDetail(new Explanation(-1f, "" + queryPoint));
       exp.addDetail(new Explanation(-1f,""+indexRect));
+      exp.addDetail(new Explanation((float)multiplier,"multiplier"));
     }
     return score;
   }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    DistanceSimilarity that = (DistanceSimilarity) o;
+
+    if (Double.compare(that.multiplier, multiplier) != 0) return false;
+    if (Double.compare(that.nullValue, nullValue) != 0) return false;
+    if (!distCalc.equals(that.distCalc)) return false;
+    if (!queryPoint.equals(that.queryPoint)) return false;
+
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    int result;
+    long temp;
+    result = queryPoint.hashCode();
+    temp = Double.doubleToLongBits(multiplier);
+    result = 31 * result + (int) (temp ^ (temp >>> 32));
+    result = 31 * result + distCalc.hashCode();
+    temp = Double.doubleToLongBits(nullValue);
+    result = 31 * result + (int) (temp ^ (temp >>> 32));
+    return result;
+  }
 }
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
index 6763c99..444762e 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
@@ -107,6 +107,14 @@ public abstract class AbstractVisitingPrefixTreeFilter extends AbstractPrefixTre
     this depth.  It would be nice if termsEnum knew how many terms
     start with the current term without having to repeatedly next() & test to find out.
 
+  * Perhaps don't do intermediate seek()'s to cells above detailLevel that have Intersects
+    relation because we won't be collecting those docs any way.  However seeking
+    does act as a short-circuit.  So maybe do some percent of the time or when the level
+    is above some threshold.
+
+  * Each shape.relate(otherShape) result could be cached since much of the same relations
+    will be invoked when multiple segments are involved.
+
   */
 
     protected final boolean hasIndexedLeaves;//if false then we can skip looking for them
@@ -171,11 +179,11 @@ public abstract class AbstractVisitingPrefixTreeFilter extends AbstractPrefixTre
         int compare = termsEnum.getComparator().compare(thisTerm, curVNodeTerm);
         if (compare > 0) {
           // leap frog (termsEnum is beyond where we would otherwise seek)
-          assert ! context.reader().terms(fieldName).iterator(null).seekExact(curVNodeTerm, false) : "should be absent";
+          assert ! context.reader().terms(fieldName).iterator(null).seekExact(curVNodeTerm) : "should be absent";
         } else {
           if (compare < 0) {
             // Seek !
-            TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(curVNodeTerm, true);
+            TermsEnum.SeekStatus seekStatus = termsEnum.seekCeil(curVNodeTerm);
             if (seekStatus == TermsEnum.SeekStatus.END)
               break; // all done
             thisTerm = termsEnum.term();
@@ -339,7 +347,7 @@ public abstract class AbstractVisitingPrefixTreeFilter extends AbstractPrefixTre
   }//class VisitorTemplate
 
   /**
-   * A Visitor Cell/Cell found via the query shape for {@link VisitorTemplate}.
+   * A visitor node/cell found via the query shape for {@link VisitorTemplate}.
    * Sometimes these are reset(cell). It's like a LinkedList node but forms a
    * tree.
    *
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/ContainsPrefixTreeFilter.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/ContainsPrefixTreeFilter.java
index bd0e954..e9ee786 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/ContainsPrefixTreeFilter.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/ContainsPrefixTreeFilter.java
@@ -131,10 +131,11 @@ public class ContainsPrefixTreeFilter extends AbstractPrefixTreeFilter {
 
     private boolean seekExact(Cell cell) throws IOException {
       assert new BytesRef(cell.getTokenBytes()).compareTo(termBytes) > 0;
-
       termBytes.bytes = cell.getTokenBytes();
       termBytes.length = termBytes.bytes.length;
-      return termsEnum.seekExact(termBytes, cell.getLevel() <= 2);
+      if (termsEnum == null)
+        return false;
+      return termsEnum.seekExact(termBytes);
     }
 
     private SmallDocSet getDocs(Cell cell, Bits acceptContains) throws IOException {
@@ -150,6 +151,8 @@ public class ContainsPrefixTreeFilter extends AbstractPrefixTreeFilter {
       assert ! leafCell.equals(lastLeaf);//don't call for same leaf again
       lastLeaf = leafCell;
 
+      if (termsEnum == null)
+        return null;
       BytesRef nextTerm = termsEnum.next();
       if (nextTerm == null) {
         termsEnum = null;//signals all done
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java
index 59f659d..297889c 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java
@@ -56,7 +56,7 @@ import java.util.concurrent.ConcurrentHashMap;
  * <li>Only {@link org.apache.lucene.spatial.query.SpatialOperation#Intersects}
  * is supported.  If only points are indexed then this is effectively equivalent
  * to IsWithin.</li>
- * <li>The strategy supports {@link #makeDistanceValueSource(com.spatial4j.core.shape.Point)}
+ * <li>The strategy supports {@link #makeDistanceValueSource(com.spatial4j.core.shape.Point,double)}
  * even for multi-valued data, so long as the indexed data is all points; the
  * behavior is undefined otherwise.  However, <em>it will likely be removed in
  * the future</em> in lieu of using another strategy with a more scalable
@@ -182,7 +182,7 @@ public abstract class PrefixTreeStrategy extends SpatialStrategy {
   }
 
   @Override
-  public ValueSource makeDistanceValueSource(Point queryPoint) {
+  public ValueSource makeDistanceValueSource(Point queryPoint, double multiplier) {
     PointPrefixTreeFieldCacheProvider p = provider.get( getFieldName() );
     if( p == null ) {
       synchronized (this) {//double checked locking idiom is okay since provider is threadsafe
@@ -194,7 +194,7 @@ public abstract class PrefixTreeStrategy extends SpatialStrategy {
       }
     }
 
-    return new ShapeFieldCacheDistanceValueSource(ctx, p, queryPoint);
+    return new ShapeFieldCacheDistanceValueSource(ctx, p, queryPoint, multiplier);
   }
 
   public SpatialPrefixTree getGrid() {
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheDistanceValueSource.java b/lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheDistanceValueSource.java
index 3d7ed14..b99e9de 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheDistanceValueSource.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapeFieldCacheDistanceValueSource.java
@@ -38,14 +38,17 @@ import java.util.Map;
  */
 public class ShapeFieldCacheDistanceValueSource extends ValueSource {
 
-  private final ShapeFieldCacheProvider<Point> provider;
   private final SpatialContext ctx;
   private final Point from;
+  private final ShapeFieldCacheProvider<Point> provider;
+  private final double multiplier;
 
-  public ShapeFieldCacheDistanceValueSource(SpatialContext ctx, ShapeFieldCacheProvider<Point> provider, Point from) {
+  public ShapeFieldCacheDistanceValueSource(SpatialContext ctx,
+      ShapeFieldCacheProvider<Point> provider, Point from, double multiplier) {
     this.ctx = ctx;
     this.from = from;
     this.provider = provider;
+    this.multiplier = multiplier;
   }
 
   @Override
@@ -60,7 +63,7 @@ public class ShapeFieldCacheDistanceValueSource extends ValueSource {
           provider.getCache(readerContext.reader());
       private final Point from = ShapeFieldCacheDistanceValueSource.this.from;
       private final DistanceCalculator calculator = ctx.getDistCalc();
-      private final double nullValue = (ctx.isGeo() ? 180 : Double.MAX_VALUE);
+      private final double nullValue = (ctx.isGeo() ? 180 * multiplier : Double.MAX_VALUE);
 
       @Override
       public float floatVal(int doc) {
@@ -69,13 +72,14 @@ public class ShapeFieldCacheDistanceValueSource extends ValueSource {
 
       @Override
       public double doubleVal(int doc) {
+
         List<Point> vals = cache.getShapes( doc );
         if( vals != null ) {
           double v = calculator.distance(from, vals.get(0));
           for( int i=1; i<vals.size(); i++ ) {
             v = Math.min(v, calculator.distance(from, vals.get(i)));
           }
-          return v;
+          return v * multiplier;
         }
         return nullValue;
       }
@@ -97,6 +101,7 @@ public class ShapeFieldCacheDistanceValueSource extends ValueSource {
     if (!ctx.equals(that.ctx)) return false;
     if (!from.equals(that.from)) return false;
     if (!provider.equals(that.provider)) return false;
+    if (multiplier != that.multiplier) return false;
 
     return true;
   }
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java b/lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java
index ff223f9..ba9621d 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/vector/DistanceValueSource.java
@@ -39,13 +39,15 @@ public class DistanceValueSource extends ValueSource {
 
   private PointVectorStrategy strategy;
   private final Point from;
+  private final double multiplier;
 
   /**
    * Constructor.
    */
-  public DistanceValueSource(PointVectorStrategy strategy, Point from) {
+  public DistanceValueSource(PointVectorStrategy strategy, Point from, double multiplier) {
     this.strategy = strategy;
     this.from = from;
+    this.multiplier = multiplier;
   }
 
   /**
@@ -72,7 +74,8 @@ public class DistanceValueSource extends ValueSource {
 
       private final Point from = DistanceValueSource.this.from;
       private final DistanceCalculator calculator = strategy.getSpatialContext().getDistCalc();
-      private final double nullValue = (strategy.getSpatialContext().isGeo() ? 180 : Double.MAX_VALUE);
+      private final double nullValue =
+          (strategy.getSpatialContext().isGeo() ? 180 * multiplier : Double.MAX_VALUE);
 
       @Override
       public float floatVal(int doc) {
@@ -84,7 +87,7 @@ public class DistanceValueSource extends ValueSource {
         // make sure it has minX and area
         if (validX.get(doc)) {
           assert validY.get(doc);
-          return calculator.distance(from, ptX.get(doc), ptY.get(doc));
+          return calculator.distance(from, ptX.get(doc), ptY.get(doc)) * multiplier;
         }
         return nullValue;
       }
@@ -105,6 +108,7 @@ public class DistanceValueSource extends ValueSource {
 
     if (!from.equals(that.from)) return false;
     if (!strategy.equals(that.strategy)) return false;
+    if (multiplier != that.multiplier) return false;
 
     return true;
   }
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/vector/PointVectorStrategy.java b/lucene/spatial/src/java/org/apache/lucene/spatial/vector/PointVectorStrategy.java
index 9126226..5ef2769 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/vector/PointVectorStrategy.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/vector/PointVectorStrategy.java
@@ -120,8 +120,8 @@ public class PointVectorStrategy extends SpatialStrategy {
   }
 
   @Override
-  public ValueSource makeDistanceValueSource(Point queryPoint) {
-    return new DistanceValueSource(this, queryPoint);
+  public ValueSource makeDistanceValueSource(Point queryPoint, double multiplier) {
+    return new DistanceValueSource(this, queryPoint, multiplier);
   }
 
   @Override
diff --git a/lucene/spatial/src/test/org/apache/lucene/spatial/DistanceStrategyTest.java b/lucene/spatial/src/test/org/apache/lucene/spatial/DistanceStrategyTest.java
index 5530ac7..f410e66 100644
--- a/lucene/spatial/src/test/org/apache/lucene/spatial/DistanceStrategyTest.java
+++ b/lucene/spatial/src/test/org/apache/lucene/spatial/DistanceStrategyTest.java
@@ -34,6 +34,7 @@ import org.junit.Test;
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.List;
 
 public class DistanceStrategyTest extends StrategyTestCase {
@@ -121,6 +122,11 @@ public class DistanceStrategyTest extends StrategyTestCase {
 
   void checkDistValueSource(String ptStr, float... distances) throws IOException {
     Point pt = (Point) ctx.readShape(ptStr);
-    checkValueSource(strategy.makeDistanceValueSource(pt), distances, 1.0e-4f);
+    float multiplier = random().nextFloat() * 100f;
+    float[] dists2 = Arrays.copyOf(distances, distances.length);
+    for (int i = 0; i < dists2.length; i++) {
+      dists2[i] *= multiplier;
+    }
+    checkValueSource(strategy.makeDistanceValueSource(pt, multiplier), dists2, 1.0e-3f);
   }
 }
diff --git a/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialExample.java b/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialExample.java
index 7729d89..3f1d975 100644
--- a/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialExample.java
+++ b/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialExample.java
@@ -162,7 +162,8 @@ public class SpatialExample extends LuceneTestCase {
     //--Match all, order by distance ascending
     {
       Point pt = ctx.makePoint(60, -50);
-      ValueSource valueSource = strategy.makeDistanceValueSource(pt);//the distance (in degrees)
+      double degToKm = DistanceUtils.degrees2Dist(1, DistanceUtils.EARTH_MEAN_RADIUS_KM);
+      ValueSource valueSource = strategy.makeDistanceValueSource(pt, degToKm);//the distance (in km)
       Sort distSort = new Sort(valueSource.getSortField(false)).rewrite(indexSearcher);//false=asc dist
       TopDocs docs = indexSearcher.search(new MatchAllDocsQuery(), 10, distSort);
       assertDocMatchedIds(indexSearcher, docs, 4, 20, 2);
diff --git a/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java b/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java
index f5b1737..896cdfd 100644
--- a/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java
+++ b/lucene/spatial/src/test/org/apache/lucene/spatial/SpatialTestCase.java
@@ -58,6 +58,8 @@ public abstract class SpatialTestCase extends LuceneTestCase {
 
     directory = newDirectory();
     indexWriter = new RandomIndexWriter(random(),directory);
+    indexReader = indexWriter.getReader();
+    indexSearcher = newSearcher(indexReader);
   }
 
   @Override
diff --git a/lucene/spatial/src/test/org/apache/lucene/spatial/prefix/SpatialOpRecursivePrefixTreeTest.java b/lucene/spatial/src/test/org/apache/lucene/spatial/prefix/SpatialOpRecursivePrefixTreeTest.java
index cbec0e2..09bf8c6 100644
--- a/lucene/spatial/src/test/org/apache/lucene/spatial/prefix/SpatialOpRecursivePrefixTreeTest.java
+++ b/lucene/spatial/src/test/org/apache/lucene/spatial/prefix/SpatialOpRecursivePrefixTreeTest.java
@@ -186,6 +186,13 @@ public class SpatialOpRecursivePrefixTreeTest extends StrategyTestCase {
   }
 
   private void doTest(final SpatialOperation operation) throws IOException {
+    //first show that when there's no data, a query will result in no results
+    {
+      Query query = strategy.makeQuery(new SpatialArgs(operation, randomRectangle()));
+      SearchResults searchResults = executeQuery(query, 1);
+      assertEquals(0, searchResults.numFound);
+    }
+
     final boolean biasContains = (operation == SpatialOperation.Contains);
 
     Map<String, Shape> indexedShapes = new LinkedHashMap<String, Shape>();
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/Dictionary.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/Dictionary.java
old mode 100755
new mode 100644
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/LevensteinDistance.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/LevensteinDistance.java
old mode 100755
new mode 100644
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/LuceneDictionary.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/LuceneDictionary.java
old mode 100755
new mode 100644
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java
old mode 100755
new mode 100644
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
old mode 100755
new mode 100644
index 7aadbb6..6f5f399
--- a/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
@@ -525,7 +525,7 @@ public class SpellChecker implements java.io.Closeable {
   
           if (!isEmpty) {
             for (TermsEnum te : termsEnums) {
-              if (te.seekExact(currentTerm, false)) {
+              if (te.seekExact(currentTerm)) {
                 continue terms;
               }
             }
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/SuggestWord.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/SuggestWord.java
old mode 100755
new mode 100644
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/SuggestWordQueue.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/SuggestWordQueue.java
old mode 100755
new mode 100644
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java b/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
index b1b9b79..ba64403 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java
@@ -159,7 +159,7 @@ public class AnalyzingSuggester extends Lookup {
 
   /** Represents the separation between tokens, if
    *  PRESERVE_SEP was specified */
-  private static final int SEP_LABEL = 0xff;
+  private static final int SEP_LABEL = '\u001F';
 
   /** Marks end of the analyzed input and start of dedup
    *  byte. */
@@ -306,44 +306,14 @@ public class AnalyzingSuggester extends Lookup {
     }
   }
 
-  /** Just escapes the 0xff byte (which we still for SEP). */
-  private static final class  EscapingTokenStreamToAutomaton extends TokenStreamToAutomaton {
-
-    final BytesRef spare = new BytesRef();
-
-    @Override
-    protected BytesRef changeToken(BytesRef in) {
-      int upto = 0;
-      for(int i=0;i<in.length;i++) {
-        byte b = in.bytes[in.offset+i];
-        if (b == (byte) 0xff) {
-          if (spare.bytes.length == upto) {
-            spare.grow(upto+2);
-          }
-          spare.bytes[upto++] = (byte) 0xff;
-          spare.bytes[upto++] = b;
-        } else {
-          if (spare.bytes.length == upto) {
-            spare.grow(upto+1);
-          }
-          spare.bytes[upto++] = b;
-        }
-      }
-      spare.offset = 0;
-      spare.length = upto;
-      return spare;
-    }
+  /** Used by subclass to change the lookup automaton, if
+   *  necessary. */
+  protected Automaton convertAutomaton(Automaton a) {
+    return a;
   }
-
+  
   TokenStreamToAutomaton getTokenStreamToAutomaton() {
-    final TokenStreamToAutomaton tsta;
-    if (preserveSep) {
-      tsta = new EscapingTokenStreamToAutomaton();
-    } else {
-      // When we're not preserving sep, we don't steal 0xff
-      // byte, so we don't need to do any escaping:
-      tsta = new TokenStreamToAutomaton();
-    }
+    final TokenStreamToAutomaton tsta = new TokenStreamToAutomaton();
     tsta.setPreservePositionIncrements(preservePositionIncrements);
     return tsta;
   }
@@ -379,11 +349,14 @@ public class AnalyzingSuggester extends Lookup {
       if (cmp != 0) {
         return cmp;
       }
+      readerA.skipBytes(scratchA.length);
+      readerB.skipBytes(scratchB.length);
 
       // Next by cost:
       long aCost = readerA.readInt();
       long bCost = readerB.readInt();
-
+      assert decodeWeight(aCost) >= 0;
+      assert decodeWeight(bCost) >= 0;
       if (aCost < bCost) {
         return -1;
       } else if (aCost > bCost) {
@@ -392,27 +365,20 @@ public class AnalyzingSuggester extends Lookup {
 
       // Finally by surface form:
       if (hasPayloads) {
-        readerA.setPosition(readerA.getPosition() + scratchA.length);
         scratchA.length = readerA.readShort();
-        scratchA.offset = readerA.getPosition();
-        readerB.setPosition(readerB.getPosition() + scratchB.length);
         scratchB.length = readerB.readShort();
+        scratchA.offset = readerA.getPosition();
         scratchB.offset = readerB.getPosition();
       } else {
         scratchA.offset = readerA.getPosition();
-        scratchA.length = a.length - scratchA.offset;
         scratchB.offset = readerB.getPosition();
+        scratchA.length = a.length - scratchA.offset;
         scratchB.length = b.length - scratchB.offset;
       }
-
-      cmp = scratchA.compareTo(scratchB);
-      if (cmp != 0) {
-        return cmp;
-      }
-
-      return 0;
+   
+      return scratchA.compareTo(scratchB);
     }
-  };
+  }
 
   @Override
   public void build(TermFreqIterator iterator) throws IOException {
@@ -654,9 +620,8 @@ public class AnalyzingSuggester extends Lookup {
       }
       assert sepIndex != -1;
       spare.grow(sepIndex);
-      int payloadLen = output2.length - sepIndex - 1;
-      output2.length = sepIndex;
-      UnicodeUtil.UTF8toUTF16(output2, spare);
+      final int payloadLen = output2.length - sepIndex - 1;
+      UnicodeUtil.UTF8toUTF16(output2.bytes, output2.offset, sepIndex, spare);
       BytesRef payload = new BytesRef(payloadLen);
       System.arraycopy(output2.bytes, sepIndex+1, payload.bytes, 0, payloadLen);
       payload.length = payloadLen;
@@ -699,6 +664,14 @@ public class AnalyzingSuggester extends Lookup {
     }
 
     //System.out.println("lookup key=" + key + " num=" + num);
+    for (int i = 0; i < key.length(); i++) {
+      if (key.charAt(i) == 0x1E) {
+        throw new IllegalArgumentException("lookup key cannot contain HOLE character U+001E; this character is reserved");
+      }
+      if (key.charAt(i) == 0x1F) {
+        throw new IllegalArgumentException("lookup key cannot contain unit separator character U+001F; this character is reserved");
+      }
+    }
     final BytesRef utf8Key = new BytesRef(key);
     try {
 
@@ -720,7 +693,7 @@ public class AnalyzingSuggester extends Lookup {
 
       final List<LookupResult> results = new ArrayList<LookupResult>();
 
-      List<FSTUtil.Path<Pair<Long,BytesRef>>> prefixPaths = FSTUtil.intersectPrefixPaths(lookupAutomaton, fst);
+      List<FSTUtil.Path<Pair<Long,BytesRef>>> prefixPaths = FSTUtil.intersectPrefixPaths(convertAutomaton(lookupAutomaton), fst);
 
       if (exactFirst) {
 
@@ -864,6 +837,7 @@ public class AnalyzingSuggester extends Lookup {
     ts.close();
 
     replaceSep(automaton);
+    automaton = convertAutomaton(automaton);
 
     assert SpecialOperations.isFinite(automaton);
 
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FuzzySuggester.java b/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FuzzySuggester.java
index 2733c6b..2456a90 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FuzzySuggester.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FuzzySuggester.java
@@ -15,16 +15,15 @@ package org.apache.lucene.search.suggest.analyzing;
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-import java.io.FileOutputStream;
+
 import java.io.IOException;
-import java.io.OutputStreamWriter;
-import java.io.Writer;
 import java.util.Arrays;
 import java.util.List;
 import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.TokenStreamToAutomaton;
 import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute; // javadocs
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IntsRef;
@@ -33,6 +32,7 @@ import org.apache.lucene.util.automaton.BasicAutomata;
 import org.apache.lucene.util.automaton.BasicOperations;
 import org.apache.lucene.util.automaton.LevenshteinAutomata;
 import org.apache.lucene.util.automaton.SpecialOperations;
+import org.apache.lucene.util.automaton.UTF32ToUTF8;
 import org.apache.lucene.util.fst.FST;
 import org.apache.lucene.util.fst.PairOutputs.Pair;
 
@@ -54,6 +54,9 @@ import org.apache.lucene.util.fst.PairOutputs.Pair;
  * #DEFAULT_NON_FUZZY_PREFIX} byte is not allowed to be
  * edited.  We allow up to 1 (@link
  * #DEFAULT_MAX_EDITS} edit.
+ * If {@link #unicodeAware} parameter in the constructor is set to true, maxEdits,
+ * minFuzzyLength, transpositions and nonFuzzyPrefix are measured in Unicode code 
+ * points (actual letters) instead of bytes. 
  *
  * <p>
  * NOTE: This suggester does not boost suggestions that
@@ -66,12 +69,20 @@ import org.apache.lucene.util.fst.PairOutputs.Pair;
  * like synonyms to keep the complexity of the prefix intersection low for good
  * lookup performance. At index time, complex analyzers can safely be used.
  * </p>
+ *
+ * @lucene.experimental
  */
 public final class FuzzySuggester extends AnalyzingSuggester {
   private final int maxEdits;
   private final boolean transpositions;
   private final int nonFuzzyPrefix;
   private final int minFuzzyLength;
+  private final boolean unicodeAware;
+
+  /** Measure maxEdits, minFuzzyLength, transpositions and nonFuzzyPrefix 
+   *  parameters in Unicode code points (actual letters)
+   *  instead of bytes. */
+  public static final boolean DEFAULT_UNICODE_AWARE = false;
 
   /**
    * The default minimum length of the key passed to {@link
@@ -114,7 +125,7 @@ public final class FuzzySuggester extends AnalyzingSuggester {
    */
   public FuzzySuggester(Analyzer indexAnalyzer, Analyzer queryAnalyzer) {
     this(indexAnalyzer, queryAnalyzer, EXACT_FIRST | PRESERVE_SEP, 256, -1, DEFAULT_MAX_EDITS, DEFAULT_TRANSPOSITIONS,
-         DEFAULT_NON_FUZZY_PREFIX, DEFAULT_MIN_FUZZY_LENGTH);
+         DEFAULT_NON_FUZZY_PREFIX, DEFAULT_MIN_FUZZY_LENGTH, DEFAULT_UNICODE_AWARE);
   }
 
   /**
@@ -138,11 +149,12 @@ public final class FuzzySuggester extends AnalyzingSuggester {
    *        Levenshtein algorithm.
    * @param nonFuzzyPrefix length of common (non-fuzzy) prefix (see default {@link #DEFAULT_NON_FUZZY_PREFIX}
    * @param minFuzzyLength minimum length of lookup key before any edits are allowed (see default {@link #DEFAULT_MIN_FUZZY_LENGTH})
+   * @param unicodeAware operate Unicode code points instead of bytes.
    */
   public FuzzySuggester(Analyzer indexAnalyzer, Analyzer queryAnalyzer,
                         int options, int maxSurfaceFormsPerAnalyzedForm, int maxGraphExpansions,
                         int maxEdits, boolean transpositions, int nonFuzzyPrefix,
-                        int minFuzzyLength) {
+                        int minFuzzyLength, boolean unicodeAware) {
     super(indexAnalyzer, queryAnalyzer, options, maxSurfaceFormsPerAnalyzedForm, maxGraphExpansions);
     if (maxEdits < 0 || maxEdits > LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE) {
       throw new IllegalArgumentException("maxEdits must be between 0 and " + LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE);
@@ -158,6 +170,7 @@ public final class FuzzySuggester extends AnalyzingSuggester {
     this.transpositions = transpositions;
     this.nonFuzzyPrefix = nonFuzzyPrefix;
     this.minFuzzyLength = minFuzzyLength;
+    this.unicodeAware = unicodeAware;
   }
   
   @Override
@@ -176,7 +189,7 @@ public final class FuzzySuggester extends AnalyzingSuggester {
     // "compete") ... in which case I think the wFST needs
     // to be log weights or something ...
 
-    Automaton levA = toLevenshteinAutomata(lookupAutomaton);
+    Automaton levA = convertAutomaton(toLevenshteinAutomata(lookupAutomaton));
     /*
       Writer w = new OutputStreamWriter(new FileOutputStream("out.dot"), "UTF-8");
       w.write(levA.toDot());
@@ -186,6 +199,24 @@ public final class FuzzySuggester extends AnalyzingSuggester {
     return FSTUtil.intersectPrefixPaths(levA, fst);
   }
 
+  @Override
+  protected Automaton convertAutomaton(Automaton a) {
+    if (unicodeAware) {
+      Automaton utf8automaton = new UTF32ToUTF8().convert(a);
+      BasicOperations.determinize(utf8automaton);
+      return utf8automaton;
+    } else {
+      return a;
+    }
+  }
+
+  @Override
+  TokenStreamToAutomaton getTokenStreamToAutomaton() {
+    final TokenStreamToAutomaton tsta = super.getTokenStreamToAutomaton();
+    tsta.setUnicodeArcs(unicodeAware);
+    return tsta;
+  }
+
   Automaton toLevenshteinAutomata(Automaton automaton) {
     final Set<IntsRef> ref = SpecialOperations.getFiniteStrings(automaton, -1);
     Automaton subs[] = new Automaton[ref.size()];
@@ -203,7 +234,7 @@ public final class FuzzySuggester extends AnalyzingSuggester {
         // to allow the trailing dedup bytes to be
         // edited... but then 0 byte is "in general" allowed
         // on input (but not in UTF8).
-        LevenshteinAutomata lev = new LevenshteinAutomata(ints, 255, transpositions);
+        LevenshteinAutomata lev = new LevenshteinAutomata(ints, unicodeAware ? Character.MAX_CODE_POINT : 255, transpositions);
         Automaton levAutomaton = lev.toAutomaton(maxEdits);
         Automaton combined = BasicOperations.concatenate(Arrays.asList(prefix, levAutomaton));
         combined.setDeterministic(true); // its like the special case in concatenate itself, except we cloneExpanded already
diff --git a/lucene/suggest/src/test/org/apache/lucene/search/spell/TestSpellChecker.java b/lucene/suggest/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
old mode 100755
new mode 100644
diff --git a/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java b/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java
index 803c5d9..995f60d 100644
--- a/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java
+++ b/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java
@@ -24,13 +24,15 @@ import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
 import java.io.Reader;
-import java.io.StringReader;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.Comparator;
+import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
+import java.util.Map;
+import java.util.Random;
 import java.util.Set;
 import java.util.TreeSet;
 
@@ -48,14 +50,14 @@ import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
+import org.apache.lucene.document.Document;
 import org.apache.lucene.search.suggest.Lookup.LookupResult;
 import org.apache.lucene.search.suggest.TermFreq;
 import org.apache.lucene.search.suggest.TermFreqArrayIterator;
 import org.apache.lucene.search.suggest.TermFreqPayload;
 import org.apache.lucene.search.suggest.TermFreqPayloadArrayIterator;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LineFileDocs;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
 
@@ -63,13 +65,16 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
   
   /** this is basically the WFST test ported to KeywordAnalyzer. so it acts the same */
   public void testKeyword() throws Exception {
-    TermFreq keys[] = new TermFreq[] {
+    Iterable<TermFreq> keys = shuffle(
         new TermFreq("foo", 50),
         new TermFreq("bar", 10),
+        new TermFreq("barbar", 10),
         new TermFreq("barbar", 12),
-        new TermFreq("barbara", 6)
-    };
-    
+        new TermFreq("barbara", 6),
+        new TermFreq("bar", 5),
+        new TermFreq("barbara", 1)
+    );
+
     AnalyzingSuggester suggester = new AnalyzingSuggester(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false));
     suggester.build(new TermFreqArrayIterator(keys));
     
@@ -106,53 +111,99 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
   }
   
   public void testKeywordWithPayloads() throws Exception {
-    TermFreqPayload keys[] = new TermFreqPayload[] {
+    Iterable<TermFreqPayload> keys = shuffle(
       new TermFreqPayload("foo", 50, new BytesRef("hello")),
       new TermFreqPayload("bar", 10, new BytesRef("goodbye")),
       new TermFreqPayload("barbar", 12, new BytesRef("thank you")),
-      new TermFreqPayload("barbara", 6, new BytesRef("for all the fish"))
-    };
+      new TermFreqPayload("bar", 9, new BytesRef("should be deduplicated")),
+      new TermFreqPayload("bar", 8, new BytesRef("should also be deduplicated")),
+      new TermFreqPayload("barbara", 6, new BytesRef("for all the fish")));
     
     AnalyzingSuggester suggester = new AnalyzingSuggester(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false));
     suggester.build(new TermFreqPayloadArrayIterator(keys));
+    for (int i = 0; i < 2; i++) {
+      // top N of 2, but only foo is available
+      List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence("f", random()), false, 2);
+      assertEquals(1, results.size());
+      assertEquals("foo", results.get(0).key.toString());
+      assertEquals(50, results.get(0).value, 0.01F);
+      assertEquals(new BytesRef("hello"), results.get(0).payload);
+      
+      // top N of 1 for 'bar': we return this even though
+      // barbar is higher because exactFirst is enabled:
+      results = suggester.lookup(_TestUtil.stringToCharSequence("bar", random()), false, 1);
+      assertEquals(1, results.size());
+      assertEquals("bar", results.get(0).key.toString());
+      assertEquals(10, results.get(0).value, 0.01F);
+      assertEquals(new BytesRef("goodbye"), results.get(0).payload);
+      
+      // top N Of 2 for 'b'
+      results = suggester.lookup(_TestUtil.stringToCharSequence("b", random()), false, 2);
+      assertEquals(2, results.size());
+      assertEquals("barbar", results.get(0).key.toString());
+      assertEquals(12, results.get(0).value, 0.01F);
+      assertEquals(new BytesRef("thank you"), results.get(0).payload);
+      assertEquals("bar", results.get(1).key.toString());
+      assertEquals(10, results.get(1).value, 0.01F);
+      assertEquals(new BytesRef("goodbye"), results.get(1).payload);
+      
+      // top N of 3 for 'ba'
+      results = suggester.lookup(_TestUtil.stringToCharSequence("ba", random()), false, 3);
+      assertEquals(3, results.size());
+      assertEquals("barbar", results.get(0).key.toString());
+      assertEquals(12, results.get(0).value, 0.01F);
+      assertEquals(new BytesRef("thank you"), results.get(0).payload);
+      assertEquals("bar", results.get(1).key.toString());
+      assertEquals(10, results.get(1).value, 0.01F);
+      assertEquals(new BytesRef("goodbye"), results.get(1).payload);
+      assertEquals("barbara", results.get(2).key.toString());
+      assertEquals(6, results.get(2).value, 0.01F);
+      assertEquals(new BytesRef("for all the fish"), results.get(2).payload);
+    }
+  }
+  
+  public void testRandomRealisticKeys() throws IOException {
+    LineFileDocs lineFile = new LineFileDocs(random());
+    Map<String, Long> mapping = new HashMap<>();
+    List<TermFreq> keys = new ArrayList<>();
     
-    // top N of 2, but only foo is available
-    List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence("f", random()), false, 2);
-    assertEquals(1, results.size());
-    assertEquals("foo", results.get(0).key.toString());
-    assertEquals(50, results.get(0).value, 0.01F);
-    assertEquals(new BytesRef("hello"), results.get(0).payload);
+    int howMany = atLeast(100); // this might bring up duplicates
+    for (int i = 0; i < howMany; i++) {
+      Document nextDoc = lineFile.nextDoc();
+      String title = nextDoc.getField("title").stringValue();
+      int randomWeight = random().nextInt(100);
+      keys.add(new TermFreq(title, randomWeight));
+      if (!mapping.containsKey(title) || mapping.get(title) < randomWeight) {
+          mapping.put(title, Long.valueOf(randomWeight));
+      }
+    }
     
-    // top N of 1 for 'bar': we return this even though
-    // barbar is higher because exactFirst is enabled:
-    results = suggester.lookup(_TestUtil.stringToCharSequence("bar", random()), false, 1);
-    assertEquals(1, results.size());
-    assertEquals("bar", results.get(0).key.toString());
-    assertEquals(10, results.get(0).value, 0.01F);
-    assertEquals(new BytesRef("goodbye"), results.get(0).payload);
+    AnalyzingSuggester analyzingSuggester = new AnalyzingSuggester(new MockAnalyzer(random()));
+    analyzingSuggester.setPreservePositionIncrements(random().nextBoolean());
+    boolean doPayloads = random().nextBoolean();
+    if (doPayloads) {
+      List<TermFreqPayload> keysAndPayloads = new ArrayList<>();
+      for (TermFreq termFreq : keys) {
+        keysAndPayloads.add(new TermFreqPayload(termFreq.term, termFreq.v, new BytesRef(Long.toString(termFreq.v))));
+      }
+      analyzingSuggester.build(new TermFreqPayloadArrayIterator(keysAndPayloads));
+    } else {
+      analyzingSuggester.build(new TermFreqArrayIterator(keys));  
+    }
     
-    // top N Of 2 for 'b'
-    results = suggester.lookup(_TestUtil.stringToCharSequence("b", random()), false, 2);
-    assertEquals(2, results.size());
-    assertEquals("barbar", results.get(0).key.toString());
-    assertEquals(12, results.get(0).value, 0.01F);
-    assertEquals(new BytesRef("thank you"), results.get(0).payload);
-    assertEquals("bar", results.get(1).key.toString());
-    assertEquals(10, results.get(1).value, 0.01F);
-    assertEquals(new BytesRef("goodbye"), results.get(1).payload);
+    for (TermFreq termFreq : keys) {
+      List<LookupResult> lookup = analyzingSuggester.lookup(termFreq.term.utf8ToString(), false, keys.size());
+      for (LookupResult lookupResult : lookup) {
+        assertEquals(mapping.get(lookupResult.key), Long.valueOf(lookupResult.value));
+        if (doPayloads) {
+          assertEquals(lookupResult.payload.utf8ToString(), Long.toString(lookupResult.value));
+        } else {
+          assertNull(lookupResult.payload);
+        }
+      }
+    }
     
-    // top N of 3 for 'ba'
-    results = suggester.lookup(_TestUtil.stringToCharSequence("ba", random()), false, 3);
-    assertEquals(3, results.size());
-    assertEquals("barbar", results.get(0).key.toString());
-    assertEquals(12, results.get(0).value, 0.01F);
-    assertEquals(new BytesRef("thank you"), results.get(0).payload);
-    assertEquals("bar", results.get(1).key.toString());
-    assertEquals(10, results.get(1).value, 0.01F);
-    assertEquals(new BytesRef("goodbye"), results.get(1).payload);
-    assertEquals("barbara", results.get(2).key.toString());
-    assertEquals(6, results.get(2).value, 0.01F);
-    assertEquals(new BytesRef("for all the fish"), results.get(2).payload);
+    lineFile.close();
   }
   
   // TODO: more tests
@@ -594,7 +645,7 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
     }
   }
 
-  private static char SEP = '\uFFFF';
+  private static char SEP = '\u001F';
 
   public void testRandom() throws Exception {
 
@@ -705,9 +756,9 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
     AnalyzingSuggester suggester = new AnalyzingSuggester(a, a,
                                                           preserveSep ? AnalyzingSuggester.PRESERVE_SEP : 0, 256, -1);
     if (doPayloads) {
-      suggester.build(new TermFreqPayloadArrayIterator(payloadKeys));
+      suggester.build(new TermFreqPayloadArrayIterator(shuffle(payloadKeys)));
     } else {
-      suggester.build(new TermFreqArrayIterator(keys));
+      suggester.build(new TermFreqArrayIterator(shuffle(keys)));
     }
 
     for (String prefix : allPrefixes) {
@@ -822,82 +873,11 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
     }
   }
 
-  public void testStolenBytes() throws Exception {
-
-    // First time w/ preserveSep, second time without:
-    for(int i=0;i<2;i++) {
-      
-      final Analyzer analyzer = new Analyzer() {
-          @Override
-          protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
-            Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);
-        
-            // TokenStream stream = new SynonymFilter(tokenizer, map, true);
-            // return new TokenStreamComponents(tokenizer, new RemoveDuplicatesTokenFilter(stream));
-            return new TokenStreamComponents(tokenizer) {
-              int tokenStreamCounter = 0;
-              final TokenStream[] tokenStreams = new TokenStream[] {
-                new CannedBinaryTokenStream(new BinaryToken[] {
-                    token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),
-                  }),
-                new CannedTokenStream(new Token[] {
-                    token("a",1,1),          
-                    token("a",1,1)
-                  }),
-                new CannedTokenStream(new Token[] {
-                    token("a",1,1),
-                    token("a",1,1)
-                  }),
-                new CannedBinaryTokenStream(new BinaryToken[] {
-                    token(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61})),
-                  })
-              };
-
-              @Override
-              public TokenStream getTokenStream() {
-                TokenStream result = tokenStreams[tokenStreamCounter];
-                tokenStreamCounter++;
-                return result;
-              }
-         
-              @Override
-              protected void setReader(final Reader reader) throws IOException {
-              }
-            };
-          }
-        };
-
-      TermFreq keys[] = new TermFreq[] {
-        new TermFreq("a a", 50),
-        new TermFreq("a b", 50),
-      };
-
-      AnalyzingSuggester suggester = new AnalyzingSuggester(analyzer, analyzer, AnalyzingSuggester.EXACT_FIRST | (i==0 ? AnalyzingSuggester.PRESERVE_SEP : 0), 256, -1);
-      suggester.build(new TermFreqArrayIterator(keys));
-      List<LookupResult> results = suggester.lookup("a a", false, 5);
-      assertEquals(1, results.size());
-      assertEquals("a b", results.get(0).key);
-      assertEquals(50, results.get(0).value);
-
-      results = suggester.lookup("a a", false, 5);
-      assertEquals(1, results.size());
-      assertEquals("a a", results.get(0).key);
-      assertEquals(50, results.get(0).value);
-    }
-  }
-
   public void testMaxSurfaceFormsPerAnalyzedForm() throws Exception {
     Analyzer a = new MockAnalyzer(random());
     AnalyzingSuggester suggester = new AnalyzingSuggester(a, a, 0, 2, -1);
-
-    List<TermFreq> keys = Arrays.asList(new TermFreq[] {
-        new TermFreq("a", 40),
-        new TermFreq("a ", 50),
-        new TermFreq(" a", 60),
-      });
-
-    Collections.shuffle(keys, random());
-    suggester.build(new TermFreqArrayIterator(keys));
+    suggester.build(new TermFreqArrayIterator(shuffle(new TermFreq("a", 40),
+        new TermFreq("a ", 50), new TermFreq(" a", 60))));
 
     List<LookupResult> results = suggester.lookup("a", false, 5);
     assertEquals(2, results.size());
@@ -992,10 +972,9 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
 
     AnalyzingSuggester suggester = new AnalyzingSuggester(a, a, 0, 256, -1);
 
-    suggester.build(new TermFreqArrayIterator(new TermFreq[] {
+    suggester.build(new TermFreqArrayIterator(shuffle(
           new TermFreq("hambone", 6),
-          new TermFreq("nellie", 5),
-        }));
+          new TermFreq("nellie", 5))));
 
     List<LookupResult> results = suggester.lookup("nellie", false, 2);
     assertEquals(2, results.size());
@@ -1193,4 +1172,34 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
     suggester.build(new TermFreqArrayIterator(new TermFreq[] {new TermFreq("a", 1)}));
     assertEquals("[a/1]", suggester.lookup("a", false, 1).toString());
   }
+  
+  public void testIllegalLookupArgument() throws Exception {
+    Analyzer a = new MockAnalyzer(random());
+    AnalyzingSuggester suggester = new AnalyzingSuggester(a, a, 0, 256, -1);
+    suggester.build(new TermFreqArrayIterator(new TermFreq[] {
+        new TermFreq("а где ???и?", 7),
+    }));
+    try {
+      suggester.lookup("а\u001E", false, 3);
+      fail("should throw IllegalArgumentException");
+    } catch (IllegalArgumentException e) {
+      // expected
+    }
+    try {
+      suggester.lookup("а\u001F", false, 3);
+      fail("should throw IllegalArgumentException");
+    } catch (IllegalArgumentException e) {
+      // expected
+    }
+  }
+
+  @SafeVarargs
+  public final <T> Iterable<T> shuffle(T...values) {
+    final List<T> asList = new ArrayList<T>(values.length);
+    for (T value : values) {
+      asList.add(value);
+    }
+    Collections.shuffle(asList, random());
+    return asList;
+  }
 }
diff --git a/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java b/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java
index 39aca88..06556f5 100644
--- a/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java
+++ b/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java
@@ -60,7 +60,9 @@ public class FuzzySuggesterTest extends LuceneTestCase {
       keys.add(new TermFreq("boo" + _TestUtil.randomSimpleString(random()), 1 + random().nextInt(100)));
     }
     keys.add(new TermFreq("foo bar boo far", 12));
-    FuzzySuggester suggester = new FuzzySuggester(new MockAnalyzer(random(), MockTokenizer.KEYWORD, false));
+    MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);
+    FuzzySuggester suggester = new FuzzySuggester(analyzer, analyzer, FuzzySuggester.EXACT_FIRST | FuzzySuggester.PRESERVE_SEP, 256, -1, FuzzySuggester.DEFAULT_MAX_EDITS, FuzzySuggester.DEFAULT_TRANSPOSITIONS,
+                                                  0, FuzzySuggester.DEFAULT_MIN_FUZZY_LENGTH, FuzzySuggester.DEFAULT_UNICODE_AWARE);
     suggester.build(new TermFreqArrayIterator(keys));
     int numIters = atLeast(10);
     for (int i = 0; i < numIters; i++) {
@@ -72,6 +74,27 @@ public class FuzzySuggesterTest extends LuceneTestCase {
     }
   }
   
+  public void testNonLatinRandomEdits() throws IOException {
+    List<TermFreq> keys = new ArrayList<TermFreq>();
+    int numTerms = atLeast(100);
+    for (int i = 0; i < numTerms; i++) {
+      keys.add(new TermFreq("б??" + _TestUtil.randomSimpleString(random()), 1 + random().nextInt(100)));
+    }
+    keys.add(new TermFreq("??? ба? б?? ?а?", 12));
+    MockAnalyzer analyzer = new MockAnalyzer(random(), MockTokenizer.KEYWORD, false);
+    FuzzySuggester suggester = new FuzzySuggester(analyzer, analyzer, FuzzySuggester.EXACT_FIRST | FuzzySuggester.PRESERVE_SEP, 256, -1, FuzzySuggester.DEFAULT_MAX_EDITS, FuzzySuggester.DEFAULT_TRANSPOSITIONS,
+        0, FuzzySuggester.DEFAULT_MIN_FUZZY_LENGTH, true);
+    suggester.build(new TermFreqArrayIterator(keys));
+    int numIters = atLeast(10);
+    for (int i = 0; i < numIters; i++) {
+      String addRandomEdit = addRandomEdit("??? ба? б??", 0);
+      List<LookupResult> results = suggester.lookup(_TestUtil.stringToCharSequence(addRandomEdit, random()), false, 2);
+      assertEquals(addRandomEdit, 1, results.size());
+      assertEquals("??? ба? б?? ?а?", results.get(0).key.toString());
+      assertEquals(12, results.get(0).value, 0.01F);
+    }
+  }
+
   /** this is basically the WFST test ported to KeywordAnalyzer. so it acts the same */
   public void testKeyword() throws Exception {
     TermFreq keys[] = new TermFreq[] {
@@ -185,7 +208,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
     int options = 0;
 
     Analyzer a = new MockAnalyzer(random());
-    FuzzySuggester suggester = new FuzzySuggester(a, a, options, 256, -1, 1, true, 1, 3);
+    FuzzySuggester suggester = new FuzzySuggester(a, a, options, 256, -1, 1, true, 1, 3, false);
     suggester.build(new TermFreqArrayIterator(keys));
     // TODO: would be nice if "ab " would allow the test to
     // pass, and more generally if the analyzer can know
@@ -394,7 +417,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
   public void testExactFirst() throws Exception {
 
     Analyzer a = getUnusualAnalyzer();
-    FuzzySuggester suggester = new FuzzySuggester(a, a, AnalyzingSuggester.EXACT_FIRST | AnalyzingSuggester.PRESERVE_SEP, 256, -1, 1, true, 1, 3);
+    FuzzySuggester suggester = new FuzzySuggester(a, a, AnalyzingSuggester.EXACT_FIRST | AnalyzingSuggester.PRESERVE_SEP, 256, -1, 1, true, 1, 3, false);
     suggester.build(new TermFreqArrayIterator(new TermFreq[] {
           new TermFreq("x y", 1),
           new TermFreq("x y z", 3),
@@ -433,7 +456,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
   public void testNonExactFirst() throws Exception {
 
     Analyzer a = getUnusualAnalyzer();
-    FuzzySuggester suggester = new FuzzySuggester(a, a, AnalyzingSuggester.PRESERVE_SEP, 256, -1, 1, true, 1, 3);
+    FuzzySuggester suggester = new FuzzySuggester(a, a, AnalyzingSuggester.PRESERVE_SEP, 256, -1, 1, true, 1, 3, false);
 
     suggester.build(new TermFreqArrayIterator(new TermFreq[] {
           new TermFreq("x y", 1),
@@ -580,12 +603,13 @@ public class FuzzySuggesterTest extends LuceneTestCase {
     TermFreq[] keys = new TermFreq[numQueries];
 
     boolean preserveSep = random().nextBoolean();
+    boolean unicodeAware = random().nextBoolean();
 
     final int numStopChars = random().nextInt(10);
     final boolean preserveHoles = random().nextBoolean();
 
     if (VERBOSE) {
-      System.out.println("TEST: " + numQueries + " words; preserveSep=" + preserveSep + " numStopChars=" + numStopChars + " preserveHoles=" + preserveHoles);
+      System.out.println("TEST: " + numQueries + " words; preserveSep=" + preserveSep + " ; unicodeAware=" + unicodeAware + " numStopChars=" + numStopChars + " preserveHoles=" + preserveHoles);
     }
     
     for (int i = 0; i < numQueries; i++) {
@@ -606,7 +630,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
               if (token > 0) {
                 key += " ";
               }
-              if (preserveSep && analyzedKey.length() > 0 && analyzedKey.charAt(analyzedKey.length()-1) != ' ') {
+              if (preserveSep && analyzedKey.length() > 0 && (unicodeAware ? analyzedKey.codePointAt(analyzedKey.codePointCount(0, analyzedKey.length())-1) != ' ' : analyzedKey.charAt(analyzedKey.length()-1) != ' ')) {
                 analyzedKey += " ";
               }
               key += s;
@@ -659,7 +683,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
 
     Analyzer a = new MockTokenEatingAnalyzer(numStopChars, preserveHoles);
     FuzzySuggester suggester = new FuzzySuggester(a, a,
-                                                  preserveSep ? AnalyzingSuggester.PRESERVE_SEP : 0, 256, -1, 1, false, 1, 3);
+                                                  preserveSep ? AnalyzingSuggester.PRESERVE_SEP : 0, 256, -1, 1, false, 1, 3, unicodeAware);
     suggester.build(new TermFreqArrayIterator(keys));
 
     for (String prefix : allPrefixes) {
@@ -728,7 +752,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
       // us the "answer key" (ie maybe we have a bug in
       // suggester.toLevA ...) ... but testRandom2() fixes
       // this:
-      Automaton automaton = suggester.toLevenshteinAutomata(suggester.toLookupAutomaton(analyzedKey));
+      Automaton automaton = suggester.convertAutomaton(suggester.toLevenshteinAutomata(suggester.toLookupAutomaton(analyzedKey)));
       assertTrue(automaton.isDeterministic());
       // TODO: could be faster... but its slowCompletor for a reason
       BytesRef spare = new BytesRef();
@@ -799,7 +823,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
 
   public void testMaxSurfaceFormsPerAnalyzedForm() throws Exception {
     Analyzer a = new MockAnalyzer(random());
-    FuzzySuggester suggester = new FuzzySuggester(a, a, 0, 2, -1, 1, true, 1, 3);
+    FuzzySuggester suggester = new FuzzySuggester(a, a, 0, 2, -1, 1, true, 1, 3, false);
 
     List<TermFreq> keys = Arrays.asList(new TermFreq[] {
         new TermFreq("a", 40),
@@ -820,7 +844,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
 
   public void testEditSeps() throws Exception {
     Analyzer a = new MockAnalyzer(random());
-    FuzzySuggester suggester = new FuzzySuggester(a, a, FuzzySuggester.PRESERVE_SEP, 2, -1, 2, true, 1, 3);
+    FuzzySuggester suggester = new FuzzySuggester(a, a, FuzzySuggester.PRESERVE_SEP, 2, -1, 2, true, 1, 3, false);
 
     List<TermFreq> keys = Arrays.asList(new TermFreq[] {
         new TermFreq("foo bar", 40),
@@ -878,7 +902,8 @@ public class FuzzySuggesterTest extends LuceneTestCase {
             // NOTE: can only use ascii here so that, in
             // UTF8 byte space it's still a single
             // insertion:
-            int x = random().nextInt(128);
+            // bytes 0x1e and 0x1f are reserved
+            int x = random().nextBoolean() ? random().nextInt(30) :  32 + random().nextInt(128 - 32);
             builder.append((char) x);
             for (int j = i; j < input.length; j++) {
               builder.append(input[j]);  
@@ -933,7 +958,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
     boolean transpositions = random().nextBoolean();
     // TODO: test graph analyzers
     // TODO: test exactFirst / preserveSep permutations
-    FuzzySuggester suggest = new FuzzySuggester(a, a, 0, 256, -1, maxEdits, transpositions, prefixLen, prefixLen);
+    FuzzySuggester suggest = new FuzzySuggester(a, a, 0, 256, -1, maxEdits, transpositions, prefixLen, prefixLen, false);
 
     if (VERBOSE) {
       System.out.println("TEST: maxEdits=" + maxEdits + " prefixLen=" + prefixLen + " transpositions=" + transpositions + " num=" + NUM);
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesFormat.java
index 919b75e..07f152c 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesFormat.java
@@ -24,8 +24,10 @@ import org.apache.lucene.codecs.DocValuesProducer;
 import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.codecs.diskdv.DiskDocValuesConsumer;
 import org.apache.lucene.codecs.diskdv.DiskDocValuesFormat;
+import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.BytesRef;
 
 /**
  * DocValues format that keeps everything on disk.
@@ -53,7 +55,13 @@ public final class CheapBastardDocValuesFormat extends DocValuesFormat {
     return new DiskDocValuesConsumer(state, DiskDocValuesFormat.DATA_CODEC, 
                                             DiskDocValuesFormat.DATA_EXTENSION, 
                                             DiskDocValuesFormat.META_CODEC, 
-                                            DiskDocValuesFormat.META_EXTENSION);
+                                            DiskDocValuesFormat.META_EXTENSION) {
+      // don't ever write an index, we dont want to use RAM :)
+      @Override
+      protected void addTermsDict(FieldInfo field, Iterable<BytesRef> values) throws IOException {
+        addBinaryField(field, values);
+      }  
+    };
   }
 
   @Override
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesProducer.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesProducer.java
index aab4acc..52f36d2 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesProducer.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/cheapbastard/CheapBastardDocValuesProducer.java
@@ -27,6 +27,7 @@ import java.util.Map;
 
 import org.apache.lucene.codecs.CodecUtil;
 import org.apache.lucene.codecs.DocValuesProducer;
+import org.apache.lucene.codecs.diskdv.DiskDocValuesConsumer;
 import org.apache.lucene.codecs.diskdv.DiskDocValuesFormat;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.index.CorruptIndexException;
@@ -58,7 +59,7 @@ class CheapBastardDocValuesProducer extends DocValuesProducer {
     final int version;
     try {
       version = CodecUtil.checkHeader(in, metaCodec, 
-                                      DiskDocValuesFormat.VERSION_START,
+                                      DiskDocValuesFormat.VERSION_CURRENT,
                                       DiskDocValuesFormat.VERSION_CURRENT);
       numerics = new HashMap<Integer,NumericEntry>();
       ords = new HashMap<Integer,NumericEntry>();
@@ -80,7 +81,7 @@ class CheapBastardDocValuesProducer extends DocValuesProducer {
       String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);
       data = state.directory.openInput(dataName, state.context);
       final int version2 = CodecUtil.checkHeader(data, dataCodec, 
-                                                 DiskDocValuesFormat.VERSION_START,
+                                                 DiskDocValuesFormat.VERSION_CURRENT,
                                                  DiskDocValuesFormat.VERSION_CURRENT);
       if (version != version2) {
         throw new CorruptIndexException("Versions mismatch");
@@ -193,6 +194,10 @@ class CheapBastardDocValuesProducer extends DocValuesProducer {
   
   static BinaryEntry readBinaryEntry(IndexInput meta) throws IOException {
     BinaryEntry entry = new BinaryEntry();
+    int format = meta.readVInt();
+    if (format != DiskDocValuesConsumer.BINARY_FIXED_UNCOMPRESSED && format != DiskDocValuesConsumer.BINARY_VARIABLE_UNCOMPRESSED) {
+      throw new CorruptIndexException("Unexpected format for binary entry: " + format + ", input=" + meta);
+    }
     entry.minLength = meta.readVInt();
     entry.maxLength = meta.readVInt();
     entry.count = meta.readVLong();
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java
index d06e1aa..a6e42d5 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41ords/Lucene41WithOrds.java
@@ -45,9 +45,15 @@ import org.apache.lucene.util.BytesRef;
  * {@link FixedGapTermsIndexWriter}.
  */
 public final class Lucene41WithOrds extends PostingsFormat {
-    
+  final int termIndexInterval;
+  
   public Lucene41WithOrds() {
+    this(FixedGapTermsIndexWriter.DEFAULT_TERM_INDEX_INTERVAL);
+  }
+  
+  public Lucene41WithOrds(int termIndexInterval) {
     super("Lucene41WithOrds");
+    this.termIndexInterval = termIndexInterval;
   }
 
   @Override
@@ -61,7 +67,7 @@ public final class Lucene41WithOrds extends PostingsFormat {
     TermsIndexWriterBase indexWriter;
     boolean success = false;
     try {
-      indexWriter = new FixedGapTermsIndexWriter(state);
+      indexWriter = new FixedGapTermsIndexWriter(state, termIndexInterval);
       success = true;
     } finally {
       if (!success) {
@@ -87,8 +93,6 @@ public final class Lucene41WithOrds extends PostingsFormat {
     }
   }
 
-  public final static int TERMS_CACHE_SIZE = 1024;
-
   @Override
   public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
     PostingsReaderBase postings = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
@@ -99,7 +103,6 @@ public final class Lucene41WithOrds extends PostingsFormat {
       indexReader = new FixedGapTermsIndexReader(state.directory,
                                                  state.fieldInfos,
                                                  state.segmentInfo.name,
-                                                 state.termsIndexDivisor,
                                                  BytesRef.getUTF8SortedAsUnicodeComparator(),
                                                  state.segmentSuffix, state.context);
       success = true;
@@ -117,7 +120,6 @@ public final class Lucene41WithOrds extends PostingsFormat {
                                                 state.segmentInfo,
                                                 postings,
                                                 state.context,
-                                                TERMS_CACHE_SIZE,
                                                 state.segmentSuffix);
       success = true;
       return ret;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapDocFreqInterval.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapDocFreqInterval.java
new file mode 100644
index 0000000..9e937e1
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapDocFreqInterval.java
@@ -0,0 +1,144 @@
+package org.apache.lucene.codecs.lucene41vargap;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.blockterms.BlockTermsReader;
+import org.apache.lucene.codecs.blockterms.BlockTermsWriter;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.blockterms.TermsIndexReaderBase;
+import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
+import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexReader;
+import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexWriter;
+import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
+import org.apache.lucene.codecs.lucene41.Lucene41PostingsReader;
+import org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+// TODO: we could make separate base class that can wrapp
+// any PostingsBaseFormat and make it ord-able...
+
+/**
+ * Customized version of {@link Lucene41PostingsFormat} that uses
+ * {@link VariableGapTermsIndexWriter} with a fixed interval, but
+ * forcing high docfreq terms to be indexed terms.
+ */
+public final class Lucene41VarGapDocFreqInterval extends PostingsFormat {
+  final int termIndexInterval;
+  final int docFreqThreshold;
+  
+  public Lucene41VarGapDocFreqInterval() {
+    this(1000000, FixedGapTermsIndexWriter.DEFAULT_TERM_INDEX_INTERVAL);
+  }
+  
+  public Lucene41VarGapDocFreqInterval(int docFreqThreshold, int termIndexInterval) {
+    super("Lucene41VarGapFixedInterval");
+    this.termIndexInterval = termIndexInterval;
+    this.docFreqThreshold = docFreqThreshold;
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase docs = new Lucene41PostingsWriter(state);
+
+    // TODO: should we make the terms index more easily
+    // pluggable?  Ie so that this codec would record which
+    // index impl was used, and switch on loading?
+    // Or... you must make a new Codec for this?
+    TermsIndexWriterBase indexWriter;
+    boolean success = false;
+    try {
+      indexWriter = new VariableGapTermsIndexWriter(state, new VariableGapTermsIndexWriter.EveryNOrDocFreqTermSelector(docFreqThreshold, termIndexInterval));
+      success = true;
+    } finally {
+      if (!success) {
+        docs.close();
+      }
+    }
+
+    success = false;
+    try {
+      // Must use BlockTermsWriter (not BlockTree) because
+      // BlockTree doens't support ords (yet)...
+      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, docs);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          docs.close();
+        } finally {
+          indexWriter.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postings = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+    TermsIndexReaderBase indexReader;
+
+    boolean success = false;
+    try {
+      indexReader = new VariableGapTermsIndexReader(state.directory,
+                                                 state.fieldInfos,
+                                                 state.segmentInfo.name,
+                                                 state.segmentSuffix, state.context);
+      success = true;
+    } finally {
+      if (!success) {
+        postings.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsProducer ret = new BlockTermsReader(indexReader,
+                                                state.directory,
+                                                state.fieldInfos,
+                                                state.segmentInfo,
+                                                postings,
+                                                state.context,
+                                                state.segmentSuffix);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postings.close();
+        } finally {
+          indexReader.close();
+        }
+      }
+    }
+  }
+
+  /** Extension of freq postings file */
+  static final String FREQ_EXTENSION = "frq";
+
+  /** Extension of prox postings file */
+  static final String PROX_EXTENSION = "prx";
+}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapFixedInterval.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapFixedInterval.java
new file mode 100644
index 0000000..5d77f03
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/Lucene41VarGapFixedInterval.java
@@ -0,0 +1,141 @@
+package org.apache.lucene.codecs.lucene41vargap;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.blockterms.BlockTermsReader;
+import org.apache.lucene.codecs.blockterms.BlockTermsWriter;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.blockterms.TermsIndexReaderBase;
+import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
+import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexReader;
+import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexWriter;
+import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat; // javadocs
+import org.apache.lucene.codecs.lucene41.Lucene41PostingsReader;
+import org.apache.lucene.codecs.lucene41.Lucene41PostingsWriter;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+// TODO: we could make separate base class that can wrapp
+// any PostingsBaseFormat and make it ord-able...
+
+/**
+ * Customized version of {@link Lucene41PostingsFormat} that uses
+ * {@link VariableGapTermsIndexWriter} with a fixed interval.
+ */
+public final class Lucene41VarGapFixedInterval extends PostingsFormat {
+  final int termIndexInterval;
+  
+  public Lucene41VarGapFixedInterval() {
+    this(FixedGapTermsIndexWriter.DEFAULT_TERM_INDEX_INTERVAL);
+  }
+  
+  public Lucene41VarGapFixedInterval(int termIndexInterval) {
+    super("Lucene41VarGapFixedInterval");
+    this.termIndexInterval = termIndexInterval;
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase docs = new Lucene41PostingsWriter(state);
+
+    // TODO: should we make the terms index more easily
+    // pluggable?  Ie so that this codec would record which
+    // index impl was used, and switch on loading?
+    // Or... you must make a new Codec for this?
+    TermsIndexWriterBase indexWriter;
+    boolean success = false;
+    try {
+      indexWriter = new VariableGapTermsIndexWriter(state, new VariableGapTermsIndexWriter.EveryNTermSelector(termIndexInterval));
+      success = true;
+    } finally {
+      if (!success) {
+        docs.close();
+      }
+    }
+
+    success = false;
+    try {
+      // Must use BlockTermsWriter (not BlockTree) because
+      // BlockTree doens't support ords (yet)...
+      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, docs);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          docs.close();
+        } finally {
+          indexWriter.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postings = new Lucene41PostingsReader(state.directory, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+    TermsIndexReaderBase indexReader;
+
+    boolean success = false;
+    try {
+      indexReader = new VariableGapTermsIndexReader(state.directory,
+                                                 state.fieldInfos,
+                                                 state.segmentInfo.name,
+                                                 state.segmentSuffix, state.context);
+      success = true;
+    } finally {
+      if (!success) {
+        postings.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsProducer ret = new BlockTermsReader(indexReader,
+                                                state.directory,
+                                                state.fieldInfos,
+                                                state.segmentInfo,
+                                                postings,
+                                                state.context,
+                                                state.segmentSuffix);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postings.close();
+        } finally {
+          indexReader.close();
+        }
+      }
+    }
+  }
+
+  /** Extension of freq postings file */
+  static final String FREQ_EXTENSION = "frq";
+
+  /** Extension of prox postings file */
+  static final String PROX_EXTENSION = "prx";
+}
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/package.html b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/package.html
new file mode 100644
index 0000000..606bb06
--- /dev/null
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene41vargap/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Codecs for testing that support {@link org.apache.lucene.codecs.blockterms.VariableGapTermsIndexReader}
+</body>
+</html>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
index d5be4c3..fff571c 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
@@ -169,7 +169,6 @@ public final class MockFixedIntBlockPostingsFormat extends PostingsFormat {
       indexReader = new FixedGapTermsIndexReader(state.directory,
                                                        state.fieldInfos,
                                                        state.segmentInfo.name,
-                                                       state.termsIndexDivisor,
                                                        BytesRef.getUTF8SortedAsUnicodeComparator(), state.segmentSuffix,
                                                        IOContext.DEFAULT);
       success = true;
@@ -187,7 +186,6 @@ public final class MockFixedIntBlockPostingsFormat extends PostingsFormat {
                                                 state.segmentInfo,
                                                 postingsReader,
                                                 state.context,
-                                                1024,
                                                 state.segmentSuffix);
       success = true;
       return ret;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
index f647b85..48face9 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
@@ -194,7 +194,6 @@ public final class MockVariableIntBlockPostingsFormat extends PostingsFormat {
       indexReader = new FixedGapTermsIndexReader(state.directory,
                                                        state.fieldInfos,
                                                        state.segmentInfo.name,
-                                                       state.termsIndexDivisor,
                                                        BytesRef.getUTF8SortedAsUnicodeComparator(),
                                                        state.segmentSuffix, state.context);
       success = true;
@@ -212,7 +211,6 @@ public final class MockVariableIntBlockPostingsFormat extends PostingsFormat {
                                                 state.segmentInfo,
                                                 postingsReader,
                                                 state.context,
-                                                1024,
                                                 state.segmentSuffix);
       success = true;
       return ret;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
index 0284a44..502457f 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
@@ -220,11 +220,11 @@ public final class MockRandomPostingsFormat extends PostingsFormat {
       final TermsIndexWriterBase indexWriter;
       try {
         if (random.nextBoolean()) {
-          state.termIndexInterval = _TestUtil.nextInt(random, 1, 100);
+          int termIndexInterval = _TestUtil.nextInt(random, 1, 100);
           if (LuceneTestCase.VERBOSE) {
-            System.out.println("MockRandomCodec: fixed-gap terms index (tii=" + state.termIndexInterval + ")");
+            System.out.println("MockRandomCodec: fixed-gap terms index (tii=" + termIndexInterval + ")");
           }
-          indexWriter = new FixedGapTermsIndexWriter(state);
+          indexWriter = new FixedGapTermsIndexWriter(state, termIndexInterval);
         } else {
           final VariableGapTermsIndexWriter.IndexTermSelector selector;
           final int n2 = random.nextInt(3);
@@ -340,8 +340,7 @@ public final class MockRandomPostingsFormat extends PostingsFormat {
                                           state.segmentInfo,
                                           postingsReader,
                                           state.context,
-                                          state.segmentSuffix,
-                                          state.termsIndexDivisor);
+                                          state.segmentSuffix);
         success = true;
       } finally {
         if (!success) {
@@ -359,20 +358,14 @@ public final class MockRandomPostingsFormat extends PostingsFormat {
         final boolean doFixedGap = random.nextBoolean();
 
         // randomness diverges from writer, here:
-        if (state.termsIndexDivisor != -1) {
-          state.termsIndexDivisor = _TestUtil.nextInt(random, 1, 10);
-        }
 
         if (doFixedGap) {
-          // if termsIndexDivisor is set to -1, we should not touch it. It means a
-          // test explicitly instructed not to load the terms index.
           if (LuceneTestCase.VERBOSE) {
-            System.out.println("MockRandomCodec: fixed-gap terms index (divisor=" + state.termsIndexDivisor + ")");
+            System.out.println("MockRandomCodec: fixed-gap terms index");
           }
           indexReader = new FixedGapTermsIndexReader(state.directory,
                                                      state.fieldInfos,
                                                      state.segmentInfo.name,
-                                                     state.termsIndexDivisor,
                                                      BytesRef.getUTF8SortedAsUnicodeComparator(),
                                                      state.segmentSuffix, state.context);
         } else {
@@ -383,12 +376,11 @@ public final class MockRandomPostingsFormat extends PostingsFormat {
             random.nextLong();
           }
           if (LuceneTestCase.VERBOSE) {
-            System.out.println("MockRandomCodec: variable-gap terms index (divisor=" + state.termsIndexDivisor + ")");
+            System.out.println("MockRandomCodec: variable-gap terms index");
           }
           indexReader = new VariableGapTermsIndexReader(state.directory,
                                                         state.fieldInfos,
                                                         state.segmentInfo.name,
-                                                        state.termsIndexDivisor,
                                                         state.segmentSuffix, state.context);
 
         }
@@ -400,8 +392,6 @@ public final class MockRandomPostingsFormat extends PostingsFormat {
         }
       }
 
-      final int termsCacheSize = _TestUtil.nextInt(random, 1, 1024);
-
       success = false;
       try {
         fields = new BlockTermsReader(indexReader,
@@ -410,7 +400,6 @@ public final class MockRandomPostingsFormat extends PostingsFormat {
                                       state.segmentInfo,
                                       postingsReader,
                                       state.context,
-                                      termsCacheSize,
                                       state.segmentSuffix);
         success = true;
       } finally {
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java
index 1047aa0..f317004 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java
@@ -92,7 +92,6 @@ public final class MockSepPostingsFormat extends PostingsFormat {
       indexReader = new FixedGapTermsIndexReader(state.directory,
                                                        state.fieldInfos,
                                                        state.segmentInfo.name,
-                                                       state.termsIndexDivisor,
                                                        BytesRef.getUTF8SortedAsUnicodeComparator(),
                                                        state.segmentSuffix, state.context);
       success = true;
@@ -110,7 +109,6 @@ public final class MockSepPostingsFormat extends PostingsFormat {
                                                 state.segmentInfo,
                                                 postingsReader,
                                                 state.context,
-                                                1024,
                                                 state.segmentSuffix);
       success = true;
       return ret;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java
index c509189..54cabfb 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java
@@ -84,8 +84,7 @@ public final class NestedPulsingPostingsFormat extends PostingsFormat {
                                                     state.directory, state.fieldInfos, state.segmentInfo,
                                                     pulsingReader,
                                                     state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
+                                                    state.segmentSuffix);
       success = true;
       return ret;
     } finally {
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java
index 2cc7384..6d35683 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java
@@ -336,7 +336,7 @@ public final class RAMOnlyPostingsFormat extends PostingsFormat {
     }
 
     @Override
-    public SeekStatus seekCeil(BytesRef term, boolean useCache) {
+    public SeekStatus seekCeil(BytesRef term) {
       current = term.utf8ToString();
       it = null;
       if (ramField.termToDocs.containsKey(current)) {
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/AssertingAtomicReader.java b/lucene/test-framework/src/java/org/apache/lucene/index/AssertingAtomicReader.java
index bbcec7a..6db5ccd 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/AssertingAtomicReader.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/AssertingAtomicReader.java
@@ -185,9 +185,9 @@ public class AssertingAtomicReader extends FilterAtomicReader {
     }
 
     @Override
-    public SeekStatus seekCeil(BytesRef term, boolean useCache) throws IOException {
+    public SeekStatus seekCeil(BytesRef term) throws IOException {
       assert term.isValid();
-      SeekStatus result = super.seekCeil(term, useCache);
+      SeekStatus result = super.seekCeil(term);
       if (result == SeekStatus.END) {
         state = State.UNPOSITIONED;
       } else {
@@ -197,9 +197,9 @@ public class AssertingAtomicReader extends FilterAtomicReader {
     }
 
     @Override
-    public boolean seekExact(BytesRef text, boolean useCache) throws IOException {
+    public boolean seekExact(BytesRef text) throws IOException {
       assert text.isValid();
-      if (super.seekExact(text, useCache)) {
+      if (super.seekExact(text)) {
         state = State.POSITIONED;
         return true;
       } else {
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
index a6eb50e..81c5cbe 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/BaseDocValuesFormatTestCase.java
@@ -746,16 +746,16 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
     assertEquals(SeekStatus.END, termsEnum.seekCeil(new BytesRef("zzz")));
     
     // seekExact()
-    assertTrue(termsEnum.seekExact(new BytesRef("beer"), true));
+    assertTrue(termsEnum.seekExact(new BytesRef("beer")));
     assertEquals("beer", termsEnum.term().utf8ToString());
     assertEquals(0, termsEnum.ord());
-    assertTrue(termsEnum.seekExact(new BytesRef("hello"), true));
+    assertTrue(termsEnum.seekExact(new BytesRef("hello")));
     assertEquals(Codec.getDefault().toString(), "hello", termsEnum.term().utf8ToString());
     assertEquals(1, termsEnum.ord());
-    assertTrue(termsEnum.seekExact(new BytesRef("world"), true));
+    assertTrue(termsEnum.seekExact(new BytesRef("world")));
     assertEquals("world", termsEnum.term().utf8ToString());
     assertEquals(2, termsEnum.ord());
-    assertFalse(termsEnum.seekExact(new BytesRef("bogus"), true));
+    assertFalse(termsEnum.seekExact(new BytesRef("bogus")));
 
     // seek(ord)
     termsEnum.seekExact(0);
@@ -1022,7 +1022,7 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
 
     writer.close(true);
 
-    DirectoryReader reader = DirectoryReader.open(dir, 1);
+    DirectoryReader reader = DirectoryReader.open(dir);
     assertEquals(1, reader.leaves().size());
   
     IndexSearcher searcher = new IndexSearcher(reader);
@@ -1350,6 +1350,57 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
     dir.close();
   }
   
+  private void doTestSortedVsFieldCache(int minLength, int maxLength) throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);
+    Document doc = new Document();
+    Field idField = new StringField("id", "", Field.Store.NO);
+    Field indexedField = new StringField("indexed", "", Field.Store.NO);
+    Field dvField = new SortedDocValuesField("dv", new BytesRef());
+    doc.add(idField);
+    doc.add(indexedField);
+    doc.add(dvField);
+    
+    // index some docs
+    int numDocs = atLeast(300);
+    for (int i = 0; i < numDocs; i++) {
+      idField.setStringValue(Integer.toString(i));
+      final int length;
+      if (minLength == maxLength) {
+        length = minLength; // fixed length
+      } else {
+        length = _TestUtil.nextInt(random(), minLength, maxLength);
+      }
+      String value = _TestUtil.randomSimpleString(random(), length);
+      indexedField.setStringValue(value);
+      dvField.setBytesValue(new BytesRef(value));
+      writer.addDocument(doc);
+      if (random().nextInt(31) == 0) {
+        writer.commit();
+      }
+    }
+    
+    // delete some docs
+    int numDeletions = random().nextInt(numDocs/10);
+    for (int i = 0; i < numDeletions; i++) {
+      int id = random().nextInt(numDocs);
+      writer.deleteDocuments(new Term("id", Integer.toString(id)));
+    }
+    writer.close();
+    
+    // compare
+    DirectoryReader ir = DirectoryReader.open(dir);
+    for (AtomicReaderContext context : ir.leaves()) {
+      AtomicReader r = context.reader();
+      SortedDocValues expected = FieldCache.DEFAULT.getTermsIndex(r, "indexed");
+      SortedDocValues actual = r.getSortedDocValues("dv");
+      assertEquals(r.maxDoc(), expected, actual);
+    }
+    ir.close();
+    dir.close();
+  }
+  
   public void testSortedFixedLengthVsStoredFields() throws Exception {
     int numIterations = atLeast(1);
     for (int i = 0; i < numIterations; i++) {
@@ -1358,6 +1409,21 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
     }
   }
   
+  public void testSortedFixedLengthVsFieldCache() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      int fixedLength = _TestUtil.nextInt(random(), 1, 10);
+      doTestSortedVsFieldCache(fixedLength, fixedLength);
+    }
+  }
+  
+  public void testSortedVariableLengthVsFieldCache() throws Exception {
+    int numIterations = atLeast(1);
+    for (int i = 0; i < numIterations; i++) {
+      doTestSortedVsFieldCache(1, 10);
+    }
+  }
+  
   public void testSortedVariableLengthVsStoredFields() throws Exception {
     int numIterations = atLeast(1);
     for (int i = 0; i < numIterations; i++) {
@@ -1788,16 +1854,16 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
     assertEquals(SeekStatus.END, termsEnum.seekCeil(new BytesRef("zzz")));
     
     // seekExact()
-    assertTrue(termsEnum.seekExact(new BytesRef("beer"), true));
+    assertTrue(termsEnum.seekExact(new BytesRef("beer")));
     assertEquals("beer", termsEnum.term().utf8ToString());
     assertEquals(0, termsEnum.ord());
-    assertTrue(termsEnum.seekExact(new BytesRef("hello"), true));
+    assertTrue(termsEnum.seekExact(new BytesRef("hello")));
     assertEquals("hello", termsEnum.term().utf8ToString());
     assertEquals(1, termsEnum.ord());
-    assertTrue(termsEnum.seekExact(new BytesRef("world"), true));
+    assertTrue(termsEnum.seekExact(new BytesRef("world")));
     assertEquals("world", termsEnum.term().utf8ToString());
     assertEquals(2, termsEnum.ord());
-    assertFalse(termsEnum.seekExact(new BytesRef("bogus"), true));
+    assertFalse(termsEnum.seekExact(new BytesRef("bogus")));
 
     // seek(ord)
     termsEnum.seekExact(0);
@@ -1905,6 +1971,10 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
     }
   }
   
+  private void assertEquals(int maxDoc, SortedDocValues expected, SortedDocValues actual) throws Exception {
+    assertEquals(maxDoc, new SingletonSortedSetDocValues(expected), new SingletonSortedSetDocValues(actual));
+  }
+  
   private void assertEquals(int maxDoc, SortedSetDocValues expected, SortedSetDocValues actual) throws Exception {
     // can be null for the segment if no docs actually had any SortedDocValues
     // in this case FC.getDocTermsOrds returns EMPTY
@@ -1932,6 +2002,74 @@ public abstract class BaseDocValuesFormatTestCase extends LuceneTestCase {
       actual.lookupTerm(actualBytes);
       assertEquals(expectedBytes, actualBytes);
     }
+    
+    // compare termsenum
+    assertEquals(expected.getValueCount(), expected.termsEnum(), actual.termsEnum());
+  }
+  
+  private void assertEquals(long numOrds, TermsEnum expected, TermsEnum actual) throws Exception {
+    BytesRef ref;
+    
+    // sequential next() through all terms
+    while ((ref = expected.next()) != null) {
+      assertEquals(ref, actual.next());
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    assertNull(actual.next());
+    
+    // sequential seekExact(ord) through all terms
+    for (long i = 0; i < numOrds; i++) {
+      expected.seekExact(i);
+      actual.seekExact(i);
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    
+    // sequential seekExact(BytesRef) through all terms
+    for (long i = 0; i < numOrds; i++) {
+      expected.seekExact(i);
+      assertTrue(actual.seekExact(expected.term()));
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    
+    // sequential seekCeil(BytesRef) through all terms
+    for (long i = 0; i < numOrds; i++) {
+      expected.seekExact(i);
+      assertEquals(SeekStatus.FOUND, actual.seekCeil(expected.term()));
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    
+    // random seekExact(ord)
+    for (long i = 0; i < numOrds; i++) {
+      long randomOrd = _TestUtil.nextLong(random(), 0, numOrds-1);
+      expected.seekExact(randomOrd);
+      actual.seekExact(randomOrd);
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    
+    // random seekExact(BytesRef)
+    for (long i = 0; i < numOrds; i++) {
+      long randomOrd = _TestUtil.nextLong(random(), 0, numOrds-1);
+      expected.seekExact(randomOrd);
+      actual.seekExact(expected.term());
+      assertEquals(expected.ord(), actual.ord());
+      assertEquals(expected.term(), actual.term());
+    }
+    
+    // random seekCeil(BytesRef)
+    for (long i = 0; i < numOrds; i++) {
+      BytesRef target = new BytesRef(_TestUtil.randomUnicodeString(random()));
+      SeekStatus expectedStatus = expected.seekCeil(target);
+      assertEquals(expectedStatus, actual.seekCeil(target));
+      if (expectedStatus != SeekStatus.END) {
+        assertEquals(expected.ord(), actual.ord());
+        assertEquals(expected.term(), actual.term());
+      }
+    }
   }
   
   private void doTestSortedSetVsUninvertedField(int minLength, int maxLength) throws Exception {
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
index fd3f90b..4cb87e6 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase.java
@@ -38,6 +38,8 @@ import org.apache.lucene.codecs.FieldsProducer;
 import org.apache.lucene.codecs.PostingsConsumer;
 import org.apache.lucene.codecs.TermStats;
 import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
 import org.apache.lucene.index.FieldInfo.DocValuesType;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.store.Directory;
@@ -353,9 +355,12 @@ public abstract class BasePostingsFormatTestCase extends LuceneTestCase {
       fields.put(field, postings);
       Set<String> seenTerms = new HashSet<String>();
 
-      // TODO:
-      //final int numTerms = atLeast(10);
-      final int numTerms = 4;
+      int numTerms;
+      if (random().nextInt(10) == 7) {
+        numTerms = atLeast(50);
+      } else {
+        numTerms = _TestUtil.nextInt(random(), 2, 20);
+      }
 
       for(int termUpto=0;termUpto<numTerms;termUpto++) {
         String term = _TestUtil.randomSimpleString(random());
@@ -483,7 +488,7 @@ public abstract class BasePostingsFormatTestCase extends LuceneTestCase {
 
     SegmentWriteState writeState = new SegmentWriteState(null, dir,
                                                          segmentInfo, newFieldInfos,
-                                                         32, null, new IOContext(new FlushInfo(maxDoc, bytes)));
+                                                         null, new IOContext(new FlushInfo(maxDoc, bytes)));
     FieldsConsumer fieldsConsumer = codec.postingsFormat().fieldsConsumer(writeState);
 
     for(Map.Entry<String,Map<BytesRef,Long>> fieldEnt : fields.entrySet()) {
@@ -567,7 +572,7 @@ public abstract class BasePostingsFormatTestCase extends LuceneTestCase {
 
     currentFieldInfos = newFieldInfos;
 
-    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT, 1);
+    SegmentReadState readState = new SegmentReadState(dir, segmentInfo, newFieldInfos, IOContext.DEFAULT);
 
     return codec.postingsFormat().fieldsProducer(readState);
   }
@@ -595,6 +600,10 @@ public abstract class BasePostingsFormatTestCase extends LuceneTestCase {
       System.out.println("  verifyEnum: options=" + options + " maxTestOptions=" + maxTestOptions);
     }
 
+    // Make sure TermsEnum really is positioned on the
+    // expected term:
+    assertEquals(term, termsEnum.term());
+
     // 50% of the time time pass liveDocs:
     boolean useLiveDocs = options.contains(Option.LIVE_DOCS) && random().nextBoolean();
     Bits liveDocs;
@@ -983,7 +992,7 @@ public abstract class BasePostingsFormatTestCase extends LuceneTestCase {
       termsEnum = terms.iterator(null);
 
       if (!useTermState) {
-        assertTrue(termsEnum.seekExact(fieldAndTerm.term, true));
+        assertTrue(termsEnum.seekExact(fieldAndTerm.term));
       } else {
         termsEnum.seekExact(fieldAndTerm.term, termState);
       }
@@ -1130,4 +1139,89 @@ public abstract class BasePostingsFormatTestCase extends LuceneTestCase {
       _TestUtil.rmDir(path);
     }
   }
+  
+  public void testEmptyField() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
+    iwc.setCodec(getCodec());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(newStringField("", "something", Field.Store.NO));
+    iw.addDocument(doc);
+    DirectoryReader ir = iw.getReader();
+    AtomicReader ar = getOnlySegmentReader(ir);
+    Fields fields = ar.fields();
+    int fieldCount = fields.size();
+    // -1 is allowed, if the codec doesn't implement fields.size():
+    assertTrue(fieldCount == 1 || fieldCount == -1);
+    Terms terms = ar.terms("");
+    assertNotNull(terms);
+    TermsEnum termsEnum = terms.iterator(null);
+    assertNotNull(termsEnum.next());
+    assertEquals(termsEnum.term(), new BytesRef("something"));
+    assertNull(termsEnum.next());
+    ir.close();
+    iw.close();
+    dir.close();
+  }
+  
+  public void testEmptyFieldAndEmptyTerm() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
+    iwc.setCodec(getCodec());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(newStringField("", "", Field.Store.NO));
+    iw.addDocument(doc);
+    DirectoryReader ir = iw.getReader();
+    AtomicReader ar = getOnlySegmentReader(ir);
+    Fields fields = ar.fields();
+    int fieldCount = fields.size();
+    // -1 is allowed, if the codec doesn't implement fields.size():
+    assertTrue(fieldCount == 1 || fieldCount == -1);
+    Terms terms = ar.terms("");
+    assertNotNull(terms);
+    TermsEnum termsEnum = terms.iterator(null);
+    assertNotNull(termsEnum.next());
+    assertEquals(termsEnum.term(), new BytesRef(""));
+    assertNull(termsEnum.next());
+    ir.close();
+    iw.close();
+    dir.close();
+  }
+  
+  // tests that ghost fields still work
+  // TODO: can this be improved?
+  public void testGhosts() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
+    iwc.setCodec(getCodec());
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    iw.addDocument(doc);
+    doc.add(newStringField("ghostField", "something", Field.Store.NO));
+    iw.addDocument(doc);
+    iw.forceMerge(1);
+    iw.deleteDocuments(new Term("ghostField", "something")); // delete the only term for the field
+    iw.forceMerge(1);
+    DirectoryReader ir = iw.getReader();
+    AtomicReader ar = getOnlySegmentReader(ir);
+    Fields fields = ar.fields();
+    // Ghost busting terms dict impls will have
+    // fields.size() == 0; all others must be == 1:
+    assertTrue(fields.size() <= 1);
+    Terms terms = fields.terms("ghostField");
+    if (terms != null) {
+      TermsEnum termsEnum = terms.iterator(null);
+      BytesRef term = termsEnum.next();
+      if (term != null) {
+        DocsEnum docsEnum = termsEnum.docs(null, null);
+        assertTrue(docsEnum.nextDoc() == DocsEnum.NO_MORE_DOCS);
+      }
+    }
+    ir.close();
+    iw.close();
+    dir.close();
+  }
 }
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java
index 4c42f38..4309487 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/BaseStoredFieldsFormatTestCase.java
@@ -509,7 +509,7 @@ public abstract class BaseStoredFieldsFormatTestCase extends LuceneTestCase {
     Directory dir = newDirectory();
     IndexWriterConfig iwConf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     iwConf.setMaxBufferedDocs(RandomInts.randomIntBetween(random(), 2, 30));
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf);
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwConf.clone());
     
     final int docCount = atLeast(200);
     final byte[][][] data = new byte [docCount][][];
@@ -548,7 +548,7 @@ public abstract class BaseStoredFieldsFormatTestCase extends LuceneTestCase {
         } else {
           iwConf.setCodec(otherCodec);
         }
-        iw = new RandomIndexWriter(random(), dir, iwConf);
+        iw = new RandomIndexWriter(random(), dir, iwConf.clone());
       }
     }
 
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.java
index ac4a69b..150922b 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/BaseTermVectorsFormatTestCase.java
@@ -500,9 +500,9 @@ public abstract class BaseTermVectorsFormatTestCase extends LuceneTestCase {
     assertNull(termsEnum.next());
     for (int i = 0; i < 5; ++i) {
       if (random().nextBoolean()) {
-        assertTrue(termsEnum.seekExact(RandomPicks.randomFrom(random(), tk.termBytes), random().nextBoolean()));
+        assertTrue(termsEnum.seekExact(RandomPicks.randomFrom(random(), tk.termBytes)));
       } else {
-        assertEquals(SeekStatus.FOUND, termsEnum.seekCeil(RandomPicks.randomFrom(random(), tk.termBytes), random().nextBoolean()));
+        assertEquals(SeekStatus.FOUND, termsEnum.seekCeil(RandomPicks.randomFrom(random(), tk.termBytes)));
       }
     }
   }
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java b/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
index 86258c0..9ecc512 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
@@ -33,6 +33,8 @@ import org.apache.lucene.codecs.asserting.AssertingDocValuesFormat;
 import org.apache.lucene.codecs.asserting.AssertingPostingsFormat;
 import org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat;
 import org.apache.lucene.codecs.lucene41ords.Lucene41WithOrds;
+import org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapDocFreqInterval;
+import org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapFixedInterval;
 import org.apache.lucene.codecs.lucene42.Lucene42Codec;
 import org.apache.lucene.codecs.lucene42.Lucene42DocValuesFormat;
 import org.apache.lucene.codecs.bloom.TestBloomFilteredLucene41Postings;
@@ -137,7 +139,9 @@ public class RandomCodec extends Lucene42Codec {
         new MockVariableIntBlockPostingsFormat( _TestUtil.nextInt(random, 1, 127)),
         new MockRandomPostingsFormat(random),
         new NestedPulsingPostingsFormat(),
-        new Lucene41WithOrds(),
+        new Lucene41WithOrds(_TestUtil.nextInt(random, 1, 1000)),
+        new Lucene41VarGapFixedInterval(_TestUtil.nextInt(random, 1, 1000)),
+        new Lucene41VarGapDocFreqInterval(_TestUtil.nextInt(random, 1, 100), _TestUtil.nextInt(random, 1, 1000)),
         new SimpleTextPostingsFormat(),
         new AssertingPostingsFormat(),
         new MemoryPostingsFormat(true, random.nextFloat()),
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java b/lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java
index 3fea86c..fb67b44 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/RandomIndexWriter.java
@@ -308,7 +308,7 @@ public class RandomIndexWriter implements Closeable {
       }
       w.commit();
       if (r.nextBoolean()) {
-        return DirectoryReader.open(w.getDirectory(), _TestUtil.nextInt(r, 1, 10));
+        return DirectoryReader.open(w.getDirectory());
       } else {
         return w.getReader(applyDeletions);
       }
diff --git a/lucene/test-framework/src/java/org/apache/lucene/search/ShardSearchingTestBase.java b/lucene/test-framework/src/java/org/apache/lucene/search/ShardSearchingTestBase.java
index 5596383..4b21b6d 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/search/ShardSearchingTestBase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/search/ShardSearchingTestBase.java
@@ -185,7 +185,7 @@ public abstract class ShardSearchingTestBase extends LuceneTestCase {
     }
     try {
       for(Term term : terms) {
-        final TermContext termContext = TermContext.build(s.getIndexReader().getContext(), term, false);
+        final TermContext termContext = TermContext.build(s.getIndexReader().getContext(), term);
         stats.put(term, s.termStatistics(term, termContext));
       }
     } finally {
@@ -370,20 +370,35 @@ public abstract class ShardSearchingTestBase extends LuceneTestCase {
       @Override
       public TopDocs searchAfter(ScoreDoc after, Query query, int numHits) throws IOException {
         final TopDocs[] shardHits = new TopDocs[nodeVersions.length];
+        // results are merged in that order: score, shardIndex, doc. therefore we set
+        // after to after.score and depending on the nodeID we set doc to either:
+        // - not collect any more documents with that score (only with worse score)
+        // - collect more documents with that score (and worse) following the last collected document
+        // - collect all documents with that score (and worse)
         ScoreDoc shardAfter = new ScoreDoc(after.doc, after.score);
-        for(int nodeID=0;nodeID<nodeVersions.length;nodeID++) {
+        for (int nodeID = 0; nodeID < nodeVersions.length; nodeID++) {
           if (nodeID < after.shardIndex) {
-            // If score is tied then no docs in this shard
-            // should be collected:
-            shardAfter.doc = Integer.MAX_VALUE;
+            // all documents with after.score were already collected, so collect
+            // only documents with worse scores.
+            final NodeState.ShardIndexSearcher s = nodes[nodeID].acquire(nodeVersions);
+            try {
+              // Setting after.doc to reader.maxDoc-1 is a way to tell
+              // TopScoreDocCollector that no more docs with that score should
+              // be collected. note that in practice the shard which sends the
+              // request to a remote shard won't have reader.maxDoc at hand, so
+              // it will send some arbitrary value which will be fixed on the
+              // other end.
+              shardAfter.doc = s.getIndexReader().maxDoc() - 1;
+            } finally {
+              nodes[nodeID].release(s);
+            }
           } else if (nodeID == after.shardIndex) {
-            // If score is tied then we break according to
-            // docID (like normal):  
+            // collect all documents following the last collected doc with
+            // after.score + documents with worse scores.  
             shardAfter.doc = after.doc;
           } else {
-            // If score is tied then all docs in this shard
-            // should be collected, because they come after
-            // the previous bottom:
+            // all documents with after.score (and worse) should be collected
+            // because they didn't make it to top-N in the previous round.
             shardAfter.doc = -1;
           }
           if (nodeID == myNodeID) {
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneJUnit3MethodProvider.java b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneJUnit3MethodProvider.java
old mode 100755
new mode 100644
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
index 3791e48..3060d68 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
@@ -20,6 +20,7 @@ package org.apache.lucene.util;
 import java.io.*;
 import java.lang.annotation.*;
 import java.lang.reflect.Constructor;
+import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
 import java.util.*;
 import java.util.concurrent.*;
@@ -754,15 +755,6 @@ public abstract class LuceneTestCase extends Assert {
       }
     }
     if (r.nextBoolean()) {
-      if (rarely(r)) {
-        // crazy value
-        c.setTermIndexInterval(r.nextBoolean() ? _TestUtil.nextInt(r, 1, 31) : _TestUtil.nextInt(r, 129, 1000));
-      } else {
-        // reasonable value
-        c.setTermIndexInterval(_TestUtil.nextInt(r, 32, 128));
-      }
-    }
-    if (r.nextBoolean()) {
       int maxNumThreadStates = rarely(r) ? _TestUtil.nextInt(r, 5, 20) // crazy value
           : _TestUtil.nextInt(r, 1, 4); // reasonable value
 
@@ -802,24 +794,31 @@ public abstract class LuceneTestCase extends Assert {
       }
     }
 
-    if (rarely(r)) {
-      c.setMergePolicy(new MockRandomMergePolicy(r));
-    } else if (r.nextBoolean()) {
-      c.setMergePolicy(newTieredMergePolicy());
-    } else if (r.nextInt(5) == 0) { 
-      c.setMergePolicy(newAlcoholicMergePolicy());
-    } else {
-      c.setMergePolicy(newLogMergePolicy());
-    }
+    c.setMergePolicy(newMergePolicy(r));
+
     if (rarely(r)) {
       c.setMergedSegmentWarmer(new SimpleMergedSegmentWarmer(c.getInfoStream()));
     }
     c.setUseCompoundFile(r.nextBoolean());
     c.setReaderPooling(r.nextBoolean());
-    c.setReaderTermsIndexDivisor(_TestUtil.nextInt(r, 1, 4));
     return c;
   }
 
+  public static MergePolicy newMergePolicy(Random r) {
+    if (rarely(r)) {
+      return new MockRandomMergePolicy(r);
+    } else if (r.nextBoolean()) {
+      return newTieredMergePolicy(r);
+    } else if (r.nextInt(5) == 0) { 
+      return newAlcoholicMergePolicy(r, classEnvRule.timeZone);
+    }
+    return newLogMergePolicy(r);
+  }
+
+  public static MergePolicy newMergePolicy() {
+    return newMergePolicy(random());
+  }
+
   public static LogMergePolicy newLogMergePolicy() {
     return newLogMergePolicy(random());
   }
@@ -1137,8 +1136,8 @@ public abstract class LuceneTestCase extends Assert {
     FSDirectory d = null;
     try {
       d = CommandLineUtil.newFSDirectory(clazz, file);
-    } catch (Exception e) {
-      d = FSDirectory.open(file);
+    } catch (NoSuchMethodException | InstantiationException | IllegalAccessException | InvocationTargetException e) {
+      Rethrow.rethrow(e);
     }
     return d;
   }
@@ -1751,14 +1750,13 @@ public abstract class LuceneTestCase extends Assert {
         rightEnum = rightTerms.iterator(rightEnum);
       }
 
-      final boolean useCache = random().nextBoolean();
       final boolean seekExact = random().nextBoolean();
 
       if (seekExact) {
-        assertEquals(info, leftEnum.seekExact(b, useCache), rightEnum.seekExact(b, useCache));
+        assertEquals(info, leftEnum.seekExact(b), rightEnum.seekExact(b));
       } else {
-        SeekStatus leftStatus = leftEnum.seekCeil(b, useCache);
-        SeekStatus rightStatus = rightEnum.seekCeil(b, useCache);
+        SeekStatus leftStatus = leftEnum.seekCeil(b);
+        SeekStatus rightStatus = rightEnum.seekCeil(b);
         assertEquals(info, leftStatus, rightStatus);
         if (leftStatus != SeekStatus.END) {
           assertEquals(info, leftEnum.term(), rightEnum.term());
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java b/lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java
index 9eccf27..e0ef230 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/_TestUtil.java
@@ -928,7 +928,7 @@ public class _TestUtil {
       return null;
     }
     final TermsEnum termsEnum = terms.iterator(null);
-    if (!termsEnum.seekExact(term, random.nextBoolean())) {
+    if (!termsEnum.seekExact(term)) {
       return null;
     }
     return docs(random, termsEnum, liveDocs, reuse, flags);
diff --git a/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
index 3b7b383..59d0dd3 100644
--- a/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
+++ b/lucene/test-framework/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -20,6 +20,8 @@ org.apache.lucene.codecs.mocksep.MockSepPostingsFormat
 org.apache.lucene.codecs.nestedpulsing.NestedPulsingPostingsFormat
 org.apache.lucene.codecs.ramonly.RAMOnlyPostingsFormat
 org.apache.lucene.codecs.lucene41ords.Lucene41WithOrds
+org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapFixedInterval
+org.apache.lucene.codecs.lucene41vargap.Lucene41VarGapDocFreqInterval
 org.apache.lucene.codecs.bloom.TestBloomFilteredLucene41Postings
 org.apache.lucene.codecs.asserting.AssertingPostingsFormat
 org.apache.lucene.codecs.lucene40.Lucene40RWPostingsFormat

