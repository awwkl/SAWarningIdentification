GitDiffStart: a55f511a77d4a3a0494505b1785ef007954d85b5 | Fri Dec 16 19:03:12 2011 +0000
diff --git a/dev-tools/idea/.idea/compiler.xml b/dev-tools/idea/.idea/compiler.xml
index 56d8b8f..39bab09 100644
--- a/dev-tools/idea/.idea/compiler.xml
+++ b/dev-tools/idea/.idea/compiler.xml
@@ -36,7 +36,7 @@
       <entry name="?*.xsl" />
       <entry name="?*.vm" />
       <entry name="?*.zip" />
-      <entry name="org.apache.lucene.index.codecs*" />
+      <entry name="org.apache.lucene.codecs*" />
       <entry name="README*" />
       <entry name="[a-zA-Z][a-zA-Z]" />
       <entry name="[a-zA-Z][a-zA-Z]-[a-zA-Z][a-zA-Z]" />
diff --git a/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java b/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
index ed82163..f07cf48 100644
--- a/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
+++ b/lucene/contrib/memory/src/test/org/apache/lucene/index/memory/MemoryIndexTest.java
@@ -29,6 +29,7 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenFilter;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
@@ -38,7 +39,6 @@ import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.queryparser.classic.QueryParser;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
diff --git a/lucene/src/java/org/apache/lucene/codecs/BlockTermState.java b/lucene/src/java/org/apache/lucene/codecs/BlockTermState.java
new file mode 100644
index 0000000..04440cd
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/BlockTermState.java
@@ -0,0 +1,54 @@
+package org.apache.lucene.codecs;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.DocsEnum; // javadocs
+import org.apache.lucene.index.OrdTermState;
+import org.apache.lucene.index.TermState;
+
+/**
+ * Holds all state required for {@link PostingsReaderBase}
+ * to produce a {@link DocsEnum} without re-seeking the
+ * terms dict.
+ */
+public class BlockTermState extends OrdTermState {
+  public int docFreq;            // how many docs have this term
+  public long totalTermFreq;     // total number of occurrences of this term
+
+  public int termBlockOrd;          // the term's ord in the current block
+  public long blockFilePointer;  // fp into the terms dict primary file (_X.tim) that holds this term
+
+  @Override
+  public void copyFrom(TermState _other) {
+    assert _other instanceof BlockTermState : "can not copy from " + _other.getClass().getName();
+    BlockTermState other = (BlockTermState) _other;
+    super.copyFrom(_other);
+    docFreq = other.docFreq;
+    totalTermFreq = other.totalTermFreq;
+    termBlockOrd = other.termBlockOrd;
+    blockFilePointer = other.blockFilePointer;
+
+    // NOTE: don't copy blockTermCount;
+    // it's "transient": used only by the "primary"
+    // termState, and regenerated on seek by TermState
+  }
+
+  @Override
+  public String toString() {
+    return "docFreq=" + docFreq + " totalTermFreq=" + totalTermFreq + " termBlockOrd=" + termBlockOrd + " blockFP=" + blockFilePointer;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/BlockTermsReader.java b/lucene/src/java/org/apache/lucene/codecs/BlockTermsReader.java
new file mode 100644
index 0000000..df0ae88
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/BlockTermsReader.java
@@ -0,0 +1,876 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.TreeMap;
+
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.DoubleBarrelLRUCache;
+
+/** Handles a terms dict, but decouples all details of
+ *  doc/freqs/positions reading to an instance of {@link
+ *  PostingsReaderBase}.  This class is reusable for
+ *  codecs that use a different format for
+ *  docs/freqs/positions (though codecs are also free to
+ *  make their own terms dict impl).
+ *
+ * <p>This class also interacts with an instance of {@link
+ * TermsIndexReaderBase}, to abstract away the specific
+ * implementation of the terms dict index. 
+ * @lucene.experimental */
+
+public class BlockTermsReader extends FieldsProducer {
+  // Open input to the main terms dict file (_X.tis)
+  private final IndexInput in;
+
+  // Reads the terms dict entries, to gather state to
+  // produce DocsEnum on demand
+  private final PostingsReaderBase postingsReader;
+
+  private final TreeMap<String,FieldReader> fields = new TreeMap<String,FieldReader>();
+
+  // Caches the most recently looked-up field + terms:
+  private final DoubleBarrelLRUCache<FieldAndTerm,BlockTermState> termsCache;
+
+  // Reads the terms index
+  private TermsIndexReaderBase indexReader;
+
+  // keeps the dirStart offset
+  protected long dirOffset;
+
+  // Used as key for the terms cache
+  private static class FieldAndTerm extends DoubleBarrelLRUCache.CloneableKey {
+    String field;
+    BytesRef term;
+
+    public FieldAndTerm() {
+    }
+
+    public FieldAndTerm(FieldAndTerm other) {
+      field = other.field;
+      term = BytesRef.deepCopyOf(other.term);
+    }
+
+    @Override
+    public boolean equals(Object _other) {
+      FieldAndTerm other = (FieldAndTerm) _other;
+      return other.field.equals(field) && term.bytesEquals(other.term);
+    }
+
+    @Override
+    public Object clone() {
+      return new FieldAndTerm(this);
+    }
+
+    @Override
+    public int hashCode() {
+      return field.hashCode() * 31 + term.hashCode();
+    }
+  }
+  
+  // private String segment;
+  
+  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, String segment, PostingsReaderBase postingsReader, IOContext context,
+                          int termsCacheSize, String segmentSuffix)
+    throws IOException {
+    
+    this.postingsReader = postingsReader;
+    termsCache = new DoubleBarrelLRUCache<FieldAndTerm,BlockTermState>(termsCacheSize);
+
+    // this.segment = segment;
+    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),
+                       context);
+
+    boolean success = false;
+    try {
+      readHeader(in);
+
+      // Have PostingsReader init itself
+      postingsReader.init(in);
+
+      // Read per-field details
+      seekDir(in, dirOffset);
+
+      final int numFields = in.readVInt();
+      for(int i=0;i<numFields;i++) {
+        final int field = in.readVInt();
+        final long numTerms = in.readVLong();
+        assert numTerms >= 0;
+        final long termsStartPointer = in.readVLong();
+        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        final long sumTotalTermFreq = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
+        final long sumDocFreq = in.readVLong();
+        final int docCount = in.readVInt();
+        assert !fields.containsKey(fieldInfo.name);
+        fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        in.close();
+      }
+    }
+
+    this.indexReader = indexReader;
+  }
+
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, BlockTermsWriter.CODEC_NAME,
+                          BlockTermsWriter.VERSION_START,
+                          BlockTermsWriter.VERSION_CURRENT);
+    dirOffset = input.readLong();
+  }
+  
+  protected void seekDir(IndexInput input, long dirOffset)
+      throws IOException {
+    input.seek(dirOffset);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      try {
+        if (indexReader != null) {
+          indexReader.close();
+        }
+      } finally {
+        // null so if an app hangs on to us (ie, we are not
+        // GCable, despite being closed) we still free most
+        // ram
+        indexReader = null;
+        if (in != null) {
+          in.close();
+        }
+      }
+    } finally {
+      if (postingsReader != null) {
+        postingsReader.close();
+      }
+    }
+  }
+
+  public static void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) {
+    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION));
+  }
+
+  @Override
+  public FieldsEnum iterator() {
+    return new TermFieldsEnum();
+  }
+
+  @Override
+  public Terms terms(String field) throws IOException {
+    return fields.get(field);
+  }
+
+  @Override
+  public int getUniqueFieldCount() {
+    return fields.size();
+  }
+
+  // Iterates through all fields
+  private class TermFieldsEnum extends FieldsEnum {
+    final Iterator<FieldReader> it;
+    FieldReader current;
+
+    TermFieldsEnum() {
+      it = fields.values().iterator();
+    }
+
+    @Override
+    public String next() {
+      if (it.hasNext()) {
+        current = it.next();
+        return current.fieldInfo.name;
+      } else {
+        current = null;
+        return null;
+      }
+    }
+    
+    @Override
+    public Terms terms() throws IOException {
+      return current;
+    }
+  }
+
+  private class FieldReader extends Terms {
+    final long numTerms;
+    final FieldInfo fieldInfo;
+    final long termsStartPointer;
+    final long sumTotalTermFreq;
+    final long sumDocFreq;
+    final int docCount;
+
+    FieldReader(FieldInfo fieldInfo, long numTerms, long termsStartPointer, long sumTotalTermFreq, long sumDocFreq, int docCount) {
+      assert numTerms > 0;
+      this.fieldInfo = fieldInfo;
+      this.numTerms = numTerms;
+      this.termsStartPointer = termsStartPointer;
+      this.sumTotalTermFreq = sumTotalTermFreq;
+      this.sumDocFreq = sumDocFreq;
+      this.docCount = docCount;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      return new SegmentTermsEnum();
+    }
+
+    @Override
+    public long getUniqueTermCount() {
+      return numTerms;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return sumTotalTermFreq;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return sumDocFreq;
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return docCount;
+    }
+
+    // Iterates through terms in this field
+    private final class SegmentTermsEnum extends TermsEnum {
+      private final IndexInput in;
+      private final BlockTermState state;
+      private final boolean doOrd;
+      private final FieldAndTerm fieldTerm = new FieldAndTerm();
+      private final TermsIndexReaderBase.FieldIndexEnum indexEnum;
+      private final BytesRef term = new BytesRef();
+
+      /* This is true if indexEnum is "still" seek'd to the index term
+         for the current term. We set it to true on seeking, and then it
+         remains valid until next() is called enough times to load another
+         terms block: */
+      private boolean indexIsCurrent;
+
+      /* True if we've already called .next() on the indexEnum, to "bracket"
+         the current block of terms: */
+      private boolean didIndexNext;
+
+      /* Next index term, bracketing the current block of terms; this is
+         only valid if didIndexNext is true: */
+      private BytesRef nextIndexTerm;
+
+      /* True after seekExact(TermState), do defer seeking.  If the app then
+         calls next() (which is not "typical"), then we'll do the real seek */
+      private boolean seekPending;
+
+      /* How many blocks we've read since last seek.  Once this
+         is >= indexEnum.getDivisor() we set indexIsCurrent to false (since
+         the index can no long bracket seek-within-block). */
+      private int blocksSinceSeek;
+
+      private byte[] termSuffixes;
+      private ByteArrayDataInput termSuffixesReader = new ByteArrayDataInput();
+
+      /* Common prefix used for all terms in this block. */
+      private int termBlockPrefix;
+
+      /* How many terms in current block */
+      private int blockTermCount;
+
+      private byte[] docFreqBytes;
+      private final ByteArrayDataInput freqReader = new ByteArrayDataInput();
+      private int metaDataUpto;
+
+      public SegmentTermsEnum() throws IOException {
+        in = (IndexInput) BlockTermsReader.this.in.clone();
+        in.seek(termsStartPointer);
+        indexEnum = indexReader.getFieldEnum(fieldInfo);
+        doOrd = indexReader.supportsOrd();
+        fieldTerm.field = fieldInfo.name;
+        state = postingsReader.newTermState();
+        state.totalTermFreq = -1;
+        state.ord = -1;
+
+        termSuffixes = new byte[128];
+        docFreqBytes = new byte[64];
+        //System.out.println("BTR.enum init this=" + this + " postingsReader=" + postingsReader);
+      }
+
+      @Override
+      public Comparator<BytesRef> getComparator() {
+        return BytesRef.getUTF8SortedAsUnicodeComparator();
+      }
+
+      // TODO: we may want an alternate mode here which is
+      // "if you are about to return NOT_FOUND I won't use
+      // the terms data from that"; eg FuzzyTermsEnum will
+      // (usually) just immediately call seek again if we
+      // return NOT_FOUND so it's a waste for us to fill in
+      // the term that was actually NOT_FOUND
+      @Override
+      public SeekStatus seekCeil(final BytesRef target, final boolean useCache) throws IOException {
+
+        if (indexEnum == null) {
+          throw new IllegalStateException("terms index was not loaded");
+        }
+   
+        //System.out.println("BTR.seek seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + term().utf8ToString() + " " + term() + " useCache=" + useCache + " indexIsCurrent=" + indexIsCurrent + " didIndexNext=" + didIndexNext + " seekPending=" + seekPending + " divisor=" + indexReader.getDivisor() + " this="  + this);
+        if (didIndexNext) {
+          if (nextIndexTerm == null) {
+            //System.out.println("  nextIndexTerm=null");
+          } else {
+            //System.out.println("  nextIndexTerm=" + nextIndexTerm.utf8ToString());
+          }
+        }
+
+        // Check cache
+        if (useCache) {
+          fieldTerm.term = target;
+          // TODO: should we differentiate "frozen"
+          // TermState (ie one that was cloned and
+          // cached/returned by termState()) from the
+          // malleable (primary) one?
+          final TermState cachedState = termsCache.get(fieldTerm);
+          if (cachedState != null) {
+            seekPending = true;
+            //System.out.println("  cached!");
+            seekExact(target, cachedState);
+            //System.out.println("  term=" + term.utf8ToString());
+            return SeekStatus.FOUND;
+          }
+        }
+
+        boolean doSeek = true;
+
+        // See if we can avoid seeking, because target term
+        // is after current term but before next index term:
+        if (indexIsCurrent) {
+
+          final int cmp = BytesRef.getUTF8SortedAsUnicodeComparator().compare(term, target);
+
+          if (cmp == 0) {
+            // Already at the requested term
+            return SeekStatus.FOUND;
+          } else if (cmp < 0) {
+
+            // Target term is after current term
+            if (!didIndexNext) {
+              if (indexEnum.next() == -1) {
+                nextIndexTerm = null;
+              } else {
+                nextIndexTerm = indexEnum.term();
+              }
+              //System.out.println("  now do index next() nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
+              didIndexNext = true;
+            }
+
+            if (nextIndexTerm == null || BytesRef.getUTF8SortedAsUnicodeComparator().compare(target, nextIndexTerm) < 0) {
+              // Optimization: requested term is within the
+              // same term block we are now in; skip seeking
+              // (but do scanning):
+              doSeek = false;
+              //System.out.println("  skip seek: nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
+            }
+          }
+        }
+
+        if (doSeek) {
+          //System.out.println("  seek");
+
+          // Ask terms index to find biggest indexed term (=
+          // first term in a block) that's <= our text:
+          in.seek(indexEnum.seek(target));
+          boolean result = nextBlock();
+
+          // Block must exist since, at least, the indexed term
+          // is in the block:
+          assert result;
+
+          indexIsCurrent = true;
+          didIndexNext = false;
+          blocksSinceSeek = 0;          
+
+          if (doOrd) {
+            state.ord = indexEnum.ord()-1;
+          }
+
+          term.copyBytes(indexEnum.term());
+          //System.out.println("  seek: term=" + term.utf8ToString());
+        } else {
+          //System.out.println("  skip seek");
+          if (state.termBlockOrd == blockTermCount && !nextBlock()) {
+            indexIsCurrent = false;
+            return SeekStatus.END;
+          }
+        }
+
+        seekPending = false;
+
+        int common = 0;
+
+        // Scan within block.  We could do this by calling
+        // _next() and testing the resulting term, but this
+        // is wasteful.  Instead, we first confirm the
+        // target matches the common prefix of this block,
+        // and then we scan the term bytes directly from the
+        // termSuffixesreader's byte[], saving a copy into
+        // the BytesRef term per term.  Only when we return
+        // do we then copy the bytes into the term.
+
+        while(true) {
+
+          // First, see if target term matches common prefix
+          // in this block:
+          if (common < termBlockPrefix) {
+            final int cmp = (term.bytes[common]&0xFF) - (target.bytes[target.offset + common]&0xFF);
+            if (cmp < 0) {
+
+              // TODO: maybe we should store common prefix
+              // in block header?  (instead of relying on
+              // last term of previous block)
+
+              // Target's prefix is after the common block
+              // prefix, so term cannot be in this block
+              // but it could be in next block.  We
+              // must scan to end-of-block to set common
+              // prefix for next block:
+              if (state.termBlockOrd < blockTermCount) {
+                while(state.termBlockOrd < blockTermCount-1) {
+                  state.termBlockOrd++;
+                  state.ord++;
+                  termSuffixesReader.skipBytes(termSuffixesReader.readVInt());
+                }
+                final int suffix = termSuffixesReader.readVInt();
+                term.length = termBlockPrefix + suffix;
+                if (term.bytes.length < term.length) {
+                  term.grow(term.length);
+                }
+                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+              }
+              state.ord++;
+              
+              if (!nextBlock()) {
+                indexIsCurrent = false;
+                return SeekStatus.END;
+              }
+              common = 0;
+
+            } else if (cmp > 0) {
+              // Target's prefix is before the common prefix
+              // of this block, so we position to start of
+              // block and return NOT_FOUND:
+              assert state.termBlockOrd == 0;
+
+              final int suffix = termSuffixesReader.readVInt();
+              term.length = termBlockPrefix + suffix;
+              if (term.bytes.length < term.length) {
+                term.grow(term.length);
+              }
+              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+              return SeekStatus.NOT_FOUND;
+            } else {
+              common++;
+            }
+
+            continue;
+          }
+
+          // Test every term in this block
+          while (true) {
+            state.termBlockOrd++;
+            state.ord++;
+
+            final int suffix = termSuffixesReader.readVInt();
+            
+            // We know the prefix matches, so just compare the new suffix:
+            final int termLen = termBlockPrefix + suffix;
+            int bytePos = termSuffixesReader.getPosition();
+
+            boolean next = false;
+            final int limit = target.offset + (termLen < target.length ? termLen : target.length);
+            int targetPos = target.offset + termBlockPrefix;
+            while(targetPos < limit) {
+              final int cmp = (termSuffixes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
+              if (cmp < 0) {
+                // Current term is still before the target;
+                // keep scanning
+                next = true;
+                break;
+              } else if (cmp > 0) {
+                // Done!  Current term is after target. Stop
+                // here, fill in real term, return NOT_FOUND.
+                term.length = termBlockPrefix + suffix;
+                if (term.bytes.length < term.length) {
+                  term.grow(term.length);
+                }
+                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+                //System.out.println("  NOT_FOUND");
+                return SeekStatus.NOT_FOUND;
+              }
+            }
+
+            if (!next && target.length <= termLen) {
+              term.length = termBlockPrefix + suffix;
+              if (term.bytes.length < term.length) {
+                term.grow(term.length);
+              }
+              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+
+              if (target.length == termLen) {
+                // Done!  Exact match.  Stop here, fill in
+                // real term, return FOUND.
+                //System.out.println("  FOUND");
+
+                if (useCache) {
+                  // Store in cache
+                  decodeMetaData();
+                  //System.out.println("  cache! state=" + state);
+                  termsCache.put(new FieldAndTerm(fieldTerm), (BlockTermState) state.clone());
+                }
+
+                return SeekStatus.FOUND;
+              } else {
+                //System.out.println("  NOT_FOUND");
+                return SeekStatus.NOT_FOUND;
+              }
+            }
+
+            if (state.termBlockOrd == blockTermCount) {
+              // Must pre-fill term for next block's common prefix
+              term.length = termBlockPrefix + suffix;
+              if (term.bytes.length < term.length) {
+                term.grow(term.length);
+              }
+              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+              break;
+            } else {
+              termSuffixesReader.skipBytes(suffix);
+            }
+          }
+
+          // The purpose of the terms dict index is to seek
+          // the enum to the closest index term before the
+          // term we are looking for.  So, we should never
+          // cross another index term (besides the first
+          // one) while we are scanning:
+
+          assert indexIsCurrent;
+
+          if (!nextBlock()) {
+            //System.out.println("  END");
+            indexIsCurrent = false;
+            return SeekStatus.END;
+          }
+          common = 0;
+        }
+      }
+
+      @Override
+      public BytesRef next() throws IOException {
+        //System.out.println("BTR.next() seekPending=" + seekPending + " pendingSeekCount=" + state.termBlockOrd);
+
+        // If seek was previously called and the term was cached,
+        // usually caller is just going to pull a D/&PEnum or get
+        // docFreq, etc.  But, if they then call next(),
+        // this method catches up all internal state so next()
+        // works properly:
+        if (seekPending) {
+          assert !indexIsCurrent;
+          in.seek(state.blockFilePointer);
+          final int pendingSeekCount = state.termBlockOrd;
+          boolean result = nextBlock();
+
+          final long savOrd = state.ord;
+
+          // Block must exist since seek(TermState) was called w/ a
+          // TermState previously returned by this enum when positioned
+          // on a real term:
+          assert result;
+
+          while(state.termBlockOrd < pendingSeekCount) {
+            BytesRef nextResult = _next();
+            assert nextResult != null;
+          }
+          seekPending = false;
+          state.ord = savOrd;
+        }
+        return _next();
+      }
+
+      /* Decodes only the term bytes of the next term.  If caller then asks for
+         metadata, ie docFreq, totalTermFreq or pulls a D/&PEnum, we then (lazily)
+         decode all metadata up to the current term. */
+      private BytesRef _next() throws IOException {
+        //System.out.println("BTR._next seg=" + segment + " this=" + this + " termCount=" + state.termBlockOrd + " (vs " + blockTermCount + ")");
+        if (state.termBlockOrd == blockTermCount && !nextBlock()) {
+          //System.out.println("  eof");
+          indexIsCurrent = false;
+          return null;
+        }
+
+        // TODO: cutover to something better for these ints!  simple64?
+        final int suffix = termSuffixesReader.readVInt();
+        //System.out.println("  suffix=" + suffix);
+
+        term.length = termBlockPrefix + suffix;
+        if (term.bytes.length < term.length) {
+          term.grow(term.length);
+        }
+        termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+        state.termBlockOrd++;
+
+        // NOTE: meaningless in the non-ord case
+        state.ord++;
+
+        //System.out.println("  return term=" + fieldInfo.name + ":" + term.utf8ToString() + " " + term + " tbOrd=" + state.termBlockOrd);
+        return term;
+      }
+
+      @Override
+      public BytesRef term() {
+        return term;
+      }
+
+      @Override
+      public int docFreq() throws IOException {
+        //System.out.println("BTR.docFreq");
+        decodeMetaData();
+        //System.out.println("  return " + state.docFreq);
+        return state.docFreq;
+      }
+
+      @Override
+      public long totalTermFreq() throws IOException {
+        decodeMetaData();
+        return state.totalTermFreq;
+      }
+
+      @Override
+      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
+        //System.out.println("BTR.docs this=" + this);
+        decodeMetaData();
+        //System.out.println("BTR.docs:  state.docFreq=" + state.docFreq);
+        return postingsReader.docs(fieldInfo, state, liveDocs, reuse, needsFreqs);
+      }
+
+      @Override
+      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
+        //System.out.println("BTR.d&p this=" + this);
+        decodeMetaData();
+        if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+          return null;
+        } else {
+          DocsAndPositionsEnum dpe = postingsReader.docsAndPositions(fieldInfo, state, liveDocs, reuse);
+          //System.out.println("  return d&pe=" + dpe);
+          return dpe;
+        }
+      }
+
+      @Override
+      public void seekExact(BytesRef target, TermState otherState) throws IOException {
+        //System.out.println("BTR.seekExact termState target=" + target.utf8ToString() + " " + target + " this=" + this);
+        assert otherState != null && otherState instanceof BlockTermState;
+        assert !doOrd || ((BlockTermState) otherState).ord < numTerms;
+        state.copyFrom(otherState);
+        seekPending = true;
+        indexIsCurrent = false;
+        term.copyBytes(target);
+      }
+      
+      @Override
+      public TermState termState() throws IOException {
+        //System.out.println("BTR.termState this=" + this);
+        decodeMetaData();
+        TermState ts = (TermState) state.clone();
+        //System.out.println("  return ts=" + ts);
+        return ts;
+      }
+
+      @Override
+      public void seekExact(long ord) throws IOException {
+        //System.out.println("BTR.seek by ord ord=" + ord);
+        if (indexEnum == null) {
+          throw new IllegalStateException("terms index was not loaded");
+        }
+
+        assert ord < numTerms;
+
+        // TODO: if ord is in same terms block and
+        // after current ord, we should avoid this seek just
+        // like we do in the seek(BytesRef) case
+        in.seek(indexEnum.seek(ord));
+        boolean result = nextBlock();
+
+        // Block must exist since ord < numTerms:
+        assert result;
+
+        indexIsCurrent = true;
+        didIndexNext = false;
+        blocksSinceSeek = 0;
+        seekPending = false;
+
+        state.ord = indexEnum.ord()-1;
+        assert state.ord >= -1: "ord=" + state.ord;
+        term.copyBytes(indexEnum.term());
+
+        // Now, scan:
+        int left = (int) (ord - state.ord);
+        while(left > 0) {
+          final BytesRef term = _next();
+          assert term != null;
+          left--;
+          assert indexIsCurrent;
+        }
+      }
+
+      @Override
+      public long ord() {
+        if (!doOrd) {
+          throw new UnsupportedOperationException();
+        }
+        return state.ord;
+      }
+
+      private void doPendingSeek() {
+      }
+
+      /* Does initial decode of next block of terms; this
+         doesn't actually decode the docFreq, totalTermFreq,
+         postings details (frq/prx offset, etc.) metadata;
+         it just loads them as byte[] blobs which are then      
+         decoded on-demand if the metadata is ever requested
+         for any term in this block.  This enables terms-only
+         intensive consumes (eg certain MTQs, respelling) to
+         not pay the price of decoding metadata they won't
+         use. */
+      private boolean nextBlock() throws IOException {
+
+        // TODO: we still lazy-decode the byte[] for each
+        // term (the suffix), but, if we decoded
+        // all N terms up front then seeking could do a fast
+        // bsearch w/in the block...
+
+        //System.out.println("BTR.nextBlock() fp=" + in.getFilePointer() + " this=" + this);
+        state.blockFilePointer = in.getFilePointer();
+        blockTermCount = in.readVInt();
+        //System.out.println("  blockTermCount=" + blockTermCount);
+        if (blockTermCount == 0) {
+          return false;
+        }
+        termBlockPrefix = in.readVInt();
+
+        // term suffixes:
+        int len = in.readVInt();
+        if (termSuffixes.length < len) {
+          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];
+        }
+        //System.out.println("  termSuffixes len=" + len);
+        in.readBytes(termSuffixes, 0, len);
+        termSuffixesReader.reset(termSuffixes, 0, len);
+
+        // docFreq, totalTermFreq
+        len = in.readVInt();
+        if (docFreqBytes.length < len) {
+          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];
+        }
+        //System.out.println("  freq bytes len=" + len);
+        in.readBytes(docFreqBytes, 0, len);
+        freqReader.reset(docFreqBytes, 0, len);
+        metaDataUpto = 0;
+
+        state.termBlockOrd = 0;
+
+        postingsReader.readTermsBlock(in, fieldInfo, state);
+
+        blocksSinceSeek++;
+        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());
+        //System.out.println("  indexIsCurrent=" + indexIsCurrent);
+
+        return true;
+      }
+     
+      private void decodeMetaData() throws IOException {
+        //System.out.println("BTR.decodeMetadata mdUpto=" + metaDataUpto + " vs termCount=" + state.termBlockOrd + " state=" + state);
+        if (!seekPending) {
+          // TODO: cutover to random-access API
+          // here.... really stupid that we have to decode N
+          // wasted term metadata just to get to the N+1th
+          // that we really need...
+
+          // lazily catch up on metadata decode:
+          final int limit = state.termBlockOrd;
+          // We must set/incr state.termCount because
+          // postings impl can look at this
+          state.termBlockOrd = metaDataUpto;
+          // TODO: better API would be "jump straight to term=N"???
+          while (metaDataUpto < limit) {
+            //System.out.println("  decode mdUpto=" + metaDataUpto);
+            // TODO: we could make "tiers" of metadata, ie,
+            // decode docFreq/totalTF but don't decode postings
+            // metadata; this way caller could get
+            // docFreq/totalTF w/o paying decode cost for
+            // postings
+
+            // TODO: if docFreq were bulk decoded we could
+            // just skipN here:
+            state.docFreq = freqReader.readVInt();
+            //System.out.println("    dF=" + state.docFreq);
+            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+              state.totalTermFreq = state.docFreq + freqReader.readVLong();
+              //System.out.println("    totTF=" + state.totalTermFreq);
+            }
+
+            postingsReader.nextTerm(fieldInfo, state);
+            metaDataUpto++;
+            state.termBlockOrd++;
+          }
+        } else {
+          //System.out.println("  skip! seekPending");
+        }
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/BlockTermsWriter.java b/lucene/src/java/org/apache/lucene/codecs/BlockTermsWriter.java
new file mode 100644
index 0000000..9342644
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/BlockTermsWriter.java
@@ -0,0 +1,319 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+// TODO: currently we encode all terms between two indexed
+// terms as a block; but, we could decouple the two, ie
+// allow several blocks in between two indexed terms
+
+/**
+ * Writes terms dict, block-encoding (column stride) each
+ * term's metadata for each set of terms between two
+ * index terms.
+ *
+ * @lucene.experimental
+ */
+
+public class BlockTermsWriter extends FieldsConsumer {
+
+  final static String CODEC_NAME = "BLOCK_TERMS_DICT";
+
+  // Initial format
+  public static final int VERSION_START = 0;
+
+  public static final int VERSION_CURRENT = VERSION_START;
+
+  /** Extension of terms file */
+  static final String TERMS_EXTENSION = "tib";
+
+  protected final IndexOutput out;
+  final PostingsWriterBase postingsWriter;
+  final FieldInfos fieldInfos;
+  FieldInfo currentField;
+  private final TermsIndexWriterBase termsIndexWriter;
+  private final List<TermsWriter> fields = new ArrayList<TermsWriter>();
+
+  // private final String segment;
+
+  public BlockTermsWriter(TermsIndexWriterBase termsIndexWriter,
+      SegmentWriteState state, PostingsWriterBase postingsWriter)
+      throws IOException {
+    final String termsFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_EXTENSION);
+    this.termsIndexWriter = termsIndexWriter;
+    out = state.directory.createOutput(termsFileName, state.context);
+    boolean success = false;
+    try {
+      fieldInfos = state.fieldInfos;
+      writeHeader(out);
+      currentField = null;
+      this.postingsWriter = postingsWriter;
+      // segment = state.segmentName;
+      
+      //System.out.println("BTW.init seg=" + state.segmentName);
+      
+      postingsWriter.start(out); // have consumer write its format/header
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); 
+
+    out.writeLong(0);                             // leave space for end index pointer    
+  }
+
+  @Override
+  public TermsConsumer addField(FieldInfo field) throws IOException {
+    //System.out.println("\nBTW.addField seg=" + segment + " field=" + field.name);
+    assert currentField == null || currentField.name.compareTo(field.name) < 0;
+    currentField = field;
+    TermsIndexWriterBase.FieldWriter fieldIndexWriter = termsIndexWriter.addField(field, out.getFilePointer());
+    final TermsWriter terms = new TermsWriter(fieldIndexWriter, field, postingsWriter);
+    fields.add(terms);
+    return terms;
+  }
+
+  @Override
+  public void close() throws IOException {
+
+    try {
+      
+      int nonZeroCount = 0;
+      for(TermsWriter field : fields) {
+        if (field.numTerms > 0) {
+          nonZeroCount++;
+        }
+      }
+
+      final long dirStart = out.getFilePointer();
+
+      out.writeVInt(nonZeroCount);
+      for(TermsWriter field : fields) {
+        if (field.numTerms > 0) {
+          out.writeVInt(field.fieldInfo.number);
+          out.writeVLong(field.numTerms);
+          out.writeVLong(field.termsStartPointer);
+          if (field.fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+            out.writeVLong(field.sumTotalTermFreq);
+          }
+          out.writeVLong(field.sumDocFreq);
+          out.writeVInt(field.docCount);
+        }
+      }
+      writeTrailer(dirStart);
+    } finally {
+      IOUtils.close(out, postingsWriter, termsIndexWriter);
+    }
+  }
+
+  protected void writeTrailer(long dirStart) throws IOException {
+    out.seek(CodecUtil.headerLength(CODEC_NAME));
+    out.writeLong(dirStart);    
+  }
+  
+  private static class TermEntry {
+    public final BytesRef term = new BytesRef();
+    public TermStats stats;
+  }
+
+  class TermsWriter extends TermsConsumer {
+    private final FieldInfo fieldInfo;
+    private final PostingsWriterBase postingsWriter;
+    private final long termsStartPointer;
+    private long numTerms;
+    private final TermsIndexWriterBase.FieldWriter fieldIndexWriter;
+    long sumTotalTermFreq;
+    long sumDocFreq;
+    int docCount;
+
+    private TermEntry[] pendingTerms;
+
+    private int pendingCount;
+
+    TermsWriter(
+        TermsIndexWriterBase.FieldWriter fieldIndexWriter,
+        FieldInfo fieldInfo,
+        PostingsWriterBase postingsWriter) 
+    {
+      this.fieldInfo = fieldInfo;
+      this.fieldIndexWriter = fieldIndexWriter;
+      pendingTerms = new TermEntry[32];
+      for(int i=0;i<pendingTerms.length;i++) {
+        pendingTerms[i] = new TermEntry();
+      }
+      termsStartPointer = out.getFilePointer();
+      postingsWriter.setField(fieldInfo);
+      this.postingsWriter = postingsWriter;
+    }
+    
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public PostingsConsumer startTerm(BytesRef text) throws IOException {
+      //System.out.println("BTW: startTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment);
+      postingsWriter.startTerm();
+      return postingsWriter;
+    }
+
+    private final BytesRef lastPrevTerm = new BytesRef();
+
+    @Override
+    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
+
+      assert stats.docFreq > 0;
+      //System.out.println("BTW: finishTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment + " df=" + stats.docFreq);
+
+      final boolean isIndexTerm = fieldIndexWriter.checkIndexTerm(text, stats);
+
+      if (isIndexTerm) {
+        if (pendingCount > 0) {
+          // Instead of writing each term, live, we gather terms
+          // in RAM in a pending buffer, and then write the
+          // entire block in between index terms:
+          flushBlock();
+        }
+        fieldIndexWriter.add(text, stats, out.getFilePointer());
+        //System.out.println("  index term!");
+      }
+
+      if (pendingTerms.length == pendingCount) {
+        final TermEntry[] newArray = new TermEntry[ArrayUtil.oversize(pendingCount+1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
+        System.arraycopy(pendingTerms, 0, newArray, 0, pendingCount);
+        for(int i=pendingCount;i<newArray.length;i++) {
+          newArray[i] = new TermEntry();
+        }
+        pendingTerms = newArray;
+      }
+      final TermEntry te = pendingTerms[pendingCount];
+      te.term.copyBytes(text);
+      te.stats = stats;
+
+      pendingCount++;
+
+      postingsWriter.finishTerm(stats);
+      numTerms++;
+    }
+
+    // Finishes all terms in this field
+    @Override
+    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
+      if (pendingCount > 0) {
+        flushBlock();
+      }
+      // EOF marker:
+      out.writeVInt(0);
+
+      this.sumTotalTermFreq = sumTotalTermFreq;
+      this.sumDocFreq = sumDocFreq;
+      this.docCount = docCount;
+      fieldIndexWriter.finish(out.getFilePointer());
+    }
+
+    private int sharedPrefix(BytesRef term1, BytesRef term2) {
+      assert term1.offset == 0;
+      assert term2.offset == 0;
+      int pos1 = 0;
+      int pos1End = pos1 + Math.min(term1.length, term2.length);
+      int pos2 = 0;
+      while(pos1 < pos1End) {
+        if (term1.bytes[pos1] != term2.bytes[pos2]) {
+          return pos1;
+        }
+        pos1++;
+        pos2++;
+      }
+      return pos1;
+    }
+
+    private final RAMOutputStream bytesWriter = new RAMOutputStream();
+
+    private void flushBlock() throws IOException {
+      //System.out.println("BTW.flushBlock seg=" + segment + " pendingCount=" + pendingCount + " fp=" + out.getFilePointer());
+
+      // First pass: compute common prefix for all terms
+      // in the block, against term before first term in
+      // this block:
+      int commonPrefix = sharedPrefix(lastPrevTerm, pendingTerms[0].term);
+      for(int termCount=1;termCount<pendingCount;termCount++) {
+        commonPrefix = Math.min(commonPrefix,
+                                sharedPrefix(lastPrevTerm,
+                                             pendingTerms[termCount].term));
+      }        
+
+      out.writeVInt(pendingCount);
+      out.writeVInt(commonPrefix);
+
+      // 2nd pass: write suffixes, as separate byte[] blob
+      for(int termCount=0;termCount<pendingCount;termCount++) {
+        final int suffix = pendingTerms[termCount].term.length - commonPrefix;
+        // TODO: cutover to better intblock codec, instead
+        // of interleaving here:
+        bytesWriter.writeVInt(suffix);
+        bytesWriter.writeBytes(pendingTerms[termCount].term.bytes, commonPrefix, suffix);
+      }
+      out.writeVInt((int) bytesWriter.getFilePointer());
+      bytesWriter.writeTo(out);
+      bytesWriter.reset();
+
+      // 3rd pass: write the freqs as byte[] blob
+      // TODO: cutover to better intblock codec.  simple64?
+      // write prefix, suffix first:
+      for(int termCount=0;termCount<pendingCount;termCount++) {
+        final TermStats stats = pendingTerms[termCount].stats;
+        assert stats != null;
+        bytesWriter.writeVInt(stats.docFreq);
+        if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+          bytesWriter.writeVLong(stats.totalTermFreq-stats.docFreq);
+        }
+      }
+
+      out.writeVInt((int) bytesWriter.getFilePointer());
+      bytesWriter.writeTo(out);
+      bytesWriter.reset();
+
+      postingsWriter.flushTermsBlock(pendingCount, pendingCount);
+      lastPrevTerm.copyBytes(pendingTerms[pendingCount-1].term);
+      pendingCount = 0;
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java b/lucene/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java
new file mode 100644
index 0000000..dce1797
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/BlockTreeTermsReader.java
@@ -0,0 +1,2845 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.Collection;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.TreeMap;
+
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.automaton.CompiledAutomaton;
+import org.apache.lucene.util.automaton.RunAutomaton;
+import org.apache.lucene.util.automaton.Transition;
+import org.apache.lucene.util.fst.ByteSequenceOutputs;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.Outputs;
+import org.apache.lucene.util.fst.Util;
+
+/** A block-based terms index and dictionary that assigns
+ *  terms to variable length blocks according to how they
+ *  share prefixes.  The terms index is a prefix trie
+ *  whose leaves are term blocks.  The advantage of this
+ *  approach is that seekExact is often able to
+ *  determine a term cannot exist without doing any IO, and
+ *  intersection with Automata is very fast.  Note that this
+ *  terms dictionary has it's own fixed terms index (ie, it
+ *  does not support a pluggable terms index
+ *  implementation).
+ *
+ *  <p><b>NOTE</b>: this terms dictionary does not support
+ *  index divisor when opening an IndexReader.  Instead, you
+ *  can change the min/maxItemsPerBlock during indexing.</p>
+ *
+ *  <p>The data structure used by this implementation is very
+ *  similar to a burst trie
+ *  (http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3499),
+ *  but with added logic to break up too-large blocks of all
+ *  terms sharing a given prefix into smaller ones.</p>
+ *
+ *  <p>Use {@link org.apache.lucene.index.CheckIndex} with the <code>-verbose</code>
+ *  option to see summary statistics on the blocks in the
+ *  dictionary.
+ *
+ *  See {@link BlockTreeTermsWriter}.
+ *
+ * @lucene.experimental
+ */
+
+public class BlockTreeTermsReader extends FieldsProducer {
+
+  // Open input to the main terms dict file (_X.tib)
+  private final IndexInput in;
+
+  //private static final boolean DEBUG = BlockTreeTermsWriter.DEBUG;
+
+  // Reads the terms dict entries, to gather state to
+  // produce DocsEnum on demand
+  private final PostingsReaderBase postingsReader;
+
+  private final TreeMap<String,FieldReader> fields = new TreeMap<String,FieldReader>();
+
+  // keeps the dirStart offset
+  protected long dirOffset;
+  protected long indexDirOffset;
+
+  private String segment;
+  
+  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, String segment,
+                              PostingsReaderBase postingsReader, IOContext ioContext,
+                              String segmentSuffix, int indexDivisor)
+    throws IOException {
+    
+    this.postingsReader = postingsReader;
+
+    this.segment = segment;
+    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),
+                       ioContext);
+
+    boolean success = false;
+    IndexInput indexIn = null;
+
+    try {
+      readHeader(in);
+      if (indexDivisor != -1) {
+        indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),
+                                ioContext);
+        readIndexHeader(indexIn);
+      }
+
+      // Have PostingsReader init itself
+      postingsReader.init(in);
+
+      // Read per-field details
+      seekDir(in, dirOffset);
+      if (indexDivisor != -1) {
+        seekDir(indexIn, indexDirOffset);
+      }
+
+      final int numFields = in.readVInt();
+
+      for(int i=0;i<numFields;i++) {
+        final int field = in.readVInt();
+        final long numTerms = in.readVLong();
+        assert numTerms >= 0;
+        final int numBytes = in.readVInt();
+        final BytesRef rootCode = new BytesRef(new byte[numBytes]);
+        in.readBytes(rootCode.bytes, 0, numBytes);
+        rootCode.length = numBytes;
+        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        assert fieldInfo != null: "field=" + field;
+        final long sumTotalTermFreq = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
+        final long sumDocFreq = in.readVLong();
+        final int docCount = in.readVInt();
+        final long indexStartFP = indexDivisor != -1 ? indexIn.readVLong() : 0;
+        assert !fields.containsKey(fieldInfo.name);
+        fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, indexIn));
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(indexIn, this);
+      } else if (indexDivisor != -1) {
+        indexIn.close();
+      }
+    }
+  }
+
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, BlockTreeTermsWriter.TERMS_CODEC_NAME,
+                          BlockTreeTermsWriter.TERMS_VERSION_START,
+                          BlockTreeTermsWriter.TERMS_VERSION_CURRENT);
+    dirOffset = input.readLong();    
+  }
+
+  protected void readIndexHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, BlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,
+                          BlockTreeTermsWriter.TERMS_INDEX_VERSION_START,
+                          BlockTreeTermsWriter.TERMS_INDEX_VERSION_CURRENT);
+    indexDirOffset = input.readLong();    
+  }
+  
+  protected void seekDir(IndexInput input, long dirOffset)
+      throws IOException {
+    input.seek(dirOffset);
+  }
+
+  // for debugging
+  // private static String toHex(int v) {
+  //   return "0x" + Integer.toHexString(v);
+  // }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(in, postingsReader);
+    } finally { 
+      // Clear so refs to terms index is GCable even if
+      // app hangs onto us:
+      fields.clear();
+    }
+  }
+
+  public static void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) {
+    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION));
+    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION));
+  }
+
+  @Override
+  public FieldsEnum iterator() {
+    return new TermFieldsEnum();
+  }
+
+  @Override
+  public Terms terms(String field) throws IOException {
+    return fields.get(field);
+  }
+
+  @Override
+  public int getUniqueFieldCount() {
+    return fields.size();
+  }
+
+  // Iterates through all fields
+  private class TermFieldsEnum extends FieldsEnum {
+    final Iterator<FieldReader> it;
+    FieldReader current;
+
+    TermFieldsEnum() {
+      it = fields.values().iterator();
+    }
+
+    @Override
+    public String next() {
+      if (it.hasNext()) {
+        current = it.next();
+        return current.fieldInfo.name;
+      } else {
+        current = null;
+        return null;
+      }
+    }
+    
+    @Override
+    public Terms terms() throws IOException {
+      return current;
+    }
+  }
+
+  // for debugging
+  String brToString(BytesRef b) {
+    if (b == null) {
+      return "null";
+    } else {
+      try {
+        return b.utf8ToString() + " " + b;
+      } catch (Throwable t) {
+        // If BytesRef isn't actually UTF8, or it's eg a
+        // prefix of UTF8 that ends mid-unicode-char, we
+        // fallback to hex:
+        return b.toString();
+      }
+    }
+  }
+
+  public static class Stats {
+    public int indexNodeCount;
+    public int indexArcCount;
+    public int indexNumBytes;
+
+    public long totalTermCount;
+    public long totalTermBytes;
+
+
+    public int nonFloorBlockCount;
+    public int floorBlockCount;
+    public int floorSubBlockCount;
+    public int mixedBlockCount;
+    public int termsOnlyBlockCount;
+    public int subBlocksOnlyBlockCount;
+    public int totalBlockCount;
+
+    public int[] blockCountByPrefixLen = new int[10];
+    private int startBlockCount;
+    private int endBlockCount;
+    public long totalBlockSuffixBytes;
+    public long totalBlockStatsBytes;
+
+    // Postings impl plus the other few vInts stored in
+    // the frame:
+    public long totalBlockOtherBytes;
+
+    public final String segment;
+    public final String field;
+
+    public Stats(String segment, String field) {
+      this.segment = segment;
+      this.field = field;
+    }
+
+    void startBlock(FieldReader.SegmentTermsEnum.Frame frame, boolean isFloor) {
+      totalBlockCount++;
+      if (isFloor) {
+        if (frame.fp == frame.fpOrig) {
+          floorBlockCount++;
+        }
+        floorSubBlockCount++;
+      } else {
+        nonFloorBlockCount++;
+      }
+
+      if (blockCountByPrefixLen.length <= frame.prefix) {
+        blockCountByPrefixLen = ArrayUtil.grow(blockCountByPrefixLen, 1+frame.prefix);
+      }
+      blockCountByPrefixLen[frame.prefix]++;
+      startBlockCount++;
+      totalBlockSuffixBytes += frame.suffixesReader.length();
+      totalBlockStatsBytes += frame.statsReader.length();
+    }
+
+    void endBlock(FieldReader.SegmentTermsEnum.Frame frame) {
+      final int termCount = frame.isLeafBlock ? frame.entCount : frame.state.termBlockOrd;
+      final int subBlockCount = frame.entCount - termCount;
+      totalTermCount += termCount;
+      if (termCount != 0 && subBlockCount != 0) {
+        mixedBlockCount++;
+      } else if (termCount != 0) {
+        termsOnlyBlockCount++;
+      } else if (subBlockCount != 0) {
+        subBlocksOnlyBlockCount++;
+      } else {
+        throw new IllegalStateException();
+      }
+      endBlockCount++;
+      final long otherBytes = frame.fpEnd - frame.fp - frame.suffixesReader.length() - frame.statsReader.length();
+      assert otherBytes > 0 : "otherBytes=" + otherBytes + " frame.fp=" + frame.fp + " frame.fpEnd=" + frame.fpEnd;
+      totalBlockOtherBytes += otherBytes;
+    }
+
+    void term(BytesRef term) {
+      totalTermBytes += term.length;
+    }
+
+    void finish() {
+      assert startBlockCount == endBlockCount: "startBlockCount=" + startBlockCount + " endBlockCount=" + endBlockCount;
+      assert totalBlockCount == floorSubBlockCount + nonFloorBlockCount: "floorSubBlockCount=" + floorSubBlockCount + " nonFloorBlockCount=" + nonFloorBlockCount + " totalBlockCount=" + totalBlockCount;
+      assert totalBlockCount == mixedBlockCount + termsOnlyBlockCount + subBlocksOnlyBlockCount: "totalBlockCount=" + totalBlockCount + " mixedBlockCount=" + mixedBlockCount + " subBlocksOnlyBlockCount=" + subBlocksOnlyBlockCount + " termsOnlyBlockCount=" + termsOnlyBlockCount;
+    }
+
+    @Override
+    public String toString() {
+      final ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
+      final PrintStream out = new PrintStream(bos);
+      
+      out.println("  index FST:");
+      out.println("    " + indexNodeCount + " nodes");
+      out.println("    " + indexArcCount + " arcs");
+      out.println("    " + indexNumBytes + " bytes");
+      out.println("  terms:");
+      out.println("    " + totalTermCount + " terms");
+      out.println("    " + totalTermBytes + " bytes" + (totalTermCount != 0 ? " (" + String.format("%.1f", ((double) totalTermBytes)/totalTermCount) + " bytes/term)" : ""));
+      out.println("  blocks:");
+      out.println("    " + totalBlockCount + " blocks");
+      out.println("    " + termsOnlyBlockCount + " terms-only blocks");
+      out.println("    " + subBlocksOnlyBlockCount + " sub-block-only blocks");
+      out.println("    " + mixedBlockCount + " mixed blocks");
+      out.println("    " + floorBlockCount + " floor blocks");
+      out.println("    " + (totalBlockCount-floorSubBlockCount) + " non-floor blocks");
+      out.println("    " + floorSubBlockCount + " floor sub-blocks");
+      out.println("    " + totalBlockSuffixBytes + " term suffix bytes" + (totalBlockCount != 0 ? " (" + String.format("%.1f", ((double) totalBlockSuffixBytes)/totalBlockCount) + " suffix-bytes/block)" : ""));
+      out.println("    " + totalBlockStatsBytes + " term stats bytes" + (totalBlockCount != 0 ? " (" + String.format("%.1f", ((double) totalBlockStatsBytes)/totalBlockCount) + " stats-bytes/block)" : ""));
+      out.println("    " + totalBlockOtherBytes + " other bytes" + (totalBlockCount != 0 ? " (" + String.format("%.1f", ((double) totalBlockOtherBytes)/totalBlockCount) + " other-bytes/block)" : ""));
+      if (totalBlockCount != 0) {
+        out.println("    by prefix length:");
+        int total = 0;
+        for(int prefix=0;prefix<blockCountByPrefixLen.length;prefix++) {
+          final int blockCount = blockCountByPrefixLen[prefix];
+          total += blockCount;
+          if (blockCount != 0) {
+            out.println("      " + String.format("%2d", prefix) + ": " + blockCount);
+          }
+        }
+        assert totalBlockCount == total;
+      }
+
+      return bos.toString();
+    }
+  }
+
+  final Outputs<BytesRef> fstOutputs = ByteSequenceOutputs.getSingleton();
+  final BytesRef NO_OUTPUT = fstOutputs.getNoOutput();
+
+  public final class FieldReader extends Terms {
+    final long numTerms;
+    final FieldInfo fieldInfo;
+    final long sumTotalTermFreq;
+    final long sumDocFreq;
+    final int docCount;
+    final long indexStartFP;
+    final long rootBlockFP;
+    final BytesRef rootCode;
+    private FST<BytesRef> index;
+
+    //private boolean DEBUG;
+
+    FieldReader(FieldInfo fieldInfo, long numTerms, BytesRef rootCode, long sumTotalTermFreq, long sumDocFreq, int docCount, long indexStartFP, IndexInput indexIn) throws IOException {
+      assert numTerms > 0;
+      this.fieldInfo = fieldInfo;
+      //DEBUG = BlockTreeTermsReader.DEBUG && fieldInfo.name.equals("id");
+      this.numTerms = numTerms;
+      this.sumTotalTermFreq = sumTotalTermFreq; 
+      this.sumDocFreq = sumDocFreq; 
+      this.docCount = docCount;
+      this.indexStartFP = indexStartFP;
+      this.rootCode = rootCode;
+      // if (DEBUG) {
+      //   System.out.println("BTTR: seg=" + segment + " field=" + fieldInfo.name + " rootBlockCode=" + rootCode + " divisor=" + indexDivisor);
+      // }
+
+      rootBlockFP = (new ByteArrayDataInput(rootCode.bytes, rootCode.offset, rootCode.length)).readVLong() >>> BlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS;
+
+      if (indexIn != null) {
+        final IndexInput clone = (IndexInput) indexIn.clone();
+        //System.out.println("start=" + indexStartFP + " field=" + fieldInfo.name);
+        clone.seek(indexStartFP);
+        index = new FST<BytesRef>(clone, ByteSequenceOutputs.getSingleton());
+        
+        /*
+        if (false) {
+          final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
+          Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
+          Util.toDot(index, w, false, false);
+          System.out.println("FST INDEX: SAVED to " + dotFileName);
+          w.close();
+        }
+        */
+      }
+    }
+
+    /** For debugging -- used by CheckIndex too*/
+    // TODO: maybe push this into Terms?
+    public Stats computeStats() throws IOException {
+      return new SegmentTermsEnum().computeBlockStats();
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      return new SegmentTermsEnum();
+    }
+
+    @Override
+    public long getUniqueTermCount() {
+      return numTerms;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return sumTotalTermFreq;
+    }
+
+    @Override
+    public long getSumDocFreq() {
+      return sumDocFreq;
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return docCount;
+    }
+
+    @Override
+    public TermsEnum intersect(CompiledAutomaton compiled, BytesRef startTerm) throws IOException {
+      if (compiled.type != CompiledAutomaton.AUTOMATON_TYPE.NORMAL) {
+        throw new IllegalArgumentException("please use CompiledAutomaton.getTermsEnum instead");
+      }
+      return new IntersectEnum(compiled, startTerm);
+    }
+    
+    // NOTE: cannot seek!
+    private final class IntersectEnum extends TermsEnum {
+      private final IndexInput in;
+
+      private Frame[] stack;
+      
+      @SuppressWarnings("unchecked") private FST.Arc<BytesRef>[] arcs = new FST.Arc[5];
+
+      private final RunAutomaton runAutomaton;
+      private final CompiledAutomaton compiledAutomaton;
+
+      private Frame currentFrame;
+
+      private final BytesRef term = new BytesRef();
+
+      // TODO: can we share this with the frame in STE?
+      private final class Frame {
+        final int ord;
+        long fp;
+        long fpOrig;
+        long fpEnd;
+        long lastSubFP;
+
+        // State in automaton
+        int state;
+
+        int metaDataUpto;
+
+        byte[] suffixBytes = new byte[128];
+        final ByteArrayDataInput suffixesReader = new ByteArrayDataInput();
+
+        byte[] statBytes = new byte[64];
+        final ByteArrayDataInput statsReader = new ByteArrayDataInput();
+
+        byte[] floorData = new byte[32];
+        final ByteArrayDataInput floorDataReader = new ByteArrayDataInput();
+
+        // Length of prefix shared by all terms in this block
+        int prefix;
+
+        // Number of entries (term or sub-block) in this block
+        int entCount;
+
+        // Which term we will next read
+        int nextEnt;
+
+        // True if this block is either not a floor block,
+        // or, it's the last sub-block of a floor block
+        boolean isLastInFloor;
+
+        // True if all entries are terms
+        boolean isLeafBlock;
+
+        int numFollowFloorBlocks;
+        int nextFloorLabel;
+        
+        Transition[] transitions;
+        int curTransitionMax;
+        int transitionIndex;
+
+        FST.Arc<BytesRef> arc;
+
+        final BlockTermState termState;
+
+        // Cumulative output so far
+        BytesRef outputPrefix;
+
+        private int startBytePos;
+        private int suffix;
+
+        public Frame(int ord) throws IOException {
+          this.ord = ord;
+          termState = postingsReader.newTermState();
+          termState.totalTermFreq = -1;
+        }
+
+        void loadNextFloorBlock() throws IOException {
+          assert numFollowFloorBlocks > 0;
+          //if (DEBUG) System.out.println("    loadNextFoorBlock trans=" + transitions[transitionIndex]);
+
+          do {
+            fp = fpOrig + (floorDataReader.readVLong() >>> 1);
+            numFollowFloorBlocks--;
+            // if (DEBUG) System.out.println("    skip floor block2!  nextFloorLabel=" + (char) nextFloorLabel + " vs target=" + (char) transitions[transitionIndex].getMin() + " newFP=" + fp + " numFollowFloorBlocks=" + numFollowFloorBlocks);
+            if (numFollowFloorBlocks != 0) {
+              nextFloorLabel = floorDataReader.readByte() & 0xff;
+            } else {
+              nextFloorLabel = 256;
+            }
+            // if (DEBUG) System.out.println("    nextFloorLabel=" + (char) nextFloorLabel);
+          } while (numFollowFloorBlocks != 0 && nextFloorLabel <= transitions[transitionIndex].getMin());
+
+          load(null);
+        }
+
+        public void setState(int state) {
+          this.state = state;
+          transitionIndex = 0;
+          transitions = compiledAutomaton.sortedTransitions[state];
+          if (transitions.length != 0) {
+            curTransitionMax = transitions[0].getMax();
+          } else {
+            curTransitionMax = -1;
+          }
+        }
+
+        void load(BytesRef frameIndexData) throws IOException {
+
+          // if (DEBUG) System.out.println("    load fp=" + fp + " fpOrig=" + fpOrig + " frameIndexData=" + frameIndexData + " trans=" + (transitions.length != 0 ? transitions[0] : "n/a" + " state=" + state));
+
+          if (frameIndexData != null && transitions.length != 0) {
+            // Floor frame
+            if (floorData.length < frameIndexData.length) {
+              this.floorData = new byte[ArrayUtil.oversize(frameIndexData.length, 1)];
+            }
+            System.arraycopy(frameIndexData.bytes, frameIndexData.offset, floorData, 0, frameIndexData.length);
+            floorDataReader.reset(floorData, 0, frameIndexData.length);
+            // Skip first long -- has redundant fp, hasTerms
+            // flag, isFloor flag
+            final long code = floorDataReader.readVLong();
+            if ((code & BlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR) != 0) {
+              numFollowFloorBlocks = floorDataReader.readVInt();
+              nextFloorLabel = floorDataReader.readByte() & 0xff;
+              // if (DEBUG) System.out.println("    numFollowFloorBlocks=" + numFollowFloorBlocks + " nextFloorLabel=" + nextFloorLabel);
+
+              // If current state is accept, we must process
+              // first block in case it has empty suffix:
+              if (!runAutomaton.isAccept(state)) {
+                // Maybe skip floor blocks:
+                while (numFollowFloorBlocks != 0 && nextFloorLabel <= transitions[0].getMin()) {
+                  fp = fpOrig + (floorDataReader.readVLong() >>> 1);
+                  numFollowFloorBlocks--;
+                  // if (DEBUG) System.out.println("    skip floor block!  nextFloorLabel=" + (char) nextFloorLabel + " vs target=" + (char) transitions[0].getMin() + " newFP=" + fp + " numFollowFloorBlocks=" + numFollowFloorBlocks);
+                  if (numFollowFloorBlocks != 0) {
+                    nextFloorLabel = floorDataReader.readByte() & 0xff;
+                  } else {
+                    nextFloorLabel = 256;
+                  }
+                }
+              }
+            }
+          }
+
+          in.seek(fp);
+          int code = in.readVInt();
+          entCount = code >>> 1;
+          assert entCount > 0;
+          isLastInFloor = (code & 1) != 0;
+
+          // term suffixes:
+          code = in.readVInt();
+          isLeafBlock = (code & 1) != 0;
+          int numBytes = code >>> 1;
+          // if (DEBUG) System.out.println("      entCount=" + entCount + " lastInFloor?=" + isLastInFloor + " leafBlock?=" + isLeafBlock + " numSuffixBytes=" + numBytes);
+          if (suffixBytes.length < numBytes) {
+            suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
+          }
+          in.readBytes(suffixBytes, 0, numBytes);
+          suffixesReader.reset(suffixBytes, 0, numBytes);
+
+          // stats
+          numBytes = in.readVInt();
+          if (statBytes.length < numBytes) {
+            statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
+          }
+          in.readBytes(statBytes, 0, numBytes);
+          statsReader.reset(statBytes, 0, numBytes);
+          metaDataUpto = 0;
+
+          termState.termBlockOrd = 0;
+          nextEnt = 0;
+          
+          postingsReader.readTermsBlock(in, fieldInfo, termState);
+
+          if (!isLastInFloor) {
+            // Sub-blocks of a single floor block are always
+            // written one after another -- tail recurse:
+            fpEnd = in.getFilePointer();
+          }
+        }
+
+        // TODO: maybe add scanToLabel; should give perf boost
+
+        public boolean next() {
+          return isLeafBlock ? nextLeaf() : nextNonLeaf();
+        }
+
+        // Decodes next entry; returns true if it's a sub-block
+        public boolean nextLeaf() {
+          //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
+          assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
+          nextEnt++;
+          suffix = suffixesReader.readVInt();
+          startBytePos = suffixesReader.getPosition();
+          suffixesReader.skipBytes(suffix);
+          return false;
+        }
+
+        public boolean nextNonLeaf() {
+          //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
+          assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
+          nextEnt++;
+          final int code = suffixesReader.readVInt();
+          suffix = code >>> 1;
+          startBytePos = suffixesReader.getPosition();
+          suffixesReader.skipBytes(suffix);
+          if ((code & 1) == 0) {
+            // A normal term
+            termState.termBlockOrd++;
+            return false;
+          } else {
+            // A sub-block; make sub-FP absolute:
+            lastSubFP = fp - suffixesReader.readVLong();
+            return true;
+          }
+        }
+
+        public int getTermBlockOrd() {
+          return isLeafBlock ? nextEnt : termState.termBlockOrd;
+        }
+
+        public void decodeMetaData() throws IOException {
+
+          // lazily catch up on metadata decode:
+          final int limit = getTermBlockOrd();
+          assert limit > 0;
+
+          // We must set/incr state.termCount because
+          // postings impl can look at this
+          termState.termBlockOrd = metaDataUpto;
+      
+          // TODO: better API would be "jump straight to term=N"???
+          while (metaDataUpto < limit) {
+
+            // TODO: we could make "tiers" of metadata, ie,
+            // decode docFreq/totalTF but don't decode postings
+            // metadata; this way caller could get
+            // docFreq/totalTF w/o paying decode cost for
+            // postings
+
+            // TODO: if docFreq were bulk decoded we could
+            // just skipN here:
+            termState.docFreq = statsReader.readVInt();
+            //if (DEBUG) System.out.println("    dF=" + state.docFreq);
+            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+              termState.totalTermFreq = termState.docFreq + statsReader.readVLong();
+              //if (DEBUG) System.out.println("    totTF=" + state.totalTermFreq);
+            }
+
+            postingsReader.nextTerm(fieldInfo, termState);
+            metaDataUpto++;
+            termState.termBlockOrd++;
+          }
+        }
+      }
+
+      private BytesRef savedStartTerm;
+      
+      // TODO: in some cases we can filter by length?  eg
+      // regexp foo*bar must be at least length 6 bytes
+      public IntersectEnum(CompiledAutomaton compiled, BytesRef startTerm) throws IOException {
+        // if (DEBUG) {
+        //   System.out.println("\nintEnum.init seg=" + segment + " commonSuffix=" + brToString(compiled.commonSuffixRef));
+        // }
+        runAutomaton = compiled.runAutomaton;
+        compiledAutomaton = compiled;
+        in = (IndexInput) BlockTreeTermsReader.this.in.clone();
+        stack = new Frame[5];
+        for(int idx=0;idx<stack.length;idx++) {
+          stack[idx] = new Frame(idx);
+        }
+        for(int arcIdx=0;arcIdx<arcs.length;arcIdx++) {
+          arcs[arcIdx] = new FST.Arc<BytesRef>();
+        }
+
+        // TODO: if the automaton is "smallish" we really
+        // should use the terms index to seek at least to
+        // the initial term and likely to subsequent terms
+        // (or, maybe just fallback to ATE for such cases).
+        // Else the seek cost of loading the frames will be
+        // too costly.
+
+        final FST.Arc<BytesRef> arc = index.getFirstArc(arcs[0]);
+        // Empty string prefix must have an output in the index!
+        assert arc.isFinal();
+
+        // Special pushFrame since it's the first one:
+        final Frame f = stack[0];
+        f.fp = f.fpOrig = rootBlockFP;
+        f.prefix = 0;
+        f.setState(runAutomaton.getInitialState());
+        f.arc = arc;
+        f.outputPrefix = arc.output;
+        f.load(rootCode);
+
+        // for assert:
+        assert setSavedStartTerm(startTerm);
+
+        currentFrame = f;
+        if (startTerm != null) {
+          seekToStartTerm(startTerm);
+        }
+      }
+
+      // only for assert:
+      private boolean setSavedStartTerm(BytesRef startTerm) {
+        savedStartTerm = startTerm == null ? null : BytesRef.deepCopyOf(startTerm);
+        return true;
+      }
+
+      @Override
+      public TermState termState() throws IOException {
+        currentFrame.decodeMetaData();
+        return (TermState) currentFrame.termState.clone();
+      }
+
+      private Frame getFrame(int ord) throws IOException {
+        if (ord >= stack.length) {
+          final Frame[] next = new Frame[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
+          System.arraycopy(stack, 0, next, 0, stack.length);
+          for(int stackOrd=stack.length;stackOrd<next.length;stackOrd++) {
+            next[stackOrd] = new Frame(stackOrd);
+          }
+          stack = next;
+        }
+        assert stack[ord].ord == ord;
+        return stack[ord];
+      }
+
+      private FST.Arc<BytesRef> getArc(int ord) {
+        if (ord >= arcs.length) {
+          @SuppressWarnings("unchecked") final FST.Arc<BytesRef>[] next = new FST.Arc[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
+          System.arraycopy(arcs, 0, next, 0, arcs.length);
+          for(int arcOrd=arcs.length;arcOrd<next.length;arcOrd++) {
+            next[arcOrd] = new FST.Arc<BytesRef>();
+          }
+          arcs = next;
+        }
+        return arcs[ord];
+      }
+
+      private Frame pushFrame(int state) throws IOException {
+        final Frame f = getFrame(currentFrame == null ? 0 : 1+currentFrame.ord);
+        
+        f.fp = f.fpOrig = currentFrame.lastSubFP;
+        f.prefix = currentFrame.prefix + currentFrame.suffix;
+        // if (DEBUG) System.out.println("    pushFrame state=" + state + " prefix=" + f.prefix);
+        f.setState(state);
+
+        // Walk the arc through the index -- we only
+        // "bother" with this so we can get the floor data
+        // from the index and skip floor blocks when
+        // possible:
+        FST.Arc<BytesRef> arc = currentFrame.arc;
+        int idx = currentFrame.prefix;
+        assert currentFrame.suffix > 0;
+        BytesRef output = currentFrame.outputPrefix;
+        while (idx < f.prefix) {
+          final int target = term.bytes[idx] & 0xff;
+          // TODO: we could be more efficient for the next()
+          // case by using current arc as starting point,
+          // passed to findTargetArc
+          arc = index.findTargetArc(target, arc, getArc(1+idx));
+          assert arc != null;
+          output = fstOutputs.add(output, arc.output);
+          idx++;
+        }
+
+        f.arc = arc;
+        f.outputPrefix = output;
+        assert arc.isFinal();
+        f.load(fstOutputs.add(output, arc.nextFinalOutput));
+        return f;
+      }
+
+      @Override
+      public BytesRef term() throws IOException {
+        return term;
+      }
+
+      @Override
+      public int docFreq() throws IOException {
+        //if (DEBUG) System.out.println("BTIR.docFreq");
+        currentFrame.decodeMetaData();
+        //if (DEBUG) System.out.println("  return " + currentFrame.termState.docFreq);
+        return currentFrame.termState.docFreq;
+      }
+
+      @Override
+      public long totalTermFreq() throws IOException {
+        currentFrame.decodeMetaData();
+        return currentFrame.termState.totalTermFreq;
+      }
+
+      @Override
+      public DocsEnum docs(Bits skipDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
+        currentFrame.decodeMetaData();
+        return postingsReader.docs(fieldInfo, currentFrame.termState, skipDocs, reuse, needsFreqs);
+      }
+
+      @Override
+      public DocsAndPositionsEnum docsAndPositions(Bits skipDocs, DocsAndPositionsEnum reuse) throws IOException {
+        if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+          return null;
+        } else {
+          currentFrame.decodeMetaData();
+          return postingsReader.docsAndPositions(fieldInfo, currentFrame.termState, skipDocs, reuse);
+        }
+      }
+
+      private int getState() {
+        int state = currentFrame.state;
+        for(int idx=0;idx<currentFrame.suffix;idx++) {
+          state = runAutomaton.step(state,  currentFrame.suffixBytes[currentFrame.startBytePos+idx] & 0xff);
+          assert state != -1;
+        }
+        return state;
+      }
+
+      // NOTE: specialized to only doing the first-time
+      // seek, but we could generalize it to allow
+      // arbitrary seekExact/Ceil.  Note that this is a
+      // seekFloor!
+      private void seekToStartTerm(BytesRef target) throws IOException {
+        //if (DEBUG) System.out.println("seek to startTerm=" + target.utf8ToString());
+        assert currentFrame.ord == 0;
+        if (term.length < target.length) {
+          term.bytes = ArrayUtil.grow(term.bytes, target.length);
+        }
+        FST.Arc<BytesRef> arc = arcs[0];
+        assert arc == currentFrame.arc;
+
+        for(int idx=0;idx<=target.length;idx++) {
+
+          while (true) {
+            final int savePos = currentFrame.suffixesReader.getPosition();
+            final int saveStartBytePos = currentFrame.startBytePos;
+            final int saveSuffix = currentFrame.suffix;
+            final long saveLastSubFP = currentFrame.lastSubFP;
+            final int saveTermBlockOrd = currentFrame.termState.termBlockOrd;
+
+            final boolean isSubBlock = currentFrame.next();
+
+            //if (DEBUG) System.out.println("    cycle ent=" + currentFrame.nextEnt + " (of " + currentFrame.entCount + ") prefix=" + currentFrame.prefix + " suffix=" + currentFrame.suffix + " isBlock=" + isSubBlock + " firstLabel=" + (currentFrame.suffix == 0 ? "" : (currentFrame.suffixBytes[currentFrame.startBytePos])&0xff));
+            term.length = currentFrame.prefix + currentFrame.suffix;
+            if (term.bytes.length < term.length) {
+              term.bytes = ArrayUtil.grow(term.bytes, term.length);
+            }
+            System.arraycopy(currentFrame.suffixBytes, currentFrame.startBytePos, term.bytes, currentFrame.prefix, currentFrame.suffix);
+
+            if (isSubBlock && StringHelper.startsWith(target, term)) {
+              // Recurse
+              //if (DEBUG) System.out.println("      recurse!");
+              currentFrame = pushFrame(getState());
+              break;
+            } else {
+              final int cmp = term.compareTo(target);
+              if (cmp < 0) {
+                if (currentFrame.nextEnt == currentFrame.entCount) {
+                  if (!currentFrame.isLastInFloor) {
+                    //if (DEBUG) System.out.println("  load floorBlock");
+                    currentFrame.loadNextFloorBlock();
+                    continue;
+                  } else {
+                    //if (DEBUG) System.out.println("  return term=" + brToString(term));
+                    return;
+                  }
+                }
+                continue;
+              } else if (cmp == 0) {
+                //if (DEBUG) System.out.println("  return term=" + brToString(term));
+                return;
+              } else {
+                // Fallback to prior entry: the semantics of
+                // this method is that the first call to
+                // next() will return the term after the
+                // requested term
+                currentFrame.nextEnt--;
+                currentFrame.lastSubFP = saveLastSubFP;
+                currentFrame.startBytePos = saveStartBytePos;
+                currentFrame.suffix = saveSuffix;
+                currentFrame.suffixesReader.setPosition(savePos);
+                currentFrame.termState.termBlockOrd = saveTermBlockOrd;
+                System.arraycopy(currentFrame.suffixBytes, currentFrame.startBytePos, term.bytes, currentFrame.prefix, currentFrame.suffix);
+                term.length = currentFrame.prefix + currentFrame.suffix;
+                // If the last entry was a block we don't
+                // need to bother recursing and pushing to
+                // the last term under it because the first
+                // next() will simply skip the frame anyway
+                return;
+              }
+            }
+          }
+        }
+
+        assert false;
+      }
+
+      @Override
+      public BytesRef next() throws IOException {
+
+        // if (DEBUG) {
+        //   System.out.println("\nintEnum.next seg=" + segment);
+        //   System.out.println("  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
+        // }
+
+        nextTerm:
+        while(true) {
+          // Pop finished frames
+          while (currentFrame.nextEnt == currentFrame.entCount) {
+            if (!currentFrame.isLastInFloor) {
+              //if (DEBUG) System.out.println("    next-floor-block");
+              currentFrame.loadNextFloorBlock();
+              //if (DEBUG) System.out.println("\n  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
+            } else {
+              //if (DEBUG) System.out.println("  pop frame");
+              if (currentFrame.ord == 0) {
+                return null;
+              }
+              final long lastFP = currentFrame.fpOrig;
+              currentFrame = stack[currentFrame.ord-1];
+              assert currentFrame.lastSubFP == lastFP;
+              //if (DEBUG) System.out.println("\n  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
+            }
+          }
+
+          final boolean isSubBlock = currentFrame.next();
+          // if (DEBUG) {
+          //   final BytesRef suffixRef = new BytesRef();
+          //   suffixRef.bytes = currentFrame.suffixBytes;
+          //   suffixRef.offset = currentFrame.startBytePos;
+          //   suffixRef.length = currentFrame.suffix;
+          //   System.out.println("    " + (isSubBlock ? "sub-block" : "term") + " " + currentFrame.nextEnt + " (of " + currentFrame.entCount + ") suffix=" + brToString(suffixRef));
+          // }
+
+          if (currentFrame.suffix != 0) {
+            final int label = currentFrame.suffixBytes[currentFrame.startBytePos] & 0xff;
+            while (label > currentFrame.curTransitionMax) {
+              if (currentFrame.transitionIndex >= currentFrame.transitions.length-1) {
+                // Stop processing this frame -- no further
+                // matches are possible because we've moved
+                // beyond what the max transition will allow
+                //if (DEBUG) System.out.println("      break: trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]));
+
+                // sneaky!  forces a pop above
+                currentFrame.isLastInFloor = true;
+                currentFrame.nextEnt = currentFrame.entCount;
+                continue nextTerm;
+              }
+              currentFrame.transitionIndex++;
+              currentFrame.curTransitionMax = currentFrame.transitions[currentFrame.transitionIndex].getMax();
+              //if (DEBUG) System.out.println("      next trans=" + currentFrame.transitions[currentFrame.transitionIndex]);
+            }
+          }
+
+          // First test the common suffix, if set:
+          if (compiledAutomaton.commonSuffixRef != null && !isSubBlock) {
+            final int termLen = currentFrame.prefix + currentFrame.suffix;
+            if (termLen < compiledAutomaton.commonSuffixRef.length) {
+              // No match
+              // if (DEBUG) {
+              //   System.out.println("      skip: common suffix length");
+              // }
+              continue nextTerm;
+            }
+
+            final byte[] suffixBytes = currentFrame.suffixBytes;
+            final byte[] commonSuffixBytes = compiledAutomaton.commonSuffixRef.bytes;
+
+            final int lenInPrefix = compiledAutomaton.commonSuffixRef.length - currentFrame.suffix;
+            assert compiledAutomaton.commonSuffixRef.offset == 0;
+            int suffixBytesPos;
+            int commonSuffixBytesPos = 0;
+
+            if (lenInPrefix > 0) {
+              // A prefix of the common suffix overlaps with
+              // the suffix of the block prefix so we first
+              // test whether the prefix part matches:
+              final byte[] termBytes = term.bytes;
+              int termBytesPos = currentFrame.prefix - lenInPrefix;
+              assert termBytesPos >= 0;
+              final int termBytesPosEnd = currentFrame.prefix;
+              while (termBytesPos < termBytesPosEnd) {
+                if (termBytes[termBytesPos++] != commonSuffixBytes[commonSuffixBytesPos++]) {
+                  // if (DEBUG) {
+                  //   System.out.println("      skip: common suffix mismatch (in prefix)");
+                  // }
+                  continue nextTerm;
+                }
+              }
+              suffixBytesPos = currentFrame.startBytePos;
+            } else {
+              suffixBytesPos = currentFrame.startBytePos + currentFrame.suffix - compiledAutomaton.commonSuffixRef.length;
+            }
+
+            // Test overlapping suffix part:
+            final int commonSuffixBytesPosEnd = compiledAutomaton.commonSuffixRef.length;
+            while (commonSuffixBytesPos < commonSuffixBytesPosEnd) {
+              if (suffixBytes[suffixBytesPos++] != commonSuffixBytes[commonSuffixBytesPos++]) {
+                // if (DEBUG) {
+                //   System.out.println("      skip: common suffix mismatch");
+                // }
+                continue nextTerm;
+              }
+            }
+          }
+
+          // TODO: maybe we should do the same linear test
+          // that AutomatonTermsEnum does, so that if we
+          // reach a part of the automaton where .* is
+          // "temporarily" accepted, we just blindly .next()
+          // until the limit
+
+          // See if the term prefix matches the automaton:
+          int state = currentFrame.state;
+          for (int idx=0;idx<currentFrame.suffix;idx++) {
+            state = runAutomaton.step(state,  currentFrame.suffixBytes[currentFrame.startBytePos+idx] & 0xff);
+            if (state == -1) {
+              // No match
+              //System.out.println("    no s=" + state);
+              continue nextTerm;
+            } else {
+              //System.out.println("    c s=" + state);
+            }
+          }
+
+          if (isSubBlock) {
+            // Match!  Recurse:
+            //if (DEBUG) System.out.println("      sub-block match to state=" + state + "; recurse fp=" + currentFrame.lastSubFP);
+            copyTerm();
+            currentFrame = pushFrame(state);
+            //if (DEBUG) System.out.println("\n  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
+          } else if (runAutomaton.isAccept(state)) {
+            copyTerm();
+            //if (DEBUG) System.out.println("      term match to state=" + state + "; return term=" + brToString(term));
+            assert savedStartTerm == null || term.compareTo(savedStartTerm) > 0: "saveStartTerm=" + savedStartTerm.utf8ToString() + " term=" + term.utf8ToString();
+            return term;
+          } else {
+            //System.out.println("    no s=" + state);
+          }
+        }
+      }
+
+      private void copyTerm() {
+        //System.out.println("      copyTerm cur.prefix=" + currentFrame.prefix + " cur.suffix=" + currentFrame.suffix + " first=" + (char) currentFrame.suffixBytes[currentFrame.startBytePos]);
+        final int len = currentFrame.prefix + currentFrame.suffix;
+        if (term.bytes.length < len) {
+          term.bytes = ArrayUtil.grow(term.bytes, len);
+        }
+        System.arraycopy(currentFrame.suffixBytes, currentFrame.startBytePos, term.bytes, currentFrame.prefix, currentFrame.suffix);
+        term.length = len;
+      }
+
+      @Override
+      public Comparator<BytesRef> getComparator() {
+        return BytesRef.getUTF8SortedAsUnicodeComparator();
+      }
+
+      @Override
+      public boolean seekExact(BytesRef text, boolean useCache) throws IOException {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public void seekExact(long ord) throws IOException {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public long ord() throws IOException {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException {
+        throw new UnsupportedOperationException();
+      }
+    }
+
+    // Iterates through terms in this field
+    private final class SegmentTermsEnum extends TermsEnum {
+      private IndexInput in;
+
+      private Frame[] stack;
+      private final Frame staticFrame;
+      private Frame currentFrame;
+      private boolean termExists;
+
+      private int targetBeforeCurrentLength;
+
+      private final ByteArrayDataInput scratchReader = new ByteArrayDataInput();
+
+      // What prefix of the current term was present in the index:
+      private int validIndexPrefix;
+
+      // assert only:
+      private boolean eof;
+
+      final BytesRef term = new BytesRef();
+
+      @SuppressWarnings("unchecked") private FST.Arc<BytesRef>[] arcs = new FST.Arc[1];
+
+      public SegmentTermsEnum() throws IOException {
+        //if (DEBUG) System.out.println("BTTR.init seg=" + segment);
+        stack = new Frame[0];
+        
+        // Used to hold seek by TermState, or cached seek
+        staticFrame = new Frame(-1);
+
+        // Init w/ root block; don't use index since it may
+        // not (and need not) have been loaded
+        for(int arcIdx=0;arcIdx<arcs.length;arcIdx++) {
+          arcs[arcIdx] = new FST.Arc<BytesRef>();
+        }
+
+        currentFrame = staticFrame;
+        final FST.Arc<BytesRef> arc;
+        if (index != null) {
+          arc = index.getFirstArc(arcs[0]);
+          // Empty string prefix must have an output in the index!
+          assert arc.isFinal();
+        } else {
+          arc = null;
+        }
+        currentFrame = staticFrame;
+        //currentFrame = pushFrame(arc, rootCode, 0);
+        //currentFrame.loadBlock();
+        validIndexPrefix = 0;
+        // if (DEBUG) {
+        //   System.out.println("init frame state " + currentFrame.ord);
+        //   printSeekState();
+        // }
+
+        //System.out.println();
+        // computeBlockStats().print(System.out);
+      }
+
+      private void initIndexInput() {
+        if (this.in == null) {
+          this.in = (IndexInput) BlockTreeTermsReader.this.in.clone();
+        }
+      }
+
+      /** Runs next() through the entire terms dict,
+       *  computing aggregate statistics. */
+      public Stats computeBlockStats() throws IOException {
+
+        Stats stats = new Stats(segment, fieldInfo.name);
+        if (index != null) {
+          stats.indexNodeCount = index.getNodeCount();
+          stats.indexArcCount = index.getArcCount();
+          stats.indexNumBytes = index.sizeInBytes();
+        }
+        
+        currentFrame = staticFrame;
+        FST.Arc<BytesRef> arc;
+        if (index != null) {
+          arc = index.getFirstArc(arcs[0]);
+          // Empty string prefix must have an output in the index!
+          assert arc.isFinal();
+        } else {
+          arc = null;
+        }
+
+        // Empty string prefix must have an output in the
+        // index!
+        currentFrame = pushFrame(arc, rootCode, 0);
+        currentFrame.fpOrig = currentFrame.fp;
+        currentFrame.loadBlock();
+        validIndexPrefix = 0;
+
+        stats.startBlock(currentFrame, !currentFrame.isLastInFloor);
+
+        allTerms:
+        while (true) {
+
+          // Pop finished blocks
+          while (currentFrame.nextEnt == currentFrame.entCount) {
+            stats.endBlock(currentFrame);
+            if (!currentFrame.isLastInFloor) {
+              currentFrame.loadNextFloorBlock();
+              stats.startBlock(currentFrame, true);
+            } else {
+              if (currentFrame.ord == 0) {
+                break allTerms;
+              }
+              final long lastFP = currentFrame.fpOrig;
+              currentFrame = stack[currentFrame.ord-1];
+              assert lastFP == currentFrame.lastSubFP;
+              // if (DEBUG) {
+              //   System.out.println("  reset validIndexPrefix=" + validIndexPrefix);
+              // }
+            }
+          }
+
+          while(true) {
+            if (currentFrame.next()) {
+              // Push to new block:
+              currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length);
+              currentFrame.fpOrig = currentFrame.fp;
+              // This is a "next" frame -- even if it's
+              // floor'd we must pretend it isn't so we don't
+              // try to scan to the right floor frame:
+              currentFrame.isFloor = false;
+              //currentFrame.hasTerms = true;
+              currentFrame.loadBlock();
+              stats.startBlock(currentFrame, !currentFrame.isLastInFloor);
+            } else {
+              stats.term(term);
+              break;
+            }
+          }
+        }
+
+        stats.finish();
+
+        // Put root frame back:
+        currentFrame = staticFrame;
+        if (index != null) {
+          arc = index.getFirstArc(arcs[0]);
+          // Empty string prefix must have an output in the index!
+          assert arc.isFinal();
+        } else {
+          arc = null;
+        }
+        currentFrame = pushFrame(arc, rootCode, 0);
+        currentFrame.rewind();
+        currentFrame.loadBlock();
+        validIndexPrefix = 0;
+        term.length = 0;
+
+        return stats;
+      }
+
+      private Frame getFrame(int ord) throws IOException {
+        if (ord >= stack.length) {
+          final Frame[] next = new Frame[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
+          System.arraycopy(stack, 0, next, 0, stack.length);
+          for(int stackOrd=stack.length;stackOrd<next.length;stackOrd++) {
+            next[stackOrd] = new Frame(stackOrd);
+          }
+          stack = next;
+        }
+        assert stack[ord].ord == ord;
+        return stack[ord];
+      }
+
+      private FST.Arc<BytesRef> getArc(int ord) {
+        if (ord >= arcs.length) {
+          @SuppressWarnings("unchecked") final FST.Arc<BytesRef>[] next = new FST.Arc[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
+          System.arraycopy(arcs, 0, next, 0, arcs.length);
+          for(int arcOrd=arcs.length;arcOrd<next.length;arcOrd++) {
+            next[arcOrd] = new FST.Arc<BytesRef>();
+          }
+          arcs = next;
+        }
+        return arcs[ord];
+      }
+
+      @Override
+      public Comparator<BytesRef> getComparator() {
+        return BytesRef.getUTF8SortedAsUnicodeComparator();
+      }
+
+      // Pushes a frame we seek'd to
+      Frame pushFrame(FST.Arc<BytesRef> arc, BytesRef frameData, int length) throws IOException {
+        scratchReader.reset(frameData.bytes, frameData.offset, frameData.length);
+        final long code = scratchReader.readVLong();
+        final long fpSeek = code >>> BlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS;
+        final Frame f = getFrame(1+currentFrame.ord);
+        f.hasTerms = (code & BlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS) != 0;
+        f.hasTermsOrig = f.hasTerms;
+        f.isFloor = (code & BlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR) != 0;
+        if (f.isFloor) {
+          f.setFloorData(scratchReader, frameData);
+        }
+        pushFrame(arc, fpSeek, length);
+
+        return f;
+      }
+
+      // Pushes next'd frame or seek'd frame; we later
+      // lazy-load the frame only when needed
+      Frame pushFrame(FST.Arc<BytesRef> arc, long fp, int length) throws IOException {
+        final Frame f = getFrame(1+currentFrame.ord);
+        f.arc = arc;
+        if (f.fpOrig == fp && f.nextEnt != -1) {
+          //if (DEBUG) System.out.println("      push reused frame ord=" + f.ord + " fp=" + f.fp + " isFloor?=" + f.isFloor + " hasTerms=" + f.hasTerms + " pref=" + term + " nextEnt=" + f.nextEnt + " targetBeforeCurrentLength=" + targetBeforeCurrentLength + " term.length=" + term.length + " vs prefix=" + f.prefix);
+          if (f.prefix > targetBeforeCurrentLength) {
+            f.rewind();
+          } else {
+            // if (DEBUG) {
+            //   System.out.println("        skip rewind!");
+            // }
+          }
+          assert length == f.prefix;
+        } else {
+          f.nextEnt = -1;
+          f.prefix = length;
+          f.state.termBlockOrd = 0;
+          f.fpOrig = f.fp = fp;
+          f.lastSubFP = -1;
+          // if (DEBUG) {
+          //   final int sav = term.length;
+          //   term.length = length;
+          //   System.out.println("      push new frame ord=" + f.ord + " fp=" + f.fp + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " pref=" + brToString(term));
+          //   term.length = sav;
+          // }
+        }
+
+        return f;
+      }
+
+      // asserts only
+      private boolean clearEOF() {
+        eof = false;
+        return true;
+      }
+
+      // asserts only
+      private boolean setEOF() {
+        eof = true;
+        return true;
+      }
+
+      @Override
+      public boolean seekExact(final BytesRef target, final boolean useCache) throws IOException {
+
+        if (index == null) {
+          throw new IllegalStateException("terms index was not loaded");
+        }
+
+        if (term.bytes.length <= target.length) {
+          term.bytes = ArrayUtil.grow(term.bytes, 1+target.length);
+        }
+
+        assert clearEOF();
+
+        // if (DEBUG) {
+        //   System.out.println("\nBTTR.seekExact seg=" + segment + " target=" + fieldInfo.name + ":" + brToString(target) + " current=" + brToString(term) + " (exists?=" + termExists + ") validIndexPrefix=" + validIndexPrefix);
+        //   printSeekState();
+        // }
+
+        FST.Arc<BytesRef> arc;
+        int targetUpto;
+        BytesRef output;
+
+        targetBeforeCurrentLength = currentFrame.ord;
+
+        if (currentFrame != staticFrame) {
+
+          // We are already seek'd; find the common
+          // prefix of new seek term vs current term and
+          // re-use the corresponding seek state.  For
+          // example, if app first seeks to foobar, then
+          // seeks to foobaz, we can re-use the seek state
+          // for the first 5 bytes.
+
+          // if (DEBUG) {
+          //   System.out.println("  re-use current seek state validIndexPrefix=" + validIndexPrefix);
+          // }
+
+          arc = arcs[0];
+          assert arc.isFinal();
+          output = arc.output;
+          targetUpto = 0;
+          
+          Frame lastFrame = stack[0];
+          assert validIndexPrefix <= term.length;
+
+          final int targetLimit = Math.min(target.length, validIndexPrefix);
+
+          int cmp = 0;
+
+          // TODO: reverse vLong byte order for better FST
+          // prefix output sharing
+
+          // First compare up to valid seek frames:
+          while (targetUpto < targetLimit) {
+            cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
+            // if (DEBUG) {
+            //   System.out.println("    cycle targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")"   + " arc.output=" + arc.output + " output=" + output);
+            // }
+            if (cmp != 0) {
+              break;
+            }
+            arc = arcs[1+targetUpto];
+            //if (arc.label != (target.bytes[target.offset + targetUpto] & 0xFF)) {
+            //System.out.println("FAIL: arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF));
+            //}
+            assert arc.label == (target.bytes[target.offset + targetUpto] & 0xFF): "arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF);
+            if (arc.output != NO_OUTPUT) {
+              output = fstOutputs.add(output, arc.output);
+            }
+            if (arc.isFinal()) {
+              lastFrame = stack[1+lastFrame.ord];
+            }
+            targetUpto++;
+          }
+
+          if (cmp == 0) {
+            final int targetUptoMid = targetUpto;
+
+            // Second compare the rest of the term, but
+            // don't save arc/output/frame; we only do this
+            // to find out if the target term is before,
+            // equal or after the current term
+            final int targetLimit2 = Math.min(target.length, term.length);
+            while (targetUpto < targetLimit2) {
+              cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
+              // if (DEBUG) {
+              //   System.out.println("    cycle2 targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")");
+              // }
+              if (cmp != 0) {
+                break;
+              }
+              targetUpto++;
+            }
+
+            if (cmp == 0) {
+              cmp = term.length - target.length;
+            }
+            targetUpto = targetUptoMid;
+          }
+
+          if (cmp < 0) {
+            // Common case: target term is after current
+            // term, ie, app is seeking multiple terms
+            // in sorted order
+            // if (DEBUG) {
+            //   System.out.println("  target is after current (shares prefixLen=" + targetUpto + "); frame.ord=" + lastFrame.ord);
+            // }
+            currentFrame = lastFrame;
+
+          } else if (cmp > 0) {
+            // Uncommon case: target term
+            // is before current term; this means we can
+            // keep the currentFrame but we must rewind it
+            // (so we scan from the start)
+            targetBeforeCurrentLength = 0;
+            // if (DEBUG) {
+            //   System.out.println("  target is before current (shares prefixLen=" + targetUpto + "); rewind frame ord=" + lastFrame.ord);
+            // }
+            currentFrame = lastFrame;
+            currentFrame.rewind();
+          } else {
+            // Target is exactly the same as current term
+            assert term.length == target.length;
+            if (termExists) {
+              // if (DEBUG) {
+              //   System.out.println("  target is same as current; return true");
+              // }
+              return true;
+            } else {
+              // if (DEBUG) {
+              //   System.out.println("  target is same as current but term doesn't exist");
+              // }
+            }
+            //validIndexPrefix = currentFrame.depth;
+            //term.length = target.length;
+            //return termExists;
+          }
+
+        } else {
+
+          targetBeforeCurrentLength = -1;
+          arc = index.getFirstArc(arcs[0]);
+
+          // Empty string prefix must have an output (block) in the index!
+          assert arc.isFinal();
+          assert arc.output != null;
+
+          // if (DEBUG) {
+          //   System.out.println("    no seek state; push root frame");
+          // }
+
+          output = arc.output;
+
+          currentFrame = staticFrame;
+
+          //term.length = 0;
+          targetUpto = 0;
+          currentFrame = pushFrame(arc, fstOutputs.add(output, arc.nextFinalOutput), 0);
+        }
+
+        // if (DEBUG) {
+        //   System.out.println("  start index loop targetUpto=" + targetUpto + " output=" + output + " currentFrame.ord=" + currentFrame.ord + " targetBeforeCurrentLength=" + targetBeforeCurrentLength);
+        // }
+
+        while (targetUpto < target.length) {
+
+          final int targetLabel = target.bytes[target.offset + targetUpto] & 0xFF;
+
+          final FST.Arc<BytesRef> nextArc = index.findTargetArc(targetLabel, arc, getArc(1+targetUpto));
+
+          if (nextArc == null) {
+
+            // Index is exhausted
+            // if (DEBUG) {
+            //   System.out.println("    index: index exhausted label=" + ((char) targetLabel) + " " + toHex(targetLabel));
+            // }
+            
+            validIndexPrefix = currentFrame.prefix;
+            //validIndexPrefix = targetUpto;
+
+            currentFrame.scanToFloorFrame(target);
+
+            if (!currentFrame.hasTerms) {
+              termExists = false;
+              term.bytes[targetUpto] = (byte) targetLabel;
+              term.length = 1+targetUpto;
+              // if (DEBUG) {
+              //   System.out.println("  FAST NOT_FOUND term=" + brToString(term));
+              // }
+              return false;
+            }
+
+            currentFrame.loadBlock();
+
+            final SeekStatus result = currentFrame.scanToTerm(target, true);            
+            if (result == SeekStatus.FOUND) {
+              // if (DEBUG) {
+              //   System.out.println("  return FOUND term=" + term.utf8ToString() + " " + term);
+              // }
+              return true;
+            } else {
+              // if (DEBUG) {
+              //   System.out.println("  got " + result + "; return NOT_FOUND term=" + brToString(term));
+              // }
+              return false;
+            }
+          } else {
+            // Follow this arc
+            arc = nextArc;
+            term.bytes[targetUpto] = (byte) targetLabel;
+            // Aggregate output as we go:
+            assert arc.output != null;
+            if (arc.output != NO_OUTPUT) {
+              output = fstOutputs.add(output, arc.output);
+            }
+
+            // if (DEBUG) {
+            //   System.out.println("    index: follow label=" + toHex(target.bytes[target.offset + targetUpto]&0xff) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextFinalOutput);
+            // }
+            targetUpto++;
+
+            if (arc.isFinal()) {
+              //if (DEBUG) System.out.println("    arc is final!");
+              currentFrame = pushFrame(arc, fstOutputs.add(output, arc.nextFinalOutput), targetUpto);
+              //if (DEBUG) System.out.println("    curFrame.ord=" + currentFrame.ord + " hasTerms=" + currentFrame.hasTerms);
+            }
+          }
+        }
+
+        //validIndexPrefix = targetUpto;
+        validIndexPrefix = currentFrame.prefix;
+
+        currentFrame.scanToFloorFrame(target);
+
+        // Target term is entirely contained in the index:
+        if (!currentFrame.hasTerms) {
+          termExists = false;
+          term.length = targetUpto;
+          // if (DEBUG) {
+          //   System.out.println("  FAST NOT_FOUND term=" + brToString(term));
+          // }
+          return false;
+        }
+
+        currentFrame.loadBlock();
+
+        final SeekStatus result = currentFrame.scanToTerm(target, true);            
+        if (result == SeekStatus.FOUND) {
+          // if (DEBUG) {
+          //   System.out.println("  return FOUND term=" + term.utf8ToString() + " " + term);
+          // }
+          return true;
+        } else {
+          // if (DEBUG) {
+          //   System.out.println("  got result " + result + "; return NOT_FOUND term=" + term.utf8ToString());
+          // }
+
+          return false;
+        }
+      }
+
+      @Override
+      public SeekStatus seekCeil(final BytesRef target, final boolean useCache) throws IOException {
+        if (index == null) {
+          throw new IllegalStateException("terms index was not loaded");
+        }
+   
+        if (term.bytes.length <= target.length) {
+          term.bytes = ArrayUtil.grow(term.bytes, 1+target.length);
+        }
+
+        assert clearEOF();
+
+        //if (DEBUG) {
+        //System.out.println("\nBTTR.seekCeil seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + brToString(term) + " (exists?=" + termExists + ") validIndexPrefix=  " + validIndexPrefix);
+        //printSeekState();
+        //}
+
+        FST.Arc<BytesRef> arc;
+        int targetUpto;
+        BytesRef output;
+
+        targetBeforeCurrentLength = currentFrame.ord;
+
+        if (currentFrame != staticFrame) {
+
+          // We are already seek'd; find the common
+          // prefix of new seek term vs current term and
+          // re-use the corresponding seek state.  For
+          // example, if app first seeks to foobar, then
+          // seeks to foobaz, we can re-use the seek state
+          // for the first 5 bytes.
+
+          //if (DEBUG) {
+          //System.out.println("  re-use current seek state validIndexPrefix=" + validIndexPrefix);
+          //}
+
+          arc = arcs[0];
+          assert arc.isFinal();
+          output = arc.output;
+          targetUpto = 0;
+          
+          Frame lastFrame = stack[0];
+          assert validIndexPrefix <= term.length;
+
+          final int targetLimit = Math.min(target.length, validIndexPrefix);
+
+          int cmp = 0;
+
+          // TOOD: we should write our vLong backwards (MSB
+          // first) to get better sharing from the FST
+
+          // First compare up to valid seek frames:
+          while (targetUpto < targetLimit) {
+            cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
+            //if (DEBUG) {
+            //System.out.println("    cycle targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")"   + " arc.output=" + arc.output + " output=" + output);
+            //}
+            if (cmp != 0) {
+              break;
+            }
+            arc = arcs[1+targetUpto];
+            assert arc.label == (target.bytes[target.offset + targetUpto] & 0xFF): "arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF);
+            // TOOD: we could save the outputs in local
+            // byte[][] instead of making new objs ever
+            // seek; but, often the FST doesn't have any
+            // shared bytes (but this could change if we
+            // reverse vLong byte order)
+            if (arc.output != NO_OUTPUT) {
+              output = fstOutputs.add(output, arc.output);
+            }
+            if (arc.isFinal()) {
+              lastFrame = stack[1+lastFrame.ord];
+            }
+            targetUpto++;
+          }
+
+
+          if (cmp == 0) {
+            final int targetUptoMid = targetUpto;
+            // Second compare the rest of the term, but
+            // don't save arc/output/frame:
+            final int targetLimit2 = Math.min(target.length, term.length);
+            while (targetUpto < targetLimit2) {
+              cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
+              //if (DEBUG) {
+              //System.out.println("    cycle2 targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")");
+              //}
+              if (cmp != 0) {
+                break;
+              }
+              targetUpto++;
+            }
+
+            if (cmp == 0) {
+              cmp = term.length - target.length;
+            }
+            targetUpto = targetUptoMid;
+          }
+
+          if (cmp < 0) {
+            // Common case: target term is after current
+            // term, ie, app is seeking multiple terms
+            // in sorted order
+            //if (DEBUG) {
+            //System.out.println("  target is after current (shares prefixLen=" + targetUpto + "); clear frame.scanned ord=" + lastFrame.ord);
+            //}
+            currentFrame = lastFrame;
+
+          } else if (cmp > 0) {
+            // Uncommon case: target term
+            // is before current term; this means we can
+            // keep the currentFrame but we must rewind it
+            // (so we scan from the start)
+            targetBeforeCurrentLength = 0;
+            //if (DEBUG) {
+            //System.out.println("  target is before current (shares prefixLen=" + targetUpto + "); rewind frame ord=" + lastFrame.ord);
+            //}
+            currentFrame = lastFrame;
+            currentFrame.rewind();
+          } else {
+            // Target is exactly the same as current term
+            assert term.length == target.length;
+            if (termExists) {
+              //if (DEBUG) {
+              //System.out.println("  target is same as current; return FOUND");
+              //}
+              return SeekStatus.FOUND;
+            } else {
+              //if (DEBUG) {
+              //System.out.println("  target is same as current but term doesn't exist");
+              //}
+            }
+          }
+
+        } else {
+
+          targetBeforeCurrentLength = -1;
+          arc = index.getFirstArc(arcs[0]);
+
+          // Empty string prefix must have an output (block) in the index!
+          assert arc.isFinal();
+          assert arc.output != null;
+
+          //if (DEBUG) {
+          //System.out.println("    no seek state; push root frame");
+          //}
+
+          output = arc.output;
+
+          currentFrame = staticFrame;
+
+          //term.length = 0;
+          targetUpto = 0;
+          currentFrame = pushFrame(arc, fstOutputs.add(output, arc.nextFinalOutput), 0);
+        }
+
+        //if (DEBUG) {
+        //System.out.println("  start index loop targetUpto=" + targetUpto + " output=" + output + " currentFrame.ord+1=" + currentFrame.ord + " targetBeforeCurrentLength=" + targetBeforeCurrentLength);
+        //}
+
+        while (targetUpto < target.length) {
+
+          final int targetLabel = target.bytes[target.offset + targetUpto] & 0xFF;
+
+          final FST.Arc<BytesRef> nextArc = index.findTargetArc(targetLabel, arc, getArc(1+targetUpto));
+
+          if (nextArc == null) {
+
+            // Index is exhausted
+            // if (DEBUG) {
+            //   System.out.println("    index: index exhausted label=" + ((char) targetLabel) + " " + toHex(targetLabel));
+            // }
+            
+            validIndexPrefix = currentFrame.prefix;
+            //validIndexPrefix = targetUpto;
+
+            currentFrame.scanToFloorFrame(target);
+
+            currentFrame.loadBlock();
+
+            final SeekStatus result = currentFrame.scanToTerm(target, false);
+            if (result == SeekStatus.END) {
+              term.copyBytes(target);
+              termExists = false;
+
+              if (next() != null) {
+                //if (DEBUG) {
+                //System.out.println("  return NOT_FOUND term=" + brToString(term) + " " + term);
+                //}
+                return SeekStatus.NOT_FOUND;
+              } else {
+                //if (DEBUG) {
+                //System.out.println("  return END");
+                //}
+                return SeekStatus.END;
+              }
+            } else {
+              //if (DEBUG) {
+              //System.out.println("  return " + result + " term=" + brToString(term) + " " + term);
+              //}
+              return result;
+            }
+          } else {
+            // Follow this arc
+            term.bytes[targetUpto] = (byte) targetLabel;
+            arc = nextArc;
+            // Aggregate output as we go:
+            assert arc.output != null;
+            if (arc.output != NO_OUTPUT) {
+              output = fstOutputs.add(output, arc.output);
+            }
+
+            //if (DEBUG) {
+            //System.out.println("    index: follow label=" + toHex(target.bytes[target.offset + targetUpto]&0xff) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextFinalOutput);
+            //}
+            targetUpto++;
+
+            if (arc.isFinal()) {
+              //if (DEBUG) System.out.println("    arc is final!");
+              currentFrame = pushFrame(arc, fstOutputs.add(output, arc.nextFinalOutput), targetUpto);
+              //if (DEBUG) System.out.println("    curFrame.ord=" + currentFrame.ord + " hasTerms=" + currentFrame.hasTerms);
+            }
+          }
+        }
+
+        //validIndexPrefix = targetUpto;
+        validIndexPrefix = currentFrame.prefix;
+
+        currentFrame.scanToFloorFrame(target);
+
+        currentFrame.loadBlock();
+
+        final SeekStatus result = currentFrame.scanToTerm(target, false);
+
+        if (result == SeekStatus.END) {
+          term.copyBytes(target);
+          termExists = false;
+          if (next() != null) {
+            //if (DEBUG) {
+            //System.out.println("  return NOT_FOUND term=" + term.utf8ToString() + " " + term);
+            //}
+            return SeekStatus.NOT_FOUND;
+          } else {
+            //if (DEBUG) {
+            //System.out.println("  return END");
+            //}
+            return SeekStatus.END;
+          }
+        } else {
+          return result;
+        }
+      }
+
+      private void printSeekState() throws IOException {
+        if (currentFrame == staticFrame) {
+          System.out.println("  no prior seek");
+        } else {
+          System.out.println("  prior seek state:");
+          int ord = 0;
+          boolean isSeekFrame = true;
+          while(true) {
+            Frame f = getFrame(ord);
+            assert f != null;
+            final BytesRef prefix = new BytesRef(term.bytes, 0, f.prefix);
+            if (f.nextEnt == -1) {
+              System.out.println("    frame " + (isSeekFrame ? "(seek)" : "(next)") + " ord=" + ord + " fp=" + f.fp + (f.isFloor ? (" (fpOrig=" + f.fpOrig + ")") : "") + " prefixLen=" + f.prefix + " prefix=" + prefix + (f.nextEnt == -1 ? "" : (" (of " + f.entCount + ")")) + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " code=" + ((f.fp<<BlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) + (f.hasTerms ? BlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) + (f.isFloor ? BlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0)) + " isLastInFloor=" + f.isLastInFloor + " mdUpto=" + f.metaDataUpto + " tbOrd=" + f.getTermBlockOrd());
+            } else {
+              System.out.println("    frame " + (isSeekFrame ? "(seek, loaded)" : "(next, loaded)") + " ord=" + ord + " fp=" + f.fp + (f.isFloor ? (" (fpOrig=" + f.fpOrig + ")") : "") + " prefixLen=" + f.prefix + " prefix=" + prefix + " nextEnt=" + f.nextEnt + (f.nextEnt == -1 ? "" : (" (of " + f.entCount + ")")) + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " code=" + ((f.fp<<BlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) + (f.hasTerms ? BlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) + (f.isFloor ? BlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0)) + " lastSubFP=" + f.lastSubFP + " isLastInFloor=" + f.isLastInFloor + " mdUpto=" + f.metaDataUpto + " tbOrd=" + f.getTermBlockOrd());
+            }
+            if (index != null) {
+              assert !isSeekFrame || f.arc != null: "isSeekFrame=" + isSeekFrame + " f.arc=" + f.arc;
+              if (f.prefix > 0 && isSeekFrame && f.arc.label != (term.bytes[f.prefix-1]&0xFF)) {
+                System.out.println("      broken seek state: arc.label=" + (char) f.arc.label + " vs term byte=" + (char) (term.bytes[f.prefix-1]&0xFF));
+                throw new RuntimeException("seek state is broken");
+              }
+              BytesRef output = Util.get(index, prefix);
+              if (output == null) {
+                System.out.println("      broken seek state: prefix is not final in index");
+                throw new RuntimeException("seek state is broken");
+              } else if (isSeekFrame && !f.isFloor) {
+                final ByteArrayDataInput reader = new ByteArrayDataInput(output.bytes, output.offset, output.length);
+                final long codeOrig = reader.readVLong();
+                final long code = (f.fp << BlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) | (f.hasTerms ? BlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) | (f.isFloor ? BlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0);
+                if (codeOrig != code) {
+                  System.out.println("      broken seek state: output code=" + codeOrig + " doesn't match frame code=" + code);
+                  throw new RuntimeException("seek state is broken");
+                }
+              }
+            }
+            if (f == currentFrame) {
+              break;
+            }
+            if (f.prefix == validIndexPrefix) {
+              isSeekFrame = false;
+            }
+            ord++;
+          }
+        }
+      }
+
+      /* Decodes only the term bytes of the next term.  If caller then asks for
+         metadata, ie docFreq, totalTermFreq or pulls a D/&PEnum, we then (lazily)
+         decode all metadata up to the current term. */
+      @Override
+      public BytesRef next() throws IOException {
+
+        if (in == null) {
+          // Fresh TermsEnum; seek to first term:
+          final FST.Arc<BytesRef> arc;
+          if (index != null) {
+            arc = index.getFirstArc(arcs[0]);
+            // Empty string prefix must have an output in the index!
+            assert arc.isFinal();
+          } else {
+            arc = null;
+          }
+          currentFrame = pushFrame(arc, rootCode, 0);
+          currentFrame.loadBlock();
+        }
+
+        targetBeforeCurrentLength = currentFrame.ord;
+
+        assert !eof;
+        //if (DEBUG) {
+        //System.out.println("\nBTTR.next seg=" + segment + " term=" + brToString(term) + " termExists?=" + termExists + " field=" + fieldInfo.name + " termBlockOrd=" + currentFrame.state.termBlockOrd + " validIndexPrefix=" + validIndexPrefix);
+        //printSeekState();
+        //}
+
+        if (currentFrame == staticFrame) {
+          // If seek was previously called and the term was
+          // cached, or seek(TermState) was called, usually
+          // caller is just going to pull a D/&PEnum or get
+          // docFreq, etc.  But, if they then call next(),
+          // this method catches up all internal state so next()
+          // works properly:
+          //if (DEBUG) System.out.println("  re-seek to pending term=" + term.utf8ToString() + " " + term);
+          final boolean result = seekExact(term, false);
+          assert result;
+        }
+
+        // Pop finished blocks
+        while (currentFrame.nextEnt == currentFrame.entCount) {
+          if (!currentFrame.isLastInFloor) {
+            currentFrame.loadNextFloorBlock();
+          } else {
+            //if (DEBUG) System.out.println("  pop frame");
+            if (currentFrame.ord == 0) {
+              //if (DEBUG) System.out.println("  return null");
+              assert setEOF();
+              term.length = 0;
+              validIndexPrefix = 0;
+              currentFrame.rewind();
+              termExists = false;
+              return null;
+            }
+            final long lastFP = currentFrame.fpOrig;
+            currentFrame = stack[currentFrame.ord-1];
+
+            if (currentFrame.nextEnt == -1 || currentFrame.lastSubFP != lastFP) {
+              // We popped into a frame that's not loaded
+              // yet or not scan'd to the right entry
+              currentFrame.scanToFloorFrame(term);
+              currentFrame.loadBlock();
+              currentFrame.scanToSubBlock(lastFP);
+            }
+
+            // Note that the seek state (last seek) has been
+            // invalidated beyond this depth
+            validIndexPrefix = Math.min(validIndexPrefix, currentFrame.prefix);
+            //if (DEBUG) {
+            //System.out.println("  reset validIndexPrefix=" + validIndexPrefix);
+            //}
+          }
+        }
+
+        while(true) {
+          if (currentFrame.next()) {
+            // Push to new block:
+            //if (DEBUG) System.out.println("  push frame");
+            currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length);
+            // This is a "next" frame -- even if it's
+            // floor'd we must pretend it isn't so we don't
+            // try to scan to the right floor frame:
+            currentFrame.isFloor = false;
+            //currentFrame.hasTerms = true;
+            currentFrame.loadBlock();
+          } else {
+            //if (DEBUG) System.out.println("  return term=" + term.utf8ToString() + " " + term + " currentFrame.ord=" + currentFrame.ord);
+            return term;
+          }
+        }
+      }
+
+      @Override
+      public BytesRef term() {
+        assert !eof;
+        return term;
+      }
+
+      @Override
+      public int docFreq() throws IOException {
+        assert !eof;
+        //if (DEBUG) System.out.println("BTR.docFreq");
+        currentFrame.decodeMetaData();
+        //if (DEBUG) System.out.println("  return " + currentFrame.state.docFreq);
+        return currentFrame.state.docFreq;
+      }
+
+      @Override
+      public long totalTermFreq() throws IOException {
+        assert !eof;
+        currentFrame.decodeMetaData();
+        return currentFrame.state.totalTermFreq;
+      }
+
+      @Override
+      public DocsEnum docs(Bits skipDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
+        assert !eof;
+        //if (DEBUG) {
+        //System.out.println("BTTR.docs seg=" + segment);
+        //}
+        currentFrame.decodeMetaData();
+        //if (DEBUG) {
+        //System.out.println("  state=" + currentFrame.state);
+        //}
+        return postingsReader.docs(fieldInfo, currentFrame.state, skipDocs, reuse, needsFreqs);
+      }
+
+      @Override
+      public DocsAndPositionsEnum docsAndPositions(Bits skipDocs, DocsAndPositionsEnum reuse) throws IOException {
+        assert !eof;
+        //System.out.println("BTR.d&p this=" + this);
+        if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+          return null;
+        } else {
+          currentFrame.decodeMetaData();
+          DocsAndPositionsEnum dpe = postingsReader.docsAndPositions(fieldInfo, currentFrame.state, skipDocs, reuse);
+          //System.out.println("  return d&pe=" + dpe);
+          return dpe;
+        }
+      }
+
+      @Override
+      public void seekExact(BytesRef target, TermState otherState) throws IOException {
+        // if (DEBUG) {
+        //   System.out.println("BTTR.seekExact termState seg=" + segment + " target=" + target.utf8ToString() + " " + target + " state=" + otherState);
+        // }
+        assert clearEOF();
+        if (target.compareTo(term) != 0 || !termExists) {
+          assert otherState != null && otherState instanceof BlockTermState;
+          currentFrame = staticFrame;
+          currentFrame.state.copyFrom(otherState);
+          term.copyBytes(target);
+          currentFrame.metaDataUpto = currentFrame.getTermBlockOrd();
+          assert currentFrame.metaDataUpto > 0;
+          validIndexPrefix = 0;
+        } else {
+          // if (DEBUG) {
+          //   System.out.println("  skip seek: already on target state=" + currentFrame.state);
+          // }
+        }
+      }
+      
+      @Override
+      public TermState termState() throws IOException {
+        assert !eof;
+        currentFrame.decodeMetaData();
+        TermState ts = (TermState) currentFrame.state.clone();
+        //if (DEBUG) System.out.println("BTTR.termState seg=" + segment + " state=" + ts);
+        return ts;
+      }
+
+      @Override
+      public void seekExact(long ord) throws IOException {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public long ord() {
+        throw new UnsupportedOperationException();
+      }
+
+      // Not static -- references term, postingsReader,
+      // fieldInfo, in
+      private final class Frame {
+        // Our index in stack[]:
+        final int ord;
+
+        boolean hasTerms;
+        boolean hasTermsOrig;
+        boolean isFloor;
+
+        FST.Arc<BytesRef> arc;
+
+        // File pointer where this block was loaded from
+        long fp;
+        long fpOrig;
+        long fpEnd;
+
+        byte[] suffixBytes = new byte[128];
+        final ByteArrayDataInput suffixesReader = new ByteArrayDataInput();
+
+        byte[] statBytes = new byte[64];
+        final ByteArrayDataInput statsReader = new ByteArrayDataInput();
+
+        byte[] floorData = new byte[32];
+        final ByteArrayDataInput floorDataReader = new ByteArrayDataInput();
+
+        // Length of prefix shared by all terms in this block
+        int prefix;
+
+        // Number of entries (term or sub-block) in this block
+        int entCount;
+
+        // Which term we will next read, or -1 if the block
+        // isn't loaded yet
+        int nextEnt;
+
+        // True if this block is either not a floor block,
+        // or, it's the last sub-block of a floor block
+        boolean isLastInFloor;
+
+        // True if all entries are terms
+        boolean isLeafBlock;
+
+        long lastSubFP;
+
+        int nextFloorLabel;
+        int numFollowFloorBlocks;
+
+        // Next term to decode metaData; we decode metaData
+        // lazily so that scanning to find the matching term is
+        // fast and only if you find a match and app wants the
+        // stats or docs/positions enums, will we decode the
+        // metaData
+        int metaDataUpto;
+
+        final BlockTermState state;
+
+        public Frame(int ord) throws IOException {
+          this.ord = ord;
+          state = postingsReader.newTermState();
+          state.totalTermFreq = -1;
+        }
+
+        public void setFloorData(ByteArrayDataInput in, BytesRef source) {
+          final int numBytes = source.length - (in.getPosition() - source.offset);
+          if (numBytes > floorData.length) {
+            floorData = new byte[ArrayUtil.oversize(numBytes, 1)];
+          }
+          System.arraycopy(source.bytes, source.offset+in.getPosition(), floorData, 0, numBytes);
+          floorDataReader.reset(floorData, 0, numBytes);
+          numFollowFloorBlocks = floorDataReader.readVInt();
+          nextFloorLabel = floorDataReader.readByte() & 0xff;
+          //if (DEBUG) {
+          //System.out.println("    setFloorData fpOrig=" + fpOrig + " bytes=" + new BytesRef(source.bytes, source.offset + in.getPosition(), numBytes) + " numFollowFloorBlocks=" + numFollowFloorBlocks + " nextFloorLabel=" + toHex(nextFloorLabel));
+          //}
+        }
+
+        public int getTermBlockOrd() {
+          return isLeafBlock ? nextEnt : state.termBlockOrd;
+        }
+
+        void loadNextFloorBlock() throws IOException {
+          //if (DEBUG) {
+          //System.out.println("    loadNextFloorBlock fp=" + fp + " fpEnd=" + fpEnd);
+          //}
+          assert arc == null || isFloor: "arc=" + arc + " isFloor=" + isFloor;
+          fp = fpEnd;
+          nextEnt = -1;
+          loadBlock();
+        }
+
+        /* Does initial decode of next block of terms; this
+           doesn't actually decode the docFreq, totalTermFreq,
+           postings details (frq/prx offset, etc.) metadata;
+           it just loads them as byte[] blobs which are then      
+           decoded on-demand if the metadata is ever requested
+           for any term in this block.  This enables terms-only
+           intensive consumes (eg certain MTQs, respelling) to
+           not pay the price of decoding metadata they won't
+           use. */
+        void loadBlock() throws IOException {
+
+          // Clone the IndexInput lazily, so that consumers
+          // that just pull a TermsEnum to
+          // seekExact(TermState) don't pay this cost:
+          initIndexInput();
+
+          if (nextEnt != -1) {
+            // Already loaded
+            return;
+          }
+          //System.out.println("blc=" + blockLoadCount);
+
+          in.seek(fp);
+          int code = in.readVInt();
+          entCount = code >>> 1;
+          assert entCount > 0;
+          isLastInFloor = (code & 1) != 0;
+          assert arc == null || (isLastInFloor || isFloor);
+
+          // TODO: if suffixes were stored in random-access
+          // array structure, then we could do binary search
+          // instead of linear scan to find target term; eg
+          // we could have simple array of offsets
+
+          // term suffixes:
+          code = in.readVInt();
+          isLeafBlock = (code & 1) != 0;
+          int numBytes = code >>> 1;
+          if (suffixBytes.length < numBytes) {
+            suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
+          }
+          in.readBytes(suffixBytes, 0, numBytes);
+          suffixesReader.reset(suffixBytes, 0, numBytes);
+
+          /*if (DEBUG) {
+            if (arc == null) {
+              System.out.println("    loadBlock (next) fp=" + fp + " entCount=" + entCount + " prefixLen=" + prefix + " isLastInFloor=" + isLastInFloor + " leaf?=" + isLeafBlock);
+            } else {
+              System.out.println("    loadBlock (seek) fp=" + fp + " entCount=" + entCount + " prefixLen=" + prefix + " hasTerms?=" + hasTerms + " isFloor?=" + isFloor + " isLastInFloor=" + isLastInFloor + " leaf?=" + isLeafBlock);
+            }
+            }*/
+
+          // stats
+          numBytes = in.readVInt();
+          if (statBytes.length < numBytes) {
+            statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
+          }
+          in.readBytes(statBytes, 0, numBytes);
+          statsReader.reset(statBytes, 0, numBytes);
+          metaDataUpto = 0;
+
+          state.termBlockOrd = 0;
+          nextEnt = 0;
+          lastSubFP = -1;
+
+          // TODO: we could skip this if !hasTerms; but
+          // that's rare so won't help much
+          postingsReader.readTermsBlock(in, fieldInfo, state);
+
+          // Sub-blocks of a single floor block are always
+          // written one after another -- tail recurse:
+          fpEnd = in.getFilePointer();
+          // if (DEBUG) {
+          //   System.out.println("      fpEnd=" + fpEnd);
+          // }
+        }
+
+        void rewind() throws IOException {
+
+          // Force reload:
+          fp = fpOrig;
+          nextEnt = -1;
+          hasTerms = hasTermsOrig;
+          if (isFloor) {
+            floorDataReader.rewind();
+            numFollowFloorBlocks = floorDataReader.readVInt();
+            nextFloorLabel = floorDataReader.readByte() & 0xff;
+          }
+
+          /*
+          //System.out.println("rewind");
+          // Keeps the block loaded, but rewinds its state:
+          if (nextEnt > 0 || fp != fpOrig) {
+            if (DEBUG) {
+              System.out.println("      rewind frame ord=" + ord + " fpOrig=" + fpOrig + " fp=" + fp + " hasTerms?=" + hasTerms + " isFloor?=" + isFloor + " nextEnt=" + nextEnt + " prefixLen=" + prefix);
+            }
+            if (fp != fpOrig) {
+              fp = fpOrig;
+              nextEnt = -1;
+            } else {
+              nextEnt = 0;
+            }
+            hasTerms = hasTermsOrig;
+            if (isFloor) {
+              floorDataReader.rewind();
+              numFollowFloorBlocks = floorDataReader.readVInt();
+              nextFloorLabel = floorDataReader.readByte() & 0xff;
+            }
+            assert suffixBytes != null;
+            suffixesReader.rewind();
+            assert statBytes != null;
+            statsReader.rewind();
+            metaDataUpto = 0;
+            state.termBlockOrd = 0;
+            // TODO: skip this if !hasTerms?  Then postings
+            // impl wouldn't have to write useless 0 byte
+            postingsReader.resetTermsBlock(fieldInfo, state);
+            lastSubFP = -1;
+          } else if (DEBUG) {
+            System.out.println("      skip rewind fp=" + fp + " fpOrig=" + fpOrig + " nextEnt=" + nextEnt + " ord=" + ord);
+          }
+          */
+        }
+
+        public boolean next() {
+          return isLeafBlock ? nextLeaf() : nextNonLeaf();
+        }
+
+        // Decodes next entry; returns true if it's a sub-block
+        public boolean nextLeaf() {
+          //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
+          assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
+          nextEnt++;
+          suffix = suffixesReader.readVInt();
+          startBytePos = suffixesReader.getPosition();
+          term.length = prefix + suffix;
+          if (term.bytes.length < term.length) {
+            term.grow(term.length);
+          }
+          suffixesReader.readBytes(term.bytes, prefix, suffix);
+          // A normal term
+          termExists = true;
+          return false;
+        }
+
+        public boolean nextNonLeaf() {
+          //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
+          assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
+          nextEnt++;
+          final int code = suffixesReader.readVInt();
+          suffix = code >>> 1;
+          startBytePos = suffixesReader.getPosition();
+          term.length = prefix + suffix;
+          if (term.bytes.length < term.length) {
+            term.grow(term.length);
+          }
+          suffixesReader.readBytes(term.bytes, prefix, suffix);
+          if ((code & 1) == 0) {
+            // A normal term
+            termExists = true;
+            state.termBlockOrd++;
+            return false;
+          } else {
+            // A sub-block; make sub-FP absolute:
+            termExists = false;
+            lastSubFP = fp - suffixesReader.readVLong();
+            //if (DEBUG) {
+            //System.out.println("    lastSubFP=" + lastSubFP);
+            //}
+            return true;
+          }
+        }
+        
+        // TODO: make this array'd so we can do bin search?
+        // likely not worth it?  need to measure how many
+        // floor blocks we "typically" get
+        public void scanToFloorFrame(BytesRef target) {
+
+          if (!isFloor || target.length <= prefix) {
+            // if (DEBUG) {
+            //   System.out.println("    scanToFloorFrame skip: isFloor=" + isFloor + " target.length=" + target.length + " vs prefix=" + prefix);
+            // }
+            return;
+          }
+
+          final int targetLabel = target.bytes[target.offset + prefix] & 0xFF;
+
+          // if (DEBUG) {
+          //   System.out.println("    scanToFloorFrame fpOrig=" + fpOrig + " targetLabel=" + toHex(targetLabel) + " vs nextFloorLabel=" + toHex(nextFloorLabel) + " numFollowFloorBlocks=" + numFollowFloorBlocks);
+          // }
+
+          if (targetLabel < nextFloorLabel) {
+            // if (DEBUG) {
+            //   System.out.println("      already on correct block");
+            // }
+            return;
+          }
+
+          assert numFollowFloorBlocks != 0;
+
+          long newFP = fpOrig;
+          while (true) {
+            final long code = floorDataReader.readVLong();
+            newFP = fpOrig + (code >>> 1);
+            hasTerms = (code & 1) != 0;
+            // if (DEBUG) {
+            //   System.out.println("      label=" + toHex(nextFloorLabel) + " fp=" + newFP + " hasTerms?=" + hasTerms + " numFollowFloor=" + numFollowFloorBlocks);
+            // }
+            
+            isLastInFloor = numFollowFloorBlocks == 1;
+            numFollowFloorBlocks--;
+
+            if (isLastInFloor) {
+              nextFloorLabel = 256;
+              // if (DEBUG) {
+              //   System.out.println("        stop!  last block nextFloorLabel=" + toHex(nextFloorLabel));
+              // }
+              break;
+            } else {
+              nextFloorLabel = floorDataReader.readByte() & 0xff;
+              if (targetLabel < nextFloorLabel) {
+                // if (DEBUG) {
+                //   System.out.println("        stop!  nextFloorLabel=" + toHex(nextFloorLabel));
+                // }
+                break;
+              }
+            }
+          }
+
+          if (newFP != fp) {
+            // Force re-load of the block:
+            // if (DEBUG) {
+            //   System.out.println("      force switch to fp=" + newFP + " oldFP=" + fp);
+            // }
+            nextEnt = -1;
+            fp = newFP;
+          } else {
+            // if (DEBUG) {
+            //   System.out.println("      stay on same fp=" + newFP);
+            // }
+          }
+        }
+    
+        public void decodeMetaData() throws IOException {
+
+          //if (DEBUG) System.out.println("\nBTTR.decodeMetadata seg=" + segment + " mdUpto=" + metaDataUpto + " vs termBlockOrd=" + state.termBlockOrd);
+
+          // lazily catch up on metadata decode:
+          final int limit = getTermBlockOrd();
+          assert limit > 0;
+
+          // We must set/incr state.termCount because
+          // postings impl can look at this
+          state.termBlockOrd = metaDataUpto;
+      
+          // TODO: better API would be "jump straight to term=N"???
+          while (metaDataUpto < limit) {
+
+            // TODO: we could make "tiers" of metadata, ie,
+            // decode docFreq/totalTF but don't decode postings
+            // metadata; this way caller could get
+            // docFreq/totalTF w/o paying decode cost for
+            // postings
+
+            // TODO: if docFreq were bulk decoded we could
+            // just skipN here:
+            state.docFreq = statsReader.readVInt();
+            //if (DEBUG) System.out.println("    dF=" + state.docFreq);
+            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+              state.totalTermFreq = state.docFreq + statsReader.readVLong();
+              //if (DEBUG) System.out.println("    totTF=" + state.totalTermFreq);
+            }
+
+            postingsReader.nextTerm(fieldInfo, state);
+            metaDataUpto++;
+            state.termBlockOrd++;
+          }
+        }
+
+        // Used only by assert
+        private boolean prefixMatches(BytesRef target) {
+          for(int bytePos=0;bytePos<prefix;bytePos++) {
+            if (target.bytes[target.offset + bytePos] != term.bytes[bytePos]) {
+              return false;
+            }
+          }
+
+          return true;
+        }
+
+        // Scans to sub-block that has this target fp; only
+        // called by next(); NOTE: does not set
+        // startBytePos/suffix as a side effect
+        public void scanToSubBlock(long subFP) {
+          assert !isLeafBlock;
+          //if (DEBUG) System.out.println("  scanToSubBlock fp=" + fp + " subFP=" + subFP + " entCount=" + entCount + " lastSubFP=" + lastSubFP);
+          //assert nextEnt == 0;
+          if (lastSubFP == subFP) {
+            //if (DEBUG) System.out.println("    already positioned");
+            return;
+          }
+          assert subFP < fp : "fp=" + fp + " subFP=" + subFP;
+          final long targetSubCode = fp - subFP;
+          //if (DEBUG) System.out.println("    targetSubCode=" + targetSubCode);
+          while(true) {
+            assert nextEnt < entCount;
+            nextEnt++;
+            final int code = suffixesReader.readVInt();
+            suffixesReader.skipBytes(isLeafBlock ? code : code >>> 1);
+            //if (DEBUG) System.out.println("    " + nextEnt + " (of " + entCount + ") ent isSubBlock=" + ((code&1)==1));
+            if ((code & 1) != 0) {
+              final long subCode = suffixesReader.readVLong();
+              //if (DEBUG) System.out.println("      subCode=" + subCode);
+              if (targetSubCode == subCode) {
+                //if (DEBUG) System.out.println("        match!");
+                lastSubFP = subFP;
+                return;
+              }
+            } else {
+              state.termBlockOrd++;
+            }
+          }
+        }
+
+        // NOTE: sets startBytePos/suffix as a side effect
+        public SeekStatus scanToTerm(BytesRef target, boolean exactOnly) throws IOException {
+          return isLeafBlock ? scanToTermLeaf(target, exactOnly) : scanToTermNonLeaf(target, exactOnly);
+        }
+
+        private int startBytePos;
+        private int suffix;
+        private long subCode;
+
+        // Target's prefix matches this block's prefix; we
+        // scan the entries check if the suffix matches.
+        public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {
+
+          // if (DEBUG) System.out.println("    scanToTermLeaf: block fp=" + fp + " prefix=" + prefix + " nextEnt=" + nextEnt + " (of " + entCount + ") target=" + brToString(target) + " term=" + brToString(term));
+
+          assert nextEnt != -1;
+
+          termExists = true;
+          subCode = 0;
+
+          if (nextEnt == entCount) {
+            if (exactOnly) {
+              fillTerm();
+            }
+            return SeekStatus.END;
+          }
+
+          assert prefixMatches(target);
+
+          // Loop over each entry (term or sub-block) in this block:
+          //nextTerm: while(nextEnt < entCount) {
+          nextTerm: while (true) {
+            nextEnt++;
+
+            suffix = suffixesReader.readVInt();
+
+            // if (DEBUG) {
+            //   BytesRef suffixBytesRef = new BytesRef();
+            //   suffixBytesRef.bytes = suffixBytes;
+            //   suffixBytesRef.offset = suffixesReader.getPosition();
+            //   suffixBytesRef.length = suffix;
+            //   System.out.println("      cycle: term " + (nextEnt-1) + " (of " + entCount + ") suffix=" + brToString(suffixBytesRef));
+            // }
+
+            final int termLen = prefix + suffix;
+            startBytePos = suffixesReader.getPosition();
+            suffixesReader.skipBytes(suffix);
+
+            final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
+            int targetPos = target.offset + prefix;
+
+            // Loop over bytes in the suffix, comparing to
+            // the target
+            int bytePos = startBytePos;
+            while(true) {
+              final int cmp;
+              final boolean stop;
+              if (targetPos < targetLimit) {
+                cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
+                stop = false;
+              } else {
+                assert targetPos == targetLimit;
+                cmp = termLen - target.length;
+                stop = true;
+              }
+
+              if (cmp < 0) {
+                // Current entry is still before the target;
+                // keep scanning
+
+                if (nextEnt == entCount) {
+                  if (exactOnly) {
+                    fillTerm();
+                  }
+                  // We are done scanning this block
+                  break nextTerm;
+                } else {
+                  continue nextTerm;
+                }
+              } else if (cmp > 0) {
+
+                // Done!  Current entry is after target --
+                // return NOT_FOUND:
+                fillTerm();
+
+                if (!exactOnly && !termExists) {
+                  // We are on a sub-block, and caller wants
+                  // us to position to the next term after
+                  // the target, so we must recurse into the
+                  // sub-frame(s):
+                  currentFrame = pushFrame(null, currentFrame.lastSubFP, termLen);
+                  currentFrame.loadBlock();
+                  while (currentFrame.next()) {
+                    currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length);
+                    currentFrame.loadBlock();
+                  }
+                }
+                
+                //if (DEBUG) System.out.println("        not found");
+                return SeekStatus.NOT_FOUND;
+              } else if (stop) {
+                // Exact match!
+
+                // This cannot be a sub-block because we
+                // would have followed the index to this
+                // sub-block from the start:
+
+                assert termExists;
+                fillTerm();
+                //if (DEBUG) System.out.println("        found!");
+                return SeekStatus.FOUND;
+              }
+            }
+          }
+
+          // It is possible (and OK) that terms index pointed us
+          // at this block, but, we scanned the entire block and
+          // did not find the term to position to.  This happens
+          // when the target is after the last term in the block
+          // (but, before the next term in the index).  EG
+          // target could be foozzz, and terms index pointed us
+          // to the foo* block, but the last term in this block
+          // was fooz (and, eg, first term in the next block will
+          // bee fop).
+          //if (DEBUG) System.out.println("      block end");
+          if (exactOnly) {
+            fillTerm();
+          }
+
+          // TODO: not consistent that in the
+          // not-exact case we don't next() into the next
+          // frame here
+          return SeekStatus.END;
+        }
+
+        // Target's prefix matches this block's prefix; we
+        // scan the entries check if the suffix matches.
+        public SeekStatus scanToTermNonLeaf(BytesRef target, boolean exactOnly) throws IOException {
+
+          //if (DEBUG) System.out.println("    scanToTermNonLeaf: block fp=" + fp + " prefix=" + prefix + " nextEnt=" + nextEnt + " (of " + entCount + ") target=" + brToString(target) + " term=" + brToString(term));
+
+          assert nextEnt != -1;
+
+          if (nextEnt == entCount) {
+            if (exactOnly) {
+              fillTerm();
+              termExists = subCode == 0;
+            }
+            return SeekStatus.END;
+          }
+
+          assert prefixMatches(target);
+
+          // Loop over each entry (term or sub-block) in this block:
+          //nextTerm: while(nextEnt < entCount) {
+          nextTerm: while (true) {
+            nextEnt++;
+
+            final int code = suffixesReader.readVInt();
+            suffix = code >>> 1;
+            // if (DEBUG) {
+            //   BytesRef suffixBytesRef = new BytesRef();
+            //   suffixBytesRef.bytes = suffixBytes;
+            //   suffixBytesRef.offset = suffixesReader.getPosition();
+            //   suffixBytesRef.length = suffix;
+            //   System.out.println("      cycle: " + ((code&1)==1 ? "sub-block" : "term") + " " + (nextEnt-1) + " (of " + entCount + ") suffix=" + brToString(suffixBytesRef));
+            // }
+
+            termExists = (code & 1) == 0;
+            final int termLen = prefix + suffix;
+            startBytePos = suffixesReader.getPosition();
+            suffixesReader.skipBytes(suffix);
+            if (termExists) {
+              state.termBlockOrd++;
+              subCode = 0;
+            } else {
+              subCode = suffixesReader.readVLong();
+              lastSubFP = fp - subCode;
+            }
+
+            final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
+            int targetPos = target.offset + prefix;
+
+            // Loop over bytes in the suffix, comparing to
+            // the target
+            int bytePos = startBytePos;
+            while(true) {
+              final int cmp;
+              final boolean stop;
+              if (targetPos < targetLimit) {
+                cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
+                stop = false;
+              } else {
+                assert targetPos == targetLimit;
+                cmp = termLen - target.length;
+                stop = true;
+              }
+
+              if (cmp < 0) {
+                // Current entry is still before the target;
+                // keep scanning
+
+                if (nextEnt == entCount) {
+                  if (exactOnly) {
+                    fillTerm();
+                    //termExists = true;
+                  }
+                  // We are done scanning this block
+                  break nextTerm;
+                } else {
+                  continue nextTerm;
+                }
+              } else if (cmp > 0) {
+
+                // Done!  Current entry is after target --
+                // return NOT_FOUND:
+                fillTerm();
+
+                if (!exactOnly && !termExists) {
+                  // We are on a sub-block, and caller wants
+                  // us to position to the next term after
+                  // the target, so we must recurse into the
+                  // sub-frame(s):
+                  currentFrame = pushFrame(null, currentFrame.lastSubFP, termLen);
+                  currentFrame.loadBlock();
+                  while (currentFrame.next()) {
+                    currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length);
+                    currentFrame.loadBlock();
+                  }
+                }
+                
+                //if (DEBUG) System.out.println("        not found");
+                return SeekStatus.NOT_FOUND;
+              } else if (stop) {
+                // Exact match!
+
+                // This cannot be a sub-block because we
+                // would have followed the index to this
+                // sub-block from the start:
+
+                assert termExists;
+                fillTerm();
+                //if (DEBUG) System.out.println("        found!");
+                return SeekStatus.FOUND;
+              }
+            }
+          }
+
+          // It is possible (and OK) that terms index pointed us
+          // at this block, but, we scanned the entire block and
+          // did not find the term to position to.  This happens
+          // when the target is after the last term in the block
+          // (but, before the next term in the index).  EG
+          // target could be foozzz, and terms index pointed us
+          // to the foo* block, but the last term in this block
+          // was fooz (and, eg, first term in the next block will
+          // bee fop).
+          //if (DEBUG) System.out.println("      block end");
+          if (exactOnly) {
+            fillTerm();
+          }
+
+          // TODO: not consistent that in the
+          // not-exact case we don't next() into the next
+          // frame here
+          return SeekStatus.END;
+        }
+
+        private void fillTerm() {
+          final int termLength = prefix + suffix;
+          term.length = prefix + suffix;
+          if (term.bytes.length < termLength) {
+            term.grow(termLength);
+          }
+          System.arraycopy(suffixBytes, startBytePos, term.bytes, prefix, suffix);
+        }
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/BlockTreeTermsWriter.java b/lucene/src/java/org/apache/lucene/codecs/BlockTreeTermsWriter.java
new file mode 100644
index 0000000..d93ba1b
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/BlockTreeTermsWriter.java
@@ -0,0 +1,949 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.OutputStreamWriter;
+import java.io.Writer;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.ByteSequenceOutputs;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.NoOutputs;
+import org.apache.lucene.util.fst.Util;
+
+/*
+  TODO:
+  
+    - Currently there is a one-to-one mapping of indexed
+      term to term block, but we could decouple the two, ie,
+      put more terms into the index than there are blocks.
+      The index would take up more RAM but then it'd be able
+      to avoid seeking more often and could make PK/FuzzyQ
+      faster if the additional indexed terms could store
+      the offset into the terms block.
+
+    - The blocks are not written in true depth-first
+      order, meaning if you just next() the file pointer will
+      sometimes jump backwards.  For example, block foo* will
+      be written before block f* because it finished before.
+      This could possibly hurt performance if the terms dict is
+      not hot, since OSs anticipate sequential file access.  We
+      could fix the writer to re-order the blocks as a 2nd
+      pass.
+
+    - Each block encodes the term suffixes packed
+      sequentially using a separate vInt per term, which is
+      1) wasteful and 2) slow (must linear scan to find a
+      particular suffix).  We should instead 1) make
+      random-access array so we can directly access the Nth
+      suffix, and 2) bulk-encode this array using bulk int[]
+      codecs; then at search time we can binary search when
+      we seek a particular term.
+
+/**
+ * Writes terms dict and index, block-encoding (column
+ * stride) each term's metadata for each set of terms
+ * between two index terms.
+ *
+ * @lucene.experimental
+ */
+
+/** See {@link BlockTreeTermsReader}.
+ *
+ * @lucene.experimental
+*/
+
+public class BlockTreeTermsWriter extends FieldsConsumer {
+
+  public final static int DEFAULT_MIN_BLOCK_SIZE = 25;
+  public final static int DEFAULT_MAX_BLOCK_SIZE = 48;
+
+  //public final static boolean DEBUG = false;
+  public final static boolean SAVE_DOT_FILES = false;
+
+  static final int OUTPUT_FLAGS_NUM_BITS = 2;
+  static final int OUTPUT_FLAGS_MASK = 0x3;
+  static final int OUTPUT_FLAG_IS_FLOOR = 0x1;
+  static final int OUTPUT_FLAG_HAS_TERMS = 0x2;
+
+  /** Extension of terms file */
+  static final String TERMS_EXTENSION = "tim";
+  final static String TERMS_CODEC_NAME = "BLOCK_TREE_TERMS_DICT";
+  // Initial format
+  public static final int TERMS_VERSION_START = 0;
+  public static final int TERMS_VERSION_CURRENT = TERMS_VERSION_START;
+
+  /** Extension of terms index file */
+  static final String TERMS_INDEX_EXTENSION = "tip";
+  final static String TERMS_INDEX_CODEC_NAME = "BLOCK_TREE_TERMS_INDEX";
+  // Initial format
+  public static final int TERMS_INDEX_VERSION_START = 0;
+  public static final int TERMS_INDEX_VERSION_CURRENT = TERMS_INDEX_VERSION_START;
+
+  private final IndexOutput out;
+  private final IndexOutput indexOut;
+  final int minItemsInBlock;
+  final int maxItemsInBlock;
+
+  final PostingsWriterBase postingsWriter;
+  final FieldInfos fieldInfos;
+  FieldInfo currentField;
+  private final List<TermsWriter> fields = new ArrayList<TermsWriter>();
+  // private final String segment;
+
+  /** Create a new writer.  The number of items (terms or
+   *  sub-blocks) per block will aim to be between
+   *  minItemsPerBlock and maxItemsPerBlock, though in some
+   *  cases the blocks may be smaller than the min. */
+  public BlockTreeTermsWriter(
+                              SegmentWriteState state,
+                              PostingsWriterBase postingsWriter,
+                              int minItemsInBlock,
+                              int maxItemsInBlock)
+    throws IOException
+  {
+    if (minItemsInBlock <= 1) {
+      throw new IllegalArgumentException("minItemsInBlock must be >= 2; got " + minItemsInBlock);
+    }
+    if (maxItemsInBlock <= 0) {
+      throw new IllegalArgumentException("maxItemsInBlock must be >= 1; got " + maxItemsInBlock);
+    }
+    if (minItemsInBlock > maxItemsInBlock) {
+      throw new IllegalArgumentException("maxItemsInBlock must be >= minItemsInBlock; got maxItemsInBlock=" + maxItemsInBlock + " minItemsInBlock=" + minItemsInBlock);
+    }
+    if (2*(minItemsInBlock-1) > maxItemsInBlock) {
+      throw new IllegalArgumentException("maxItemsInBlock must be at least 2*(minItemsInBlock-1); got maxItemsInBlock=" + maxItemsInBlock + " minItemsInBlock=" + minItemsInBlock);
+    }
+
+    final String termsFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_EXTENSION);
+    out = state.directory.createOutput(termsFileName, state.context);
+    boolean success = false;
+    IndexOutput indexOut = null;
+    try {
+      fieldInfos = state.fieldInfos;
+      this.minItemsInBlock = minItemsInBlock;
+      this.maxItemsInBlock = maxItemsInBlock;
+      writeHeader(out);
+
+      //DEBUG = state.segmentName.equals("_4a");
+
+      final String termsIndexFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_INDEX_EXTENSION);
+      indexOut = state.directory.createOutput(termsIndexFileName, state.context);
+      writeIndexHeader(indexOut);
+
+      currentField = null;
+      this.postingsWriter = postingsWriter;
+      // segment = state.segmentName;
+
+      // System.out.println("BTW.init seg=" + state.segmentName);
+
+      postingsWriter.start(out);                          // have consumer write its format/header
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out, indexOut);
+      }
+    }
+    this.indexOut = indexOut;
+  }
+  
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, TERMS_CODEC_NAME, TERMS_VERSION_CURRENT); 
+    out.writeLong(0);                             // leave space for end index pointer    
+  }
+
+  protected void writeIndexHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, TERMS_INDEX_CODEC_NAME, TERMS_INDEX_VERSION_CURRENT); 
+    out.writeLong(0);                             // leave space for end index pointer    
+  }
+
+  protected void writeTrailer(IndexOutput out, long dirStart) throws IOException {
+    out.seek(CodecUtil.headerLength(TERMS_CODEC_NAME));
+    out.writeLong(dirStart);    
+  }
+
+  protected void writeIndexTrailer(IndexOutput indexOut, long dirStart) throws IOException {
+    indexOut.seek(CodecUtil.headerLength(TERMS_INDEX_CODEC_NAME));
+    indexOut.writeLong(dirStart);    
+  }
+  
+  @Override
+  public TermsConsumer addField(FieldInfo field) throws IOException {
+    //DEBUG = field.name.equals("id");
+    //if (DEBUG) System.out.println("\nBTTW.addField seg=" + segment + " field=" + field.name);
+    assert currentField == null || currentField.name.compareTo(field.name) < 0;
+    currentField = field;
+    final TermsWriter terms = new TermsWriter(field);
+    fields.add(terms);
+    return terms;
+  }
+
+  static long encodeOutput(long fp, boolean hasTerms, boolean isFloor) {
+    assert fp < (1L << 62);
+    return (fp << 2) | (hasTerms ? OUTPUT_FLAG_HAS_TERMS : 0) | (isFloor ? OUTPUT_FLAG_IS_FLOOR : 0);
+  }
+
+  private static class PendingEntry {
+    public final boolean isTerm;
+
+    protected PendingEntry(boolean isTerm) {
+      this.isTerm = isTerm;
+    }
+  }
+
+  private static final class PendingTerm extends PendingEntry {
+    public final BytesRef term;
+    public final TermStats stats;
+
+    public PendingTerm(BytesRef term, TermStats stats) {
+      super(true);
+      this.term = term;
+      this.stats = stats;
+    }
+
+    @Override
+    public String toString() {
+      return term.utf8ToString();
+    }
+  }
+
+  private static final class PendingBlock extends PendingEntry {
+    public final BytesRef prefix;
+    public final long fp;
+    public FST<BytesRef> index;
+    public List<FST<BytesRef>> subIndices;
+    public final boolean hasTerms;
+    public final boolean isFloor;
+    public final int floorLeadByte;
+
+    public PendingBlock(BytesRef prefix, long fp, boolean hasTerms, boolean isFloor, int floorLeadByte, List<FST<BytesRef>> subIndices) {
+      super(false);
+      this.prefix = prefix;
+      this.fp = fp;
+      this.hasTerms = hasTerms;
+      this.isFloor = isFloor;
+      this.floorLeadByte = floorLeadByte;
+      this.subIndices = subIndices;
+    }
+
+    @Override
+    public String toString() {
+      return "BLOCK: " + prefix.utf8ToString();
+    }
+
+    public void compileIndex(List<PendingBlock> floorBlocks, RAMOutputStream scratchBytes) throws IOException {
+
+      assert (isFloor && floorBlocks != null && floorBlocks.size() != 0) || (!isFloor && floorBlocks == null): "isFloor=" + isFloor + " floorBlocks=" + floorBlocks;
+
+      assert scratchBytes.getFilePointer() == 0;
+
+      // TODO: try writing the leading vLong in MSB order
+      // (opposite of what Lucene does today), for better
+      // outputs sharing in the FST
+      scratchBytes.writeVLong(encodeOutput(fp, hasTerms, isFloor));
+      if (isFloor) {
+        scratchBytes.writeVInt(floorBlocks.size());
+        for (PendingBlock sub : floorBlocks) {
+          assert sub.floorLeadByte != -1;
+          //if (DEBUG) {
+          //  System.out.println("    write floorLeadByte=" + Integer.toHexString(sub.floorLeadByte&0xff));
+          //}
+          scratchBytes.writeByte((byte) sub.floorLeadByte);
+          assert sub.fp > fp;
+          scratchBytes.writeVLong((sub.fp - fp) << 1 | (sub.hasTerms ? 1 : 0));
+        }
+      }
+
+      final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
+      final Builder<BytesRef> indexBuilder = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1,
+                                                                   0, 0, true, false, Integer.MAX_VALUE,
+                                                                   outputs, null);
+      //if (DEBUG) {
+      //  System.out.println("  compile index for prefix=" + prefix);
+      //}
+      //indexBuilder.DEBUG = false;
+      final byte[] bytes = new byte[(int) scratchBytes.getFilePointer()];
+      assert bytes.length > 0;
+      scratchBytes.writeTo(bytes, 0);
+      indexBuilder.add(prefix, new BytesRef(bytes, 0, bytes.length));
+      scratchBytes.reset();
+
+      // Copy over index for all sub-blocks
+
+      if (subIndices != null) {
+        for(FST<BytesRef> subIndex : subIndices) {
+          append(indexBuilder, subIndex);
+        }
+      }
+
+      if (floorBlocks != null) {
+        for (PendingBlock sub : floorBlocks) {
+          if (sub.subIndices != null) {
+            for(FST<BytesRef> subIndex : sub.subIndices) {
+              append(indexBuilder, subIndex);
+            }
+          }
+          sub.subIndices = null;
+        }
+      }
+
+      index = indexBuilder.finish();
+      subIndices = null;
+
+      /*
+      Writer w = new OutputStreamWriter(new FileOutputStream("out.dot"));
+      Util.toDot(index, w, false, false);
+      System.out.println("SAVED to out.dot");
+      w.close();
+      */
+    }
+
+    // TODO: maybe we could add bulk-add method to
+    // Builder?  Takes FST and unions it w/ current
+    // FST.
+    private void append(Builder<BytesRef> builder, FST<BytesRef> subIndex) throws IOException {
+      final BytesRefFSTEnum<BytesRef> subIndexEnum = new BytesRefFSTEnum<BytesRef>(subIndex);
+      BytesRefFSTEnum.InputOutput<BytesRef> indexEnt;
+      while((indexEnt = subIndexEnum.next()) != null) {
+        //if (DEBUG) {
+        //  System.out.println("      add sub=" + indexEnt.input + " " + indexEnt.input + " output=" + indexEnt.output);
+        //}
+        builder.add(indexEnt.input, indexEnt.output);
+      }
+    }
+  }
+
+  final RAMOutputStream scratchBytes = new RAMOutputStream();
+
+  class TermsWriter extends TermsConsumer {
+    private final FieldInfo fieldInfo;
+    private long numTerms;
+    long sumTotalTermFreq;
+    long sumDocFreq;
+    int docCount;
+    long indexStartFP;
+
+    // Used only to partition terms into the block tree; we
+    // don't pull an FST from this builder:
+    private final NoOutputs noOutputs;
+    private final Builder<Object> blockBuilder;
+
+    // PendingTerm or PendingBlock:
+    private final List<PendingEntry> pending = new ArrayList<PendingEntry>();
+
+    // Index into pending of most recently written block
+    private int lastBlockIndex = -1;
+
+    // Re-used when segmenting a too-large block into floor
+    // blocks:
+    private int[] subBytes = new int[10];
+    private int[] subTermCounts = new int[10];
+    private int[] subTermCountSums = new int[10];
+    private int[] subSubCounts = new int[10];
+
+    // This class assigns terms to blocks "naturally", ie,
+    // according to the number of terms under a given prefix
+    // that we encounter:
+    private class FindBlocks extends Builder.FreezeTail<Object> {
+
+      @Override
+      public void freeze(final Builder.UnCompiledNode<Object>[] frontier, int prefixLenPlus1, final IntsRef lastInput) throws IOException {
+
+        //if (DEBUG) System.out.println("  freeze prefixLenPlus1=" + prefixLenPlus1);
+
+        for(int idx=lastInput.length; idx >= prefixLenPlus1; idx--) {
+          final Builder.UnCompiledNode<Object> node = frontier[idx];
+
+          long totCount = 0;
+
+          if (node.isFinal) {
+            totCount++;
+          }
+
+          for(int arcIdx=0;arcIdx<node.numArcs;arcIdx++) {
+            @SuppressWarnings("unchecked") final Builder.UnCompiledNode<Object> target = (Builder.UnCompiledNode<Object>) node.arcs[arcIdx].target;
+            totCount += target.inputCount;
+            target.clear();
+            node.arcs[arcIdx].target = null;
+          }
+          node.numArcs = 0;
+
+          if (totCount >= minItemsInBlock || idx == 0) {
+            // We are on a prefix node that has enough
+            // entries (terms or sub-blocks) under it to let
+            // us write a new block or multiple blocks (main
+            // block + follow on floor blocks):
+            //if (DEBUG) {
+            //  if (totCount < minItemsInBlock && idx != 0) {
+            //    System.out.println("  force block has terms");
+            //  }
+            //}
+            writeBlocks(lastInput, idx, (int) totCount);
+            node.inputCount = 1;
+          } else {
+            // stragglers!  carry count upwards
+            node.inputCount = totCount;
+          }
+          frontier[idx] = new Builder.UnCompiledNode<Object>(blockBuilder, idx);
+        }
+      }
+    }
+
+    // Write the top count entries on the pending stack as
+    // one or more blocks.  Returns how many blocks were
+    // written.  If the entry count is <= maxItemsPerBlock
+    // we just write a single block; else we break into
+    // primary (initial) block and then one or more
+    // following floor blocks:
+
+    void writeBlocks(IntsRef prevTerm, int prefixLength, int count) throws IOException {
+      if (prefixLength == 0 || count <= maxItemsInBlock) {
+        // Easy case: not floor block.  Eg, prefix is "foo",
+        // and we found 30 terms/sub-blocks starting w/ that
+        // prefix, and minItemsInBlock <= 30 <=
+        // maxItemsInBlock.
+        final PendingBlock nonFloorBlock = writeBlock(prevTerm, prefixLength, prefixLength, count, count, 0, false, -1, true);
+        nonFloorBlock.compileIndex(null, scratchBytes);
+        pending.add(nonFloorBlock);
+      } else {
+        // Floor block case.  Eg, prefix is "foo" but we
+        // have 100 terms/sub-blocks starting w/ that
+        // prefix.  We segment the entries into a primary
+        // block and following floor blocks using the first
+        // label in the suffix to assign to floor blocks.
+
+        // TODO: we could store min & max suffix start byte
+        // in each block, to make floor blocks authoritative
+
+        //if (DEBUG) {
+        //  final BytesRef prefix = new BytesRef(prefixLength);
+        //  for(int m=0;m<prefixLength;m++) {
+        //    prefix.bytes[m] = (byte) prevTerm.ints[m];
+        //  }
+        //  prefix.length = prefixLength;
+        //  //System.out.println("\nWBS count=" + count + " prefix=" + prefix.utf8ToString() + " " + prefix);
+        //  System.out.println("writeBlocks: prefix=" + prefix + " " + prefix + " count=" + count + " pending.size()=" + pending.size());
+        //}
+        //System.out.println("\nwbs count=" + count);
+
+        final int savLabel = prevTerm.ints[prevTerm.offset + prefixLength];
+
+        // Count up how many items fall under
+        // each unique label after the prefix.
+        
+        // TODO: this is wasteful since the builder had
+        // already done this (partitioned these sub-terms
+        // according to their leading prefix byte)
+        
+        final List<PendingEntry> slice = pending.subList(pending.size()-count, pending.size());
+        int lastSuffixLeadLabel = -1;
+        int termCount = 0;
+        int subCount = 0;
+        int numSubs = 0;
+
+        for(PendingEntry ent : slice) {
+
+          // First byte in the suffix of this term
+          final int suffixLeadLabel;
+          if (ent.isTerm) {
+            PendingTerm term = (PendingTerm) ent;
+            if (term.term.length == prefixLength) {
+              // Suffix is 0, ie prefix 'foo' and term is
+              // 'foo' so the term has empty string suffix
+              // in this block
+              assert lastSuffixLeadLabel == -1;
+              assert numSubs == 0;
+              suffixLeadLabel = -1;
+            } else {
+              suffixLeadLabel = term.term.bytes[term.term.offset + prefixLength] & 0xff;
+            }
+          } else {
+            PendingBlock block = (PendingBlock) ent;
+            assert block.prefix.length > prefixLength;
+            suffixLeadLabel = block.prefix.bytes[block.prefix.offset + prefixLength] & 0xff;
+          }
+
+          if (suffixLeadLabel != lastSuffixLeadLabel && (termCount + subCount) != 0) {
+            if (subBytes.length == numSubs) {
+              subBytes = ArrayUtil.grow(subBytes);
+              subTermCounts = ArrayUtil.grow(subTermCounts);
+              subSubCounts = ArrayUtil.grow(subSubCounts);
+            }
+            subBytes[numSubs] = lastSuffixLeadLabel;
+            lastSuffixLeadLabel = suffixLeadLabel;
+            subTermCounts[numSubs] = termCount;
+            subSubCounts[numSubs] = subCount;
+            /*
+            if (suffixLeadLabel == -1) {
+              System.out.println("  sub " + -1 + " termCount=" + termCount + " subCount=" + subCount);
+            } else {
+              System.out.println("  sub " + Integer.toHexString(suffixLeadLabel) + " termCount=" + termCount + " subCount=" + subCount);
+            }
+            */
+            termCount = subCount = 0;
+            numSubs++;
+          }
+
+          if (ent.isTerm) {
+            termCount++;
+          } else {
+            subCount++;
+          }
+        }
+
+        if (subBytes.length == numSubs) {
+          subBytes = ArrayUtil.grow(subBytes);
+          subTermCounts = ArrayUtil.grow(subTermCounts);
+          subSubCounts = ArrayUtil.grow(subSubCounts);
+        }
+
+        subBytes[numSubs] = lastSuffixLeadLabel;
+        subTermCounts[numSubs] = termCount;
+        subSubCounts[numSubs] = subCount;
+        numSubs++;
+        /*
+        if (lastSuffixLeadLabel == -1) {
+          System.out.println("  sub " + -1 + " termCount=" + termCount + " subCount=" + subCount);
+        } else {
+          System.out.println("  sub " + Integer.toHexString(lastSuffixLeadLabel) + " termCount=" + termCount + " subCount=" + subCount);
+        }
+        */
+
+        if (subTermCountSums.length < numSubs) {
+          subTermCountSums = ArrayUtil.grow(subTermCountSums, numSubs);
+        }
+
+        // Roll up (backwards) the termCounts; postings impl
+        // needs this to know where to pull the term slice
+        // from its pending terms stack:
+        int sum = 0;
+        for(int idx=numSubs-1;idx>=0;idx--) {
+          sum += subTermCounts[idx];
+          subTermCountSums[idx] = sum;
+        }
+
+        // TODO: make a better segmenter?  It'd have to
+        // absorb the too-small end blocks backwards into
+        // the previous blocks
+
+        // Naive greedy segmentation; this is not always
+        // best (it can produce a too-small block as the
+        // last block):
+        int pendingCount = 0;
+        int startLabel = subBytes[0];
+        int curStart = count;
+        subCount = 0;
+
+        final List<PendingBlock> floorBlocks = new ArrayList<PendingBlock>();
+        PendingBlock firstBlock = null;
+
+        for(int sub=0;sub<numSubs;sub++) {
+          pendingCount += subTermCounts[sub] + subSubCounts[sub];
+          //System.out.println("  " + (subTermCounts[sub] + subSubCounts[sub]));
+          subCount++;
+
+          // Greedily make a floor block as soon as we've
+          // crossed the min count
+          if (pendingCount >= minItemsInBlock) {
+            final int curPrefixLength;
+            if (startLabel == -1) {
+              curPrefixLength = prefixLength;
+            } else {
+              curPrefixLength = 1+prefixLength;
+              // floor term:
+              prevTerm.ints[prevTerm.offset + prefixLength] = startLabel;
+            }
+            //System.out.println("  " + subCount + " subs");
+            final PendingBlock floorBlock = writeBlock(prevTerm, prefixLength, curPrefixLength, curStart, pendingCount, subTermCountSums[1+sub], true, startLabel, curStart == pendingCount);
+            if (firstBlock == null) {
+              firstBlock = floorBlock;
+            } else {
+              floorBlocks.add(floorBlock);
+            }
+            curStart -= pendingCount;
+            //System.out.println("    = " + pendingCount);
+            pendingCount = 0;
+
+            assert minItemsInBlock == 1 || subCount > 1: "minItemsInBlock=" + minItemsInBlock + " subCount=" + subCount + " sub=" + sub + " of " + numSubs + " subTermCount=" + subTermCountSums[sub] + " subSubCount=" + subSubCounts[sub] + " depth=" + prefixLength;
+            subCount = 0;
+            startLabel = subBytes[sub+1];
+
+            if (curStart == 0) {
+              break;
+            }
+
+            if (curStart <= maxItemsInBlock) {
+              // remainder is small enough to fit into a
+              // block.  NOTE that this may be too small (<
+              // minItemsInBlock); need a true segmenter
+              // here
+              assert startLabel != -1;
+              assert firstBlock != null;
+              prevTerm.ints[prevTerm.offset + prefixLength] = startLabel;
+              //System.out.println("  final " + (numSubs-sub-1) + " subs");
+              /*
+              for(sub++;sub < numSubs;sub++) {
+                System.out.println("  " + (subTermCounts[sub] + subSubCounts[sub]));
+              }
+              System.out.println("    = " + curStart);
+              if (curStart < minItemsInBlock) {
+                System.out.println("      **");
+              }
+              */
+              floorBlocks.add(writeBlock(prevTerm, prefixLength, prefixLength+1, curStart, curStart, 0, true, startLabel, true));
+              break;
+            }
+          }
+        }
+
+        prevTerm.ints[prevTerm.offset + prefixLength] = savLabel;
+
+        assert firstBlock != null;
+        firstBlock.compileIndex(floorBlocks, scratchBytes);
+
+        pending.add(firstBlock);
+        //if (DEBUG) System.out.println("  done pending.size()=" + pending.size());
+      }
+      lastBlockIndex = pending.size()-1;
+    }
+
+    // for debugging
+    private String toString(BytesRef b) {
+      try {
+        return b.utf8ToString() + " " + b;
+      } catch (Throwable t) {
+        // If BytesRef isn't actually UTF8, or it's eg a
+        // prefix of UTF8 that ends mid-unicode-char, we
+        // fallback to hex:
+        return b.toString();
+      }
+    }
+
+    // Writes all entries in the pending slice as a single
+    // block: 
+    private PendingBlock writeBlock(IntsRef prevTerm, int prefixLength, int indexPrefixLength, int startBackwards, int length,
+                                    int futureTermCount, boolean isFloor, int floorLeadByte, boolean isLastInFloor) throws IOException {
+
+      assert length > 0;
+
+      final int start = pending.size()-startBackwards;
+
+      assert start >= 0: "pending.size()=" + pending.size() + " startBackwards=" + startBackwards + " length=" + length;
+
+      final List<PendingEntry> slice = pending.subList(start, start + length);
+
+      final long startFP = out.getFilePointer();
+
+      final BytesRef prefix = new BytesRef(indexPrefixLength);
+      for(int m=0;m<indexPrefixLength;m++) {
+        prefix.bytes[m] = (byte) prevTerm.ints[m];
+      }
+      prefix.length = indexPrefixLength;
+
+      // Write block header:
+      out.writeVInt((length<<1)|(isLastInFloor ? 1:0));
+
+      // if (DEBUG) {
+      //   System.out.println("  writeBlock " + (isFloor ? "(floor) " : "") + "seg=" + segment + " pending.size()=" + pending.size() + " prefixLength=" + prefixLength + " indexPrefix=" + toString(prefix) + " entCount=" + length + " startFP=" + startFP + " futureTermCount=" + futureTermCount + (isFloor ? (" floorLeadByte=" + Integer.toHexString(floorLeadByte&0xff)) : "") + " isLastInFloor=" + isLastInFloor);
+      // }
+
+      // 1st pass: pack term suffix bytes into byte[] blob
+      // TODO: cutover to bulk int codec... simple64?
+
+      final boolean isLeafBlock;
+      if (lastBlockIndex < start) {
+        // This block definitely does not contain sub-blocks:
+        isLeafBlock = true;
+        //System.out.println("no scan true isFloor=" + isFloor);
+      } else if (!isFloor) {
+        // This block definitely does contain at least one sub-block:
+        isLeafBlock = false;
+        //System.out.println("no scan false " + lastBlockIndex + " vs start=" + start + " len=" + length);
+      } else {
+        // Must scan up-front to see if there is a sub-block
+        boolean v = true;
+        //System.out.println("scan " + lastBlockIndex + " vs start=" + start + " len=" + length);
+        for (PendingEntry ent : slice) {
+          if (!ent.isTerm) {
+            v = false;
+            break;
+          }
+        }
+        isLeafBlock = v;
+      }
+
+      final List<FST<BytesRef>> subIndices;
+
+      int termCount;
+      if (isLeafBlock) {
+        subIndices = null;
+        for (PendingEntry ent : slice) {
+          assert ent.isTerm;
+          PendingTerm term = (PendingTerm) ent;
+          final int suffix = term.term.length - prefixLength;
+          // if (DEBUG) {
+          //   BytesRef suffixBytes = new BytesRef(suffix);
+          //   System.arraycopy(term.term.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
+          //   suffixBytes.length = suffix;
+          //   System.out.println("    write term suffix=" + suffixBytes);
+          // }
+          // For leaf block we write suffix straight
+          bytesWriter.writeVInt(suffix);
+          bytesWriter.writeBytes(term.term.bytes, prefixLength, suffix);
+
+          // Write term stats, to separate byte[] blob:
+          bytesWriter2.writeVInt(term.stats.docFreq);
+          if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+            assert term.stats.totalTermFreq >= term.stats.docFreq;
+            bytesWriter2.writeVLong(term.stats.totalTermFreq - term.stats.docFreq);
+          }
+        }
+        termCount = length;
+      } else {
+        subIndices = new ArrayList<FST<BytesRef>>();
+        termCount = 0;
+        for (PendingEntry ent : slice) {
+          if (ent.isTerm) {
+            PendingTerm term = (PendingTerm) ent;
+            final int suffix = term.term.length - prefixLength;
+            // if (DEBUG) {
+            //   BytesRef suffixBytes = new BytesRef(suffix);
+            //   System.arraycopy(term.term.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
+            //   suffixBytes.length = suffix;
+            //   System.out.println("    write term suffix=" + suffixBytes);
+            // }
+            // For non-leaf block we borrow 1 bit to record
+            // if entry is term or sub-block
+            bytesWriter.writeVInt(suffix<<1);
+            bytesWriter.writeBytes(term.term.bytes, prefixLength, suffix);
+
+            // Write term stats, to separate byte[] blob:
+            bytesWriter2.writeVInt(term.stats.docFreq);
+            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+              assert term.stats.totalTermFreq >= term.stats.docFreq;
+              bytesWriter2.writeVLong(term.stats.totalTermFreq - term.stats.docFreq);
+            }
+
+            termCount++;
+          } else {
+            PendingBlock block = (PendingBlock) ent;
+            final int suffix = block.prefix.length - prefixLength;
+
+            assert suffix > 0;
+
+            // For non-leaf block we borrow 1 bit to record
+            // if entry is term or sub-block
+            bytesWriter.writeVInt((suffix<<1)|1);
+            bytesWriter.writeBytes(block.prefix.bytes, prefixLength, suffix);
+            assert block.fp < startFP;
+
+            // if (DEBUG) {
+            //   BytesRef suffixBytes = new BytesRef(suffix);
+            //   System.arraycopy(block.prefix.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
+            //   suffixBytes.length = suffix;
+            //   System.out.println("    write sub-block suffix=" + toString(suffixBytes) + " subFP=" + block.fp + " subCode=" + (startFP-block.fp) + " floor=" + block.isFloor);
+            // }
+
+            bytesWriter.writeVLong(startFP - block.fp);
+            subIndices.add(block.index);
+          }
+        }
+
+        assert subIndices.size() != 0;
+      }
+
+      // TODO: we could block-write the term suffix pointers;
+      // this would take more space but would enable binary
+      // search on lookup
+
+      // Write suffixes byte[] blob to terms dict output:
+      out.writeVInt((int) (bytesWriter.getFilePointer() << 1) | (isLeafBlock ? 1:0));
+      bytesWriter.writeTo(out);
+      bytesWriter.reset();
+
+      // Write term stats byte[] blob
+      out.writeVInt((int) bytesWriter2.getFilePointer());
+      bytesWriter2.writeTo(out);
+      bytesWriter2.reset();
+
+      // Have postings writer write block
+      postingsWriter.flushTermsBlock(futureTermCount+termCount, termCount);
+
+      // Remove slice replaced by block:
+      slice.clear();
+
+      if (lastBlockIndex >= start) {
+        if (lastBlockIndex < start+length) {
+          lastBlockIndex = start;
+        } else {
+          lastBlockIndex -= length;
+        }
+      }
+
+      // if (DEBUG) {
+      //   System.out.println("      fpEnd=" + out.getFilePointer());
+      // }
+
+      return new PendingBlock(prefix, startFP, termCount != 0, isFloor, floorLeadByte, subIndices);
+    }
+
+    TermsWriter(FieldInfo fieldInfo) {
+      this.fieldInfo = fieldInfo;
+
+      noOutputs = NoOutputs.getSingleton();
+
+      // This Builder is just used transiently to fragment
+      // terms into "good" blocks; we don't save the
+      // resulting FST:
+      blockBuilder = new Builder<Object>(FST.INPUT_TYPE.BYTE1,
+                                         0, 0, true,
+                                         true, Integer.MAX_VALUE,
+                                         noOutputs,
+                                         new FindBlocks());
+
+      postingsWriter.setField(fieldInfo);
+    }
+    
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public PostingsConsumer startTerm(BytesRef text) throws IOException {
+      //if (DEBUG) System.out.println("\nBTTW.startTerm term=" + fieldInfo.name + ":" + toString(text) + " seg=" + segment);
+      postingsWriter.startTerm();
+      /*
+      if (fieldInfo.name.equals("id")) {
+        postingsWriter.termID = Integer.parseInt(text.utf8ToString());
+      } else {
+        postingsWriter.termID = -1;
+      }
+      */
+      return postingsWriter;
+    }
+
+    @Override
+    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
+
+      assert stats.docFreq > 0;
+      //if (DEBUG) System.out.println("BTTW.finishTerm term=" + fieldInfo.name + ":" + toString(text) + " seg=" + segment + " df=" + stats.docFreq);
+
+      blockBuilder.add(text, noOutputs.getNoOutput());
+      pending.add(new PendingTerm(BytesRef.deepCopyOf(text), stats));
+      postingsWriter.finishTerm(stats);
+      numTerms++;
+    }
+
+    // Finishes all terms in this field
+    @Override
+    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
+      if (numTerms > 0) {
+        blockBuilder.finish();
+
+        // We better have one final "root" block:
+        assert pending.size() == 1 && !pending.get(0).isTerm: "pending.size()=" + pending.size() + " pending=" + pending;
+        final PendingBlock root = (PendingBlock) pending.get(0);
+        assert root.prefix.length == 0;
+        assert root.index.getEmptyOutput() != null;
+
+        this.sumTotalTermFreq = sumTotalTermFreq;
+        this.sumDocFreq = sumDocFreq;
+        this.docCount = docCount;
+
+        // Write FST to index
+        indexStartFP = indexOut.getFilePointer();
+        root.index.save(indexOut);
+        //System.out.println("  write FST " + indexStartFP + " field=" + fieldInfo.name);
+
+        // if (SAVE_DOT_FILES || DEBUG) {
+        //   final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
+        //   Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
+        //   Util.toDot(root.index, w, false, false);
+        //   System.out.println("SAVED to " + dotFileName);
+        //   w.close();
+        // }
+      }
+    }
+
+    private final RAMOutputStream bytesWriter = new RAMOutputStream();
+    private final RAMOutputStream bytesWriter2 = new RAMOutputStream();
+  }
+
+  @Override
+  public void close() throws IOException {
+
+    IOException ioe = null;
+    try {
+      
+      int nonZeroCount = 0;
+      for(TermsWriter field : fields) {
+        if (field.numTerms > 0) {
+          nonZeroCount++;
+        }
+      }
+
+      final long dirStart = out.getFilePointer();
+      final long indexDirStart = indexOut.getFilePointer();
+
+      out.writeVInt(nonZeroCount);
+      
+      for(TermsWriter field : fields) {
+        if (field.numTerms > 0) {
+          //System.out.println("  field " + field.fieldInfo.name + " " + field.numTerms + " terms");
+          out.writeVInt(field.fieldInfo.number);
+          out.writeVLong(field.numTerms);
+          final BytesRef rootCode = ((PendingBlock) field.pending.get(0)).index.getEmptyOutput();
+          assert rootCode != null: "field=" + field.fieldInfo.name + " numTerms=" + field.numTerms;
+          out.writeVInt(rootCode.length);
+          out.writeBytes(rootCode.bytes, rootCode.offset, rootCode.length);
+          if (field.fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+            out.writeVLong(field.sumTotalTermFreq);
+          }
+          out.writeVLong(field.sumDocFreq);
+          out.writeVInt(field.docCount);
+          indexOut.writeVLong(field.indexStartFP);
+        }
+      }
+      writeTrailer(out, dirStart);
+      writeIndexTrailer(indexOut, indexDirStart);
+    } catch (IOException ioe2) {
+      ioe = ioe2;
+    } finally {
+      IOUtils.closeWhileHandlingException(ioe, out, indexOut, postingsWriter);
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/Codec.java b/lucene/src/java/org/apache/lucene/codecs/Codec.java
new file mode 100644
index 0000000..6d5514a
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/Codec.java
@@ -0,0 +1,103 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.util.NamedSPILoader;
+import org.apache.lucene.store.Directory;
+
+/**
+ * Encodes/decodes an inverted index segment
+ */
+public abstract class Codec implements NamedSPILoader.NamedSPI {
+
+  private static final NamedSPILoader<Codec> loader =
+    new NamedSPILoader<Codec>(Codec.class);
+
+  private final String name;
+
+  public Codec(String name) {
+    this.name = name;
+  }
+  
+  @Override
+  public String getName() {
+    return name;
+  }
+  
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    postingsFormat().files(dir, info, "", files);
+    storedFieldsFormat().files(dir, info, files);
+    termVectorsFormat().files(dir, info, files);
+    fieldInfosFormat().files(dir, info, files);
+    // TODO: segmentInfosFormat should be allowed to declare additional files
+    // if it wants, in addition to segments_N
+    docValuesFormat().files(dir, info, files);
+    normsFormat().files(dir, info, files);
+  }
+  
+  /** Encodes/decodes postings */
+  public abstract PostingsFormat postingsFormat();
+  
+  /** Encodes/decodes docvalues */
+  public abstract DocValuesFormat docValuesFormat();
+  
+  /** Encodes/decodes stored fields */
+  public abstract StoredFieldsFormat storedFieldsFormat();
+  
+  /** Encodes/decodes term vectors */
+  public abstract TermVectorsFormat termVectorsFormat();
+  
+  /** Encodes/decodes field infos file */
+  public abstract FieldInfosFormat fieldInfosFormat();
+  
+  /** Encodes/decodes segments file */
+  public abstract SegmentInfosFormat segmentInfosFormat();
+  
+  /** Encodes/decodes document normalization values */
+  public abstract NormsFormat normsFormat();
+  
+  /** looks up a codec by name */
+  public static Codec forName(String name) {
+    return loader.lookup(name);
+  }
+  
+  /** returns a list of all available codec names */
+  public static Set<String> availableCodecs() {
+    return loader.availableServices();
+  }
+  
+  private static Codec defaultCodec = Codec.forName("Lucene40");
+  
+  // TODO: should we use this, or maybe a system property is better?
+  public static Codec getDefault() {
+    return defaultCodec;
+  }
+  
+  public static void setDefault(Codec codec) {
+    defaultCodec = codec;
+  }
+
+  @Override
+  public String toString() {
+    return name;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/DocValuesConsumer.java b/lucene/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
new file mode 100644
index 0000000..3644add
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/DocValuesConsumer.java
@@ -0,0 +1,134 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.DocValue;
+import org.apache.lucene.util.Bits;
+
+/**
+ * Abstract API that consumes {@link DocValue}s.
+ * {@link DocValuesConsumer} are always associated with a specific field and
+ * segments. Concrete implementations of this API write the given
+ * {@link DocValue} into a implementation specific format depending on
+ * the fields meta-data.
+ * 
+ * @lucene.experimental
+ */
+public abstract class DocValuesConsumer {
+
+  /**
+   * Adds the given {@link DocValue} instance to this
+   * {@link DocValuesConsumer}
+   * 
+   * @param docID
+   *          the document ID to add the value for. The docID must always
+   *          increase or be <tt>0</tt> if it is the first call to this method.
+   * @param docValue
+   *          the value to add
+   * @throws IOException
+   *           if an {@link IOException} occurs
+   */
+  public abstract void add(int docID, DocValue docValue)
+      throws IOException;
+
+  /**
+   * Called when the consumer of this API is doc with adding
+   * {@link DocValue} to this {@link DocValuesConsumer}
+   * 
+   * @param docCount
+   *          the total number of documents in this {@link DocValuesConsumer}.
+   *          Must be greater than or equal the last given docID to
+   *          {@link #add(int, DocValue)}.
+   * @throws IOException
+   */
+  public abstract void finish(int docCount) throws IOException;
+
+  /**
+   * Merges the given {@link org.apache.lucene.index.MergeState} into
+   * this {@link DocValuesConsumer}.
+   * 
+   * @param mergeState
+   *          the state to merge
+   * @param docValues docValues array containing one instance per reader (
+   *          {@link org.apache.lucene.index.MergeState#readers}) or <code>null</code> if the reader has
+   *          no {@link DocValues} instance.
+   * @throws IOException
+   *           if an {@link IOException} occurs
+   */
+  public void merge(MergeState mergeState, DocValues[] docValues) throws IOException {
+    assert mergeState != null;
+    boolean hasMerged = false;
+    for(int readerIDX=0;readerIDX<mergeState.readers.size();readerIDX++) {
+      final org.apache.lucene.index.MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(readerIDX);
+      if (docValues[readerIDX] != null) {
+        hasMerged = true;
+        merge(new SingleSubMergeState(docValues[readerIDX], mergeState.docBase[readerIDX], reader.reader.maxDoc(),
+                                    reader.liveDocs));
+      }
+    }
+    // only finish if no exception is thrown!
+    if (hasMerged) {
+      finish(mergeState.mergedDocCount);
+    }
+  }
+
+  /**
+   * Merges the given {@link SingleSubMergeState} into this {@link DocValuesConsumer}.
+   * 
+   * @param mergeState
+   *          the {@link SingleSubMergeState} to merge
+   * @throws IOException
+   *           if an {@link IOException} occurs
+   */
+  // TODO: can't we have a default implementation here that merges naively with our apis?
+  // this is how stored fields and term vectors work. its a pain to have to impl merging
+  // (should be an optimization to override it)
+  protected abstract void merge(SingleSubMergeState mergeState) throws IOException;
+
+  /**
+   * Specialized auxiliary MergeState is necessary since we don't want to
+   * exploit internals up to the codecs consumer. An instance of this class is
+   * created for each merged low level {@link IndexReader} we are merging to
+   * support low level bulk copies.
+   */
+  public static class SingleSubMergeState {
+    /**
+     * the source reader for this MergeState - merged values should be read from
+     * this instance
+     */
+    public final DocValues reader;
+    /** the absolute docBase for this MergeState within the resulting segment */
+    public final int docBase;
+    /** the number of documents in this MergeState */
+    public final int docCount;
+    /** the not deleted bits for this MergeState */
+    public final Bits liveDocs;
+
+    public SingleSubMergeState(DocValues reader, int docBase, int docCount, Bits liveDocs) {
+      assert reader != null;
+      this.reader = reader;
+      this.docBase = docBase;
+      this.docCount = docCount;
+      this.liveDocs = liveDocs;
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/DocValuesFormat.java b/lucene/src/java/org/apache/lucene/codecs/DocValuesFormat.java
new file mode 100644
index 0000000..8017573
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/DocValuesFormat.java
@@ -0,0 +1,32 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.Directory;
+
+public abstract class DocValuesFormat {
+  public abstract PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException;
+  public abstract PerDocProducer docsProducer(SegmentReadState state) throws IOException;
+  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/DocValuesReaderBase.java b/lucene/src/java/org/apache/lucene/codecs/DocValuesReaderBase.java
new file mode 100644
index 0000000..7053e0c
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/DocValuesReaderBase.java
@@ -0,0 +1,139 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Comparator;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.lucene40.values.Bytes;
+import org.apache.lucene.codecs.lucene40.values.Floats;
+import org.apache.lucene.codecs.lucene40.values.Ints;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type; // javadocs
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Abstract base class for PerDocProducer implementations
+ * @lucene.experimental
+ */
+// TODO: this needs to go under lucene40 codec (its specific to its impl)
+public abstract class DocValuesReaderBase extends PerDocProducer {
+  
+  protected abstract void closeInternal(Collection<? extends Closeable> closeables) throws IOException;
+  protected abstract Map<String, DocValues> docValues();
+  
+  @Override
+  public void close() throws IOException {
+    closeInternal(docValues().values());
+  }
+  
+  @Override
+  public DocValues docValues(String field) throws IOException {
+    return docValues().get(field);
+  }
+  
+  public Comparator<BytesRef> getComparator() throws IOException {
+    return BytesRef.getUTF8SortedAsUnicodeComparator();
+  }
+
+  // Only opens files... doesn't actually load any values
+  protected TreeMap<String, DocValues> load(FieldInfos fieldInfos,
+      String segment, int docCount, Directory dir, IOContext context)
+      throws IOException {
+    TreeMap<String, DocValues> values = new TreeMap<String, DocValues>();
+    boolean success = false;
+    try {
+
+      for (FieldInfo fieldInfo : fieldInfos) {
+        if (fieldInfo.hasDocValues()) {
+          final String field = fieldInfo.name;
+          // TODO can we have a compound file per segment and codec for
+          // docvalues?
+          final String id = DocValuesWriterBase.docValuesId(segment,
+              fieldInfo.number);
+          values.put(field,
+              loadDocValues(docCount, dir, id, fieldInfo.getDocValuesType(), context));
+        }
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        // if we fail we must close all opened resources if there are any
+        closeInternal(values.values());
+      }
+    }
+    return values;
+  }
+  
+  /**
+   * Loads a {@link DocValues} instance depending on the given {@link Type}.
+   * Codecs that use different implementations for a certain {@link Type} can
+   * simply override this method and return their custom implementations.
+   * 
+   * @param docCount
+   *          number of documents in the segment
+   * @param dir
+   *          the {@link Directory} to load the {@link DocValues} from
+   * @param id
+   *          the unique file ID within the segment
+   * @param type
+   *          the type to load
+   * @return a {@link DocValues} instance for the given type
+   * @throws IOException
+   *           if an {@link IOException} occurs
+   * @throws IllegalArgumentException
+   *           if the given {@link Type} is not supported
+   */
+  protected DocValues loadDocValues(int docCount, Directory dir, String id,
+      DocValues.Type type, IOContext context) throws IOException {
+    switch (type) {
+    case FIXED_INTS_16:
+    case FIXED_INTS_32:
+    case FIXED_INTS_64:
+    case FIXED_INTS_8:
+    case VAR_INTS:
+      return Ints.getValues(dir, id, docCount, type, context);
+    case FLOAT_32:
+      return Floats.getValues(dir, id, docCount, context, type);
+    case FLOAT_64:
+      return Floats.getValues(dir, id, docCount, context, type);
+    case BYTES_FIXED_STRAIGHT:
+      return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, true, docCount, getComparator(), context);
+    case BYTES_FIXED_DEREF:
+      return Bytes.getValues(dir, id, Bytes.Mode.DEREF, true, docCount, getComparator(), context);
+    case BYTES_FIXED_SORTED:
+      return Bytes.getValues(dir, id, Bytes.Mode.SORTED, true, docCount, getComparator(), context);
+    case BYTES_VAR_STRAIGHT:
+      return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, false, docCount, getComparator(), context);
+    case BYTES_VAR_DEREF:
+      return Bytes.getValues(dir, id, Bytes.Mode.DEREF, false, docCount, getComparator(), context);
+    case BYTES_VAR_SORTED:
+      return Bytes.getValues(dir, id, Bytes.Mode.SORTED, false, docCount, getComparator(), context);
+    default:
+      throw new IllegalStateException("unrecognized index values mode " + type);
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/DocValuesWriterBase.java b/lucene/src/java/org/apache/lucene/codecs/DocValuesWriterBase.java
new file mode 100644
index 0000000..86c364c
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/DocValuesWriterBase.java
@@ -0,0 +1,71 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.codecs.lucene40.values.Writer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+
+/**
+ * Abstract base class for PerDocConsumer implementations
+ * @lucene.experimental
+ */
+//TODO: this needs to go under lucene40 codec (its specific to its impl)
+public abstract class DocValuesWriterBase extends PerDocConsumer {
+  protected final String segmentName;
+  protected final String segmentSuffix;
+  private final Counter bytesUsed;
+  protected final IOContext context;
+  
+  protected DocValuesWriterBase(PerDocWriteState state) {
+    this.segmentName = state.segmentName;
+    this.segmentSuffix = state.segmentSuffix;
+    this.bytesUsed = state.bytesUsed;
+    this.context = state.context;
+  }
+
+  protected abstract Directory getDirectory() throws IOException;
+  
+  @Override
+  public void close() throws IOException {   
+  }
+
+  @Override
+  public DocValuesConsumer addValuesField(DocValues.Type valueType, FieldInfo field) throws IOException {
+    return Writer.create(valueType,
+        docValuesId(segmentName, field.number), 
+        getDirectory(), getComparator(), bytesUsed, context);
+  }
+
+  public static String docValuesId(String segmentsName, int fieldId) {
+    return segmentsName + "_" + fieldId;
+  }
+  
+  
+  public Comparator<BytesRef> getComparator() throws IOException {
+    return BytesRef.getUTF8SortedAsUnicodeComparator();
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/FieldInfosFormat.java b/lucene/src/java/org/apache/lucene/codecs/FieldInfosFormat.java
new file mode 100644
index 0000000..8491e16
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/FieldInfosFormat.java
@@ -0,0 +1,33 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+
+/**
+ * @lucene.experimental
+ */
+public abstract class FieldInfosFormat {
+  public abstract FieldInfosReader getFieldInfosReader() throws IOException;
+  public abstract FieldInfosWriter getFieldInfosWriter() throws IOException;
+  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/FieldInfosReader.java b/lucene/src/java/org/apache/lucene/codecs/FieldInfosReader.java
new file mode 100644
index 0000000..dce9fbd
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/FieldInfosReader.java
@@ -0,0 +1,31 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * @lucene.experimental
+ */
+public abstract class FieldInfosReader {
+  public abstract FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/FieldInfosWriter.java b/lucene/src/java/org/apache/lucene/codecs/FieldInfosWriter.java
new file mode 100644
index 0000000..bcdfce9
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/FieldInfosWriter.java
@@ -0,0 +1,31 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * @lucene.experimental
+ */
+public abstract class FieldInfosWriter {
+  public abstract void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/FieldsConsumer.java b/lucene/src/java/org/apache/lucene/codecs/FieldsConsumer.java
new file mode 100644
index 0000000..ea47207
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/FieldsConsumer.java
@@ -0,0 +1,58 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.Terms;
+
+/** Abstract API that consumes terms, doc, freq, prox and
+ *  payloads postings.  Concrete implementations of this
+ *  actually do "something" with the postings (write it into
+ *  the index in a specific format).
+ *
+ * @lucene.experimental
+ */
+public abstract class FieldsConsumer implements Closeable {
+
+  /** Add a new field */
+  public abstract TermsConsumer addField(FieldInfo field) throws IOException;
+  
+  /** Called when we are done adding everything. */
+  public abstract void close() throws IOException;
+
+  public void merge(MergeState mergeState, Fields fields) throws IOException {
+    FieldsEnum fieldsEnum = fields.iterator();
+    assert fieldsEnum != null;
+    String field;
+    while((field = fieldsEnum.next()) != null) {
+      mergeState.fieldInfo = mergeState.fieldInfos.fieldInfo(field);
+      assert mergeState.fieldInfo != null : "FieldInfo for field is null: "+ field;
+      Terms terms = fieldsEnum.terms();
+      if (terms != null) {
+        final TermsConsumer termsConsumer = addField(mergeState.fieldInfo);
+        termsConsumer.merge(mergeState, terms.iterator(null));
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/FieldsProducer.java b/lucene/src/java/org/apache/lucene/codecs/FieldsProducer.java
new file mode 100644
index 0000000..d2862ad
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/FieldsProducer.java
@@ -0,0 +1,37 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.Terms;
+
+/** Abstract API that consumes terms, doc, freq, prox and
+ *  payloads postings.  Concrete implementations of this
+ *  actually do "something" with the postings (write it into
+ *  the index in a specific format).
+ *
+ * @lucene.experimental
+ */
+
+public abstract class FieldsProducer extends Fields implements Closeable {
+  public abstract void close() throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java b/lucene/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java
new file mode 100644
index 0000000..d58d721
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java
@@ -0,0 +1,407 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.packed.PackedInts;
+
+import java.util.HashMap;
+import java.util.Collection;
+import java.util.Comparator;
+import java.io.IOException;
+
+import org.apache.lucene.index.IndexFileNames;
+
+/** @lucene.experimental */
+public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
+
+  // NOTE: long is overkill here, since this number is 128
+  // by default and only indexDivisor * 128 if you change
+  // the indexDivisor at search time.  But, we use this in a
+  // number of places to multiply out the actual ord, and we
+  // will overflow int during those multiplies.  So to avoid
+  // having to upgrade each multiple to long in multiple
+  // places (error prone), we use long here:
+  private long totalIndexInterval;
+
+  private int indexDivisor;
+  final private int indexInterval;
+
+  // Closed if indexLoaded is true:
+  private IndexInput in;
+  private volatile boolean indexLoaded;
+
+  private final Comparator<BytesRef> termComp;
+
+  private final static int PAGED_BYTES_BITS = 15;
+
+  // all fields share this single logical byte[]
+  private final PagedBytes termBytes = new PagedBytes(PAGED_BYTES_BITS);
+  private PagedBytes.Reader termBytesReader;
+
+  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
+  
+  // start of the field info data
+  protected long dirOffset;
+
+  public FixedGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, Comparator<BytesRef> termComp, String segmentSuffix, IOContext context)
+    throws IOException {
+
+    this.termComp = termComp;
+
+    assert indexDivisor == -1 || indexDivisor > 0;
+
+    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION), context);
+    
+    boolean success = false;
+
+    try {
+      
+      readHeader(in);
+      indexInterval = in.readInt();
+      this.indexDivisor = indexDivisor;
+
+      if (indexDivisor < 0) {
+        totalIndexInterval = indexInterval;
+      } else {
+        // In case terms index gets loaded, later, on demand
+        totalIndexInterval = indexInterval * indexDivisor;
+      }
+      assert totalIndexInterval > 0;
+      
+      seekDir(in, dirOffset);
+
+      // Read directory
+      final int numFields = in.readVInt();      
+      //System.out.println("FGR: init seg=" + segment + " div=" + indexDivisor + " nF=" + numFields);
+      for(int i=0;i<numFields;i++) {
+        final int field = in.readVInt();
+        final int numIndexTerms = in.readVInt();
+        final long termsStart = in.readVLong();
+        final long indexStart = in.readVLong();
+        final long packedIndexStart = in.readVLong();
+        final long packedOffsetsStart = in.readVLong();
+        assert packedIndexStart >= indexStart: "packedStart=" + packedIndexStart + " indexStart=" + indexStart + " numIndexTerms=" + numIndexTerms + " seg=" + segment;
+        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        fields.put(fieldInfo, new FieldIndexData(fieldInfo, numIndexTerms, indexStart, termsStart, packedIndexStart, packedOffsetsStart));
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(in);
+      }
+      if (indexDivisor > 0) {
+        in.close();
+        in = null;
+        if (success) {
+          indexLoaded = true;
+        }
+        termBytesReader = termBytes.freeze(true);
+      }
+    }
+  }
+  
+  @Override
+  public int getDivisor() {
+    return indexDivisor;
+  }
+
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, FixedGapTermsIndexWriter.CODEC_NAME,
+      FixedGapTermsIndexWriter.VERSION_START, FixedGapTermsIndexWriter.VERSION_START);
+    dirOffset = input.readLong();
+  }
+
+  private class IndexEnum extends FieldIndexEnum {
+    private final FieldIndexData.CoreFieldIndex fieldIndex;
+    private final BytesRef term = new BytesRef();
+    private long ord;
+
+    public IndexEnum(FieldIndexData.CoreFieldIndex fieldIndex) {
+      this.fieldIndex = fieldIndex;
+    }
+
+    @Override
+    public BytesRef term() {
+      return term;
+    }
+
+    @Override
+    public long seek(BytesRef target) {
+      int lo = 0;				  // binary search
+      int hi = fieldIndex.numIndexTerms - 1;
+      assert totalIndexInterval > 0 : "totalIndexInterval=" + totalIndexInterval;
+
+      while (hi >= lo) {
+        int mid = (lo + hi) >>> 1;
+
+        final long offset = fieldIndex.termOffsets.get(mid);
+        final int length = (int) (fieldIndex.termOffsets.get(1+mid) - offset);
+        termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+
+        int delta = termComp.compare(target, term);
+        if (delta < 0) {
+          hi = mid - 1;
+        } else if (delta > 0) {
+          lo = mid + 1;
+        } else {
+          assert mid >= 0;
+          ord = mid*totalIndexInterval;
+          return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(mid);
+        }
+      }
+
+      if (hi < 0) {
+        assert hi == -1;
+        hi = 0;
+      }
+
+      final long offset = fieldIndex.termOffsets.get(hi);
+      final int length = (int) (fieldIndex.termOffsets.get(1+hi) - offset);
+      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+
+      ord = hi*totalIndexInterval;
+      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(hi);
+    }
+
+    @Override
+    public long next() {
+      final int idx = 1 + (int) (ord / totalIndexInterval);
+      if (idx >= fieldIndex.numIndexTerms) {
+        return -1;
+      }
+      ord += totalIndexInterval;
+
+      final long offset = fieldIndex.termOffsets.get(idx);
+      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
+      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
+    }
+
+    @Override
+    public long ord() {
+      return ord;
+    }
+
+    @Override
+    public long seek(long ord) {
+      int idx = (int) (ord / totalIndexInterval);
+      // caller must ensure ord is in bounds
+      assert idx < fieldIndex.numIndexTerms;
+      final long offset = fieldIndex.termOffsets.get(idx);
+      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
+      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+      this.ord = idx * totalIndexInterval;
+      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
+    }
+  }
+
+  @Override
+  public boolean supportsOrd() {
+    return true;
+  }
+
+  private final class FieldIndexData {
+
+    final private FieldInfo fieldInfo;
+
+    volatile CoreFieldIndex coreIndex;
+
+    private final long indexStart;
+    private final long termsStart;
+    private final long packedIndexStart;
+    private final long packedOffsetsStart;
+
+    private final int numIndexTerms;
+
+    public FieldIndexData(FieldInfo fieldInfo, int numIndexTerms, long indexStart, long termsStart, long packedIndexStart,
+                          long packedOffsetsStart) throws IOException {
+
+      this.fieldInfo = fieldInfo;
+      this.termsStart = termsStart;
+      this.indexStart = indexStart;
+      this.packedIndexStart = packedIndexStart;
+      this.packedOffsetsStart = packedOffsetsStart;
+      this.numIndexTerms = numIndexTerms;
+
+      if (indexDivisor > 0) {
+        loadTermsIndex();
+      }
+    }
+
+    private void loadTermsIndex() throws IOException {
+      if (coreIndex == null) {
+        coreIndex = new CoreFieldIndex(indexStart, termsStart, packedIndexStart, packedOffsetsStart, numIndexTerms);
+      }
+    }
+
+    private final class CoreFieldIndex {
+
+      // where this field's terms begin in the packed byte[]
+      // data
+      final long termBytesStart;
+
+      // offset into index termBytes
+      final PackedInts.Reader termOffsets;
+
+      // index pointers into main terms dict
+      final PackedInts.Reader termsDictOffsets;
+
+      final int numIndexTerms;
+      final long termsStart;
+
+      public CoreFieldIndex(long indexStart, long termsStart, long packedIndexStart, long packedOffsetsStart, int numIndexTerms) throws IOException {
+
+        this.termsStart = termsStart;
+        termBytesStart = termBytes.getPointer();
+
+        IndexInput clone = (IndexInput) in.clone();
+        clone.seek(indexStart);
+
+        // -1 is passed to mean "don't load term index", but
+        // if we are then later loaded it's overwritten with
+        // a real value
+        assert indexDivisor > 0;
+
+        this.numIndexTerms = 1+(numIndexTerms-1) / indexDivisor;
+
+        assert this.numIndexTerms  > 0: "numIndexTerms=" + numIndexTerms + " indexDivisor=" + indexDivisor;
+
+        if (indexDivisor == 1) {
+          // Default (load all index terms) is fast -- slurp in the images from disk:
+          
+          try {
+            final long numTermBytes = packedIndexStart - indexStart;
+            termBytes.copy(clone, numTermBytes);
+
+            // records offsets into main terms dict file
+            termsDictOffsets = PackedInts.getReader(clone);
+            assert termsDictOffsets.size() == numIndexTerms;
+
+            // records offsets into byte[] term data
+            termOffsets = PackedInts.getReader(clone);
+            assert termOffsets.size() == 1+numIndexTerms;
+          } finally {
+            clone.close();
+          }
+        } else {
+          // Get packed iterators
+          final IndexInput clone1 = (IndexInput) in.clone();
+          final IndexInput clone2 = (IndexInput) in.clone();
+
+          try {
+            // Subsample the index terms
+            clone1.seek(packedIndexStart);
+            final PackedInts.ReaderIterator termsDictOffsetsIter = PackedInts.getReaderIterator(clone1);
+
+            clone2.seek(packedOffsetsStart);
+            final PackedInts.ReaderIterator termOffsetsIter = PackedInts.getReaderIterator(clone2);
+
+            // TODO: often we can get by w/ fewer bits per
+            // value, below.. .but this'd be more complex:
+            // we'd have to try @ fewer bits and then grow
+            // if we overflowed it.
+
+            PackedInts.Mutable termsDictOffsetsM = PackedInts.getMutable(this.numIndexTerms, termsDictOffsetsIter.getBitsPerValue());
+            PackedInts.Mutable termOffsetsM = PackedInts.getMutable(this.numIndexTerms+1, termOffsetsIter.getBitsPerValue());
+
+            termsDictOffsets = termsDictOffsetsM;
+            termOffsets = termOffsetsM;
+
+            int upto = 0;
+
+            long termOffsetUpto = 0;
+
+            while(upto < this.numIndexTerms) {
+              // main file offset copies straight over
+              termsDictOffsetsM.set(upto, termsDictOffsetsIter.next());
+
+              termOffsetsM.set(upto, termOffsetUpto);
+
+              long termOffset = termOffsetsIter.next();
+              long nextTermOffset = termOffsetsIter.next();
+              final int numTermBytes = (int) (nextTermOffset - termOffset);
+
+              clone.seek(indexStart + termOffset);
+              assert indexStart + termOffset < clone.length() : "indexStart=" + indexStart + " termOffset=" + termOffset + " len=" + clone.length();
+              assert indexStart + termOffset + numTermBytes < clone.length();
+
+              termBytes.copy(clone, numTermBytes);
+              termOffsetUpto += numTermBytes;
+
+              upto++;
+              if (upto == this.numIndexTerms) {
+                break;
+              }
+
+              // skip terms:
+              termsDictOffsetsIter.next();
+              for(int i=0;i<indexDivisor-2;i++) {
+                termOffsetsIter.next();
+                termsDictOffsetsIter.next();
+              }
+            }
+            termOffsetsM.set(upto, termOffsetUpto);
+
+          } finally {
+            clone1.close();
+            clone2.close();
+            clone.close();
+          }
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
+    final FieldIndexData fieldData = fields.get(fieldInfo);
+    if (fieldData.coreIndex == null) {
+      return null;
+    } else {
+      return new IndexEnum(fieldData.coreIndex);
+    }
+  }
+
+  public static void files(Directory dir, SegmentInfo info, String segmentSuffix, Collection<String> files) {
+    files.add(IndexFileNames.segmentFileName(info.name, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION));
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (in != null && !indexLoaded) {
+      in.close();
+    }
+    if (termBytesReader != null) {
+      termBytesReader.close();
+    }
+  }
+
+  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
+    input.seek(dirOffset);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java b/lucene/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java
new file mode 100644
index 0000000..ab3453c
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java
@@ -0,0 +1,255 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.io.IOException;
+
+/**
+ * Selects every Nth term as and index term, and hold term
+ * bytes fully expanded in memory.  This terms index
+ * supports seeking by ord.  See {@link
+ * VariableGapTermsIndexWriter} for a more memory efficient
+ * terms index that does not support seeking by ord.
+ *
+ * @lucene.experimental */
+public class FixedGapTermsIndexWriter extends TermsIndexWriterBase {
+  protected final IndexOutput out;
+
+  /** Extension of terms index file */
+  static final String TERMS_INDEX_EXTENSION = "tii";
+
+  final static String CODEC_NAME = "SIMPLE_STANDARD_TERMS_INDEX";
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  final private int termIndexInterval;
+
+  private final List<SimpleFieldWriter> fields = new ArrayList<SimpleFieldWriter>();
+  private final FieldInfos fieldInfos; // unread
+
+  public FixedGapTermsIndexWriter(SegmentWriteState state) throws IOException {
+    final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_INDEX_EXTENSION);
+    termIndexInterval = state.termIndexInterval;
+    out = state.directory.createOutput(indexFileName, state.context);
+    boolean success = false;
+    try {
+      fieldInfos = state.fieldInfos;
+      writeHeader(out);
+      out.writeInt(termIndexInterval);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
+    // Placeholder for dir offset
+    out.writeLong(0);
+  }
+
+  @Override
+  public FieldWriter addField(FieldInfo field, long termsFilePointer) {
+    //System.out.println("FGW: addFfield=" + field.name);
+    SimpleFieldWriter writer = new SimpleFieldWriter(field, termsFilePointer);
+    fields.add(writer);
+    return writer;
+  }
+
+  /** NOTE: if your codec does not sort in unicode code
+   *  point order, you must override this method, to simply
+   *  return indexedTerm.length. */
+  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
+    // As long as codec sorts terms in unicode codepoint
+    // order, we can safely strip off the non-distinguishing
+    // suffix to save RAM in the loaded terms index.
+    final int idxTermOffset = indexedTerm.offset;
+    final int priorTermOffset = priorTerm.offset;
+    final int limit = Math.min(priorTerm.length, indexedTerm.length);
+    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
+      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
+        return byteIdx+1;
+      }
+    }
+    return Math.min(1+priorTerm.length, indexedTerm.length);
+  }
+
+  private class SimpleFieldWriter extends FieldWriter {
+    final FieldInfo fieldInfo;
+    int numIndexTerms;
+    final long indexStart;
+    final long termsStart;
+    long packedIndexStart;
+    long packedOffsetsStart;
+    private long numTerms;
+
+    // TODO: we could conceivably make a PackedInts wrapper
+    // that auto-grows... then we wouldn't force 6 bytes RAM
+    // per index term:
+    private short[] termLengths;
+    private int[] termsPointerDeltas;
+    private long lastTermsPointer;
+    private long totTermLength;
+
+    private final BytesRef lastTerm = new BytesRef();
+
+    SimpleFieldWriter(FieldInfo fieldInfo, long termsFilePointer) {
+      this.fieldInfo = fieldInfo;
+      indexStart = out.getFilePointer();
+      termsStart = lastTermsPointer = termsFilePointer;
+      termLengths = new short[0];
+      termsPointerDeltas = new int[0];
+    }
+
+    @Override
+    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
+      // First term is first indexed term:
+      //System.out.println("FGW: checkIndexTerm text=" + text.utf8ToString());
+      if (0 == (numTerms++ % termIndexInterval)) {
+        return true;
+      } else {
+        if (0 == numTerms % termIndexInterval) {
+          // save last term just before next index term so we
+          // can compute wasted suffix
+          lastTerm.copyBytes(text);
+        }
+        return false;
+      }
+    }
+
+    @Override
+    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
+      final int indexedTermLength = indexedTermPrefixLength(lastTerm, text);
+      //System.out.println("FGW: add text=" + text.utf8ToString() + " " + text + " fp=" + termsFilePointer);
+
+      // write only the min prefix that shows the diff
+      // against prior term
+      out.writeBytes(text.bytes, text.offset, indexedTermLength);
+
+      if (termLengths.length == numIndexTerms) {
+        termLengths = ArrayUtil.grow(termLengths);
+      }
+      if (termsPointerDeltas.length == numIndexTerms) {
+        termsPointerDeltas = ArrayUtil.grow(termsPointerDeltas);
+      }
+
+      // save delta terms pointer
+      termsPointerDeltas[numIndexTerms] = (int) (termsFilePointer - lastTermsPointer);
+      lastTermsPointer = termsFilePointer;
+
+      // save term length (in bytes)
+      assert indexedTermLength <= Short.MAX_VALUE;
+      termLengths[numIndexTerms] = (short) indexedTermLength;
+      totTermLength += indexedTermLength;
+
+      lastTerm.copyBytes(text);
+      numIndexTerms++;
+    }
+
+    @Override
+    public void finish(long termsFilePointer) throws IOException {
+
+      // write primary terms dict offsets
+      packedIndexStart = out.getFilePointer();
+
+      PackedInts.Writer w = PackedInts.getWriter(out, numIndexTerms, PackedInts.bitsRequired(termsFilePointer));
+
+      // relative to our indexStart
+      long upto = 0;
+      for(int i=0;i<numIndexTerms;i++) {
+        upto += termsPointerDeltas[i];
+        w.add(upto);
+      }
+      w.finish();
+
+      packedOffsetsStart = out.getFilePointer();
+
+      // write offsets into the byte[] terms
+      w = PackedInts.getWriter(out, 1+numIndexTerms, PackedInts.bitsRequired(totTermLength));
+      upto = 0;
+      for(int i=0;i<numIndexTerms;i++) {
+        w.add(upto);
+        upto += termLengths[i];
+      }
+      w.add(upto);
+      w.finish();
+
+      // our referrer holds onto us, while other fields are
+      // being written, so don't tie up this RAM:
+      termLengths = null;
+      termsPointerDeltas = null;
+    }
+  }
+
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      final long dirStart = out.getFilePointer();
+      final int fieldCount = fields.size();
+      
+      int nonNullFieldCount = 0;
+      for(int i=0;i<fieldCount;i++) {
+        SimpleFieldWriter field = fields.get(i);
+        if (field.numIndexTerms > 0) {
+          nonNullFieldCount++;
+        }
+      }
+      
+      out.writeVInt(nonNullFieldCount);
+      for(int i=0;i<fieldCount;i++) {
+        SimpleFieldWriter field = fields.get(i);
+        if (field.numIndexTerms > 0) {
+          out.writeVInt(field.fieldInfo.number);
+          out.writeVInt(field.numIndexTerms);
+          out.writeVLong(field.termsStart);
+          out.writeVLong(field.indexStart);
+          out.writeVLong(field.packedIndexStart);
+          out.writeVLong(field.packedOffsetsStart);
+        }
+      }
+      writeTrailer(dirStart);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(out);
+      } else {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+
+  protected void writeTrailer(long dirStart) throws IOException {
+    out.seek(CodecUtil.headerLength(CODEC_NAME));
+    out.writeLong(dirStart);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/MappingMultiDocsAndPositionsEnum.java b/lucene/src/java/org/apache/lucene/codecs/MappingMultiDocsAndPositionsEnum.java
new file mode 100644
index 0000000..346f8ad
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/MappingMultiDocsAndPositionsEnum.java
@@ -0,0 +1,120 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.MultiDocsAndPositionsEnum;
+import java.io.IOException;
+
+/**
+ * Exposes flex API, merged from flex API of sub-segments,
+ * remapping docIDs (this is used for segment merging).
+ *
+ * @lucene.experimental
+ */
+
+public final class MappingMultiDocsAndPositionsEnum extends DocsAndPositionsEnum {
+  private MultiDocsAndPositionsEnum.EnumWithSlice[] subs;
+  int numSubs;
+  int upto;
+  int[] currentMap;
+  DocsAndPositionsEnum current;
+  int currentBase;
+  int doc = -1;
+  private MergeState mergeState;
+
+  MappingMultiDocsAndPositionsEnum reset(MultiDocsAndPositionsEnum postingsEnum) throws IOException {
+    this.numSubs = postingsEnum.getNumSubs();
+    this.subs = postingsEnum.getSubs();
+    upto = -1;
+    current = null;
+    return this;
+  }
+
+  public void setMergeState(MergeState mergeState) {
+    this.mergeState = mergeState;
+  }
+
+  @Override
+  public int freq() {
+    return current.freq();
+  }
+
+  @Override
+  public int docID() {
+    return doc;
+  }
+
+  @Override
+  public int advance(int target) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public int nextDoc() throws IOException {
+    while(true) {
+      if (current == null) {
+        if (upto == numSubs-1) {
+          return this.doc = NO_MORE_DOCS;
+        } else {
+          upto++;
+          final int reader = subs[upto].slice.readerIndex;
+          current = subs[upto].docsAndPositionsEnum;
+          currentBase = mergeState.docBase[reader];
+          currentMap = mergeState.docMaps[reader];
+        }
+      }
+
+      int doc = current.nextDoc();
+      if (doc != NO_MORE_DOCS) {
+        if (currentMap != null) {
+          // compact deletions
+          doc = currentMap[doc];
+          if (doc == -1) {
+            continue;
+          }
+        }
+        return this.doc = currentBase + doc;
+      } else {
+        current = null;
+      }
+    }
+  }
+
+  @Override
+  public int nextPosition() throws IOException {
+    return current.nextPosition();
+  }
+  
+  @Override
+  public BytesRef getPayload() throws IOException {
+    BytesRef payload = current.getPayload();
+    if (mergeState.currentPayloadProcessor[upto] != null) {
+      mergeState.currentPayloadProcessor[upto].processPayload(payload);
+    }
+    return payload;
+  }
+
+  @Override
+  public boolean hasPayload() {
+    return current.hasPayload();
+  }
+}
+
diff --git a/lucene/src/java/org/apache/lucene/codecs/MappingMultiDocsEnum.java b/lucene/src/java/org/apache/lucene/codecs/MappingMultiDocsEnum.java
new file mode 100644
index 0000000..0f60e13
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/MappingMultiDocsEnum.java
@@ -0,0 +1,100 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.MultiDocsEnum;
+import java.io.IOException;
+
+/**
+ * Exposes flex API, merged from flex API of sub-segments,
+ * remapping docIDs (this is used for segment merging).
+ *
+ * @lucene.experimental
+ */
+
+public final class MappingMultiDocsEnum extends DocsEnum {
+  private MultiDocsEnum.EnumWithSlice[] subs;
+  int numSubs;
+  int upto;
+  int[] currentMap;
+  DocsEnum current;
+  int currentBase;
+  int doc = -1;
+  private MergeState mergeState;
+
+  MappingMultiDocsEnum reset(MultiDocsEnum docsEnum) throws IOException {
+    this.numSubs = docsEnum.getNumSubs();
+    this.subs = docsEnum.getSubs();
+    upto = -1;
+    current = null;
+    return this;
+  }
+
+  public void setMergeState(MergeState mergeState) {
+    this.mergeState = mergeState;
+  }
+
+  @Override
+  public int freq() {
+    return current.freq();
+  }
+
+  @Override
+  public int docID() {
+    return doc;
+  }
+
+  @Override
+  public int advance(int target) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public int nextDoc() throws IOException {
+    while(true) {
+      if (current == null) {
+        if (upto == numSubs-1) {
+          return this.doc = NO_MORE_DOCS;
+        } else {
+          upto++;
+          final int reader = subs[upto].slice.readerIndex;
+          current = subs[upto].docsEnum;
+          currentBase = mergeState.docBase[reader];
+          currentMap = mergeState.docMaps[reader];
+        }
+      }
+
+      int doc = current.nextDoc();
+      if (doc != NO_MORE_DOCS) {
+        if (currentMap != null) {
+          // compact deletions
+          doc = currentMap[doc];
+          if (doc == -1) {
+            continue;
+          }
+        }
+        return this.doc = currentBase + doc;
+      } else {
+        current = null;
+      }
+    }
+  }
+}
+
diff --git a/lucene/src/java/org/apache/lucene/codecs/MultiLevelSkipListReader.java b/lucene/src/java/org/apache/lucene/codecs/MultiLevelSkipListReader.java
new file mode 100644
index 0000000..7c23721
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/MultiLevelSkipListReader.java
@@ -0,0 +1,296 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.BufferedIndexInput;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * This abstract class reads skip lists with multiple levels.
+ * 
+ * See {@link MultiLevelSkipListWriter} for the information about the encoding 
+ * of the multi level skip lists. 
+ * 
+ * Subclasses must implement the abstract method {@link #readSkipData(int, IndexInput)}
+ * which defines the actual format of the skip data.
+ * @lucene.experimental
+ */
+
+public abstract class MultiLevelSkipListReader {
+  // the maximum number of skip levels possible for this index
+  protected int maxNumberOfSkipLevels; 
+  
+  // number of levels in this skip list
+  private int numberOfSkipLevels;
+  
+  // Expert: defines the number of top skip levels to buffer in memory.
+  // Reducing this number results in less memory usage, but possibly
+  // slower performance due to more random I/Os.
+  // Please notice that the space each level occupies is limited by
+  // the skipInterval. The top level can not contain more than
+  // skipLevel entries, the second top level can not contain more
+  // than skipLevel^2 entries and so forth.
+  private int numberOfLevelsToBuffer = 1;
+  
+  private int docCount;
+  private boolean haveSkipped;
+  
+  private IndexInput[] skipStream;    // skipStream for each level
+  private long skipPointer[];         // the start pointer of each skip level
+  private int skipInterval[];         // skipInterval of each level
+  private int[] numSkipped;           // number of docs skipped per level
+    
+  private int[] skipDoc;              // doc id of current skip entry per level 
+  private int lastDoc;                // doc id of last read skip entry with docId <= target
+  private long[] childPointer;        // child pointer of current skip entry per level
+  private long lastChildPointer;      // childPointer of last read skip entry with docId <= target
+  
+  private boolean inputIsBuffered;
+  
+  public MultiLevelSkipListReader(IndexInput skipStream, int maxSkipLevels, int skipInterval) {
+    this.skipStream = new IndexInput[maxSkipLevels];
+    this.skipPointer = new long[maxSkipLevels];
+    this.childPointer = new long[maxSkipLevels];
+    this.numSkipped = new int[maxSkipLevels];
+    this.maxNumberOfSkipLevels = maxSkipLevels;
+    this.skipInterval = new int[maxSkipLevels];
+    this.skipStream [0]= skipStream;
+    this.inputIsBuffered = (skipStream instanceof BufferedIndexInput);
+    this.skipInterval[0] = skipInterval;
+    for (int i = 1; i < maxSkipLevels; i++) {
+      // cache skip intervals
+      this.skipInterval[i] = this.skipInterval[i - 1] * skipInterval;
+    }
+    skipDoc = new int[maxSkipLevels];
+  }
+
+  
+  /** Returns the id of the doc to which the last call of {@link #skipTo(int)}
+   *  has skipped.  */
+  public int getDoc() {
+    return lastDoc;
+  }
+  
+  
+  /** Skips entries to the first beyond the current whose document number is
+   *  greater than or equal to <i>target</i>. Returns the current doc count. 
+   */
+  public int skipTo(int target) throws IOException {
+    if (!haveSkipped) {
+      // first time, load skip levels
+      loadSkipLevels();
+      haveSkipped = true;
+    }
+  
+    // walk up the levels until highest level is found that has a skip
+    // for this target
+    int level = 0;
+    while (level < numberOfSkipLevels - 1 && target > skipDoc[level + 1]) {
+      level++;
+    }    
+
+    while (level >= 0) {
+      if (target > skipDoc[level]) {
+        if (!loadNextSkip(level)) {
+          continue;
+        }
+      } else {
+        // no more skips on this level, go down one level
+        if (level > 0 && lastChildPointer > skipStream[level - 1].getFilePointer()) {
+          seekChild(level - 1);
+        } 
+        level--;
+      }
+    }
+    
+    return numSkipped[0] - skipInterval[0] - 1;
+  }
+  
+  private boolean loadNextSkip(int level) throws IOException {
+    // we have to skip, the target document is greater than the current
+    // skip list entry        
+    setLastSkipData(level);
+      
+    numSkipped[level] += skipInterval[level];
+      
+    if (numSkipped[level] > docCount) {
+      // this skip list is exhausted
+      skipDoc[level] = Integer.MAX_VALUE;
+      if (numberOfSkipLevels > level) numberOfSkipLevels = level; 
+      return false;
+    }
+
+    // read next skip entry
+    skipDoc[level] += readSkipData(level, skipStream[level]);
+    
+    if (level != 0) {
+      // read the child pointer if we are not on the leaf level
+      childPointer[level] = skipStream[level].readVLong() + skipPointer[level - 1];
+    }
+    
+    return true;
+
+  }
+  
+  /** Seeks the skip entry on the given level */
+  protected void seekChild(int level) throws IOException {
+    skipStream[level].seek(lastChildPointer);
+    numSkipped[level] = numSkipped[level + 1] - skipInterval[level + 1];
+    skipDoc[level] = lastDoc;
+    if (level > 0) {
+        childPointer[level] = skipStream[level].readVLong() + skipPointer[level - 1];
+    }
+  }
+
+  public void close() throws IOException {
+    for (int i = 1; i < skipStream.length; i++) {
+      if (skipStream[i] != null) {
+        skipStream[i].close();
+      }
+    }
+  }
+
+  /** initializes the reader */
+  public void init(long skipPointer, int df) {
+    this.skipPointer[0] = skipPointer;
+    this.docCount = df;
+    assert skipPointer >= 0 && skipPointer <= skipStream[0].length() 
+    : "invalid skip pointer: " + skipPointer + ", length=" + skipStream[0].length();
+    Arrays.fill(skipDoc, 0);
+    Arrays.fill(numSkipped, 0);
+    Arrays.fill(childPointer, 0);
+    
+    haveSkipped = false;
+    for (int i = 1; i < numberOfSkipLevels; i++) {
+      skipStream[i] = null;
+    }
+  }
+  
+  /** returns x == 0 ? 0 : Math.floor(Math.log(x) / Math.log(base)) */
+  static int log(int x, int base) {
+    assert base >= 2;
+    int ret = 0;
+    long n = base; // needs to be a long to avoid overflow
+    while (x >= n) {
+      n *= base;
+      ret++;
+    }
+    return ret;
+  }
+  
+  /** Loads the skip levels  */
+  private void loadSkipLevels() throws IOException {
+    numberOfSkipLevels = log(docCount, skipInterval[0]);
+    if (numberOfSkipLevels > maxNumberOfSkipLevels) {
+      numberOfSkipLevels = maxNumberOfSkipLevels;
+    }
+
+    skipStream[0].seek(skipPointer[0]);
+    
+    int toBuffer = numberOfLevelsToBuffer;
+    
+    for (int i = numberOfSkipLevels - 1; i > 0; i--) {
+      // the length of the current level
+      long length = skipStream[0].readVLong();
+      
+      // the start pointer of the current level
+      skipPointer[i] = skipStream[0].getFilePointer();
+      if (toBuffer > 0) {
+        // buffer this level
+        skipStream[i] = new SkipBuffer(skipStream[0], (int) length);
+        toBuffer--;
+      } else {
+        // clone this stream, it is already at the start of the current level
+        skipStream[i] = (IndexInput) skipStream[0].clone();
+        if (inputIsBuffered && length < BufferedIndexInput.BUFFER_SIZE) {
+          ((BufferedIndexInput) skipStream[i]).setBufferSize((int) length);
+        }
+        
+        // move base stream beyond the current level
+        skipStream[0].seek(skipStream[0].getFilePointer() + length);
+      }
+    }
+   
+    // use base stream for the lowest level
+    skipPointer[0] = skipStream[0].getFilePointer();
+  }
+  
+  /**
+   * Subclasses must implement the actual skip data encoding in this method.
+   *  
+   * @param level the level skip data shall be read from
+   * @param skipStream the skip stream to read from
+   */  
+  protected abstract int readSkipData(int level, IndexInput skipStream) throws IOException;
+  
+  /** Copies the values of the last read skip entry on this level */
+  protected void setLastSkipData(int level) {
+    lastDoc = skipDoc[level];
+    lastChildPointer = childPointer[level];
+  }
+
+  
+  /** used to buffer the top skip levels */
+  private final static class SkipBuffer extends IndexInput {
+    private byte[] data;
+    private long pointer;
+    private int pos;
+    
+    SkipBuffer(IndexInput input, int length) throws IOException {
+      super("SkipBuffer on " + input);
+      data = new byte[length];
+      pointer = input.getFilePointer();
+      input.readBytes(data, 0, length);
+    }
+    
+    @Override
+    public void close() throws IOException {
+      data = null;
+    }
+
+    @Override
+    public long getFilePointer() {
+      return pointer + pos;
+    }
+
+    @Override
+    public long length() {
+      return data.length;
+    }
+
+    @Override
+    public byte readByte() throws IOException {
+      return data[pos++];
+    }
+
+    @Override
+    public void readBytes(byte[] b, int offset, int len) throws IOException {
+      System.arraycopy(data, pos, b, offset, len);
+      pos += len;
+    }
+
+    @Override
+    public void seek(long pos) throws IOException {
+      this.pos =  (int) (pos - pointer);
+    }
+    
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/MultiLevelSkipListWriter.java b/lucene/src/java/org/apache/lucene/codecs/MultiLevelSkipListWriter.java
new file mode 100644
index 0000000..30c9138
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/MultiLevelSkipListWriter.java
@@ -0,0 +1,153 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+
+/**
+ * This abstract class writes skip lists with multiple levels.
+ * 
+ * Example for skipInterval = 3:
+ *                                                     c            (skip level 2)
+ *                 c                 c                 c            (skip level 1) 
+ *     x     x     x     x     x     x     x     x     x     x      (skip level 0)
+ * d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d  (posting list)
+ *     3     6     9     12    15    18    21    24    27    30     (df)
+ * 
+ * d - document
+ * x - skip data
+ * c - skip data with child pointer
+ * 
+ * Skip level i contains every skipInterval-th entry from skip level i-1.
+ * Therefore the number of entries on level i is: floor(df / ((skipInterval ^ (i + 1))).
+ * 
+ * Each skip entry on a level i>0 contains a pointer to the corresponding skip entry in list i-1.
+ * This guarantees a logarithmic amount of skips to find the target document.
+ * 
+ * While this class takes care of writing the different skip levels,
+ * subclasses must define the actual format of the skip data.
+ * @lucene.experimental
+ */
+
+public abstract class MultiLevelSkipListWriter {
+  // number of levels in this skip list
+  protected int numberOfSkipLevels;
+  
+  // the skip interval in the list with level = 0
+  private int skipInterval;
+  
+  // for every skip level a different buffer is used 
+  private RAMOutputStream[] skipBuffer;
+
+  protected MultiLevelSkipListWriter(int skipInterval, int maxSkipLevels, int df) {
+    this.skipInterval = skipInterval;
+    
+    // calculate the maximum number of skip levels for this document frequency
+    numberOfSkipLevels = MultiLevelSkipListReader.log(df, skipInterval);
+    
+    // make sure it does not exceed maxSkipLevels
+    if (numberOfSkipLevels > maxSkipLevels) {
+      numberOfSkipLevels = maxSkipLevels;
+    }
+  }
+  
+  protected void init() {
+    skipBuffer = new RAMOutputStream[numberOfSkipLevels];
+    for (int i = 0; i < numberOfSkipLevels; i++) {
+      skipBuffer[i] = new RAMOutputStream();
+    }
+  }
+
+  protected void resetSkip() {
+    // creates new buffers or empties the existing ones
+    if (skipBuffer == null) {
+      init();
+    } else {
+      for (int i = 0; i < skipBuffer.length; i++) {
+        skipBuffer[i].reset();
+      }
+    }      
+  }
+
+  /**
+   * Subclasses must implement the actual skip data encoding in this method.
+   *  
+   * @param level the level skip data shall be writing for
+   * @param skipBuffer the skip buffer to write to
+   */
+  protected abstract void writeSkipData(int level, IndexOutput skipBuffer) throws IOException;
+  
+  /**
+   * Writes the current skip data to the buffers. The current document frequency determines
+   * the max level is skip data is to be written to. 
+   * 
+   * @param df the current document frequency 
+   * @throws IOException
+   */
+  public void bufferSkip(int df) throws IOException {
+    int numLevels;
+   
+    // determine max level
+    for (numLevels = 0; (df % skipInterval) == 0 && numLevels < numberOfSkipLevels; df /= skipInterval) {
+      numLevels++;
+    }
+    
+    long childPointer = 0;
+    
+    for (int level = 0; level < numLevels; level++) {
+      writeSkipData(level, skipBuffer[level]);
+      
+      long newChildPointer = skipBuffer[level].getFilePointer();
+      
+      if (level != 0) {
+        // store child pointers for all levels except the lowest
+        skipBuffer[level].writeVLong(childPointer);
+      }
+      
+      //remember the childPointer for the next level
+      childPointer = newChildPointer;
+    }
+  }
+
+  /**
+   * Writes the buffered skip lists to the given output.
+   * 
+   * @param output the IndexOutput the skip lists shall be written to 
+   * @return the pointer the skip list starts
+   */
+  public long writeSkip(IndexOutput output) throws IOException {
+    long skipPointer = output.getFilePointer();
+    //System.out.println("skipper.writeSkip fp=" + skipPointer);
+    if (skipBuffer == null || skipBuffer.length == 0) return skipPointer;
+    
+    for (int level = numberOfSkipLevels - 1; level > 0; level--) {
+      long length = skipBuffer[level].getFilePointer();
+      if (length > 0) {
+        output.writeVLong(length);
+        skipBuffer[level].writeTo(output);
+      }
+    }
+    skipBuffer[0].writeTo(output);
+    
+    return skipPointer;
+  }
+
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/NormsFormat.java b/lucene/src/java/org/apache/lucene/codecs/NormsFormat.java
new file mode 100644
index 0000000..d829e8b
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/NormsFormat.java
@@ -0,0 +1,44 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * format for normalization factors
+ */
+public abstract class NormsFormat {
+  /** Note: separateNormsDir should not be used! */
+  public abstract NormsReader normsReader(Directory dir, SegmentInfo info, FieldInfos fields, IOContext context, Directory separateNormsDir) throws IOException;
+  public abstract NormsWriter normsWriter(SegmentWriteState state) throws IOException;
+  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
+  
+  /** 
+   * Note: this should not be overridden! 
+   * @deprecated 
+   */
+  @Deprecated
+  public void separateFiles(Directory dir, SegmentInfo info, Set<String> files) throws IOException {};
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/NormsReader.java b/lucene/src/java/org/apache/lucene/codecs/NormsReader.java
new file mode 100644
index 0000000..ca741d4
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/NormsReader.java
@@ -0,0 +1,26 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+//simple api just for now before switching to docvalues apis
+public abstract class NormsReader implements Closeable {
+  public abstract byte[] norms(String name) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/NormsWriter.java b/lucene/src/java/org/apache/lucene/codecs/NormsWriter.java
new file mode 100644
index 0000000..17949bc
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/NormsWriter.java
@@ -0,0 +1,70 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.util.Bits;
+
+// simple api just for now before switching to docvalues apis
+public abstract class NormsWriter implements Closeable {
+
+  // TODO: I think IW should set info.normValueType from Similarity,
+  // and then this method just returns DocValuesConsumer
+  public abstract void startField(FieldInfo info) throws IOException;
+  public abstract void writeNorm(byte norm) throws IOException;
+  public abstract void finish(int numDocs) throws IOException;
+  
+  public int merge(MergeState mergeState) throws IOException {
+    int numMergedDocs = 0;
+    for (FieldInfo fi : mergeState.fieldInfos) {
+      if (fi.isIndexed && !fi.omitNorms) {
+        startField(fi);
+        int numMergedDocsForField = 0;
+        for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
+          final int maxDoc = reader.reader.maxDoc();
+          byte normBuffer[] = reader.reader.norms(fi.name);
+          if (normBuffer == null) {
+            // Can be null if this segment doesn't have
+            // any docs with this field
+            normBuffer = new byte[maxDoc];
+            Arrays.fill(normBuffer, (byte)0);
+          }
+          // this segment has deleted docs, so we have to
+          // check for every doc if it is deleted or not
+          final Bits liveDocs = reader.liveDocs;
+          for (int k = 0; k < maxDoc; k++) {
+            if (liveDocs == null || liveDocs.get(k)) {
+              writeNorm(normBuffer[k]);
+              numMergedDocsForField++;
+            }
+          }
+          mergeState.checkAbort.work(maxDoc);
+        }
+        assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;
+        numMergedDocs = numMergedDocsForField;
+      }
+    }
+    finish(numMergedDocs);
+    return numMergedDocs;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/PerDocConsumer.java b/lucene/src/java/org/apache/lucene/codecs/PerDocConsumer.java
new file mode 100644
index 0000000..fd2f2da
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/PerDocConsumer.java
@@ -0,0 +1,59 @@
+package org.apache.lucene.codecs;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.MergeState;
+
+/**
+ * Abstract API that consumes per document values. Concrete implementations of
+ * this convert field values into a Codec specific format during indexing.
+ * <p>
+ * The {@link PerDocConsumer} API is accessible through the
+ * {@link PostingsFormat} - API providing per field consumers and producers for inverted
+ * data (terms, postings) as well as per-document data.
+ * 
+ * @lucene.experimental
+ */
+public abstract class PerDocConsumer implements Closeable{
+  /** Adds a new DocValuesField */
+  public abstract DocValuesConsumer addValuesField(DocValues.Type type, FieldInfo field)
+      throws IOException;
+
+  /**
+   * Consumes and merges the given {@link PerDocProducer} producer
+   * into this consumers format.   
+   */
+  public void merge(MergeState mergeState) throws IOException {
+    final DocValues[] docValues = new DocValues[mergeState.readers.size()];
+
+    for (FieldInfo fieldInfo : mergeState.fieldInfos) {
+      mergeState.fieldInfo = fieldInfo; // set the field we are merging
+      if (fieldInfo.hasDocValues()) {
+        for (int i = 0; i < docValues.length; i++) {
+          docValues[i] = mergeState.readers.get(i).reader.docValues(fieldInfo.name);
+        }
+        final DocValuesConsumer docValuesConsumer = addValuesField(fieldInfo.getDocValuesType(), fieldInfo);
+        assert docValuesConsumer != null;
+        docValuesConsumer.merge(mergeState, docValues);
+      }
+    }
+  }  
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/PerDocProducer.java b/lucene/src/java/org/apache/lucene/codecs/PerDocProducer.java
new file mode 100644
index 0000000..d3b091e
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/PerDocProducer.java
@@ -0,0 +1,48 @@
+package org.apache.lucene.codecs;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.DocValues;
+
+/**
+ * Abstract API that provides access to one or more per-document storage
+ * features. The concrete implementations provide access to the underlying
+ * storage on a per-document basis corresponding to their actual
+ * {@link PerDocConsumer} counterpart.
+ * <p>
+ * The {@link PerDocProducer} API is accessible through the
+ * {@link PostingsFormat} - API providing per field consumers and producers for inverted
+ * data (terms, postings) as well as per-document data.
+ * 
+ * @lucene.experimental
+ */
+public abstract class PerDocProducer implements Closeable {
+  /**
+   * Returns {@link DocValues} for the current field.
+   * 
+   * @param field
+   *          the field name
+   * @return the {@link DocValues} for this field or <code>null</code> if not
+   *         applicable.
+   * @throws IOException
+   */
+  public abstract DocValues docValues(String field) throws IOException;
+
+  public static final PerDocProducer[] EMPTY_ARRAY = new PerDocProducer[0];
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/PostingsBaseFormat.java b/lucene/src/java/org/apache/lucene/codecs/PostingsBaseFormat.java
new file mode 100644
index 0000000..921a0fc
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/PostingsBaseFormat.java
@@ -0,0 +1,55 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.Directory;
+
+/** 
+ * Provides a {@link PostingsReaderBase} and {@link
+ * PostingsWriterBase}.
+ *
+ * @lucene.experimental */
+
+// TODO: find a better name; this defines the API that the
+// terms dict impls use to talk to a postings impl.
+// TermsDict + PostingsReader/WriterBase == PostingsConsumer/Producer
+
+// can we clean this up and do this some other way? 
+// refactor some of these classes and use covariant return?
+public abstract class PostingsBaseFormat {
+
+  /** Unique name that's used to retrieve this codec when
+   *  reading the index */
+  public final String name;
+  
+  protected PostingsBaseFormat(String name) {
+    this.name = name;
+  }
+
+  public abstract PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException;
+
+  public abstract PostingsWriterBase postingsWriterBase(SegmentWriteState state) throws IOException;
+
+  public abstract void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/PostingsConsumer.java b/lucene/src/java/org/apache/lucene/codecs/PostingsConsumer.java
new file mode 100644
index 0000000..c7525c0
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/PostingsConsumer.java
@@ -0,0 +1,118 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+/**
+ * @lucene.experimental
+ */
+
+public abstract class PostingsConsumer {
+
+  /** Adds a new doc in this term.  If this field omits term
+   *  freqs & positions then termDocFreq should be ignored,
+   *  and, finishDoc will not be called. */
+  public abstract void startDoc(int docID, int termDocFreq) throws IOException;
+
+  public static class PostingsMergeState {
+    DocsEnum docsEnum;
+    int[] docMap;
+    int docBase;
+  }
+
+  /** Add a new position & payload.  A null payload means no
+   *  payload; a non-null payload with zero length also
+   *  means no payload.  Caller may reuse the {@link
+   *  BytesRef} for the payload between calls (method must
+   *  fully consume the payload). */
+  public abstract void addPosition(int position, BytesRef payload) throws IOException;
+
+  /** Called when we are done adding positions & payloads
+   *  for each doc.  Not called  when the field omits term
+   *  freq and positions. */
+  public abstract void finishDoc() throws IOException;
+
+  /** Default merge impl: append documents, mapping around
+   *  deletes */
+  public TermStats merge(final MergeState mergeState, final DocsEnum postings, final FixedBitSet visitedDocs) throws IOException {
+
+    int df = 0;
+    long totTF = 0;
+
+    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
+      while(true) {
+        final int doc = postings.nextDoc();
+        if (doc == DocIdSetIterator.NO_MORE_DOCS) {
+          break;
+        }
+        visitedDocs.set(doc);
+        this.startDoc(doc, 0);
+        this.finishDoc();
+        df++;
+      }
+      totTF = -1;
+    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {
+      while(true) {
+        final int doc = postings.nextDoc();
+        if (doc == DocIdSetIterator.NO_MORE_DOCS) {
+          break;
+        }
+        visitedDocs.set(doc);
+        final int freq = postings.freq();
+        this.startDoc(doc, freq);
+        this.finishDoc();
+        df++;
+        totTF += freq;
+      }
+    } else {
+      final DocsAndPositionsEnum postingsEnum = (DocsAndPositionsEnum) postings;
+      while(true) {
+        final int doc = postingsEnum.nextDoc();
+        if (doc == DocIdSetIterator.NO_MORE_DOCS) {
+          break;
+        }
+        visitedDocs.set(doc);
+        final int freq = postingsEnum.freq();
+        this.startDoc(doc, freq);
+        totTF += freq;
+        for(int i=0;i<freq;i++) {
+          final int position = postingsEnum.nextPosition();
+          final BytesRef payload;
+          if (postingsEnum.hasPayload()) {
+            payload = postingsEnum.getPayload();
+          } else {
+            payload = null;
+          }
+          this.addPosition(position, payload);
+        }
+        this.finishDoc();
+        df++;
+      }
+    }
+    return new TermStats(df, totTF);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/PostingsFormat.java b/lucene/src/java/org/apache/lucene/codecs/PostingsFormat.java
new file mode 100644
index 0000000..db328d9
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/PostingsFormat.java
@@ -0,0 +1,84 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.util.NamedSPILoader;
+
+import org.apache.lucene.store.Directory;
+
+/** @lucene.experimental */
+public abstract class PostingsFormat implements NamedSPILoader.NamedSPI {
+
+  private static final NamedSPILoader<PostingsFormat> loader =
+    new NamedSPILoader<PostingsFormat>(PostingsFormat.class);
+
+  public static final PostingsFormat[] EMPTY = new PostingsFormat[0];
+  /** Unique name that's used to retrieve this format when
+   *  reading the index.
+   */
+  private final String name;
+  
+  protected PostingsFormat(String name) {
+    this.name = name;
+  }
+
+  @Override
+  public String getName() {
+    return name;
+  }
+  
+  /** Writes a new segment */
+  public abstract FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException;
+
+  /** Reads a segment.  NOTE: by the time this call
+   *  returns, it must hold open any files it will need to
+   *  use; else, those files may be deleted. */
+  public abstract FieldsProducer fieldsProducer(SegmentReadState state) throws IOException;
+
+  /**
+   * Gathers files associated with this segment
+   * 
+   * @param dir the {@link Directory} this segment was written to
+   * @param segmentInfo the {@link SegmentInfo} for this segment 
+   * @param segmentSuffix the format's suffix within this segment
+   * @param files the of files to add the codec files to.
+   */
+  public abstract void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException;
+
+  @Override
+  public String toString() {
+    return "PostingsFormat(name=" + name + ")";
+  }
+  
+  /** looks up a format by name */
+  public static PostingsFormat forName(String name) {
+    return loader.lookup(name);
+  }
+  
+  /** returns a list of all available format names */
+  public static Set<String> availablePostingsFormats() {
+    return loader.availableServices();
+  }
+  
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/PostingsReaderBase.java b/lucene/src/java/org/apache/lucene/codecs/PostingsReaderBase.java
new file mode 100644
index 0000000..7e040e6
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/PostingsReaderBase.java
@@ -0,0 +1,66 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Closeable;
+
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+
+/** The core terms dictionaries (BlockTermsReader,
+ *  BlockTreeTermsReader) interact with a single instance
+ *  of this class to manage creation of {@link DocsEnum} and
+ *  {@link DocsAndPositionsEnum} instances.  It provides an
+ *  IndexInput (termsIn) where this class may read any
+ *  previously stored data that it had written in its
+ *  corresponding {@link PostingsWriterBase} at indexing
+ *  time. 
+ *  @lucene.experimental */
+
+// TODO: find a better name; this defines the API that the
+// terms dict impls use to talk to a postings impl.
+// TermsDict + PostingsReader/WriterBase == PostingsConsumer/Producer
+public abstract class PostingsReaderBase implements Closeable {
+
+  public abstract void init(IndexInput termsIn) throws IOException;
+
+  /** Return a newly created empty TermState */
+  public abstract BlockTermState newTermState() throws IOException;
+
+  /** Actually decode metadata for next term */
+  public abstract void nextTerm(FieldInfo fieldInfo, BlockTermState state) throws IOException;
+
+  /** Must fully consume state, since after this call that
+   *  TermState may be reused. */
+  public abstract DocsEnum docs(FieldInfo fieldInfo, BlockTermState state, Bits skipDocs, DocsEnum reuse, boolean needsFreqs) throws IOException;
+
+  /** Must fully consume state, since after this call that
+   *  TermState may be reused. */
+  public abstract DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState state, Bits skipDocs, DocsAndPositionsEnum reuse) throws IOException;
+
+  public abstract void close() throws IOException;
+
+  /** Reads data for all terms in the next block; this
+   *  method should merely load the byte[] blob but not
+   *  decode, which is done in {@link #nextTerm}. */
+  public abstract void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState termState) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/PostingsWriterBase.java b/lucene/src/java/org/apache/lucene/codecs/PostingsWriterBase.java
new file mode 100644
index 0000000..d86eb44
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/PostingsWriterBase.java
@@ -0,0 +1,51 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Closeable;
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.index.FieldInfo;
+
+/**
+ * @lucene.experimental
+ */
+
+// TODO: find a better name; this defines the API that the
+// terms dict impls use to talk to a postings impl.
+// TermsDict + PostingsReader/WriterBase == PostingsConsumer/Producer
+public abstract class PostingsWriterBase extends PostingsConsumer implements Closeable {
+
+  public abstract void start(IndexOutput termsOut) throws IOException;
+
+  public abstract void startTerm() throws IOException;
+
+  /** Flush count terms starting at start "backwards", as a
+   *  block. start is a negative offset from the end of the
+   *  terms stack, ie bigger start means further back in
+   *  the stack. */
+  public abstract void flushTermsBlock(int start, int count) throws IOException;
+
+  /** Finishes the current term */
+  public abstract void finishTerm(TermStats stats) throws IOException;
+
+  public abstract void setField(FieldInfo fieldInfo);
+
+  public abstract void close() throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/SegmentInfosFormat.java b/lucene/src/java/org/apache/lucene/codecs/SegmentInfosFormat.java
new file mode 100644
index 0000000..3bbf924
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/SegmentInfosFormat.java
@@ -0,0 +1,37 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Expert: Controls the format of the segments file.
+ * Note, this isn't a per-segment file, if you change the format, other versions
+ * of lucene won't be able to read it, yackedy schmackedy
+ * 
+ * @lucene.experimental
+ */
+// TODO: would be great to handle this situation better.
+// ideally a custom implementation could implement two-phase commit differently,
+// (e.g. atomic rename), and ideally all versions of lucene could still read it.
+// but this is just reflecting reality as it is today...
+//
+// also, perhaps the name should change (to cover all global files like .fnx?)
+// then again, maybe we can just remove that file...
+public abstract class SegmentInfosFormat {
+  public abstract SegmentInfosReader getSegmentInfosReader();
+  public abstract SegmentInfosWriter getSegmentInfosWriter();
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/SegmentInfosReader.java b/lucene/src/java/org/apache/lucene/codecs/SegmentInfosReader.java
new file mode 100644
index 0000000..0aa3e65
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/SegmentInfosReader.java
@@ -0,0 +1,42 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Specifies an API for classes that can read {@link SegmentInfos} information.
+ * @lucene.experimental
+ */
+public abstract class SegmentInfosReader {
+
+  /**
+   * Read {@link SegmentInfos} data from a directory.
+   * @param directory directory to read from
+   * @param segmentsFileName name of the "segments_N" file
+   * @param header input of "segments_N" file after reading preamble
+   * @param infos empty instance to be populated with data
+   * @throws IOException
+   */
+  public abstract void read(Directory directory, String segmentsFileName, ChecksumIndexInput header, SegmentInfos infos, IOContext context) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/SegmentInfosWriter.java b/lucene/src/java/org/apache/lucene/codecs/SegmentInfosWriter.java
new file mode 100644
index 0000000..a00ecf9
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/SegmentInfosWriter.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * Specifies an API for classes that can write out {@link SegmentInfos} data.
+ * @lucene.experimental
+ */
+public abstract class SegmentInfosWriter {
+
+  /**
+   * Write {@link SegmentInfos} data without closing the output. The returned
+   * output will become finished only after a successful completion of
+   * "two phase commit" that first calls {@link #prepareCommit(IndexOutput)} and
+   * then {@link #finishCommit(IndexOutput)}.
+   * @param dir directory to write data to
+   * @param segmentsFileName name of the "segments_N" file to create
+   * @param infos data to write
+   * @return an instance of {@link IndexOutput} to be used in subsequent "two
+   * phase commit" operations as described above.
+   * @throws IOException
+   */
+  public abstract IndexOutput writeInfos(Directory dir, String segmentsFileName, String codecID, SegmentInfos infos, IOContext context) throws IOException;
+  
+  /**
+   * First phase of the two-phase commit - ensure that all output can be
+   * successfully written out.
+   * @param out an instance of {@link IndexOutput} returned from a previous
+   * call to {@link #writeInfos(Directory, String, String, SegmentInfos, IOContext)}.
+   * @throws IOException
+   */
+  public abstract void prepareCommit(IndexOutput out) throws IOException;
+  
+  /**
+   * Second phase of the two-phase commit. In this step the output should be
+   * finalized and closed.
+   * @param out an instance of {@link IndexOutput} returned from a previous
+   * call to {@link #writeInfos(Directory, String, String, SegmentInfos, IOContext)}.
+   * @throws IOException
+   */
+  public abstract void finishCommit(IndexOutput out) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/StoredFieldsFormat.java b/lucene/src/java/org/apache/lucene/codecs/StoredFieldsFormat.java
new file mode 100644
index 0000000..e6ec46d
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/StoredFieldsFormat.java
@@ -0,0 +1,35 @@
+package org.apache.lucene.codecs;
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Controls the format of stored fields
+ */
+public abstract class StoredFieldsFormat {
+  public abstract StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException;
+  public abstract StoredFieldsWriter fieldsWriter(Directory directory, String segment, IOContext context) throws IOException;
+  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/StoredFieldsReader.java b/lucene/src/java/org/apache/lucene/codecs/StoredFieldsReader.java
new file mode 100644
index 0000000..f758bd8
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/StoredFieldsReader.java
@@ -0,0 +1,39 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you may not
+ * use this file except in compliance with the License. You may obtain a copy of
+ * the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.StoredFieldVisitor;
+
+/**
+ * Codec API for reading stored fields:
+ * 
+ * You need to implement {@link #visitDocument(int, StoredFieldVisitor)} to
+ * read the stored fields for a document, implement {@link #clone()} (creating
+ * clones of any IndexInputs used, etc), and {@link #close()}
+ * @lucene.experimental
+ */
+public abstract class StoredFieldsReader implements Cloneable, Closeable {
+  
+  /** Visit the stored fields for document <code>n</code> */
+  public abstract void visitDocument(int n, StoredFieldVisitor visitor) throws CorruptIndexException, IOException;
+
+  public abstract StoredFieldsReader clone();
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java b/lucene/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
new file mode 100644
index 0000000..01a3924
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
@@ -0,0 +1,118 @@
+package org.apache.lucene.codecs;
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.util.Bits;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you may not
+ * use this file except in compliance with the License. You may obtain a copy of
+ * the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+/**
+ * Codec API for writing stored fields:
+ * <p>
+ * <ol>
+ *   <li>For every document, {@link #startDocument(int)} is called,
+ *       informing the Codec how many fields will be written.
+ *   <li>{@link #writeField(FieldInfo, IndexableField)} is called for 
+ *       each field in the document.
+ *   <li>After all documents have been written, {@link #finish(int)} 
+ *       is called for verification/sanity-checks.
+ *   <li>Finally the writer is closed ({@link #close()})
+ * </ol>
+ * 
+ * @lucene.experimental
+ */
+public abstract class StoredFieldsWriter implements Closeable {
+  
+  /** Called before writing the stored fields of the document.
+   *  {@link #writeField(FieldInfo, IndexableField)} will be called
+   *  <code>numStoredFields</code> times. Note that this is
+   *  called even if the document has no stored fields, in
+   *  this case <code>numStoredFields</code> will be zero. */
+  public abstract void startDocument(int numStoredFields) throws IOException;
+  
+  /** Writes a single stored field. */
+  public abstract void writeField(FieldInfo info, IndexableField field) throws IOException;
+
+  /** Aborts writing entirely, implementation should remove
+   *  any partially-written files, etc. */
+  public abstract void abort();
+  
+  /** Called before {@link #close()}, passing in the number
+   *  of documents that were written. Note that this is 
+   *  intentionally redundant (equivalent to the number of
+   *  calls to {@link #startDocument(int)}, but a Codec should
+   *  check that this is the case to detect the JRE bug described 
+   *  in LUCENE-1282. */
+  public abstract void finish(int numDocs) throws IOException;
+  
+  /** Merges in the stored fields from the readers in 
+   *  <code>mergeState</code>. The default implementation skips
+   *  over deleted documents, and uses {@link #startDocument(int)},
+   *  {@link #writeField(FieldInfo, IndexableField)}, and {@link #finish(int)},
+   *  returning the number of documents that were written.
+   *  Implementations can override this method for more sophisticated
+   *  merging (bulk-byte copying, etc). */
+  public int merge(MergeState mergeState) throws IOException {
+    int docCount = 0;
+    for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
+      final int maxDoc = reader.reader.maxDoc();
+      final Bits liveDocs = reader.liveDocs;
+      for (int i = 0; i < maxDoc; i++) {
+        if (liveDocs != null && !liveDocs.get(i)) {
+          // skip deleted docs
+          continue;
+        }
+        // TODO: this could be more efficient using
+        // FieldVisitor instead of loading/writing entire
+        // doc; ie we just have to renumber the field number
+        // on the fly?
+        // NOTE: it's very important to first assign to doc then pass it to
+        // fieldsWriter.addDocument; see LUCENE-1282
+        Document doc = reader.reader.document(i);
+        addDocument(doc, mergeState.fieldInfos);
+        docCount++;
+        mergeState.checkAbort.work(300);
+      }
+    }
+    finish(docCount);
+    return docCount;
+  }
+  
+  /** sugar method for startDocument() + writeField() for every stored field in the document */
+  protected final void addDocument(Iterable<? extends IndexableField> doc, FieldInfos fieldInfos) throws IOException {
+    int storedCount = 0;
+    for (IndexableField field : doc) {
+      if (field.fieldType().stored()) {
+        storedCount++;
+      }
+    }
+    
+    startDocument(storedCount);
+
+    for (IndexableField field : doc) {
+      if (field.fieldType().stored()) {
+        writeField(fieldInfos.fieldInfo(field.name()), field);
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/TermStats.java b/lucene/src/java/org/apache/lucene/codecs/TermStats.java
new file mode 100644
index 0000000..edbd2fd
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/TermStats.java
@@ -0,0 +1,28 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TermStats {
+  public final int docFreq;
+  public final long totalTermFreq;
+
+  public TermStats(int docFreq, long totalTermFreq) {
+    this.docFreq = docFreq;
+    this.totalTermFreq = totalTermFreq;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/TermVectorsFormat.java b/lucene/src/java/org/apache/lucene/codecs/TermVectorsFormat.java
new file mode 100644
index 0000000..4d4801c
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/TermVectorsFormat.java
@@ -0,0 +1,35 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Controls the format of term vectors
+ */
+public abstract class TermVectorsFormat {
+  public abstract TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException;
+  public abstract TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException;
+  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/TermVectorsReader.java b/lucene/src/java/org/apache/lucene/codecs/TermVectorsReader.java
new file mode 100644
index 0000000..742150f
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/TermVectorsReader.java
@@ -0,0 +1,43 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute; // javadocs
+import org.apache.lucene.index.DocsAndPositionsEnum; // javadocs
+import org.apache.lucene.index.Fields;
+
+/**
+ * Codec API for reading term vectors:
+ * 
+ * @lucene.experimental
+ */
+public abstract class TermVectorsReader implements Cloneable,Closeable {
+
+  /** Returns term vectors for this document, or null if
+   *  term vectors were not indexed. If offsets are
+   *  available they are in an {@link OffsetAttribute}
+   *  available from the {@link DocsAndPositionsEnum}. */
+  public abstract Fields get(int doc) throws IOException;
+
+  /** Create a clone that one caller at a time may use to
+   *  read term vectors. */
+  public abstract TermVectorsReader clone();
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/TermVectorsWriter.java b/lucene/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
new file mode 100644
index 0000000..b977a21
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
@@ -0,0 +1,278 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Codec API for writing term vectors:
+ * <p>
+ * <ol>
+ *   <li>For every document, {@link #startDocument(int)} is called,
+ *       informing the Codec how many fields will be written.
+ *   <li>{@link #startField(FieldInfo, int, boolean, boolean)} is called for 
+ *       each field in the document, informing the codec how many terms
+ *       will be written for that field, and whether or not positions
+ *       or offsets are enabled.
+ *   <li>Within each field, {@link #startTerm(BytesRef, int)} is called
+ *       for each term.
+ *   <li>If offsets and/or positions are enabled, then 
+ *       {@link #addPosition(int, int, int)} will be called for each term
+ *       occurrence.
+ *   <li>After all documents have been written, {@link #finish(int)} 
+ *       is called for verification/sanity-checks.
+ *   <li>Finally the writer is closed ({@link #close()})
+ * </ol>
+ * 
+ * @lucene.experimental
+ */
+public abstract class TermVectorsWriter implements Closeable {
+  
+  /** Called before writing the term vectors of the document.
+   *  {@link #startField(FieldInfo, int, boolean, boolean)} will 
+   *  be called <code>numVectorFields</code> times. Note that if term 
+   *  vectors are enabled, this is called even if the document 
+   *  has no vector fields, in this case <code>numVectorFields</code> 
+   *  will be zero. */
+  public abstract void startDocument(int numVectorFields) throws IOException;
+  
+  /** Called before writing the terms of the field.
+   *  {@link #startTerm(BytesRef, int)} will be called <code>numTerms</code> times. */
+  public abstract void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets) throws IOException;
+  
+  /** Adds a term and its term frequency <code>freq</code>.
+   * If this field has positions and/or offsets enabled, then
+   * {@link #addPosition(int, int, int)} will be called 
+   * <code>freq</code> times respectively.
+   */
+  public abstract void startTerm(BytesRef term, int freq) throws IOException;
+  
+  /** Adds a term position and offsets */
+  public abstract void addPosition(int position, int startOffset, int endOffset) throws IOException;
+  
+  /** Aborts writing entirely, implementation should remove
+   *  any partially-written files, etc. */
+  public abstract void abort();
+
+  /** Called before {@link #close()}, passing in the number
+   *  of documents that were written. Note that this is 
+   *  intentionally redundant (equivalent to the number of
+   *  calls to {@link #startDocument(int)}, but a Codec should
+   *  check that this is the case to detect the JRE bug described 
+   *  in LUCENE-1282. */
+  public abstract void finish(int numDocs) throws IOException;
+  
+  /** 
+   * Called by IndexWriter when writing new segments.
+   * <p>
+   * This is an expert API that allows the codec to consume 
+   * positions and offsets directly from the indexer.
+   * <p>
+   * The default implementation calls {@link #addPosition(int, int, int)},
+   * but subclasses can override this if they want to efficiently write 
+   * all the positions, then all the offsets, for example.
+   * <p>
+   * NOTE: This API is extremely expert and subject to change or removal!!!
+   * @lucene.internal
+   */
+  // TODO: we should probably nuke this and make a more efficient 4.x format
+  // PreFlex-RW could then be slow and buffer (its only used in tests...)
+  public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
+    int position = 0;
+    int lastOffset = 0;
+
+    for (int i = 0; i < numProx; i++) {
+      final int startOffset;
+      final int endOffset;
+      
+      if (positions == null) {
+        position = -1;
+      } else {
+        position += positions.readVInt();
+      }
+      
+      if (offsets == null) {
+        startOffset = endOffset = -1;
+      } else {
+        startOffset = lastOffset + offsets.readVInt();
+        endOffset = startOffset + offsets.readVInt();
+        lastOffset = endOffset;
+      }
+      addPosition(position, startOffset, endOffset);
+    }
+  }
+  
+  /** Merges in the term vectors from the readers in 
+   *  <code>mergeState</code>. The default implementation skips
+   *  over deleted documents, and uses {@link #startDocument(int)},
+   *  {@link #startField(FieldInfo, int, boolean, boolean)}, 
+   *  {@link #startTerm(BytesRef, int)}, {@link #addPosition(int, int, int)},
+   *  and {@link #finish(int)},
+   *  returning the number of documents that were written.
+   *  Implementations can override this method for more sophisticated
+   *  merging (bulk-byte copying, etc). */
+  public int merge(MergeState mergeState) throws IOException {
+    int docCount = 0;
+    for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
+      final int maxDoc = reader.reader.maxDoc();
+      final Bits liveDocs = reader.liveDocs;
+      for (int docID = 0; docID < maxDoc; docID++) {
+        if (liveDocs != null && !liveDocs.get(docID)) {
+          // skip deleted docs
+          continue;
+        }
+        // NOTE: it's very important to first assign to vectors then pass it to
+        // termVectorsWriter.addAllDocVectors; see LUCENE-1282
+        Fields vectors = reader.reader.getTermVectors(docID);
+        addAllDocVectors(vectors, mergeState.fieldInfos);
+        docCount++;
+        mergeState.checkAbort.work(300);
+      }
+    }
+    finish(docCount);
+    return docCount;
+  }
+  
+  /** Safe (but, slowish) default method to write every
+   *  vector field in the document.  This default
+   *  implementation requires that the vectors implement
+   *  both Fields.getUniqueFieldCount and
+   *  Terms.getUniqueTermCount. */
+  protected final void addAllDocVectors(Fields vectors, FieldInfos fieldInfos) throws IOException {
+    if (vectors == null) {
+      startDocument(0);
+      return;
+    }
+
+    final int numFields = vectors.getUniqueFieldCount();
+    if (numFields == -1) {
+      throw new IllegalStateException("vectors.getUniqueFieldCount() must be implemented (it returned -1)");
+    }
+    startDocument(numFields);
+    
+    final FieldsEnum fieldsEnum = vectors.iterator();
+    String fieldName;
+    String lastFieldName = null;
+
+    while((fieldName = fieldsEnum.next()) != null) {
+      
+      final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldName);
+
+      assert lastFieldName == null || fieldName.compareTo(lastFieldName) > 0: "lastFieldName=" + lastFieldName + " fieldName=" + fieldName;
+      lastFieldName = fieldName;
+
+      final Terms terms = fieldsEnum.terms();
+      if (terms == null) {
+        // FieldsEnum shouldn't lie...
+        continue;
+      }
+      final int numTerms = (int) terms.getUniqueTermCount();
+      if (numTerms == -1) {
+        throw new IllegalStateException("vector.getUniqueTermCount() must be implemented (it returned -1)");
+      }
+
+      final boolean positions;
+
+      OffsetAttribute offsetAtt;
+
+      final TermsEnum termsEnum = terms.iterator(null);
+
+      DocsAndPositionsEnum docsAndPositionsEnum = null;
+
+      if (termsEnum.next() != null) {
+        assert numTerms > 0;
+        docsAndPositionsEnum = termsEnum.docsAndPositions(null, null);
+        if (docsAndPositionsEnum != null) {
+          // has positions
+          positions = true;
+          if (docsAndPositionsEnum.attributes().hasAttribute(OffsetAttribute.class)) {
+            offsetAtt = docsAndPositionsEnum.attributes().getAttribute(OffsetAttribute.class);
+          } else {
+            offsetAtt = null;
+          }
+        } else {
+          positions = false;
+          offsetAtt = null;
+        }
+      } else {
+        // no terms in this field (hmm why is field present
+        // then...?)
+        assert numTerms == 0;
+        positions = false;
+        offsetAtt = null;
+      }
+      
+      startField(fieldInfo, numTerms, positions, offsetAtt != null);
+
+      int termCount = 1;
+
+      // NOTE: we already .next()'d the TermsEnum above, to
+      // peek @ first term to see if positions/offsets are
+      // present
+      while(true) {
+        final int freq = (int) termsEnum.totalTermFreq();
+        startTerm(termsEnum.term(), freq);
+
+        if (positions || offsetAtt != null) {
+          DocsAndPositionsEnum dp = termsEnum.docsAndPositions(null, docsAndPositionsEnum);
+          // TODO: add startOffset()/endOffset() to d&pEnum... this is insanity
+          if (dp != docsAndPositionsEnum) {
+            // producer didnt reuse, must re-pull attributes
+            if (offsetAtt != null) {
+              assert dp.attributes().hasAttribute(OffsetAttribute.class);
+              offsetAtt = dp.attributes().getAttribute(OffsetAttribute.class);
+            }
+          }
+          docsAndPositionsEnum = dp;
+          final int docID = docsAndPositionsEnum.nextDoc();
+          assert docID != DocsEnum.NO_MORE_DOCS;
+          assert docsAndPositionsEnum.freq() == freq;
+
+          for(int posUpto=0; posUpto<freq; posUpto++) {
+            final int pos = docsAndPositionsEnum.nextPosition();
+            final int startOffset = offsetAtt == null ? -1 : offsetAtt.startOffset();
+            final int endOffset = offsetAtt == null ? -1 : offsetAtt.endOffset();
+            
+            addPosition(pos, startOffset, endOffset);
+          }
+        }
+        
+        if (termsEnum.next() == null) {
+          assert termCount == numTerms;
+          break;
+        }
+        termCount++;
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/TermsConsumer.java b/lucene/src/java/org/apache/lucene/codecs/TermsConsumer.java
new file mode 100644
index 0000000..78ffa11
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/TermsConsumer.java
@@ -0,0 +1,160 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.MultiDocsEnum;
+import org.apache.lucene.index.MultiDocsAndPositionsEnum;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+/**
+ * @lucene.experimental
+ */
+
+public abstract class TermsConsumer {
+
+  /** Starts a new term in this field; this may be called
+   *  with no corresponding call to finish if the term had
+   *  no docs. */
+  public abstract PostingsConsumer startTerm(BytesRef text) throws IOException;
+
+  /** Finishes the current term; numDocs must be > 0. */
+  public abstract void finishTerm(BytesRef text, TermStats stats) throws IOException;
+
+  /** Called when we are done adding terms to this field */
+  public abstract void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException;
+
+  /** Return the BytesRef Comparator used to sort terms
+   *  before feeding to this API. */
+  public abstract Comparator<BytesRef> getComparator() throws IOException;
+
+  /** Default merge impl */
+  private MappingMultiDocsEnum docsEnum;
+  private MappingMultiDocsEnum docsAndFreqsEnum;
+  private MappingMultiDocsAndPositionsEnum postingsEnum;
+
+  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {
+
+    BytesRef term;
+    assert termsEnum != null;
+    long sumTotalTermFreq = 0;
+    long sumDocFreq = 0;
+    long sumDFsinceLastAbortCheck = 0;
+    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);
+
+    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
+      if (docsEnum == null) {
+        docsEnum = new MappingMultiDocsEnum();
+      }
+      docsEnum.setMergeState(mergeState);
+
+      MultiDocsEnum docsEnumIn = null;
+
+      while((term = termsEnum.next()) != null) {
+        // We can pass null for liveDocs, because the
+        // mapping enum will skip the non-live docs:
+        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);
+        if (docsEnumIn != null) {
+          docsEnum.reset(docsEnumIn);
+          final PostingsConsumer postingsConsumer = startTerm(term);
+          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);
+          if (stats.docFreq > 0) {
+            finishTerm(term, stats);
+            sumTotalTermFreq += stats.docFreq;
+            sumDFsinceLastAbortCheck += stats.docFreq;
+            sumDocFreq += stats.docFreq;
+            if (sumDFsinceLastAbortCheck > 60000) {
+              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);
+              sumDFsinceLastAbortCheck = 0;
+            }
+          }
+        }
+      }
+    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {
+      if (docsAndFreqsEnum == null) {
+        docsAndFreqsEnum = new MappingMultiDocsEnum();
+      }
+      docsAndFreqsEnum.setMergeState(mergeState);
+
+      MultiDocsEnum docsAndFreqsEnumIn = null;
+
+      while((term = termsEnum.next()) != null) {
+        // We can pass null for liveDocs, because the
+        // mapping enum will skip the non-live docs:
+        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);
+        assert docsAndFreqsEnumIn != null;
+        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);
+        final PostingsConsumer postingsConsumer = startTerm(term);
+        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);
+        if (stats.docFreq > 0) {
+          finishTerm(term, stats);
+          sumTotalTermFreq += stats.totalTermFreq;
+          sumDFsinceLastAbortCheck += stats.docFreq;
+          sumDocFreq += stats.docFreq;
+          if (sumDFsinceLastAbortCheck > 60000) {
+            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);
+            sumDFsinceLastAbortCheck = 0;
+          }
+        }
+      }
+    } else {
+      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+      if (postingsEnum == null) {
+        postingsEnum = new MappingMultiDocsAndPositionsEnum();
+      }
+      postingsEnum.setMergeState(mergeState);
+      MultiDocsAndPositionsEnum postingsEnumIn = null;
+      while((term = termsEnum.next()) != null) {
+        // We can pass null for liveDocs, because the
+        // mapping enum will skip the non-live docs:
+        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);
+        assert postingsEnumIn != null;
+        postingsEnum.reset(postingsEnumIn);
+        // set PayloadProcessor
+        if (mergeState.payloadProcessorProvider != null) {
+          for (int i = 0; i < mergeState.readers.size(); i++) {
+            if (mergeState.dirPayloadProcessor[i] != null) {
+              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);
+            }
+          }
+        }
+        final PostingsConsumer postingsConsumer = startTerm(term);
+        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);
+        if (stats.docFreq > 0) {
+          finishTerm(term, stats);
+          sumTotalTermFreq += stats.totalTermFreq;
+          sumDFsinceLastAbortCheck += stats.docFreq;
+          sumDocFreq += stats.docFreq;
+          if (sumDFsinceLastAbortCheck > 60000) {
+            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);
+            sumDFsinceLastAbortCheck = 0;
+          }
+        }
+      }
+    }
+
+    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java b/lucene/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java
new file mode 100644
index 0000000..bbfbb1b
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java
@@ -0,0 +1,71 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.util.BytesRef;
+
+import java.io.IOException;
+import java.io.Closeable;
+import java.util.Collection;
+
+
+// TODO
+//   - allow for non-regular index intervals?  eg with a
+//     long string of rare terms, you don't need such
+//     frequent indexing
+
+/**
+ * TermsDictReader interacts with an instance of this class
+ * to manage its terms index.  The writer must accept
+ * indexed terms (many pairs of CharSequence text + long
+ * fileOffset), and then this reader must be able to
+ * retrieve the nearest index term to a provided term
+ * text. 
+ * @lucene.experimental */
+
+public abstract class TermsIndexReaderBase implements Closeable {
+
+  public abstract FieldIndexEnum getFieldEnum(FieldInfo fieldInfo);
+
+  public abstract void close() throws IOException;
+
+  public abstract boolean supportsOrd();
+
+  public abstract int getDivisor();
+
+  // Similar to TermsEnum, except, the only "metadata" it
+  // reports for a given indexed term is the long fileOffset
+  // into the main terms dict (_X.tis) file:
+  public static abstract class FieldIndexEnum {
+
+    /** Seeks to "largest" indexed term that's <=
+     *  term; retruns file pointer index (into the main
+     *  terms index file) for that term */
+    public abstract long seek(BytesRef term) throws IOException;
+
+    /** Returns -1 at end */
+    public abstract long next() throws IOException;
+
+    public abstract BytesRef term();
+
+    // Only impl'd if supportsOrd() returns true!
+    public abstract long seek(long ord) throws IOException;
+    public abstract long ord();
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java b/lucene/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java
new file mode 100644
index 0000000..bc98b9c
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java
@@ -0,0 +1,36 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.util.BytesRef;
+
+import java.io.Closeable;
+import java.io.IOException;
+
+/** @lucene.experimental */
+public abstract class TermsIndexWriterBase implements Closeable {
+
+  public abstract class FieldWriter {
+    public abstract boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException;
+    public abstract void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException;
+    public abstract void finish(long termsFilePointer) throws IOException;
+  }
+
+  public abstract FieldWriter addField(FieldInfo fieldInfo, long termsFilePointer) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java b/lucene/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java
new file mode 100644
index 0000000..2c0e72a
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java
@@ -0,0 +1,232 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.FileOutputStream;   // for toDot
+import java.io.OutputStreamWriter; // for toDot
+import java.io.Writer;             // for toDot
+import java.util.Collection;
+import java.util.HashMap;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util; // for toDot
+
+/** See {@link VariableGapTermsIndexWriter}
+ * 
+ * @lucene.experimental */
+public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
+
+  private final PositiveIntOutputs fstOutputs = PositiveIntOutputs.getSingleton(true);
+  private int indexDivisor;
+
+  // Closed if indexLoaded is true:
+  private IndexInput in;
+  private volatile boolean indexLoaded;
+
+  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
+  
+  // start of the field info data
+  protected long dirOffset;
+
+  final String segment;
+  public VariableGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, String segmentSuffix, IOContext context)
+    throws IOException {
+    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION), new IOContext(context, true));
+    this.segment = segment;
+    boolean success = false;
+    assert indexDivisor == -1 || indexDivisor > 0;
+
+    try {
+      
+      readHeader(in);
+      this.indexDivisor = indexDivisor;
+
+      seekDir(in, dirOffset);
+
+      // Read directory
+      final int numFields = in.readVInt();
+
+      for(int i=0;i<numFields;i++) {
+        final int field = in.readVInt();
+        final long indexStart = in.readVLong();
+        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        fields.put(fieldInfo, new FieldIndexData(fieldInfo, indexStart));
+      }
+      success = true;
+    } finally {
+      if (indexDivisor > 0) {
+        in.close();
+        in = null;
+        if (success) {
+          indexLoaded = true;
+        }
+      }
+    }
+  }
+
+  @Override
+  public int getDivisor() {
+    return indexDivisor;
+  }
+  
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, VariableGapTermsIndexWriter.CODEC_NAME,
+      VariableGapTermsIndexWriter.VERSION_START, VariableGapTermsIndexWriter.VERSION_START);
+    dirOffset = input.readLong();
+  }
+
+  private static class IndexEnum extends FieldIndexEnum {
+    private final BytesRefFSTEnum<Long> fstEnum;
+    private BytesRefFSTEnum.InputOutput<Long> current;
+
+    public IndexEnum(FST<Long> fst) {
+      fstEnum = new BytesRefFSTEnum<Long>(fst);
+    }
+
+    @Override
+    public BytesRef term() {
+      if (current == null) {
+        return null;
+      } else {
+        return current.input;
+      }
+    }
+
+    @Override
+    public long seek(BytesRef target) throws IOException {
+      //System.out.println("VGR: seek field=" + fieldInfo.name + " target=" + target);
+      current = fstEnum.seekFloor(target);
+      //System.out.println("  got input=" + current.input + " output=" + current.output);
+      return current.output;
+    }
+
+    @Override
+    public long next() throws IOException {
+      //System.out.println("VGR: next field=" + fieldInfo.name);
+      current = fstEnum.next();
+      if (current == null) {
+        //System.out.println("  eof");
+        return -1;
+      } else {
+        return current.output;
+      }
+    }
+
+    @Override
+    public long ord() {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public long seek(long ord) {
+      throw new UnsupportedOperationException();
+    }
+  }
+
+  @Override
+  public boolean supportsOrd() {
+    return false;
+  }
+
+  private final class FieldIndexData {
+
+    private final long indexStart;
+    // Set only if terms index is loaded:
+    private volatile FST<Long> fst;
+
+    public FieldIndexData(FieldInfo fieldInfo, long indexStart) throws IOException {
+      this.indexStart = indexStart;
+
+      if (indexDivisor > 0) {
+        loadTermsIndex();
+      }
+    }
+
+    private void loadTermsIndex() throws IOException {
+      if (fst == null) {
+        IndexInput clone = (IndexInput) in.clone();
+        clone.seek(indexStart);
+        fst = new FST<Long>(clone, fstOutputs);
+        clone.close();
+
+        /*
+        final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
+        Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
+        Util.toDot(fst, w, false, false);
+        System.out.println("FST INDEX: SAVED to " + dotFileName);
+        w.close();
+        */
+
+        if (indexDivisor > 1) {
+          // subsample
+          final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton(true);
+          final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
+          final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst);
+          BytesRefFSTEnum.InputOutput<Long> result;
+          int count = indexDivisor;
+          while((result = fstEnum.next()) != null) {
+            if (count == indexDivisor) {
+              builder.add(result.input, result.output);
+              count = 0;
+            }
+            count++;
+          }
+          fst = builder.finish();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
+    final FieldIndexData fieldData = fields.get(fieldInfo);
+    if (fieldData.fst == null) {
+      return null;
+    } else {
+      return new IndexEnum(fieldData.fst);
+    }
+  }
+
+  public static void files(Directory dir, SegmentInfo info, String segmentSuffix, Collection<String> files) {
+    files.add(IndexFileNames.segmentFileName(info.name, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION));
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (in != null && !indexLoaded) {
+      in.close();
+    }
+  }
+
+  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
+    input.seek(dirOffset);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java b/lucene/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java
new file mode 100644
index 0000000..40df569
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java
@@ -0,0 +1,306 @@
+package org.apache.lucene.codecs;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+
+/**
+ * Selects index terms according to provided pluggable
+ * IndexTermPolicy, and stores them in a prefix trie that's
+ * loaded entirely in RAM stored as an FST.  This terms
+ * index only supports unsigned byte term sort order
+ * (unicode codepoint order when the bytes are UTF8).
+ *
+ * @lucene.experimental */
+public class VariableGapTermsIndexWriter extends TermsIndexWriterBase {
+  protected final IndexOutput out;
+
+  /** Extension of terms index file */
+  static final String TERMS_INDEX_EXTENSION = "tiv";
+
+  final static String CODEC_NAME = "VARIABLE_GAP_TERMS_INDEX";
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  private final List<FSTFieldWriter> fields = new ArrayList<FSTFieldWriter>();
+  private final FieldInfos fieldInfos; // unread
+  private final IndexTermSelector policy;
+
+  /** @lucene.experimental */
+  public static abstract class IndexTermSelector {
+    // Called sequentially on every term being written,
+    // returning true if this term should be indexed
+    public abstract boolean isIndexTerm(BytesRef term, TermStats stats);
+    public abstract void newField(FieldInfo fieldInfo);
+  }
+
+  /** Same policy as {@link FixedGapTermsIndexWriter} */
+  public static final class EveryNTermSelector extends IndexTermSelector {
+    private int count;
+    private final int interval;
+
+    public EveryNTermSelector(int interval) {
+      this.interval = interval;
+      // First term is first indexed term:
+      count = interval;
+    }
+
+    @Override
+    public boolean isIndexTerm(BytesRef term, TermStats stats) {
+      if (count >= interval) {
+        count = 1;
+        return true;
+      } else {
+        count++;
+        return false;
+      }
+    }
+
+    @Override
+    public void newField(FieldInfo fieldInfo) {
+      count = interval;
+    }
+  }
+
+  /** Sets an index term when docFreq >= docFreqThresh, or
+   *  every interval terms.  This should reduce seek time
+   *  to high docFreq terms.  */
+  public static final class EveryNOrDocFreqTermSelector extends IndexTermSelector {
+    private int count;
+    private final int docFreqThresh;
+    private final int interval;
+
+    public EveryNOrDocFreqTermSelector(int docFreqThresh, int interval) {
+      this.interval = interval;
+      this.docFreqThresh = docFreqThresh;
+
+      // First term is first indexed term:
+      count = interval;
+    }
+
+    @Override
+    public boolean isIndexTerm(BytesRef term, TermStats stats) {
+      if (stats.docFreq >= docFreqThresh || count >= interval) {
+        count = 1;
+        return true;
+      } else {
+        count++;
+        return false;
+      }
+    }
+
+    @Override
+    public void newField(FieldInfo fieldInfo) {
+      count = interval;
+    }
+  }
+
+  // TODO: it'd be nice to let the FST builder prune based
+  // on term count of each node (the prune1/prune2 that it
+  // accepts), and build the index based on that.  This
+  // should result in a more compact terms index, more like
+  // a prefix trie than the other selectors, because it
+  // only stores enough leading bytes to get down to N
+  // terms that may complete that prefix.  It becomes
+  // "deeper" when terms are dense, and "shallow" when they
+  // are less dense.
+  //
+  // However, it's not easy to make that work this this
+  // API, because that pruning doesn't immediately know on
+  // seeing each term whether that term will be a seek point
+  // or not.  It requires some non-causality in the API, ie
+  // only on seeing some number of future terms will the
+  // builder decide which past terms are seek points.
+  // Somehow the API'd need to be able to return a "I don't
+  // know" value, eg like a Future, which only later on is
+  // flipped (frozen) to true or false.
+  //
+  // We could solve this with a 2-pass approach, where the
+  // first pass would build an FSA (no outputs) solely to
+  // determine which prefixes are the 'leaves' in the
+  // pruning. The 2nd pass would then look at this prefix
+  // trie to mark the seek points and build the FST mapping
+  // to the true output.
+  //
+  // But, one downside to this approach is that it'd result
+  // in uneven index term selection.  EG with prune1=10, the
+  // resulting index terms could be as frequent as every 10
+  // terms or as rare as every <maxArcCount> * 10 (eg 2560),
+  // in the extremes.
+
+  public VariableGapTermsIndexWriter(SegmentWriteState state, IndexTermSelector policy) throws IOException {
+    final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_INDEX_EXTENSION);
+    out = state.directory.createOutput(indexFileName, state.context);
+    boolean success = false;
+    try {
+      fieldInfos = state.fieldInfos;
+      this.policy = policy;
+      writeHeader(out);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
+    // Placeholder for dir offset
+    out.writeLong(0);
+  }
+
+  @Override
+  public FieldWriter addField(FieldInfo field, long termsFilePointer) throws IOException {
+    ////System.out.println("VGW: field=" + field.name);
+    policy.newField(field);
+    FSTFieldWriter writer = new FSTFieldWriter(field, termsFilePointer);
+    fields.add(writer);
+    return writer;
+  }
+
+  /** NOTE: if your codec does not sort in unicode code
+   *  point order, you must override this method, to simply
+   *  return indexedTerm.length. */
+  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
+    // As long as codec sorts terms in unicode codepoint
+    // order, we can safely strip off the non-distinguishing
+    // suffix to save RAM in the loaded terms index.
+    final int idxTermOffset = indexedTerm.offset;
+    final int priorTermOffset = priorTerm.offset;
+    final int limit = Math.min(priorTerm.length, indexedTerm.length);
+    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
+      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
+        return byteIdx+1;
+      }
+    }
+    return Math.min(1+priorTerm.length, indexedTerm.length);
+  }
+
+  private class FSTFieldWriter extends FieldWriter {
+    private final Builder<Long> fstBuilder;
+    private final PositiveIntOutputs fstOutputs;
+    private final long startTermsFilePointer;
+
+    final FieldInfo fieldInfo;
+    int numIndexTerms;
+    FST<Long> fst;
+    final long indexStart;
+
+    private final BytesRef lastTerm = new BytesRef();
+    private boolean first = true;
+
+    public FSTFieldWriter(FieldInfo fieldInfo, long termsFilePointer) throws IOException {
+      this.fieldInfo = fieldInfo;
+      fstOutputs = PositiveIntOutputs.getSingleton(true);
+      fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, fstOutputs);
+      indexStart = out.getFilePointer();
+      ////System.out.println("VGW: field=" + fieldInfo.name);
+
+      // Always put empty string in
+      fstBuilder.add(new BytesRef(), fstOutputs.get(termsFilePointer));
+      startTermsFilePointer = termsFilePointer;
+    }
+
+    @Override
+    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
+      //System.out.println("VGW: index term=" + text.utf8ToString());
+      // NOTE: we must force the first term per field to be
+      // indexed, in case policy doesn't:
+      if (policy.isIndexTerm(text, stats) || first) {
+        first = false;
+        //System.out.println("  YES");
+        return true;
+      } else {
+        lastTerm.copyBytes(text);
+        return false;
+      }
+    }
+
+    @Override
+    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
+      if (text.length == 0) {
+        // We already added empty string in ctor
+        assert termsFilePointer == startTermsFilePointer;
+        return;
+      }
+      final int lengthSave = text.length;
+      text.length = indexedTermPrefixLength(lastTerm, text);
+      try {
+        fstBuilder.add(text, fstOutputs.get(termsFilePointer));
+      } finally {
+        text.length = lengthSave;
+      }
+      lastTerm.copyBytes(text);
+    }
+
+    @Override
+    public void finish(long termsFilePointer) throws IOException {
+      fst = fstBuilder.finish();
+      if (fst != null) {
+        fst.save(out);
+      }
+    }
+  }
+
+  public void close() throws IOException {
+    try {
+    final long dirStart = out.getFilePointer();
+    final int fieldCount = fields.size();
+
+    int nonNullFieldCount = 0;
+    for(int i=0;i<fieldCount;i++) {
+      FSTFieldWriter field = fields.get(i);
+      if (field.fst != null) {
+        nonNullFieldCount++;
+      }
+    }
+
+    out.writeVInt(nonNullFieldCount);
+    for(int i=0;i<fieldCount;i++) {
+      FSTFieldWriter field = fields.get(i);
+      if (field.fst != null) {
+        out.writeVInt(field.fieldInfo.number);
+        out.writeVLong(field.indexStart);
+      }
+    }
+    writeTrailer(dirStart);
+    } finally {
+    out.close();
+  }
+  }
+
+  protected void writeTrailer(long dirStart) throws IOException {
+    out.seek(CodecUtil.headerLength(CODEC_NAME));
+    out.writeLong(dirStart);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java
new file mode 100644
index 0000000..d5fd741
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java
@@ -0,0 +1,88 @@
+package org.apache.lucene.codecs.appending;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40Codec;
+import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
+
+/**
+ * This codec extends {@link Lucene40Codec} to work on append-only outputs, such
+ * as plain output streams and append-only filesystems.
+ * 
+ * @lucene.experimental
+ */
+public class AppendingCodec extends Codec {
+  public AppendingCodec() {
+    super("Appending");
+  }
+
+  private final PostingsFormat postings = new AppendingPostingsFormat();
+  private final SegmentInfosFormat infos = new AppendingSegmentInfosFormat();
+  private final StoredFieldsFormat fields = new Lucene40StoredFieldsFormat();
+  private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat();
+  private final TermVectorsFormat vectors = new Lucene40TermVectorsFormat();
+  private final DocValuesFormat docValues = new Lucene40DocValuesFormat();
+  private final NormsFormat norms = new Lucene40NormsFormat();
+  
+  @Override
+  public PostingsFormat postingsFormat() {
+    return postings;
+  }
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return fields;
+  }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectors;
+  }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValues;
+  }
+
+  @Override
+  public SegmentInfosFormat segmentInfosFormat() {
+    return infos;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfos;
+  }
+  
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java
new file mode 100644
index 0000000..4664a9b
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java
@@ -0,0 +1,91 @@
+package org.apache.lucene.codecs.appending;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.BlockTreeTermsReader;
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsReader;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.Directory;
+
+/**
+ * Appending postings impl
+ */
+class AppendingPostingsFormat extends PostingsFormat {
+  public static String CODEC_NAME = "Appending";
+  
+  public AppendingPostingsFormat() {
+    super(CODEC_NAME);
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase docsWriter = new Lucene40PostingsWriter(state);
+    boolean success = false;
+    try {
+      FieldsConsumer ret = new AppendingTermsWriter(state, docsWriter, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        docsWriter.close();
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+    
+    boolean success = false;
+    try {
+      FieldsProducer ret = new AppendingTermsReader(
+                                                    state.dir,
+                                                    state.fieldInfos,
+                                                    state.segmentInfo.name,
+                                                    postings,
+                                                    state.context,
+                                                    state.segmentSuffix,
+                                                    state.termsIndexDivisor);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        postings.close();
+      }
+    }
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files)
+          throws IOException {
+    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
+    BlockTreeTermsReader.files(dir, segmentInfo, segmentSuffix, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosFormat.java b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosFormat.java
new file mode 100644
index 0000000..daf398c
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosFormat.java
@@ -0,0 +1,30 @@
+package org.apache.lucene.codecs.appending;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.SegmentInfosWriter;
+import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfosFormat;
+
+public class AppendingSegmentInfosFormat extends Lucene40SegmentInfosFormat {
+  private final SegmentInfosWriter writer = new AppendingSegmentInfosWriter();
+
+  @Override
+  public SegmentInfosWriter getSegmentInfosWriter() {
+    return writer;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosWriter.java b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosWriter.java
new file mode 100644
index 0000000..bd69ac8
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingSegmentInfosWriter.java
@@ -0,0 +1,32 @@
+package org.apache.lucene.codecs.appending;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfosWriter;
+import org.apache.lucene.store.IndexOutput;
+
+public class AppendingSegmentInfosWriter extends Lucene40SegmentInfosWriter {
+
+  @Override
+  public void prepareCommit(IndexOutput segmentOutput) throws IOException {
+    // noop
+  }
+
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/appending/AppendingTermsReader.java b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingTermsReader.java
new file mode 100644
index 0000000..facd143
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingTermsReader.java
@@ -0,0 +1,61 @@
+package org.apache.lucene.codecs.appending;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTreeTermsReader;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.CodecUtil;
+
+/**
+ * Reads append-only terms from {@link AppendingTermsWriter}
+ * @lucene.experimental
+ */
+public class AppendingTermsReader extends BlockTreeTermsReader {
+
+  public AppendingTermsReader(Directory dir, FieldInfos fieldInfos, String segment, PostingsReaderBase postingsReader, 
+      IOContext ioContext, String segmentSuffix, int indexDivisor) throws IOException {
+    super(dir, fieldInfos, segment, postingsReader, ioContext, segmentSuffix, indexDivisor);
+  }
+
+  @Override
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, AppendingTermsWriter.TERMS_CODEC_NAME,
+        AppendingTermsWriter.TERMS_VERSION_START,
+        AppendingTermsWriter.TERMS_VERSION_CURRENT);  
+  }
+
+  @Override
+  protected void readIndexHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, AppendingTermsWriter.TERMS_INDEX_CODEC_NAME,
+        AppendingTermsWriter.TERMS_INDEX_VERSION_START,
+        AppendingTermsWriter.TERMS_INDEX_VERSION_CURRENT);
+  }
+  
+  @Override
+  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
+    input.seek(input.length() - Long.SIZE / 8);
+    long offset = input.readLong();
+    input.seek(offset);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/appending/AppendingTermsWriter.java b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingTermsWriter.java
new file mode 100644
index 0000000..8a47e02
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/appending/AppendingTermsWriter.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.codecs.appending;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.CodecUtil;
+
+/**
+ * Append-only version of {@link BlockTreeTermsWriter}
+ * @lucene.experimental
+ */
+public class AppendingTermsWriter extends BlockTreeTermsWriter {
+  final static String TERMS_CODEC_NAME = "APPENDING_TERMS_DICT";
+  final static int TERMS_VERSION_START = 0;
+  final static int TERMS_VERSION_CURRENT = TERMS_VERSION_START;
+  
+  final static String TERMS_INDEX_CODEC_NAME = "APPENDING_TERMS_INDEX";
+  final static int TERMS_INDEX_VERSION_START = 0;
+  final static int TERMS_INDEX_VERSION_CURRENT = TERMS_INDEX_VERSION_START;
+  
+  public AppendingTermsWriter(SegmentWriteState state, PostingsWriterBase postingsWriter, int minItemsInBlock, int maxItemsInBlock) throws IOException {
+    super(state, postingsWriter, minItemsInBlock, maxItemsInBlock);
+  }
+
+  @Override
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, TERMS_CODEC_NAME, TERMS_VERSION_CURRENT);
+  }
+
+  @Override
+  protected void writeIndexHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, TERMS_INDEX_CODEC_NAME, TERMS_INDEX_VERSION_CURRENT);
+  }
+
+  @Override
+  protected void writeTrailer(IndexOutput out, long dirStart) throws IOException {
+    out.writeLong(dirStart);
+  }
+
+  @Override
+  protected void writeIndexTrailer(IndexOutput indexOut, long dirStart) throws IOException {
+    indexOut.writeLong(dirStart);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexInput.java b/lucene/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexInput.java
new file mode 100644
index 0000000..de3c3fc
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexInput.java
@@ -0,0 +1,196 @@
+package org.apache.lucene.codecs.intblock;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Naive int block API that writes vInts.  This is
+ *  expected to give poor performance; it's really only for
+ *  testing the pluggability.  One should typically use pfor instead. */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.sep.IntIndexInput;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.IntsRef;
+
+/** Abstract base class that reads fixed-size blocks of ints
+ *  from an IndexInput.  While this is a simple approach, a
+ *  more performant approach would directly create an impl
+ *  of IntIndexInput inside Directory.  Wrapping a generic
+ *  IndexInput will likely cost performance.
+ *
+ * @lucene.experimental
+ */
+public abstract class FixedIntBlockIndexInput extends IntIndexInput {
+
+  private final IndexInput in;
+  protected final int blockSize;
+  
+  public FixedIntBlockIndexInput(final IndexInput in) throws IOException {
+    this.in = in;
+    blockSize = in.readVInt();
+  }
+
+  @Override
+  public Reader reader() throws IOException {
+    final int[] buffer = new int[blockSize];
+    final IndexInput clone = (IndexInput) in.clone();
+    // TODO: can this be simplified?
+    return new Reader(clone, buffer, this.getBlockReader(clone, buffer));
+  }
+
+  @Override
+  public void close() throws IOException {
+    in.close();
+  }
+
+  @Override
+  public Index index() {
+    return new Index();
+  }
+
+  protected abstract BlockReader getBlockReader(IndexInput in, int[] buffer) throws IOException;
+
+  public interface BlockReader {
+    public void readBlock() throws IOException;
+  }
+
+  private static class Reader extends IntIndexInput.Reader {
+    private final IndexInput in;
+
+    protected final int[] pending;
+    int upto;
+
+    private boolean seekPending;
+    private long pendingFP;
+    private int pendingUpto;
+    private long lastBlockFP;
+    private final BlockReader blockReader;
+    private final int blockSize;
+    private final IntsRef bulkResult = new IntsRef();
+
+    public Reader(final IndexInput in, final int[] pending, final BlockReader blockReader)
+    throws IOException {
+      this.in = in;
+      this.pending = pending;
+      this.blockSize = pending.length;
+      bulkResult.ints = pending;
+      this.blockReader = blockReader;
+      upto = blockSize;
+    }
+
+    void seek(final long fp, final int upto) {
+      pendingFP = fp;
+      pendingUpto = upto;
+      seekPending = true;
+    }
+
+    private void maybeSeek() throws IOException {
+      if (seekPending) {
+        if (pendingFP != lastBlockFP) {
+          // need new block
+          in.seek(pendingFP);
+          lastBlockFP = pendingFP;
+          blockReader.readBlock();
+        }
+        upto = pendingUpto;
+        seekPending = false;
+      }
+    }
+
+    @Override
+    public int next() throws IOException {
+      this.maybeSeek();
+      if (upto == blockSize) {
+        lastBlockFP = in.getFilePointer();
+        blockReader.readBlock();
+        upto = 0;
+      }
+
+      return pending[upto++];
+    }
+
+    @Override
+    public IntsRef read(final int count) throws IOException {
+      this.maybeSeek();
+      if (upto == blockSize) {
+        blockReader.readBlock();
+        upto = 0;
+      }
+      bulkResult.offset = upto;
+      if (upto + count < blockSize) {
+        bulkResult.length = count;
+        upto += count;
+      } else {
+        bulkResult.length = blockSize - upto;
+        upto = blockSize;
+      }
+
+      return bulkResult;
+    }
+  }
+
+  private class Index extends IntIndexInput.Index {
+    private long fp;
+    private int upto;
+
+    @Override
+    public void read(final DataInput indexIn, final boolean absolute) throws IOException {
+      if (absolute) {
+        upto = indexIn.readVInt();
+        fp = indexIn.readVLong();
+      } else {
+        final int uptoDelta = indexIn.readVInt();
+        if ((uptoDelta & 1) == 1) {
+          // same block
+          upto += uptoDelta >>> 1;
+        } else {
+          // new block
+          upto = uptoDelta >>> 1;
+          fp += indexIn.readVLong();
+        }
+      }
+      assert upto < blockSize;
+    }
+
+    @Override
+    public void seek(final IntIndexInput.Reader other) throws IOException {
+      ((Reader) other).seek(fp, upto);
+    }
+
+    @Override
+    public void set(final IntIndexInput.Index other) {
+      final Index idx = (Index) other;
+      fp = idx.fp;
+      upto = idx.upto;
+    }
+
+    @Override
+    public Object clone() {
+      Index other = new Index();
+      other.fp = fp;
+      other.upto = upto;
+      return other;
+    }
+    
+    @Override
+    public String toString() {
+      return "fp=" + fp + " upto=" + upto;
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexOutput.java b/lucene/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexOutput.java
new file mode 100644
index 0000000..a293d06
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexOutput.java
@@ -0,0 +1,127 @@
+package org.apache.lucene.codecs.intblock;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Naive int block API that writes vInts.  This is
+ *  expected to give poor performance; it's really only for
+ *  testing the pluggability.  One should typically use pfor instead. */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.sep.IntIndexOutput;
+import org.apache.lucene.store.IndexOutput;
+
+/** Abstract base class that writes fixed-size blocks of ints
+ *  to an IndexOutput.  While this is a simple approach, a
+ *  more performant approach would directly create an impl
+ *  of IntIndexOutput inside Directory.  Wrapping a generic
+ *  IndexInput will likely cost performance.
+ *
+ * @lucene.experimental
+ */
+public abstract class FixedIntBlockIndexOutput extends IntIndexOutput {
+
+  protected final IndexOutput out;
+  private final int blockSize;
+  protected final int[] buffer;
+  private int upto;
+
+  protected FixedIntBlockIndexOutput(IndexOutput out, int fixedBlockSize) throws IOException {
+    blockSize = fixedBlockSize;
+    this.out = out;
+    out.writeVInt(blockSize);
+    buffer = new int[blockSize];
+  }
+
+  protected abstract void flushBlock() throws IOException;
+
+  @Override
+  public Index index() throws IOException {
+    return new Index();
+  }
+
+  private class Index extends IntIndexOutput.Index {
+    long fp;
+    int upto;
+    long lastFP;
+    int lastUpto;
+
+    @Override
+    public void mark() throws IOException {
+      fp = out.getFilePointer();
+      upto = FixedIntBlockIndexOutput.this.upto;
+    }
+
+    @Override
+    public void copyFrom(IntIndexOutput.Index other, boolean copyLast) throws IOException {
+      Index idx = (Index) other;
+      fp = idx.fp;
+      upto = idx.upto;
+      if (copyLast) {
+        lastFP = fp;
+        lastUpto = upto;
+      }
+    }
+
+    @Override
+    public void write(IndexOutput indexOut, boolean absolute) throws IOException {
+      if (absolute) {
+        indexOut.writeVInt(upto);
+        indexOut.writeVLong(fp);
+      } else if (fp == lastFP) {
+        // same block
+        assert upto >= lastUpto;
+        int uptoDelta = upto - lastUpto;
+        indexOut.writeVInt(uptoDelta << 1 | 1);
+      } else {      
+        // new block
+        indexOut.writeVInt(upto << 1);
+        indexOut.writeVLong(fp - lastFP);
+      }
+      lastUpto = upto;
+      lastFP = fp;
+    }
+
+    @Override
+    public String toString() {
+      return "fp=" + fp + " upto=" + upto;
+    }
+  }
+
+  @Override
+  public void write(int v) throws IOException {
+    buffer[upto++] = v;
+    if (upto == blockSize) {
+      flushBlock();
+      upto = 0;
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      if (upto > 0) {
+        // NOTE: entries in the block after current upto are
+        // invalid
+        flushBlock();
+      }
+    } finally {
+      out.close();
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexInput.java b/lucene/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexInput.java
new file mode 100644
index 0000000..28a2e22
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexInput.java
@@ -0,0 +1,217 @@
+package org.apache.lucene.codecs.intblock;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Naive int block API that writes vInts.  This is
+ *  expected to give poor performance; it's really only for
+ *  testing the pluggability.  One should typically use pfor instead. */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.sep.IntIndexInput;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.IntsRef;
+
+// TODO: much of this can be shared code w/ the fixed case
+
+/** Abstract base class that reads variable-size blocks of ints
+ *  from an IndexInput.  While this is a simple approach, a
+ *  more performant approach would directly create an impl
+ *  of IntIndexInput inside Directory.  Wrapping a generic
+ *  IndexInput will likely cost performance.
+ *
+ * @lucene.experimental
+ */
+public abstract class VariableIntBlockIndexInput extends IntIndexInput {
+
+  protected final IndexInput in;
+  protected final int maxBlockSize;
+
+  protected VariableIntBlockIndexInput(final IndexInput in) throws IOException {
+    this.in = in;
+    maxBlockSize = in.readInt();
+  }
+
+  @Override
+  public Reader reader() throws IOException {
+    final int[] buffer = new int[maxBlockSize];
+    final IndexInput clone = (IndexInput) in.clone();
+    // TODO: can this be simplified?
+    return new Reader(clone, buffer, this.getBlockReader(clone, buffer));
+  }
+
+  @Override
+  public void close() throws IOException {
+    in.close();
+  }
+
+  @Override
+  public Index index() {
+    return new Index();
+  }
+
+  protected abstract BlockReader getBlockReader(IndexInput in, int[] buffer) throws IOException;
+
+  public interface BlockReader {
+    public int readBlock() throws IOException;
+    public void seek(long pos) throws IOException;
+  }
+
+  public static class Reader extends IntIndexInput.Reader {
+    private final IndexInput in;
+
+    public final int[] pending;
+    int upto;
+
+    private boolean seekPending;
+    private long pendingFP;
+    private int pendingUpto;
+    private long lastBlockFP;
+    private int blockSize;
+    private final BlockReader blockReader;
+    private final IntsRef bulkResult = new IntsRef();
+
+    public Reader(final IndexInput in, final int[] pending, final BlockReader blockReader)
+      throws IOException {
+      this.in = in;
+      this.pending = pending;
+      bulkResult.ints = pending;
+      this.blockReader = blockReader;
+    }
+
+    void seek(final long fp, final int upto) throws IOException {
+      // TODO: should we do this in real-time, not lazy?
+      pendingFP = fp;
+      pendingUpto = upto;
+      assert pendingUpto >= 0: "pendingUpto=" + pendingUpto;
+      seekPending = true;
+    }
+
+    private final void maybeSeek() throws IOException {
+      if (seekPending) {
+        if (pendingFP != lastBlockFP) {
+          // need new block
+          in.seek(pendingFP);
+          blockReader.seek(pendingFP);
+          lastBlockFP = pendingFP;
+          blockSize = blockReader.readBlock();
+        }
+        upto = pendingUpto;
+
+        // TODO: if we were more clever when writing the
+        // index, such that a seek point wouldn't be written
+        // until the int encoder "committed", we could avoid
+        // this (likely minor) inefficiency:
+
+        // This is necessary for int encoders that are
+        // non-causal, ie must see future int values to
+        // encode the current ones.
+        while(upto >= blockSize) {
+          upto -= blockSize;
+          lastBlockFP = in.getFilePointer();
+          blockSize = blockReader.readBlock();
+        }
+        seekPending = false;
+      }
+    }
+
+    @Override
+    public int next() throws IOException {
+      this.maybeSeek();
+      if (upto == blockSize) {
+        lastBlockFP = in.getFilePointer();
+        blockSize = blockReader.readBlock();
+        upto = 0;
+      }
+
+      return pending[upto++];
+    }
+
+    @Override
+    public IntsRef read(final int count) throws IOException {
+      this.maybeSeek();
+      if (upto == blockSize) {
+        lastBlockFP = in.getFilePointer();
+        blockSize = blockReader.readBlock();
+        upto = 0;
+      }
+      bulkResult.offset = upto;
+      if (upto + count < blockSize) {
+        bulkResult.length = count;
+        upto += count;
+      } else {
+        bulkResult.length = blockSize - upto;
+        upto = blockSize;
+      }
+
+      return bulkResult;
+    }
+  }
+
+  private class Index extends IntIndexInput.Index {
+    private long fp;
+    private int upto;
+
+    @Override
+    public void read(final DataInput indexIn, final boolean absolute) throws IOException {
+      if (absolute) {
+        upto = indexIn.readVInt();
+        fp = indexIn.readVLong();
+      } else {
+        final int uptoDelta = indexIn.readVInt();
+        if ((uptoDelta & 1) == 1) {
+          // same block
+          upto += uptoDelta >>> 1;
+        } else {
+          // new block
+          upto = uptoDelta >>> 1;
+          fp += indexIn.readVLong();
+        }
+      }
+      // TODO: we can't do this assert because non-causal
+      // int encoders can have upto over the buffer size
+      //assert upto < maxBlockSize: "upto=" + upto + " max=" + maxBlockSize;
+    }
+
+    @Override
+    public String toString() {
+      return "VarIntBlock.Index fp=" + fp + " upto=" + upto + " maxBlock=" + maxBlockSize;
+    }
+
+    @Override
+    public void seek(final IntIndexInput.Reader other) throws IOException {
+      ((Reader) other).seek(fp, upto);
+    }
+
+    @Override
+    public void set(final IntIndexInput.Index other) {
+      final Index idx = (Index) other;
+      fp = idx.fp;
+      upto = idx.upto;
+    }
+
+    @Override
+    public Object clone() {
+      Index other = new Index();
+      other.fp = fp;
+      other.upto = upto;
+      return other;
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexOutput.java b/lucene/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexOutput.java
new file mode 100644
index 0000000..196e019
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexOutput.java
@@ -0,0 +1,135 @@
+package org.apache.lucene.codecs.intblock;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Naive int block API that writes vInts.  This is
+ *  expected to give poor performance; it's really only for
+ *  testing the pluggability.  One should typically use pfor instead. */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.sep.IntIndexOutput;
+import org.apache.lucene.store.IndexOutput;
+
+// TODO: much of this can be shared code w/ the fixed case
+
+/** Abstract base class that writes variable-size blocks of ints
+ *  to an IndexOutput.  While this is a simple approach, a
+ *  more performant approach would directly create an impl
+ *  of IntIndexOutput inside Directory.  Wrapping a generic
+ *  IndexInput will likely cost performance.
+ *
+ * @lucene.experimental
+ */
+public abstract class VariableIntBlockIndexOutput extends IntIndexOutput {
+
+  protected final IndexOutput out;
+
+  private int upto;
+  private boolean hitExcDuringWrite;
+
+  // TODO what Var-Var codecs exist in practice... and what are there blocksizes like?
+  // if its less than 128 we should set that as max and use byte?
+
+  /** NOTE: maxBlockSize must be the maximum block size 
+   *  plus the max non-causal lookahead of your codec.  EG Simple9
+   *  requires lookahead=1 because on seeing the Nth value
+   *  it knows it must now encode the N-1 values before it. */
+  protected VariableIntBlockIndexOutput(IndexOutput out, int maxBlockSize) throws IOException {
+    this.out = out;
+    out.writeInt(maxBlockSize);
+  }
+
+  /** Called one value at a time.  Return the number of
+   *  buffered input values that have been written to out. */
+  protected abstract int add(int value) throws IOException;
+
+  @Override
+  public Index index() throws IOException {
+    return new Index();
+  }
+
+  private class Index extends IntIndexOutput.Index {
+    long fp;
+    int upto;
+    long lastFP;
+    int lastUpto;
+
+    @Override
+    public void mark() throws IOException {
+      fp = out.getFilePointer();
+      upto = VariableIntBlockIndexOutput.this.upto;
+    }
+
+    @Override
+    public void copyFrom(IntIndexOutput.Index other, boolean copyLast) throws IOException {
+      Index idx = (Index) other;
+      fp = idx.fp;
+      upto = idx.upto;
+      if (copyLast) {
+        lastFP = fp;
+        lastUpto = upto;
+      }
+    }
+
+    @Override
+    public void write(IndexOutput indexOut, boolean absolute) throws IOException {
+      assert upto >= 0;
+      if (absolute) {
+        indexOut.writeVInt(upto);
+        indexOut.writeVLong(fp);
+      } else if (fp == lastFP) {
+        // same block
+        assert upto >= lastUpto;
+        int uptoDelta = upto - lastUpto;
+        indexOut.writeVInt(uptoDelta << 1 | 1);
+      } else {      
+        // new block
+        indexOut.writeVInt(upto << 1);
+        indexOut.writeVLong(fp - lastFP);
+      }
+      lastUpto = upto;
+      lastFP = fp;
+    }
+  }
+
+  @Override
+  public void write(int v) throws IOException {
+    hitExcDuringWrite = true;
+    upto -= add(v)-1;
+    hitExcDuringWrite = false;
+    assert upto >= 0;
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      if (!hitExcDuringWrite) {
+        // stuff 0s in until the "real" data is flushed:
+        int stuffed = 0;
+        while(upto > stuffed) {
+          upto -= add(0)-1;
+          assert upto >= 0;
+          stuffed += 1;
+        }
+      }
+    } finally {
+      out.close();
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/intblock/package.html b/lucene/src/java/org/apache/lucene/codecs/intblock/package.html
new file mode 100644
index 0000000..403ea1b
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/intblock/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Intblock: base support for fixed or variable length block integer encoders
+</body>
+</html>
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xCodec.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xCodec.java
new file mode 100644
index 0000000..b24b0a2
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xCodec.java
@@ -0,0 +1,120 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PerDocConsumer;
+import org.apache.lucene.codecs.PerDocProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.Directory;
+
+/**
+ * Supports the Lucene 3.x index format (readonly)
+ */
+public class Lucene3xCodec extends Codec {
+  public Lucene3xCodec() {
+    super("Lucene3x");
+  }
+
+  private final PostingsFormat postingsFormat = new Lucene3xPostingsFormat();
+  
+  // TODO: this should really be a different impl
+  private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat();
+  
+  // TODO: this should really be a different impl
+  private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
+  
+  // TODO: this should really be a different impl
+  private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
+
+  // TODO: this should really be a different impl
+  // also if we want preflex to *really* be read-only it should throw exception for the writer?
+  // this way IR.commit fails on delete/undelete/setNorm/etc ?
+  private final SegmentInfosFormat infosFormat = new Lucene40SegmentInfosFormat();
+  
+  // TODO: this should really be a different impl
+  private final NormsFormat normsFormat = new Lucene40NormsFormat();
+  
+  // 3.x doesn't support docvalues
+  private final DocValuesFormat docValuesFormat = new DocValuesFormat() {
+    @Override
+    public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
+      return null;
+    }
+
+    @Override
+    public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
+      return null;
+    }
+
+    @Override
+    public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {}
+  };
+  
+  @Override
+  public PostingsFormat postingsFormat() {
+    return postingsFormat;
+  }
+  
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValuesFormat;
+  }
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return fieldsFormat;
+  }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+
+  @Override
+  public SegmentInfosFormat segmentInfosFormat() {
+    return infosFormat;
+  }
+
+  @Override
+  public NormsFormat normsFormat() {
+    return normsFormat;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFields.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFields.java
new file mode 100644
index 0000000..d1a8b91
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xFields.java
@@ -0,0 +1,1109 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.UnicodeUtil;
+
+/** Exposes flex API on a pre-flex index, as a codec. 
+ * @lucene.experimental
+ * @deprecated (4.0)
+ */
+@Deprecated
+public class Lucene3xFields extends FieldsProducer {
+  
+  private static final boolean DEBUG_SURROGATES = false;
+
+  public TermInfosReader tis;
+  public final TermInfosReader tisNoIndex;
+
+  public final IndexInput freqStream;
+  public final IndexInput proxStream;
+  final private FieldInfos fieldInfos;
+  private final SegmentInfo si;
+  final TreeMap<String,FieldInfo> fields = new TreeMap<String,FieldInfo>();
+  final Map<String,Terms> preTerms = new HashMap<String,Terms>();
+  private final Directory dir;
+  private final IOContext context;
+  private Directory cfsReader;
+
+  public Lucene3xFields(Directory dir, FieldInfos fieldInfos, SegmentInfo info, IOContext context, int indexDivisor)
+    throws IOException {
+
+    si = info;
+
+    // NOTE: we must always load terms index, even for
+    // "sequential" scan during merging, because what is
+    // sequential to merger may not be to TermInfosReader
+    // since we do the surrogates dance:
+    if (indexDivisor < 0) {
+      indexDivisor = -indexDivisor;
+    }
+    
+    boolean success = false;
+    try {
+      TermInfosReader r = new TermInfosReader(dir, info.name, fieldInfos, context, indexDivisor);    
+      if (indexDivisor == -1) {
+        tisNoIndex = r;
+      } else {
+        tisNoIndex = null;
+        tis = r;
+      }
+      this.context = context;
+      this.fieldInfos = fieldInfos;
+
+      // make sure that all index files have been read or are kept open
+      // so that if an index update removes them we'll still have them
+      freqStream = dir.openInput(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.FREQ_EXTENSION), context);
+      boolean anyProx = false;
+      for (FieldInfo fi : fieldInfos) {
+        if (fi.isIndexed) {
+          fields.put(fi.name, fi);
+          preTerms.put(fi.name, new PreTerms(fi));
+          if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+            anyProx = true;
+          }
+        }
+      }
+
+      if (anyProx) {
+        proxStream = dir.openInput(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.PROX_EXTENSION), context);
+      } else {
+        proxStream = null;
+      }
+      success = true;
+    } finally {
+      // With lock-less commits, it's entirely possible (and
+      // fine) to hit a FileNotFound exception above. In
+      // this case, we want to explicitly close any subset
+      // of things that were opened so that we don't have to
+      // wait for a GC to do so.
+      if (!success) {
+        close();
+      }
+    }
+    this.dir = dir;
+  }
+
+  // If this returns, we do the surrogates dance so that the
+  // terms are sorted by unicode sort order.  This should be
+  // true when segments are used for "normal" searching;
+  // it's only false during testing, to create a pre-flex
+  // index, using the test-only PreFlexRW.
+  protected boolean sortTermsByUnicode() {
+    return true;
+  }
+
+  static void files(Directory dir, SegmentInfo info, Collection<String> files) throws IOException {
+    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.TERMS_EXTENSION));
+    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION));
+    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.FREQ_EXTENSION));
+    if (info.getHasProx()) {
+      // LUCENE-1739: for certain versions of 2.9-dev,
+      // hasProx would be incorrectly computed during
+      // indexing as true, and then stored into the segments
+      // file, when it should have been false.  So we do the
+      // extra check, here:
+      final String prx = IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.PROX_EXTENSION);
+      if (dir.fileExists(prx)) {
+        files.add(prx);
+      }
+    }
+  }
+
+  @Override
+  public FieldsEnum iterator() throws IOException {
+    return new PreFlexFieldsEnum();
+  }
+
+  @Override
+  public Terms terms(String field) {
+    return preTerms.get(field);
+  }
+
+  @Override
+  public int getUniqueFieldCount() {
+    return preTerms.size();
+  }
+
+  @Override
+  public long getUniqueTermCount() throws IOException {
+    return getTermsDict().size();
+  }
+
+  synchronized private TermInfosReader getTermsDict() {
+    if (tis != null) {
+      return tis;
+    } else {
+      return tisNoIndex;
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (tis != null) {
+      tis.close();
+    }
+    if (tisNoIndex != null) {
+      tisNoIndex.close();
+    }
+    if (cfsReader != null) {
+      cfsReader.close();
+    }
+    if (freqStream != null) {
+      freqStream.close();
+    }
+    if (proxStream != null) {
+      proxStream.close();
+    }
+  }
+
+  private class PreFlexFieldsEnum extends FieldsEnum {
+    final Iterator<FieldInfo> it;
+    FieldInfo current;
+
+    public PreFlexFieldsEnum() throws IOException {
+      it = fields.values().iterator();
+    }
+
+    @Override
+    public String next() {
+      if (it.hasNext()) {
+        current = it.next();
+        return current.name;
+      } else {
+        return null;
+      }
+    }
+
+    @Override
+    public Terms terms() throws IOException {
+      return Lucene3xFields.this.terms(current.name);
+    }
+  }
+  
+  private class PreTerms extends Terms {
+    final FieldInfo fieldInfo;
+    PreTerms(FieldInfo fieldInfo) {
+      this.fieldInfo = fieldInfo;
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {    
+      PreTermsEnum termsEnum = new PreTermsEnum();
+      termsEnum.reset(fieldInfo);
+      return termsEnum;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      // Pre-flex indexes always sorted in UTF16 order, but
+      // we remap on-the-fly to unicode order
+      if (sortTermsByUnicode()) {
+        return BytesRef.getUTF8SortedAsUnicodeComparator();
+      } else {
+        return BytesRef.getUTF8SortedAsUTF16Comparator();
+      }
+    }
+
+    @Override
+    public long getUniqueTermCount() throws IOException {
+      return -1;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return -1;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return -1;
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return -1;
+    }
+  }
+
+  private class PreTermsEnum extends TermsEnum {
+    private SegmentTermEnum termEnum;
+    private FieldInfo fieldInfo;
+    private String internedFieldName;
+    private boolean skipNext;
+    private BytesRef current;
+
+    private SegmentTermEnum seekTermEnum;
+    
+    private static final byte UTF8_NON_BMP_LEAD = (byte) 0xf0;
+    private static final byte UTF8_HIGH_BMP_LEAD = (byte) 0xee;
+
+    // Returns true if the unicode char is "after" the
+    // surrogates in UTF16, ie >= U+E000 and <= U+FFFF:
+    private final boolean isHighBMPChar(byte[] b, int idx) {
+      return (b[idx] & UTF8_HIGH_BMP_LEAD) == UTF8_HIGH_BMP_LEAD;
+    }
+
+    // Returns true if the unicode char in the UTF8 byte
+    // sequence starting at idx encodes a char outside of
+    // BMP (ie what would be a surrogate pair in UTF16):
+    private final boolean isNonBMPChar(byte[] b, int idx) {
+      return (b[idx] & UTF8_NON_BMP_LEAD) == UTF8_NON_BMP_LEAD;
+    }
+
+    private final byte[] scratch = new byte[4];
+    private final BytesRef prevTerm = new BytesRef();
+    private final BytesRef scratchTerm = new BytesRef();
+    private int newSuffixStart;
+
+    // Swap in S, in place of E:
+    private boolean seekToNonBMP(SegmentTermEnum te, BytesRef term, int pos) throws IOException {
+      final int savLength = term.length;
+
+      assert term.offset == 0;
+
+      // The 3 bytes starting at downTo make up 1
+      // unicode character:
+      assert isHighBMPChar(term.bytes, pos);
+
+      // NOTE: we cannot make this assert, because
+      // AutomatonQuery legitimately sends us malformed UTF8
+      // (eg the UTF8 bytes with just 0xee)
+      // assert term.length >= pos + 3: "term.length=" + term.length + " pos+3=" + (pos+3) + " byte=" + Integer.toHexString(term.bytes[pos]) + " term=" + term.toString();
+
+      // Save the bytes && length, since we need to
+      // restore this if seek "back" finds no matching
+      // terms
+      if (term.bytes.length < 4+pos) {
+        term.grow(4+pos);
+      }
+
+      scratch[0] = term.bytes[pos];
+      scratch[1] = term.bytes[pos+1];
+      scratch[2] = term.bytes[pos+2];
+
+      term.bytes[pos] = (byte) 0xf0;
+      term.bytes[pos+1] = (byte) 0x90;
+      term.bytes[pos+2] = (byte) 0x80;
+      term.bytes[pos+3] = (byte) 0x80;
+      term.length = 4+pos;
+
+      if (DEBUG_SURROGATES) {
+        System.out.println("      try seek term=" + UnicodeUtil.toHexString(term.utf8ToString()));
+      }
+
+      // Seek "back":
+      getTermsDict().seekEnum(te, new Term(fieldInfo.name, term), true);
+
+      // Test if the term we seek'd to in fact found a
+      // surrogate pair at the same position as the E:
+      Term t2 = te.term();
+
+      // Cannot be null (or move to next field) because at
+      // "worst" it'd seek to the same term we are on now,
+      // unless we are being called from seek
+      if (t2 == null || t2.field() != internedFieldName) {
+        return false;
+      }
+
+      if (DEBUG_SURROGATES) {
+        System.out.println("      got term=" + UnicodeUtil.toHexString(t2.text()));
+      }
+
+      // Now test if prefix is identical and we found
+      // a non-BMP char at the same position:
+      BytesRef b2 = t2.bytes();
+      assert b2.offset == 0;
+
+      boolean matches;
+      if (b2.length >= term.length && isNonBMPChar(b2.bytes, pos)) {
+        matches = true;
+        for(int i=0;i<pos;i++) {
+          if (term.bytes[i] != b2.bytes[i]) {
+            matches = false;
+            break;
+          }
+        }              
+      } else {
+        matches = false;
+      }
+
+      // Restore term:
+      term.length = savLength;
+      term.bytes[pos] = scratch[0];
+      term.bytes[pos+1] = scratch[1];
+      term.bytes[pos+2] = scratch[2];
+
+      return matches;
+    }
+
+    // Seek type 2 "continue" (back to the start of the
+    // surrogates): scan the stripped suffix from the
+    // prior term, backwards. If there was an E in that
+    // part, then we try to seek back to S.  If that
+    // seek finds a matching term, we go there.
+    private boolean doContinue() throws IOException {
+
+      if (DEBUG_SURROGATES) {
+        System.out.println("  try cont");
+      }
+
+      int downTo = prevTerm.length-1;
+
+      boolean didSeek = false;
+      
+      final int limit = Math.min(newSuffixStart, scratchTerm.length-1);
+
+      while(downTo > limit) {
+
+        if (isHighBMPChar(prevTerm.bytes, downTo)) {
+
+          if (DEBUG_SURROGATES) {
+            System.out.println("    found E pos=" + downTo + " vs len=" + prevTerm.length);
+          }
+
+          if (seekToNonBMP(seekTermEnum, prevTerm, downTo)) {
+            // TODO: more efficient seek?
+            getTermsDict().seekEnum(termEnum, seekTermEnum.term(), true);
+            //newSuffixStart = downTo+4;
+            newSuffixStart = downTo;
+            scratchTerm.copyBytes(termEnum.term().bytes());
+            didSeek = true;
+            if (DEBUG_SURROGATES) {
+              System.out.println("      seek!");
+            }
+            break;
+          } else {
+            if (DEBUG_SURROGATES) {
+              System.out.println("      no seek");
+            }
+          }
+        }
+
+        // Shorten prevTerm in place so that we don't redo
+        // this loop if we come back here:
+        if ((prevTerm.bytes[downTo] & 0xc0) == 0xc0 || (prevTerm.bytes[downTo] & 0x80) == 0) {
+          prevTerm.length = downTo;
+        }
+        
+        downTo--;
+      }
+
+      return didSeek;
+    }
+
+    // Look for seek type 3 ("pop"): if the delta from
+    // prev -> current was replacing an S with an E,
+    // we must now seek to beyond that E.  This seek
+    // "finishes" the dance at this character
+    // position.
+    private boolean doPop() throws IOException {
+
+      if (DEBUG_SURROGATES) {
+        System.out.println("  try pop");
+      }
+
+      assert newSuffixStart <= prevTerm.length;
+      assert newSuffixStart < scratchTerm.length || newSuffixStart == 0;
+
+      if (prevTerm.length > newSuffixStart &&
+          isNonBMPChar(prevTerm.bytes, newSuffixStart) &&
+          isHighBMPChar(scratchTerm.bytes, newSuffixStart)) {
+
+        // Seek type 2 -- put 0xFF at this position:
+        scratchTerm.bytes[newSuffixStart] = (byte) 0xff;
+        scratchTerm.length = newSuffixStart+1;
+
+        if (DEBUG_SURROGATES) {
+          System.out.println("    seek to term=" + UnicodeUtil.toHexString(scratchTerm.utf8ToString()) + " " + scratchTerm.toString());
+        }
+          
+        // TODO: more efficient seek?  can we simply swap
+        // the enums?
+        getTermsDict().seekEnum(termEnum, new Term(fieldInfo.name, scratchTerm), true);
+
+        final Term t2 = termEnum.term();
+
+        // We could hit EOF or different field since this
+        // was a seek "forward":
+        if (t2 != null && t2.field() == internedFieldName) {
+
+          if (DEBUG_SURROGATES) {
+            System.out.println("      got term=" + UnicodeUtil.toHexString(t2.text()) + " " + t2.bytes());
+          }
+
+          final BytesRef b2 = t2.bytes();
+          assert b2.offset == 0;
+
+
+          // Set newSuffixStart -- we can't use
+          // termEnum's since the above seek may have
+          // done no scanning (eg, term was precisely
+          // and index term, or, was in the term seek
+          // cache):
+          scratchTerm.copyBytes(b2);
+          setNewSuffixStart(prevTerm, scratchTerm);
+
+          return true;
+        } else if (newSuffixStart != 0 || scratchTerm.length != 0) {
+          if (DEBUG_SURROGATES) {
+            System.out.println("      got term=null (or next field)");
+          }
+          newSuffixStart = 0;
+          scratchTerm.length = 0;
+          return true;
+        }
+      }
+
+      return false;
+    }
+
+    // Pre-flex indices store terms in UTF16 sort order, but
+    // certain queries require Unicode codepoint order; this
+    // method carefully seeks around surrogates to handle
+    // this impedance mismatch
+
+    private void surrogateDance() throws IOException {
+
+      if (!unicodeSortOrder) {
+        return;
+      }
+
+      // We are invoked after TIS.next() (by UTF16 order) to
+      // possibly seek to a different "next" (by unicode
+      // order) term.
+
+      // We scan only the "delta" from the last term to the
+      // current term, in UTF8 bytes.  We look at 1) the bytes
+      // stripped from the prior term, and then 2) the bytes
+      // appended to that prior term's prefix.
+    
+      // We don't care about specific UTF8 sequences, just
+      // the "category" of the UTF16 character.  Category S
+      // is a high/low surrogate pair (it non-BMP).
+      // Category E is any BMP char > UNI_SUR_LOW_END (and <
+      // U+FFFF). Category A is the rest (any unicode char
+      // <= UNI_SUR_HIGH_START).
+
+      // The core issue is that pre-flex indices sort the
+      // characters as ASE, while flex must sort as AES.  So
+      // when scanning, when we hit S, we must 1) seek
+      // forward to E and enum the terms there, then 2) seek
+      // back to S and enum all terms there, then 3) seek to
+      // after E.  Three different seek points (1, 2, 3).
+    
+      // We can easily detect S in UTF8: if a byte has
+      // prefix 11110 (0xf0), then that byte and the
+      // following 3 bytes encode a single unicode codepoint
+      // in S.  Similarly, we can detect E: if a byte has
+      // prefix 1110111 (0xee), then that byte and the
+      // following 2 bytes encode a single unicode codepoint
+      // in E.
+
+      // Note that this is really a recursive process --
+      // maybe the char at pos 2 needs to dance, but any
+      // point in its dance, suddenly pos 4 needs to dance
+      // so you must finish pos 4 before returning to pos
+      // 2.  But then during pos 4's dance maybe pos 7 needs
+      // to dance, etc.  However, despite being recursive,
+      // we don't need to hold any state because the state
+      // can always be derived by looking at prior term &
+      // current term.
+
+      // TODO: can we avoid this copy?
+      if (termEnum.term() == null || termEnum.term().field() != internedFieldName) {
+        scratchTerm.length = 0;
+      } else {
+        scratchTerm.copyBytes(termEnum.term().bytes());
+      }
+      
+      if (DEBUG_SURROGATES) {
+        System.out.println("  dance");
+        System.out.println("    prev=" + UnicodeUtil.toHexString(prevTerm.utf8ToString()));
+        System.out.println("         " + prevTerm.toString());
+        System.out.println("    term=" + UnicodeUtil.toHexString(scratchTerm.utf8ToString()));
+        System.out.println("         " + scratchTerm.toString());
+      }
+
+      // This code assumes TermInfosReader/SegmentTermEnum
+      // always use BytesRef.offset == 0
+      assert prevTerm.offset == 0;
+      assert scratchTerm.offset == 0;
+
+      // Need to loop here because we may need to do multiple
+      // pops, and possibly a continue in the end, ie:
+      //
+      //  cont
+      //  pop, cont
+      //  pop, pop, cont
+      //  <nothing>
+      //
+
+      while(true) {
+        if (doContinue()) {
+          break;
+        } else {
+          if (!doPop()) {
+            break;
+          }
+        }
+      }
+
+      if (DEBUG_SURROGATES) {
+        System.out.println("  finish bmp ends");
+      }
+
+      doPushes();
+    }
+
+
+    // Look for seek type 1 ("push"): if the newly added
+    // suffix contains any S, we must try to seek to the
+    // corresponding E.  If we find a match, we go there;
+    // else we keep looking for additional S's in the new
+    // suffix.  This "starts" the dance, at this character
+    // position:
+    private void doPushes() throws IOException {
+
+      int upTo = newSuffixStart;
+      if (DEBUG_SURROGATES) {
+        System.out.println("  try push newSuffixStart=" + newSuffixStart + " scratchLen=" + scratchTerm.length);
+      }
+
+      while(upTo < scratchTerm.length) {
+        if (isNonBMPChar(scratchTerm.bytes, upTo) &&
+            (upTo > newSuffixStart ||
+             (upTo >= prevTerm.length ||
+              (!isNonBMPChar(prevTerm.bytes, upTo) &&
+               !isHighBMPChar(prevTerm.bytes, upTo))))) {
+
+          // A non-BMP char (4 bytes UTF8) starts here:
+          assert scratchTerm.length >= upTo + 4;
+          
+          final int savLength = scratchTerm.length;
+          scratch[0] = scratchTerm.bytes[upTo];
+          scratch[1] = scratchTerm.bytes[upTo+1];
+          scratch[2] = scratchTerm.bytes[upTo+2];
+
+          scratchTerm.bytes[upTo] = UTF8_HIGH_BMP_LEAD;
+          scratchTerm.bytes[upTo+1] = (byte) 0x80;
+          scratchTerm.bytes[upTo+2] = (byte) 0x80;
+          scratchTerm.length = upTo+3;
+
+          if (DEBUG_SURROGATES) {
+            System.out.println("    try seek 1 pos=" + upTo + " term=" + UnicodeUtil.toHexString(scratchTerm.utf8ToString()) + " " + scratchTerm.toString() + " len=" + scratchTerm.length);
+          }
+
+          // Seek "forward":
+          // TODO: more efficient seek?
+          getTermsDict().seekEnum(seekTermEnum, new Term(fieldInfo.name, scratchTerm), true);
+
+          scratchTerm.bytes[upTo] = scratch[0];
+          scratchTerm.bytes[upTo+1] = scratch[1];
+          scratchTerm.bytes[upTo+2] = scratch[2];
+          scratchTerm.length = savLength;
+
+          // Did we find a match?
+          final Term t2 = seekTermEnum.term();
+            
+          if (DEBUG_SURROGATES) {
+            if (t2 == null) {
+              System.out.println("      hit term=null");
+            } else {
+              System.out.println("      hit term=" + UnicodeUtil.toHexString(t2.text()) + " " + (t2==null? null:t2.bytes()));
+            }
+          }
+
+          // Since this was a seek "forward", we could hit
+          // EOF or a different field:
+          boolean matches;
+
+          if (t2 != null && t2.field() == internedFieldName) {
+            final BytesRef b2 = t2.bytes();
+            assert b2.offset == 0;
+            if (b2.length >= upTo+3 && isHighBMPChar(b2.bytes, upTo)) {
+              matches = true;
+              for(int i=0;i<upTo;i++) {
+                if (scratchTerm.bytes[i] != b2.bytes[i]) {
+                  matches = false;
+                  break;
+                }
+              }              
+                
+            } else {
+              matches = false;
+            }
+          } else {
+            matches = false;
+          }
+
+          if (matches) {
+
+            if (DEBUG_SURROGATES) {
+              System.out.println("      matches!");
+            }
+
+            // OK seek "back"
+            // TODO: more efficient seek?
+            getTermsDict().seekEnum(termEnum, seekTermEnum.term(), true);
+
+            scratchTerm.copyBytes(seekTermEnum.term().bytes());
+
+            // +3 because we don't need to check the char
+            // at upTo: we know it's > BMP
+            upTo += 3;
+
+            // NOTE: we keep iterating, now, since this
+            // can easily "recurse".  Ie, after seeking
+            // forward at a certain char position, we may
+            // find another surrogate in our [new] suffix
+            // and must then do another seek (recurse)
+          } else {
+            upTo++;
+          }
+        } else {
+          upTo++;
+        }
+      }
+    }
+
+    private boolean unicodeSortOrder;
+
+    void reset(FieldInfo fieldInfo) throws IOException {
+      //System.out.println("pff.reset te=" + termEnum);
+      this.fieldInfo = fieldInfo;
+      internedFieldName = fieldInfo.name.intern();
+      final Term term = new Term(internedFieldName);
+      if (termEnum == null) {
+        termEnum = getTermsDict().terms(term);
+        seekTermEnum = getTermsDict().terms(term);
+        //System.out.println("  term=" + termEnum.term());
+      } else {
+        getTermsDict().seekEnum(termEnum, term, true);
+      }
+      skipNext = true;
+
+      unicodeSortOrder = sortTermsByUnicode();
+
+      final Term t = termEnum.term();
+      if (t != null && t.field() == internedFieldName) {
+        newSuffixStart = 0;
+        prevTerm.length = 0;
+        surrogateDance();
+      }
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      // Pre-flex indexes always sorted in UTF16 order, but
+      // we remap on-the-fly to unicode order
+      if (unicodeSortOrder) {
+        return BytesRef.getUTF8SortedAsUnicodeComparator();
+      } else {
+        return BytesRef.getUTF8SortedAsUTF16Comparator();
+      }
+    }
+
+    @Override
+    public void seekExact(long ord) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public long ord() throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public SeekStatus seekCeil(BytesRef term, boolean useCache) throws IOException {
+      if (DEBUG_SURROGATES) {
+        System.out.println("TE.seek target=" + UnicodeUtil.toHexString(term.utf8ToString()));
+      }
+      skipNext = false;
+      final TermInfosReader tis = getTermsDict();
+      final Term t0 = new Term(fieldInfo.name, term);
+
+      assert termEnum != null;
+
+      tis.seekEnum(termEnum, t0, useCache);
+
+      final Term t = termEnum.term();
+
+      if (t != null && t.field() == internedFieldName && term.bytesEquals(t.bytes())) {
+        // If we found an exact match, no need to do the
+        // surrogate dance
+        if (DEBUG_SURROGATES) {
+          System.out.println("  seek exact match");
+        }
+        current = t.bytes();
+        return SeekStatus.FOUND;
+      } else if (t == null || t.field() != internedFieldName) {
+
+        // TODO: maybe we can handle this like the next()
+        // into null?  set term as prevTerm then dance?
+
+        if (DEBUG_SURROGATES) {
+          System.out.println("  seek hit EOF");
+        }
+
+        // We hit EOF; try end-case surrogate dance: if we
+        // find an E, try swapping in S, backwards:
+        scratchTerm.copyBytes(term);
+
+        assert scratchTerm.offset == 0;
+
+        for(int i=scratchTerm.length-1;i>=0;i--) {
+          if (isHighBMPChar(scratchTerm.bytes, i)) {
+            if (DEBUG_SURROGATES) {
+              System.out.println("    found E pos=" + i + "; try seek");
+            }
+
+            if (seekToNonBMP(seekTermEnum, scratchTerm, i)) {
+
+              scratchTerm.copyBytes(seekTermEnum.term().bytes());
+              getTermsDict().seekEnum(termEnum, seekTermEnum.term(), useCache);
+
+              newSuffixStart = 1+i;
+
+              doPushes();
+
+              // Found a match
+              // TODO: faster seek?
+              current = termEnum.term().bytes();
+              return SeekStatus.NOT_FOUND;
+            }
+          }
+        }
+        
+        if (DEBUG_SURROGATES) {
+          System.out.println("  seek END");
+        }
+
+        current = null;
+        return SeekStatus.END;
+      } else {
+
+        // We found a non-exact but non-null term; this one
+        // is fun -- just treat it like next, by pretending
+        // requested term was prev:
+        prevTerm.copyBytes(term);
+
+        if (DEBUG_SURROGATES) {
+          System.out.println("  seek hit non-exact term=" + UnicodeUtil.toHexString(t.text()));
+        }
+
+        final BytesRef br = t.bytes();
+        assert br.offset == 0;
+
+        setNewSuffixStart(term, br);
+
+        surrogateDance();
+
+        final Term t2 = termEnum.term();
+        if (t2 == null || t2.field() != internedFieldName) {
+          // PreFlex codec interns field names; verify:
+          assert t2 == null || !t2.field().equals(internedFieldName);
+          current = null;
+          return SeekStatus.END;
+        } else {
+          current = t2.bytes();
+          assert !unicodeSortOrder || term.compareTo(current) < 0 : "term=" + UnicodeUtil.toHexString(term.utf8ToString()) + " vs current=" + UnicodeUtil.toHexString(current.utf8ToString());
+          return SeekStatus.NOT_FOUND;
+        }
+      }
+    }
+
+    private void setNewSuffixStart(BytesRef br1, BytesRef br2) {
+      final int limit = Math.min(br1.length, br2.length);
+      int lastStart = 0;
+      for(int i=0;i<limit;i++) {
+        if ((br1.bytes[br1.offset+i] & 0xc0) == 0xc0 || (br1.bytes[br1.offset+i] & 0x80) == 0) {
+          lastStart = i;
+        }
+        if (br1.bytes[br1.offset+i] != br2.bytes[br2.offset+i]) {
+          newSuffixStart = lastStart;
+          if (DEBUG_SURROGATES) {
+            System.out.println("    set newSuffixStart=" + newSuffixStart);
+          }
+          return;
+        }
+      }
+      newSuffixStart = limit;
+      if (DEBUG_SURROGATES) {
+        System.out.println("    set newSuffixStart=" + newSuffixStart);
+      }
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      if (DEBUG_SURROGATES) {
+        System.out.println("TE.next()");
+      }
+      if (skipNext) {
+        if (DEBUG_SURROGATES) {
+          System.out.println("  skipNext=true");
+        }
+        skipNext = false;
+        if (termEnum.term() == null) {
+          return null;
+        // PreFlex codec interns field names:
+        } else if (termEnum.term().field() != internedFieldName) {
+          return null;
+        } else {
+          return current = termEnum.term().bytes();
+        }
+      }
+
+      // TODO: can we use STE's prevBuffer here?
+      prevTerm.copyBytes(termEnum.term().bytes());
+
+      if (termEnum.next() && termEnum.term().field() == internedFieldName) {
+        newSuffixStart = termEnum.newSuffixStart;
+        if (DEBUG_SURROGATES) {
+          System.out.println("  newSuffixStart=" + newSuffixStart);
+        }
+        surrogateDance();
+        final Term t = termEnum.term();
+        if (t == null || t.field() != internedFieldName) {
+          // PreFlex codec interns field names; verify:
+          assert t == null || !t.field().equals(internedFieldName);
+          current = null;
+        } else {
+          current = t.bytes();
+        }
+        return current;
+      } else {
+        // This field is exhausted, but we have to give
+        // surrogateDance a chance to seek back:
+        if (DEBUG_SURROGATES) {
+          System.out.println("  force cont");
+        }
+        //newSuffixStart = prevTerm.length;
+        newSuffixStart = 0;
+        surrogateDance();
+        
+        final Term t = termEnum.term();
+        if (t == null || t.field() != internedFieldName) {
+          // PreFlex codec interns field names; verify:
+          assert t == null || !t.field().equals(internedFieldName);
+          return null;
+        } else {
+          current = t.bytes();
+          return current;
+        }
+      }
+    }
+
+    @Override
+    public BytesRef term() {
+      return current;
+    }
+
+    @Override
+    public int docFreq() {
+      return termEnum.docFreq();
+    }
+
+    @Override
+    public long totalTermFreq() {
+      return -1;
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
+      PreDocsEnum docsEnum;
+      if (needsFreqs && fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
+        return null;
+      } else if (reuse == null || !(reuse instanceof PreDocsEnum)) {
+        docsEnum = new PreDocsEnum();
+      } else {
+        docsEnum = (PreDocsEnum) reuse;
+        if (docsEnum.getFreqStream() != freqStream) {
+          docsEnum = new PreDocsEnum();
+        }
+      }
+      return docsEnum.reset(termEnum, liveDocs);
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
+      PreDocsAndPositionsEnum docsPosEnum;
+      if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+        return null;
+      } else if (reuse == null || !(reuse instanceof PreDocsAndPositionsEnum)) {
+        docsPosEnum = new PreDocsAndPositionsEnum();
+      } else {
+        docsPosEnum = (PreDocsAndPositionsEnum) reuse;
+        if (docsPosEnum.getFreqStream() != freqStream) {
+          docsPosEnum = new PreDocsAndPositionsEnum();
+        }
+      }
+      return docsPosEnum.reset(termEnum, liveDocs);        
+    }
+  }
+
+  private final class PreDocsEnum extends DocsEnum {
+    final private SegmentTermDocs docs;
+    private int docID = -1;
+    PreDocsEnum() throws IOException {
+      docs = new SegmentTermDocs(freqStream, getTermsDict(), fieldInfos);
+    }
+
+    IndexInput getFreqStream() {
+      return freqStream;
+    }
+
+    public PreDocsEnum reset(SegmentTermEnum termEnum, Bits liveDocs) throws IOException {
+      docs.setLiveDocs(liveDocs);
+      docs.seek(termEnum);
+      docID = -1;
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      if (docs.next()) {
+        return docID = docs.doc();
+      } else {
+        return docID = NO_MORE_DOCS;
+      }
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      if (docs.skipTo(target)) {
+        return docID = docs.doc();
+      } else {
+        return docID = NO_MORE_DOCS;
+      }
+    }
+
+    @Override
+    public int freq() {
+      return docs.freq();
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+  }
+
+  private final class PreDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    final private SegmentTermPositions pos;
+    private int docID = -1;
+    PreDocsAndPositionsEnum() throws IOException {
+      pos = new SegmentTermPositions(freqStream, proxStream, getTermsDict(), fieldInfos);
+    }
+
+    IndexInput getFreqStream() {
+      return freqStream;
+    }
+
+    public DocsAndPositionsEnum reset(SegmentTermEnum termEnum, Bits liveDocs) throws IOException {
+      pos.setLiveDocs(liveDocs);
+      pos.seek(termEnum);
+      docID = -1;
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      if (pos.next()) {
+        return docID = pos.doc();
+      } else {
+        return docID = NO_MORE_DOCS;
+      }
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      if (pos.skipTo(target)) {
+        return docID = pos.doc();
+      } else {
+        return docID = NO_MORE_DOCS;
+      }
+    }
+
+    @Override
+    public int freq() {
+      return pos.freq();
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      assert docID != NO_MORE_DOCS;
+      return pos.nextPosition();
+    }
+
+    @Override
+    public boolean hasPayload() {
+      assert docID != NO_MORE_DOCS;
+      return pos.isPayloadAvailable();
+    }
+
+    private BytesRef payload;
+
+    @Override
+    public BytesRef getPayload() throws IOException {
+      final int len = pos.getPayloadLength();
+      if (payload == null) {
+        payload = new BytesRef();
+        payload.bytes = new byte[len];
+      } else {
+        if (payload.bytes.length < len) {
+          payload.grow(len);
+        }
+      }
+      
+      payload.bytes = pos.getPayload(payload.bytes, 0);
+      payload.length = len;
+      return payload;
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xPostingsFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xPostingsFormat.java
new file mode 100644
index 0000000..e5a6497
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/Lucene3xPostingsFormat.java
@@ -0,0 +1,73 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Set;
+import java.io.IOException;
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+
+/** Codec that reads the pre-flex-indexing postings
+ *  format.  It does not provide a writer because newly
+ *  written segments should use StandardCodec.
+ *
+ * @deprecated (4.0) This is only used to read indexes created
+ * before 4.0.
+ * @lucene.experimental
+ */
+@Deprecated
+public class Lucene3xPostingsFormat extends PostingsFormat {
+
+  /** Extension of terms file */
+  public static final String TERMS_EXTENSION = "tis";
+
+  /** Extension of terms index file */
+  public static final String TERMS_INDEX_EXTENSION = "tii";
+
+  /** Extension of freq postings file */
+  public static final String FREQ_EXTENSION = "frq";
+
+  /** Extension of prox postings file */
+  public static final String PROX_EXTENSION = "prx";
+
+  public Lucene3xPostingsFormat() {
+    super("Lucene3x");
+  }
+  
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    throw new IllegalArgumentException("this codec can only be used for reading");
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new Lucene3xFields(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.termsIndexDivisor);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, String segmentSuffix, Set<String> files) throws IOException {
+    // preflex fields have no segmentSuffix - we ignore it here
+    Lucene3xFields.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermDocs.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermDocs.java
new file mode 100644
index 0000000..c398c78
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermDocs.java
@@ -0,0 +1,230 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.lucene40.Lucene40SkipListReader;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+
+/** @deprecated (4.0)
+ *  @lucene.experimental */
+@Deprecated
+public class SegmentTermDocs {
+  //protected SegmentReader parent;
+  private final FieldInfos fieldInfos;
+  private final TermInfosReader tis;
+  protected Bits liveDocs;
+  protected IndexInput freqStream;
+  protected int count;
+  protected int df;
+  int doc = 0;
+  int freq;
+
+  private int skipInterval;
+  private int maxSkipLevels;
+  private Lucene40SkipListReader skipListReader;
+  
+  private long freqBasePointer;
+  private long proxBasePointer;
+
+  private long skipPointer;
+  private boolean haveSkipped;
+  
+  protected boolean currentFieldStoresPayloads;
+  protected IndexOptions indexOptions;
+  
+  public SegmentTermDocs(IndexInput freqStream, TermInfosReader tis, FieldInfos fieldInfos) {
+    this.freqStream = (IndexInput) freqStream.clone();
+    this.tis = tis;
+    this.fieldInfos = fieldInfos;
+    skipInterval = tis.getSkipInterval();
+    maxSkipLevels = tis.getMaxSkipLevels();
+  }
+
+  public void seek(Term term) throws IOException {
+    TermInfo ti = tis.get(term);
+    seek(ti, term);
+  }
+
+  public void setLiveDocs(Bits liveDocs) {
+    this.liveDocs = liveDocs;
+  }
+
+  public void seek(SegmentTermEnum segmentTermEnum) throws IOException {
+    TermInfo ti;
+    Term term;
+    
+    // use comparison of fieldinfos to verify that termEnum belongs to the same segment as this SegmentTermDocs
+    if (segmentTermEnum.fieldInfos == fieldInfos) {        // optimized case
+      term = segmentTermEnum.term();
+      ti = segmentTermEnum.termInfo();
+    } else  {                                         // punt case
+      term = segmentTermEnum.term();
+      ti = tis.get(term); 
+    }
+    
+    seek(ti, term);
+  }
+
+  void seek(TermInfo ti, Term term) throws IOException {
+    count = 0;
+    FieldInfo fi = fieldInfos.fieldInfo(term.field());
+    this.indexOptions = (fi != null) ? fi.indexOptions : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+    currentFieldStoresPayloads = (fi != null) ? fi.storePayloads : false;
+    if (ti == null) {
+      df = 0;
+    } else {
+      df = ti.docFreq;
+      doc = 0;
+      freqBasePointer = ti.freqPointer;
+      proxBasePointer = ti.proxPointer;
+      skipPointer = freqBasePointer + ti.skipOffset;
+      freqStream.seek(freqBasePointer);
+      haveSkipped = false;
+    }
+  }
+
+  public void close() throws IOException {
+    freqStream.close();
+    if (skipListReader != null)
+      skipListReader.close();
+  }
+
+  public final int doc() { return doc; }
+  public final int freq() {
+    assert indexOptions != IndexOptions.DOCS_ONLY;
+    return freq;
+  }
+
+  protected void skippingDoc() throws IOException {
+  }
+
+  public boolean next() throws IOException {
+    while (true) {
+      if (count == df)
+        return false;
+      final int docCode = freqStream.readVInt();
+      
+      if (indexOptions == IndexOptions.DOCS_ONLY) {
+        doc += docCode;
+      } else {
+        doc += docCode >>> 1;       // shift off low bit
+        if ((docCode & 1) != 0)       // if low bit is set
+          freq = 1;         // freq is one
+        else {
+          freq = freqStream.readVInt();     // else read freq
+          assert freq != 1;
+        }
+      }
+      
+      count++;
+
+      if (liveDocs == null || liveDocs.get(doc)) {
+        break;
+      }
+      skippingDoc();
+    }
+    return true;
+  }
+
+  /** Optimized implementation. */
+  public int read(final int[] docs, final int[] freqs)
+          throws IOException {
+    final int length = docs.length;
+    if (indexOptions == IndexOptions.DOCS_ONLY) {
+      return readNoTf(docs, freqs, length);
+    } else {
+      int i = 0;
+      while (i < length && count < df) {
+        // manually inlined call to next() for speed
+        final int docCode = freqStream.readVInt();
+        doc += docCode >>> 1;       // shift off low bit
+        if ((docCode & 1) != 0)       // if low bit is set
+          freq = 1;         // freq is one
+        else
+          freq = freqStream.readVInt();     // else read freq
+        count++;
+
+        if (liveDocs == null || liveDocs.get(doc)) {
+          docs[i] = doc;
+          freqs[i] = freq;
+          ++i;
+        }
+      }
+      return i;
+    }
+  }
+
+  private final int readNoTf(final int[] docs, final int[] freqs, final int length) throws IOException {
+    int i = 0;
+    while (i < length && count < df) {
+      // manually inlined call to next() for speed
+      doc += freqStream.readVInt();       
+      count++;
+
+      if (liveDocs == null || liveDocs.get(doc)) {
+        docs[i] = doc;
+        // Hardware freq to 1 when term freqs were not
+        // stored in the index
+        freqs[i] = 1;
+        ++i;
+      }
+    }
+    return i;
+  }
+ 
+  
+  /** Overridden by SegmentTermPositions to skip in prox stream. */
+  protected void skipProx(long proxPointer, int payloadLength) throws IOException {}
+
+  /** Optimized implementation. */
+  public boolean skipTo(int target) throws IOException {
+    // don't skip if the target is close (within skipInterval docs away)
+    if ((target - skipInterval) >= doc && df >= skipInterval) {                      // optimized case
+      if (skipListReader == null)
+        skipListReader = new Lucene40SkipListReader((IndexInput) freqStream.clone(), maxSkipLevels, skipInterval); // lazily clone
+
+      if (!haveSkipped) {                          // lazily initialize skip stream
+        skipListReader.init(skipPointer, freqBasePointer, proxBasePointer, df, currentFieldStoresPayloads);
+        haveSkipped = true;
+      }
+
+      int newCount = skipListReader.skipTo(target); 
+      if (newCount > count) {
+        freqStream.seek(skipListReader.getFreqPointer());
+        skipProx(skipListReader.getProxPointer(), skipListReader.getPayloadLength());
+
+        doc = skipListReader.getDoc();
+        count = newCount;
+      }      
+    }
+
+    // done skipping, now just scan
+    do {
+      if (!next())
+        return false;
+    } while (target > doc);
+    return true;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermEnum.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermEnum.java
new file mode 100644
index 0000000..281eed1
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermEnum.java
@@ -0,0 +1,225 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.IndexFormatTooNewException;
+
+/**
+ * @deprecated (4.0) No longer used with flex indexing, except for
+ * reading old segments 
+ * @lucene.experimental */
+
+@Deprecated
+public final class SegmentTermEnum implements Cloneable {
+  private IndexInput input;
+  FieldInfos fieldInfos;
+  long size;
+  long position = -1;
+
+  // Changed strings to true utf8 with length-in-bytes not
+  // length-in-chars
+  public static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = -4;
+
+  // NOTE: always change this if you switch to a new format!
+  // whenever you add a new format, make it 1 smaller (negative version logic)!
+  public static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
+  
+  // when removing support for old versions, leave the last supported version here
+  public static final int FORMAT_MINIMUM = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
+
+  private TermBuffer termBuffer = new TermBuffer();
+  private TermBuffer prevBuffer = new TermBuffer();
+  private TermBuffer scanBuffer = new TermBuffer(); // used for scanning
+
+  TermInfo termInfo = new TermInfo();
+
+  private int format;
+  private boolean isIndex = false;
+  long indexPointer = 0;
+  int indexInterval;
+  int skipInterval;
+  int newSuffixStart;
+  int maxSkipLevels;
+  private boolean first = true;
+
+  SegmentTermEnum(IndexInput i, FieldInfos fis, boolean isi)
+          throws CorruptIndexException, IOException {
+    input = i;
+    fieldInfos = fis;
+    isIndex = isi;
+    maxSkipLevels = 1; // use single-level skip lists for formats > -3 
+    
+    int firstInt = input.readInt();
+    if (firstInt >= 0) {
+      // original-format file, without explicit format version number
+      format = 0;
+      size = firstInt;
+
+      // back-compatible settings
+      indexInterval = 128;
+      skipInterval = Integer.MAX_VALUE; // switch off skipTo optimization
+    } else {
+      // we have a format version number
+      format = firstInt;
+
+      // check that it is a format we can understand
+      if (format > FORMAT_MINIMUM)
+        throw new IndexFormatTooOldException(input, format, FORMAT_MINIMUM, FORMAT_CURRENT);
+      if (format < FORMAT_CURRENT)
+        throw new IndexFormatTooNewException(input, format, FORMAT_MINIMUM, FORMAT_CURRENT);
+
+      size = input.readLong();                    // read the size
+      
+      indexInterval = input.readInt();
+      skipInterval = input.readInt();
+      maxSkipLevels = input.readInt();
+      assert indexInterval > 0: "indexInterval=" + indexInterval + " is negative; must be > 0";
+      assert skipInterval > 0: "skipInterval=" + skipInterval + " is negative; must be > 0";
+    }
+  }
+
+  @Override
+  protected Object clone() {
+    SegmentTermEnum clone = null;
+    try {
+      clone = (SegmentTermEnum) super.clone();
+    } catch (CloneNotSupportedException e) {}
+
+    clone.input = (IndexInput) input.clone();
+    clone.termInfo = new TermInfo(termInfo);
+
+    clone.termBuffer = (TermBuffer)termBuffer.clone();
+    clone.prevBuffer = (TermBuffer)prevBuffer.clone();
+    clone.scanBuffer = new TermBuffer();
+
+    return clone;
+  }
+
+  final void seek(long pointer, long p, Term t, TermInfo ti)
+          throws IOException {
+    input.seek(pointer);
+    position = p;
+    termBuffer.set(t);
+    prevBuffer.reset();
+    //System.out.println("  ste doSeek prev=" + prevBuffer.toTerm() + " this=" + this);
+    termInfo.set(ti);
+    first = p == -1;
+  }
+
+  /** Increments the enumeration to the next element.  True if one exists.*/
+  public final boolean next() throws IOException {
+    prevBuffer.set(termBuffer);
+    //System.out.println("  ste setPrev=" + prev() + " this=" + this);
+
+    if (position++ >= size - 1) {
+      termBuffer.reset();
+      //System.out.println("    EOF");
+      return false;
+    }
+
+    termBuffer.read(input, fieldInfos);
+    newSuffixStart = termBuffer.newSuffixStart;
+
+    termInfo.docFreq = input.readVInt();	  // read doc freq
+    termInfo.freqPointer += input.readVLong();	  // read freq pointer
+    termInfo.proxPointer += input.readVLong();	  // read prox pointer
+    
+    if (termInfo.docFreq >= skipInterval) 
+      termInfo.skipOffset = input.readVInt();
+
+    if (isIndex)
+      indexPointer += input.readVLong();	  // read index pointer
+
+    //System.out.println("  ste ret term=" + term());
+    return true;
+  }
+
+  /* Optimized scan, without allocating new terms. 
+   *  Return number of invocations to next().
+   *
+   * NOTE: LUCENE-3183: if you pass Term("", "") here then this
+   * will incorrectly return before positioning the enum,
+   * and position will be -1; caller must detect this. */
+  final int scanTo(Term term) throws IOException {
+    scanBuffer.set(term);
+    int count = 0;
+    if (first) {
+      // Always force initial next() in case term is
+      // Term("", "")
+      next();
+      first = false;
+      count++;
+    }
+    while (scanBuffer.compareTo(termBuffer) > 0 && next()) {
+      count++;
+    }
+    return count;
+  }
+
+  /** Returns the current Term in the enumeration.
+   Initially invalid, valid after next() called for the first time.*/
+  public final Term term() {
+    return termBuffer.toTerm();
+  }
+
+  /** Returns the previous Term enumerated. Initially null.*/
+  final Term prev() {
+    return prevBuffer.toTerm();
+  }
+
+  /** Returns the current TermInfo in the enumeration.
+   Initially invalid, valid after next() called for the first time.*/
+  final TermInfo termInfo() {
+    return new TermInfo(termInfo);
+  }
+
+  /** Sets the argument to the current TermInfo in the enumeration.
+   Initially invalid, valid after next() called for the first time.*/
+  final void termInfo(TermInfo ti) {
+    ti.set(termInfo);
+  }
+
+  /** Returns the docFreq from the current TermInfo in the enumeration.
+   Initially invalid, valid after next() called for the first time.*/
+  public final int docFreq() {
+    return termInfo.docFreq;
+  }
+
+  /* Returns the freqPointer from the current TermInfo in the enumeration.
+    Initially invalid, valid after next() called for the first time.*/
+  final long freqPointer() {
+    return termInfo.freqPointer;
+  }
+
+  /* Returns the proxPointer from the current TermInfo in the enumeration.
+    Initially invalid, valid after next() called for the first time.*/
+  final long proxPointer() {
+    return termInfo.proxPointer;
+  }
+
+  /** Closes the enumeration to further activity, freeing resources. */
+  public final void close() throws IOException {
+    input.close();
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermPositions.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermPositions.java
new file mode 100644
index 0000000..bdd45bb
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/SegmentTermPositions.java
@@ -0,0 +1,219 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * @lucene.experimental
+ * @deprecated (4.0)
+ */
+@Deprecated
+public final class SegmentTermPositions
+extends SegmentTermDocs  {
+  private IndexInput proxStream;
+  private IndexInput proxStreamOrig;
+  private int proxCount;
+  private int position;
+  
+  // the current payload length
+  private int payloadLength;
+  // indicates whether the payload of the current position has
+  // been read from the proxStream yet
+  private boolean needToLoadPayload;
+  
+  // these variables are being used to remember information
+  // for a lazy skip
+  private long lazySkipPointer = -1;
+  private int lazySkipProxCount = 0;
+
+  /*
+  SegmentTermPositions(SegmentReader p) {
+    super(p);
+    this.proxStream = null;  // the proxStream will be cloned lazily when nextPosition() is called for the first time
+  }
+  */
+
+  public SegmentTermPositions(IndexInput freqStream, IndexInput proxStream, TermInfosReader tis, FieldInfos fieldInfos) {
+    super(freqStream, tis, fieldInfos);
+    this.proxStreamOrig = proxStream;  // the proxStream will be cloned lazily when nextPosition() is called for the first time
+  }
+
+  @Override
+  final void seek(TermInfo ti, Term term) throws IOException {
+    super.seek(ti, term);
+    if (ti != null)
+      lazySkipPointer = ti.proxPointer;
+    
+    lazySkipProxCount = 0;
+    proxCount = 0;
+    payloadLength = 0;
+    needToLoadPayload = false;
+  }
+
+  @Override
+  public final void close() throws IOException {
+    super.close();
+    if (proxStream != null) proxStream.close();
+  }
+
+  public final int nextPosition() throws IOException {
+    if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)
+      // This field does not store positions, payloads
+      return 0;
+    // perform lazy skips if necessary
+    lazySkip();
+    proxCount--;
+    return position += readDeltaPosition();
+  }
+
+  private final int readDeltaPosition() throws IOException {
+    int delta = proxStream.readVInt();
+    if (currentFieldStoresPayloads) {
+      // if the current field stores payloads then
+      // the position delta is shifted one bit to the left.
+      // if the LSB is set, then we have to read the current
+      // payload length
+      if ((delta & 1) != 0) {
+        payloadLength = proxStream.readVInt();
+      } 
+      delta >>>= 1;
+      needToLoadPayload = true;
+    }
+    return delta;
+  }
+  
+  @Override
+  protected final void skippingDoc() throws IOException {
+    // we remember to skip a document lazily
+    lazySkipProxCount += freq;
+  }
+
+  @Override
+  public final boolean next() throws IOException {
+    // we remember to skip the remaining positions of the current
+    // document lazily
+    lazySkipProxCount += proxCount;
+    
+    if (super.next()) {               // run super
+      proxCount = freq;               // note frequency
+      position = 0;               // reset position
+      return true;
+    }
+    return false;
+  }
+
+  @Override
+  public final int read(final int[] docs, final int[] freqs) {
+    throw new UnsupportedOperationException("TermPositions does not support processing multiple documents in one call. Use TermDocs instead.");
+  }
+
+
+  /** Called by super.skipTo(). */
+  @Override
+  protected void skipProx(long proxPointer, int payloadLength) throws IOException {
+    // we save the pointer, we might have to skip there lazily
+    lazySkipPointer = proxPointer;
+    lazySkipProxCount = 0;
+    proxCount = 0;
+    this.payloadLength = payloadLength;
+    needToLoadPayload = false;
+  }
+
+  private void skipPositions(int n) throws IOException {
+    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+    for (int f = n; f > 0; f--) {        // skip unread positions
+      readDeltaPosition();
+      skipPayload();
+    }      
+  }
+  
+  private void skipPayload() throws IOException {
+    if (needToLoadPayload && payloadLength > 0) {
+      proxStream.seek(proxStream.getFilePointer() + payloadLength);
+    }
+    needToLoadPayload = false;
+  }
+
+  // It is not always necessary to move the prox pointer
+  // to a new document after the freq pointer has been moved.
+  // Consider for example a phrase query with two terms:
+  // the freq pointer for term 1 has to move to document x
+  // to answer the question if the term occurs in that document. But
+  // only if term 2 also matches document x, the positions have to be
+  // read to figure out if term 1 and term 2 appear next
+  // to each other in document x and thus satisfy the query.
+  // So we move the prox pointer lazily to the document
+  // as soon as positions are requested.
+  private void lazySkip() throws IOException {
+    if (proxStream == null) {
+      // clone lazily
+      proxStream = (IndexInput)proxStreamOrig.clone();
+    }
+    
+    // we might have to skip the current payload
+    // if it was not read yet
+    skipPayload();
+      
+    if (lazySkipPointer != -1) {
+      proxStream.seek(lazySkipPointer);
+      lazySkipPointer = -1;
+    }
+     
+    if (lazySkipProxCount != 0) {
+      skipPositions(lazySkipProxCount);
+      lazySkipProxCount = 0;
+    }
+  }
+  
+  public int getPayloadLength() {
+    return payloadLength;
+  }
+
+  public byte[] getPayload(byte[] data, int offset) throws IOException {
+    if (!needToLoadPayload) {
+      throw new IOException("Either no payload exists at this term position or an attempt was made to load it more than once.");
+    }
+
+    // read payloads lazily
+    byte[] retArray;
+    int retOffset;
+    if (data == null || data.length - offset < payloadLength) {
+      // the array is too small to store the payload data,
+      // so we allocate a new one
+      retArray = new byte[payloadLength];
+      retOffset = 0;
+    } else {
+      retArray = data;
+      retOffset = offset;
+    }
+    proxStream.readBytes(retArray, retOffset, payloadLength);
+    needToLoadPayload = false;
+    return retArray;
+  }
+
+  public boolean isPayloadAvailable() {
+    return needToLoadPayload && payloadLength > 0;
+  }
+
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermBuffer.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermBuffer.java
new file mode 100644
index 0000000..1ea4bc5
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermBuffer.java
@@ -0,0 +1,122 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.FieldInfos;
+
+/**
+ * @lucene.experimental
+ * @deprecated (4.0)
+ */
+@Deprecated
+final class TermBuffer implements Cloneable {
+
+  private String field;
+  private Term term;                            // cached
+
+  private BytesRef bytes = new BytesRef(10);
+
+  // Cannot be -1 since (strangely) we write that
+  // fieldNumber into index for first indexed term:
+  private int currentFieldNumber = -2;
+
+  private static final Comparator<BytesRef> utf8AsUTF16Comparator = BytesRef.getUTF8SortedAsUTF16Comparator();
+
+  int newSuffixStart;                             // only valid right after .read is called
+
+  public int compareTo(TermBuffer other) {
+    if (field == other.field) 	  // fields are interned
+                                  // (only by PreFlex codec)
+      return utf8AsUTF16Comparator.compare(bytes, other.bytes);
+    else
+      return field.compareTo(other.field);
+  }
+
+  public void read(IndexInput input, FieldInfos fieldInfos)
+    throws IOException {
+    this.term = null;                           // invalidate cache
+    newSuffixStart = input.readVInt();
+    int length = input.readVInt();
+    int totalLength = newSuffixStart + length;
+    if (bytes.bytes.length < totalLength) {
+      bytes.grow(totalLength);
+    }
+    bytes.length = totalLength;
+    input.readBytes(bytes.bytes, newSuffixStart, length);
+    final int fieldNumber = input.readVInt();
+    if (fieldNumber != currentFieldNumber) {
+      currentFieldNumber = fieldNumber;
+      field = fieldInfos.fieldName(currentFieldNumber).intern();
+    } else {
+      assert field.equals(fieldInfos.fieldName(fieldNumber)): "currentFieldNumber=" + currentFieldNumber + " field=" + field + " vs " + fieldInfos.fieldName(fieldNumber);
+    }
+  }
+
+  public void set(Term term) {
+    if (term == null) {
+      reset();
+      return;
+    }
+    bytes.copyBytes(term.bytes());
+    field = term.field().intern();
+    currentFieldNumber = -1;
+    this.term = term;
+  }
+
+  public void set(TermBuffer other) {
+    field = other.field;
+    currentFieldNumber = other.currentFieldNumber;
+    // dangerous to copy Term over, since the underlying
+    // BytesRef could subsequently be modified:
+    term = null;
+    bytes.copyBytes(other.bytes);
+  }
+
+  public void reset() {
+    field = null;
+    term = null;
+    currentFieldNumber=  -1;
+  }
+
+  public Term toTerm() {
+    if (field == null)                            // unset
+      return null;
+
+    if (term == null) {
+      term = new Term(field, BytesRef.deepCopyOf(bytes));
+    }
+
+    return term;
+  }
+
+  @Override
+  protected Object clone() {
+    TermBuffer clone = null;
+    try {
+      clone = (TermBuffer)super.clone();
+    } catch (CloneNotSupportedException e) {}
+    clone.bytes = BytesRef.deepCopyOf(bytes);
+    return clone;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfo.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfo.java
new file mode 100644
index 0000000..9b2efe5
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfo.java
@@ -0,0 +1,63 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** A TermInfo is the record of information stored for a
+ * term
+ * @deprecated (4.0) This class is no longer used in flexible
+ * indexing. */
+
+@Deprecated
+public class TermInfo {
+  /** The number of documents which contain the term. */
+  public int docFreq = 0;
+
+  public long freqPointer = 0;
+  public long proxPointer = 0;
+  public int skipOffset;
+
+  public TermInfo() {}
+
+  public TermInfo(int df, long fp, long pp) {
+    docFreq = df;
+    freqPointer = fp;
+    proxPointer = pp;
+  }
+
+  public TermInfo(TermInfo ti) {
+    docFreq = ti.docFreq;
+    freqPointer = ti.freqPointer;
+    proxPointer = ti.proxPointer;
+    skipOffset = ti.skipOffset;
+  }
+
+  public final void set(int docFreq,
+                 long freqPointer, long proxPointer, int skipOffset) {
+    this.docFreq = docFreq;
+    this.freqPointer = freqPointer;
+    this.proxPointer = proxPointer;
+    this.skipOffset = skipOffset;
+  }
+
+  public final void set(TermInfo ti) {
+    docFreq = ti.docFreq;
+    freqPointer = ti.freqPointer;
+    proxPointer = ti.proxPointer;
+    skipOffset = ti.skipOffset;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReader.java
new file mode 100644
index 0000000..73582d7
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReader.java
@@ -0,0 +1,347 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CloseableThreadLocal;
+import org.apache.lucene.util.DoubleBarrelLRUCache;
+
+/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
+ * Directory.  Pairs are accessed either by Term or by ordinal position the
+ * set
+ * @deprecated (4.0) This class has been replaced by
+ * FormatPostingsTermsDictReader, except for reading old segments. 
+ * @lucene.experimental
+ */
+@Deprecated
+public final class TermInfosReader {
+  private final Directory directory;
+  private final String segment;
+  private final FieldInfos fieldInfos;
+
+  private final CloseableThreadLocal<ThreadResources> threadResources = new CloseableThreadLocal<ThreadResources>();
+  private final SegmentTermEnum origEnum;
+  private final long size;
+
+  private final TermInfosReaderIndex index;
+  private final int indexLength;
+  
+  private final int totalIndexInterval;
+
+  private final static int DEFAULT_CACHE_SIZE = 1024;
+  
+  // Just adds term's ord to TermInfo
+  private final static class TermInfoAndOrd extends TermInfo {
+    final long termOrd;
+    public TermInfoAndOrd(TermInfo ti, long termOrd) {
+      super(ti);
+      assert termOrd >= 0;
+      this.termOrd = termOrd;
+    }
+  }
+
+  private static class CloneableTerm extends DoubleBarrelLRUCache.CloneableKey {
+    Term term;
+    public CloneableTerm(Term t) {
+      this.term = t;
+    }
+
+    @Override
+    public boolean equals(Object other) {
+      CloneableTerm t = (CloneableTerm) other;
+      return this.term.equals(t.term);
+    }
+
+    @Override
+    public int hashCode() {
+      return term.hashCode();
+    }
+
+    @Override
+    public Object clone() {
+      return new CloneableTerm(term);
+    }
+  }
+
+  private final DoubleBarrelLRUCache<CloneableTerm,TermInfoAndOrd> termsCache = new DoubleBarrelLRUCache<CloneableTerm,TermInfoAndOrd>(DEFAULT_CACHE_SIZE);
+
+  /**
+   * Per-thread resources managed by ThreadLocal
+   */
+  private static final class ThreadResources {
+    SegmentTermEnum termEnum;
+  }
+  
+  TermInfosReader(Directory dir, String seg, FieldInfos fis, IOContext context, int indexDivisor)
+       throws CorruptIndexException, IOException {
+    boolean success = false;
+
+    if (indexDivisor < 1 && indexDivisor != -1) {
+      throw new IllegalArgumentException("indexDivisor must be -1 (don't load terms index) or greater than 0: got " + indexDivisor);
+    }
+
+    try {
+      directory = dir;
+      segment = seg;
+      fieldInfos = fis;
+
+      origEnum = new SegmentTermEnum(directory.openInput(IndexFileNames.segmentFileName(segment, "", Lucene3xPostingsFormat.TERMS_EXTENSION),
+                                                         context), fieldInfos, false);
+      size = origEnum.size;
+
+
+      if (indexDivisor != -1) {
+        // Load terms index
+        totalIndexInterval = origEnum.indexInterval * indexDivisor;
+
+        final String indexFileName = IndexFileNames.segmentFileName(segment, "", Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION);
+        final SegmentTermEnum indexEnum = new SegmentTermEnum(directory.openInput(indexFileName,
+                                                                                   context), fieldInfos, true);
+
+        try {
+          index = new TermInfosReaderIndex(indexEnum, indexDivisor, dir.fileLength(indexFileName), totalIndexInterval);
+          indexLength = index.length();
+        } finally {
+          indexEnum.close();
+        }
+      } else {
+        // Do not load terms index:
+        totalIndexInterval = -1;
+        index = null;
+        indexLength = -1;
+      }
+      success = true;
+    } finally {
+      // With lock-less commits, it's entirely possible (and
+      // fine) to hit a FileNotFound exception above. In
+      // this case, we want to explicitly close any subset
+      // of things that were opened so that we don't have to
+      // wait for a GC to do so.
+      if (!success) {
+        close();
+      }
+    }
+  }
+
+  public int getSkipInterval() {
+    return origEnum.skipInterval;
+  }
+  
+  public int getMaxSkipLevels() {
+    return origEnum.maxSkipLevels;
+  }
+
+  void close() throws IOException {
+    if (origEnum != null)
+      origEnum.close();
+    threadResources.close();
+  }
+
+  /** Returns the number of term/value pairs in the set. */
+  long size() {
+    return size;
+  }
+
+  private ThreadResources getThreadResources() {
+    ThreadResources resources = threadResources.get();
+    if (resources == null) {
+      resources = new ThreadResources();
+      resources.termEnum = terms();
+      threadResources.set(resources);
+    }
+    return resources;
+  }
+  
+  private static final Comparator<BytesRef> legacyComparator = 
+    BytesRef.getUTF8SortedAsUTF16Comparator();
+
+  private final int compareAsUTF16(Term term1, Term term2) {
+    if (term1.field().equals(term2.field())) {
+      return legacyComparator.compare(term1.bytes(), term2.bytes());
+    } else {
+      return term1.field().compareTo(term2.field());
+    }
+  }
+
+  /** Returns the TermInfo for a Term in the set, or null. */
+  TermInfo get(Term term) throws IOException {
+    return get(term, false);
+  }
+  
+  /** Returns the TermInfo for a Term in the set, or null. */
+  private TermInfo get(Term term, boolean mustSeekEnum) throws IOException {
+    if (size == 0) return null;
+
+    ensureIndexIsRead();
+    TermInfoAndOrd tiOrd = termsCache.get(new CloneableTerm(term));
+    ThreadResources resources = getThreadResources();
+
+    if (!mustSeekEnum && tiOrd != null) {
+      return tiOrd;
+    }
+
+    return seekEnum(resources.termEnum, term, tiOrd, true);
+  }
+
+  public void cacheCurrentTerm(SegmentTermEnum enumerator) {
+    termsCache.put(new CloneableTerm(enumerator.term()),
+                   new TermInfoAndOrd(enumerator.termInfo,
+                                      enumerator.position));
+  }
+
+  TermInfo seekEnum(SegmentTermEnum enumerator, Term term, boolean useCache) throws IOException {
+    if (useCache) {
+      return seekEnum(enumerator, term, termsCache.get(new CloneableTerm(term)), useCache);
+    } else {
+      return seekEnum(enumerator, term, null, useCache);
+    }
+  }
+
+  TermInfo seekEnum(SegmentTermEnum enumerator, Term term, TermInfoAndOrd tiOrd, boolean useCache) throws IOException {
+    if (size == 0) {
+      return null;
+    }
+
+    // optimize sequential access: first try scanning cached enum w/o seeking
+    if (enumerator.term() != null                 // term is at or past current
+  && ((enumerator.prev() != null && compareAsUTF16(term, enumerator.prev())> 0)
+      || compareAsUTF16(term, enumerator.term()) >= 0)) {
+      int enumOffset = (int)(enumerator.position/totalIndexInterval)+1;
+      if (indexLength == enumOffset    // but before end of block
+    || index.compareTo(term, enumOffset) < 0) {
+       // no need to seek
+
+        final TermInfo ti;
+        int numScans = enumerator.scanTo(term);
+        if (enumerator.term() != null && compareAsUTF16(term, enumerator.term()) == 0) {
+          ti = enumerator.termInfo;
+          if (numScans > 1) {
+            // we only  want to put this TermInfo into the cache if
+            // scanEnum skipped more than one dictionary entry.
+            // This prevents RangeQueries or WildcardQueries to 
+            // wipe out the cache when they iterate over a large numbers
+            // of terms in order
+            if (tiOrd == null) {
+              if (useCache) {
+                termsCache.put(new CloneableTerm(term), new TermInfoAndOrd(ti, enumerator.position));
+              }
+            } else {
+              assert sameTermInfo(ti, tiOrd, enumerator);
+              assert (int) enumerator.position == tiOrd.termOrd;
+            }
+          }
+        } else {
+          ti = null;
+        }
+
+        return ti;
+      }  
+    }
+
+    // random-access: must seek
+    final int indexPos;
+    if (tiOrd != null) {
+      indexPos = (int) (tiOrd.termOrd / totalIndexInterval);
+    } else {
+      // Must do binary search:
+      indexPos = index.getIndexOffset(term);
+    }
+
+    index.seekEnum(enumerator, indexPos);
+    enumerator.scanTo(term);
+    final TermInfo ti;
+
+    if (enumerator.term() != null && compareAsUTF16(term, enumerator.term()) == 0) {
+      ti = enumerator.termInfo;
+      if (tiOrd == null) {
+        if (useCache) {
+          termsCache.put(new CloneableTerm(term), new TermInfoAndOrd(ti, enumerator.position));
+        }
+      } else {
+        assert sameTermInfo(ti, tiOrd, enumerator);
+        assert enumerator.position == tiOrd.termOrd;
+      }
+    } else {
+      ti = null;
+    }
+    return ti;
+  }
+
+  // called only from asserts
+  private boolean sameTermInfo(TermInfo ti1, TermInfo ti2, SegmentTermEnum enumerator) {
+    if (ti1.docFreq != ti2.docFreq) {
+      return false;
+    }
+    if (ti1.freqPointer != ti2.freqPointer) {
+      return false;
+    }
+    if (ti1.proxPointer != ti2.proxPointer) {
+      return false;
+    }
+    // skipOffset is only valid when docFreq >= skipInterval:
+    if (ti1.docFreq >= enumerator.skipInterval &&
+        ti1.skipOffset != ti2.skipOffset) {
+      return false;
+    }
+    return true;
+  }
+
+  private void ensureIndexIsRead() {
+    if (index == null) {
+      throw new IllegalStateException("terms index was not loaded when this reader was created");
+    }
+  }
+
+  /** Returns the position of a Term in the set or -1. */
+  long getPosition(Term term) throws IOException {
+    if (size == 0) return -1;
+
+    ensureIndexIsRead();
+    int indexOffset = index.getIndexOffset(term);
+    
+    SegmentTermEnum enumerator = getThreadResources().termEnum;
+    index.seekEnum(enumerator, indexOffset);
+
+    while(compareAsUTF16(term, enumerator.term()) > 0 && enumerator.next()) {}
+
+    if (compareAsUTF16(term, enumerator.term()) == 0)
+      return enumerator.position;
+    else
+      return -1;
+  }
+
+  /** Returns an enumeration of all the Terms and TermInfos in the set. */
+  public SegmentTermEnum terms() {
+    return (SegmentTermEnum)origEnum.clone();
+  }
+
+  /** Returns an enumeration of terms starting at or after the named term. */
+  public SegmentTermEnum terms(Term term) throws IOException {
+    get(term, true);
+    return (SegmentTermEnum)getThreadResources().termEnum.clone();
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex.java b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex.java
new file mode 100644
index 0000000..e9d7130
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/TermInfosReaderIndex.java
@@ -0,0 +1,252 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.util.BitUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.PagedBytes.PagedBytesDataInput;
+import org.apache.lucene.util.PagedBytes.PagedBytesDataOutput;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.packed.GrowableWriter;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * This stores a monotonically increasing set of <Term, TermInfo> pairs in an
+ * index segment. Pairs are accessed either by Term or by ordinal position the
+ * set. The Terms and TermInfo are actually serialized and stored into a byte
+ * array and pointers to the position of each are stored in a int array.
+ */
+class TermInfosReaderIndex {
+
+  private static final int MAX_PAGE_BITS = 18; // 256 KB block
+  private Term[] fields;
+  private int totalIndexInterval;
+  private Comparator<BytesRef> comparator = BytesRef.getUTF8SortedAsUTF16Comparator();
+  private final PagedBytesDataInput dataInput;
+  private final PackedInts.Reader indexToDataOffset;
+  private final int indexSize;
+  private final int skipInterval;
+
+  /**
+   * Loads the segment information at segment load time.
+   * 
+   * @param indexEnum
+   *          the term enum.
+   * @param indexDivisor
+   *          the index divisor.
+   * @param tiiFileLength
+   *          the size of the tii file, used to approximate the size of the
+   *          buffer.
+   * @param totalIndexInterval
+   *          the total index interval.
+   */
+  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {
+    this.totalIndexInterval = totalIndexInterval;
+    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;
+    skipInterval = indexEnum.skipInterval;
+    // this is only an inital size, it will be GCed once the build is complete
+    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;
+    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));
+    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();
+
+    GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false);
+    String currentField = null;
+    List<String> fieldStrs = new ArrayList<String>();
+    int fieldCounter = -1;
+    for (int i = 0; indexEnum.next(); i++) {
+      Term term = indexEnum.term();
+      if (currentField == null || !currentField.equals(term.field())) {
+        currentField = term.field();
+        fieldStrs.add(currentField);
+        fieldCounter++;
+      }
+      TermInfo termInfo = indexEnum.termInfo();
+      indexToTerms.set(i, dataOutput.getPosition());
+      dataOutput.writeVInt(fieldCounter);
+      dataOutput.writeString(term.text());
+      dataOutput.writeVInt(termInfo.docFreq);
+      if (termInfo.docFreq >= skipInterval) {
+        dataOutput.writeVInt(termInfo.skipOffset);
+      }
+      dataOutput.writeVLong(termInfo.freqPointer);
+      dataOutput.writeVLong(termInfo.proxPointer);
+      dataOutput.writeVLong(indexEnum.indexPointer);
+      for (int j = 1; j < indexDivisor; j++) {
+        if (!indexEnum.next()) {
+          break;
+        }
+      }
+    }
+
+    fields = new Term[fieldStrs.size()];
+    for (int i = 0; i < fields.length; i++) {
+      fields[i] = new Term(fieldStrs.get(i));
+    }
+    
+    dataPagedBytes.freeze(true);
+    dataInput = dataPagedBytes.getDataInput();
+    indexToDataOffset = indexToTerms.getMutable();
+  }
+
+  private static int estimatePageBits(long estSize) {
+    return Math.max(Math.min(64 - BitUtil.nlz(estSize), MAX_PAGE_BITS), 4);
+  }
+
+  void seekEnum(SegmentTermEnum enumerator, int indexOffset) throws IOException {
+    PagedBytesDataInput input = (PagedBytesDataInput) dataInput.clone();
+    
+    input.setPosition(indexToDataOffset.get(indexOffset));
+
+    // read the term
+    int fieldId = input.readVInt();
+    Term field = fields[fieldId];
+    Term term = new Term(field.field(), input.readString());
+
+    // read the terminfo
+    TermInfo termInfo = new TermInfo();
+    termInfo.docFreq = input.readVInt();
+    if (termInfo.docFreq >= skipInterval) {
+      termInfo.skipOffset = input.readVInt();
+    } else {
+      termInfo.skipOffset = 0;
+    }
+    termInfo.freqPointer = input.readVLong();
+    termInfo.proxPointer = input.readVLong();
+
+    long pointer = input.readVLong();
+
+    // perform the seek
+    enumerator.seek(pointer, ((long) indexOffset * totalIndexInterval) - 1, term, termInfo);
+  }
+
+  /**
+   * Binary search for the given term.
+   * 
+   * @param term
+   *          the term to locate.
+   * @throws IOException 
+   */
+  int getIndexOffset(Term term) throws IOException {
+    int lo = 0;
+    int hi = indexSize - 1;
+    PagedBytesDataInput input = (PagedBytesDataInput) dataInput.clone();
+    BytesRef scratch = new BytesRef();
+    while (hi >= lo) {
+      int mid = (lo + hi) >>> 1;
+      int delta = compareTo(term, mid, input, scratch);
+      if (delta < 0)
+        hi = mid - 1;
+      else if (delta > 0)
+        lo = mid + 1;
+      else
+        return mid;
+    }
+    return hi;
+  }
+
+  /**
+   * Gets the term at the given position.  For testing.
+   * 
+   * @param termIndex
+   *          the position to read the term from the index.
+   * @return the term.
+   * @throws IOException
+   */
+  Term getTerm(int termIndex) throws IOException {
+    PagedBytesDataInput input = (PagedBytesDataInput) dataInput.clone();
+    input.setPosition(indexToDataOffset.get(termIndex));
+
+    // read the term
+    int fieldId = input.readVInt();
+    Term field = fields[fieldId];
+    return new Term(field.field(), input.readString());
+  }
+
+  /**
+   * Returns the number of terms.
+   * 
+   * @return int.
+   */
+  int length() {
+    return indexSize;
+  }
+
+  /**
+   * The compares the given term against the term in the index specified by the
+   * term index. ie It returns negative N when term is less than index term;
+   * 
+   * @param term
+   *          the given term.
+   * @param termIndex
+   *          the index of the of term to compare.
+   * @return int.
+   * @throws IOException 
+   */
+  int compareTo(Term term, int termIndex) throws IOException {
+    return compareTo(term, termIndex, (PagedBytesDataInput) dataInput.clone(), new BytesRef());
+  }
+
+  /**
+   * Compare the fields of the terms first, and if not equals return from
+   * compare. If equal compare terms.
+   * 
+   * @param term
+   *          the term to compare.
+   * @param termIndex
+   *          the position of the term in the input to compare
+   * @param input
+   *          the input buffer.
+   * @return int.
+   * @throws IOException 
+   */
+  private int compareTo(Term term, int termIndex, PagedBytesDataInput input, BytesRef reuse) throws IOException {
+    // if term field does not equal mid's field index, then compare fields
+    // else if they are equal, compare term's string values...
+    int c = compareField(term, termIndex, input);
+    if (c == 0) {
+      reuse.length = input.readVInt();
+      reuse.grow(reuse.length);
+      input.readBytes(reuse.bytes, 0, reuse.length);
+      return comparator.compare(term.bytes(), reuse);
+    }
+    return c;
+  }
+
+  /**
+   * Compares the fields before checking the text of the terms.
+   * 
+   * @param term
+   *          the given term.
+   * @param termIndex
+   *          the term that exists in the data block.
+   * @param input
+   *          the data block.
+   * @return int.
+   * @throws IOException 
+   */
+  private int compareField(Term term, int termIndex, PagedBytesDataInput input) throws IOException {
+    input.setPosition(indexToDataOffset.get(termIndex));
+    return term.field().compareTo(fields[input.readVInt()].field());
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene3x/package.html b/lucene/src/java/org/apache/lucene/codecs/lucene3x/package.html
new file mode 100644
index 0000000..c6c96c9
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene3x/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Preflex codec: supports Lucene 3.x indexes (readonly)
+</body>
+</html>
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
new file mode 100644
index 0000000..b0e9626
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40Codec.java
@@ -0,0 +1,101 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
+
+/**
+ * Implements the Lucene 4.0 index format, with configurable per-field postings formats.
+ *
+ * @lucene.experimental
+ */
+// NOTE: if we make largish changes in a minor release, easier to just make Lucene42Codec or whatever
+// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
+// (it writes a minor version, etc).
+public class Lucene40Codec extends Codec {
+  private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat();
+  private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
+  private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
+  private final DocValuesFormat docValuesFormat = new Lucene40DocValuesFormat();
+  private final SegmentInfosFormat infosFormat = new Lucene40SegmentInfosFormat();
+  private final NormsFormat normsFormat = new Lucene40NormsFormat();
+  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
+    @Override
+    public PostingsFormat getPostingsFormatForField(String field) {
+      return Lucene40Codec.this.getPostingsFormatForField(field);
+    }
+  };
+
+  public Lucene40Codec() {
+    super("Lucene40");
+  }
+  
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return fieldsFormat;
+  }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValuesFormat;
+  }
+
+  @Override
+  public PostingsFormat postingsFormat() {
+    return postingsFormat;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+  
+  @Override
+  public SegmentInfosFormat segmentInfosFormat() {
+    return infosFormat;
+  }
+
+  @Override
+  public NormsFormat normsFormat() {
+    return normsFormat;
+  }
+
+  /** Returns the postings format that should be used for writing 
+   *  new segments of <code>field</code>.
+   *  
+   *  The default implementation always returns "Lucene40"
+   */
+  public PostingsFormat getPostingsFormatForField(String field) {
+    return defaultFormat;
+  }
+  
+  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene40");
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesConsumer.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesConsumer.java
new file mode 100644
index 0000000..a58e60d
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesConsumer.java
@@ -0,0 +1,78 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.DocValuesWriterBase;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.store.Directory;
+
+/**
+ * Default PerDocConsumer implementation that uses compound file.
+ * @lucene.experimental
+ */
+public class Lucene40DocValuesConsumer extends DocValuesWriterBase {
+  private final Directory mainDirectory;
+  private Directory directory;
+
+  final static String DOC_VALUES_SEGMENT_SUFFIX = "dv";
+  
+  public Lucene40DocValuesConsumer(PerDocWriteState state) throws IOException {
+    super(state);
+    mainDirectory = state.directory;
+    //TODO maybe we should enable a global CFS that all codecs can pull on demand to further reduce the number of files?
+  }
+  
+  @Override
+  protected Directory getDirectory() throws IOException {
+    // lazy init
+    if (directory == null) {
+      directory = new CompoundFileDirectory(mainDirectory,
+                                            IndexFileNames.segmentFileName(segmentName, DOC_VALUES_SEGMENT_SUFFIX,
+                                                                           IndexFileNames.COMPOUND_FILE_EXTENSION), context, true);
+    }
+    return directory;
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (directory != null) {
+      directory.close();
+    }
+  }
+
+  public static void files(Directory dir, SegmentInfo segmentInfo, Set<String> files) throws IOException {
+    FieldInfos fieldInfos = segmentInfo.getFieldInfos();
+    for (FieldInfo fieldInfo : fieldInfos) {
+      if (fieldInfo.hasDocValues()) {
+        files.add(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_EXTENSION));
+        files.add(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
+        assert dir.fileExists(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION)); 
+        assert dir.fileExists(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_EXTENSION)); 
+        break;
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java
new file mode 100644
index 0000000..8bb5ab7
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesFormat.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.PerDocConsumer;
+import org.apache.lucene.codecs.PerDocProducer;
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.Directory;
+
+public class Lucene40DocValuesFormat extends DocValuesFormat {
+
+  @Override
+  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
+    return new Lucene40DocValuesConsumer(state);
+  }
+
+  @Override
+  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
+    return new Lucene40DocValuesProducer(state);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    Lucene40DocValuesConsumer.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesProducer.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesProducer.java
new file mode 100644
index 0000000..7cdabe6
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40DocValuesProducer.java
@@ -0,0 +1,75 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.DocValuesReaderBase;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Default PerDocProducer implementation that uses compound file.
+ * @lucene.experimental
+ */
+public class Lucene40DocValuesProducer extends DocValuesReaderBase {
+  protected final TreeMap<String,DocValues> docValues;
+  private final Directory cfs;
+
+  /**
+   * Creates a new {@link Lucene40DocValuesProducer} instance and loads all
+   * {@link DocValues} instances for this segment and codec.
+   */
+  public Lucene40DocValuesProducer(SegmentReadState state) throws IOException {
+    if (state.fieldInfos.anyDocValuesFields()) {
+      cfs = new CompoundFileDirectory(state.dir, 
+                                      IndexFileNames.segmentFileName(state.segmentInfo.name,
+                                                                     Lucene40DocValuesConsumer.DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_EXTENSION), 
+                                      state.context, false);
+      docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.docCount, cfs, state.context);
+    } else {
+      cfs = null;
+      docValues = new TreeMap<String,DocValues>();
+    }
+  }
+  
+  @Override
+  protected Map<String,DocValues> docValues() {
+    return docValues;
+  }
+
+  @Override
+  protected void closeInternal(Collection<? extends Closeable> closeables) throws IOException {
+    if (cfs != null) {
+      final ArrayList<Closeable> list = new ArrayList<Closeable>(closeables);
+      list.add(cfs);
+      IOUtils.close(list);
+    } else {
+      IOUtils.close(closeables);
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java
new file mode 100644
index 0000000..072a0c4
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosFormat.java
@@ -0,0 +1,50 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+
+/**
+ * @lucene.experimental
+ */
+public class Lucene40FieldInfosFormat extends FieldInfosFormat {
+  private final FieldInfosReader reader = new Lucene40FieldInfosReader();
+  private final FieldInfosWriter writer = new Lucene40FieldInfosWriter();
+  
+  @Override
+  public FieldInfosReader getFieldInfosReader() throws IOException {
+    return reader;
+  }
+
+  @Override
+  public FieldInfosWriter getFieldInfosWriter() throws IOException {
+    return writer;
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    Lucene40FieldInfosReader.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java
new file mode 100644
index 0000000..97d545e
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosReader.java
@@ -0,0 +1,167 @@
+package org.apache.lucene.codecs.lucene40;
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexFormatTooNewException;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @lucene.experimental
+ */
+public class Lucene40FieldInfosReader extends FieldInfosReader {
+
+  static final int FORMAT_MINIMUM = Lucene40FieldInfosWriter.FORMAT_START;
+
+  @Override
+  public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene40FieldInfosWriter.FIELD_INFOS_EXTENSION);
+    IndexInput input = directory.openInput(fileName, iocontext);
+
+    boolean hasVectors = false;
+    boolean hasFreq = false;
+    boolean hasProx = false;
+    
+    try {
+      final int format = input.readVInt();
+
+      if (format > FORMAT_MINIMUM) {
+        throw new IndexFormatTooOldException(input, format, FORMAT_MINIMUM, Lucene40FieldInfosWriter.FORMAT_CURRENT);
+      }
+      if (format < Lucene40FieldInfosWriter.FORMAT_CURRENT) {
+        throw new IndexFormatTooNewException(input, format, FORMAT_MINIMUM, Lucene40FieldInfosWriter.FORMAT_CURRENT);
+      }
+
+      final int size = input.readVInt(); //read in the size
+      FieldInfo infos[] = new FieldInfo[size];
+
+      for (int i = 0; i < size; i++) {
+        String name = input.readString();
+        final int fieldNumber = format <= Lucene40FieldInfosWriter.FORMAT_FLEX? input.readInt():i;
+        byte bits = input.readByte();
+        boolean isIndexed = (bits & Lucene40FieldInfosWriter.IS_INDEXED) != 0;
+        boolean storeTermVector = (bits & Lucene40FieldInfosWriter.STORE_TERMVECTOR) != 0;
+        boolean storePositionsWithTermVector = (bits & Lucene40FieldInfosWriter.STORE_POSITIONS_WITH_TERMVECTOR) != 0;
+        boolean storeOffsetWithTermVector = (bits & Lucene40FieldInfosWriter.STORE_OFFSET_WITH_TERMVECTOR) != 0;
+        boolean omitNorms = (bits & Lucene40FieldInfosWriter.OMIT_NORMS) != 0;
+        boolean storePayloads = (bits & Lucene40FieldInfosWriter.STORE_PAYLOADS) != 0;
+        final IndexOptions indexOptions;
+        if ((bits & Lucene40FieldInfosWriter.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
+          indexOptions = IndexOptions.DOCS_ONLY;
+        } else if ((bits & Lucene40FieldInfosWriter.OMIT_POSITIONS) != 0) {
+          if (format <= Lucene40FieldInfosWriter.FORMAT_OMIT_POSITIONS) {
+            indexOptions = IndexOptions.DOCS_AND_FREQS;
+          } else {
+            throw new CorruptIndexException("Corrupt fieldinfos, OMIT_POSITIONS set but format=" + format + " (resource: " + input + ")");
+          }
+        } else {
+          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+        }
+
+        // LUCENE-3027: past indices were able to write
+        // storePayloads=true when omitTFAP is also true,
+        // which is invalid.  We correct that, here:
+        if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+          storePayloads = false;
+        }
+        hasVectors |= storeTermVector;
+        hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
+        DocValues.Type docValuesType = null;
+        if (format <= Lucene40FieldInfosWriter.FORMAT_FLEX) {
+          final byte b = input.readByte();
+          switch(b) {
+            case 0:
+              docValuesType = null;
+              break;
+            case 1:
+              docValuesType = DocValues.Type.VAR_INTS;
+              break;
+            case 2:
+              docValuesType = DocValues.Type.FLOAT_32;
+              break;
+            case 3:
+              docValuesType = DocValues.Type.FLOAT_64;
+              break;
+            case 4:
+              docValuesType = DocValues.Type.BYTES_FIXED_STRAIGHT;
+              break;
+            case 5:
+              docValuesType = DocValues.Type.BYTES_FIXED_DEREF;
+              break;
+            case 6:
+              docValuesType = DocValues.Type.BYTES_VAR_STRAIGHT;
+              break;
+            case 7:
+              docValuesType = DocValues.Type.BYTES_VAR_DEREF;
+              break;
+            case 8:
+              docValuesType = DocValues.Type.FIXED_INTS_16;
+              break;
+            case 9:
+              docValuesType = DocValues.Type.FIXED_INTS_32;
+              break;
+            case 10:
+              docValuesType = DocValues.Type.FIXED_INTS_64;
+              break;
+            case 11:
+              docValuesType = DocValues.Type.FIXED_INTS_8;
+              break;
+            case 12:
+              docValuesType = DocValues.Type.BYTES_FIXED_SORTED;
+              break;
+            case 13:
+              docValuesType = DocValues.Type.BYTES_VAR_SORTED;
+              break;
+        
+            default:
+              throw new IllegalStateException("unhandled indexValues type " + b);
+          }
+        }
+        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
+          storePositionsWithTermVector, storeOffsetWithTermVector, 
+          omitNorms, storePayloads, indexOptions, docValuesType);
+      }
+
+      if (input.getFilePointer() != input.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
+      }
+      
+      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
+    } finally {
+      input.close();
+    }
+  }
+  
+  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene40FieldInfosWriter.FIELD_INFOS_EXTENSION));
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java
new file mode 100644
index 0000000..a3a9668
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40FieldInfosWriter.java
@@ -0,0 +1,137 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * @lucene.experimental
+ */
+public class Lucene40FieldInfosWriter extends FieldInfosWriter {
+  
+  /** Extension of field infos */
+  static final String FIELD_INFOS_EXTENSION = "fnm";
+  
+  // First used in 2.9; prior to 2.9 there was no format header
+  static final int FORMAT_START = -2;
+  // First used in 3.4: omit only positional information
+  static final int FORMAT_OMIT_POSITIONS = -3;
+  // per-field codec support, records index values for fields
+  static final int FORMAT_FLEX = -4;
+
+  // whenever you add a new format, make it 1 smaller (negative version logic)!
+  static final int FORMAT_CURRENT = FORMAT_FLEX;
+  
+  static final byte IS_INDEXED = 0x1;
+  static final byte STORE_TERMVECTOR = 0x2;
+  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x4;
+  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x8;
+  static final byte OMIT_NORMS = 0x10;
+  static final byte STORE_PAYLOADS = 0x20;
+  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
+  static final byte OMIT_POSITIONS = -128;
+  
+  @Override
+  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
+    IndexOutput output = directory.createOutput(fileName, context);
+    try {
+      output.writeVInt(FORMAT_CURRENT);
+      output.writeVInt(infos.size());
+      for (FieldInfo fi : infos) {
+        assert fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !fi.storePayloads;
+        byte bits = 0x0;
+        if (fi.isIndexed) bits |= IS_INDEXED;
+        if (fi.storeTermVector) bits |= STORE_TERMVECTOR;
+        if (fi.storePositionWithTermVector) bits |= STORE_POSITIONS_WITH_TERMVECTOR;
+        if (fi.storeOffsetWithTermVector) bits |= STORE_OFFSET_WITH_TERMVECTOR;
+        if (fi.omitNorms) bits |= OMIT_NORMS;
+        if (fi.storePayloads) bits |= STORE_PAYLOADS;
+        if (fi.indexOptions == IndexOptions.DOCS_ONLY)
+          bits |= OMIT_TERM_FREQ_AND_POSITIONS;
+        else if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS)
+          bits |= OMIT_POSITIONS;
+        output.writeString(fi.name);
+        output.writeInt(fi.number);
+        output.writeByte(bits);
+
+        final byte b;
+
+        if (!fi.hasDocValues()) {
+          b = 0;
+        } else {
+          switch(fi.getDocValuesType()) {
+          case VAR_INTS:
+            b = 1;
+            break;
+          case FLOAT_32:
+            b = 2;
+            break;
+          case FLOAT_64:
+            b = 3;
+            break;
+          case BYTES_FIXED_STRAIGHT:
+            b = 4;
+            break;
+          case BYTES_FIXED_DEREF:
+            b = 5;
+            break;
+          case BYTES_VAR_STRAIGHT:
+            b = 6;
+            break;
+          case BYTES_VAR_DEREF:
+            b = 7;
+            break;
+          case FIXED_INTS_16:
+            b = 8;
+            break;
+          case FIXED_INTS_32:
+            b = 9;
+            break;
+          case FIXED_INTS_64:
+            b = 10;
+            break;
+          case FIXED_INTS_8:
+            b = 11;
+            break;
+          case BYTES_FIXED_SORTED:
+            b = 12;
+            break;
+          case BYTES_VAR_SORTED:
+            b = 13;
+            break;
+          default:
+            throw new IllegalStateException("unhandled indexValues type " + fi.getDocValuesType());
+          }
+        }
+        output.writeByte(b);
+      }
+    } finally {
+      output.close();
+    }
+  }
+  
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
new file mode 100644
index 0000000..776c0e4
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsFormat.java
@@ -0,0 +1,53 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.NormsReader;
+import org.apache.lucene.codecs.NormsWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+public class Lucene40NormsFormat extends NormsFormat {
+
+  @Override
+  public NormsReader normsReader(Directory dir, SegmentInfo info, FieldInfos fields, IOContext context, Directory separateNormsDir) throws IOException {
+    return new Lucene40NormsReader(dir, info, fields, context, separateNormsDir);
+  }
+
+  @Override
+  public NormsWriter normsWriter(SegmentWriteState state) throws IOException {
+    return new Lucene40NormsWriter(state.directory, state.segmentName, state.context);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    Lucene40NormsReader.files(dir, info, files);
+  }
+
+  @Override
+  public void separateFiles(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    Lucene40NormsReader.separateFiles(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java
new file mode 100644
index 0000000..289ee6e
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsReader.java
@@ -0,0 +1,196 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.IdentityHashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.Map.Entry;
+
+import org.apache.lucene.codecs.NormsReader;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.MapBackedSet;
+import org.apache.lucene.util.StringHelper;
+
+public class Lucene40NormsReader extends NormsReader {
+  // this would be replaced by Source/SourceCache in a dv impl.
+  // for now we have our own mini-version
+  final Map<String,Norm> norms = new HashMap<String,Norm>();
+  // any .nrm or .sNN files we have open at any time.
+  // TODO: just a list, and double-close() separate norms files?
+  final Set<IndexInput> openFiles = new MapBackedSet<IndexInput>(new IdentityHashMap<IndexInput,Boolean>());
+  // points to a singleNormFile
+  IndexInput singleNormStream;
+  final int maxdoc;
+  
+  // note: just like segmentreader in 3.x, we open up all the files here (including separate norms) up front.
+  // but we just don't do any seeks or reading yet.
+  public Lucene40NormsReader(Directory dir, SegmentInfo info, FieldInfos fields, IOContext context, Directory separateNormsDir) throws IOException {
+    maxdoc = info.docCount;
+    String segmentName = info.name;
+    Map<Integer,Long> normGen = info.getNormGen();
+    boolean success = false;
+    try {
+      long nextNormSeek = Lucene40NormsWriter.NORMS_HEADER.length; //skip header (header unused for now)
+      for (FieldInfo fi : fields) {
+        if (fi.isIndexed && !fi.omitNorms) {
+          String fileName = getNormFilename(segmentName, normGen, fi.number);
+          Directory d = hasSeparateNorms(normGen, fi.number) ? separateNormsDir : dir;
+        
+          // singleNormFile means multiple norms share this file
+          boolean singleNormFile = IndexFileNames.matchesExtension(fileName, Lucene40NormsWriter.NORMS_EXTENSION);
+          IndexInput normInput = null;
+          long normSeek;
+
+          if (singleNormFile) {
+            normSeek = nextNormSeek;
+            if (singleNormStream == null) {
+              singleNormStream = d.openInput(fileName, context);
+              openFiles.add(singleNormStream);
+            }
+            // All norms in the .nrm file can share a single IndexInput since
+            // they are only used in a synchronized context.
+            // If this were to change in the future, a clone could be done here.
+            normInput = singleNormStream;
+          } else {
+            normInput = d.openInput(fileName, context);
+            openFiles.add(normInput);
+            // if the segment was created in 3.2 or after, we wrote the header for sure,
+            // and don't need to do the sketchy file size check. otherwise, we check 
+            // if the size is exactly equal to maxDoc to detect a headerless file.
+            // NOTE: remove this check in Lucene 5.0!
+            String version = info.getVersion();
+            final boolean isUnversioned = 
+                (version == null || StringHelper.getVersionComparator().compare(version, "3.2") < 0)
+                && normInput.length() == maxdoc;
+            if (isUnversioned) {
+              normSeek = 0;
+            } else {
+              normSeek = Lucene40NormsWriter.NORMS_HEADER.length;
+            }
+          }
+
+          Norm norm = new Norm();
+          norm.file = normInput;
+          norm.offset = normSeek;
+          norms.put(fi.name, norm);
+          nextNormSeek += maxdoc; // increment also if some norms are separate
+        }
+      }
+      // TODO: change to a real check? see LUCENE-3619
+      assert singleNormStream == null || nextNormSeek == singleNormStream.length();
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(openFiles);
+      }
+    }
+  }
+  
+  @Override
+  public byte[] norms(String name) throws IOException {
+    Norm norm = norms.get(name);
+    return norm == null ? null : norm.bytes();
+  }
+  
+
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(openFiles);
+    } finally {
+      norms.clear();
+      openFiles.clear();
+    }
+  }
+  
+  private static String getNormFilename(String segmentName, Map<Integer,Long> normGen, int number) {
+    if (hasSeparateNorms(normGen, number)) {
+      return IndexFileNames.fileNameFromGeneration(segmentName, Lucene40NormsWriter.SEPARATE_NORMS_EXTENSION + number, normGen.get(number));
+    } else {
+      // single file for all norms
+      return IndexFileNames.fileNameFromGeneration(segmentName, Lucene40NormsWriter.NORMS_EXTENSION, SegmentInfo.WITHOUT_GEN);
+    }
+  }
+  
+  private static boolean hasSeparateNorms(Map<Integer,Long> normGen, int number) {
+    if (normGen == null) {
+      return false;
+    }
+
+    Long gen = normGen.get(number);
+    return gen != null && gen.longValue() != SegmentInfo.NO;
+  }
+  
+  class Norm {
+    IndexInput file;
+    long offset;
+    byte bytes[];
+    
+    synchronized byte[] bytes() throws IOException {
+      if (bytes == null) {
+        bytes = new byte[maxdoc];
+        // some norms share fds
+        synchronized(file) {
+          file.seek(offset);
+          file.readBytes(bytes, 0, bytes.length, false);
+        }
+        // we are done with this file
+        if (file != singleNormStream) {
+          openFiles.remove(file);
+          file.close();
+          file = null;
+        }
+      }
+      return bytes;
+    }
+  }
+  
+  static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    // TODO: This is what SI always did... but we can do this cleaner?
+    // like first FI that has norms but doesn't have separate norms?
+    final String normsFileName = IndexFileNames.segmentFileName(info.name, "", Lucene40NormsWriter.NORMS_EXTENSION);
+    if (dir.fileExists(normsFileName)) {
+      files.add(normsFileName);
+    }
+  }
+  
+  /** @deprecated */
+  @Deprecated
+  static void separateFiles(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    Map<Integer,Long> normGen = info.getNormGen();
+    if (normGen != null) {
+      for (Entry<Integer,Long> entry : normGen.entrySet()) {
+        long gen = entry.getValue();
+        if (gen >= SegmentInfo.YES) {
+          // Definitely a separate norm file, with generation:
+          files.add(IndexFileNames.fileNameFromGeneration(info.name, Lucene40NormsWriter.SEPARATE_NORMS_EXTENSION + entry.getKey(), gen));
+        }
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsWriter.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsWriter.java
new file mode 100644
index 0000000..0903d90
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40NormsWriter.java
@@ -0,0 +1,130 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.codecs.NormsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.IOUtils;
+
+public class Lucene40NormsWriter extends NormsWriter {
+  private IndexOutput out;
+  private int normCount = 0;
+  
+  /** norms header placeholder */
+  static final byte[] NORMS_HEADER = new byte[]{'N','R','M',-1};
+  
+  /** Extension of norms file */
+  static final String NORMS_EXTENSION = "nrm";
+  
+  /** Extension of separate norms file
+   * @deprecated */
+  @Deprecated
+  static final String SEPARATE_NORMS_EXTENSION = "s";
+  
+  public Lucene40NormsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    final String normsFileName = IndexFileNames.segmentFileName(segment, "", NORMS_EXTENSION);
+    boolean success = false;
+    try {
+      out = directory.createOutput(normsFileName, context);
+      out.writeBytes(NORMS_HEADER, 0, NORMS_HEADER.length);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+
+  @Override
+  public void startField(FieldInfo info) throws IOException {
+    assert info.omitNorms == false;
+    normCount++;
+  }
+  
+  @Override
+  public void writeNorm(byte norm) throws IOException {
+    out.writeByte(norm);
+  }
+  
+  @Override
+  public void finish(int numDocs) throws IOException {
+    if (4+normCount*(long)numDocs != out.getFilePointer()) {
+      throw new RuntimeException(".nrm file size mismatch: expected=" + (4+normCount*(long)numDocs) + " actual=" + out.getFilePointer());
+    }
+  }
+  
+  /** we override merge and bulk-merge norms when there are no deletions */
+  @Override
+  public int merge(MergeState mergeState) throws IOException {
+    int numMergedDocs = 0;
+    for (FieldInfo fi : mergeState.fieldInfos) {
+      if (fi.isIndexed && !fi.omitNorms) {
+        startField(fi);
+        int numMergedDocsForField = 0;
+        for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
+          final int maxDoc = reader.reader.maxDoc();
+          byte normBuffer[] = reader.reader.norms(fi.name);
+          if (normBuffer == null) {
+            // Can be null if this segment doesn't have
+            // any docs with this field
+            normBuffer = new byte[maxDoc];
+            Arrays.fill(normBuffer, (byte)0);
+          }
+          if (reader.liveDocs == null) {
+            //optimized case for segments without deleted docs
+            out.writeBytes(normBuffer, maxDoc);
+            numMergedDocsForField += maxDoc;
+          } else {
+            // this segment has deleted docs, so we have to
+            // check for every doc if it is deleted or not
+            final Bits liveDocs = reader.liveDocs;
+            for (int k = 0; k < maxDoc; k++) {
+              if (liveDocs.get(k)) {
+                numMergedDocsForField++;
+                out.writeByte(normBuffer[k]);
+              }
+            }
+          }
+          mergeState.checkAbort.work(maxDoc);
+        }
+        assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;
+        numMergedDocs = numMergedDocsForField;
+      }
+    }
+    finish(numMergedDocs);
+    return numMergedDocs;
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(out);
+    } finally {
+      out = null;
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java
new file mode 100644
index 0000000..41baace
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsBaseFormat.java
@@ -0,0 +1,58 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.PostingsBaseFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.Directory;
+
+/** 
+ * Provides a {@link PostingsReaderBase} and {@link
+ * PostingsWriterBase}.
+ *
+ * @lucene.experimental */
+
+// TODO: should these also be named / looked up via SPI?
+public final class Lucene40PostingsBaseFormat extends PostingsBaseFormat {
+
+  public Lucene40PostingsBaseFormat() {
+    super("Lucene40");
+  }
+
+  @Override
+  public PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException {
+    return new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+  }
+
+  @Override
+  public PostingsWriterBase postingsWriterBase(SegmentWriteState state) throws IOException {
+    return new Lucene40PostingsWriter(state);
+  }
+  
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
+    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
new file mode 100644
index 0000000..a8ae8bf
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
@@ -0,0 +1,118 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.BlockTreeTermsReader;
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.Directory;
+
+/** Default codec. 
+ *  @lucene.experimental */
+
+// TODO: this class could be created by wrapping
+// BlockTreeTermsDict around Lucene40PostingsBaseFormat; ie
+// we should not duplicate the code from that class here:
+public class Lucene40PostingsFormat extends PostingsFormat {
+
+  private final int minBlockSize;
+  private final int maxBlockSize;
+
+  public Lucene40PostingsFormat() {
+    this(BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+  }
+
+  public Lucene40PostingsFormat(int minBlockSize, int maxBlockSize) {
+    super("Lucene40");
+    this.minBlockSize = minBlockSize;
+    assert minBlockSize > 1;
+    this.maxBlockSize = maxBlockSize;
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase docs = new Lucene40PostingsWriter(state);
+
+    // TODO: should we make the terms index more easily
+    // pluggable?  Ie so that this codec would record which
+    // index impl was used, and switch on loading?
+    // Or... you must make a new Codec for this?
+    boolean success = false;
+    try {
+      FieldsConsumer ret = new BlockTreeTermsWriter(state, docs, minBlockSize, maxBlockSize);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        docs.close();
+      }
+    }
+  }
+
+  public final static int TERMS_CACHE_SIZE = 1024;
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+
+    boolean success = false;
+    try {
+      FieldsProducer ret = new BlockTreeTermsReader(
+                                                    state.dir,
+                                                    state.fieldInfos,
+                                                    state.segmentInfo.name,
+                                                    postings,
+                                                    state.context,
+                                                    state.segmentSuffix,
+                                                    state.termsIndexDivisor);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        postings.close();
+      }
+    }
+  }
+
+  /** Extension of freq postings file */
+  static final String FREQ_EXTENSION = "frq";
+
+  /** Extension of prox postings file */
+  static final String PROX_EXTENSION = "prx";
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
+    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
+    BlockTreeTermsReader.files(dir, segmentInfo, segmentSuffix, files);
+  }
+
+  @Override
+  public String toString() {
+    return getName() + "(minBlockSize=" + minBlockSize + " maxBlockSize=" + maxBlockSize + ")";
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
new file mode 100644
index 0000000..e1bec03
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
@@ -0,0 +1,1102 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+
+/** Concrete class that reads the current doc/freq/skip
+ *  postings format. 
+ *  @lucene.experimental */
+
+public class Lucene40PostingsReader extends PostingsReaderBase {
+
+  private final IndexInput freqIn;
+  private final IndexInput proxIn;
+  // public static boolean DEBUG = BlockTreeTermsWriter.DEBUG;
+
+  int skipInterval;
+  int maxSkipLevels;
+  int skipMinimum;
+
+  // private String segment;
+
+  public Lucene40PostingsReader(Directory dir, SegmentInfo segmentInfo, IOContext ioContext, String segmentSuffix) throws IOException {
+    freqIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION),
+                           ioContext);
+    // this.segment = segmentInfo.name;
+    if (segmentInfo.getHasProx()) {
+      boolean success = false;
+      try {
+        proxIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION),
+                               ioContext);
+        success = true;
+      } finally {
+        if (!success) {
+          freqIn.close();
+        }
+      }
+    } else {
+      proxIn = null;
+    }
+  }
+
+  public static void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) throws IOException {
+    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION));
+    if (segmentInfo.getHasProx()) {
+      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION));
+    }
+  }
+
+  @Override
+  public void init(IndexInput termsIn) throws IOException {
+
+    // Make sure we are talking to the matching past writer
+    CodecUtil.checkHeader(termsIn, Lucene40PostingsWriter.CODEC,
+      Lucene40PostingsWriter.VERSION_START, Lucene40PostingsWriter.VERSION_START);
+
+    skipInterval = termsIn.readInt();
+    maxSkipLevels = termsIn.readInt();
+    skipMinimum = termsIn.readInt();
+  }
+
+  // Must keep final because we do non-standard clone
+  private final static class StandardTermState extends BlockTermState {
+    long freqOffset;
+    long proxOffset;
+    int skipOffset;
+
+    // Only used by the "primary" TermState -- clones don't
+    // copy this (basically they are "transient"):
+    ByteArrayDataInput bytesReader;  // TODO: should this NOT be in the TermState...?
+    byte[] bytes;
+
+    @Override
+    public Object clone() {
+      StandardTermState other = new StandardTermState();
+      other.copyFrom(this);
+      return other;
+    }
+
+    @Override
+    public void copyFrom(TermState _other) {
+      super.copyFrom(_other);
+      StandardTermState other = (StandardTermState) _other;
+      freqOffset = other.freqOffset;
+      proxOffset = other.proxOffset;
+      skipOffset = other.skipOffset;
+
+      // Do not copy bytes, bytesReader (else TermState is
+      // very heavy, ie drags around the entire block's
+      // byte[]).  On seek back, if next() is in fact used
+      // (rare!), they will be re-read from disk.
+    }
+
+    @Override
+    public String toString() {
+      return super.toString() + " freqFP=" + freqOffset + " proxFP=" + proxOffset + " skipOffset=" + skipOffset;
+    }
+  }
+
+  @Override
+  public BlockTermState newTermState() {
+    return new StandardTermState();
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      if (freqIn != null) {
+        freqIn.close();
+      }
+    } finally {
+      if (proxIn != null) {
+        proxIn.close();
+      }
+    }
+  }
+
+  /* Reads but does not decode the byte[] blob holding
+     metadata for the current terms block */
+  @Override
+  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
+    final StandardTermState termState = (StandardTermState) _termState;
+
+    final int len = termsIn.readVInt();
+
+    // if (DEBUG) System.out.println("  SPR.readTermsBlock bytes=" + len + " ts=" + _termState);
+    if (termState.bytes == null) {
+      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
+      termState.bytesReader = new ByteArrayDataInput();
+    } else if (termState.bytes.length < len) {
+      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
+    }
+
+    termsIn.readBytes(termState.bytes, 0, len);
+    termState.bytesReader.reset(termState.bytes, 0, len);
+  }
+
+  @Override
+  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState)
+    throws IOException {
+    final StandardTermState termState = (StandardTermState) _termState;
+    // if (DEBUG) System.out.println("SPR: nextTerm seg=" + segment + " tbOrd=" + termState.termBlockOrd + " bytesReader.fp=" + termState.bytesReader.getPosition());
+    final boolean isFirstTerm = termState.termBlockOrd == 0;
+
+    if (isFirstTerm) {
+      termState.freqOffset = termState.bytesReader.readVLong();
+    } else {
+      termState.freqOffset += termState.bytesReader.readVLong();
+    }
+    /*
+    if (DEBUG) {
+      System.out.println("  dF=" + termState.docFreq);
+      System.out.println("  freqFP=" + termState.freqOffset);
+    }
+    */
+    assert termState.freqOffset < freqIn.length();
+
+    if (termState.docFreq >= skipMinimum) {
+      termState.skipOffset = termState.bytesReader.readVInt();
+      // if (DEBUG) System.out.println("  skipOffset=" + termState.skipOffset + " vs freqIn.length=" + freqIn.length());
+      assert termState.freqOffset + termState.skipOffset < freqIn.length();
+    } else {
+      // undefined
+    }
+
+    if (fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      if (isFirstTerm) {
+        termState.proxOffset = termState.bytesReader.readVLong();
+      } else {
+        termState.proxOffset += termState.bytesReader.readVLong();
+      }
+      // if (DEBUG) System.out.println("  proxFP=" + termState.proxOffset);
+    }
+  }
+    
+  @Override
+  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
+    if (needsFreqs && fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
+      return null;
+    } else if (canReuse(reuse, liveDocs)) {
+      // if (DEBUG) System.out.println("SPR.docs ts=" + termState);
+      return ((SegmentDocsEnumBase) reuse).reset(fieldInfo, (StandardTermState)termState);
+    }
+    return newDocsEnum(liveDocs, fieldInfo, (StandardTermState)termState);
+  }
+  
+  private boolean canReuse(DocsEnum reuse, Bits liveDocs) {
+    if (reuse != null && (reuse instanceof SegmentDocsEnumBase)) {
+      SegmentDocsEnumBase docsEnum = (SegmentDocsEnumBase) reuse;
+      // If you are using ParellelReader, and pass in a
+      // reused DocsEnum, it could have come from another
+      // reader also using standard codec
+      if (docsEnum.startFreqIn == freqIn) {
+        // we only reuse if the the actual the incoming enum has the same liveDocs as the given liveDocs
+        return liveDocs == docsEnum.liveDocs;
+      }
+    }
+    return false;
+  }
+  
+  private DocsEnum newDocsEnum(Bits liveDocs, FieldInfo fieldInfo, StandardTermState termState) throws IOException {
+    if (liveDocs == null) {
+      return new AllDocsSegmentDocsEnum(freqIn).reset(fieldInfo, termState);
+    } else {
+      return new LiveDocsSegmentDocsEnum(freqIn, liveDocs).reset(fieldInfo, termState);
+    }
+  }
+
+  @Override
+  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
+    if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      return null;
+    }
+    
+    // TODO: refactor
+    if (fieldInfo.storePayloads) {
+      SegmentDocsAndPositionsAndPayloadsEnum docsEnum;
+      if (reuse == null || !(reuse instanceof SegmentDocsAndPositionsAndPayloadsEnum)) {
+        docsEnum = new SegmentDocsAndPositionsAndPayloadsEnum(freqIn, proxIn);
+      } else {
+        docsEnum = (SegmentDocsAndPositionsAndPayloadsEnum) reuse;
+        if (docsEnum.startFreqIn != freqIn) {
+          // If you are using ParellelReader, and pass in a
+          // reused DocsEnum, it could have come from another
+          // reader also using standard codec
+          docsEnum = new SegmentDocsAndPositionsAndPayloadsEnum(freqIn, proxIn);
+        }
+      }
+      return docsEnum.reset(fieldInfo, (StandardTermState) termState, liveDocs);
+    } else {
+      SegmentDocsAndPositionsEnum docsEnum;
+      if (reuse == null || !(reuse instanceof SegmentDocsAndPositionsEnum)) {
+        docsEnum = new SegmentDocsAndPositionsEnum(freqIn, proxIn);
+      } else {
+        docsEnum = (SegmentDocsAndPositionsEnum) reuse;
+        if (docsEnum.startFreqIn != freqIn) {
+          // If you are using ParellelReader, and pass in a
+          // reused DocsEnum, it could have come from another
+          // reader also using standard codec
+          docsEnum = new SegmentDocsAndPositionsEnum(freqIn, proxIn);
+        }
+      }
+      return docsEnum.reset(fieldInfo, (StandardTermState) termState, liveDocs);
+    }
+  }
+
+  static final int BUFFERSIZE = 64;
+  
+  private abstract class SegmentDocsEnumBase extends DocsEnum {
+    
+    protected final int[] docs = new int[BUFFERSIZE];
+    protected final int[] freqs = new int[BUFFERSIZE];
+    
+    final IndexInput freqIn; // reuse
+    final IndexInput startFreqIn; // reuse
+    Lucene40SkipListReader skipper; // reuse - lazy loaded
+    
+    protected boolean indexOmitsTF;                               // does current field omit term freq?
+    protected boolean storePayloads;                        // does current field store payloads?
+
+    protected int limit;                                    // number of docs in this posting
+    protected int ord;                                      // how many docs we've read
+    protected int doc;                                 // doc we last read
+    protected int accum;                                    // accumulator for doc deltas
+    protected int freq;                                     // freq we last read
+    protected int maxBufferedDocId;
+    
+    protected int start;
+    protected int count;
+
+
+    protected long freqOffset;
+    protected int skipOffset;
+
+    protected boolean skipped;
+    protected final Bits liveDocs;
+    
+    SegmentDocsEnumBase(IndexInput startFreqIn, Bits liveDocs) throws IOException {
+      this.startFreqIn = startFreqIn;
+      this.freqIn = (IndexInput)startFreqIn.clone();
+      this.liveDocs = liveDocs;
+      
+    }
+    
+    
+    DocsEnum reset(FieldInfo fieldInfo, StandardTermState termState) throws IOException {
+      indexOmitsTF = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY;
+      storePayloads = fieldInfo.storePayloads;
+      freqOffset = termState.freqOffset;
+      skipOffset = termState.skipOffset;
+
+      // TODO: for full enum case (eg segment merging) this
+      // seek is unnecessary; maybe we can avoid in such
+      // cases
+      freqIn.seek(termState.freqOffset);
+      limit = termState.docFreq;
+      assert limit > 0;
+      ord = 0;
+      doc = -1;
+      accum = 0;
+      // if (DEBUG) System.out.println("  sde limit=" + limit + " freqFP=" + freqOffset);
+      skipped = false;
+
+      start = -1;
+      count = 0;
+      maxBufferedDocId = -1;
+      return this;
+    }
+    
+    @Override
+    public final int freq() {
+      assert !indexOmitsTF;
+      return freq;
+    }
+
+    @Override
+    public final int docID() {
+      return doc;
+    }
+    
+    @Override
+    public final int advance(int target) throws IOException {
+      // last doc in our buffer is >= target, binary search + next()
+      if (++start < count && maxBufferedDocId >= target) {
+        if ((count-start) > 32) { // 32 seemed to be a sweetspot here so use binsearch if the pending results are a lot
+          start = binarySearch(count - 1, start, target, docs);
+          return nextDoc();
+        } else {
+          return linearScan(target);
+        }
+      }
+      
+      start = count; // buffer is consumed
+      
+      return doc = skipTo(target, liveDocs);
+    }
+    
+    private final int binarySearch(int hi, int low, int target, int[] docs) {
+      while (low <= hi) {
+        int mid = (hi + low) >>> 1;
+        int doc = docs[mid];
+        if (doc < target) {
+          low = mid + 1;
+        } else if (doc > target) {
+          hi = mid - 1;
+        } else {
+          low = mid;
+          break;
+        }
+      }
+      return low-1;
+    }
+    
+    final int readFreq(final IndexInput freqIn, final int code)
+        throws IOException {
+      if ((code & 1) != 0) { // if low bit is set
+        return 1; // freq is one
+      } else {
+        return freqIn.readVInt(); // else read freq
+      }
+    }
+    
+    protected abstract int linearScan(int scanTo) throws IOException;
+    
+    protected abstract int scanTo(int target) throws IOException;
+
+    protected final int refill() throws IOException {
+      final int doc = nextUnreadDoc();
+      count = 0;
+      start = -1;
+      if (doc == NO_MORE_DOCS) {
+        return NO_MORE_DOCS;
+      }
+      final int numDocs = Math.min(docs.length, limit - ord);
+      ord += numDocs;
+      if (indexOmitsTF) {
+        count = fillDocs(numDocs);
+      } else {
+        count = fillDocsAndFreqs(numDocs);
+      }
+      maxBufferedDocId = count > 0 ? docs[count-1] : NO_MORE_DOCS;
+      return doc;
+    }
+    
+
+    protected abstract int nextUnreadDoc() throws IOException;
+
+
+    private final int fillDocs(int size) throws IOException {
+      final IndexInput freqIn = this.freqIn;
+      final int docs[] = this.docs;
+      int docAc = accum;
+      for (int i = 0; i < size; i++) {
+        docAc += freqIn.readVInt();
+        docs[i] = docAc;
+      }
+      accum = docAc;
+      return size;
+    }
+    
+    private final int fillDocsAndFreqs(int size) throws IOException {
+      final IndexInput freqIn = this.freqIn;
+      final int docs[] = this.docs;
+      final int freqs[] = this.freqs;
+      int docAc = accum;
+      for (int i = 0; i < size; i++) {
+        final int code = freqIn.readVInt();
+        docAc += code >>> 1; // shift off low bit
+        freqs[i] = readFreq(freqIn, code);
+        docs[i] = docAc;
+      }
+      accum = docAc;
+      return size;
+     
+    }
+
+    private final int skipTo(int target, Bits liveDocs) throws IOException {
+      if ((target - skipInterval) >= accum && limit >= skipMinimum) {
+
+        // There are enough docs in the posting to have
+        // skip data, and it isn't too close.
+
+        if (skipper == null) {
+          // This is the first time this enum has ever been used for skipping -- do lazy init
+          skipper = new Lucene40SkipListReader((IndexInput) freqIn.clone(), maxSkipLevels, skipInterval);
+        }
+
+        if (!skipped) {
+
+          // This is the first time this posting has
+          // skipped since reset() was called, so now we
+          // load the skip data for this posting
+
+          skipper.init(freqOffset + skipOffset,
+                       freqOffset, 0,
+                       limit, storePayloads);
+
+          skipped = true;
+        }
+
+        final int newOrd = skipper.skipTo(target); 
+
+        if (newOrd > ord) {
+          // Skipper moved
+
+          ord = newOrd;
+          accum = skipper.getDoc();
+          freqIn.seek(skipper.getFreqPointer());
+        }
+      }
+      return scanTo(target);
+    }
+  }
+  
+  private final class AllDocsSegmentDocsEnum extends SegmentDocsEnumBase {
+
+    AllDocsSegmentDocsEnum(IndexInput startFreqIn) throws IOException {
+      super(startFreqIn, null);
+      assert liveDocs == null;
+    }
+    
+    @Override
+    public final int nextDoc() throws IOException {
+      if (++start < count) {
+        freq = freqs[start];
+        return doc = docs[start];
+      }
+      return doc = refill();
+    }
+    
+
+    @Override
+    protected final int linearScan(int scanTo) throws IOException {
+      final int[] docs = this.docs;
+      final int upTo = count;
+      for (int i = start; i < upTo; i++) {
+        final int d = docs[i];
+        if (scanTo <= d) {
+          start = i;
+          freq = freqs[i];
+          return doc = docs[i];
+        }
+      }
+      return refill();
+    }
+
+    @Override
+    protected int scanTo(int target) throws IOException { 
+      int docAcc = accum;
+      int frq = 1;
+      final IndexInput freqIn = this.freqIn;
+      final boolean omitTF = indexOmitsTF;
+      final int loopLimit = limit;
+      for (int i = ord; i < loopLimit; i++) {
+        int code = freqIn.readVInt();
+        if (omitTF) {
+          docAcc += code;
+        } else {
+          docAcc += code >>> 1; // shift off low bit
+          frq = readFreq(freqIn, code);
+        }
+        if (docAcc >= target) {
+          freq = frq;
+          ord = i + 1;
+          return accum = docAcc;
+        }
+      }
+      ord = limit;
+      freq = frq;
+      accum = docAcc;
+      return NO_MORE_DOCS;
+    }
+
+    @Override
+    protected final int nextUnreadDoc() throws IOException {
+      if (ord++ < limit) {
+        int code = freqIn.readVInt();
+        if (indexOmitsTF) {
+          accum += code;
+        } else {
+          accum += code >>> 1; // shift off low bit
+          freq = readFreq(freqIn, code);
+        }
+        return accum;
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+    
+  }
+  
+  private final class LiveDocsSegmentDocsEnum extends SegmentDocsEnumBase {
+
+    LiveDocsSegmentDocsEnum(IndexInput startFreqIn, Bits liveDocs) throws IOException {
+      super(startFreqIn, liveDocs);
+      assert liveDocs != null;
+    }
+    
+    @Override
+    public final int nextDoc() throws IOException {
+      final Bits liveDocs = this.liveDocs;
+      for (int i = start+1; i < count; i++) {
+        int d = docs[i];
+        if (liveDocs.get(d)) {
+          start = i;
+          freq = freqs[i];
+          return doc = d;
+        }
+      }
+      start = count;
+      return doc = refill();
+    }
+
+    @Override
+    protected final int linearScan(int scanTo) throws IOException {
+      final int[] docs = this.docs;
+      final int upTo = count;
+      final Bits liveDocs = this.liveDocs;
+      for (int i = start; i < upTo; i++) {
+        int d = docs[i];
+        if (scanTo <= d && liveDocs.get(d)) {
+          start = i;
+          freq = freqs[i];
+          return doc = docs[i];
+        }
+      }
+      return refill();
+    }
+    
+    @Override
+    protected int scanTo(int target) throws IOException { 
+      int docAcc = accum;
+      int frq = 1;
+      final IndexInput freqIn = this.freqIn;
+      final boolean omitTF = indexOmitsTF;
+      final int loopLimit = limit;
+      final Bits liveDocs = this.liveDocs;
+      for (int i = ord; i < loopLimit; i++) {
+        int code = freqIn.readVInt();
+        if (omitTF) {
+          docAcc += code;
+        } else {
+          docAcc += code >>> 1; // shift off low bit
+          frq = readFreq(freqIn, code);
+        }
+        if (docAcc >= target && liveDocs.get(docAcc)) {
+          freq = frq;
+          ord = i + 1;
+          return accum = docAcc;
+        }
+      }
+      ord = limit;
+      freq = frq;
+      accum = docAcc;
+      return NO_MORE_DOCS;
+    }
+
+    @Override
+    protected final int nextUnreadDoc() throws IOException {
+      int docAcc = accum;
+      int frq = 1;
+      final IndexInput freqIn = this.freqIn;
+      final boolean omitTF = indexOmitsTF;
+      final int loopLimit = limit;
+      final Bits liveDocs = this.liveDocs;
+      for (int i = ord; i < loopLimit; i++) {
+        int code = freqIn.readVInt();
+        if (omitTF) {
+          docAcc += code;
+        } else {
+          docAcc += code >>> 1; // shift off low bit
+          frq = readFreq(freqIn, code);
+        }
+        if (liveDocs.get(docAcc)) {
+          freq = frq;
+          ord = i + 1;
+          return accum = docAcc;
+        }
+      }
+      ord = limit;
+      freq = frq;
+      accum = docAcc;
+      return NO_MORE_DOCS;
+      
+    }
+  }
+  
+  // TODO specialize DocsAndPosEnum too
+  
+  // Decodes docs & positions. payloads are not present.
+  private final class SegmentDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    final IndexInput startFreqIn;
+    private final IndexInput freqIn;
+    private final IndexInput proxIn;
+    int limit;                                    // number of docs in this posting
+    int ord;                                      // how many docs we've read
+    int doc = -1;                                 // doc we last read
+    int accum;                                    // accumulator for doc deltas
+    int freq;                                     // freq we last read
+    int position;
+
+    Bits liveDocs;
+
+    long freqOffset;
+    int skipOffset;
+    long proxOffset;
+
+    int posPendingCount;
+
+    boolean skipped;
+    Lucene40SkipListReader skipper;
+    private long lazyProxPointer;
+
+    public SegmentDocsAndPositionsEnum(IndexInput freqIn, IndexInput proxIn) throws IOException {
+      startFreqIn = freqIn;
+      this.freqIn = (IndexInput) freqIn.clone();
+      this.proxIn = (IndexInput) proxIn.clone();
+    }
+
+    public SegmentDocsAndPositionsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
+      assert fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+      assert !fieldInfo.storePayloads;
+
+      this.liveDocs = liveDocs;
+
+      // TODO: for full enum case (eg segment merging) this
+      // seek is unnecessary; maybe we can avoid in such
+      // cases
+      freqIn.seek(termState.freqOffset);
+      lazyProxPointer = termState.proxOffset;
+
+      limit = termState.docFreq;
+      assert limit > 0;
+
+      ord = 0;
+      doc = -1;
+      accum = 0;
+      position = 0;
+
+      skipped = false;
+      posPendingCount = 0;
+
+      freqOffset = termState.freqOffset;
+      proxOffset = termState.proxOffset;
+      skipOffset = termState.skipOffset;
+      // if (DEBUG) System.out.println("StandardR.D&PE reset seg=" + segment + " limit=" + limit + " freqFP=" + freqOffset + " proxFP=" + proxOffset);
+
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      // if (DEBUG) System.out.println("SPR.nextDoc seg=" + segment + " freqIn.fp=" + freqIn.getFilePointer());
+      while(true) {
+        if (ord == limit) {
+          // if (DEBUG) System.out.println("  return END");
+          return doc = NO_MORE_DOCS;
+        }
+
+        ord++;
+
+        // Decode next doc/freq pair
+        final int code = freqIn.readVInt();
+
+        accum += code >>> 1;              // shift off low bit
+        if ((code & 1) != 0) {          // if low bit is set
+          freq = 1;                     // freq is one
+        } else {
+          freq = freqIn.readVInt();     // else read freq
+        }
+        posPendingCount += freq;
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          break;
+        }
+      }
+
+      position = 0;
+
+      // if (DEBUG) System.out.println("  return doc=" + doc);
+      return (doc = accum);
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+
+      //System.out.println("StandardR.D&PE advance target=" + target);
+
+      if ((target - skipInterval) >= doc && limit >= skipMinimum) {
+
+        // There are enough docs in the posting to have
+        // skip data, and it isn't too close
+
+        if (skipper == null) {
+          // This is the first time this enum has ever been used for skipping -- do lazy init
+          skipper = new Lucene40SkipListReader((IndexInput) freqIn.clone(), maxSkipLevels, skipInterval);
+        }
+
+        if (!skipped) {
+
+          // This is the first time this posting has
+          // skipped, since reset() was called, so now we
+          // load the skip data for this posting
+
+          skipper.init(freqOffset+skipOffset,
+                       freqOffset, proxOffset,
+                       limit, false);
+
+          skipped = true;
+        }
+
+        final int newOrd = skipper.skipTo(target); 
+
+        if (newOrd > ord) {
+          // Skipper moved
+          ord = newOrd;
+          doc = accum = skipper.getDoc();
+          freqIn.seek(skipper.getFreqPointer());
+          lazyProxPointer = skipper.getProxPointer();
+          posPendingCount = 0;
+          position = 0;
+        }
+      }
+        
+      // Now, linear scan for the rest:
+      do {
+        nextDoc();
+      } while (target > doc);
+
+      return doc;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+
+      if (lazyProxPointer != -1) {
+        proxIn.seek(lazyProxPointer);
+        lazyProxPointer = -1;
+      }
+
+      // scan over any docs that were iterated without their positions
+      if (posPendingCount > freq) {
+        position = 0;
+        while(posPendingCount != freq) {
+          if ((proxIn.readByte() & 0x80) == 0) {
+            posPendingCount--;
+          }
+        }
+      }
+
+      position += proxIn.readVInt();
+
+      posPendingCount--;
+
+      assert posPendingCount >= 0: "nextPosition() was called too many times (more than freq() times) posPendingCount=" + posPendingCount;
+
+      return position;
+    }
+
+    /** Returns the payload at this position, or null if no
+     *  payload was indexed. */
+    @Override
+    public BytesRef getPayload() throws IOException {
+      throw new IOException("No payloads exist for this field!");
+    }
+
+    @Override
+    public boolean hasPayload() {
+      return false;
+    }
+  }
+  
+  // Decodes docs & positions & payloads
+  private class SegmentDocsAndPositionsAndPayloadsEnum extends DocsAndPositionsEnum {
+    final IndexInput startFreqIn;
+    private final IndexInput freqIn;
+    private final IndexInput proxIn;
+
+    int limit;                                    // number of docs in this posting
+    int ord;                                      // how many docs we've read
+    int doc = -1;                                 // doc we last read
+    int accum;                                    // accumulator for doc deltas
+    int freq;                                     // freq we last read
+    int position;
+
+    Bits liveDocs;
+
+    long freqOffset;
+    int skipOffset;
+    long proxOffset;
+
+    int posPendingCount;
+    int payloadLength;
+    boolean payloadPending;
+
+    boolean skipped;
+    Lucene40SkipListReader skipper;
+    private BytesRef payload;
+    private long lazyProxPointer;
+
+    public SegmentDocsAndPositionsAndPayloadsEnum(IndexInput freqIn, IndexInput proxIn) throws IOException {
+      startFreqIn = freqIn;
+      this.freqIn = (IndexInput) freqIn.clone();
+      this.proxIn = (IndexInput) proxIn.clone();
+    }
+
+    public SegmentDocsAndPositionsAndPayloadsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
+      assert fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+      assert fieldInfo.storePayloads;
+      if (payload == null) {
+        payload = new BytesRef();
+        payload.bytes = new byte[1];
+      }
+
+      this.liveDocs = liveDocs;
+
+      // TODO: for full enum case (eg segment merging) this
+      // seek is unnecessary; maybe we can avoid in such
+      // cases
+      freqIn.seek(termState.freqOffset);
+      lazyProxPointer = termState.proxOffset;
+
+      limit = termState.docFreq;
+      ord = 0;
+      doc = -1;
+      accum = 0;
+      position = 0;
+
+      skipped = false;
+      posPendingCount = 0;
+      payloadPending = false;
+
+      freqOffset = termState.freqOffset;
+      proxOffset = termState.proxOffset;
+      skipOffset = termState.skipOffset;
+      //System.out.println("StandardR.D&PE reset seg=" + segment + " limit=" + limit + " freqFP=" + freqOffset + " proxFP=" + proxOffset + " this=" + this);
+
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      while(true) {
+        if (ord == limit) {
+          //System.out.println("StandardR.D&PE seg=" + segment + " nextDoc return doc=END");
+          return doc = NO_MORE_DOCS;
+        }
+
+        ord++;
+
+        // Decode next doc/freq pair
+        final int code = freqIn.readVInt();
+
+        accum += code >>> 1; // shift off low bit
+        if ((code & 1) != 0) { // if low bit is set
+          freq = 1; // freq is one
+        } else {
+          freq = freqIn.readVInt(); // else read freq
+        }
+        posPendingCount += freq;
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          break;
+        }
+      }
+
+      position = 0;
+
+      //System.out.println("StandardR.D&PE nextDoc seg=" + segment + " return doc=" + doc);
+      return (doc = accum);
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+
+      //System.out.println("StandardR.D&PE advance seg=" + segment + " target=" + target + " this=" + this);
+
+      if ((target - skipInterval) >= doc && limit >= skipMinimum) {
+
+        // There are enough docs in the posting to have
+        // skip data, and it isn't too close
+
+        if (skipper == null) {
+          // This is the first time this enum has ever been used for skipping -- do lazy init
+          skipper = new Lucene40SkipListReader((IndexInput) freqIn.clone(), maxSkipLevels, skipInterval);
+        }
+
+        if (!skipped) {
+
+          // This is the first time this posting has
+          // skipped, since reset() was called, so now we
+          // load the skip data for this posting
+          //System.out.println("  init skipper freqOffset=" + freqOffset + " skipOffset=" + skipOffset + " vs len=" + freqIn.length());
+          skipper.init(freqOffset+skipOffset,
+                       freqOffset, proxOffset,
+                       limit, true);
+
+          skipped = true;
+        }
+
+        final int newOrd = skipper.skipTo(target); 
+
+        if (newOrd > ord) {
+          // Skipper moved
+          ord = newOrd;
+          doc = accum = skipper.getDoc();
+          freqIn.seek(skipper.getFreqPointer());
+          lazyProxPointer = skipper.getProxPointer();
+          posPendingCount = 0;
+          position = 0;
+          payloadPending = false;
+          payloadLength = skipper.getPayloadLength();
+        }
+      }
+        
+      // Now, linear scan for the rest:
+      do {
+        nextDoc();
+      } while (target > doc);
+
+      return doc;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+
+      if (lazyProxPointer != -1) {
+        proxIn.seek(lazyProxPointer);
+        lazyProxPointer = -1;
+      }
+      
+      if (payloadPending && payloadLength > 0) {
+        // payload of last position as never retrieved -- skip it
+        proxIn.seek(proxIn.getFilePointer() + payloadLength);
+        payloadPending = false;
+      }
+
+      // scan over any docs that were iterated without their positions
+      while(posPendingCount > freq) {
+
+        final int code = proxIn.readVInt();
+
+        if ((code & 1) != 0) {
+          // new payload length
+          payloadLength = proxIn.readVInt();
+          assert payloadLength >= 0;
+        }
+        
+        assert payloadLength != -1;
+        proxIn.seek(proxIn.getFilePointer() + payloadLength);
+
+        posPendingCount--;
+        position = 0;
+        payloadPending = false;
+        //System.out.println("StandardR.D&PE skipPos");
+      }
+
+      // read next position
+      if (payloadPending && payloadLength > 0) {
+        // payload wasn't retrieved for last position
+        proxIn.seek(proxIn.getFilePointer()+payloadLength);
+      }
+
+      final int code = proxIn.readVInt();
+      if ((code & 1) != 0) {
+        // new payload length
+        payloadLength = proxIn.readVInt();
+        assert payloadLength >= 0;
+      }
+      assert payloadLength != -1;
+          
+      payloadPending = true;
+      position += code >>> 1;
+
+      posPendingCount--;
+
+      assert posPendingCount >= 0: "nextPosition() was called too many times (more than freq() times) posPendingCount=" + posPendingCount;
+
+      //System.out.println("StandardR.D&PE nextPos   return pos=" + position);
+      return position;
+    }
+
+    /** Returns the payload at this position, or null if no
+     *  payload was indexed. */
+    @Override
+    public BytesRef getPayload() throws IOException {
+      assert lazyProxPointer == -1;
+      assert posPendingCount < freq;
+      if (!payloadPending) {
+        throw new IOException("Either no payload exists at this term position or an attempt was made to load it more than once.");
+      }
+      if (payloadLength > payload.bytes.length) {
+        payload.grow(payloadLength);
+      }
+
+      proxIn.readBytes(payload.bytes, 0, payloadLength);
+      payload.length = payloadLength;
+      payloadPending = false;
+
+      return payload;
+    }
+
+    @Override
+    public boolean hasPayload() {
+      return payloadPending && payloadLength > 0;
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
new file mode 100644
index 0000000..cbcecd5
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
@@ -0,0 +1,334 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Consumes doc & freq, writing them using the current
+ *  index file format */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+
+/** @lucene.experimental */
+public final class Lucene40PostingsWriter extends PostingsWriterBase {
+  final static String CODEC = "Lucene40PostingsWriter";
+
+  //private static boolean DEBUG = BlockTreeTermsWriter.DEBUG;
+  
+  // Increment version to change it:
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  final IndexOutput freqOut;
+  final IndexOutput proxOut;
+  final Lucene40SkipListWriter skipListWriter;
+  /** Expert: The fraction of TermDocs entries stored in skip tables,
+   * used to accelerate {@link DocsEnum#advance(int)}.  Larger values result in
+   * smaller indexes, greater acceleration, but fewer accelerable cases, while
+   * smaller values result in bigger indexes, less acceleration and more
+   * accelerable cases. More detailed experiments would be useful here. */
+  static final int DEFAULT_SKIP_INTERVAL = 16;
+  final int skipInterval;
+  
+  /**
+   * Expert: minimum docFreq to write any skip data at all
+   */
+  final int skipMinimum;
+
+  /** Expert: The maximum number of skip levels. Smaller values result in 
+   * slightly smaller indexes, but slower skipping in big posting lists.
+   */
+  final int maxSkipLevels = 10;
+  final int totalNumDocs;
+  IndexOutput termsOut;
+
+  IndexOptions indexOptions;
+  boolean storePayloads;
+  // Starts a new term
+  long freqStart;
+  long proxStart;
+  FieldInfo fieldInfo;
+  int lastPayloadLength;
+  int lastPosition;
+
+  // private String segment;
+
+  public Lucene40PostingsWriter(SegmentWriteState state) throws IOException {
+    this(state, DEFAULT_SKIP_INTERVAL);
+  }
+  
+  public Lucene40PostingsWriter(SegmentWriteState state, int skipInterval) throws IOException {
+    super();
+    this.skipInterval = skipInterval;
+    this.skipMinimum = skipInterval; /* set to the same for now */
+    // this.segment = state.segmentName;
+    String fileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION);
+    freqOut = state.directory.createOutput(fileName, state.context);
+    boolean success = false;
+    try {
+      if (state.fieldInfos.hasProx()) {
+        // At least one field does not omit TF, so create the
+        // prox file
+        fileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION);
+        proxOut = state.directory.createOutput(fileName, state.context);
+      } else {
+        // Every field omits TF so we will write no prox file
+        proxOut = null;
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(freqOut);
+      }
+    }
+
+    totalNumDocs = state.numDocs;
+
+    skipListWriter = new Lucene40SkipListWriter(skipInterval,
+                                               maxSkipLevels,
+                                               state.numDocs,
+                                               freqOut,
+                                               proxOut);
+  }
+
+  @Override
+  public void start(IndexOutput termsOut) throws IOException {
+    this.termsOut = termsOut;
+    CodecUtil.writeHeader(termsOut, CODEC, VERSION_CURRENT);
+    termsOut.writeInt(skipInterval);                // write skipInterval
+    termsOut.writeInt(maxSkipLevels);               // write maxSkipLevels
+    termsOut.writeInt(skipMinimum);                 // write skipMinimum
+  }
+
+  @Override
+  public void startTerm() {
+    freqStart = freqOut.getFilePointer();
+    //if (DEBUG) System.out.println("SPW: startTerm freqOut.fp=" + freqStart);
+    if (proxOut != null) {
+      proxStart = proxOut.getFilePointer();
+      // force first payload to write its length
+      lastPayloadLength = -1;
+    }
+    skipListWriter.resetSkip();
+  }
+
+  // Currently, this instance is re-used across fields, so
+  // our parent calls setField whenever the field changes
+  @Override
+  public void setField(FieldInfo fieldInfo) {
+    //System.out.println("SPW: setField");
+    /*
+    if (BlockTreeTermsWriter.DEBUG && fieldInfo.name.equals("id")) {
+      DEBUG = true;
+    } else {
+      DEBUG = false;
+    }
+    */
+    this.fieldInfo = fieldInfo;
+    indexOptions = fieldInfo.indexOptions;
+    storePayloads = fieldInfo.storePayloads;
+    //System.out.println("  set init blockFreqStart=" + freqStart);
+    //System.out.println("  set init blockProxStart=" + proxStart);
+  }
+
+  int lastDocID;
+  int df;
+  
+  /** Adds a new doc in this term.  If this returns null
+   *  then we just skip consuming positions/payloads. */
+  @Override
+  public void startDoc(int docID, int termDocFreq) throws IOException {
+    // if (DEBUG) System.out.println("SPW:   startDoc seg=" + segment + " docID=" + docID + " tf=" + termDocFreq + " freqOut.fp=" + freqOut.getFilePointer());
+
+    final int delta = docID - lastDocID;
+    
+    if (docID < 0 || (df > 0 && delta <= 0)) {
+      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " ) (freqOut: " + freqOut + ")");
+    }
+
+    if ((++df % skipInterval) == 0) {
+      skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength);
+      skipListWriter.bufferSkip(df);
+    }
+
+    assert docID < totalNumDocs: "docID=" + docID + " totalNumDocs=" + totalNumDocs;
+
+    lastDocID = docID;
+    if (indexOptions == IndexOptions.DOCS_ONLY) {
+      freqOut.writeVInt(delta);
+    } else if (1 == termDocFreq) {
+      freqOut.writeVInt((delta<<1) | 1);
+    } else {
+      freqOut.writeVInt(delta<<1);
+      freqOut.writeVInt(termDocFreq);
+    }
+
+    lastPosition = 0;
+  }
+
+  /** Add a new position & payload */
+  @Override
+  public void addPosition(int position, BytesRef payload) throws IOException {
+    //if (DEBUG) System.out.println("SPW:     addPos pos=" + position + " payload=" + (payload == null ? "null" : (payload.length + " bytes")) + " proxFP=" + proxOut.getFilePointer());
+    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS: "invalid indexOptions: " + indexOptions;
+    assert proxOut != null;
+
+    final int delta = position - lastPosition;
+    
+    assert delta >= 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
+
+    lastPosition = position;
+
+    if (storePayloads) {
+      final int payloadLength = payload == null ? 0 : payload.length;
+
+      if (payloadLength != lastPayloadLength) {
+        lastPayloadLength = payloadLength;
+        proxOut.writeVInt((delta<<1)|1);
+        proxOut.writeVInt(payloadLength);
+      } else {
+        proxOut.writeVInt(delta << 1);
+      }
+
+      if (payloadLength > 0) {
+        proxOut.writeBytes(payload.bytes, payload.offset, payloadLength);
+      }
+    } else {
+      proxOut.writeVInt(delta);
+    }
+  }
+
+  @Override
+  public void finishDoc() {
+  }
+
+  private static class PendingTerm {
+    public final long freqStart;
+    public final long proxStart;
+    public final int skipOffset;
+
+    public PendingTerm(long freqStart, long proxStart, int skipOffset) {
+      this.freqStart = freqStart;
+      this.proxStart = proxStart;
+      this.skipOffset = skipOffset;
+    }
+  }
+
+  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
+
+  /** Called when we are done adding docs to this term */
+  @Override
+  public void finishTerm(TermStats stats) throws IOException {
+
+    // if (DEBUG) System.out.println("SPW: finishTerm seg=" + segment + " freqStart=" + freqStart);
+    assert stats.docFreq > 0;
+
+    // TODO: wasteful we are counting this (counting # docs
+    // for this term) in two places?
+    assert stats.docFreq == df;
+
+    final int skipOffset;
+    if (df >= skipMinimum) {
+      skipOffset = (int) (skipListWriter.writeSkip(freqOut)-freqStart);
+    } else {
+      skipOffset = -1;
+    }
+
+    pendingTerms.add(new PendingTerm(freqStart, proxStart, skipOffset));
+
+    lastDocID = 0;
+    df = 0;
+  }
+
+  private final RAMOutputStream bytesWriter = new RAMOutputStream();
+
+  @Override
+  public void flushTermsBlock(int start, int count) throws IOException {
+    //if (DEBUG) System.out.println("SPW: flushTermsBlock start=" + start + " count=" + count + " left=" + (pendingTerms.size()-count) + " pendingTerms.size()=" + pendingTerms.size());
+
+    if (count == 0) {
+      termsOut.writeByte((byte) 0);
+      return;
+    }
+
+    assert start <= pendingTerms.size();
+    assert count <= start;
+
+    final int limit = pendingTerms.size() - start + count;
+    final PendingTerm firstTerm = pendingTerms.get(limit - count);
+    // First term in block is abs coded:
+    bytesWriter.writeVLong(firstTerm.freqStart);
+
+    if (firstTerm.skipOffset != -1) {
+      assert firstTerm.skipOffset > 0;
+      bytesWriter.writeVInt(firstTerm.skipOffset);
+    }
+    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      bytesWriter.writeVLong(firstTerm.proxStart);
+    }
+    long lastFreqStart = firstTerm.freqStart;
+    long lastProxStart = firstTerm.proxStart;
+    for(int idx=limit-count+1; idx<limit; idx++) {
+      final PendingTerm term = pendingTerms.get(idx);
+      //if (DEBUG) System.out.println("  write term freqStart=" + term.freqStart);
+      // The rest of the terms term are delta coded:
+      bytesWriter.writeVLong(term.freqStart - lastFreqStart);
+      lastFreqStart = term.freqStart;
+      if (term.skipOffset != -1) {
+        assert term.skipOffset > 0;
+        bytesWriter.writeVInt(term.skipOffset);
+      }
+      if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+        bytesWriter.writeVLong(term.proxStart - lastProxStart);
+        lastProxStart = term.proxStart;
+      }
+    }
+
+    termsOut.writeVInt((int) bytesWriter.getFilePointer());
+    bytesWriter.writeTo(termsOut);
+    bytesWriter.reset();
+
+    // Remove the terms we just wrote:
+    pendingTerms.subList(limit-count, limit).clear();
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      freqOut.close();
+    } finally {
+      if (proxOut != null) {
+        proxOut.close();
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosFormat.java
new file mode 100644
index 0000000..a5e18ec
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosFormat.java
@@ -0,0 +1,40 @@
+package org.apache.lucene.codecs.lucene40;
+
+import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.SegmentInfosReader;
+import org.apache.lucene.codecs.SegmentInfosWriter;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @lucene.experimental
+ */
+public class Lucene40SegmentInfosFormat extends SegmentInfosFormat {
+  private final SegmentInfosReader reader = new Lucene40SegmentInfosReader();
+  private final SegmentInfosWriter writer = new Lucene40SegmentInfosWriter();
+  
+  @Override
+  public SegmentInfosReader getSegmentInfosReader() {
+    return reader;
+  }
+
+  @Override
+  public SegmentInfosWriter getSegmentInfosWriter() {
+    return writer;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosReader.java
new file mode 100644
index 0000000..0ccce9c
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosReader.java
@@ -0,0 +1,191 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.SegmentInfosReader;
+import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsReader;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.CompoundFileDirectory;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * Default implementation of {@link SegmentInfosReader}.
+ * @lucene.experimental
+ */
+public class Lucene40SegmentInfosReader extends SegmentInfosReader {
+
+  // TODO: shove all backwards code to preflex!
+  // this is a little tricky, because of IR.commit(), two options:
+  // 1. PreFlex writes 4.x SIS format, but reads both 3.x and 4.x
+  //    (and maybe RW always only writes the 3.x one? for that to work well,
+  //     we have to move .fnx file to codec too, not too bad but more work).
+  //     or we just have crappier RW testing like today.
+  // 2. PreFlex writes 3.x SIS format, and only reads 3.x
+  //    (in this case we have to move .fnx file to codec as well)
+  @Override
+  public void read(Directory directory, String segmentsFileName, ChecksumIndexInput input, SegmentInfos infos, IOContext context) throws IOException { 
+    infos.version = input.readLong(); // read version
+    infos.counter = input.readInt(); // read counter
+    final int format = infos.getFormat();
+    for (int i = input.readInt(); i > 0; i--) { // read segmentInfos
+      SegmentInfo si = readSegmentInfo(directory, format, input);
+      if (si.getVersion() == null) {
+        // Could be a 3.0 - try to open the doc stores - if it fails, it's a
+        // 2.x segment, and an IndexFormatTooOldException will be thrown,
+        // which is what we want.
+        Directory dir = directory;
+        if (si.getDocStoreOffset() != -1) {
+          if (si.getDocStoreIsCompoundFile()) {
+            dir = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(
+                si.getDocStoreSegment(), "",
+                IndexFileNames.COMPOUND_FILE_STORE_EXTENSION), context, false);
+          }
+        } else if (si.getUseCompoundFile()) {
+          dir = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(
+              si.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);
+        }
+
+        try {
+          Lucene40StoredFieldsReader.checkCodeVersion(dir, si.getDocStoreSegment());
+        } finally {
+          // If we opened the directory, close it
+          if (dir != directory) dir.close();
+        }
+          
+        // Above call succeeded, so it's a 3.0 segment. Upgrade it so the next
+        // time the segment is read, its version won't be null and we won't
+        // need to open FieldsReader every time for each such segment.
+        si.setVersion("3.0");
+      } else if (si.getVersion().equals("2.x")) {
+        // If it's a 3x index touched by 3.1+ code, then segments record their
+        // version, whether they are 2.x ones or not. We detect that and throw
+        // appropriate exception.
+        throw new IndexFormatTooOldException("segment " + si.name + " in resource " + input, si.getVersion());
+      }
+      infos.add(si);
+    }
+      
+    infos.userData = input.readStringStringMap();
+  }
+  
+  // if we make a preflex impl we can remove a lot of this hair...
+  public SegmentInfo readSegmentInfo(Directory dir, int format, ChecksumIndexInput input) throws IOException {
+    final String version;
+    if (format <= SegmentInfos.FORMAT_3_1) {
+      version = input.readString();
+    } else {
+      version = null;
+    }
+    final String name = input.readString();
+    final int docCount = input.readInt();
+    final long delGen = input.readLong();
+    final int docStoreOffset = input.readInt();
+    final String docStoreSegment;
+    final boolean docStoreIsCompoundFile;
+    if (docStoreOffset != -1) {
+      docStoreSegment = input.readString();
+      docStoreIsCompoundFile = input.readByte() == SegmentInfo.YES;
+    } else {
+      docStoreSegment = name;
+      docStoreIsCompoundFile = false;
+    }
+
+    if (format > SegmentInfos.FORMAT_4_0) {
+      // pre-4.0 indexes write a byte if there is a single norms file
+      byte b = input.readByte();
+      assert 1 == b;
+    }
+
+    final int numNormGen = input.readInt();
+    final Map<Integer,Long> normGen;
+    if (numNormGen == SegmentInfo.NO) {
+      normGen = null;
+    } else {
+      normGen = new HashMap<Integer, Long>();
+      for(int j=0;j<numNormGen;j++) {
+        int fieldNumber = j;
+        if (format <= SegmentInfos.FORMAT_4_0) {
+          fieldNumber = input.readInt();
+        }
+
+        normGen.put(fieldNumber, input.readLong());
+      }
+    }
+    final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
+
+    final int delCount = input.readInt();
+    assert delCount <= docCount;
+
+    final int hasProx = input.readByte();
+
+    final Codec codec;
+    // note: if the codec is not available: Codec.forName will throw an exception.
+    if (format <= SegmentInfos.FORMAT_4_0) {
+      codec = Codec.forName(input.readString());
+    } else {
+      codec = Codec.forName("Lucene3x");
+    }
+    final Map<String,String> diagnostics = input.readStringStringMap();
+
+    final int hasVectors;
+    if (format <= SegmentInfos.FORMAT_HAS_VECTORS) {
+      hasVectors = input.readByte();
+    } else {
+      final String storesSegment;
+      final String ext;
+      final boolean storeIsCompoundFile;
+      if (docStoreOffset != -1) {
+        storesSegment = docStoreSegment;
+        storeIsCompoundFile = docStoreIsCompoundFile;
+        ext = IndexFileNames.COMPOUND_FILE_STORE_EXTENSION;
+      } else {
+        storesSegment = name;
+        storeIsCompoundFile = isCompoundFile;
+        ext = IndexFileNames.COMPOUND_FILE_EXTENSION;
+      }
+      final Directory dirToTest;
+      if (storeIsCompoundFile) {
+        dirToTest = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(storesSegment, "", ext), IOContext.READONCE, false);
+      } else {
+        dirToTest = dir;
+      }
+      try {
+        // TODO: remove this manual file check or push to preflex codec
+        hasVectors = dirToTest.fileExists(IndexFileNames.segmentFileName(storesSegment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION)) ? SegmentInfo.YES : SegmentInfo.NO;
+      } finally {
+        if (isCompoundFile) {
+          dirToTest.close();
+        }
+      }
+    }
+    
+    return new SegmentInfo(dir, version, name, docCount, delGen, docStoreOffset,
+      docStoreSegment, docStoreIsCompoundFile, normGen, isCompoundFile,
+      delCount, hasProx, codec, diagnostics, hasVectors);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosWriter.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosWriter.java
new file mode 100644
index 0000000..3b757b9
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SegmentInfosWriter.java
@@ -0,0 +1,115 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.lucene.codecs.SegmentInfosWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.store.ChecksumIndexOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FlushInfo;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Default implementation of {@link SegmentInfosWriter}.
+ * @lucene.experimental
+ */
+public class Lucene40SegmentInfosWriter extends SegmentInfosWriter {
+
+  @Override
+  public IndexOutput writeInfos(Directory dir, String segmentFileName, String codecID, SegmentInfos infos, IOContext context)
+          throws IOException {
+    IndexOutput out = createOutput(dir, segmentFileName, new IOContext(new FlushInfo(infos.size(), infos.totalDocCount())));
+    boolean success = false;
+    try {
+      out.writeInt(SegmentInfos.FORMAT_CURRENT); // write FORMAT
+      out.writeString(codecID); // write codecID
+      out.writeLong(infos.version);
+      out.writeInt(infos.counter); // write counter
+      out.writeInt(infos.size()); // write infos
+      for (SegmentInfo si : infos) {
+        writeInfo(out, si);
+      }
+      out.writeStringStringMap(infos.getUserData());
+      success = true;
+      return out;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  /** Save a single segment's info. */
+  private void writeInfo(IndexOutput output, SegmentInfo si) throws IOException {
+    assert si.getDelCount() <= si.docCount: "delCount=" + si.getDelCount() + " docCount=" + si.docCount + " segment=" + si.name;
+    // Write the Lucene version that created this segment, since 3.1
+    output.writeString(si.getVersion());
+    output.writeString(si.name);
+    output.writeInt(si.docCount);
+    output.writeLong(si.getDelGen());
+
+    output.writeInt(si.getDocStoreOffset());
+    if (si.getDocStoreOffset() != -1) {
+      output.writeString(si.getDocStoreSegment());
+      output.writeByte((byte) (si.getDocStoreIsCompoundFile() ? 1:0));
+    }
+
+    Map<Integer,Long> normGen = si.getNormGen();
+    if (normGen == null) {
+      output.writeInt(SegmentInfo.NO);
+    } else {
+      output.writeInt(normGen.size());
+      for (Entry<Integer,Long> entry : normGen.entrySet()) {
+        output.writeInt(entry.getKey());
+        output.writeLong(entry.getValue());
+      }
+    }
+
+    output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
+    output.writeInt(si.getDelCount());
+    output.writeByte((byte) (si.getHasProxInternal()));
+    output.writeString(si.getCodec().getName());
+    output.writeStringStringMap(si.getDiagnostics());
+    output.writeByte((byte) (si.getHasVectorsInternal()));
+  }
+  
+  protected IndexOutput createOutput(Directory dir, String segmentFileName, IOContext context)
+      throws IOException {
+    IndexOutput plainOut = dir.createOutput(segmentFileName, context);
+    ChecksumIndexOutput out = new ChecksumIndexOutput(plainOut);
+    return out;
+  }
+
+  @Override
+  public void prepareCommit(IndexOutput segmentOutput) throws IOException {
+    ((ChecksumIndexOutput)segmentOutput).prepareCommit();
+  }
+
+  @Override
+  public void finishCommit(IndexOutput out) throws IOException {
+    ((ChecksumIndexOutput)out).finishCommit();
+    out.close();
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java
new file mode 100644
index 0000000..e9fe6c8
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListReader.java
@@ -0,0 +1,118 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.codecs.MultiLevelSkipListReader;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * Implements the skip list reader for the default posting list format
+ * that stores positions and payloads.
+ * @lucene.experimental
+ */
+public class Lucene40SkipListReader extends MultiLevelSkipListReader {
+  private boolean currentFieldStoresPayloads;
+  private long freqPointer[];
+  private long proxPointer[];
+  private int payloadLength[];
+  
+  private long lastFreqPointer;
+  private long lastProxPointer;
+  private int lastPayloadLength;
+                           
+
+  public Lucene40SkipListReader(IndexInput skipStream, int maxSkipLevels, int skipInterval) {
+    super(skipStream, maxSkipLevels, skipInterval);
+    freqPointer = new long[maxSkipLevels];
+    proxPointer = new long[maxSkipLevels];
+    payloadLength = new int[maxSkipLevels];
+  }
+
+  public void init(long skipPointer, long freqBasePointer, long proxBasePointer, int df, boolean storesPayloads) {
+    super.init(skipPointer, df);
+    this.currentFieldStoresPayloads = storesPayloads;
+    lastFreqPointer = freqBasePointer;
+    lastProxPointer = proxBasePointer;
+
+    Arrays.fill(freqPointer, freqBasePointer);
+    Arrays.fill(proxPointer, proxBasePointer);
+    Arrays.fill(payloadLength, 0);
+  }
+
+  /** Returns the freq pointer of the doc to which the last call of 
+   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
+  public long getFreqPointer() {
+    return lastFreqPointer;
+  }
+
+  /** Returns the prox pointer of the doc to which the last call of 
+   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
+  public long getProxPointer() {
+    return lastProxPointer;
+  }
+  
+  /** Returns the payload length of the payload stored just before 
+   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
+   * has skipped.  */
+  public int getPayloadLength() {
+    return lastPayloadLength;
+  }
+  
+  @Override
+  protected void seekChild(int level) throws IOException {
+    super.seekChild(level);
+    freqPointer[level] = lastFreqPointer;
+    proxPointer[level] = lastProxPointer;
+    payloadLength[level] = lastPayloadLength;
+  }
+  
+  @Override
+  protected void setLastSkipData(int level) {
+    super.setLastSkipData(level);
+    lastFreqPointer = freqPointer[level];
+    lastProxPointer = proxPointer[level];
+    lastPayloadLength = payloadLength[level];
+  }
+
+
+  @Override
+  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
+    int delta;
+    if (currentFieldStoresPayloads) {
+      // the current field stores payloads.
+      // if the doc delta is odd then we have
+      // to read the current payload length
+      // because it differs from the length of the
+      // previous payload
+      delta = skipStream.readVInt();
+      if ((delta & 1) != 0) {
+        payloadLength[level] = skipStream.readVInt();
+      }
+      delta >>>= 1;
+    } else {
+      delta = skipStream.readVInt();
+    }
+    freqPointer[level] += skipStream.readVInt();
+    proxPointer[level] += skipStream.readVInt();
+    
+    return delta;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java
new file mode 100644
index 0000000..1793751
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40SkipListWriter.java
@@ -0,0 +1,128 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.codecs.MultiLevelSkipListWriter;
+
+
+/**
+ * Implements the skip list writer for the default posting list format
+ * that stores positions and payloads.
+ * @lucene.experimental
+ */
+public class Lucene40SkipListWriter extends MultiLevelSkipListWriter {
+  private int[] lastSkipDoc;
+  private int[] lastSkipPayloadLength;
+  private long[] lastSkipFreqPointer;
+  private long[] lastSkipProxPointer;
+  
+  private IndexOutput freqOutput;
+  private IndexOutput proxOutput;
+
+  private int curDoc;
+  private boolean curStorePayloads;
+  private int curPayloadLength;
+  private long curFreqPointer;
+  private long curProxPointer;
+
+  public Lucene40SkipListWriter(int skipInterval, int numberOfSkipLevels, int docCount, IndexOutput freqOutput, IndexOutput proxOutput) {
+    super(skipInterval, numberOfSkipLevels, docCount);
+    this.freqOutput = freqOutput;
+    this.proxOutput = proxOutput;
+    
+    lastSkipDoc = new int[numberOfSkipLevels];
+    lastSkipPayloadLength = new int[numberOfSkipLevels];
+    lastSkipFreqPointer = new long[numberOfSkipLevels];
+    lastSkipProxPointer = new long[numberOfSkipLevels];
+  }
+
+  /**
+   * Sets the values for the current skip data. 
+   */
+  public void setSkipData(int doc, boolean storePayloads, int payloadLength) {
+    this.curDoc = doc;
+    this.curStorePayloads = storePayloads;
+    this.curPayloadLength = payloadLength;
+    this.curFreqPointer = freqOutput.getFilePointer();
+    if (proxOutput != null)
+      this.curProxPointer = proxOutput.getFilePointer();
+  }
+
+  @Override
+  public void resetSkip() {
+    super.resetSkip();
+    Arrays.fill(lastSkipDoc, 0);
+    Arrays.fill(lastSkipPayloadLength, -1);  // we don't have to write the first length in the skip list
+    Arrays.fill(lastSkipFreqPointer, freqOutput.getFilePointer());
+    if (proxOutput != null)
+      Arrays.fill(lastSkipProxPointer, proxOutput.getFilePointer());
+  }
+  
+  @Override
+  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
+    // To efficiently store payloads in the posting lists we do not store the length of
+    // every payload. Instead we omit the length for a payload if the previous payload had
+    // the same length.
+    // However, in order to support skipping the payload length at every skip point must be known.
+    // So we use the same length encoding that we use for the posting lists for the skip data as well:
+    // Case 1: current field does not store payloads
+    //           SkipDatum                 --> DocSkip, FreqSkip, ProxSkip
+    //           DocSkip,FreqSkip,ProxSkip --> VInt
+    //           DocSkip records the document number before every SkipInterval th  document in TermFreqs. 
+    //           Document numbers are represented as differences from the previous value in the sequence.
+    // Case 2: current field stores payloads
+    //           SkipDatum                 --> DocSkip, PayloadLength?, FreqSkip,ProxSkip
+    //           DocSkip,FreqSkip,ProxSkip --> VInt
+    //           PayloadLength             --> VInt    
+    //         In this case DocSkip/2 is the difference between
+    //         the current and the previous value. If DocSkip
+    //         is odd, then a PayloadLength encoded as VInt follows,
+    //         if DocSkip is even, then it is assumed that the
+    //         current payload length equals the length at the previous
+    //         skip point
+    if (curStorePayloads) {
+      int delta = curDoc - lastSkipDoc[level];
+      if (curPayloadLength == lastSkipPayloadLength[level]) {
+        // the current payload length equals the length at the previous skip point,
+        // so we don't store the length again
+        skipBuffer.writeVInt(delta * 2);
+      } else {
+        // the payload length is different from the previous one. We shift the DocSkip, 
+        // set the lowest bit and store the current payload length as VInt.
+        skipBuffer.writeVInt(delta * 2 + 1);
+        skipBuffer.writeVInt(curPayloadLength);
+        lastSkipPayloadLength[level] = curPayloadLength;
+      }
+    } else {
+      // current field does not store payloads
+      skipBuffer.writeVInt(curDoc - lastSkipDoc[level]);
+    }
+    skipBuffer.writeVInt((int) (curFreqPointer - lastSkipFreqPointer[level]));
+    skipBuffer.writeVInt((int) (curProxPointer - lastSkipProxPointer[level]));
+
+    lastSkipDoc[level] = curDoc;
+    
+    lastSkipFreqPointer[level] = curFreqPointer;
+    lastSkipProxPointer[level] = curProxPointer;
+  }
+
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java
new file mode 100644
index 0000000..62e5887
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsFormat.java
@@ -0,0 +1,50 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/** @lucene.experimental */
+public class Lucene40StoredFieldsFormat extends StoredFieldsFormat {
+
+  @Override
+  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si,
+      FieldInfos fn, IOContext context) throws IOException {
+    return new Lucene40StoredFieldsReader(directory, si, fn, context);
+  }
+
+  @Override
+  public StoredFieldsWriter fieldsWriter(Directory directory, String segment,
+      IOContext context) throws IOException {
+    return new Lucene40StoredFieldsWriter(directory, segment, context);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    Lucene40StoredFieldsReader.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java
new file mode 100644
index 0000000..fb5e2ce
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsReader.java
@@ -0,0 +1,302 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexFormatTooNewException;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.StoredFieldVisitor;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.IOUtils;
+
+import java.io.Closeable;
+import java.nio.charset.Charset;
+import java.util.Set;
+
+/**
+ * Class responsible for access to stored document fields.
+ * <p/>
+ * It uses &lt;segment&gt;.fdt and &lt;segment&gt;.fdx; files.
+ * 
+ * @lucene.internal
+ */
+public final class Lucene40StoredFieldsReader extends StoredFieldsReader implements Cloneable, Closeable {
+  private final static int FORMAT_SIZE = 4;
+
+  private final FieldInfos fieldInfos;
+  private final IndexInput fieldsStream;
+  private final IndexInput indexStream;
+  private int numTotalDocs;
+  private int size;
+  private boolean closed;
+  private final int format;
+
+  // The docID offset where our docs begin in the index
+  // file.  This will be 0 if we have our own private file.
+  private int docStoreOffset;
+
+  /** Returns a cloned FieldsReader that shares open
+   *  IndexInputs with the original one.  It is the caller's
+   *  job not to close the original FieldsReader until all
+   *  clones are called (eg, currently SegmentReader manages
+   *  this logic). */
+  @Override
+  public Lucene40StoredFieldsReader clone() {
+    ensureOpen();
+    return new Lucene40StoredFieldsReader(fieldInfos, numTotalDocs, size, format, docStoreOffset, (IndexInput)fieldsStream.clone(), (IndexInput)indexStream.clone());
+  }
+
+  /** Verifies that the code version which wrote the segment is supported. */
+  public static void checkCodeVersion(Directory dir, String segment) throws IOException {
+    final String indexStreamFN = IndexFileNames.segmentFileName(segment, "", Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION);
+    IndexInput idxStream = dir.openInput(indexStreamFN, IOContext.DEFAULT);
+    
+    try {
+      int format = idxStream.readInt();
+      if (format < Lucene40StoredFieldsWriter.FORMAT_MINIMUM)
+        throw new IndexFormatTooOldException(idxStream, format, Lucene40StoredFieldsWriter.FORMAT_MINIMUM, Lucene40StoredFieldsWriter.FORMAT_CURRENT);
+      if (format > Lucene40StoredFieldsWriter.FORMAT_CURRENT)
+        throw new IndexFormatTooNewException(idxStream, format, Lucene40StoredFieldsWriter.FORMAT_MINIMUM, Lucene40StoredFieldsWriter.FORMAT_CURRENT);
+    } finally {
+      idxStream.close();
+    }
+  
+  }
+  
+  // Used only by clone
+  private Lucene40StoredFieldsReader(FieldInfos fieldInfos, int numTotalDocs, int size, int format, int docStoreOffset,
+                       IndexInput fieldsStream, IndexInput indexStream) {
+    this.fieldInfos = fieldInfos;
+    this.numTotalDocs = numTotalDocs;
+    this.size = size;
+    this.format = format;
+    this.docStoreOffset = docStoreOffset;
+    this.fieldsStream = fieldsStream;
+    this.indexStream = indexStream;
+  }
+
+  public Lucene40StoredFieldsReader(Directory d, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
+    final String segment = si.getDocStoreSegment();
+    final int docStoreOffset = si.getDocStoreOffset();
+    final int size = si.docCount;
+    boolean success = false;
+    fieldInfos = fn;
+    try {
+      fieldsStream = d.openInput(IndexFileNames.segmentFileName(segment, "", Lucene40StoredFieldsWriter.FIELDS_EXTENSION), context);
+      final String indexStreamFN = IndexFileNames.segmentFileName(segment, "", Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION);
+      indexStream = d.openInput(indexStreamFN, context);
+      
+      format = indexStream.readInt();
+
+      if (format < Lucene40StoredFieldsWriter.FORMAT_MINIMUM)
+        throw new IndexFormatTooOldException(indexStream, format, Lucene40StoredFieldsWriter.FORMAT_MINIMUM, Lucene40StoredFieldsWriter.FORMAT_CURRENT);
+      if (format > Lucene40StoredFieldsWriter.FORMAT_CURRENT)
+        throw new IndexFormatTooNewException(indexStream, format, Lucene40StoredFieldsWriter.FORMAT_MINIMUM, Lucene40StoredFieldsWriter.FORMAT_CURRENT);
+
+      final long indexSize = indexStream.length() - FORMAT_SIZE;
+      
+      if (docStoreOffset != -1) {
+        // We read only a slice out of this shared fields file
+        this.docStoreOffset = docStoreOffset;
+        this.size = size;
+
+        // Verify the file is long enough to hold all of our
+        // docs
+        assert ((int) (indexSize / 8)) >= size + this.docStoreOffset: "indexSize=" + indexSize + " size=" + size + " docStoreOffset=" + docStoreOffset;
+      } else {
+        this.docStoreOffset = 0;
+        this.size = (int) (indexSize >> 3);
+        // Verify two sources of "maxDoc" agree:
+        if (this.size != si.docCount) {
+          throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + this.size + " but segmentInfo shows " + si.docCount);
+        }
+      }
+      numTotalDocs = (int) (indexSize >> 3);
+      success = true;
+    } finally {
+      // With lock-less commits, it's entirely possible (and
+      // fine) to hit a FileNotFound exception above. In
+      // this case, we want to explicitly close any subset
+      // of things that were opened so that we don't have to
+      // wait for a GC to do so.
+      if (!success) {
+        close();
+      }
+    }
+  }
+
+  /**
+   * @throws AlreadyClosedException if this FieldsReader is closed
+   */
+  private void ensureOpen() throws AlreadyClosedException {
+    if (closed) {
+      throw new AlreadyClosedException("this FieldsReader is closed");
+    }
+  }
+
+  /**
+   * Closes the underlying {@link org.apache.lucene.store.IndexInput} streams.
+   * This means that the Fields values will not be accessible.
+   *
+   * @throws IOException
+   */
+  public final void close() throws IOException {
+    if (!closed) {
+      IOUtils.close(fieldsStream, indexStream);
+      closed = true;
+    }
+  }
+
+  public final int size() {
+    return size;
+  }
+
+  private void seekIndex(int docID) throws IOException {
+    indexStream.seek(FORMAT_SIZE + (docID + docStoreOffset) * 8L);
+  }
+
+  public final void visitDocument(int n, StoredFieldVisitor visitor) throws CorruptIndexException, IOException {
+    seekIndex(n);
+    fieldsStream.seek(indexStream.readLong());
+
+    final int numFields = fieldsStream.readVInt();
+    for (int fieldIDX = 0; fieldIDX < numFields; fieldIDX++) {
+      int fieldNumber = fieldsStream.readVInt();
+      FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
+      
+      int bits = fieldsStream.readByte() & 0xFF;
+      assert bits <= (Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_MASK | Lucene40StoredFieldsWriter.FIELD_IS_BINARY): "bits=" + Integer.toHexString(bits);
+
+      switch(visitor.needsField(fieldInfo)) {
+        case YES:
+          readField(visitor, fieldInfo, bits);
+          break;
+        case NO: 
+          skipField(bits);
+          break;
+        case STOP: 
+          return;
+      }
+    }
+  }
+  
+  static final Charset UTF8 = Charset.forName("UTF-8");
+
+  private void readField(StoredFieldVisitor visitor, FieldInfo info, int bits) throws IOException {
+    final int numeric = bits & Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_MASK;
+    if (numeric != 0) {
+      switch(numeric) {
+        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_INT:
+          visitor.intField(info, fieldsStream.readInt());
+          return;
+        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_LONG:
+          visitor.longField(info, fieldsStream.readLong());
+          return;
+        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_FLOAT:
+          visitor.floatField(info, Float.intBitsToFloat(fieldsStream.readInt()));
+          return;
+        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_DOUBLE:
+          visitor.doubleField(info, Double.longBitsToDouble(fieldsStream.readLong()));
+          return;
+        default:
+          throw new CorruptIndexException("Invalid numeric type: " + Integer.toHexString(numeric));
+      }
+    } else { 
+      final int length = fieldsStream.readVInt();
+      byte bytes[] = new byte[length];
+      fieldsStream.readBytes(bytes, 0, length);
+      if ((bits & Lucene40StoredFieldsWriter.FIELD_IS_BINARY) != 0) {
+        visitor.binaryField(info, bytes, 0, bytes.length);
+      } else {
+        visitor.stringField(info, new String(bytes, 0, bytes.length, UTF8));
+      }
+    }
+  }
+  
+  private void skipField(int bits) throws IOException {
+    final int numeric = bits & Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_MASK;
+    if (numeric != 0) {
+      switch(numeric) {
+        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_INT:
+        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_FLOAT:
+          fieldsStream.readInt();
+          return;
+        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_LONG:
+        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_DOUBLE:
+          fieldsStream.readLong();
+          return;
+        default: 
+          throw new CorruptIndexException("Invalid numeric type: " + Integer.toHexString(numeric));
+      }
+    } else {
+      final int length = fieldsStream.readVInt();
+      fieldsStream.seek(fieldsStream.getFilePointer() + length);
+    }
+  }
+
+  /** Returns the length in bytes of each raw document in a
+   *  contiguous range of length numDocs starting with
+   *  startDocID.  Returns the IndexInput (the fieldStream),
+   *  already seeked to the starting point for startDocID.*/
+  public final IndexInput rawDocs(int[] lengths, int startDocID, int numDocs) throws IOException {
+    seekIndex(startDocID);
+    long startOffset = indexStream.readLong();
+    long lastOffset = startOffset;
+    int count = 0;
+    while (count < numDocs) {
+      final long offset;
+      final int docID = docStoreOffset + startDocID + count + 1;
+      assert docID <= numTotalDocs;
+      if (docID < numTotalDocs) 
+        offset = indexStream.readLong();
+      else
+        offset = fieldsStream.length();
+      lengths[count++] = (int) (offset-lastOffset);
+      lastOffset = offset;
+    }
+
+    fieldsStream.seek(startOffset);
+
+    return fieldsStream;
+  }
+  
+  // TODO: split into PreFlexFieldsReader so it can handle this shared docstore crap?
+  // only preflex segments refer to these?
+  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    if (info.getDocStoreOffset() != -1) {
+      assert info.getDocStoreSegment() != null;
+      if (!info.getDocStoreIsCompoundFile()) {
+        files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION));
+        files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", Lucene40StoredFieldsWriter.FIELDS_EXTENSION));
+      }
+    } else {
+      files.add(IndexFileNames.segmentFileName(info.name, "", Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION));
+      files.add(IndexFileNames.segmentFileName(info.name, "", Lucene40StoredFieldsWriter.FIELDS_EXTENSION));
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java
new file mode 100644
index 0000000..f161012
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40StoredFieldsWriter.java
@@ -0,0 +1,336 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Copyright 2004 The Apache Software Foundation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License"); you may not
+ * use this file except in compliance with the License. You may obtain a copy of
+ * the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.SegmentReader;
+import org.apache.lucene.index.MergePolicy.MergeAbortedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/** @lucene.experimental */
+public final class Lucene40StoredFieldsWriter extends StoredFieldsWriter {
+  // NOTE: bit 0 is free here!  You can steal it!
+  static final int FIELD_IS_BINARY = 1 << 1;
+
+  // the old bit 1 << 2 was compressed, is now left out
+
+  private static final int _NUMERIC_BIT_SHIFT = 3;
+  static final int FIELD_IS_NUMERIC_MASK = 0x07 << _NUMERIC_BIT_SHIFT;
+
+  static final int FIELD_IS_NUMERIC_INT = 1 << _NUMERIC_BIT_SHIFT;
+  static final int FIELD_IS_NUMERIC_LONG = 2 << _NUMERIC_BIT_SHIFT;
+  static final int FIELD_IS_NUMERIC_FLOAT = 3 << _NUMERIC_BIT_SHIFT;
+  static final int FIELD_IS_NUMERIC_DOUBLE = 4 << _NUMERIC_BIT_SHIFT;
+  // currently unused: static final int FIELD_IS_NUMERIC_SHORT = 5 << _NUMERIC_BIT_SHIFT;
+  // currently unused: static final int FIELD_IS_NUMERIC_BYTE = 6 << _NUMERIC_BIT_SHIFT;
+
+  // the next possible bits are: 1 << 6; 1 << 7
+  
+  // Lucene 3.0: Removal of compressed fields
+  static final int FORMAT_LUCENE_3_0_NO_COMPRESSED_FIELDS = 2;
+
+  // Lucene 3.2: NumericFields are stored in binary format
+  static final int FORMAT_LUCENE_3_2_NUMERIC_FIELDS = 3;
+
+  // NOTE: if you introduce a new format, make it 1 higher
+  // than the current one, and always change this if you
+  // switch to a new format!
+  static final int FORMAT_CURRENT = FORMAT_LUCENE_3_2_NUMERIC_FIELDS;
+
+  // when removing support for old versions, leave the last supported version here
+  static final int FORMAT_MINIMUM = FORMAT_LUCENE_3_0_NO_COMPRESSED_FIELDS;
+
+  /** Extension of stored fields file */
+  public static final String FIELDS_EXTENSION = "fdt";
+  
+  /** Extension of stored fields index file */
+  public static final String FIELDS_INDEX_EXTENSION = "fdx";
+
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput fieldsStream;
+  private IndexOutput indexStream;
+
+  public Lucene40StoredFieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    assert directory != null;
+    this.directory = directory;
+    this.segment = segment;
+
+    boolean success = false;
+    try {
+      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
+      indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION), context);
+
+      fieldsStream.writeInt(FORMAT_CURRENT);
+      indexStream.writeInt(FORMAT_CURRENT);
+
+      success = true;
+    } finally {
+      if (!success) {
+        abort();
+      }
+    }
+  }
+
+  // Writes the contents of buffer into the fields stream
+  // and adds a new entry for this document into the index
+  // stream.  This assumes the buffer was already written
+  // in the correct fields format.
+  public void startDocument(int numStoredFields) throws IOException {
+    indexStream.writeLong(fieldsStream.getFilePointer());
+    fieldsStream.writeVInt(numStoredFields);
+  }
+
+  public void close() throws IOException {
+    try {
+      IOUtils.close(fieldsStream, indexStream);
+    } finally {
+      fieldsStream = indexStream = null;
+    }
+  }
+
+  public void abort() {
+    try {
+      close();
+    } catch (IOException ignored) {}
+    
+    try {
+      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION));
+    } catch (IOException ignored) {}
+    
+    try {
+      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION));
+    } catch (IOException ignored) {}
+  }
+
+  public final void writeField(FieldInfo info, IndexableField field) throws IOException {
+    fieldsStream.writeVInt(info.number);
+    int bits = 0;
+    final BytesRef bytes;
+    final String string;
+    // TODO: maybe a field should serialize itself?
+    // this way we don't bake into indexer all these
+    // specific encodings for different fields?  and apps
+    // can customize...
+    if (field.numeric()) {
+      switch (field.numericDataType()) {
+        case INT:
+          bits |= FIELD_IS_NUMERIC_INT; break;
+        case LONG:
+          bits |= FIELD_IS_NUMERIC_LONG; break;
+        case FLOAT:
+          bits |= FIELD_IS_NUMERIC_FLOAT; break;
+        case DOUBLE:
+          bits |= FIELD_IS_NUMERIC_DOUBLE; break;
+        default:
+          assert false : "Should never get here";
+      }
+      string = null;
+      bytes = null;
+    } else {
+      bytes = field.binaryValue();
+      if (bytes != null) {
+        bits |= FIELD_IS_BINARY;
+        string = null;
+      } else {
+        string = field.stringValue();
+      }
+    }
+
+    fieldsStream.writeByte((byte) bits);
+
+    if (bytes != null) {
+      fieldsStream.writeVInt(bytes.length);
+      fieldsStream.writeBytes(bytes.bytes, bytes.offset, bytes.length);
+    } else if (string != null) {
+      fieldsStream.writeString(field.stringValue());
+    } else {
+      final Number n = field.numericValue();
+      if (n == null) {
+        throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
+      }
+      switch (field.numericDataType()) {
+        case INT:
+          fieldsStream.writeInt(n.intValue()); break;
+        case LONG:
+          fieldsStream.writeLong(n.longValue()); break;
+        case FLOAT:
+          fieldsStream.writeInt(Float.floatToIntBits(n.floatValue())); break;
+        case DOUBLE:
+          fieldsStream.writeLong(Double.doubleToLongBits(n.doubleValue())); break;
+        default:
+          assert false : "Should never get here";
+      }
+    }
+  }
+
+  /** Bulk write a contiguous series of documents.  The
+   *  lengths array is the length (in bytes) of each raw
+   *  document.  The stream IndexInput is the
+   *  fieldsStream from which we should bulk-copy all
+   *  bytes. */
+  public final void addRawDocuments(IndexInput stream, int[] lengths, int numDocs) throws IOException {
+    long position = fieldsStream.getFilePointer();
+    long start = position;
+    for(int i=0;i<numDocs;i++) {
+      indexStream.writeLong(position);
+      position += lengths[i];
+    }
+    fieldsStream.copyBytes(stream, position-start);
+    assert fieldsStream.getFilePointer() == position;
+  }
+
+  @Override
+  public void finish(int numDocs) throws IOException {
+    if (4+((long) numDocs)*8 != indexStream.getFilePointer())
+      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
+      // we detect that the bug has struck, here, and
+      // throw an exception to prevent the corruption from
+      // entering the index.  See LUCENE-1282 for
+      // details.
+      throw new RuntimeException("mergeFields produced an invalid result: docCount is " + numDocs + " but fdx file size is " + indexStream.getFilePointer() + " file=" + indexStream.toString() + "; now aborting this merge to prevent index corruption");
+  }
+  
+  @Override
+  public int merge(MergeState mergeState) throws IOException {
+    int docCount = 0;
+    // Used for bulk-reading raw bytes for stored fields
+    int rawDocLengths[] = new int[MAX_RAW_MERGE_DOCS];
+    int idx = 0;
+    
+    for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
+      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];
+      Lucene40StoredFieldsReader matchingFieldsReader = null;
+      if (matchingSegmentReader != null) {
+        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();
+        // we can only bulk-copy if the matching reader is also a Lucene40FieldsReader
+        if (fieldsReader != null && fieldsReader instanceof Lucene40StoredFieldsReader) {
+          matchingFieldsReader = (Lucene40StoredFieldsReader) fieldsReader;
+        }
+      }
+    
+      if (reader.liveDocs != null) {
+        docCount += copyFieldsWithDeletions(mergeState,
+                                            reader, matchingFieldsReader, rawDocLengths);
+      } else {
+        docCount += copyFieldsNoDeletions(mergeState,
+                                          reader, matchingFieldsReader, rawDocLengths);
+      }
+    }
+    finish(docCount);
+    return docCount;
+  }
+
+  /** Maximum number of contiguous documents to bulk-copy
+      when merging stored fields */
+  private final static int MAX_RAW_MERGE_DOCS = 4192;
+
+  private int copyFieldsWithDeletions(MergeState mergeState, final MergeState.IndexReaderAndLiveDocs reader,
+                                      final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])
+    throws IOException, MergeAbortedException, CorruptIndexException {
+    int docCount = 0;
+    final int maxDoc = reader.reader.maxDoc();
+    final Bits liveDocs = reader.liveDocs;
+    assert liveDocs != null;
+    if (matchingFieldsReader != null) {
+      // We can bulk-copy because the fieldInfos are "congruent"
+      for (int j = 0; j < maxDoc;) {
+        if (!liveDocs.get(j)) {
+          // skip deleted docs
+          ++j;
+          continue;
+        }
+        // We can optimize this case (doing a bulk byte copy) since the field
+        // numbers are identical
+        int start = j, numDocs = 0;
+        do {
+          j++;
+          numDocs++;
+          if (j >= maxDoc) break;
+          if (!liveDocs.get(j)) {
+            j++;
+            break;
+          }
+        } while(numDocs < MAX_RAW_MERGE_DOCS);
+
+        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);
+        addRawDocuments(stream, rawDocLengths, numDocs);
+        docCount += numDocs;
+        mergeState.checkAbort.work(300 * numDocs);
+      }
+    } else {
+      for (int j = 0; j < maxDoc; j++) {
+        if (!liveDocs.get(j)) {
+          // skip deleted docs
+          continue;
+        }
+        // TODO: this could be more efficient using
+        // FieldVisitor instead of loading/writing entire
+        // doc; ie we just have to renumber the field number
+        // on the fly?
+        // NOTE: it's very important to first assign to doc then pass it to
+        // fieldsWriter.addDocument; see LUCENE-1282
+        Document doc = reader.reader.document(j);
+        addDocument(doc, mergeState.fieldInfos);
+        docCount++;
+        mergeState.checkAbort.work(300);
+      }
+    }
+    return docCount;
+  }
+
+  private int copyFieldsNoDeletions(MergeState mergeState, final MergeState.IndexReaderAndLiveDocs reader,
+                                    final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])
+    throws IOException, MergeAbortedException, CorruptIndexException {
+    final int maxDoc = reader.reader.maxDoc();
+    int docCount = 0;
+    if (matchingFieldsReader != null) {
+      // We can bulk-copy because the fieldInfos are "congruent"
+      while (docCount < maxDoc) {
+        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);
+        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);
+        addRawDocuments(stream, rawDocLengths, len);
+        docCount += len;
+        mergeState.checkAbort.work(300 * len);
+      }
+    } else {
+      for (; docCount < maxDoc; docCount++) {
+        // NOTE: it's very important to first assign to doc then pass it to
+        // fieldsWriter.addDocument; see LUCENE-1282
+        Document doc = reader.reader.document(docCount);
+        addDocument(doc, mergeState.fieldInfos);
+        mergeState.checkAbort.work(300);
+      }
+    }
+    return docCount;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java
new file mode 100644
index 0000000..9eca7fe
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsFormat.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+public class Lucene40TermVectorsFormat extends TermVectorsFormat {
+
+  @Override
+  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
+    return new Lucene40TermVectorsReader(directory, segmentInfo, fieldInfos, context);
+  }
+
+  @Override
+  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    return new Lucene40TermVectorsWriter(directory, segment, context);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    Lucene40TermVectorsReader.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
new file mode 100644
index 0000000..8c3548a
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsReader.java
@@ -0,0 +1,742 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexFormatTooNewException;
+import org.apache.lucene.index.IndexFormatTooOldException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+public class Lucene40TermVectorsReader extends TermVectorsReader {
+
+  // NOTE: if you make a new format, it must be larger than
+  // the current format
+
+  // Changed strings to UTF8 with length-in-bytes not length-in-chars
+  static final int FORMAT_UTF8_LENGTH_IN_BYTES = 4;
+
+  // NOTE: always change this if you switch to a new format!
+  // whenever you add a new format, make it 1 larger (positive version logic)!
+  static final int FORMAT_CURRENT = FORMAT_UTF8_LENGTH_IN_BYTES;
+  
+  // when removing support for old versions, leave the last supported version here
+  static final int FORMAT_MINIMUM = FORMAT_UTF8_LENGTH_IN_BYTES;
+
+  //The size in bytes that the FORMAT_VERSION will take up at the beginning of each file 
+  static final int FORMAT_SIZE = 4;
+
+  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1;
+
+  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2;
+  
+  /** Extension of vectors fields file */
+  static final String VECTORS_FIELDS_EXTENSION = "tvf";
+
+  /** Extension of vectors documents file */
+  static final String VECTORS_DOCUMENTS_EXTENSION = "tvd";
+
+  /** Extension of vectors index file */
+  // TODO: shouldnt be visible to segments reader, preflex should do this itself somehow
+  public static final String VECTORS_INDEX_EXTENSION = "tvx";
+
+  private FieldInfos fieldInfos;
+
+  private IndexInput tvx;
+  private IndexInput tvd;
+  private IndexInput tvf;
+  private int size;
+  private int numTotalDocs;
+
+  // The docID offset where our docs begin in the index
+  // file.  This will be 0 if we have our own private file.
+  private int docStoreOffset;
+  
+  private final int format;
+
+  // used by clone
+  Lucene40TermVectorsReader(FieldInfos fieldInfos, IndexInput tvx, IndexInput tvd, IndexInput tvf, int size, int numTotalDocs, int docStoreOffset, int format) {
+    this.fieldInfos = fieldInfos;
+    this.tvx = tvx;
+    this.tvd = tvd;
+    this.tvf = tvf;
+    this.size = size;
+    this.numTotalDocs = numTotalDocs;
+    this.docStoreOffset = docStoreOffset;
+    this.format = format;
+  }
+    
+  public Lucene40TermVectorsReader(Directory d, SegmentInfo si, FieldInfos fieldInfos, IOContext context)
+    throws CorruptIndexException, IOException {
+    final String segment = si.getDocStoreSegment();
+    final int docStoreOffset = si.getDocStoreOffset();
+    final int size = si.docCount;
+    
+    boolean success = false;
+
+    try {
+      String idxName = IndexFileNames.segmentFileName(segment, "", VECTORS_INDEX_EXTENSION);
+      tvx = d.openInput(idxName, context);
+      format = checkValidFormat(tvx);
+      String fn = IndexFileNames.segmentFileName(segment, "", VECTORS_DOCUMENTS_EXTENSION);
+      tvd = d.openInput(fn, context);
+      final int tvdFormat = checkValidFormat(tvd);
+      fn = IndexFileNames.segmentFileName(segment, "", VECTORS_FIELDS_EXTENSION);
+      tvf = d.openInput(fn, context);
+      final int tvfFormat = checkValidFormat(tvf);
+
+      assert format == tvdFormat;
+      assert format == tvfFormat;
+
+      numTotalDocs = (int) (tvx.length() >> 4);
+
+      if (-1 == docStoreOffset) {
+        this.docStoreOffset = 0;
+        this.size = numTotalDocs;
+        assert size == 0 || numTotalDocs == size;
+      } else {
+        this.docStoreOffset = docStoreOffset;
+        this.size = size;
+        // Verify the file is long enough to hold all of our
+        // docs
+        assert numTotalDocs >= size + docStoreOffset: "numTotalDocs=" + numTotalDocs + " size=" + size + " docStoreOffset=" + docStoreOffset;
+      }
+
+      this.fieldInfos = fieldInfos;
+      success = true;
+    } finally {
+      // With lock-less commits, it's entirely possible (and
+      // fine) to hit a FileNotFound exception above. In
+      // this case, we want to explicitly close any subset
+      // of things that were opened so that we don't have to
+      // wait for a GC to do so.
+      if (!success) {
+        close();
+      }
+    }
+  }
+
+  // Used for bulk copy when merging
+  IndexInput getTvdStream() {
+    return tvd;
+  }
+
+  // Used for bulk copy when merging
+  IndexInput getTvfStream() {
+    return tvf;
+  }
+
+  private void seekTvx(final int docNum) throws IOException {
+    tvx.seek((docNum + docStoreOffset) * 16L + FORMAT_SIZE);
+  }
+
+  boolean canReadRawDocs() {
+    // we can always read raw docs, unless the term vectors
+    // didn't exist
+    return format != 0;
+  }
+
+  /** Retrieve the length (in bytes) of the tvd and tvf
+   *  entries for the next numDocs starting with
+   *  startDocID.  This is used for bulk copying when
+   *  merging segments, if the field numbers are
+   *  congruent.  Once this returns, the tvf & tvd streams
+   *  are seeked to the startDocID. */
+  final void rawDocs(int[] tvdLengths, int[] tvfLengths, int startDocID, int numDocs) throws IOException {
+
+    if (tvx == null) {
+      Arrays.fill(tvdLengths, 0);
+      Arrays.fill(tvfLengths, 0);
+      return;
+    }
+
+    seekTvx(startDocID);
+
+    long tvdPosition = tvx.readLong();
+    tvd.seek(tvdPosition);
+
+    long tvfPosition = tvx.readLong();
+    tvf.seek(tvfPosition);
+
+    long lastTvdPosition = tvdPosition;
+    long lastTvfPosition = tvfPosition;
+
+    int count = 0;
+    while (count < numDocs) {
+      final int docID = docStoreOffset + startDocID + count + 1;
+      assert docID <= numTotalDocs;
+      if (docID < numTotalDocs)  {
+        tvdPosition = tvx.readLong();
+        tvfPosition = tvx.readLong();
+      } else {
+        tvdPosition = tvd.length();
+        tvfPosition = tvf.length();
+        assert count == numDocs-1;
+      }
+      tvdLengths[count] = (int) (tvdPosition-lastTvdPosition);
+      tvfLengths[count] = (int) (tvfPosition-lastTvfPosition);
+      count++;
+      lastTvdPosition = tvdPosition;
+      lastTvfPosition = tvfPosition;
+    }
+  }
+
+  private int checkValidFormat(IndexInput in) throws CorruptIndexException, IOException
+  {
+    int format = in.readInt();
+    if (format < FORMAT_MINIMUM)
+      throw new IndexFormatTooOldException(in, format, FORMAT_MINIMUM, FORMAT_CURRENT);
+    if (format > FORMAT_CURRENT)
+      throw new IndexFormatTooNewException(in, format, FORMAT_MINIMUM, FORMAT_CURRENT);
+    return format;
+  }
+
+  public void close() throws IOException {
+    IOUtils.close(tvx, tvd, tvf);
+  }
+
+  /**
+   * 
+   * @return The number of documents in the reader
+   */
+  int size() {
+    return size;
+  }
+
+  private class TVFields extends Fields {
+    private final int[] fieldNumbers;
+    private final long[] fieldFPs;
+    private final Map<Integer,Integer> fieldNumberToIndex = new HashMap<Integer,Integer>();
+
+    public TVFields(int docID) throws IOException {
+      seekTvx(docID);
+      tvd.seek(tvx.readLong());
+      
+      final int fieldCount = tvd.readVInt();
+      assert fieldCount >= 0;
+      if (fieldCount != 0) {
+        fieldNumbers = new int[fieldCount];
+        fieldFPs = new long[fieldCount];
+        for(int fieldUpto=0;fieldUpto<fieldCount;fieldUpto++) {
+          final int fieldNumber = tvd.readVInt();
+          fieldNumbers[fieldUpto] = fieldNumber;
+          fieldNumberToIndex.put(fieldNumber, fieldUpto);
+        }
+
+        long position = tvx.readLong();
+        fieldFPs[0] = position;
+        for(int fieldUpto=1;fieldUpto<fieldCount;fieldUpto++) {
+          position += tvd.readVLong();
+          fieldFPs[fieldUpto] = position;
+        }
+      } else {
+        // TODO: we can improve writer here, eg write 0 into
+        // tvx file, so we know on first read from tvx that
+        // this doc has no TVs
+        fieldNumbers = null;
+        fieldFPs = null;
+      }
+    }
+    
+    @Override
+    public FieldsEnum iterator() throws IOException {
+
+      return new FieldsEnum() {
+        private int fieldUpto;
+
+        @Override
+        public String next() throws IOException {
+          if (fieldNumbers != null && fieldUpto < fieldNumbers.length) {
+            return fieldInfos.fieldName(fieldNumbers[fieldUpto++]);
+          } else {
+            return null;
+          }
+        }
+
+        @Override
+        public Terms terms() throws IOException {
+          return TVFields.this.terms(fieldInfos.fieldName(fieldNumbers[fieldUpto-1]));
+        }
+      };
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+      if (fieldInfo == null) {
+        // No such field
+        return null;
+      }
+
+      final Integer fieldIndex = fieldNumberToIndex.get(fieldInfo.number);
+      if (fieldIndex == null) {
+        // Term vectors were not indexed for this field
+        return null;
+      }
+
+      return new TVTerms(fieldFPs[fieldIndex]);
+    }
+
+    @Override
+    public int getUniqueFieldCount() {
+      if (fieldNumbers == null) {
+        return 0;
+      } else {
+        return fieldNumbers.length;
+      }
+    }
+  }
+
+  private class TVTerms extends Terms {
+    private final int numTerms;
+    private final long tvfFPStart;
+
+    public TVTerms(long tvfFP) throws IOException {
+      tvf.seek(tvfFP);
+      numTerms = tvf.readVInt();
+      tvfFPStart = tvf.getFilePointer();
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      TVTermsEnum termsEnum;
+      if (reuse instanceof TVTermsEnum) {
+        termsEnum = (TVTermsEnum) reuse;
+        if (!termsEnum.canReuse(tvf)) {
+          termsEnum = new TVTermsEnum();
+        }
+      } else {
+        termsEnum = new TVTermsEnum();
+      }
+      termsEnum.reset(numTerms, tvfFPStart);
+      return termsEnum;
+    }
+
+    @Override
+    public long getUniqueTermCount() {
+      return numTerms;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return -1;
+    }
+
+    @Override
+    public long getSumDocFreq() {
+      // Every term occurs in just one doc:
+      return numTerms;
+    }
+
+    @Override
+    public int getDocCount() {
+      return 1;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      // TODO: really indexer hardwires
+      // this...?  I guess codec could buffer and re-sort...
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+
+  private class TVTermsEnum extends TermsEnum {
+    private final IndexInput origTVF;
+    private final IndexInput tvf;
+    private int numTerms;
+    private int nextTerm;
+    private int freq;
+    private BytesRef lastTerm = new BytesRef();
+    private BytesRef term = new BytesRef();
+    private boolean storePositions;
+    private boolean storeOffsets;
+    private long tvfFP;
+
+    private int[] positions;
+    private int[] startOffsets;
+    private int[] endOffsets;
+
+    // NOTE: tvf is pre-positioned by caller
+    public TVTermsEnum() throws IOException {
+      this.origTVF = Lucene40TermVectorsReader.this.tvf;
+      tvf = (IndexInput) origTVF.clone();
+    }
+
+    public boolean canReuse(IndexInput tvf) {
+      return tvf == origTVF;
+    }
+
+    public void reset(int numTerms, long tvfFPStart) throws IOException {
+      this.numTerms = numTerms;
+      nextTerm = 0;
+      tvf.seek(tvfFPStart);
+      final byte bits = tvf.readByte();
+      storePositions = (bits & STORE_POSITIONS_WITH_TERMVECTOR) != 0;
+      storeOffsets = (bits & STORE_OFFSET_WITH_TERMVECTOR) != 0;
+      tvfFP = 1+tvfFPStart;
+      positions = null;
+      startOffsets = null;
+      endOffsets = null;
+    }
+
+    // NOTE: slow!  (linear scan)
+    @Override
+    public SeekStatus seekCeil(BytesRef text, boolean useCache)
+      throws IOException {
+      if (nextTerm != 0 && text.compareTo(term) < 0) {
+        nextTerm = 0;
+        tvf.seek(tvfFP);
+      }
+
+      while (next() != null) {
+        final int cmp = text.compareTo(term);
+        if (cmp < 0) {
+          return SeekStatus.NOT_FOUND;
+        } else if (cmp == 0) {
+          return SeekStatus.FOUND;
+        }
+      }
+
+      return SeekStatus.END;
+    }
+
+    @Override
+    public void seekExact(long ord) {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      if (nextTerm >= numTerms) {
+        return null;
+      }
+      term.copyBytes(lastTerm);
+      final int start = tvf.readVInt();
+      final int deltaLen = tvf.readVInt();
+      term.length = start + deltaLen;
+      term.grow(term.length);
+      tvf.readBytes(term.bytes, start, deltaLen);
+      freq = tvf.readVInt();
+
+      if (storePositions) {
+        // TODO: we could maybe reuse last array, if we can
+        // somehow be careful about consumer never using two
+        // D&PEnums at once...
+        positions = new int[freq];
+        int pos = 0;
+        for(int posUpto=0;posUpto<freq;posUpto++) {
+          pos += tvf.readVInt();
+          positions[posUpto] = pos;
+        }
+      }
+
+      if (storeOffsets) {
+        startOffsets = new int[freq];
+        endOffsets = new int[freq];
+        int offset = 0;
+        for(int posUpto=0;posUpto<freq;posUpto++) {
+          startOffsets[posUpto] = offset + tvf.readVInt();
+          offset = endOffsets[posUpto] = startOffsets[posUpto] + tvf.readVInt();
+        }
+      }
+
+      lastTerm.copyBytes(term);
+      nextTerm++;
+      return term;
+    }
+
+    @Override
+    public BytesRef term() {
+      return term;
+    }
+
+    @Override
+    public long ord() {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public int docFreq() {
+      return 1;
+    }
+
+    @Override
+    public long totalTermFreq() {
+      return freq;
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs /* ignored */) throws IOException {
+      TVDocsEnum docsEnum;
+      if (reuse != null && reuse instanceof TVDocsEnum) {
+        docsEnum = (TVDocsEnum) reuse;
+      } else {
+        docsEnum = new TVDocsEnum();
+      }
+      docsEnum.reset(liveDocs, freq);
+      return docsEnum;
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
+      if (!storePositions && !storeOffsets) {
+        return null;
+      }
+      
+      TVDocsAndPositionsEnum docsAndPositionsEnum;
+      if (reuse != null) {
+        docsAndPositionsEnum = (TVDocsAndPositionsEnum) reuse;
+        if (docsAndPositionsEnum.canReuse(storeOffsets)) {
+          docsAndPositionsEnum = (TVDocsAndPositionsEnum) reuse;
+        } else {
+          docsAndPositionsEnum = new TVDocsAndPositionsEnum(storeOffsets);
+        }
+      } else {
+        docsAndPositionsEnum = new TVDocsAndPositionsEnum(storeOffsets);
+      }
+      docsAndPositionsEnum.reset(liveDocs, positions, startOffsets, endOffsets);
+      return docsAndPositionsEnum;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      // TODO: really indexer hardwires
+      // this...?  I guess codec could buffer and re-sort...
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+
+  // NOTE: sort of a silly class, since you can get the
+  // freq() already by TermsEnum.totalTermFreq
+  private static class TVDocsEnum extends DocsEnum {
+    private boolean didNext;
+    private int doc = -1;
+    private int freq;
+    private Bits liveDocs;
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return (doc = 0);
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+      if (!didNext && target == 0) {
+        return nextDoc();
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    public void reset(Bits liveDocs, int freq) {
+      this.liveDocs = liveDocs;
+      this.freq = freq;
+      this.doc = -1;
+      didNext = false;
+    }
+  }
+
+  private static class TVDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private final OffsetAttribute offsetAtt;
+    private boolean didNext;
+    private int doc = -1;
+    private int nextPos;
+    private Bits liveDocs;
+    private int[] positions;
+    private int[] startOffsets;
+    private int[] endOffsets;
+
+    public TVDocsAndPositionsEnum(boolean storeOffsets) {
+      if (storeOffsets) {
+        offsetAtt = attributes().addAttribute(OffsetAttribute.class);
+      } else {
+        offsetAtt = null;
+      }
+    }
+
+    public boolean canReuse(boolean storeOffsets) {
+      return storeOffsets == (offsetAtt != null);
+    }
+
+    @Override
+    public int freq() {
+      if (positions != null) {
+        return positions.length;
+      } else {
+        assert startOffsets != null;
+        return startOffsets.length;
+      }
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return (doc = 0);
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+      if (!didNext && target == 0) {
+        return nextDoc();
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    public void reset(Bits liveDocs, int[] positions, int[] startOffsets, int[] endOffsets) {
+      this.liveDocs = liveDocs;
+      this.positions = positions;
+      this.startOffsets = startOffsets;
+      assert (offsetAtt != null) == (startOffsets != null);
+      this.endOffsets = endOffsets;
+      this.doc = -1;
+      didNext = false;
+      nextPos = 0;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      return null;
+    }
+
+    @Override
+    public boolean hasPayload() {
+      return false;
+    }
+
+    @Override
+    public int nextPosition() {
+      assert (positions != null && nextPos < positions.length) ||
+        startOffsets != null && nextPos < startOffsets.length;
+
+      if (startOffsets != null) {
+        offsetAtt.setOffset(startOffsets[nextPos],
+                            endOffsets[nextPos]);
+      }
+      if (positions != null) {
+        return positions[nextPos++];
+      } else {
+        nextPos++;
+        return -1;
+      }
+    }
+  }
+
+  @Override
+  public Fields get(int docID) throws IOException {
+    if (docID < 0 || docID >= numTotalDocs) {
+      throw new IllegalArgumentException("doID=" + docID + " is out of bounds [0.." + (numTotalDocs-1) + "]");
+    }
+    if (tvx != null) {
+      Fields fields = new TVFields(docID);
+      if (fields.getUniqueFieldCount() == 0) {
+        // TODO: we can improve writer here, eg write 0 into
+        // tvx file, so we know on first read from tvx that
+        // this doc has no TVs
+        return null;
+      } else {
+        return fields;
+      }
+    } else {
+      return null;
+    }
+  }
+
+  @Override
+  public TermVectorsReader clone() {
+    IndexInput cloneTvx = null;
+    IndexInput cloneTvd = null;
+    IndexInput cloneTvf = null;
+
+    // These are null when a TermVectorsReader was created
+    // on a segment that did not have term vectors saved
+    if (tvx != null && tvd != null && tvf != null) {
+      cloneTvx = (IndexInput) tvx.clone();
+      cloneTvd = (IndexInput) tvd.clone();
+      cloneTvf = (IndexInput) tvf.clone();
+    }
+    
+    return new Lucene40TermVectorsReader(fieldInfos, cloneTvx, cloneTvd, cloneTvf, size, numTotalDocs, docStoreOffset, format);
+  }
+  
+  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    if (info.getHasVectors()) {
+      if (info.getDocStoreOffset() != -1) {
+        assert info.getDocStoreSegment() != null;
+        if (!info.getDocStoreIsCompoundFile()) {
+          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_INDEX_EXTENSION));
+          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_FIELDS_EXTENSION));
+          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_DOCUMENTS_EXTENSION));
+        }
+      } else {
+        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_INDEX_EXTENSION));
+        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_FIELDS_EXTENSION));
+        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_DOCUMENTS_EXTENSION));
+      }
+    }
+  }
+}
+
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java
new file mode 100644
index 0000000..09ba8b0
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/Lucene40TermVectorsWriter.java
@@ -0,0 +1,377 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.MergePolicy.MergeAbortedException;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.SegmentReader;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+
+// TODO: make a new 4.0 TV format that encodes better
+//   - use startOffset (not endOffset) as base for delta on
+//     next startOffset because today for syns or ngrams or
+//     WDF or shingles etc. we are encoding negative vints
+//     (= slow, 5 bytes per)
+//   - if doc has no term vectors, write 0 into the tvx
+//     file; saves a seek to tvd only to read a 0 vint (and
+//     saves a byte in tvd)
+
+public final class Lucene40TermVectorsWriter extends TermVectorsWriter {
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput tvx = null, tvd = null, tvf = null;
+
+  public Lucene40TermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    this.directory = directory;
+    this.segment = segment;
+    boolean success = false;
+    try {
+      // Open files for TermVector storage
+      tvx = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION), context);
+      tvx.writeInt(Lucene40TermVectorsReader.FORMAT_CURRENT);
+      tvd = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_DOCUMENTS_EXTENSION), context);
+      tvd.writeInt(Lucene40TermVectorsReader.FORMAT_CURRENT);
+      tvf = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_FIELDS_EXTENSION), context);
+      tvf.writeInt(Lucene40TermVectorsReader.FORMAT_CURRENT);
+      success = true;
+    } finally {
+      if (!success) {
+        abort();
+      }
+    }
+  }
+ 
+  @Override
+  public void startDocument(int numVectorFields) throws IOException {
+    lastFieldName = null;
+    this.numVectorFields = numVectorFields;
+    tvx.writeLong(tvd.getFilePointer());
+    tvx.writeLong(tvf.getFilePointer());
+    tvd.writeVInt(numVectorFields);
+    fieldCount = 0;
+    fps = ArrayUtil.grow(fps, numVectorFields);
+  }
+  
+  private long fps[] = new long[10]; // pointers to the tvf before writing each field 
+  private int fieldCount = 0;        // number of fields we have written so far for this document
+  private int numVectorFields = 0;   // total number of fields we will write for this document
+  private String lastFieldName;
+
+  @Override
+  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets) throws IOException {
+    assert lastFieldName == null || info.name.compareTo(lastFieldName) > 0: "fieldName=" + info.name + " lastFieldName=" + lastFieldName;
+    lastFieldName = info.name;
+    this.positions = positions;
+    this.offsets = offsets;
+    lastTerm.length = 0;
+    fps[fieldCount++] = tvf.getFilePointer();
+    tvd.writeVInt(info.number);
+    tvf.writeVInt(numTerms);
+    byte bits = 0x0;
+    if (positions)
+      bits |= Lucene40TermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR;
+    if (offsets)
+      bits |= Lucene40TermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR;
+    tvf.writeByte(bits);
+    
+    assert fieldCount <= numVectorFields;
+    if (fieldCount == numVectorFields) {
+      // last field of the document
+      // this is crazy because the file format is crazy!
+      for (int i = 1; i < fieldCount; i++) {
+        tvd.writeVLong(fps[i] - fps[i-1]);
+      }
+    }
+  }
+  
+  private final BytesRef lastTerm = new BytesRef(10);
+
+  // NOTE: we override addProx, so we don't need to buffer when indexing.
+  // we also don't buffer during bulk merges.
+  private int offsetStartBuffer[] = new int[10];
+  private int offsetEndBuffer[] = new int[10];
+  private int offsetIndex = 0;
+  private int offsetFreq = 0;
+  private boolean positions = false;
+  private boolean offsets = false;
+
+  @Override
+  public void startTerm(BytesRef term, int freq) throws IOException {
+    final int prefix = StringHelper.bytesDifference(lastTerm, term);
+    final int suffix = term.length - prefix;
+    tvf.writeVInt(prefix);
+    tvf.writeVInt(suffix);
+    tvf.writeBytes(term.bytes, term.offset + prefix, suffix);
+    tvf.writeVInt(freq);
+    lastTerm.copyBytes(term);
+    lastPosition = lastOffset = 0;
+    
+    if (offsets && positions) {
+      // we might need to buffer if its a non-bulk merge
+      offsetStartBuffer = ArrayUtil.grow(offsetStartBuffer, freq);
+      offsetEndBuffer = ArrayUtil.grow(offsetEndBuffer, freq);
+      offsetIndex = 0;
+      offsetFreq = freq;
+    }
+  }
+
+  int lastPosition = 0;
+  int lastOffset = 0;
+
+  @Override
+  public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
+    // TODO: technically we could just copy bytes and not re-encode if we knew the length...
+    if (positions != null) {
+      for (int i = 0; i < numProx; i++) {
+        tvf.writeVInt(positions.readVInt());
+      }
+    }
+    
+    if (offsets != null) {
+      for (int i = 0; i < numProx; i++) {
+        tvf.writeVInt(offsets.readVInt());
+        tvf.writeVInt(offsets.readVInt());
+      }
+    }
+  }
+
+  @Override
+  public void addPosition(int position, int startOffset, int endOffset) throws IOException {
+    if (positions && offsets) {
+      // write position delta
+      tvf.writeVInt(position - lastPosition);
+      lastPosition = position;
+      
+      // buffer offsets
+      offsetStartBuffer[offsetIndex] = startOffset;
+      offsetEndBuffer[offsetIndex] = endOffset;
+      offsetIndex++;
+      
+      // dump buffer if we are done
+      if (offsetIndex == offsetFreq) {
+        for (int i = 0; i < offsetIndex; i++) {
+          tvf.writeVInt(offsetStartBuffer[i] - lastOffset);
+          tvf.writeVInt(offsetEndBuffer[i] - offsetStartBuffer[i]);
+          lastOffset = offsetEndBuffer[i];
+        }
+      }
+    } else if (positions) {
+      // write position delta
+      tvf.writeVInt(position - lastPosition);
+      lastPosition = position;
+    } else if (offsets) {
+      // write offset deltas
+      tvf.writeVInt(startOffset - lastOffset);
+      tvf.writeVInt(endOffset - startOffset);
+      lastOffset = endOffset;
+    }
+  }
+
+  @Override
+  public void abort() {
+    try {
+      close();
+    } catch (IOException ignored) {}
+    
+    try {
+      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION));
+    } catch (IOException ignored) {}
+    
+    try {
+      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_DOCUMENTS_EXTENSION));
+    } catch (IOException ignored) {}
+    
+    try {
+      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_FIELDS_EXTENSION));
+    } catch (IOException ignored) {}
+  }
+
+  /**
+   * Do a bulk copy of numDocs documents from reader to our
+   * streams.  This is used to expedite merging, if the
+   * field numbers are congruent.
+   */
+  private void addRawDocuments(Lucene40TermVectorsReader reader, int[] tvdLengths, int[] tvfLengths, int numDocs) throws IOException {
+    long tvdPosition = tvd.getFilePointer();
+    long tvfPosition = tvf.getFilePointer();
+    long tvdStart = tvdPosition;
+    long tvfStart = tvfPosition;
+    for(int i=0;i<numDocs;i++) {
+      tvx.writeLong(tvdPosition);
+      tvdPosition += tvdLengths[i];
+      tvx.writeLong(tvfPosition);
+      tvfPosition += tvfLengths[i];
+    }
+    tvd.copyBytes(reader.getTvdStream(), tvdPosition-tvdStart);
+    tvf.copyBytes(reader.getTvfStream(), tvfPosition-tvfStart);
+    assert tvd.getFilePointer() == tvdPosition;
+    assert tvf.getFilePointer() == tvfPosition;
+  }
+
+  @Override
+  public final int merge(MergeState mergeState) throws IOException {
+    // Used for bulk-reading raw bytes for term vectors
+    int rawDocLengths[] = new int[MAX_RAW_MERGE_DOCS];
+    int rawDocLengths2[] = new int[MAX_RAW_MERGE_DOCS];
+
+    int idx = 0;
+    int numDocs = 0;
+    for (final MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
+      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];
+      Lucene40TermVectorsReader matchingVectorsReader = null;
+      if (matchingSegmentReader != null) {
+        TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();
+
+        if (vectorsReader != null && vectorsReader instanceof Lucene40TermVectorsReader) {
+          // If the TV* files are an older format then they cannot read raw docs:
+          if (((Lucene40TermVectorsReader)vectorsReader).canReadRawDocs()) {
+            matchingVectorsReader = (Lucene40TermVectorsReader) vectorsReader;
+          }
+        }
+      }
+      if (reader.liveDocs != null) {
+        numDocs += copyVectorsWithDeletions(mergeState, matchingVectorsReader, reader, rawDocLengths, rawDocLengths2);
+      } else {
+        numDocs += copyVectorsNoDeletions(mergeState, matchingVectorsReader, reader, rawDocLengths, rawDocLengths2);
+      }
+    }
+    finish(numDocs);
+    return numDocs;
+  }
+
+  /** Maximum number of contiguous documents to bulk-copy
+      when merging term vectors */
+  private final static int MAX_RAW_MERGE_DOCS = 4192;
+
+  private int copyVectorsWithDeletions(MergeState mergeState,
+                                        final Lucene40TermVectorsReader matchingVectorsReader,
+                                        final MergeState.IndexReaderAndLiveDocs reader,
+                                        int rawDocLengths[],
+                                        int rawDocLengths2[])
+          throws IOException, MergeAbortedException {
+    final int maxDoc = reader.reader.maxDoc();
+    final Bits liveDocs = reader.liveDocs;
+    int totalNumDocs = 0;
+    if (matchingVectorsReader != null) {
+      // We can bulk-copy because the fieldInfos are "congruent"
+      for (int docNum = 0; docNum < maxDoc;) {
+        if (!liveDocs.get(docNum)) {
+          // skip deleted docs
+          ++docNum;
+          continue;
+        }
+        // We can optimize this case (doing a bulk byte copy) since the field
+        // numbers are identical
+        int start = docNum, numDocs = 0;
+        do {
+          docNum++;
+          numDocs++;
+          if (docNum >= maxDoc) break;
+          if (!liveDocs.get(docNum)) {
+            docNum++;
+            break;
+          }
+        } while(numDocs < MAX_RAW_MERGE_DOCS);
+        
+        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);
+        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);
+        totalNumDocs += numDocs;
+        mergeState.checkAbort.work(300 * numDocs);
+      }
+    } else {
+      for (int docNum = 0; docNum < maxDoc; docNum++) {
+        if (!liveDocs.get(docNum)) {
+          // skip deleted docs
+          continue;
+        }
+        
+        // NOTE: it's very important to first assign to vectors then pass it to
+        // termVectorsWriter.addAllDocVectors; see LUCENE-1282
+        Fields vectors = reader.reader.getTermVectors(docNum);
+        addAllDocVectors(vectors, mergeState.fieldInfos);
+        totalNumDocs++;
+        mergeState.checkAbort.work(300);
+      }
+    }
+    return totalNumDocs;
+  }
+  
+  private int copyVectorsNoDeletions(MergeState mergeState,
+                                      final Lucene40TermVectorsReader matchingVectorsReader,
+                                      final MergeState.IndexReaderAndLiveDocs reader,
+                                      int rawDocLengths[],
+                                      int rawDocLengths2[])
+          throws IOException, MergeAbortedException {
+    final int maxDoc = reader.reader.maxDoc();
+    if (matchingVectorsReader != null) {
+      // We can bulk-copy because the fieldInfos are "congruent"
+      int docCount = 0;
+      while (docCount < maxDoc) {
+        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);
+        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);
+        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);
+        docCount += len;
+        mergeState.checkAbort.work(300 * len);
+      }
+    } else {
+      for (int docNum = 0; docNum < maxDoc; docNum++) {
+        // NOTE: it's very important to first assign to vectors then pass it to
+        // termVectorsWriter.addAllDocVectors; see LUCENE-1282
+        Fields vectors = reader.reader.getTermVectors(docNum);
+        addAllDocVectors(vectors, mergeState.fieldInfos);
+        mergeState.checkAbort.work(300);
+      }
+    }
+    return maxDoc;
+  }
+  
+  @Override
+  public void finish(int numDocs) throws IOException {
+    if (4+((long) numDocs)*16 != tvx.getFilePointer())
+      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
+      // we detect that the bug has struck, here, and
+      // throw an exception to prevent the corruption from
+      // entering the index.  See LUCENE-1282 for
+      // details.
+      throw new RuntimeException("mergeVectors produced an invalid result: mergedDocs is " + numDocs + " but tvx size is " + tvx.getFilePointer() + " file=" + tvx.toString() + "; now aborting this merge to prevent index corruption");
+  }
+
+  /** Close all streams. */
+  @Override
+  public void close() throws IOException {
+    // make an effort to close all streams we can but remember and re-throw
+    // the first exception encountered in this process
+    IOUtils.close(tvx, tvd, tvf);
+    tvx = tvd = tvf = null;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/package.html b/lucene/src/java/org/apache/lucene/codecs/lucene40/package.html
new file mode 100644
index 0000000..aca1dc4
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Standard Codec
+</body>
+</html>
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Bytes.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Bytes.java
new file mode 100644
index 0000000..83866a0
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Bytes.java
@@ -0,0 +1,601 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Base class for specific Bytes Reader/Writer implementations */
+import java.io.IOException;
+import java.util.Comparator;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.DocValue;
+import org.apache.lucene.index.DocValues.SortedSource;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.ByteBlockPool.Allocator;
+import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
+import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefHash.TrackingDirectBytesStartArray;
+import org.apache.lucene.util.BytesRefHash;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Provides concrete Writer/Reader implementations for <tt>byte[]</tt> value per
+ * document. There are 6 package-private default implementations of this, for
+ * all combinations of {@link Mode#DEREF}/{@link Mode#STRAIGHT} x fixed-length/variable-length.
+ * 
+ * <p>
+ * NOTE: Currently the total amount of byte[] data stored (across a single
+ * segment) cannot exceed 2GB.
+ * </p>
+ * <p>
+ * NOTE: Each byte[] must be <= 32768 bytes in length
+ * </p>
+ * 
+ * @lucene.experimental
+ */
+public final class Bytes {
+
+  static final String DV_SEGMENT_SUFFIX = "dv";
+
+  // TODO - add bulk copy where possible
+  private Bytes() { /* don't instantiate! */
+  }
+
+  /**
+   * Defines the {@link Writer}s store mode. The writer will either store the
+   * bytes sequentially ({@link #STRAIGHT}, dereferenced ({@link #DEREF}) or
+   * sorted ({@link #SORTED})
+   * 
+   * @lucene.experimental
+   */
+  public static enum Mode {
+    /**
+     * Mode for sequentially stored bytes
+     */
+    STRAIGHT,
+    /**
+     * Mode for dereferenced stored bytes
+     */
+    DEREF,
+    /**
+     * Mode for sorted stored bytes
+     */
+    SORTED
+  };
+
+  /**
+   * Creates a new <tt>byte[]</tt> {@link Writer} instances for the given
+   * directory.
+   * 
+   * @param dir
+   *          the directory to write the values to
+   * @param id
+   *          the id used to create a unique file name. Usually composed out of
+   *          the segment name and a unique id per segment.
+   * @param mode
+   *          the writers store mode
+   * @param fixedSize
+   *          <code>true</code> if all bytes subsequently passed to the
+   *          {@link Writer} will have the same length
+   * @param sortComparator {@link BytesRef} comparator used by sorted variants. 
+   *        If <code>null</code> {@link BytesRef#getUTF8SortedAsUnicodeComparator()}
+   *        is used instead
+   * @param bytesUsed
+   *          an {@link AtomicLong} instance to track the used bytes within the
+   *          {@link Writer}. A call to {@link Writer#finish(int)} will release
+   *          all internally used resources and frees the memory tracking
+   *          reference.
+   * @param context 
+   * @return a new {@link Writer} instance
+   * @throws IOException
+   *           if the files for the writer can not be created.
+   */
+  public static Writer getWriter(Directory dir, String id, Mode mode,
+      boolean fixedSize, Comparator<BytesRef> sortComparator, Counter bytesUsed, IOContext context)
+      throws IOException {
+    // TODO -- i shouldn't have to specify fixed? can
+    // track itself & do the write thing at write time?
+    if (sortComparator == null) {
+      sortComparator = BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    if (fixedSize) {
+      if (mode == Mode.STRAIGHT) {
+        return new FixedStraightBytesImpl.Writer(dir, id, bytesUsed, context);
+      } else if (mode == Mode.DEREF) {
+        return new FixedDerefBytesImpl.Writer(dir, id, bytesUsed, context);
+      } else if (mode == Mode.SORTED) {
+        return new FixedSortedBytesImpl.Writer(dir, id, sortComparator, bytesUsed, context);
+      }
+    } else {
+      if (mode == Mode.STRAIGHT) {
+        return new VarStraightBytesImpl.Writer(dir, id, bytesUsed, context);
+      } else if (mode == Mode.DEREF) {
+        return new VarDerefBytesImpl.Writer(dir, id, bytesUsed, context);
+      } else if (mode == Mode.SORTED) {
+        return new VarSortedBytesImpl.Writer(dir, id, sortComparator, bytesUsed, context);
+      }
+    }
+
+    throw new IllegalArgumentException("");
+  }
+
+  /**
+   * Creates a new {@link DocValues} instance that provides either memory
+   * resident or iterative access to a per-document stored <tt>byte[]</tt>
+   * value. The returned {@link DocValues} instance will be initialized without
+   * consuming a significant amount of memory.
+   * 
+   * @param dir
+   *          the directory to load the {@link DocValues} from.
+   * @param id
+   *          the file ID in the {@link Directory} to load the values from.
+   * @param mode
+   *          the mode used to store the values
+   * @param fixedSize
+   *          <code>true</code> iff the values are stored with fixed-size,
+   *          otherwise <code>false</code>
+   * @param maxDoc
+   *          the number of document values stored for the given ID
+   * @param sortComparator {@link BytesRef} comparator used by sorted variants. 
+   *        If <code>null</code> {@link BytesRef#getUTF8SortedAsUnicodeComparator()}
+   *        is used instead
+   * @return an initialized {@link DocValues} instance.
+   * @throws IOException
+   *           if an {@link IOException} occurs
+   */
+  public static DocValues getValues(Directory dir, String id, Mode mode,
+      boolean fixedSize, int maxDoc, Comparator<BytesRef> sortComparator, IOContext context) throws IOException {
+    if (sortComparator == null) {
+      sortComparator = BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+    // TODO -- I can peek @ header to determing fixed/mode?
+    if (fixedSize) {
+      if (mode == Mode.STRAIGHT) {
+        return new FixedStraightBytesImpl.FixedStraightReader(dir, id, maxDoc, context);
+      } else if (mode == Mode.DEREF) {
+        return new FixedDerefBytesImpl.FixedDerefReader(dir, id, maxDoc, context);
+      } else if (mode == Mode.SORTED) {
+        return new FixedSortedBytesImpl.Reader(dir, id, maxDoc, context, Type.BYTES_FIXED_SORTED, sortComparator);
+      }
+    } else {
+      if (mode == Mode.STRAIGHT) {
+        return new VarStraightBytesImpl.VarStraightReader(dir, id, maxDoc, context);
+      } else if (mode == Mode.DEREF) {
+        return new VarDerefBytesImpl.VarDerefReader(dir, id, maxDoc, context);
+      } else if (mode == Mode.SORTED) {
+        return new VarSortedBytesImpl.Reader(dir, id, maxDoc,context, Type.BYTES_VAR_SORTED, sortComparator);
+      }
+    }
+
+    throw new IllegalArgumentException("Illegal Mode: " + mode);
+  }
+
+  // TODO open up this API?
+  static abstract class BytesSourceBase extends Source {
+    private final PagedBytes pagedBytes;
+    protected final IndexInput datIn;
+    protected final IndexInput idxIn;
+    protected final static int PAGED_BYTES_BITS = 15;
+    protected final PagedBytes.Reader data;
+    protected final long totalLengthInBytes;
+    
+
+    protected BytesSourceBase(IndexInput datIn, IndexInput idxIn,
+        PagedBytes pagedBytes, long bytesToRead, Type type) throws IOException {
+      super(type);
+      assert bytesToRead <= datIn.length() : " file size is less than the expected size diff: "
+          + (bytesToRead - datIn.length()) + " pos: " + datIn.getFilePointer();
+      this.datIn = datIn;
+      this.totalLengthInBytes = bytesToRead;
+      this.pagedBytes = pagedBytes;
+      this.pagedBytes.copy(datIn, bytesToRead);
+      data = pagedBytes.freeze(true);
+      this.idxIn = idxIn;
+    }
+  }
+  
+  // TODO: open up this API?!
+  static abstract class BytesWriterBase extends Writer {
+    private final String id;
+    private IndexOutput idxOut;
+    private IndexOutput datOut;
+    protected BytesRef bytesRef = new BytesRef();
+    private final Directory dir;
+    private final String codecName;
+    private final int version;
+    private final IOContext context;
+
+    protected BytesWriterBase(Directory dir, String id, String codecName,
+        int version, Counter bytesUsed, IOContext context) throws IOException {
+      super(bytesUsed);
+      this.id = id;
+      this.dir = dir;
+      this.codecName = codecName;
+      this.version = version;
+      this.context = context;
+    }
+    
+    protected IndexOutput getOrCreateDataOut() throws IOException {
+      if (datOut == null) {
+        boolean success = false;
+        try {
+          datOut = dir.createOutput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
+              DATA_EXTENSION), context);
+          CodecUtil.writeHeader(datOut, codecName, version);
+          success = true;
+        } finally {
+          if (!success) {
+            IOUtils.closeWhileHandlingException(datOut);
+          }
+        }
+      }
+      return datOut;
+    }
+    
+    protected IndexOutput getIndexOut() {
+      return idxOut;
+    }
+    
+    protected IndexOutput getDataOut() {
+      return datOut;
+    }
+
+    protected IndexOutput getOrCreateIndexOut() throws IOException {
+      boolean success = false;
+      try {
+        if (idxOut == null) {
+          idxOut = dir.createOutput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
+              INDEX_EXTENSION), context);
+          CodecUtil.writeHeader(idxOut, codecName, version);
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(idxOut);
+        }
+      }
+      return idxOut;
+    }
+    /**
+     * Must be called only with increasing docIDs. It's OK for some docIDs to be
+     * skipped; they will be filled with 0 bytes.
+     */
+    @Override
+    public abstract void add(int docID, BytesRef bytes) throws IOException;
+
+    @Override
+    public abstract void finish(int docCount) throws IOException;
+
+    @Override
+    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
+      add(docID, currentMergeSource.getBytes(sourceDoc, bytesRef));
+    }
+
+    @Override
+    public void add(int docID, DocValue docValue) throws IOException {
+      final BytesRef ref;
+      if ((ref = docValue.getBytes()) != null) {
+        add(docID, ref);
+      }
+    }
+  }
+
+  /**
+   * Opens all necessary files, but does not read any data in until you call
+   * {@link #load}.
+   */
+  static abstract class BytesReaderBase extends DocValues {
+    protected final IndexInput idxIn;
+    protected final IndexInput datIn;
+    protected final int version;
+    protected final String id;
+    protected final Type type;
+
+    protected BytesReaderBase(Directory dir, String id, String codecName,
+        int maxVersion, boolean doIndex, IOContext context, Type type) throws IOException {
+      IndexInput dataIn = null;
+      IndexInput indexIn = null;
+      boolean success = false;
+      try {
+        dataIn = dir.openInput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
+                                                              Writer.DATA_EXTENSION), context);
+        version = CodecUtil.checkHeader(dataIn, codecName, maxVersion, maxVersion);
+        if (doIndex) {
+          indexIn = dir.openInput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
+                                                                 Writer.INDEX_EXTENSION), context);
+          final int version2 = CodecUtil.checkHeader(indexIn, codecName,
+                                                     maxVersion, maxVersion);
+          assert version == version2;
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(dataIn, indexIn);
+        }
+      }
+      datIn = dataIn;
+      idxIn = indexIn;
+      this.type = type;
+      this.id = id;
+    }
+
+    /**
+     * clones and returns the data {@link IndexInput}
+     */
+    protected final IndexInput cloneData() {
+      assert datIn != null;
+      return (IndexInput) datIn.clone();
+    }
+
+    /**
+     * clones and returns the indexing {@link IndexInput}
+     */
+    protected final IndexInput cloneIndex() {
+      assert idxIn != null;
+      return (IndexInput) idxIn.clone();
+    }
+
+    @Override
+    public void close() throws IOException {
+      try {
+        super.close();
+      } finally {
+         IOUtils.close(datIn, idxIn);
+      }
+    }
+
+    @Override
+    public Type type() {
+      return type;
+    }
+    
+  }
+  
+  static abstract class DerefBytesWriterBase extends BytesWriterBase {
+    protected int size = -1;
+    protected int lastDocId = -1;
+    protected int[] docToEntry;
+    protected final BytesRefHash hash;
+    protected long maxBytes = 0;
+    
+    protected DerefBytesWriterBase(Directory dir, String id, String codecName,
+        int codecVersion, Counter bytesUsed, IOContext context)
+        throws IOException {
+      this(dir, id, codecName, codecVersion, new DirectTrackingAllocator(
+          ByteBlockPool.BYTE_BLOCK_SIZE, bytesUsed), bytesUsed, context);
+    }
+
+    protected DerefBytesWriterBase(Directory dir, String id, String codecName, int codecVersion, Allocator allocator,
+        Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, codecName, codecVersion, bytesUsed, context);
+      hash = new BytesRefHash(new ByteBlockPool(allocator),
+          BytesRefHash.DEFAULT_CAPACITY, new TrackingDirectBytesStartArray(
+              BytesRefHash.DEFAULT_CAPACITY, bytesUsed));
+      docToEntry = new int[1];
+      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT);
+    }
+    
+    protected static int writePrefixLength(DataOutput datOut, BytesRef bytes)
+        throws IOException {
+      if (bytes.length < 128) {
+        datOut.writeByte((byte) bytes.length);
+        return 1;
+      } else {
+        datOut.writeByte((byte) (0x80 | (bytes.length >> 8)));
+        datOut.writeByte((byte) (bytes.length & 0xff));
+        return 2;
+      }
+    }
+
+    @Override
+    public void add(int docID, BytesRef bytes) throws IOException {
+      if (bytes.length == 0) { // default value - skip it
+        return;
+      }
+      checkSize(bytes);
+      fillDefault(docID);
+      int ord = hash.add(bytes);
+      if (ord < 0) {
+        ord = (-ord) - 1;
+      } else {
+        maxBytes += bytes.length;
+      }
+      
+      
+      docToEntry[docID] = ord;
+      lastDocId = docID;
+    }
+    
+    protected void fillDefault(int docID) {
+      if (docID >= docToEntry.length) {
+        final int size = docToEntry.length;
+        docToEntry = ArrayUtil.grow(docToEntry, 1 + docID);
+        bytesUsed.addAndGet((docToEntry.length - size)
+            * RamUsageEstimator.NUM_BYTES_INT);
+      }
+      assert size >= 0;
+      BytesRef ref = new BytesRef(size);
+      ref.length = size;
+      int ord = hash.add(ref);
+      if (ord < 0) {
+        ord = (-ord) - 1;
+      }
+      for (int i = lastDocId+1; i < docID; i++) {
+        docToEntry[i] = ord;
+      }
+    }
+    
+    protected void checkSize(BytesRef bytes) {
+      if (size == -1) {
+        size = bytes.length;
+      } else if (bytes.length != size) {
+        throw new IllegalArgumentException("expected bytes size=" + size
+            + " but got " + bytes.length);
+      }
+    }
+    
+    // Important that we get docCount, in case there were
+    // some last docs that we didn't see
+    @Override
+    public void finish(int docCount) throws IOException {
+      boolean success = false;
+      try {
+        finishInternal(docCount);
+        success = true;
+      } finally {
+        releaseResources();
+        if (success) {
+          IOUtils.close(getIndexOut(), getDataOut());
+        } else {
+          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
+        }
+        
+      }
+    }
+    
+    protected abstract void finishInternal(int docCount) throws IOException;
+    
+    protected void releaseResources() {
+      hash.close();
+      bytesUsed.addAndGet((-docToEntry.length) * RamUsageEstimator.NUM_BYTES_INT);
+      docToEntry = null;
+    }
+    
+    protected void writeIndex(IndexOutput idxOut, int docCount,
+        long maxValue, int[] toEntry) throws IOException {
+      writeIndex(idxOut, docCount, maxValue, (int[])null, toEntry);
+    }
+    
+    protected void writeIndex(IndexOutput idxOut, int docCount,
+        long maxValue, int[] addresses, int[] toEntry) throws IOException {
+      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,
+          PackedInts.bitsRequired(maxValue));
+      final int limit = docCount > docToEntry.length ? docToEntry.length
+          : docCount;
+      assert toEntry.length >= limit -1;
+      if (addresses != null) {
+        for (int i = 0; i < limit; i++) {
+          assert addresses[toEntry[i]] >= 0;
+          w.add(addresses[toEntry[i]]);
+        }
+      } else {
+        for (int i = 0; i < limit; i++) {
+          assert toEntry[i] >= 0;
+          w.add(toEntry[i]);
+        }
+      }
+      for (int i = limit; i < docCount; i++) {
+        w.add(0);
+      }
+      w.finish();
+    }
+    
+    protected void writeIndex(IndexOutput idxOut, int docCount,
+        long maxValue, long[] addresses, int[] toEntry) throws IOException {
+      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,
+          PackedInts.bitsRequired(maxValue));
+      final int limit = docCount > docToEntry.length ? docToEntry.length
+          : docCount;
+      assert toEntry.length >= limit -1;
+      if (addresses != null) {
+        for (int i = 0; i < limit; i++) {
+          assert addresses[toEntry[i]] >= 0;
+          w.add(addresses[toEntry[i]]);
+        }
+      } else {
+        for (int i = 0; i < limit; i++) {
+          assert toEntry[i] >= 0;
+          w.add(toEntry[i]);
+        }
+      }
+      for (int i = limit; i < docCount; i++) {
+        w.add(0);
+      }
+      w.finish();
+    }
+    
+  }
+  
+  static abstract class BytesSortedSourceBase extends SortedSource {
+    private final PagedBytes pagedBytes;
+    
+    protected final PackedInts.Reader docToOrdIndex;
+    protected final PackedInts.Reader ordToOffsetIndex;
+
+    protected final IndexInput datIn;
+    protected final IndexInput idxIn;
+    protected final BytesRef defaultValue = new BytesRef();
+    protected final static int PAGED_BYTES_BITS = 15;
+    protected final PagedBytes.Reader data;
+
+    protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
+        Comparator<BytesRef> comp, long bytesToRead, Type type, boolean hasOffsets) throws IOException {
+      this(datIn, idxIn, comp, new PagedBytes(PAGED_BYTES_BITS), bytesToRead, type, hasOffsets);
+    }
+    
+    protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
+        Comparator<BytesRef> comp, PagedBytes pagedBytes, long bytesToRead, Type type, boolean hasOffsets)
+        throws IOException {
+      super(type, comp);
+      assert bytesToRead <= datIn.length() : " file size is less than the expected size diff: "
+          + (bytesToRead - datIn.length()) + " pos: " + datIn.getFilePointer();
+      this.datIn = datIn;
+      this.pagedBytes = pagedBytes;
+      this.pagedBytes.copy(datIn, bytesToRead);
+      data = pagedBytes.freeze(true);
+      this.idxIn = idxIn;
+      ordToOffsetIndex = hasOffsets ? PackedInts.getReader(idxIn) : null; 
+      docToOrdIndex = PackedInts.getReader(idxIn);
+    }
+
+    @Override
+    public boolean hasPackedDocToOrd() {
+      return true;
+    }
+
+    @Override
+    public PackedInts.Reader getDocToOrd() {
+      return docToOrdIndex;
+    }
+    
+    @Override
+    public int ord(int docID) {
+      assert docToOrdIndex.get(docID) < getValueCount();
+      return (int) docToOrdIndex.get(docID);
+    }
+
+    protected void closeIndexInput() throws IOException {
+      IOUtils.close(datIn, idxIn);
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/BytesRefUtils.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/BytesRefUtils.java
new file mode 100644
index 0000000..885d2f8
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/BytesRefUtils.java
@@ -0,0 +1,120 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Package private BytesRefUtils - can move this into the o.a.l.utils package if
+ * needed.
+ * 
+ * @lucene.internal
+ */
+public final class BytesRefUtils {
+
+  private BytesRefUtils() {
+  }
+
+  /**
+   * Copies the given long value and encodes it as 8 byte Big-Endian.
+   * <p>
+   * NOTE: this method resets the offset to 0, length to 8 and resizes the
+   * reference array if needed.
+   */
+  public static void copyLong(BytesRef ref, long value) {
+    if (ref.bytes.length < 8) {
+      ref.bytes = new byte[8];
+    }
+    copyInternal(ref, (int) (value >> 32), ref.offset = 0);
+    copyInternal(ref, (int) value, 4);
+    ref.length = 8;
+  }
+
+  /**
+   * Copies the given int value and encodes it as 4 byte Big-Endian.
+   * <p>
+   * NOTE: this method resets the offset to 0, length to 4 and resizes the
+   * reference array if needed.
+   */
+  public static void copyInt(BytesRef ref, int value) {
+    if (ref.bytes.length < 4) {
+      ref.bytes = new byte[4];
+    }
+    copyInternal(ref, value, ref.offset = 0);
+    ref.length = 4;
+  }
+
+  /**
+   * Copies the given short value and encodes it as a 2 byte Big-Endian.
+   * <p>
+   * NOTE: this method resets the offset to 0, length to 2 and resizes the
+   * reference array if needed.
+   */
+  public static void copyShort(BytesRef ref, short value) {
+    if (ref.bytes.length < 2) {
+      ref.bytes = new byte[2];
+    }
+    ref.bytes[ref.offset] = (byte) (value >> 8);
+    ref.bytes[ref.offset + 1] = (byte) (value);
+    ref.length = 2;
+  }
+
+  private static void copyInternal(BytesRef ref, int value, int startOffset) {
+    ref.bytes[startOffset] = (byte) (value >> 24);
+    ref.bytes[startOffset + 1] = (byte) (value >> 16);
+    ref.bytes[startOffset + 2] = (byte) (value >> 8);
+    ref.bytes[startOffset + 3] = (byte) (value);
+  }
+
+  /**
+   * Converts 2 consecutive bytes from the current offset to a short. Bytes are
+   * interpreted as Big-Endian (most significant bit first)
+   * <p>
+   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
+   */
+  public static short asShort(BytesRef b) {
+    return (short) (0xFFFF & ((b.bytes[b.offset] & 0xFF) << 8) | (b.bytes[b.offset + 1] & 0xFF));
+  }
+
+  /**
+   * Converts 4 consecutive bytes from the current offset to an int. Bytes are
+   * interpreted as Big-Endian (most significant bit first)
+   * <p>
+   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
+   */
+  public static int asInt(BytesRef b) {
+    return asIntInternal(b, b.offset);
+  }
+
+  /**
+   * Converts 8 consecutive bytes from the current offset to a long. Bytes are
+   * interpreted as Big-Endian (most significant bit first)
+   * <p>
+   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
+   */
+  public static long asLong(BytesRef b) {
+    return (((long) asIntInternal(b, b.offset) << 32) | asIntInternal(b,
+        b.offset + 4) & 0xFFFFFFFFL);
+  }
+
+  private static int asIntInternal(BytesRef b, int pos) {
+    return ((b.bytes[pos++] & 0xFF) << 24) | ((b.bytes[pos++] & 0xFF) << 16)
+        | ((b.bytes[pos++] & 0xFF) << 8) | (b.bytes[pos] & 0xFF);
+  }
+
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/DirectSource.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/DirectSource.java
new file mode 100644
index 0000000..d26b72d
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/DirectSource.java
@@ -0,0 +1,139 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Base class for disk resident source implementations
+ * @lucene.internal
+ */
+public abstract class DirectSource extends Source {
+
+  protected final IndexInput data;
+  private final ToNumeric toNumeric;
+  protected final long baseOffset;
+
+  public DirectSource(IndexInput input, Type type) {
+    super(type);
+    this.data = input;
+    baseOffset = input.getFilePointer();
+    switch (type) {
+    case FIXED_INTS_16:
+      toNumeric = new ShortToLong();
+      break;
+    case FLOAT_32:
+    case FIXED_INTS_32:
+      toNumeric = new IntToLong();
+      break;
+    case FIXED_INTS_8:
+      toNumeric = new ByteToLong();
+      break;
+    default:
+      toNumeric = new LongToLong();
+    }
+  }
+
+  @Override
+  public BytesRef getBytes(int docID, BytesRef ref) {
+    try {
+      final int sizeToRead = position(docID);
+      ref.grow(sizeToRead);
+      data.readBytes(ref.bytes, 0, sizeToRead);
+      ref.length = sizeToRead;
+      ref.offset = 0;
+      return ref;
+    } catch (IOException ex) {
+      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
+    }
+  }
+
+  @Override
+  public long getInt(int docID) {
+    try {
+      position(docID);
+      return toNumeric.toLong(data);
+    } catch (IOException ex) {
+      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
+    }
+  }
+
+  @Override
+  public double getFloat(int docID) {
+    try {
+      position(docID);
+      return toNumeric.toDouble(data);
+    } catch (IOException ex) {
+      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
+    }
+  }
+
+  protected abstract int position(int docID) throws IOException;
+
+  private abstract static class ToNumeric {
+    abstract long toLong(IndexInput input) throws IOException;
+
+    double toDouble(IndexInput input) throws IOException {
+      return toLong(input);
+    }
+  }
+
+  private static final class ByteToLong extends ToNumeric {
+    @Override
+    long toLong(IndexInput input) throws IOException {
+      return input.readByte();
+    }
+
+  }
+
+  private static final class ShortToLong extends ToNumeric {
+    @Override
+    long toLong(IndexInput input) throws IOException {
+      return input.readShort();
+    }
+  }
+
+  private static final class IntToLong extends ToNumeric {
+    @Override
+    long toLong(IndexInput input) throws IOException {
+      return input.readInt();
+    }
+
+    double toDouble(IndexInput input) throws IOException {
+      return Float.intBitsToFloat(input.readInt());
+    }
+  }
+
+  private static final class LongToLong extends ToNumeric {
+    @Override
+    long toLong(IndexInput input) throws IOException {
+      return input.readLong();
+    }
+
+    double toDouble(IndexInput input) throws IOException {
+      return Double.longBitsToDouble(input.readLong());
+    }
+  }
+
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/DocValuesArray.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/DocValuesArray.java
new file mode 100644
index 0000000..668f094
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/DocValuesArray.java
@@ -0,0 +1,306 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.EnumMap;
+import java.util.Map;
+
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+/**
+ * @lucene.experimental
+ */
+abstract class DocValuesArray extends Source {
+
+  static final Map<Type, DocValuesArray> TEMPLATES;
+
+  static {
+    EnumMap<Type, DocValuesArray> templates = new EnumMap<Type, DocValuesArray>(
+        Type.class);
+    templates.put(Type.FIXED_INTS_16, new ShortValues());
+    templates.put(Type.FIXED_INTS_32, new IntValues());
+    templates.put(Type.FIXED_INTS_64, new LongValues());
+    templates.put(Type.FIXED_INTS_8, new ByteValues());
+    templates.put(Type.FLOAT_32, new FloatValues());
+    templates.put(Type.FLOAT_64, new DoubleValues());
+    TEMPLATES = Collections.unmodifiableMap(templates);
+  }
+
+  protected final int bytesPerValue;
+
+  DocValuesArray(int bytesPerValue, Type type) {
+    super(type);
+    this.bytesPerValue = bytesPerValue;
+  }
+
+  public abstract DocValuesArray newFromInput(IndexInput input, int numDocs)
+      throws IOException;
+
+  @Override
+  public final boolean hasArray() {
+    return true;
+  }
+
+  void toBytes(long value, BytesRef bytesRef) {
+    BytesRefUtils.copyLong(bytesRef, value);
+  }
+
+  void toBytes(double value, BytesRef bytesRef) {
+    BytesRefUtils.copyLong(bytesRef, Double.doubleToRawLongBits(value));
+  }
+
+  final static class ByteValues extends DocValuesArray {
+    private final byte[] values;
+
+    ByteValues() {
+      super(1, Type.FIXED_INTS_8);
+      values = new byte[0];
+    }
+
+    private ByteValues(IndexInput input, int numDocs) throws IOException {
+      super(1, Type.FIXED_INTS_8);
+      values = new byte[numDocs];
+      input.readBytes(values, 0, values.length, false);
+    }
+
+    @Override
+    public byte[] getArray() {
+      return values;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return values[docID];
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new ByteValues(input, numDocs);
+    }
+
+    void toBytes(long value, BytesRef bytesRef) {
+      bytesRef.bytes[0] = (byte) (0xFFL & value);
+    }
+
+  };
+
+  final static class ShortValues extends DocValuesArray {
+    private final short[] values;
+
+    ShortValues() {
+      super(RamUsageEstimator.NUM_BYTES_SHORT, Type.FIXED_INTS_16);
+      values = new short[0];
+    }
+
+    private ShortValues(IndexInput input, int numDocs) throws IOException {
+      super(RamUsageEstimator.NUM_BYTES_SHORT, Type.FIXED_INTS_16);
+      values = new short[numDocs];
+      for (int i = 0; i < values.length; i++) {
+        values[i] = input.readShort();
+      }
+    }
+
+    @Override
+    public short[] getArray() {
+      return values;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return values[docID];
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new ShortValues(input, numDocs);
+    }
+
+    void toBytes(long value, BytesRef bytesRef) {
+      BytesRefUtils.copyShort(bytesRef, (short) (0xFFFFL & value));
+    }
+
+  };
+
+  final static class IntValues extends DocValuesArray {
+    private final int[] values;
+
+    IntValues() {
+      super(RamUsageEstimator.NUM_BYTES_INT, Type.FIXED_INTS_32);
+      values = new int[0];
+    }
+
+    private IntValues(IndexInput input, int numDocs) throws IOException {
+      super(RamUsageEstimator.NUM_BYTES_INT, Type.FIXED_INTS_32);
+      values = new int[numDocs];
+      for (int i = 0; i < values.length; i++) {
+        values[i] = input.readInt();
+      }
+    }
+
+    @Override
+    public int[] getArray() {
+      return values;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return 0xFFFFFFFF & values[docID];
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new IntValues(input, numDocs);
+    }
+
+    void toBytes(long value, BytesRef bytesRef) {
+      BytesRefUtils.copyInt(bytesRef, (int) (0xFFFFFFFF & value));
+    }
+
+  };
+
+  final static class LongValues extends DocValuesArray {
+    private final long[] values;
+
+    LongValues() {
+      super(RamUsageEstimator.NUM_BYTES_LONG, Type.FIXED_INTS_64);
+      values = new long[0];
+    }
+
+    private LongValues(IndexInput input, int numDocs) throws IOException {
+      super(RamUsageEstimator.NUM_BYTES_LONG, Type.FIXED_INTS_64);
+      values = new long[numDocs];
+      for (int i = 0; i < values.length; i++) {
+        values[i] = input.readLong();
+      }
+    }
+
+    @Override
+    public long[] getArray() {
+      return values;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return values[docID];
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new LongValues(input, numDocs);
+    }
+
+  };
+
+  final static class FloatValues extends DocValuesArray {
+    private final float[] values;
+
+    FloatValues() {
+      super(RamUsageEstimator.NUM_BYTES_FLOAT, Type.FLOAT_32);
+      values = new float[0];
+    }
+
+    private FloatValues(IndexInput input, int numDocs) throws IOException {
+      super(RamUsageEstimator.NUM_BYTES_FLOAT, Type.FLOAT_32);
+      values = new float[numDocs];
+      /*
+       * we always read BIG_ENDIAN here since the writer serialized plain bytes
+       * we can simply read the ints / longs back in using readInt / readLong
+       */
+      for (int i = 0; i < values.length; i++) {
+        values[i] = Float.intBitsToFloat(input.readInt());
+      }
+    }
+
+    @Override
+    public float[] getArray() {
+      return values;
+    }
+
+    @Override
+    public double getFloat(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return values[docID];
+    }
+    
+    @Override
+    void toBytes(double value, BytesRef bytesRef) {
+      BytesRefUtils.copyInt(bytesRef, Float.floatToRawIntBits((float)value));
+
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new FloatValues(input, numDocs);
+    }
+  };
+
+  final static class DoubleValues extends DocValuesArray {
+    private final double[] values;
+
+    DoubleValues() {
+      super(RamUsageEstimator.NUM_BYTES_DOUBLE, Type.FLOAT_64);
+      values = new double[0];
+    }
+
+    private DoubleValues(IndexInput input, int numDocs) throws IOException {
+      super(RamUsageEstimator.NUM_BYTES_DOUBLE, Type.FLOAT_64);
+      values = new double[numDocs];
+      /*
+       * we always read BIG_ENDIAN here since the writer serialized plain bytes
+       * we can simply read the ints / longs back in using readInt / readLong
+       */
+      for (int i = 0; i < values.length; i++) {
+        values[i] = Double.longBitsToDouble(input.readLong());
+      }
+    }
+
+    @Override
+    public double[] getArray() {
+      return values;
+    }
+
+    @Override
+    public double getFloat(int docID) {
+      assert docID >= 0 && docID < values.length;
+      return values[docID];
+    }
+
+    @Override
+    public DocValuesArray newFromInput(IndexInput input, int numDocs)
+        throws IOException {
+      return new DoubleValues(input, numDocs);
+    }
+
+  };
+
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedDerefBytesImpl.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedDerefBytesImpl.java
new file mode 100644
index 0000000..72efc15
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedDerefBytesImpl.java
@@ -0,0 +1,134 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSourceBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.packed.PackedInts;
+
+// Stores fixed-length byte[] by deref, ie when two docs
+// have the same value, they store only 1 byte[]
+/**
+ * @lucene.experimental
+ */
+class FixedDerefBytesImpl {
+
+  static final String CODEC_NAME = "FixedDerefBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  public static class Writer extends DerefBytesWriterBase {
+    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+    }
+
+    @Override
+    protected void finishInternal(int docCount) throws IOException {
+      final int numValues = hash.size();
+      final IndexOutput datOut = getOrCreateDataOut();
+      datOut.writeInt(size);
+      if (size != -1) {
+        final BytesRef bytesRef = new BytesRef(size);
+        for (int i = 0; i < numValues; i++) {
+          hash.get(i, bytesRef);
+          datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+        }
+      }
+      final IndexOutput idxOut = getOrCreateIndexOut();
+      idxOut.writeInt(numValues);
+      writeIndex(idxOut, docCount, numValues, docToEntry);
+    }
+  }
+
+  public static class FixedDerefReader extends BytesReaderBase {
+    private final int size;
+    private final int numValuesStored;
+    FixedDerefReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_START, true, context, Type.BYTES_FIXED_DEREF);
+      size = datIn.readInt();
+      numValuesStored = idxIn.readInt();
+    }
+
+    @Override
+    public Source load() throws IOException {
+      return new FixedDerefSource(cloneData(), cloneIndex(), size, numValuesStored);
+    }
+
+    @Override
+    public Source getDirectSource()
+        throws IOException {
+      return new DirectFixedDerefSource(cloneData(), cloneIndex(), size, type());
+    }
+
+    @Override
+    public int getValueSize() {
+      return size;
+    }
+    
+  }
+  
+  static final class FixedDerefSource extends BytesSourceBase {
+    private final int size;
+    private final PackedInts.Reader addresses;
+
+    protected FixedDerefSource(IndexInput datIn, IndexInput idxIn, int size, long numValues) throws IOException {
+      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), size * numValues,
+          Type.BYTES_FIXED_DEREF);
+      this.size = size;
+      addresses = PackedInts.getReader(idxIn);
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      final int id = (int) addresses.get(docID);
+      return data.fillSlice(bytesRef, (id * size), size);
+    }
+
+  }
+  
+  final static class DirectFixedDerefSource extends DirectSource {
+    private final PackedInts.Reader index;
+    private final int size;
+
+    DirectFixedDerefSource(IndexInput data, IndexInput index, int size, Type type)
+        throws IOException {
+      super(data, type);
+      this.size = size;
+      this.index = PackedInts.getDirectReader(index);
+    }
+
+    @Override
+    protected int position(int docID) throws IOException {
+      data.seek(baseOffset + index.get(docID) * size);
+      return size;
+    }
+  }
+
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedSortedBytesImpl.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedSortedBytesImpl.java
new file mode 100644
index 0000000..aa3c7df
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedSortedBytesImpl.java
@@ -0,0 +1,230 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSortedSourceBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.SortedBytesMergeUtils;
+import org.apache.lucene.index.DocValues.SortedSource;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.SortedBytesMergeUtils.MergeContext;
+import org.apache.lucene.index.SortedBytesMergeUtils.SortedSourceSlice;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+// Stores fixed-length byte[] by deref, ie when two docs
+// have the same value, they store only 1 byte[]
+
+/**
+ * @lucene.experimental
+ */
+class FixedSortedBytesImpl {
+
+  static final String CODEC_NAME = "FixedSortedBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  static final class Writer extends DerefBytesWriterBase {
+    private final Comparator<BytesRef> comp;
+
+    public Writer(Directory dir, String id, Comparator<BytesRef> comp,
+        Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      this.comp = comp;
+    }
+
+    @Override
+    public void merge(MergeState mergeState, DocValues[] docValues)
+        throws IOException {
+      boolean success = false;
+      try {
+        final MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_FIXED_SORTED, docValues, comp, mergeState);
+        List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState, docValues, ctx);
+        final IndexOutput datOut = getOrCreateDataOut();
+        datOut.writeInt(ctx.sizePerValues);
+        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, datOut, slices);
+        
+        final IndexOutput idxOut = getOrCreateIndexOut();
+        idxOut.writeInt(maxOrd);
+        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
+            PackedInts.bitsRequired(maxOrd));
+        for (SortedSourceSlice slice : slices) {
+          slice.writeOrds(ordsWriter);
+        }
+        ordsWriter.finish();
+        success = true;
+      } finally {
+        releaseResources();
+        if (success) {
+          IOUtils.close(getIndexOut(), getDataOut());
+        } else {
+          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
+        }
+
+      }
+    }
+
+    // Important that we get docCount, in case there were
+    // some last docs that we didn't see
+    @Override
+    public void finishInternal(int docCount) throws IOException {
+      fillDefault(docCount);
+      final IndexOutput datOut = getOrCreateDataOut();
+      final int count = hash.size();
+      final int[] address = new int[count];
+      datOut.writeInt(size);
+      if (size != -1) {
+        final int[] sortedEntries = hash.sort(comp);
+        // first dump bytes data, recording address as we go
+        final BytesRef spare = new BytesRef(size);
+        for (int i = 0; i < count; i++) {
+          final int e = sortedEntries[i];
+          final BytesRef bytes = hash.get(e, spare);
+          assert bytes.length == size;
+          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
+          address[e] = i;
+        }
+      }
+      final IndexOutput idxOut = getOrCreateIndexOut();
+      idxOut.writeInt(count);
+      writeIndex(idxOut, docCount, count, address, docToEntry);
+    }
+  }
+
+  static final class Reader extends BytesReaderBase {
+    private final int size;
+    private final int valueCount;
+    private final Comparator<BytesRef> comparator;
+
+    public Reader(Directory dir, String id, int maxDoc, IOContext context,
+        Type type, Comparator<BytesRef> comparator) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_START, true, context, type);
+      size = datIn.readInt();
+      valueCount = idxIn.readInt();
+      this.comparator = comparator;
+    }
+
+    @Override
+    public Source load() throws IOException {
+      return new FixedSortedSource(cloneData(), cloneIndex(), size, valueCount,
+          comparator);
+    }
+
+    @Override
+    public Source getDirectSource() throws IOException {
+      return new DirectFixedSortedSource(cloneData(), cloneIndex(), size,
+          valueCount, comparator, type);
+    }
+
+    @Override
+    public int getValueSize() {
+      return size;
+    }
+  }
+
+  static final class FixedSortedSource extends BytesSortedSourceBase {
+    private final int valueCount;
+    private final int size;
+
+    FixedSortedSource(IndexInput datIn, IndexInput idxIn, int size,
+        int numValues, Comparator<BytesRef> comp) throws IOException {
+      super(datIn, idxIn, comp, size * numValues, Type.BYTES_FIXED_SORTED,
+          false);
+      this.size = size;
+      this.valueCount = numValues;
+      closeIndexInput();
+    }
+
+    @Override
+    public int getValueCount() {
+      return valueCount;
+    }
+
+    @Override
+    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+      return data.fillSlice(bytesRef, (ord * size), size);
+    }
+  }
+
+  static final class DirectFixedSortedSource extends SortedSource {
+    final PackedInts.Reader docToOrdIndex;
+    private final IndexInput datIn;
+    private final long basePointer;
+    private final int size;
+    private final int valueCount;
+
+    DirectFixedSortedSource(IndexInput datIn, IndexInput idxIn, int size,
+        int valueCount, Comparator<BytesRef> comp, Type type)
+        throws IOException {
+      super(type, comp);
+      docToOrdIndex = PackedInts.getDirectReader(idxIn);
+      basePointer = datIn.getFilePointer();
+      this.datIn = datIn;
+      this.size = size;
+      this.valueCount = valueCount;
+    }
+
+    @Override
+    public int ord(int docID) {
+      return (int) docToOrdIndex.get(docID);
+    }
+
+    @Override
+    public boolean hasPackedDocToOrd() {
+      return true;
+    }
+
+    @Override
+    public PackedInts.Reader getDocToOrd() {
+      return docToOrdIndex;
+    }
+
+    @Override
+    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+      try {
+        datIn.seek(basePointer + size * ord);
+        bytesRef.grow(size);
+        datIn.readBytes(bytesRef.bytes, 0, size);
+        bytesRef.length = size;
+        bytesRef.offset = 0;
+        return bytesRef;
+      } catch (IOException ex) {
+        throw new IllegalStateException("failed to getByOrd", ex);
+      }
+    }
+
+    @Override
+    public int getValueCount() {
+      return valueCount;
+    }
+  }
+
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedStraightBytesImpl.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedStraightBytesImpl.java
new file mode 100644
index 0000000..63d8f77
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/FixedStraightBytesImpl.java
@@ -0,0 +1,355 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSourceBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesWriterBase;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.PagedBytes;
+
+// Simplest storage: stores fixed length byte[] per
+// document, with no dedup and no sorting.
+/**
+ * @lucene.experimental
+ */
+class FixedStraightBytesImpl {
+
+  static final String CODEC_NAME = "FixedStraightBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+  
+  static abstract class FixedBytesWriterBase extends BytesWriterBase {
+    protected int lastDocID = -1;
+    // start at -1 if the first added value is > 0
+    protected int size = -1;
+    private final int byteBlockSize = BYTE_BLOCK_SIZE;
+    private final ByteBlockPool pool;
+
+    protected FixedBytesWriterBase(Directory dir, String id, String codecName,
+        int version, Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, codecName, version, bytesUsed, context);
+      pool = new ByteBlockPool(new DirectTrackingAllocator(bytesUsed));
+      pool.nextBuffer();
+    }
+    
+    @Override
+    public void add(int docID, BytesRef bytes) throws IOException {
+      assert lastDocID < docID;
+
+      if (size == -1) {
+        if (bytes.length > BYTE_BLOCK_SIZE) {
+          throw new IllegalArgumentException("bytes arrays > " + Short.MAX_VALUE + " are not supported");
+        }
+        size = bytes.length;
+      } else if (bytes.length != size) {
+        throw new IllegalArgumentException("expected bytes size=" + size
+            + " but got " + bytes.length);
+      }
+      if (lastDocID+1 < docID) {
+        advancePool(docID);
+      }
+      pool.copy(bytes);
+      lastDocID = docID;
+    }
+    
+    private final void advancePool(int docID) {
+      long numBytes = (docID - (lastDocID+1))*size;
+      while(numBytes > 0) {
+        if (numBytes + pool.byteUpto < byteBlockSize) {
+          pool.byteUpto += numBytes;
+          numBytes = 0;
+        } else {
+          numBytes -= byteBlockSize - pool.byteUpto;
+          pool.nextBuffer();
+        }
+      }
+      assert numBytes == 0;
+    }
+    
+    protected void set(BytesRef ref, int docId) {
+      assert BYTE_BLOCK_SIZE % size == 0 : "BYTE_BLOCK_SIZE ("+ BYTE_BLOCK_SIZE + ") must be a multiple of the size: " + size;
+      ref.offset = docId*size;
+      ref.length = size;
+      pool.deref(ref);
+    }
+    
+    protected void resetPool() {
+      pool.dropBuffersAndReset();
+    }
+    
+    protected void writeData(IndexOutput out) throws IOException {
+      pool.writePool(out);
+    }
+    
+    protected void writeZeros(int num, IndexOutput out) throws IOException {
+      final byte[] zeros = new byte[size];
+      for (int i = 0; i < num; i++) {
+        out.writeBytes(zeros, zeros.length);
+      }
+    }
+  }
+
+  static class Writer extends FixedBytesWriterBase {
+    private boolean hasMerged;
+    private IndexOutput datOut;
+    
+    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+    }
+
+    public Writer(Directory dir, String id, String codecName, int version, Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, codecName, version, bytesUsed, context);
+    }
+
+
+    @Override
+    protected void merge(SingleSubMergeState state) throws IOException {
+      datOut = getOrCreateDataOut();
+      boolean success = false;
+      try {
+        if (!hasMerged && size != -1) {
+          datOut.writeInt(size);
+        }
+
+        if (state.liveDocs == null && tryBulkMerge(state.reader)) {
+          FixedStraightReader reader = (FixedStraightReader) state.reader;
+          final int maxDocs = reader.maxDoc;
+          if (maxDocs == 0) {
+            return;
+          }
+          if (size == -1) {
+            size = reader.size;
+            datOut.writeInt(size);
+          } else if (size != reader.size) {
+            throw new IllegalArgumentException("expected bytes size=" + size
+                + " but got " + reader.size);
+           }
+          if (lastDocID+1 < state.docBase) {
+            fill(datOut, state.docBase);
+            lastDocID = state.docBase-1;
+          }
+          // TODO should we add a transfer to API to each reader?
+          final IndexInput cloneData = reader.cloneData();
+          try {
+            datOut.copyBytes(cloneData, size * maxDocs);
+          } finally {
+            IOUtils.close(cloneData);  
+          }
+        
+          lastDocID += maxDocs;
+        } else {
+          super.merge(state);
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(datOut);
+        }
+        hasMerged = true;
+      }
+    }
+    
+    protected boolean tryBulkMerge(DocValues docValues) {
+      return docValues instanceof FixedStraightReader;
+    }
+    
+    @Override
+    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
+      assert lastDocID < docID;
+      setMergeBytes(sourceDoc);
+      if (size == -1) {
+        size = bytesRef.length;
+        datOut.writeInt(size);
+      }
+      assert size == bytesRef.length : "size: " + size + " ref: " + bytesRef.length;
+      if (lastDocID+1 < docID) {
+        fill(datOut, docID);
+      }
+      datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+      lastDocID = docID;
+    }
+    
+    protected void setMergeBytes(int sourceDoc) {
+      currentMergeSource.getBytes(sourceDoc, bytesRef);
+    }
+
+
+
+    // Fills up to but not including this docID
+    private void fill(IndexOutput datOut, int docID) throws IOException {
+      assert size >= 0;
+      writeZeros((docID - (lastDocID+1)), datOut);
+    }
+
+    @Override
+    public void finish(int docCount) throws IOException {
+      boolean success = false;
+      try {
+        if (!hasMerged) {
+          // indexing path - no disk IO until here
+          assert datOut == null;
+          datOut = getOrCreateDataOut();
+          if (size == -1) {
+            datOut.writeInt(0);
+          } else {
+            datOut.writeInt(size);
+            writeData(datOut);
+          }
+          if (lastDocID + 1 < docCount) {
+            fill(datOut, docCount);
+          }
+        } else {
+          // merge path - datOut should be initialized
+          assert datOut != null;
+          if (size == -1) {// no data added
+            datOut.writeInt(0);
+          } else {
+            fill(datOut, docCount);
+          }
+        }
+        success = true;
+      } finally {
+        resetPool();
+        if (success) {
+          IOUtils.close(datOut);
+        } else {
+          IOUtils.closeWhileHandlingException(datOut);
+        }
+      }
+    }
+  
+  }
+  
+  public static class FixedStraightReader extends BytesReaderBase {
+    protected final int size;
+    protected final int maxDoc;
+    
+    FixedStraightReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
+      this(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc, context, Type.BYTES_FIXED_STRAIGHT);
+    }
+
+    protected FixedStraightReader(Directory dir, String id, String codec, int version, int maxDoc, IOContext context, Type type) throws IOException {
+      super(dir, id, codec, version, false, context, type);
+      size = datIn.readInt();
+      this.maxDoc = maxDoc;
+    }
+
+    @Override
+    public Source load() throws IOException {
+      return size == 1 ? new SingleByteSource(cloneData(), maxDoc) : 
+        new FixedStraightSource(cloneData(), size, maxDoc, type);
+    }
+
+    @Override
+    public void close() throws IOException {
+      datIn.close();
+    }
+   
+    @Override
+    public Source getDirectSource() throws IOException {
+      return new DirectFixedStraightSource(cloneData(), size, type());
+    }
+    
+    @Override
+    public int getValueSize() {
+      return size;
+    }
+  }
+  
+  // specialized version for single bytes
+  private static final class SingleByteSource extends Source {
+    private final byte[] data;
+
+    public SingleByteSource(IndexInput datIn, int maxDoc) throws IOException {
+      super(Type.BYTES_FIXED_STRAIGHT);
+      try {
+        data = new byte[maxDoc];
+        datIn.readBytes(data, 0, data.length, false);
+      } finally {
+        IOUtils.close(datIn);
+      }
+    }
+    
+    @Override
+    public boolean hasArray() {
+      return true;
+    }
+
+    @Override
+    public Object getArray() {
+      return data;
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      bytesRef.length = 1;
+      bytesRef.bytes = data;
+      bytesRef.offset = docID;
+      return bytesRef;
+    }
+  }
+
+  
+  private final static class FixedStraightSource extends BytesSourceBase {
+    private final int size;
+
+    public FixedStraightSource(IndexInput datIn, int size, int maxDoc, Type type)
+        throws IOException {
+      super(datIn, null, new PagedBytes(PAGED_BYTES_BITS), size * maxDoc,
+          type);
+      this.size = size;
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      return data.fillSlice(bytesRef, docID * size, size);
+    }
+  }
+  
+  public final static class DirectFixedStraightSource extends DirectSource {
+    private final int size;
+
+    DirectFixedStraightSource(IndexInput input, int size, Type type) {
+      super(input, type);
+      this.size = size;
+    }
+
+    @Override
+    protected int position(int docID) throws IOException {
+      data.seek(baseOffset + size * docID);
+      return size;
+    }
+
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java
new file mode 100644
index 0000000..644b28b
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Floats.java
@@ -0,0 +1,126 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValue;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Exposes {@link Writer} and reader ({@link Source}) for 32 bit and 64 bit
+ * floating point values.
+ * <p>
+ * Current implementations store either 4 byte or 8 byte floating points with
+ * full precision without any compression.
+ * 
+ * @lucene.experimental
+ */
+public class Floats {
+  
+  protected static final String CODEC_NAME = "Floats";
+  protected static final int VERSION_START = 0;
+  protected static final int VERSION_CURRENT = VERSION_START;
+  
+  public static Writer getWriter(Directory dir, String id, Counter bytesUsed,
+      IOContext context, Type type) throws IOException {
+    return new FloatsWriter(dir, id, bytesUsed, context, type);
+  }
+
+  public static DocValues getValues(Directory dir, String id, int maxDoc, IOContext context, Type type)
+      throws IOException {
+    return new FloatsReader(dir, id, maxDoc, context, type);
+  }
+  
+  private static int typeToSize(Type type) {
+    switch (type) {
+    case FLOAT_32:
+      return 4;
+    case FLOAT_64:
+      return 8;
+    default:
+      throw new IllegalStateException("illegal type " + type);
+    }
+  }
+  
+  final static class FloatsWriter extends FixedStraightBytesImpl.Writer {
+   
+    private final int size; 
+    private final DocValuesArray template;
+    public FloatsWriter(Directory dir, String id, Counter bytesUsed,
+        IOContext context, Type type) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      size = typeToSize(type);
+      this.bytesRef = new BytesRef(size);
+      bytesRef.length = size;
+      template = DocValuesArray.TEMPLATES.get(type);
+      assert template != null;
+    }
+    
+    public void add(int docID, double v) throws IOException {
+      template.toBytes(v, bytesRef);
+      add(docID, bytesRef);
+    }
+    
+    @Override
+    public void add(int docID, DocValue docValue) throws IOException {
+      add(docID, docValue.getFloat());
+    }
+    
+    @Override
+    protected boolean tryBulkMerge(DocValues docValues) {
+      // only bulk merge if value type is the same otherwise size differs
+      return super.tryBulkMerge(docValues) && docValues.type() == template.type();
+    }
+    
+    @Override
+    protected void setMergeBytes(int sourceDoc) {
+      final double value = currentMergeSource.getFloat(sourceDoc);
+      template.toBytes(value, bytesRef);
+    }
+  }
+  
+  final static class FloatsReader extends FixedStraightBytesImpl.FixedStraightReader {
+    final DocValuesArray arrayTemplate;
+    FloatsReader(Directory dir, String id, int maxDoc, IOContext context, Type type)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc, context, type);
+      arrayTemplate = DocValuesArray.TEMPLATES.get(type);
+      assert size == 4 || size == 8;
+    }
+    
+    @Override
+    public Source load() throws IOException {
+      final IndexInput indexInput = cloneData();
+      try {
+        return arrayTemplate.newFromInput(indexInput, maxDoc);
+      } finally {
+        IOUtils.close(indexInput);
+      }
+    }
+    
+  }
+
+}
\ No newline at end of file
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Ints.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Ints.java
new file mode 100644
index 0000000..131e5f0
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Ints.java
@@ -0,0 +1,151 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.DocValue;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Stores ints packed and fixed with fixed-bit precision.
+ * 
+ * @lucene.experimental
+ */
+public final class Ints {
+  protected static final String CODEC_NAME = "Ints";
+  protected static final int VERSION_START = 0;
+  protected static final int VERSION_CURRENT = VERSION_START;
+
+  private Ints() {
+  }
+  
+  public static Writer getWriter(Directory dir, String id, Counter bytesUsed,
+      Type type, IOContext context) throws IOException {
+    return type == Type.VAR_INTS ? new PackedIntValues.PackedIntsWriter(dir, id,
+        bytesUsed, context) : new IntsWriter(dir, id, bytesUsed, context, type);
+  }
+
+  public static DocValues getValues(Directory dir, String id, int numDocs,
+      Type type, IOContext context) throws IOException {
+    return type == Type.VAR_INTS ? new PackedIntValues.PackedIntsReader(dir, id,
+        numDocs, context) : new IntsReader(dir, id, numDocs, context, type);
+  }
+  
+  private static Type sizeToType(int size) {
+    switch (size) {
+    case 1:
+      return Type.FIXED_INTS_8;
+    case 2:
+      return Type.FIXED_INTS_16;
+    case 4:
+      return Type.FIXED_INTS_32;
+    case 8:
+      return Type.FIXED_INTS_64;
+    default:
+      throw new IllegalStateException("illegal size " + size);
+    }
+  }
+  
+  private static int typeToSize(Type type) {
+    switch (type) {
+    case FIXED_INTS_16:
+      return 2;
+    case FIXED_INTS_32:
+      return 4;
+    case FIXED_INTS_64:
+      return 8;
+    case FIXED_INTS_8:
+      return 1;
+    default:
+      throw new IllegalStateException("illegal type " + type);
+    }
+  }
+
+
+  static class IntsWriter extends FixedStraightBytesImpl.Writer {
+    private final DocValuesArray template;
+
+    public IntsWriter(Directory dir, String id, Counter bytesUsed,
+        IOContext context, Type valueType) throws IOException {
+      this(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, valueType);
+    }
+
+    protected IntsWriter(Directory dir, String id, String codecName,
+        int version, Counter bytesUsed, IOContext context, Type valueType) throws IOException {
+      super(dir, id, codecName, version, bytesUsed, context);
+      size = typeToSize(valueType);
+      this.bytesRef = new BytesRef(size);
+      bytesRef.length = size;
+      template = DocValuesArray.TEMPLATES.get(valueType);
+    }
+    
+    @Override
+    public void add(int docID, long v) throws IOException {
+      template.toBytes(v, bytesRef);
+      add(docID, bytesRef);
+    }
+
+    @Override
+    public void add(int docID, DocValue docValue) throws IOException {
+      add(docID, docValue.getInt());
+    }
+    
+    @Override
+    protected void setMergeBytes(int sourceDoc) {
+      final long value = currentMergeSource.getInt(sourceDoc);
+      template.toBytes(value, bytesRef);
+    }
+    
+    @Override
+    protected boolean tryBulkMerge(DocValues docValues) {
+      // only bulk merge if value type is the same otherwise size differs
+      return super.tryBulkMerge(docValues) && docValues.type() == template.type();
+    }
+  }
+  
+  final static class IntsReader extends FixedStraightBytesImpl.FixedStraightReader {
+    private final DocValuesArray arrayTemplate;
+
+    IntsReader(Directory dir, String id, int maxDoc, IOContext context, Type type)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc,
+          context, type);
+      arrayTemplate = DocValuesArray.TEMPLATES.get(type);
+      assert arrayTemplate != null;
+      assert type == sizeToType(size);
+    }
+
+    @Override
+    public Source load() throws IOException {
+      final IndexInput indexInput = cloneData();
+      try {
+        return arrayTemplate.newFromInput(indexInput, maxDoc);
+      } finally {
+        IOUtils.close(indexInput);
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/PackedIntValues.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/PackedIntValues.java
new file mode 100644
index 0000000..7c8c87c
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/PackedIntValues.java
@@ -0,0 +1,265 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.codecs.lucene40.values.DocValuesArray.LongValues;
+import org.apache.lucene.codecs.lucene40.values.FixedStraightBytesImpl.FixedBytesWriterBase;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.DocValue;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Stores integers using {@link PackedInts}
+ * 
+ * @lucene.experimental
+ * */
+class PackedIntValues {
+
+  private static final String CODEC_NAME = "PackedInts";
+  private static final byte PACKED = 0x00;
+  private static final byte FIXED_64 = 0x01;
+
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  static class PackedIntsWriter extends FixedBytesWriterBase {
+
+    private long minValue;
+    private long maxValue;
+    private boolean started;
+    private int lastDocId = -1;
+
+    protected PackedIntsWriter(Directory dir, String id, Counter bytesUsed,
+        IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      bytesRef = new BytesRef(8);
+    }
+
+    @Override
+    public void add(int docID, long v) throws IOException {
+      assert lastDocId < docID;
+      if (!started) {
+        started = true;
+        minValue = maxValue = v;
+      } else {
+        if (v < minValue) {
+          minValue = v;
+        } else if (v > maxValue) {
+          maxValue = v;
+        }
+      }
+      lastDocId = docID;
+      BytesRefUtils.copyLong(bytesRef, v);
+      add(docID, bytesRef);
+    }
+
+    @Override
+    public void finish(int docCount) throws IOException {
+      boolean success = false;
+      final IndexOutput dataOut = getOrCreateDataOut();
+      try {
+        if (!started) {
+          minValue = maxValue = 0;
+        }
+        final long delta = maxValue - minValue;
+        // if we exceed the range of positive longs we must switch to fixed
+        // ints
+        if (delta <= (maxValue >= 0 && minValue <= 0 ? Long.MAX_VALUE
+            : Long.MAX_VALUE - 1) && delta >= 0) {
+          dataOut.writeByte(PACKED);
+          writePackedInts(dataOut, docCount);
+          return; // done
+        } else {
+          dataOut.writeByte(FIXED_64);
+        }
+        writeData(dataOut);
+        writeZeros(docCount - (lastDocID + 1), dataOut);
+        success = true;
+      } finally {
+        resetPool();
+        if (success) {
+          IOUtils.close(dataOut);
+        } else {
+          IOUtils.closeWhileHandlingException(dataOut);
+        }
+      }
+    }
+
+    @Override
+    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
+      assert docID > lastDocId : "docID: " + docID
+          + " must be greater than the last added doc id: " + lastDocId;
+        add(docID, currentMergeSource.getInt(sourceDoc));
+    }
+
+    private void writePackedInts(IndexOutput datOut, int docCount) throws IOException {
+      datOut.writeLong(minValue);
+      
+      // write a default value to recognize docs without a value for that
+      // field
+      final long defaultValue = maxValue >= 0 && minValue <= 0 ? 0 - minValue
+          : ++maxValue - minValue;
+      datOut.writeLong(defaultValue);
+      PackedInts.Writer w = PackedInts.getWriter(datOut, docCount,
+          PackedInts.bitsRequired(maxValue - minValue));
+      for (int i = 0; i < lastDocID + 1; i++) {
+        set(bytesRef, i);
+        byte[] bytes = bytesRef.bytes;
+        int offset = bytesRef.offset;
+        long asLong =  
+           (((long)(bytes[offset+0] & 0xff) << 56) |
+            ((long)(bytes[offset+1] & 0xff) << 48) |
+            ((long)(bytes[offset+2] & 0xff) << 40) |
+            ((long)(bytes[offset+3] & 0xff) << 32) |
+            ((long)(bytes[offset+4] & 0xff) << 24) |
+            ((long)(bytes[offset+5] & 0xff) << 16) |
+            ((long)(bytes[offset+6] & 0xff) <<  8) |
+            ((long)(bytes[offset+7] & 0xff)));
+        w.add(asLong == 0 ? defaultValue : asLong - minValue);
+      }
+      for (int i = lastDocID + 1; i < docCount; i++) {
+        w.add(defaultValue);
+      }
+      w.finish();
+    }
+
+    @Override
+    public void add(int docID, DocValue docValue) throws IOException {
+      add(docID, docValue.getInt());
+    }
+  }
+
+  /**
+   * Opens all necessary files, but does not read any data in until you call
+   * {@link #load}.
+   */
+  static class PackedIntsReader extends DocValues {
+    private final IndexInput datIn;
+    private final byte type;
+    private final int numDocs;
+    private final LongValues values;
+
+    protected PackedIntsReader(Directory dir, String id, int numDocs,
+        IOContext context) throws IOException {
+      datIn = dir.openInput(
+                IndexFileNames.segmentFileName(id, Bytes.DV_SEGMENT_SUFFIX, Writer.DATA_EXTENSION),
+          context);
+      this.numDocs = numDocs;
+      boolean success = false;
+      try {
+        CodecUtil.checkHeader(datIn, CODEC_NAME, VERSION_START, VERSION_START);
+        type = datIn.readByte();
+        values = type == FIXED_64 ? new LongValues() : null;
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(datIn);
+        }
+      }
+    }
+
+
+    /**
+     * Loads the actual values. You may call this more than once, eg if you
+     * already previously loaded but then discarded the Source.
+     */
+    @Override
+    public Source load() throws IOException {
+      boolean success = false;
+      final Source source;
+      IndexInput input = null;
+      try {
+        input = (IndexInput) datIn.clone();
+        
+        if (values == null) {
+          source = new PackedIntsSource(input, false);
+        } else {
+          source = values.newFromInput(input, numDocs);
+        }
+        success = true;
+        return source;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(input, datIn);
+        }
+      }
+    }
+
+    @Override
+    public void close() throws IOException {
+      super.close();
+      datIn.close();
+    }
+
+
+    @Override
+    public Type type() {
+      return Type.VAR_INTS;
+    }
+
+
+    @Override
+    public Source getDirectSource() throws IOException {
+      return values != null ? new FixedStraightBytesImpl.DirectFixedStraightSource((IndexInput) datIn.clone(), 8, Type.FIXED_INTS_64) : new PackedIntsSource((IndexInput) datIn.clone(), true);
+    }
+  }
+
+  
+  static class PackedIntsSource extends Source {
+    private final long minValue;
+    private final long defaultValue;
+    private final PackedInts.Reader values;
+
+    public PackedIntsSource(IndexInput dataIn, boolean direct) throws IOException {
+      super(Type.VAR_INTS);
+      minValue = dataIn.readLong();
+      defaultValue = dataIn.readLong();
+      values = direct ? PackedInts.getDirectReader(dataIn) : PackedInts.getReader(dataIn);
+    }
+    
+    @Override
+    public BytesRef getBytes(int docID, BytesRef ref) {
+      ref.grow(8);
+      BytesRefUtils.copyLong(ref, getInt(docID));
+      return ref;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      // TODO -- can we somehow avoid 2X method calls
+      // on each get? must push minValue down, and make
+      // PackedInts implement Ints.Source
+      assert docID >= 0;
+      final long value = values.get(docID);
+      return value == defaultValue ? 0 : minValue + value;
+    }
+  }
+
+}
\ No newline at end of file
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarDerefBytesImpl.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarDerefBytesImpl.java
new file mode 100644
index 0000000..19a7bd7
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarDerefBytesImpl.java
@@ -0,0 +1,151 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSourceBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.packed.PackedInts;
+
+// Stores variable-length byte[] by deref, ie when two docs
+// have the same value, they store only 1 byte[] and both
+// docs reference that single source
+
+/**
+ * @lucene.experimental
+ */
+class VarDerefBytesImpl {
+
+  static final String CODEC_NAME = "VarDerefBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  /*
+   * TODO: if impls like this are merged we are bound to the amount of memory we
+   * can store into a BytesRefHash and therefore how much memory a ByteBlockPool
+   * can address. This is currently limited to 2GB. While we could extend that
+   * and use 64bit for addressing this still limits us to the existing main
+   * memory as all distinct bytes will be loaded up into main memory. We could
+   * move the byte[] writing to #finish(int) and store the bytes in sorted
+   * order and merge them in a streamed fashion. 
+   */
+  static class Writer extends DerefBytesWriterBase {
+    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      size = 0;
+    }
+    
+    @Override
+    protected void checkSize(BytesRef bytes) {
+      // allow var bytes sizes
+    }
+
+    // Important that we get docCount, in case there were
+    // some last docs that we didn't see
+    @Override
+    public void finishInternal(int docCount) throws IOException {
+      fillDefault(docCount);
+      final int size = hash.size();
+      final long[] addresses = new long[size];
+      final IndexOutput datOut = getOrCreateDataOut();
+      int addr = 0;
+      final BytesRef bytesRef = new BytesRef();
+      for (int i = 0; i < size; i++) {
+        hash.get(i, bytesRef);
+        addresses[i] = addr;
+        addr += writePrefixLength(datOut, bytesRef) + bytesRef.length;
+        datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+      }
+
+      final IndexOutput idxOut = getOrCreateIndexOut();
+      // write the max address to read directly on source load
+      idxOut.writeLong(addr);
+      writeIndex(idxOut, docCount, addresses[addresses.length-1], addresses, docToEntry);
+    }
+  }
+
+  public static class VarDerefReader extends BytesReaderBase {
+    private final long totalBytes;
+    VarDerefReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_START, true, context, Type.BYTES_VAR_DEREF);
+      totalBytes = idxIn.readLong();
+    }
+
+    @Override
+    public Source load() throws IOException {
+      return new VarDerefSource(cloneData(), cloneIndex(), totalBytes);
+    }
+   
+    @Override
+    public Source getDirectSource()
+        throws IOException {
+      return new DirectVarDerefSource(cloneData(), cloneIndex(), type());
+    }
+  }
+  
+  final static class VarDerefSource extends BytesSourceBase {
+    private final PackedInts.Reader addresses;
+
+    public VarDerefSource(IndexInput datIn, IndexInput idxIn, long totalBytes)
+        throws IOException {
+      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), totalBytes,
+          Type.BYTES_VAR_DEREF);
+      addresses = PackedInts.getReader(idxIn);
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      return data.fillSliceWithPrefix(bytesRef,
+          addresses.get(docID));
+    }
+  }
+
+  
+  final static class DirectVarDerefSource extends DirectSource {
+    private final PackedInts.Reader index;
+
+    DirectVarDerefSource(IndexInput data, IndexInput index, Type type)
+        throws IOException {
+      super(data, type);
+      this.index = PackedInts.getDirectReader(index);
+    }
+    
+    @Override
+    protected int position(int docID) throws IOException {
+      data.seek(baseOffset + index.get(docID));
+      final byte sizeByte = data.readByte();
+      if ((sizeByte & 128) == 0) {
+        // length is 1 byte
+        return sizeByte;
+      } else {
+        return ((sizeByte & 0x7f) << 8) | ((data.readByte() & 0xff));
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarSortedBytesImpl.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarSortedBytesImpl.java
new file mode 100644
index 0000000..40fe7e1
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarSortedBytesImpl.java
@@ -0,0 +1,254 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSortedSourceBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.SortedBytesMergeUtils;
+import org.apache.lucene.index.DocValues.SortedSource;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.SortedBytesMergeUtils.MergeContext;
+import org.apache.lucene.index.SortedBytesMergeUtils.SortedSourceSlice;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+// Stores variable-length byte[] by deref, ie when two docs
+// have the same value, they store only 1 byte[] and both
+// docs reference that single source
+
+/**
+ * @lucene.experimental
+ */
+final class VarSortedBytesImpl {
+
+  static final String CODEC_NAME = "VarDerefBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  final static class Writer extends DerefBytesWriterBase {
+    private final Comparator<BytesRef> comp;
+
+    public Writer(Directory dir, String id, Comparator<BytesRef> comp,
+        Counter bytesUsed, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      this.comp = comp;
+      size = 0;
+    }
+    @Override
+    public void merge(MergeState mergeState, DocValues[] docValues)
+        throws IOException {
+      boolean success = false;
+      try {
+        MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_VAR_SORTED, docValues, comp, mergeState);
+        final List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState, docValues, ctx);
+        IndexOutput datOut = getOrCreateDataOut();
+        
+        ctx.offsets = new long[1];
+        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, datOut, slices);
+        final long[] offsets = ctx.offsets;
+        maxBytes = offsets[maxOrd-1];
+        final IndexOutput idxOut = getOrCreateIndexOut();
+        
+        idxOut.writeLong(maxBytes);
+        final PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, maxOrd+1,
+            PackedInts.bitsRequired(maxBytes));
+        offsetWriter.add(0);
+        for (int i = 0; i < maxOrd; i++) {
+          offsetWriter.add(offsets[i]);
+        }
+        offsetWriter.finish();
+        
+        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
+            PackedInts.bitsRequired(maxOrd-1));
+        for (SortedSourceSlice slice : slices) {
+          slice.writeOrds(ordsWriter);
+        }
+        ordsWriter.finish();
+        success = true;
+      } finally {
+        releaseResources();
+        if (success) {
+          IOUtils.close(getIndexOut(), getDataOut());
+        } else {
+          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
+        }
+
+      }
+    }
+
+    @Override
+    protected void checkSize(BytesRef bytes) {
+      // allow var bytes sizes
+    }
+
+    // Important that we get docCount, in case there were
+    // some last docs that we didn't see
+    @Override
+    public void finishInternal(int docCount) throws IOException {
+      fillDefault(docCount);
+      final int count = hash.size();
+      final IndexOutput datOut = getOrCreateDataOut();
+      final IndexOutput idxOut = getOrCreateIndexOut();
+      long offset = 0;
+      final int[] index = new int[count];
+      final int[] sortedEntries = hash.sort(comp);
+      // total bytes of data
+      idxOut.writeLong(maxBytes);
+      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,
+          PackedInts.bitsRequired(maxBytes));
+      // first dump bytes data, recording index & write offset as
+      // we go
+      final BytesRef spare = new BytesRef();
+      for (int i = 0; i < count; i++) {
+        final int e = sortedEntries[i];
+        offsetWriter.add(offset);
+        index[e] = i;
+        final BytesRef bytes = hash.get(e, spare);
+        // TODO: we could prefix code...
+        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
+        offset += bytes.length;
+      }
+      // write sentinel
+      offsetWriter.add(offset);
+      offsetWriter.finish();
+      // write index
+      writeIndex(idxOut, docCount, count, index, docToEntry);
+
+    }
+  }
+
+  public static class Reader extends BytesReaderBase {
+
+    private final Comparator<BytesRef> comparator;
+
+    Reader(Directory dir, String id, int maxDoc,
+        IOContext context, Type type, Comparator<BytesRef> comparator)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_START, true, context, type);
+      this.comparator = comparator;
+    }
+
+    @Override
+    public org.apache.lucene.index.DocValues.Source load()
+        throws IOException {
+      return new VarSortedSource(cloneData(), cloneIndex(), comparator);
+    }
+
+    @Override
+    public Source getDirectSource() throws IOException {
+      return new DirectSortedSource(cloneData(), cloneIndex(), comparator, type());
+    }
+    
+  }
+  private static final class VarSortedSource extends BytesSortedSourceBase {
+    private final int valueCount;
+
+    VarSortedSource(IndexInput datIn, IndexInput idxIn,
+        Comparator<BytesRef> comp) throws IOException {
+      super(datIn, idxIn, comp, idxIn.readLong(), Type.BYTES_VAR_SORTED, true);
+      valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
+      closeIndexInput();
+    }
+
+    @Override
+    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+      final long offset = ordToOffsetIndex.get(ord);
+      final long nextOffset = ordToOffsetIndex.get(1 + ord);
+      data.fillSlice(bytesRef, offset, (int) (nextOffset - offset));
+      return bytesRef;
+    }
+
+    @Override
+    public int getValueCount() {
+      return valueCount;
+    }
+  }
+
+  private static final class DirectSortedSource extends SortedSource {
+    private final PackedInts.Reader docToOrdIndex;
+    private final PackedInts.Reader ordToOffsetIndex;
+    private final IndexInput datIn;
+    private final long basePointer;
+    private final int valueCount;
+    
+    DirectSortedSource(IndexInput datIn, IndexInput idxIn,
+        Comparator<BytesRef> comparator, Type type) throws IOException {
+      super(type, comparator);
+      idxIn.readLong();
+      ordToOffsetIndex = PackedInts.getDirectReader(idxIn);
+      valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
+      // advance this iterator to the end and clone the stream once it points to the docToOrdIndex header
+      ordToOffsetIndex.get(valueCount);
+      docToOrdIndex = PackedInts.getDirectReader((IndexInput) idxIn.clone()); // read the ords in to prevent too many random disk seeks
+      basePointer = datIn.getFilePointer();
+      this.datIn = datIn;
+    }
+
+    @Override
+    public int ord(int docID) {
+      return (int) docToOrdIndex.get(docID);
+    }
+
+    @Override
+    public boolean hasPackedDocToOrd() {
+      return true;
+    }
+
+    @Override
+    public PackedInts.Reader getDocToOrd() {
+      return docToOrdIndex;
+    }
+
+    @Override
+    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+      try {
+        final long offset = ordToOffsetIndex.get(ord);
+        // 1+ord is safe because we write a sentinel at the end
+        final long nextOffset = ordToOffsetIndex.get(1+ord);
+        datIn.seek(basePointer + offset);
+        final int length = (int) (nextOffset - offset);
+        bytesRef.grow(length);
+        datIn.readBytes(bytesRef.bytes, 0, length);
+        bytesRef.length = length;
+        bytesRef.offset = 0;
+        return bytesRef;
+      } catch (IOException ex) {
+        throw new IllegalStateException("failed", ex);
+      }
+    }
+    
+    @Override
+    public int getValueCount() {
+      return valueCount;
+    }
+
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarStraightBytesImpl.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarStraightBytesImpl.java
new file mode 100644
index 0000000..cf716c7
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/VarStraightBytesImpl.java
@@ -0,0 +1,285 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesReaderBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesSourceBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes.BytesWriterBase;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.ByteBlockPool;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
+import org.apache.lucene.util.packed.PackedInts;
+import org.apache.lucene.util.packed.PackedInts.ReaderIterator;
+
+// Variable length byte[] per document, no sharing
+
+/**
+ * @lucene.experimental
+ */
+class VarStraightBytesImpl {
+
+  static final String CODEC_NAME = "VarStraightBytes";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+
+  static class Writer extends BytesWriterBase {
+    private long address;
+    // start at -1 if the first added value is > 0
+    private int lastDocID = -1;
+    private long[] docToAddress;
+    private final ByteBlockPool pool;
+    private IndexOutput datOut;
+    private boolean merge = false;
+    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
+        throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
+      pool = new ByteBlockPool(new DirectTrackingAllocator(bytesUsed));
+      docToAddress = new long[1];
+      pool.nextBuffer(); // init
+      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT);
+    }
+
+    // Fills up to but not including this docID
+    private void fill(final int docID, final long nextAddress) {
+      if (docID >= docToAddress.length) {
+        int oldSize = docToAddress.length;
+        docToAddress = ArrayUtil.grow(docToAddress, 1 + docID);
+        bytesUsed.addAndGet((docToAddress.length - oldSize)
+            * RamUsageEstimator.NUM_BYTES_INT);
+      }
+      for (int i = lastDocID + 1; i < docID; i++) {
+        docToAddress[i] = nextAddress;
+      }
+    }
+
+    @Override
+    public void add(int docID, BytesRef bytes) throws IOException {
+      assert !merge;
+      if (bytes.length == 0) {
+        return; // default
+      }
+      fill(docID, address);
+      docToAddress[docID] = address;
+      pool.copy(bytes);
+      address += bytes.length;
+      lastDocID = docID;
+    }
+    
+    @Override
+    protected void merge(SingleSubMergeState state) throws IOException {
+      merge = true;
+      datOut = getOrCreateDataOut();
+      boolean success = false;
+      try {
+        if (state.liveDocs == null && state.reader instanceof VarStraightReader) {
+          // bulk merge since we don't have any deletes
+          VarStraightReader reader = (VarStraightReader) state.reader;
+          final int maxDocs = reader.maxDoc;
+          if (maxDocs == 0) {
+            return;
+          }
+          if (lastDocID+1 < state.docBase) {
+            fill(state.docBase, address);
+            lastDocID = state.docBase-1;
+          }
+          final long numDataBytes;
+          final IndexInput cloneIdx = reader.cloneIndex();
+          try {
+            numDataBytes = cloneIdx.readVLong();
+            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);
+            for (int i = 0; i < maxDocs; i++) {
+              long offset = iter.next();
+              ++lastDocID;
+              if (lastDocID >= docToAddress.length) {
+                int oldSize = docToAddress.length;
+                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);
+                bytesUsed.addAndGet((docToAddress.length - oldSize)
+                    * RamUsageEstimator.NUM_BYTES_INT);
+              }
+              docToAddress[lastDocID] = address + offset;
+            }
+            address += numDataBytes; // this is the address after all addr pointers are updated
+            iter.close();
+          } finally {
+            IOUtils.close(cloneIdx);
+          }
+          final IndexInput cloneData = reader.cloneData();
+          try {
+            datOut.copyBytes(cloneData, numDataBytes);
+          } finally {
+            IOUtils.close(cloneData);  
+          }
+        } else {
+          super.merge(state);
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(datOut);
+        }
+      }
+    }
+    
+    @Override
+    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
+      assert merge;
+      assert lastDocID < docID;
+      currentMergeSource.getBytes(sourceDoc, bytesRef);
+      if (bytesRef.length == 0) {
+        return; // default
+      }
+      fill(docID, address);
+      datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
+      docToAddress[docID] = address;
+      address += bytesRef.length;
+      lastDocID = docID;
+    }
+    
+
+    @Override
+    public void finish(int docCount) throws IOException {
+      boolean success = false;
+      assert (!merge && datOut == null) || (merge && datOut != null); 
+      final IndexOutput datOut = getOrCreateDataOut();
+      try {
+        if (!merge) {
+          // header is already written in getDataOut()
+          pool.writePool(datOut);
+        }
+        success = true;
+      } finally {
+        if (success) {
+          IOUtils.close(datOut);
+        } else {
+          IOUtils.closeWhileHandlingException(datOut);
+        }
+        pool.dropBuffersAndReset();
+      }
+
+      success = false;
+      final IndexOutput idxOut = getOrCreateIndexOut();
+      try {
+        if (lastDocID == -1) {
+          idxOut.writeVLong(0);
+          final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount+1,
+              PackedInts.bitsRequired(0));
+          // docCount+1 so we write sentinel
+          for (int i = 0; i < docCount+1; i++) {
+            w.add(0);
+          }
+          w.finish();
+        } else {
+          fill(docCount, address);
+          idxOut.writeVLong(address);
+          final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount+1,
+              PackedInts.bitsRequired(address));
+          for (int i = 0; i < docCount; i++) {
+            w.add(docToAddress[i]);
+          }
+          // write sentinel
+          w.add(address);
+          w.finish();
+        }
+        success = true;
+      } finally {
+        bytesUsed.addAndGet(-(docToAddress.length)
+            * RamUsageEstimator.NUM_BYTES_INT);
+        docToAddress = null;
+        if (success) {
+          IOUtils.close(idxOut);
+        } else {
+          IOUtils.closeWhileHandlingException(idxOut);
+        }
+      }
+    }
+
+    public long ramBytesUsed() {
+      return bytesUsed.get();
+    }
+  }
+
+  public static class VarStraightReader extends BytesReaderBase {
+    private final int maxDoc;
+
+    VarStraightReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
+      super(dir, id, CODEC_NAME, VERSION_START, true, context, Type.BYTES_VAR_STRAIGHT);
+      this.maxDoc = maxDoc;
+    }
+
+    @Override
+    public Source load() throws IOException {
+      return new VarStraightSource(cloneData(), cloneIndex());
+    }
+
+    @Override
+    public Source getDirectSource()
+        throws IOException {
+      return new DirectVarStraightSource(cloneData(), cloneIndex(), type());
+    }
+  }
+  
+  private static final class VarStraightSource extends BytesSourceBase {
+    private final PackedInts.Reader addresses;
+
+    public VarStraightSource(IndexInput datIn, IndexInput idxIn) throws IOException {
+      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), idxIn.readVLong(),
+          Type.BYTES_VAR_STRAIGHT);
+      addresses = PackedInts.getReader(idxIn);
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef bytesRef) {
+      final long address = addresses.get(docID);
+      return data.fillSlice(bytesRef, address,
+          (int) (addresses.get(docID + 1) - address));
+    }
+  }
+  
+  public final static class DirectVarStraightSource extends DirectSource {
+
+    private final PackedInts.Reader index;
+
+    DirectVarStraightSource(IndexInput data, IndexInput index, Type type)
+        throws IOException {
+      super(data, type);
+      index.readVLong();
+      this.index = PackedInts.getDirectReader(index);
+    }
+
+    @Override
+    protected int position(int docID) throws IOException {
+      final long offset = index.get(docID);
+      data.seek(baseOffset + offset);
+      // Safe to do 1+docID because we write sentinel at the end:
+      final long nextOffset = index.get(1+docID);
+      return (int) (nextOffset - offset);
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Writer.java b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Writer.java
new file mode 100644
index 0000000..9243ab8
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/lucene40/values/Writer.java
@@ -0,0 +1,219 @@
+package org.apache.lucene.codecs.lucene40.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+
+/**
+ * Abstract API for per-document stored primitive values of type <tt>byte[]</tt>
+ * , <tt>long</tt> or <tt>double</tt>. The API accepts a single value for each
+ * document. The underlying storage mechanism, file formats, data-structures and
+ * representations depend on the actual implementation.
+ * <p>
+ * Document IDs passed to this API must always be increasing unless stated
+ * otherwise.
+ * </p>
+ * 
+ * @lucene.experimental
+ */
+public abstract class Writer extends DocValuesConsumer {
+  protected Source currentMergeSource;
+  protected final Counter bytesUsed;
+
+  /**
+   * Creates a new {@link Writer}.
+   * 
+   * @param bytesUsed
+   *          bytes-usage tracking reference used by implementation to track
+   *          internally allocated memory. All tracked bytes must be released
+   *          once {@link #finish(int)} has been called.
+   */
+  protected Writer(Counter bytesUsed) {
+    this.bytesUsed = bytesUsed;
+  }
+
+  /**
+   * Filename extension for index files
+   */
+  public static final String INDEX_EXTENSION = "idx";
+  
+  /**
+   * Filename extension for data files.
+   */
+  public static final String DATA_EXTENSION = "dat";
+
+  /**
+   * Records the specified <tt>long</tt> value for the docID or throws an
+   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
+   * <tt>long</tt> values.
+   * 
+   * @throws UnsupportedOperationException
+   *           if this writer doesn't record <tt>long</tt> values
+   */
+  public void add(int docID, long value) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  /**
+   * Records the specified <tt>double</tt> value for the docID or throws an
+   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
+   * <tt>double</tt> values.
+   * 
+   * @throws UnsupportedOperationException
+   *           if this writer doesn't record <tt>double</tt> values
+   */
+  public void add(int docID, double value) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  /**
+   * Records the specified {@link BytesRef} value for the docID or throws an
+   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
+   * {@link BytesRef} values.
+   * 
+   * @throws UnsupportedOperationException
+   *           if this writer doesn't record {@link BytesRef} values
+   */
+  public void add(int docID, BytesRef value) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+
+  /**
+   * Merges a document with the given <code>docID</code>. The methods
+   * implementation obtains the value for the <i>sourceDoc</i> id from the
+   * current {@link Source} set to <i>setNextMergeSource(Source)</i>.
+   * <p>
+   * This method is used during merging to provide implementation agnostic
+   * default merge implementation.
+   * </p>
+   * <p>
+   * All documents IDs between the given ID and the previously given ID or
+   * <tt>0</tt> if the method is call the first time are filled with default
+   * values depending on the {@link Writer} implementation. The given document
+   * ID must always be greater than the previous ID or <tt>0</tt> if called the
+   * first time.
+   */
+  protected abstract void mergeDoc(int docID, int sourceDoc) throws IOException;
+
+  /**
+   * Sets the next {@link Source} to consume values from on calls to
+   * {@link #mergeDoc(int, int)}
+   * 
+   * @param mergeSource
+   *          the next {@link Source}, this must not be null
+   */
+  protected void setNextMergeSource(Source mergeSource) {
+    currentMergeSource = mergeSource;
+  }
+
+  /**
+   * Finish writing and close any files and resources used by this Writer.
+   * 
+   * @param docCount
+   *          the total number of documents for this writer. This must be
+   *          greater that or equal to the largest document id passed to one of
+   *          the add methods after the {@link Writer} was created.
+   */
+  public abstract void finish(int docCount) throws IOException;
+
+  @Override
+  protected void merge(SingleSubMergeState state) throws IOException {
+    // This enables bulk copies in subclasses per MergeState, subclasses can
+    // simply override this and decide if they want to merge
+    // segments using this generic implementation or if a bulk merge is possible
+    // / feasible.
+    final Source source = state.reader.getDirectSource();
+    assert source != null;
+    setNextMergeSource(source); // set the current enum we are working on - the
+    // impl. will get the correct reference for the type
+    // it supports
+    int docID = state.docBase;
+    final Bits liveDocs = state.liveDocs;
+    final int docCount = state.docCount;
+    for (int i = 0; i < docCount; i++) {
+      if (liveDocs == null || liveDocs.get(i)) {
+        mergeDoc(docID++, i);
+      }
+    }
+    
+  }
+
+  /**
+   * Factory method to create a {@link Writer} instance for a given type. This
+   * method returns default implementations for each of the different types
+   * defined in the {@link Type} enumeration.
+   * 
+   * @param type
+   *          the {@link Type} to create the {@link Writer} for
+   * @param id
+   *          the file name id used to create files within the writer.
+   * @param directory
+   *          the {@link Directory} to create the files from.
+   * @param bytesUsed
+   *          a byte-usage tracking reference
+   * @return a new {@link Writer} instance for the given {@link Type}
+   * @throws IOException
+   */
+  public static Writer create(Type type, String id, Directory directory,
+      Comparator<BytesRef> comp, Counter bytesUsed, IOContext context) throws IOException {
+    if (comp == null) {
+      comp = BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+    switch (type) {
+    case FIXED_INTS_16:
+    case FIXED_INTS_32:
+    case FIXED_INTS_64:
+    case FIXED_INTS_8:
+    case VAR_INTS:
+      return Ints.getWriter(directory, id, bytesUsed, type, context);
+    case FLOAT_32:
+      return Floats.getWriter(directory, id, bytesUsed, context, type);
+    case FLOAT_64:
+      return Floats.getWriter(directory, id, bytesUsed, context, type);
+    case BYTES_FIXED_STRAIGHT:
+      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, true, comp,
+          bytesUsed, context);
+    case BYTES_FIXED_DEREF:
+      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, true, comp,
+          bytesUsed, context);
+    case BYTES_FIXED_SORTED:
+      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, true, comp,
+          bytesUsed, context);
+    case BYTES_VAR_STRAIGHT:
+      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, false, comp,
+          bytesUsed, context);
+    case BYTES_VAR_DEREF:
+      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, false, comp,
+          bytesUsed, context);
+    case BYTES_VAR_SORTED:
+      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, false, comp,
+          bytesUsed, context);
+    default:
+      throw new IllegalArgumentException("Unknown Values: " + type);
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java b/lucene/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
new file mode 100644
index 0000000..8d2c2b0
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
@@ -0,0 +1,809 @@
+package org.apache.lucene.codecs.memory;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsConsumer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.ByteSequenceOutputs;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.FST;
+
+// TODO: would be nice to somehow allow this to act like
+// InstantiatedIndex, by never writing to disk; ie you write
+// to this Codec in RAM only and then when you open a reader
+// it pulls the FST directly from what you wrote w/o going
+// to disk.
+
+/** Stores terms & postings (docs, positions, payloads) in
+ *  RAM, using an FST.
+ *
+ * <p>Note that this codec implements advance as a linear
+ * scan!  This means if you store large fields in here,
+ * queries that rely on advance will (AND BooleanQuery,
+ * PhraseQuery) will be relatively slow!
+ *
+ * <p><b>NOTE</b>: this codec cannot address more than ~2.1 GB
+ * of postings, because the underlying FST uses an int
+ * to address the underlying byte[].
+ *
+ * @lucene.experimental */
+
+// TODO: Maybe name this 'Cached' or something to reflect
+// the reality that it is actually written to disk, but
+// loads itself in ram?
+public class MemoryPostingsFormat extends PostingsFormat {
+  
+  public MemoryPostingsFormat() {
+    super("Memory");
+  }
+
+  private static final boolean VERBOSE = false;
+
+  private final static class TermsWriter extends TermsConsumer {
+    private final IndexOutput out;
+    private final FieldInfo field;
+    private final Builder<BytesRef> builder;
+    private final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
+    private int termCount;
+
+    public TermsWriter(IndexOutput out, FieldInfo field) {
+      this.out = out;
+      this.field = field;
+      builder = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1, outputs);
+    }
+
+    private class PostingsWriter extends PostingsConsumer {
+      private int lastDocID;
+      private int lastPos;
+      private int lastPayloadLen;
+
+      // NOTE: not private so we don't pay access check at runtime:
+      int docCount;
+      RAMOutputStream buffer = new RAMOutputStream();
+
+      @Override
+      public void startDoc(int docID, int termDocFreq) throws IOException {
+        if (VERBOSE) System.out.println("    startDoc docID=" + docID + " freq=" + termDocFreq);
+        final int delta = docID - lastDocID;
+        assert docID == 0 || delta > 0;
+        lastDocID = docID;
+        docCount++;
+
+        if (field.indexOptions == IndexOptions.DOCS_ONLY) {
+          buffer.writeVInt(delta);
+        } else if (termDocFreq == 1) {
+          buffer.writeVInt((delta<<1) | 1);
+        } else {
+          buffer.writeVInt(delta<<1);
+          assert termDocFreq > 0;
+          buffer.writeVInt(termDocFreq);
+        }
+
+        lastPos = 0;
+      }
+
+      @Override
+      public void addPosition(int pos, BytesRef payload) throws IOException {
+        assert payload == null || field.storePayloads;
+
+        if (VERBOSE) System.out.println("      addPos pos=" + pos + " payload=" + payload);
+
+        final int delta = pos - lastPos;
+        assert delta >= 0;
+        lastPos = pos;
+        
+        if (field.storePayloads) {
+          final int payloadLen = payload == null ? 0 : payload.length;
+          if (payloadLen != lastPayloadLen) {
+            lastPayloadLen = payloadLen;
+            buffer.writeVInt((delta<<1)|1);
+            buffer.writeVInt(payloadLen);
+          } else {
+            buffer.writeVInt(delta<<1);
+          }
+
+          if (payloadLen > 0) {
+            buffer.writeBytes(payload.bytes, payload.offset, payloadLen);
+          }
+        } else {
+          buffer.writeVInt(delta);
+        }
+      }
+
+      @Override
+      public void finishDoc() {
+      }
+
+      public PostingsWriter reset() {
+        assert buffer.getFilePointer() == 0;
+        lastDocID = 0;
+        docCount = 0;
+        lastPayloadLen = 0;
+        return this;
+      }
+    }
+
+    private final PostingsWriter postingsWriter = new PostingsWriter();
+
+    @Override
+    public PostingsConsumer startTerm(BytesRef text) {
+      if (VERBOSE) System.out.println("  startTerm term=" + text.utf8ToString());
+      return postingsWriter.reset();
+    }
+
+    private final RAMOutputStream buffer2 = new RAMOutputStream();
+    private final BytesRef spare = new BytesRef();
+    private byte[] finalBuffer = new byte[128];
+
+    @Override
+    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
+
+      assert postingsWriter.docCount == stats.docFreq;
+
+      assert buffer2.getFilePointer() == 0;
+
+      buffer2.writeVInt(stats.docFreq);
+      if (field.indexOptions != IndexOptions.DOCS_ONLY) {
+        buffer2.writeVLong(stats.totalTermFreq-stats.docFreq);
+      }
+      int pos = (int) buffer2.getFilePointer();
+      buffer2.writeTo(finalBuffer, 0);
+      buffer2.reset();
+
+      final int totalBytes = pos + (int) postingsWriter.buffer.getFilePointer();
+      if (totalBytes > finalBuffer.length) {
+        finalBuffer = ArrayUtil.grow(finalBuffer, totalBytes);
+      }
+      postingsWriter.buffer.writeTo(finalBuffer, pos);
+      postingsWriter.buffer.reset();
+
+      spare.bytes = finalBuffer;
+      spare.length = totalBytes;
+      if (VERBOSE) {
+        System.out.println("    finishTerm term=" + text.utf8ToString() + " " + totalBytes + " bytes totalTF=" + stats.totalTermFreq);
+        for(int i=0;i<totalBytes;i++) {
+          System.out.println("      " + Integer.toHexString(finalBuffer[i]&0xFF));
+        }
+      }
+      builder.add(text, BytesRef.deepCopyOf(spare));
+      termCount++;
+    }
+
+    @Override
+    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
+      if (termCount > 0) {
+        out.writeVInt(termCount);
+        out.writeVInt(field.number);
+        if (field.indexOptions != IndexOptions.DOCS_ONLY) {
+          out.writeVLong(sumTotalTermFreq);
+        }
+        out.writeVLong(sumDocFreq);
+        out.writeVInt(docCount);
+        builder.finish().save(out);
+        if (VERBOSE) System.out.println("finish field=" + field.name + " fp=" + out.getFilePointer());
+      }
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+
+  private static String EXTENSION = "ram";
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+
+    final String fileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, EXTENSION);
+    final IndexOutput out = state.directory.createOutput(fileName, state.context);
+    
+    return new FieldsConsumer() {
+      @Override
+      public TermsConsumer addField(FieldInfo field) {
+        if (VERBOSE) System.out.println("\naddField field=" + field.name);
+        return new TermsWriter(out, field);
+      }
+
+      @Override
+      public void close() throws IOException {
+        // EOF marker:
+        try {
+          out.writeVInt(0);
+        } finally {
+          out.close();
+        }
+      }
+    };
+  }
+
+  private final static class FSTDocsEnum extends DocsEnum {
+    private final IndexOptions indexOptions;
+    private final boolean storePayloads;
+    private byte[] buffer = new byte[16];
+    private final ByteArrayDataInput in = new ByteArrayDataInput(buffer);
+
+    private Bits liveDocs;
+    private int docUpto;
+    private int docID = -1;
+    private int accum;
+    private int freq;
+    private int payloadLen;
+    private int numDocs;
+
+    public FSTDocsEnum(IndexOptions indexOptions, boolean storePayloads) {
+      this.indexOptions = indexOptions;
+      this.storePayloads = storePayloads;
+    }
+
+    public boolean canReuse(IndexOptions indexOptions, boolean storePayloads) {
+      return indexOptions == this.indexOptions && storePayloads == this.storePayloads;
+    }
+    
+    public FSTDocsEnum reset(BytesRef bufferIn, Bits liveDocs, int numDocs) {
+      assert numDocs > 0;
+      if (buffer.length < bufferIn.length - bufferIn.offset) {
+        buffer = ArrayUtil.grow(buffer, bufferIn.length - bufferIn.offset);
+      }
+      in.reset(buffer, 0, bufferIn.length - bufferIn.offset);
+      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length - bufferIn.offset);
+      this.liveDocs = liveDocs;
+      docID = -1;
+      accum = 0;
+      docUpto = 0;
+      payloadLen = 0;
+      this.numDocs = numDocs;
+      return this;
+    }
+
+    @Override
+    public int nextDoc() {
+      while(true) {
+        if (VERBOSE) System.out.println("  nextDoc cycle docUpto=" + docUpto + " numDocs=" + numDocs + " fp=" + in.getPosition() + " this=" + this);
+        if (docUpto == numDocs) {
+          if (VERBOSE) {
+            System.out.println("    END");
+          }
+          return docID = NO_MORE_DOCS;
+        }
+        docUpto++;
+        if (indexOptions == IndexOptions.DOCS_ONLY) {
+          accum += in.readVInt();
+        } else {
+          final int code = in.readVInt();
+          accum += code >>> 1;
+          if (VERBOSE) System.out.println("  docID=" + accum + " code=" + code);
+          if ((code & 1) != 0) {
+            freq = 1;
+          } else {
+            freq = in.readVInt();
+            assert freq > 0;
+          }
+
+          if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+            // Skip positions
+            for(int posUpto=0;posUpto<freq;posUpto++) {
+              if (!storePayloads) {
+                in.readVInt();
+              } else {
+                final int posCode = in.readVInt();
+                if ((posCode & 1) != 0) {
+                  payloadLen = in.readVInt();
+                }
+                in.skipBytes(payloadLen);
+              }
+            }
+          }
+        }
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          if (VERBOSE) System.out.println("    return docID=" + accum + " freq=" + freq);
+          return (docID = accum);
+        }
+      }
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int advance(int target) {
+      // TODO: we could make more efficient version, but, it
+      // should be rare that this will matter in practice
+      // since usually apps will not store "big" fields in
+      // this codec!
+      //System.out.println("advance start docID=" + docID + " target=" + target);
+      while(nextDoc() < target) {
+      }
+      return docID;
+    }
+
+    @Override
+    public int freq() {
+      assert indexOptions != IndexOptions.DOCS_ONLY;
+      return freq;
+    }
+  }
+
+  private final static class FSTDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private final boolean storePayloads;
+    private byte[] buffer = new byte[16];
+    private final ByteArrayDataInput in = new ByteArrayDataInput(buffer);
+
+    private Bits liveDocs;
+    private int docUpto;
+    private int docID = -1;
+    private int accum;
+    private int freq;
+    private int numDocs;
+    private int posPending;
+    private int payloadLength;
+    private boolean payloadRetrieved;
+
+    private int pos;
+    private final BytesRef payload = new BytesRef();
+
+    public FSTDocsAndPositionsEnum(boolean storePayloads) {
+      this.storePayloads = storePayloads;
+    }
+
+    public boolean canReuse(boolean storePayloads) {
+      return storePayloads == this.storePayloads;
+    }
+    
+    public FSTDocsAndPositionsEnum reset(BytesRef bufferIn, Bits liveDocs, int numDocs) {
+      assert numDocs > 0;
+      if (VERBOSE) {
+        System.out.println("D&P reset bytes this=" + this);
+        for(int i=bufferIn.offset;i<bufferIn.length;i++) {
+          System.out.println("  " + Integer.toHexString(bufferIn.bytes[i]&0xFF));
+        }
+      }
+      if (buffer.length < bufferIn.length - bufferIn.offset) {
+        buffer = ArrayUtil.grow(buffer, bufferIn.length - bufferIn.offset);
+      }
+      in.reset(buffer, 0, bufferIn.length - bufferIn.offset);
+      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length - bufferIn.offset);
+      this.liveDocs = liveDocs;
+      docID = -1;
+      accum = 0;
+      docUpto = 0;
+      payload.bytes = buffer;
+      payloadLength = 0;
+      this.numDocs = numDocs;
+      posPending = 0;
+      payloadRetrieved = false;
+      return this;
+    }
+
+    @Override
+    public int nextDoc() {
+      while (posPending > 0) {
+        nextPosition();
+      }
+      while(true) {
+        if (VERBOSE) System.out.println("  nextDoc cycle docUpto=" + docUpto + " numDocs=" + numDocs + " fp=" + in.getPosition() + " this=" + this);
+        if (docUpto == numDocs) {
+          if (VERBOSE) System.out.println("    END");
+          return docID = NO_MORE_DOCS;
+        }
+        docUpto++;
+        
+        final int code = in.readVInt();
+        accum += code >>> 1;
+        if ((code & 1) != 0) {
+          freq = 1;
+        } else {
+          freq = in.readVInt();
+          assert freq > 0;
+        }
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          pos = 0;
+          posPending = freq;
+          if (VERBOSE) System.out.println("    return docID=" + accum + " freq=" + freq);
+          return (docID = accum);
+        }
+
+        // Skip positions
+        for(int posUpto=0;posUpto<freq;posUpto++) {
+          if (!storePayloads) {
+            in.readVInt();
+          } else {
+            final int skipCode = in.readVInt();
+            if ((skipCode & 1) != 0) {
+              payloadLength = in.readVInt();
+              if (VERBOSE) System.out.println("    new payloadLen=" + payloadLength);
+            }
+            in.skipBytes(payloadLength);
+          }
+        }
+      }
+    }
+
+    @Override
+    public int nextPosition() {
+      if (VERBOSE) System.out.println("    nextPos storePayloads=" + storePayloads + " this=" + this);
+      assert posPending > 0;
+      posPending--;
+      if (!storePayloads) {
+        pos += in.readVInt();
+      } else {
+        final int code = in.readVInt();
+        pos += code >>> 1;
+        if ((code & 1) != 0) {
+          payloadLength = in.readVInt();
+          //System.out.println("      new payloadLen=" + payloadLength);
+          //} else {
+          //System.out.println("      same payloadLen=" + payloadLength);
+        }
+        payload.offset = in.getPosition();
+        in.skipBytes(payloadLength);
+        payload.length = payloadLength;
+        // Necessary, in case caller changed the
+        // payload.bytes from prior call:
+        payload.bytes = buffer;
+        payloadRetrieved = false;
+      }
+
+      if (VERBOSE) System.out.println("      pos=" + pos + " payload=" + payload + " fp=" + in.getPosition());
+      return pos;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      payloadRetrieved = true;
+      return payload;
+    }
+
+    @Override
+    public boolean hasPayload() {
+      return !payloadRetrieved && payload.length > 0;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int advance(int target) {
+      // TODO: we could make more efficient version, but, it
+      // should be rare that this will matter in practice
+      // since usually apps will not store "big" fields in
+      // this codec!
+      //System.out.println("advance target=" + target);
+      while(nextDoc() < target) {
+      }
+      //System.out.println("  return " + docID);
+      return docID;
+    }
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+  }
+
+  private final static class FSTTermsEnum extends TermsEnum {
+    private final FieldInfo field;
+    private final BytesRefFSTEnum<BytesRef> fstEnum;
+    private final ByteArrayDataInput buffer = new ByteArrayDataInput();
+    private boolean didDecode;
+
+    private int docFreq;
+    private long totalTermFreq;
+    private BytesRefFSTEnum.InputOutput<BytesRef> current;
+
+    public FSTTermsEnum(FieldInfo field, FST<BytesRef> fst) {
+      this.field = field;
+      fstEnum = new BytesRefFSTEnum<BytesRef>(fst);
+    }
+
+    private void decodeMetaData() throws IOException {
+      if (!didDecode) {
+        buffer.reset(current.output.bytes, 0, current.output.length);
+        docFreq = buffer.readVInt();
+        if (field.indexOptions != IndexOptions.DOCS_ONLY) {
+          totalTermFreq = docFreq + buffer.readVLong();
+        } else {
+          totalTermFreq = -1;
+        }
+        current.output.offset = buffer.getPosition();
+        if (VERBOSE) System.out.println("  df=" + docFreq + " totTF=" + totalTermFreq + " offset=" + buffer.getPosition() + " len=" + current.output.length);
+        didDecode = true;
+      }
+    }
+
+    @Override
+    public boolean seekExact(BytesRef text, boolean useCache /* ignored */) throws IOException {
+      if (VERBOSE) System.out.println("te.seekExact text=" + field.name + ":" + text.utf8ToString() + " this=" + this);
+      current = fstEnum.seekExact(text);
+      didDecode = false;
+      return current != null;
+    }
+
+    @Override
+    public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
+      if (VERBOSE) System.out.println("te.seek text=" + field.name + ":" + text.utf8ToString() + " this=" + this);
+      current = fstEnum.seekCeil(text);
+      if (current == null) {
+        return SeekStatus.END;
+      } else {
+        if (VERBOSE) {
+          System.out.println("  got term=" + current.input.utf8ToString());
+          for(int i=0;i<current.output.length;i++) {
+            System.out.println("    " + Integer.toHexString(current.output.bytes[i]&0xFF));
+          }
+        }
+
+        didDecode = false;
+
+        if (text.equals(current.input)) {
+          if (VERBOSE) System.out.println("  found!");
+          return SeekStatus.FOUND;
+        } else {
+          if (VERBOSE) System.out.println("  not found: " + current.input.utf8ToString());
+          return SeekStatus.NOT_FOUND;
+        }
+      }
+    }
+    
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
+      decodeMetaData();
+      FSTDocsEnum docsEnum;
+
+      if (needsFreqs && field.indexOptions == IndexOptions.DOCS_ONLY) {
+        return null;
+      } else if (reuse == null || !(reuse instanceof FSTDocsEnum)) {
+        docsEnum = new FSTDocsEnum(field.indexOptions, field.storePayloads);
+      } else {
+        docsEnum = (FSTDocsEnum) reuse;        
+        if (!docsEnum.canReuse(field.indexOptions, field.storePayloads)) {
+          docsEnum = new FSTDocsEnum(field.indexOptions, field.storePayloads);
+        }
+      }
+      return docsEnum.reset(current.output, liveDocs, docFreq);
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
+      if (field.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+        return null;
+      }
+      decodeMetaData();
+      FSTDocsAndPositionsEnum docsAndPositionsEnum;
+      if (reuse == null || !(reuse instanceof FSTDocsAndPositionsEnum)) {
+        docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.storePayloads);
+      } else {
+        docsAndPositionsEnum = (FSTDocsAndPositionsEnum) reuse;        
+        if (!docsAndPositionsEnum.canReuse(field.storePayloads)) {
+          docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.storePayloads);
+        }
+      }
+      if (VERBOSE) System.out.println("D&P reset this=" + this);
+      return docsAndPositionsEnum.reset(current.output, liveDocs, docFreq);
+    }
+
+    @Override
+    public BytesRef term() {
+      return current.input;
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      if (VERBOSE) System.out.println("te.next");
+      current = fstEnum.next();
+      if (current == null) {
+        if (VERBOSE) System.out.println("  END");
+        return null;
+      }
+      didDecode = false;
+      if (VERBOSE) System.out.println("  term=" + field.name + ":" + current.input.utf8ToString());
+      return current.input;
+    }
+
+    @Override
+    public int docFreq() throws IOException {
+      decodeMetaData();
+      return docFreq;
+    }
+
+    @Override
+    public long totalTermFreq() throws IOException {
+      decodeMetaData();
+      return totalTermFreq;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public void seekExact(long ord) {
+      // NOTE: we could add this...
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public long ord() {
+      // NOTE: we could add this...
+      throw new UnsupportedOperationException();
+    }
+  }
+
+  private final static class TermsReader extends Terms {
+
+    private final long sumTotalTermFreq;
+    private final long sumDocFreq;
+    private final int docCount;
+    private final int termCount;
+    private FST<BytesRef> fst;
+    private final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
+    private final FieldInfo field;
+
+    public TermsReader(FieldInfos fieldInfos, IndexInput in, int termCount) throws IOException {
+      this.termCount = termCount;
+      final int fieldNumber = in.readVInt();
+      field = fieldInfos.fieldInfo(fieldNumber);
+      if (field.indexOptions != IndexOptions.DOCS_ONLY) {
+        sumTotalTermFreq = in.readVLong();
+      } else {
+        sumTotalTermFreq = -1;
+      }
+      sumDocFreq = in.readVLong();
+      docCount = in.readVInt();
+      
+      fst = new FST<BytesRef>(in, outputs);
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return sumTotalTermFreq;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return sumDocFreq;
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return docCount;
+    }
+
+    @Override
+    public long getUniqueTermCount() throws IOException {
+      return termCount;
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) {
+      return new FSTTermsEnum(field, fst);
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, EXTENSION);
+    final IndexInput in = state.dir.openInput(fileName, IOContext.READONCE);
+
+    final SortedMap<String,TermsReader> fields = new TreeMap<String,TermsReader>();
+
+    try {
+      while(true) {
+        final int termCount = in.readVInt();
+        if (termCount == 0) {
+          break;
+        }
+        final TermsReader termsReader = new TermsReader(state.fieldInfos, in, termCount);
+        fields.put(termsReader.field.name, termsReader);
+      }
+    } finally {
+      in.close();
+    }
+
+    return new FieldsProducer() {
+      @Override
+      public FieldsEnum iterator() {
+        final Iterator<TermsReader> iter = fields.values().iterator();
+
+        return new FieldsEnum() {
+
+          private TermsReader current;
+
+          @Override
+          public String next() {
+            current = iter.next();
+            return current.field.name;
+          }
+
+          @Override
+          public Terms terms() {
+            return current;
+          }
+        };
+      }
+
+      @Override
+      public Terms terms(String field) {
+        return fields.get(field);
+      }
+      
+      @Override
+      public int getUniqueFieldCount() {
+        return fields.size();
+      }
+
+      @Override
+      public void close() {
+        // Drop ref to FST:
+        for(TermsReader termsReader : fields.values()) {
+          termsReader.fst = null;
+        }
+      }
+    };
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
+    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, EXTENSION));
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/package.html b/lucene/src/java/org/apache/lucene/codecs/package.html
new file mode 100644
index 0000000..78dcb95
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Codecs API: API for customization of the encoding and structure of the index.
+</body>
+</html>
diff --git a/lucene/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java b/lucene/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java
new file mode 100644
index 0000000..07bb07d
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/perfield/PerFieldPostingsFormat.java
@@ -0,0 +1,340 @@
+package org.apache.lucene.codecs.perfield;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.IdentityHashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Set;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Enables per field format support.
+ * 
+ * @lucene.experimental
+ */
+
+public abstract class PerFieldPostingsFormat extends PostingsFormat {
+
+  public static final String PER_FIELD_EXTENSION = "per";
+  public static final String PER_FIELD_NAME = "PerField40";
+
+  public static final int VERSION_START = 0;
+  public static final int VERSION_LATEST = VERSION_START;
+
+  public PerFieldPostingsFormat() {
+    super(PER_FIELD_NAME);
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state)
+      throws IOException {
+    return new FieldsWriter(state);
+  }
+
+  // NOTE: not private to avoid $accessN at runtime!!
+  static class FieldsConsumerAndID implements Closeable {
+    final FieldsConsumer fieldsConsumer;
+    final String segmentSuffix;
+
+    public FieldsConsumerAndID(FieldsConsumer fieldsConsumer, String segmentSuffix) {
+      this.fieldsConsumer = fieldsConsumer;
+      this.segmentSuffix = segmentSuffix;
+    }
+
+    @Override
+    public void close() throws IOException {
+      fieldsConsumer.close();
+    }
+  };
+    
+  private class FieldsWriter extends FieldsConsumer {
+
+    private final Map<PostingsFormat,FieldsConsumerAndID> formats = new IdentityHashMap<PostingsFormat,FieldsConsumerAndID>();
+
+    /** Records all fields we wrote. */
+    private final Map<String,PostingsFormat> fieldToFormat = new HashMap<String,PostingsFormat>();
+
+    private final SegmentWriteState segmentWriteState;
+
+    public FieldsWriter(SegmentWriteState state) throws IOException {
+      segmentWriteState = state;
+    }
+
+    @Override
+    public TermsConsumer addField(FieldInfo field) throws IOException {
+      final PostingsFormat format = getPostingsFormatForField(field.name);
+      if (format == null) {
+        throw new IllegalStateException("invalid null PostingsFormat for field=\"" + field.name + "\"");
+      }
+
+      assert !fieldToFormat.containsKey(field.name);
+      fieldToFormat.put(field.name, format);
+
+      FieldsConsumerAndID consumerAndId = formats.get(format);
+      if (consumerAndId == null) {
+        // First time we are seeing this format; assign
+        // next id and init it:
+        final String segmentSuffix = getFullSegmentSuffix(field.name,
+                                                          segmentWriteState.segmentSuffix,
+                                                          ""+formats.size());
+        consumerAndId = new FieldsConsumerAndID(format.fieldsConsumer(new SegmentWriteState(segmentWriteState, segmentSuffix)),
+                                                segmentSuffix);
+        formats.put(format, consumerAndId);
+      }
+
+      return consumerAndId.fieldsConsumer.addField(field);
+    }
+
+    @Override
+    public void close() throws IOException {
+
+      // Close all subs
+      IOUtils.close(formats.values());
+
+      // Write _X.per: maps field name -> format name and
+      // format name -> format id
+      final String mapFileName = IndexFileNames.segmentFileName(segmentWriteState.segmentName, segmentWriteState.segmentSuffix, PER_FIELD_EXTENSION);
+      final IndexOutput out = segmentWriteState.directory.createOutput(mapFileName, segmentWriteState.context);
+      boolean success = false;
+      try {
+        CodecUtil.writeHeader(out, PER_FIELD_NAME, VERSION_LATEST);
+
+        // format name -> int id
+        out.writeVInt(formats.size());
+        for(Map.Entry<PostingsFormat,FieldsConsumerAndID> ent : formats.entrySet()) {
+          out.writeString(ent.getValue().segmentSuffix);
+          out.writeString(ent.getKey().getName());
+        }
+
+        // field name -> format name
+        out.writeVInt(fieldToFormat.size());
+        for(Map.Entry<String,PostingsFormat> ent : fieldToFormat.entrySet()) {
+          out.writeString(ent.getKey());
+          out.writeString(ent.getValue().getName());
+        }
+
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(out);
+        } else {
+          IOUtils.close(out);
+        }
+      }
+    }
+  }
+
+  static String getFullSegmentSuffix(String fieldName, String outerSegmentSuffix, String segmentSuffix) {
+    if (outerSegmentSuffix.length() == 0) {
+      return segmentSuffix;
+    } else {
+      // TODO: support embedding; I think it should work but
+      // we need a test confirm to confirm
+      // return outerSegmentSuffix + "_" + segmentSuffix;
+      throw new IllegalStateException("cannot embed PerFieldPostingsFormat inside itself (field \"" + fieldName + "\" returned PerFieldPostingsFormat)");
+    }
+  }
+
+  private class FieldsReader extends FieldsProducer {
+
+    private final Map<String,FieldsProducer> fields = new TreeMap<String,FieldsProducer>();
+    private final Map<PostingsFormat,FieldsProducer> formats = new IdentityHashMap<PostingsFormat,FieldsProducer>();
+
+    public FieldsReader(final SegmentReadState readState) throws IOException {
+
+      // Read _X.per and init each format:
+      boolean success = false;
+      try {
+        new VisitPerFieldFile(readState.dir, readState.segmentInfo.name, readState.segmentSuffix) {
+          @Override
+          protected void visitOneFormat(String segmentSuffix, PostingsFormat postingsFormat) throws IOException {
+            formats.put(postingsFormat, postingsFormat.fieldsProducer(new SegmentReadState(readState, segmentSuffix)));
+          }
+
+          @Override
+          protected void visitOneField(String fieldName, PostingsFormat postingsFormat) throws IOException {
+            assert formats.containsKey(postingsFormat);
+            fields.put(fieldName, formats.get(postingsFormat));
+          }
+        };
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(formats.values());
+        }
+      }
+    }
+
+    private final class FieldsIterator extends FieldsEnum {
+      private final Iterator<String> it;
+      private String current;
+
+      public FieldsIterator() {
+        it = fields.keySet().iterator();
+      }
+
+      @Override
+      public String next() throws IOException {
+        if (it.hasNext()) {
+          current = it.next();
+        } else {
+          current = null;
+        }
+
+        return current;
+      }
+
+      @Override
+      public Terms terms() throws IOException {
+        return fields.get(current).terms(current);
+      }
+    }
+
+    @Override
+    public FieldsEnum iterator() throws IOException {
+      return new FieldsIterator();
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      FieldsProducer fieldsProducer = fields.get(field);
+      return fieldsProducer == null ? null : fieldsProducer.terms(field);
+    }
+    
+    @Override
+    public int getUniqueFieldCount() {
+      return fields.size();
+    }
+
+    @Override
+    public void close() throws IOException {
+      IOUtils.close(formats.values());
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state)
+      throws IOException {
+    return new FieldsReader(state);
+  }
+
+  private abstract class VisitPerFieldFile {
+    public VisitPerFieldFile(Directory dir, String segmentName, String outerSegmentSuffix) throws IOException {
+      final String mapFileName = IndexFileNames.segmentFileName(segmentName, outerSegmentSuffix, PER_FIELD_EXTENSION);
+      final IndexInput in = dir.openInput(mapFileName, IOContext.READONCE);
+      boolean success = false;
+      try {
+        CodecUtil.checkHeader(in, PER_FIELD_NAME, VERSION_START, VERSION_LATEST);
+
+        // Read format name -> format id
+        final int formatCount = in.readVInt();
+        for(int formatIDX=0;formatIDX<formatCount;formatIDX++) {
+          final String segmentSuffix = in.readString();
+          final String formatName = in.readString();
+          PostingsFormat postingsFormat = PostingsFormat.forName(formatName);
+          //System.out.println("do lookup " + formatName + " -> " + postingsFormat);
+          if (postingsFormat == null) {
+            throw new IllegalStateException("unable to lookup PostingsFormat for name=\"" + formatName + "\": got null");
+          }
+
+          // Better be defined, because it was defined
+          // during indexing:
+          visitOneFormat(segmentSuffix, postingsFormat);
+        }
+
+        // Read field name -> format name
+        final int fieldCount = in.readVInt();
+        for(int fieldIDX=0;fieldIDX<fieldCount;fieldIDX++) {
+          final String fieldName = in.readString();
+          final String formatName = in.readString();
+          visitOneField(fieldName, PostingsFormat.forName(formatName));
+        }
+
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(in);
+        } else {
+          IOUtils.close(in);
+        }
+      }
+    }
+
+    // This is called first, for all formats:
+    protected abstract void visitOneFormat(String segmentSuffix, PostingsFormat format) throws IOException;
+
+    // ... then this is called, for all fields:
+    protected abstract void visitOneField(String fieldName, PostingsFormat format) throws IOException;
+  }
+
+  @Override
+  public void files(final Directory dir, final SegmentInfo info, String segmentSuffix, final Set<String> files)
+      throws IOException {
+
+    final String mapFileName = IndexFileNames.segmentFileName(info.name, segmentSuffix, PER_FIELD_EXTENSION);
+    files.add(mapFileName);
+
+    try {
+      new VisitPerFieldFile(dir, info.name, segmentSuffix) {
+        @Override
+        protected void visitOneFormat(String segmentSuffix, PostingsFormat format) throws IOException {
+          format.files(dir, info, segmentSuffix, files);
+        }
+
+        @Override
+          protected void visitOneField(String field, PostingsFormat format) {
+        }
+      };
+    } catch (FileNotFoundException fnfe) {
+      // TODO: this is somewhat shady... if we can't open
+      // the .per file then most likely someone is calling
+      // .files() after this segment was deleted, so, they
+      // wouldn't be able to do anything with the files even
+      // if we could return them, so we don't add any files
+      // in this case.
+    }
+  }
+
+  // NOTE: only called during writing; for reading we read
+  // all we need from the index (ie we save the field ->
+  // format mapping)
+  public abstract PostingsFormat getPostingsFormatForField(String field);
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/pulsing/Pulsing40PostingsFormat.java b/lucene/src/java/org/apache/lucene/codecs/pulsing/Pulsing40PostingsFormat.java
new file mode 100644
index 0000000..9c23d87
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/pulsing/Pulsing40PostingsFormat.java
@@ -0,0 +1,42 @@
+package org.apache.lucene.codecs.pulsing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsBaseFormat;
+
+/**
+ * @lucene.experimental
+ */
+public class Pulsing40PostingsFormat extends PulsingPostingsFormat {
+
+  /** Inlines docFreq=1 terms, otherwise uses the normal "Lucene40" format. */
+  public Pulsing40PostingsFormat() {
+    this(1);
+  }
+
+  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene40" format. */
+  public Pulsing40PostingsFormat(int freqCutoff) {
+    this(freqCutoff, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+  }
+
+  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene40" format. */
+  public Pulsing40PostingsFormat(int freqCutoff, int minBlockSize, int maxBlockSize) {
+    super("Pulsing40", new Lucene40PostingsBaseFormat(), freqCutoff, minBlockSize, maxBlockSize);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java b/lucene/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java
new file mode 100644
index 0000000..e0eff4a
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java
@@ -0,0 +1,122 @@
+package org.apache.lucene.codecs.pulsing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.BlockTreeTermsReader;
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsBaseFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.Directory;
+
+/** This postings format "inlines" the postings for terms that have
+ *  low docFreq.  It wraps another postings format, which is used for
+ *  writing the non-inlined terms.
+ *
+ *  @lucene.experimental */
+
+public abstract class PulsingPostingsFormat extends PostingsFormat {
+
+  private final int freqCutoff;
+  private final int minBlockSize;
+  private final int maxBlockSize;
+  private final PostingsBaseFormat wrappedPostingsBaseFormat;
+  
+  public PulsingPostingsFormat(String name, PostingsBaseFormat wrappedPostingsBaseFormat, int freqCutoff) {
+    this(name, wrappedPostingsBaseFormat, freqCutoff, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+  }
+
+  /** Terms with freq <= freqCutoff are inlined into terms
+   *  dict. */
+  public PulsingPostingsFormat(String name, PostingsBaseFormat wrappedPostingsBaseFormat, int freqCutoff, int minBlockSize, int maxBlockSize) {
+    super(name);
+    this.freqCutoff = freqCutoff;
+    this.minBlockSize = minBlockSize;
+    assert minBlockSize > 1;
+    this.maxBlockSize = maxBlockSize;
+    this.wrappedPostingsBaseFormat = wrappedPostingsBaseFormat;
+  }
+
+  @Override
+  public String toString() {
+    return getName() + "(freqCutoff=" + freqCutoff + " minBlockSize=" + minBlockSize + " maxBlockSize=" + maxBlockSize + ")";
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase docsWriter = wrappedPostingsBaseFormat.postingsWriterBase(state);
+
+    // Terms that have <= freqCutoff number of docs are
+    // "pulsed" (inlined):
+    PostingsWriterBase pulsingWriter = new PulsingPostingsWriter(freqCutoff, docsWriter);
+
+    // Terms dict
+    boolean success = false;
+    try {
+      FieldsConsumer ret = new BlockTreeTermsWriter(state, pulsingWriter, minBlockSize, maxBlockSize);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        pulsingWriter.close();
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+
+    PostingsReaderBase docsReader = wrappedPostingsBaseFormat.postingsReaderBase(state);
+    PostingsReaderBase pulsingReader = new PulsingPostingsReader(docsReader);
+
+    boolean success = false;
+    try {
+      FieldsProducer ret = new BlockTreeTermsReader(
+                                                    state.dir, state.fieldInfos, state.segmentInfo.name,
+                                                    pulsingReader,
+                                                    state.context,
+                                                    state.segmentSuffix,
+                                                    state.termsIndexDivisor);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        pulsingReader.close();
+      }
+    }
+  }
+
+  public int getFreqCutoff() {
+    return freqCutoff;
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
+    wrappedPostingsBaseFormat.files(dir, segmentInfo, segmentSuffix, files);
+    BlockTreeTermsReader.files(dir, segmentInfo, segmentSuffix, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java b/lucene/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java
new file mode 100644
index 0000000..dc1c0f7
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java
@@ -0,0 +1,596 @@
+package org.apache.lucene.codecs.pulsing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.IdentityHashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Attribute;
+import org.apache.lucene.util.AttributeImpl;
+import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+
+/** Concrete class that reads the current doc/freq/skip
+ *  postings format 
+ *  @lucene.experimental */
+
+// TODO: -- should we switch "hasProx" higher up?  and
+// create two separate docs readers, one that also reads
+// prox and one that doesn't?
+
+public class PulsingPostingsReader extends PostingsReaderBase {
+
+  // Fallback reader for non-pulsed terms:
+  final PostingsReaderBase wrappedPostingsReader;
+  int maxPositions;
+
+  public PulsingPostingsReader(PostingsReaderBase wrappedPostingsReader) throws IOException {
+    this.wrappedPostingsReader = wrappedPostingsReader;
+  }
+
+  @Override
+  public void init(IndexInput termsIn) throws IOException {
+    CodecUtil.checkHeader(termsIn, PulsingPostingsWriter.CODEC,
+      PulsingPostingsWriter.VERSION_START, PulsingPostingsWriter.VERSION_START);
+    maxPositions = termsIn.readVInt();
+    wrappedPostingsReader.init(termsIn);
+  }
+
+  private static class PulsingTermState extends BlockTermState {
+    private byte[] postings;
+    private int postingsSize;                     // -1 if this term was not inlined
+    private BlockTermState wrappedTermState;
+
+    ByteArrayDataInput inlinedBytesReader;
+    private byte[] inlinedBytes;
+
+    @Override
+    public Object clone() {
+      PulsingTermState clone;
+      clone = (PulsingTermState) super.clone();
+      if (postingsSize != -1) {
+        clone.postings = new byte[postingsSize];
+        System.arraycopy(postings, 0, clone.postings, 0, postingsSize);
+      } else {
+        assert wrappedTermState != null;
+        clone.wrappedTermState = (BlockTermState) wrappedTermState.clone();
+      }
+      return clone;
+    }
+
+    @Override
+    public void copyFrom(TermState _other) {
+      super.copyFrom(_other);
+      PulsingTermState other = (PulsingTermState) _other;
+      postingsSize = other.postingsSize;
+      if (other.postingsSize != -1) {
+        if (postings == null || postings.length < other.postingsSize) {
+          postings = new byte[ArrayUtil.oversize(other.postingsSize, 1)];
+        }
+        System.arraycopy(other.postings, 0, postings, 0, other.postingsSize);
+      } else {
+        wrappedTermState.copyFrom(other.wrappedTermState);
+      }
+
+      // NOTE: we do not copy the
+      // inlinedBytes/inlinedBytesReader; these are only
+      // stored on the "primary" TermState.  They are
+      // "transient" to cloned term states.
+    }
+
+    @Override
+    public String toString() {
+      if (postingsSize == -1) {
+        return "PulsingTermState: not inlined: wrapped=" + wrappedTermState;
+      } else {
+        return "PulsingTermState: inlined size=" + postingsSize + " " + super.toString();
+      }
+    }
+  }
+
+  @Override
+  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
+    //System.out.println("PR.readTermsBlock state=" + _termState);
+    final PulsingTermState termState = (PulsingTermState) _termState;
+    if (termState.inlinedBytes == null) {
+      termState.inlinedBytes = new byte[128];
+      termState.inlinedBytesReader = new ByteArrayDataInput();
+    }
+    int len = termsIn.readVInt();
+    //System.out.println("  len=" + len + " fp=" + termsIn.getFilePointer());
+    if (termState.inlinedBytes.length < len) {
+      termState.inlinedBytes = new byte[ArrayUtil.oversize(len, 1)];
+    }
+    termsIn.readBytes(termState.inlinedBytes, 0, len);
+    termState.inlinedBytesReader.reset(termState.inlinedBytes);
+    termState.wrappedTermState.termBlockOrd = 0;
+    wrappedPostingsReader.readTermsBlock(termsIn, fieldInfo, termState.wrappedTermState);
+  }
+
+  @Override
+  public BlockTermState newTermState() throws IOException {
+    PulsingTermState state = new PulsingTermState();
+    state.wrappedTermState = wrappedPostingsReader.newTermState();
+    return state;
+  }
+
+  @Override
+  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
+    //System.out.println("PR nextTerm");
+    PulsingTermState termState = (PulsingTermState) _termState;
+
+    // if we have positions, its total TF, otherwise its computed based on docFreq.
+    long count = fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS ? termState.totalTermFreq : termState.docFreq;
+    //System.out.println("  count=" + count + " threshold=" + maxPositions);
+
+    if (count <= maxPositions) {
+
+      // Inlined into terms dict -- just read the byte[] blob in,
+      // but don't decode it now (we only decode when a DocsEnum
+      // or D&PEnum is pulled):
+      termState.postingsSize = termState.inlinedBytesReader.readVInt();
+      if (termState.postings == null || termState.postings.length < termState.postingsSize) {
+        termState.postings = new byte[ArrayUtil.oversize(termState.postingsSize, 1)];
+      }
+      // TODO: sort of silly to copy from one big byte[]
+      // (the blob holding all inlined terms' blobs for
+      // current term block) into another byte[] (just the
+      // blob for this term)...
+      termState.inlinedBytesReader.readBytes(termState.postings, 0, termState.postingsSize);
+      //System.out.println("  inlined bytes=" + termState.postingsSize);
+    } else {
+      //System.out.println("  not inlined");
+      termState.postingsSize = -1;
+      // TODO: should we do full copyFrom?  much heavier...?
+      termState.wrappedTermState.docFreq = termState.docFreq;
+      termState.wrappedTermState.totalTermFreq = termState.totalTermFreq;
+      wrappedPostingsReader.nextTerm(fieldInfo, termState.wrappedTermState);
+      termState.wrappedTermState.termBlockOrd++;
+    }
+  }
+
+  @Override
+  public DocsEnum docs(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
+    if (needsFreqs && field.indexOptions == IndexOptions.DOCS_ONLY) {
+      return null;
+    }
+    PulsingTermState termState = (PulsingTermState) _termState;
+    if (termState.postingsSize != -1) {
+      PulsingDocsEnum postings;
+      if (reuse instanceof PulsingDocsEnum) {
+        postings = (PulsingDocsEnum) reuse;
+        if (!postings.canReuse(field)) {
+          postings = new PulsingDocsEnum(field);
+        }
+      } else {
+        // the 'reuse' is actually the wrapped enum
+        PulsingDocsEnum previous = (PulsingDocsEnum) getOther(reuse);
+        if (previous != null && previous.canReuse(field)) {
+          postings = previous;
+        } else {
+          postings = new PulsingDocsEnum(field);
+        }
+      }
+      if (reuse != postings) {
+        setOther(postings, reuse); // postings.other = reuse
+      }
+      return postings.reset(liveDocs, termState);
+    } else {
+      if (reuse instanceof PulsingDocsEnum) {
+        DocsEnum wrapped = wrappedPostingsReader.docs(field, termState.wrappedTermState, liveDocs, getOther(reuse), needsFreqs);
+        setOther(wrapped, reuse); // wrapped.other = reuse
+        return wrapped;
+      } else {
+        return wrappedPostingsReader.docs(field, termState.wrappedTermState, liveDocs, reuse, needsFreqs);
+      }
+    }
+  }
+
+  @Override
+  public DocsAndPositionsEnum docsAndPositions(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
+    if (field.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      return null;
+    }
+    //System.out.println("D&P: field=" + field.name);
+
+    final PulsingTermState termState = (PulsingTermState) _termState;
+
+    if (termState.postingsSize != -1) {
+      PulsingDocsAndPositionsEnum postings;
+      if (reuse instanceof PulsingDocsAndPositionsEnum) {
+        postings = (PulsingDocsAndPositionsEnum) reuse;
+        if (!postings.canReuse(field)) {
+          postings = new PulsingDocsAndPositionsEnum(field);
+        }
+      } else {
+        // the 'reuse' is actually the wrapped enum
+        PulsingDocsAndPositionsEnum previous = (PulsingDocsAndPositionsEnum) getOther(reuse);
+        if (previous != null && previous.canReuse(field)) {
+          postings = previous;
+        } else {
+          postings = new PulsingDocsAndPositionsEnum(field);
+        }
+      }
+      if (reuse != postings) {
+        setOther(postings, reuse); // postings.other = reuse 
+      }
+      return postings.reset(liveDocs, termState);
+    } else {
+      if (reuse instanceof PulsingDocsAndPositionsEnum) {
+        DocsAndPositionsEnum wrapped = wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, liveDocs, (DocsAndPositionsEnum) getOther(reuse));
+        setOther(wrapped, reuse); // wrapped.other = reuse
+        return wrapped;
+      } else {
+        return wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, liveDocs, reuse);
+      }
+    }
+  }
+
+  private static class PulsingDocsEnum extends DocsEnum {
+    private byte[] postingsBytes;
+    private final ByteArrayDataInput postings = new ByteArrayDataInput();
+    private final IndexOptions indexOptions;
+    private final boolean storePayloads;
+    private Bits liveDocs;
+    private int docID = -1;
+    private int accum;
+    private int freq;
+    private int payloadLength;
+
+    public PulsingDocsEnum(FieldInfo fieldInfo) {
+      indexOptions = fieldInfo.indexOptions;
+      storePayloads = fieldInfo.storePayloads;
+    }
+
+    public PulsingDocsEnum reset(Bits liveDocs, PulsingTermState termState) {
+      //System.out.println("PR docsEnum termState=" + termState + " docFreq=" + termState.docFreq);
+      assert termState.postingsSize != -1;
+
+      // Must make a copy of termState's byte[] so that if
+      // app does TermsEnum.next(), this DocsEnum is not affected
+      if (postingsBytes == null) {
+        postingsBytes = new byte[termState.postingsSize];
+      } else if (postingsBytes.length < termState.postingsSize) {
+        postingsBytes = ArrayUtil.grow(postingsBytes, termState.postingsSize);
+      }
+      System.arraycopy(termState.postings, 0, postingsBytes, 0, termState.postingsSize);
+      postings.reset(postingsBytes, 0, termState.postingsSize);
+      docID = -1;
+      accum = 0;
+      payloadLength = 0;
+      this.liveDocs = liveDocs;
+      return this;
+    }
+
+    boolean canReuse(FieldInfo fieldInfo) {
+      return indexOptions == fieldInfo.indexOptions && storePayloads == fieldInfo.storePayloads;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      //System.out.println("PR nextDoc this= "+ this);
+      while(true) {
+        if (postings.eof()) {
+          //System.out.println("PR   END");
+          return docID = NO_MORE_DOCS;
+        }
+
+        final int code = postings.readVInt();
+        //System.out.println("  read code=" + code);
+        if (indexOptions == IndexOptions.DOCS_ONLY) {
+          accum += code;
+        } else {
+          accum += code >>> 1;              // shift off low bit
+          if ((code & 1) != 0) {          // if low bit is set
+            freq = 1;                     // freq is one
+          } else {
+            freq = postings.readVInt();     // else read freq
+          }
+
+          if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+            // Skip positions
+            if (storePayloads) {
+              for(int pos=0;pos<freq;pos++) {
+                final int posCode = postings.readVInt();
+                if ((posCode & 1) != 0) {
+                  payloadLength = postings.readVInt();
+                }
+                if (payloadLength != 0) {
+                  postings.skipBytes(payloadLength);
+                }
+              }
+            } else {
+              for(int pos=0;pos<freq;pos++) {
+                // TODO: skipVInt
+                postings.readVInt();
+              }
+            }
+          }
+        }
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          return (docID = accum);
+        }
+      }
+    }
+
+    @Override
+    public int freq() {
+      assert indexOptions != IndexOptions.DOCS_ONLY;
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      int doc;
+      while((doc=nextDoc()) != NO_MORE_DOCS) {
+        if (doc >= target)
+          return doc;
+      }
+      return docID = NO_MORE_DOCS;
+    }
+  }
+
+  private static class PulsingDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private byte[] postingsBytes;
+    private final ByteArrayDataInput postings = new ByteArrayDataInput();
+    private final boolean storePayloads;
+
+    private Bits liveDocs;
+    private int docID = -1;
+    private int accum;
+    private int freq;
+    private int posPending;
+    private int position;
+    private int payloadLength;
+    private BytesRef payload;
+
+    private boolean payloadRetrieved;
+
+    public PulsingDocsAndPositionsEnum(FieldInfo fieldInfo) {
+      storePayloads = fieldInfo.storePayloads;
+    }
+
+    boolean canReuse(FieldInfo fieldInfo) {
+      return storePayloads == fieldInfo.storePayloads;
+    }
+
+    public PulsingDocsAndPositionsEnum reset(Bits liveDocs, PulsingTermState termState) {
+      assert termState.postingsSize != -1;
+      if (postingsBytes == null) {
+        postingsBytes = new byte[termState.postingsSize];
+      } else if (postingsBytes.length < termState.postingsSize) {
+        postingsBytes = ArrayUtil.grow(postingsBytes, termState.postingsSize);
+      }
+      System.arraycopy(termState.postings, 0, postingsBytes, 0, termState.postingsSize);
+      postings.reset(postingsBytes, 0, termState.postingsSize);
+      this.liveDocs = liveDocs;
+      payloadLength = 0;
+      posPending = 0;
+      docID = -1;
+      accum = 0;
+      //System.out.println("PR d&p reset storesPayloads=" + storePayloads + " bytes=" + bytes.length + " this=" + this);
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      //System.out.println("PR d&p nextDoc this=" + this);
+
+      while(true) {
+        //System.out.println("  cycle skip posPending=" + posPending);
+
+        skipPositions();
+
+        if (postings.eof()) {
+          //System.out.println("PR   END");
+          return docID = NO_MORE_DOCS;
+        }
+
+        final int code = postings.readVInt();
+        accum += code >>> 1;            // shift off low bit
+        if ((code & 1) != 0) {          // if low bit is set
+          freq = 1;                     // freq is one
+        } else {
+          freq = postings.readVInt();     // else read freq
+        }
+        posPending = freq;
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          //System.out.println("  return docID=" + docID + " freq=" + freq);
+          position = 0;
+          return (docID = accum);
+        }
+      }
+    }
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      int doc;
+      while((doc=nextDoc()) != NO_MORE_DOCS) {
+        if (doc >= target) {
+          return docID = doc;
+        }
+      }
+      return docID = NO_MORE_DOCS;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      //System.out.println("PR d&p nextPosition posPending=" + posPending + " vs freq=" + freq);
+      
+      assert posPending > 0;
+      posPending--;
+
+      if (storePayloads) {
+        if (!payloadRetrieved) {
+          //System.out.println("PR     skip payload=" + payloadLength);
+          postings.skipBytes(payloadLength);
+        }
+        final int code = postings.readVInt();
+        //System.out.println("PR     code=" + code);
+        if ((code & 1) != 0) {
+          payloadLength = postings.readVInt();
+          //System.out.println("PR     new payload len=" + payloadLength);
+        }
+        position += code >> 1;
+        payloadRetrieved = false;
+      } else {
+        position += postings.readVInt();
+      }
+
+      //System.out.println("PR d&p nextPos return pos=" + position + " this=" + this);
+      return position;
+    }
+
+    private void skipPositions() throws IOException {
+      while(posPending != 0) {
+        nextPosition();
+      }
+      if (storePayloads && !payloadRetrieved) {
+        //System.out.println("  skip payload len=" + payloadLength);
+        postings.skipBytes(payloadLength);
+        payloadRetrieved = true;
+      }
+    }
+
+    @Override
+    public boolean hasPayload() {
+      return storePayloads && !payloadRetrieved && payloadLength > 0;
+    }
+
+    @Override
+    public BytesRef getPayload() throws IOException {
+      //System.out.println("PR  getPayload payloadLength=" + payloadLength + " this=" + this);
+      if (payloadRetrieved) {
+        throw new IOException("Either no payload exists at this term position or an attempt was made to load it more than once.");
+      }
+      payloadRetrieved = true;
+      if (payloadLength > 0) {
+        if (payload == null) {
+          payload = new BytesRef(payloadLength);
+        } else {
+          payload.grow(payloadLength);
+        }
+        postings.readBytes(payload.bytes, 0, payloadLength);
+        payload.length = payloadLength;
+        return payload;
+      } else {
+        return null;
+      }
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    wrappedPostingsReader.close();
+  }
+  
+  /** for a docsenum, gets the 'other' reused enum.
+   * Example: Pulsing(Standard).
+   * when doing a term range query you are switching back and forth
+   * between Pulsing and Standard
+   * 
+   * The way the reuse works is that Pulsing.other = Standard and
+   * Standard.other = Pulsing.
+   */
+  private DocsEnum getOther(DocsEnum de) {
+    if (de == null) {
+      return null;
+    } else {
+      final AttributeSource atts = de.attributes();
+      return atts.addAttribute(PulsingEnumAttribute.class).enums().get(this);
+    }
+  }
+  
+  /** 
+   * for a docsenum, sets the 'other' reused enum.
+   * see getOther for an example.
+   */
+  private DocsEnum setOther(DocsEnum de, DocsEnum other) {
+    final AttributeSource atts = de.attributes();
+    return atts.addAttribute(PulsingEnumAttribute.class).enums().put(this, other);
+  }
+
+  /** 
+   * A per-docsenum attribute that stores additional reuse information
+   * so that pulsing enums can keep a reference to their wrapped enums,
+   * and vice versa. this way we can always reuse.
+   * 
+   * @lucene.internal */
+  public static interface PulsingEnumAttribute extends Attribute {
+    public Map<PulsingPostingsReader,DocsEnum> enums();
+  }
+    
+  /** @lucene.internal */
+  public static final class PulsingEnumAttributeImpl extends AttributeImpl implements PulsingEnumAttribute {
+    // we could store 'other', but what if someone 'chained' multiple postings readers,
+    // this could cause problems?
+    // TODO: we should consider nuking this map and just making it so if you do this,
+    // you don't reuse? and maybe pulsingPostingsReader should throw an exc if it wraps
+    // another pulsing, because this is just stupid and wasteful. 
+    // we still have to be careful in case someone does Pulsing(Stomping(Pulsing(...
+    private final Map<PulsingPostingsReader,DocsEnum> enums = 
+      new IdentityHashMap<PulsingPostingsReader,DocsEnum>();
+      
+    public Map<PulsingPostingsReader,DocsEnum> enums() {
+      return enums;
+    }
+
+    @Override
+    public void clear() {
+      // our state is per-docsenum, so this makes no sense.
+      // its best not to clear, in case a wrapped enum has a per-doc attribute or something
+      // and is calling clearAttributes(), so they don't nuke the reuse information!
+    }
+
+    @Override
+    public void copyTo(AttributeImpl target) {
+      // this makes no sense for us, because our state is per-docsenum.
+      // we don't want to copy any stuff over to another docsenum ever!
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java b/lucene/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java
new file mode 100644
index 0000000..f4b980a
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java
@@ -0,0 +1,389 @@
+package org.apache.lucene.codecs.pulsing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+import java.util.ArrayList;
+
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+
+// TODO: we now inline based on total TF of the term,
+// but it might be better to inline by "net bytes used"
+// so that a term that has only 1 posting but a huge
+// payload would not be inlined.  Though this is
+// presumably rare in practice...
+
+/** @lucene.experimental */
+public final class PulsingPostingsWriter extends PostingsWriterBase {
+
+  final static String CODEC = "PulsedPostingsWriter";
+
+  // To add a new version, increment from the last one, and
+  // change VERSION_CURRENT to point to your new version:
+  final static int VERSION_START = 0;
+
+  final static int VERSION_CURRENT = VERSION_START;
+
+  private IndexOutput termsOut;
+
+  private IndexOptions indexOptions;
+  private boolean storePayloads;
+
+  private static class PendingTerm {
+    private final byte[] bytes;
+    public PendingTerm(byte[] bytes) {
+      this.bytes = bytes;
+    }
+  }
+
+  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
+
+  // one entry per position
+  private final Position[] pending;
+  private int pendingCount = 0;                           // -1 once we've hit too many positions
+  private Position currentDoc;                    // first Position entry of current doc
+
+  private static final class Position {
+    BytesRef payload;
+    int termFreq;                                 // only incremented on first position for a given doc
+    int pos;
+    int docID;
+  }
+
+  // TODO: -- lazy init this?  ie, if every single term
+  // was inlined (eg for a "primary key" field) then we
+  // never need to use this fallback?  Fallback writer for
+  // non-inlined terms:
+  final PostingsWriterBase wrappedPostingsWriter;
+
+  /** If the total number of positions (summed across all docs
+   *  for this term) is <= maxPositions, then the postings are
+   *  inlined into terms dict */
+  public PulsingPostingsWriter(int maxPositions, PostingsWriterBase wrappedPostingsWriter) throws IOException {
+    pending = new Position[maxPositions];
+    for(int i=0;i<maxPositions;i++) {
+      pending[i] = new Position();
+    }
+
+    // We simply wrap another postings writer, but only call
+    // on it when tot positions is >= the cutoff:
+    this.wrappedPostingsWriter = wrappedPostingsWriter;
+  }
+
+  @Override
+  public void start(IndexOutput termsOut) throws IOException {
+    this.termsOut = termsOut;
+    CodecUtil.writeHeader(termsOut, CODEC, VERSION_CURRENT);
+    termsOut.writeVInt(pending.length); // encode maxPositions in header
+    wrappedPostingsWriter.start(termsOut);
+  }
+
+  @Override
+  public void startTerm() {
+    if (DEBUG) System.out.println("PW   startTerm");
+    assert pendingCount == 0;
+  }
+
+  // TODO: -- should we NOT reuse across fields?  would
+  // be cleaner
+
+  // Currently, this instance is re-used across fields, so
+  // our parent calls setField whenever the field changes
+  @Override
+  public void setField(FieldInfo fieldInfo) {
+    this.indexOptions = fieldInfo.indexOptions;
+    if (DEBUG) System.out.println("PW field=" + fieldInfo.name + " indexOptions=" + indexOptions);
+    storePayloads = fieldInfo.storePayloads;
+    wrappedPostingsWriter.setField(fieldInfo);
+    //DEBUG = BlockTreeTermsWriter.DEBUG;
+  }
+
+  private boolean DEBUG;
+
+  @Override
+  public void startDoc(int docID, int termDocFreq) throws IOException {
+    assert docID >= 0: "got docID=" + docID;
+
+    /*
+    if (termID != -1) {
+      if (docID == 0) {
+        baseDocID = termID;
+      } else if (baseDocID + docID != termID) {
+        throw new RuntimeException("WRITE: baseDocID=" + baseDocID + " docID=" + docID + " termID=" + termID);
+      }
+    }
+    */
+
+    if (DEBUG) System.out.println("PW     doc=" + docID);
+
+    if (pendingCount == pending.length) {
+      push();
+      if (DEBUG) System.out.println("PW: wrapped.finishDoc");
+      wrappedPostingsWriter.finishDoc();
+    }
+
+    if (pendingCount != -1) {
+      assert pendingCount < pending.length;
+      currentDoc = pending[pendingCount];
+      currentDoc.docID = docID;
+      if (indexOptions == IndexOptions.DOCS_ONLY) {
+        pendingCount++;
+      } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) { 
+        pendingCount++;
+        currentDoc.termFreq = termDocFreq;
+      } else {
+        currentDoc.termFreq = termDocFreq;
+      }
+    } else {
+      // We've already seen too many docs for this term --
+      // just forward to our fallback writer
+      wrappedPostingsWriter.startDoc(docID, termDocFreq);
+    }
+  }
+
+  @Override
+  public void addPosition(int position, BytesRef payload) throws IOException {
+
+    if (DEBUG) System.out.println("PW       pos=" + position + " payload=" + (payload == null ? "null" : payload.length + " bytes"));
+    if (pendingCount == pending.length) {
+      push();
+    }
+
+    if (pendingCount == -1) {
+      // We've already seen too many docs for this term --
+      // just forward to our fallback writer
+      wrappedPostingsWriter.addPosition(position, payload);
+    } else {
+      // buffer up
+      final Position pos = pending[pendingCount++];
+      pos.pos = position;
+      pos.docID = currentDoc.docID;
+      if (payload != null && payload.length > 0) {
+        if (pos.payload == null) {
+          pos.payload = BytesRef.deepCopyOf(payload);
+        } else {
+          pos.payload.copyBytes(payload);
+        }
+      } else if (pos.payload != null) {
+        pos.payload.length = 0;
+      }
+    }
+  }
+
+  @Override
+  public void finishDoc() throws IOException {
+    if (DEBUG) System.out.println("PW     finishDoc");
+    if (pendingCount == -1) {
+      wrappedPostingsWriter.finishDoc();
+    }
+  }
+
+  private final RAMOutputStream buffer = new RAMOutputStream();
+
+  // private int baseDocID;
+
+  /** Called when we are done adding docs to this term */
+  @Override
+  public void finishTerm(TermStats stats) throws IOException {
+    if (DEBUG) System.out.println("PW   finishTerm docCount=" + stats.docFreq + " pendingCount=" + pendingCount + " pendingTerms.size()=" + pendingTerms.size());
+
+    assert pendingCount > 0 || pendingCount == -1;
+
+    if (pendingCount == -1) {
+      wrappedPostingsWriter.finishTerm(stats);
+      // Must add null entry to record terms that our
+      // wrapped postings impl added
+      pendingTerms.add(null);
+    } else {
+
+      // There were few enough total occurrences for this
+      // term, so we fully inline our postings data into
+      // terms dict, now:
+
+      // TODO: it'd be better to share this encoding logic
+      // in some inner codec that knows how to write a
+      // single doc / single position, etc.  This way if a
+      // given codec wants to store other interesting
+      // stuff, it could use this pulsing codec to do so
+
+      if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+        int lastDocID = 0;
+        int pendingIDX = 0;
+        int lastPayloadLength = -1;
+        while(pendingIDX < pendingCount) {
+          final Position doc = pending[pendingIDX];
+
+          final int delta = doc.docID - lastDocID;
+          lastDocID = doc.docID;
+
+          if (DEBUG) System.out.println("  write doc=" + doc.docID + " freq=" + doc.termFreq);
+
+          if (doc.termFreq == 1) {
+            buffer.writeVInt((delta<<1)|1);
+          } else {
+            buffer.writeVInt(delta<<1);
+            buffer.writeVInt(doc.termFreq);
+          }
+
+          int lastPos = 0;
+          for(int posIDX=0;posIDX<doc.termFreq;posIDX++) {
+            final Position pos = pending[pendingIDX++];
+            assert pos.docID == doc.docID;
+            final int posDelta = pos.pos - lastPos;
+            lastPos = pos.pos;
+            if (DEBUG) System.out.println("    write pos=" + pos.pos);
+            if (storePayloads) {
+              final int payloadLength = pos.payload == null ? 0 : pos.payload.length;
+              if (payloadLength != lastPayloadLength) {
+                buffer.writeVInt((posDelta << 1)|1);
+                buffer.writeVInt(payloadLength);
+                lastPayloadLength = payloadLength;
+              } else {
+                buffer.writeVInt(posDelta << 1);
+              }
+              if (payloadLength > 0) {
+                buffer.writeBytes(pos.payload.bytes, 0, pos.payload.length);
+              }
+            } else {
+              buffer.writeVInt(posDelta);
+            }
+          }
+        }
+      } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
+        int lastDocID = 0;
+        for(int posIDX=0;posIDX<pendingCount;posIDX++) {
+          final Position doc = pending[posIDX];
+          final int delta = doc.docID - lastDocID;
+          assert doc.termFreq != 0;
+          if (doc.termFreq == 1) {
+            buffer.writeVInt((delta<<1)|1);
+          } else {
+            buffer.writeVInt(delta<<1);
+            buffer.writeVInt(doc.termFreq);
+          }
+          lastDocID = doc.docID;
+        }
+      } else if (indexOptions == IndexOptions.DOCS_ONLY) {
+        int lastDocID = 0;
+        for(int posIDX=0;posIDX<pendingCount;posIDX++) {
+          final Position doc = pending[posIDX];
+          buffer.writeVInt(doc.docID - lastDocID);
+          lastDocID = doc.docID;
+        }
+      }
+
+      final byte[] bytes = new byte[(int) buffer.getFilePointer()];
+      buffer.writeTo(bytes, 0);
+      pendingTerms.add(new PendingTerm(bytes));
+      buffer.reset();
+    }
+
+    pendingCount = 0;
+  }
+
+  @Override
+  public void close() throws IOException {
+    wrappedPostingsWriter.close();
+  }
+
+  @Override
+  public void flushTermsBlock(int start, int count) throws IOException {
+    if (DEBUG) System.out.println("PW: flushTermsBlock start=" + start + " count=" + count + " pendingTerms.size()=" + pendingTerms.size());
+    int wrappedCount = 0;
+    assert buffer.getFilePointer() == 0;
+    assert start >= count;
+
+    final int limit = pendingTerms.size() - start + count;
+
+    for(int idx=pendingTerms.size()-start; idx<limit; idx++) {
+      final PendingTerm term = pendingTerms.get(idx);
+      if (term == null) {
+        wrappedCount++;
+      } else {
+        buffer.writeVInt(term.bytes.length);
+        buffer.writeBytes(term.bytes, 0, term.bytes.length);
+      }
+    }
+
+    termsOut.writeVInt((int) buffer.getFilePointer());
+    buffer.writeTo(termsOut);
+    buffer.reset();
+
+    // TDOO: this could be somewhat costly since
+    // pendingTerms.size() could be biggish?
+    int futureWrappedCount = 0;
+    final int limit2 = pendingTerms.size();
+    for(int idx=limit;idx<limit2;idx++) {
+      if (pendingTerms.get(idx) == null) {
+        futureWrappedCount++;
+      }
+    }
+
+    // Remove the terms we just wrote:
+    pendingTerms.subList(pendingTerms.size()-start, limit).clear();
+
+    if (DEBUG) System.out.println("PW:   len=" + buffer.getFilePointer() + " fp=" + termsOut.getFilePointer() + " futureWrappedCount=" + futureWrappedCount + " wrappedCount=" + wrappedCount);
+    // TODO: can we avoid calling this if all terms
+    // were inlined...?  Eg for a "primary key" field, the
+    // wrapped codec is never invoked...
+    wrappedPostingsWriter.flushTermsBlock(futureWrappedCount+wrappedCount, wrappedCount);
+  }
+
+  // Pushes pending positions to the wrapped codec
+  private void push() throws IOException {
+    if (DEBUG) System.out.println("PW now push @ " + pendingCount + " wrapped=" + wrappedPostingsWriter);
+    assert pendingCount == pending.length;
+      
+    wrappedPostingsWriter.startTerm();
+      
+    // Flush all buffered docs
+    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      Position doc = null;
+      for(Position pos : pending) {
+        if (doc == null) {
+          doc = pos;
+          if (DEBUG) System.out.println("PW: wrapped.startDoc docID=" + doc.docID + " tf=" + doc.termFreq);
+          wrappedPostingsWriter.startDoc(doc.docID, doc.termFreq);
+        } else if (doc.docID != pos.docID) {
+          assert pos.docID > doc.docID;
+          if (DEBUG) System.out.println("PW: wrapped.finishDoc");
+          wrappedPostingsWriter.finishDoc();
+          doc = pos;
+          if (DEBUG) System.out.println("PW: wrapped.startDoc docID=" + doc.docID + " tf=" + doc.termFreq);
+          wrappedPostingsWriter.startDoc(doc.docID, doc.termFreq);
+        }
+        if (DEBUG) System.out.println("PW:   wrapped.addPos pos=" + pos.pos);
+        wrappedPostingsWriter.addPosition(pos.pos, pos.payload);
+      }
+      //wrappedPostingsWriter.finishDoc();
+    } else {
+      for(Position doc : pending) {
+        wrappedPostingsWriter.startDoc(doc.docID, indexOptions == IndexOptions.DOCS_ONLY ? 0 : doc.termFreq);
+      }
+    }
+    pendingCount = -1;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/pulsing/package.html b/lucene/src/java/org/apache/lucene/codecs/pulsing/package.html
new file mode 100644
index 0000000..4216cc6
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/pulsing/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Pulsing Codec: inlines low frequency terms' postings into terms dictionary.
+</body>
+</html>
diff --git a/lucene/src/java/org/apache/lucene/codecs/sep/IntIndexInput.java b/lucene/src/java/org/apache/lucene/codecs/sep/IntIndexInput.java
new file mode 100644
index 0000000..6caa1cc
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/sep/IntIndexInput.java
@@ -0,0 +1,76 @@
+package org.apache.lucene.codecs.sep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.util.IntsRef;
+
+/** Defines basic API for writing ints to an IndexOutput.
+ *  IntBlockCodec interacts with this API. @see
+ *  IntBlockReader
+ *
+ * @lucene.experimental */
+public abstract class IntIndexInput implements Closeable {
+
+  public abstract Reader reader() throws IOException;
+
+  public abstract void close() throws IOException;
+
+  public abstract Index index() throws IOException;
+  
+  // TODO: -- can we simplify this?
+  public abstract static class Index {
+
+    public abstract void read(DataInput indexIn, boolean absolute) throws IOException;
+
+    /** Seeks primary stream to the last read offset */
+    public abstract void seek(IntIndexInput.Reader stream) throws IOException;
+
+    public abstract void set(Index other);
+    
+    @Override
+    public abstract Object clone();
+  }
+
+  public abstract static class Reader {
+
+    /** Reads next single int */
+    public abstract int next() throws IOException;
+
+    /** Reads next chunk of ints */
+    private IntsRef bulkResult;
+
+    /** Read up to count ints. */
+    public IntsRef read(int count) throws IOException {
+      if (bulkResult == null) {
+        bulkResult = new IntsRef();
+        bulkResult.ints = new int[count];
+      } else {
+        bulkResult.grow(count);
+      }
+      for(int i=0;i<count;i++) {
+        bulkResult.ints[i] = next();
+      }
+      bulkResult.length = count;
+      return bulkResult;
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/sep/IntIndexOutput.java b/lucene/src/java/org/apache/lucene/codecs/sep/IntIndexOutput.java
new file mode 100644
index 0000000..14723d2
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/sep/IntIndexOutput.java
@@ -0,0 +1,59 @@
+package org.apache.lucene.codecs.sep;
+
+/**
+ * LICENSED to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// TODO: we may want tighter integration w/ IndexOutput --
+// may give better perf:
+
+import org.apache.lucene.store.IndexOutput;
+
+import java.io.IOException;
+import java.io.Closeable;
+
+/** Defines basic API for writing ints to an IndexOutput.
+ *  IntBlockCodec interacts with this API. @see
+ *  IntBlockReader.
+ *
+ * <p>NOTE: block sizes could be variable
+ *
+ * @lucene.experimental */
+public abstract class IntIndexOutput implements Closeable {
+
+  /** Write an int to the primary file.  The value must be
+   * >= 0.  */
+  public abstract void write(int v) throws IOException;
+
+  public abstract static class Index {
+
+    /** Internally records the current location */
+    public abstract void mark() throws IOException;
+
+    /** Copies index from other */
+    public abstract void copyFrom(Index other, boolean copyLast) throws IOException;
+
+    /** Writes "location" of current output pointer of primary
+     *  output to different output (out) */
+    public abstract void write(IndexOutput indexOut, boolean absolute) throws IOException;
+  }
+
+  /** If you are indexing the primary output file, call
+   *  this and interact with the returned IndexWriter. */
+  public abstract Index index() throws IOException;
+
+  public abstract void close() throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/sep/IntStreamFactory.java b/lucene/src/java/org/apache/lucene/codecs/sep/IntStreamFactory.java
new file mode 100644
index 0000000..a880254
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/sep/IntStreamFactory.java
@@ -0,0 +1,29 @@
+package org.apache.lucene.codecs.sep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+import java.io.IOException;
+
+/** @lucene.experimental */
+public abstract class IntStreamFactory {
+  public abstract IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException;
+  public abstract IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException;
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java b/lucene/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java
new file mode 100644
index 0000000..45f05b4
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.codecs.sep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.DocValuesWriterBase;
+import org.apache.lucene.codecs.lucene40.values.Writer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+
+/**
+ * Implementation of PerDocConsumer that uses separate files.
+ * @lucene.experimental
+ */
+public class SepDocValuesConsumer extends DocValuesWriterBase {
+  private final Directory directory;
+  
+  public SepDocValuesConsumer(PerDocWriteState state) throws IOException {
+    super(state);
+    this.directory = state.directory;
+  }
+  
+  @Override
+  protected Directory getDirectory() {
+    return directory;
+  }
+
+  @SuppressWarnings("fallthrough")
+  public static void files(Directory dir, SegmentInfo segmentInfo,
+      Set<String> files) throws IOException {
+    FieldInfos fieldInfos = segmentInfo.getFieldInfos();
+    for (FieldInfo fieldInfo : fieldInfos) {
+      if (fieldInfo.hasDocValues()) {
+        String filename = docValuesId(segmentInfo.name, fieldInfo.number);
+        switch (fieldInfo.getDocValuesType()) {
+          case BYTES_FIXED_DEREF:
+          case BYTES_VAR_DEREF:
+          case BYTES_VAR_STRAIGHT:
+          case BYTES_FIXED_SORTED:
+          case BYTES_VAR_SORTED:
+            files.add(IndexFileNames.segmentFileName(filename, "",
+                Writer.INDEX_EXTENSION));
+            assert dir.fileExists(IndexFileNames.segmentFileName(filename, "",
+                Writer.INDEX_EXTENSION));
+            // until here all types use an index
+          case BYTES_FIXED_STRAIGHT:
+          case FLOAT_32:
+          case FLOAT_64:
+          case VAR_INTS:
+          case FIXED_INTS_16:
+          case FIXED_INTS_32:
+          case FIXED_INTS_64:
+          case FIXED_INTS_8:
+            files.add(IndexFileNames.segmentFileName(filename, "",
+                Writer.DATA_EXTENSION));
+            assert dir.fileExists(IndexFileNames.segmentFileName(filename, "",
+                Writer.DATA_EXTENSION));
+            break;
+          default:
+            assert false;
+        }
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java b/lucene/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java
new file mode 100644
index 0000000..c752825
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java
@@ -0,0 +1,54 @@
+package org.apache.lucene.codecs.sep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.DocValuesReaderBase;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Implementation of PerDocProducer that uses separate files.
+ * @lucene.experimental
+ */
+public class SepDocValuesProducer extends DocValuesReaderBase {
+  private final TreeMap<String, DocValues> docValues;
+
+  /**
+   * Creates a new {@link SepDocValuesProducer} instance and loads all
+   * {@link DocValues} instances for this segment and codec.
+   */
+  public SepDocValuesProducer(SegmentReadState state) throws IOException {
+    docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.docCount, state.dir, state.context);
+  }
+  
+  @Override
+  protected Map<String,DocValues> docValues() {
+    return docValues;
+  }
+  
+  @Override
+  protected void closeInternal(Collection<? extends Closeable> closeables) throws IOException {
+    IOUtils.close(closeables);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java b/lucene/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java
new file mode 100644
index 0000000..3a8f6fe
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java
@@ -0,0 +1,749 @@
+package org.apache.lucene.codecs.sep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+
+/** Concrete class that reads the current doc/freq/skip
+ *  postings format.    
+ *
+ * @lucene.experimental
+ */
+
+// TODO: -- should we switch "hasProx" higher up?  and
+// create two separate docs readers, one that also reads
+// prox and one that doesn't?
+
+public class SepPostingsReader extends PostingsReaderBase {
+
+  final IntIndexInput freqIn;
+  final IntIndexInput docIn;
+  final IntIndexInput posIn;
+  final IndexInput payloadIn;
+  final IndexInput skipIn;
+
+  int skipInterval;
+  int maxSkipLevels;
+  int skipMinimum;
+
+  public SepPostingsReader(Directory dir, SegmentInfo segmentInfo, IOContext context, IntStreamFactory intFactory, String segmentSuffix) throws IOException {
+    boolean success = false;
+    try {
+
+      final String docFileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.DOC_EXTENSION);
+      docIn = intFactory.openInput(dir, docFileName, context);
+
+      skipIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.SKIP_EXTENSION), context);
+
+      if (segmentInfo.getFieldInfos().hasFreq()) {
+        freqIn = intFactory.openInput(dir, IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.FREQ_EXTENSION), context);        
+      } else {
+        freqIn = null;
+      }
+      if (segmentInfo.getHasProx()) {
+        posIn = intFactory.openInput(dir, IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.POS_EXTENSION), context);
+        payloadIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.PAYLOAD_EXTENSION), context);
+      } else {
+        posIn = null;
+        payloadIn = null;
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        close();
+      }
+    }
+  }
+
+  public static void files(SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) throws IOException {
+    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.DOC_EXTENSION));
+    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.SKIP_EXTENSION));
+
+    if (segmentInfo.getFieldInfos().hasFreq()) {
+      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.FREQ_EXTENSION));
+    }
+
+    if (segmentInfo.getHasProx()) {
+      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.POS_EXTENSION));
+      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.PAYLOAD_EXTENSION));
+    }
+  }
+
+  @Override
+  public void init(IndexInput termsIn) throws IOException {
+    // Make sure we are talking to the matching past writer
+    CodecUtil.checkHeader(termsIn, SepPostingsWriter.CODEC,
+      SepPostingsWriter.VERSION_START, SepPostingsWriter.VERSION_START);
+    skipInterval = termsIn.readInt();
+    maxSkipLevels = termsIn.readInt();
+    skipMinimum = termsIn.readInt();
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      if (freqIn != null)
+        freqIn.close();
+    } finally {
+      try {
+        if (docIn != null)
+          docIn.close();
+      } finally {
+        try {
+          if (skipIn != null)
+            skipIn.close();
+        } finally {
+          try {
+            if (posIn != null) {
+              posIn.close();
+            }
+          } finally {
+            if (payloadIn != null) {
+              payloadIn.close();
+            }
+          }
+        }
+      }
+    }
+  }
+
+  private static final class SepTermState extends BlockTermState {
+    // We store only the seek point to the docs file because
+    // the rest of the info (freqIndex, posIndex, etc.) is
+    // stored in the docs file:
+    IntIndexInput.Index docIndex;
+    IntIndexInput.Index posIndex;
+    IntIndexInput.Index freqIndex;
+    long payloadFP;
+    long skipFP;
+
+    // Only used for "primary" term state; these are never
+    // copied on clone:
+    
+    // TODO: these should somehow be stored per-TermsEnum
+    // not per TermState; maybe somehow the terms dict
+    // should load/manage the byte[]/DataReader for us?
+    byte[] bytes;
+    ByteArrayDataInput bytesReader;
+
+    @Override
+    public Object clone() {
+      SepTermState other = new SepTermState();
+      other.copyFrom(this);
+      return other;
+    }
+
+    @Override
+    public void copyFrom(TermState _other) {
+      super.copyFrom(_other);
+      SepTermState other = (SepTermState) _other;
+      if (docIndex == null) {
+        docIndex = (IntIndexInput.Index) other.docIndex.clone();
+      } else {
+        docIndex.set(other.docIndex);
+      }
+      if (other.freqIndex != null) {
+        if (freqIndex == null) {
+          freqIndex = (IntIndexInput.Index) other.freqIndex.clone();
+        } else {
+          freqIndex.set(other.freqIndex);
+        }
+      } else {
+        freqIndex = null;
+      }
+      if (other.posIndex != null) {
+        if (posIndex == null) {
+          posIndex = (IntIndexInput.Index) other.posIndex.clone();
+        } else {
+          posIndex.set(other.posIndex);
+        }
+      } else {
+        posIndex = null;
+      }
+      payloadFP = other.payloadFP;
+      skipFP = other.skipFP;
+    }
+
+    @Override
+    public String toString() {
+      return super.toString() + " docIndex=" + docIndex + " freqIndex=" + freqIndex + " posIndex=" + posIndex + " payloadFP=" + payloadFP + " skipFP=" + skipFP;
+    }
+  }
+
+  @Override
+  public BlockTermState newTermState() throws IOException {
+    final SepTermState state = new SepTermState();
+    state.docIndex = docIn.index();
+    if (freqIn != null) {
+      state.freqIndex = freqIn.index();
+    }
+    if (posIn != null) {
+      state.posIndex = posIn.index();
+    }
+    return state;
+  }
+
+  @Override
+  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
+    final SepTermState termState = (SepTermState) _termState;
+    //System.out.println("SEPR: readTermsBlock termsIn.fp=" + termsIn.getFilePointer());
+    final int len = termsIn.readVInt();
+    //System.out.println("  numBytes=" + len);
+    if (termState.bytes == null) {
+      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
+      termState.bytesReader = new ByteArrayDataInput(termState.bytes);
+    } else if (termState.bytes.length < len) {
+      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
+    }
+    termState.bytesReader.reset(termState.bytes, 0, len);
+    termsIn.readBytes(termState.bytes, 0, len);
+  }
+
+  @Override
+  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
+    final SepTermState termState = (SepTermState) _termState;
+    final boolean isFirstTerm = termState.termBlockOrd == 0;
+    //System.out.println("SEPR.nextTerm termCount=" + termState.termBlockOrd + " isFirstTerm=" + isFirstTerm + " bytesReader.pos=" + termState.bytesReader.getPosition());
+    //System.out.println("  docFreq=" + termState.docFreq);
+    termState.docIndex.read(termState.bytesReader, isFirstTerm);
+    //System.out.println("  docIndex=" + termState.docIndex);
+    if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
+      termState.freqIndex.read(termState.bytesReader, isFirstTerm);
+      if (fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+        //System.out.println("  freqIndex=" + termState.freqIndex);
+        termState.posIndex.read(termState.bytesReader, isFirstTerm);
+        //System.out.println("  posIndex=" + termState.posIndex);
+        if (fieldInfo.storePayloads) {
+          if (isFirstTerm) {
+            termState.payloadFP = termState.bytesReader.readVLong();
+          } else {
+            termState.payloadFP += termState.bytesReader.readVLong();
+          }
+          //System.out.println("  payloadFP=" + termState.payloadFP);
+        }
+      }
+    }
+
+    if (termState.docFreq >= skipMinimum) {
+      //System.out.println("   readSkip @ " + termState.bytesReader.getPosition());
+      if (isFirstTerm) {
+        termState.skipFP = termState.bytesReader.readVLong();
+      } else {
+        termState.skipFP += termState.bytesReader.readVLong();
+      }
+      //System.out.println("  skipFP=" + termState.skipFP);
+    } else if (isFirstTerm) {
+      termState.skipFP = 0;
+    }
+  }
+
+  @Override
+  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState _termState, Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
+    if (needsFreqs && fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
+      return null;
+    }
+    final SepTermState termState = (SepTermState) _termState;
+    SepDocsEnum docsEnum;
+    if (reuse == null || !(reuse instanceof SepDocsEnum)) {
+      docsEnum = new SepDocsEnum();
+    } else {
+      docsEnum = (SepDocsEnum) reuse;
+      if (docsEnum.startDocIn != docIn) {
+        // If you are using ParellelReader, and pass in a
+        // reused DocsAndPositionsEnum, it could have come
+        // from another reader also using sep codec
+        docsEnum = new SepDocsEnum();        
+      }
+    }
+
+    return docsEnum.init(fieldInfo, termState, liveDocs);
+  }
+
+  @Override
+  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState _termState, Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
+    assert fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+    final SepTermState termState = (SepTermState) _termState;
+    SepDocsAndPositionsEnum postingsEnum;
+    if (reuse == null || !(reuse instanceof SepDocsAndPositionsEnum)) {
+      postingsEnum = new SepDocsAndPositionsEnum();
+    } else {
+      postingsEnum = (SepDocsAndPositionsEnum) reuse;
+      if (postingsEnum.startDocIn != docIn) {
+        // If you are using ParellelReader, and pass in a
+        // reused DocsAndPositionsEnum, it could have come
+        // from another reader also using sep codec
+        postingsEnum = new SepDocsAndPositionsEnum();        
+      }
+    }
+
+    return postingsEnum.init(fieldInfo, termState, liveDocs);
+  }
+
+  class SepDocsEnum extends DocsEnum {
+    int docFreq;
+    int doc = -1;
+    int accum;
+    int count;
+    int freq;
+    long freqStart;
+
+    // TODO: -- should we do omitTF with 2 different enum classes?
+    private boolean omitTF;
+    private IndexOptions indexOptions;
+    private boolean storePayloads;
+    private Bits liveDocs;
+    private final IntIndexInput.Reader docReader;
+    private final IntIndexInput.Reader freqReader;
+    private long skipFP;
+
+    private final IntIndexInput.Index docIndex;
+    private final IntIndexInput.Index freqIndex;
+    private final IntIndexInput.Index posIndex;
+    private final IntIndexInput startDocIn;
+
+    // TODO: -- should we do hasProx with 2 different enum classes?
+
+    boolean skipped;
+    SepSkipListReader skipper;
+
+    SepDocsEnum() throws IOException {
+      startDocIn = docIn;
+      docReader = docIn.reader();
+      docIndex = docIn.index();
+      if (freqIn != null) {
+        freqReader = freqIn.reader();
+        freqIndex = freqIn.index();
+      } else {
+        freqReader = null;
+        freqIndex = null;
+      }
+      if (posIn != null) {
+        posIndex = posIn.index();                 // only init this so skipper can read it
+      } else {
+        posIndex = null;
+      }
+    }
+
+    SepDocsEnum init(FieldInfo fieldInfo, SepTermState termState, Bits liveDocs) throws IOException {
+      this.liveDocs = liveDocs;
+      this.indexOptions = fieldInfo.indexOptions;
+      omitTF = indexOptions == IndexOptions.DOCS_ONLY;
+      storePayloads = fieldInfo.storePayloads;
+
+      // TODO: can't we only do this if consumer
+      // skipped consuming the previous docs?
+      docIndex.set(termState.docIndex);
+      docIndex.seek(docReader);
+
+      if (!omitTF) {
+        freqIndex.set(termState.freqIndex);
+        freqIndex.seek(freqReader);
+      }
+
+      docFreq = termState.docFreq;
+      // NOTE: unused if docFreq < skipMinimum:
+      skipFP = termState.skipFP;
+      count = 0;
+      doc = -1;
+      accum = 0;
+      skipped = false;
+
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+
+      while(true) {
+        if (count == docFreq) {
+          return doc = NO_MORE_DOCS;
+        }
+
+        count++;
+
+        // Decode next doc
+        //System.out.println("decode docDelta:");
+        accum += docReader.next();
+          
+        if (!omitTF) {
+          //System.out.println("decode freq:");
+          freq = freqReader.next();
+        }
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          break;
+        }
+      }
+      return (doc = accum);
+    }
+
+    @Override
+    public int freq() {
+      assert !omitTF;
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+
+      if ((target - skipInterval) >= doc && docFreq >= skipMinimum) {
+
+        // There are enough docs in the posting to have
+        // skip data, and its not too close
+
+        if (skipper == null) {
+          // This DocsEnum has never done any skipping
+          skipper = new SepSkipListReader((IndexInput) skipIn.clone(),
+                                          freqIn,
+                                          docIn,
+                                          posIn,
+                                          maxSkipLevels, skipInterval);
+
+        }
+
+        if (!skipped) {
+          // We haven't yet skipped for this posting
+          skipper.init(skipFP,
+                       docIndex,
+                       freqIndex,
+                       posIndex,
+                       0,
+                       docFreq,
+                       storePayloads);
+          skipper.setIndexOptions(indexOptions);
+
+          skipped = true;
+        }
+
+        final int newCount = skipper.skipTo(target); 
+
+        if (newCount > count) {
+
+          // Skipper did move
+          if (!omitTF) {
+            skipper.getFreqIndex().seek(freqReader);
+          }
+          skipper.getDocIndex().seek(docReader);
+          count = newCount;
+          doc = accum = skipper.getDoc();
+        }
+      }
+        
+      // Now, linear scan for the rest:
+      do {
+        if (nextDoc() == NO_MORE_DOCS) {
+          return NO_MORE_DOCS;
+        }
+      } while (target > doc);
+
+      return doc;
+    }
+  }
+
+  class SepDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    int docFreq;
+    int doc = -1;
+    int accum;
+    int count;
+    int freq;
+    long freqStart;
+
+    private boolean storePayloads;
+    private Bits liveDocs;
+    private final IntIndexInput.Reader docReader;
+    private final IntIndexInput.Reader freqReader;
+    private final IntIndexInput.Reader posReader;
+    private final IndexInput payloadIn;
+    private long skipFP;
+
+    private final IntIndexInput.Index docIndex;
+    private final IntIndexInput.Index freqIndex;
+    private final IntIndexInput.Index posIndex;
+    private final IntIndexInput startDocIn;
+
+    private long payloadFP;
+
+    private int pendingPosCount;
+    private int position;
+    private int payloadLength;
+    private long pendingPayloadBytes;
+
+    private boolean skipped;
+    private SepSkipListReader skipper;
+    private boolean payloadPending;
+    private boolean posSeekPending;
+
+    SepDocsAndPositionsEnum() throws IOException {
+      startDocIn = docIn;
+      docReader = docIn.reader();
+      docIndex = docIn.index();
+      freqReader = freqIn.reader();
+      freqIndex = freqIn.index();
+      posReader = posIn.reader();
+      posIndex = posIn.index();
+      payloadIn = (IndexInput) SepPostingsReader.this.payloadIn.clone();
+    }
+
+    SepDocsAndPositionsEnum init(FieldInfo fieldInfo, SepTermState termState, Bits liveDocs) throws IOException {
+      this.liveDocs = liveDocs;
+      storePayloads = fieldInfo.storePayloads;
+      //System.out.println("Sep D&P init");
+
+      // TODO: can't we only do this if consumer
+      // skipped consuming the previous docs?
+      docIndex.set(termState.docIndex);
+      docIndex.seek(docReader);
+      //System.out.println("  docIndex=" + docIndex);
+
+      freqIndex.set(termState.freqIndex);
+      freqIndex.seek(freqReader);
+      //System.out.println("  freqIndex=" + freqIndex);
+
+      posIndex.set(termState.posIndex);
+      //System.out.println("  posIndex=" + posIndex);
+      posSeekPending = true;
+      payloadPending = false;
+
+      payloadFP = termState.payloadFP;
+      skipFP = termState.skipFP;
+      //System.out.println("  skipFP=" + skipFP);
+
+      docFreq = termState.docFreq;
+      count = 0;
+      doc = -1;
+      accum = 0;
+      pendingPosCount = 0;
+      pendingPayloadBytes = 0;
+      skipped = false;
+
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+
+      while(true) {
+        if (count == docFreq) {
+          return doc = NO_MORE_DOCS;
+        }
+
+        count++;
+
+        // TODO: maybe we should do the 1-bit trick for encoding
+        // freq=1 case?
+
+        // Decode next doc
+        //System.out.println("  sep d&p read doc");
+        accum += docReader.next();
+
+        //System.out.println("  sep d&p read freq");
+        freq = freqReader.next();
+
+        pendingPosCount += freq;
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          break;
+        }
+      }
+
+      position = 0;
+      return (doc = accum);
+    }
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      //System.out.println("SepD&P advance target=" + target + " vs current=" + doc + " this=" + this);
+
+      if ((target - skipInterval) >= doc && docFreq >= skipMinimum) {
+
+        // There are enough docs in the posting to have
+        // skip data, and its not too close
+
+        if (skipper == null) {
+          //System.out.println("  create skipper");
+          // This DocsEnum has never done any skipping
+          skipper = new SepSkipListReader((IndexInput) skipIn.clone(),
+                                          freqIn,
+                                          docIn,
+                                          posIn,
+                                          maxSkipLevels, skipInterval);
+        }
+
+        if (!skipped) {
+          //System.out.println("  init skip data skipFP=" + skipFP);
+          // We haven't yet skipped for this posting
+          skipper.init(skipFP,
+                       docIndex,
+                       freqIndex,
+                       posIndex,
+                       payloadFP,
+                       docFreq,
+                       storePayloads);
+          skipper.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
+          skipped = true;
+        }
+        final int newCount = skipper.skipTo(target); 
+        //System.out.println("  skip newCount=" + newCount + " vs " + count);
+
+        if (newCount > count) {
+
+          // Skipper did move
+          skipper.getFreqIndex().seek(freqReader);
+          skipper.getDocIndex().seek(docReader);
+          //System.out.println("  doc seek'd to " + skipper.getDocIndex());
+          // NOTE: don't seek pos here; do it lazily
+          // instead.  Eg a PhraseQuery may skip to many
+          // docs before finally asking for positions...
+          posIndex.set(skipper.getPosIndex());
+          posSeekPending = true;
+          count = newCount;
+          doc = accum = skipper.getDoc();
+          //System.out.println("    moved to doc=" + doc);
+          //payloadIn.seek(skipper.getPayloadPointer());
+          payloadFP = skipper.getPayloadPointer();
+          pendingPosCount = 0;
+          pendingPayloadBytes = 0;
+          payloadPending = false;
+          payloadLength = skipper.getPayloadLength();
+          //System.out.println("    move payloadLen=" + payloadLength);
+        }
+      }
+        
+      // Now, linear scan for the rest:
+      do {
+        if (nextDoc() == NO_MORE_DOCS) {
+          //System.out.println("  advance nextDoc=END");
+          return NO_MORE_DOCS;
+        }
+        //System.out.println("  advance nextDoc=" + doc);
+      } while (target > doc);
+
+      //System.out.println("  return doc=" + doc);
+      return doc;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      if (posSeekPending) {
+        posIndex.seek(posReader);
+        payloadIn.seek(payloadFP);
+        posSeekPending = false;
+      }
+
+      // scan over any docs that were iterated without their
+      // positions
+      while (pendingPosCount > freq) {
+        final int code = posReader.next();
+        if (storePayloads && (code & 1) != 0) {
+          // Payload length has changed
+          payloadLength = posReader.next();
+          assert payloadLength >= 0;
+        }
+        pendingPosCount--;
+        position = 0;
+        pendingPayloadBytes += payloadLength;
+      }
+
+      final int code = posReader.next();
+      assert code >= 0;
+      if (storePayloads) {
+        if ((code & 1) != 0) {
+          // Payload length has changed
+          payloadLength = posReader.next();
+          assert payloadLength >= 0;
+        }
+        position += code >> 1;
+        pendingPayloadBytes += payloadLength;
+        payloadPending = payloadLength > 0;
+      } else {
+        position += code;
+      }
+    
+      pendingPosCount--;
+      assert pendingPosCount >= 0;
+      return position;
+    }
+
+    private BytesRef payload;
+
+    @Override
+    public BytesRef getPayload() throws IOException {
+      if (!payloadPending) {
+        throw new IOException("Either no payload exists at this term position or an attempt was made to load it more than once.");
+      }
+
+      assert pendingPayloadBytes >= payloadLength;
+
+      if (pendingPayloadBytes > payloadLength) {
+        payloadIn.seek(payloadIn.getFilePointer() + (pendingPayloadBytes - payloadLength));
+      }
+
+      if (payload == null) {
+        payload = new BytesRef();
+        payload.bytes = new byte[payloadLength];
+      } else if (payload.bytes.length < payloadLength) {
+        payload.grow(payloadLength);
+      }
+
+      payloadIn.readBytes(payload.bytes, 0, payloadLength);
+      payloadPending = false;
+      payload.length = payloadLength;
+      pendingPayloadBytes = 0;
+      return payload;
+    }
+
+    @Override
+    public boolean hasPayload() {
+      return payloadPending && payloadLength > 0;
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java b/lucene/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java
new file mode 100644
index 0000000..26e09be
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java
@@ -0,0 +1,392 @@
+package org.apache.lucene.codecs.sep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+
+/** Writes frq to .frq, docs to .doc, pos to .pos, payloads
+ *  to .pyl, skip data to .skp
+ *
+ * @lucene.experimental */
+public final class SepPostingsWriter extends PostingsWriterBase {
+  final static String CODEC = "SepPostingsWriter";
+
+  final static String DOC_EXTENSION = "doc";
+  final static String SKIP_EXTENSION = "skp";
+  final static String FREQ_EXTENSION = "frq";
+  final static String POS_EXTENSION = "pos";
+  final static String PAYLOAD_EXTENSION = "pyl";
+
+  // Increment version to change it:
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  IntIndexOutput freqOut;
+  IntIndexOutput.Index freqIndex;
+
+  IntIndexOutput posOut;
+  IntIndexOutput.Index posIndex;
+
+  IntIndexOutput docOut;
+  IntIndexOutput.Index docIndex;
+
+  IndexOutput payloadOut;
+
+  IndexOutput skipOut;
+  IndexOutput termsOut;
+
+  final SepSkipListWriter skipListWriter;
+  /** Expert: The fraction of TermDocs entries stored in skip tables,
+   * used to accelerate {@link DocsEnum#advance(int)}.  Larger values result in
+   * smaller indexes, greater acceleration, but fewer accelerable cases, while
+   * smaller values result in bigger indexes, less acceleration and more
+   * accelerable cases. More detailed experiments would be useful here. */
+  final int skipInterval;
+  static final int DEFAULT_SKIP_INTERVAL = 16;
+  
+  /**
+   * Expert: minimum docFreq to write any skip data at all
+   */
+  final int skipMinimum;
+
+  /** Expert: The maximum number of skip levels. Smaller values result in 
+   * slightly smaller indexes, but slower skipping in big posting lists.
+   */
+  final int maxSkipLevels = 10;
+
+  final int totalNumDocs;
+
+  boolean storePayloads;
+  IndexOptions indexOptions;
+
+  FieldInfo fieldInfo;
+
+  int lastPayloadLength;
+  int lastPosition;
+  long payloadStart;
+  int lastDocID;
+  int df;
+
+  // Holds pending byte[] blob for the current terms block
+  private final RAMOutputStream indexBytesWriter = new RAMOutputStream();
+
+  public SepPostingsWriter(SegmentWriteState state, IntStreamFactory factory) throws IOException {
+    this(state, factory, DEFAULT_SKIP_INTERVAL);
+  }
+
+  public SepPostingsWriter(SegmentWriteState state, IntStreamFactory factory, int skipInterval) throws IOException {
+    freqOut = null;
+    freqIndex = null;
+    posOut = null;
+    posIndex = null;
+    payloadOut = null;
+    boolean success = false;
+    try {
+      this.skipInterval = skipInterval;
+      this.skipMinimum = skipInterval; /* set to the same for now */
+      final String docFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, DOC_EXTENSION);
+      docOut = factory.createOutput(state.directory, docFileName, state.context);
+      docIndex = docOut.index();
+      
+      if (state.fieldInfos.hasFreq()) {
+        final String frqFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, FREQ_EXTENSION);
+        freqOut = factory.createOutput(state.directory, frqFileName, state.context);
+        freqIndex = freqOut.index();
+      }
+
+      if (state.fieldInfos.hasProx()) {      
+        final String posFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, POS_EXTENSION);
+        posOut = factory.createOutput(state.directory, posFileName, state.context);
+        posIndex = posOut.index();
+        
+        // TODO: -- only if at least one field stores payloads?
+        final String payloadFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, PAYLOAD_EXTENSION);
+        payloadOut = state.directory.createOutput(payloadFileName, state.context);
+      }
+      
+      final String skipFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, SKIP_EXTENSION);
+      skipOut = state.directory.createOutput(skipFileName, state.context);
+      
+      totalNumDocs = state.numDocs;
+      
+      skipListWriter = new SepSkipListWriter(skipInterval,
+          maxSkipLevels,
+          state.numDocs,
+          freqOut, docOut,
+          posOut, payloadOut);
+      
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(docOut, skipOut, freqOut, posOut, payloadOut);
+      }
+    }
+  }
+
+  @Override
+  public void start(IndexOutput termsOut) throws IOException {
+    this.termsOut = termsOut;
+    CodecUtil.writeHeader(termsOut, CODEC, VERSION_CURRENT);
+    // TODO: -- just ask skipper to "start" here
+    termsOut.writeInt(skipInterval);                // write skipInterval
+    termsOut.writeInt(maxSkipLevels);               // write maxSkipLevels
+    termsOut.writeInt(skipMinimum);                 // write skipMinimum
+  }
+
+  @Override
+  public void startTerm() throws IOException {
+    docIndex.mark();
+    //System.out.println("SEPW: startTerm docIndex=" + docIndex);
+
+    if (indexOptions != IndexOptions.DOCS_ONLY) {
+      freqIndex.mark();
+    }
+    
+    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      posIndex.mark();
+      payloadStart = payloadOut.getFilePointer();
+      lastPayloadLength = -1;
+    }
+
+    skipListWriter.resetSkip(docIndex, freqIndex, posIndex);
+  }
+
+  // Currently, this instance is re-used across fields, so
+  // our parent calls setField whenever the field changes
+  @Override
+  public void setField(FieldInfo fieldInfo) {
+    this.fieldInfo = fieldInfo;
+    this.indexOptions = fieldInfo.indexOptions;
+    skipListWriter.setIndexOptions(indexOptions);
+    storePayloads = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS && fieldInfo.storePayloads;
+  }
+
+  /** Adds a new doc in this term.  If this returns null
+   *  then we just skip consuming positions/payloads. */
+  @Override
+  public void startDoc(int docID, int termDocFreq) throws IOException {
+
+    final int delta = docID - lastDocID;
+    //System.out.println("SEPW: startDoc: write doc=" + docID + " delta=" + delta + " out.fp=" + docOut);
+
+    if (docID < 0 || (df > 0 && delta <= 0)) {
+      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " ) (docOut: " + docOut + ")");
+    }
+
+    if ((++df % skipInterval) == 0) {
+      // TODO: -- awkward we have to make these two
+      // separate calls to skipper
+      //System.out.println("    buffer skip lastDocID=" + lastDocID);
+      skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength);
+      skipListWriter.bufferSkip(df);
+    }
+
+    lastDocID = docID;
+    docOut.write(delta);
+    if (indexOptions != IndexOptions.DOCS_ONLY) {
+      //System.out.println("    sepw startDoc: write freq=" + termDocFreq);
+      freqOut.write(termDocFreq);
+    }
+  }
+
+  /** Add a new position & payload */
+  @Override
+  public void addPosition(int position, BytesRef payload) throws IOException {
+    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+
+    final int delta = position - lastPosition;
+    assert delta >= 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
+    lastPosition = position;
+
+    if (storePayloads) {
+      final int payloadLength = payload == null ? 0 : payload.length;
+      if (payloadLength != lastPayloadLength) {
+        lastPayloadLength = payloadLength;
+        // TODO: explore whether we get better compression
+        // by not storing payloadLength into prox stream?
+        posOut.write((delta<<1)|1);
+        posOut.write(payloadLength);
+      } else {
+        posOut.write(delta << 1);
+      }
+
+      if (payloadLength > 0) {
+        payloadOut.writeBytes(payload.bytes, payload.offset, payloadLength);
+      }
+    } else {
+      posOut.write(delta);
+    }
+
+    lastPosition = position;
+  }
+
+  /** Called when we are done adding positions & payloads */
+  @Override
+  public void finishDoc() {       
+    lastPosition = 0;
+  }
+
+  private static class PendingTerm {
+    public final IntIndexOutput.Index docIndex;
+    public final IntIndexOutput.Index freqIndex;
+    public final IntIndexOutput.Index posIndex;
+    public final long payloadFP;
+    public final long skipFP;
+
+    public PendingTerm(IntIndexOutput.Index docIndex, IntIndexOutput.Index freqIndex, IntIndexOutput.Index posIndex, long payloadFP, long skipFP) {
+      this.docIndex = docIndex;
+      this.freqIndex = freqIndex;
+      this.posIndex = posIndex;
+      this.payloadFP = payloadFP;
+      this.skipFP = skipFP;
+    }
+  }
+
+  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
+
+  /** Called when we are done adding docs to this term */
+  @Override
+  public void finishTerm(TermStats stats) throws IOException {
+    // TODO: -- wasteful we are counting this in two places?
+    assert stats.docFreq > 0;
+    assert stats.docFreq == df;
+
+    final IntIndexOutput.Index docIndexCopy = docOut.index();
+    docIndexCopy.copyFrom(docIndex, false);
+
+    final IntIndexOutput.Index freqIndexCopy;
+    final IntIndexOutput.Index posIndexCopy;
+    if (indexOptions != IndexOptions.DOCS_ONLY) {
+      freqIndexCopy = freqOut.index();
+      freqIndexCopy.copyFrom(freqIndex, false);
+      if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+        posIndexCopy = posOut.index();
+        posIndexCopy.copyFrom(posIndex, false);
+      } else {
+        posIndexCopy = null;
+      }
+    } else {
+      freqIndexCopy = null;
+      posIndexCopy = null;
+    }
+
+    final long skipFP;
+    if (df >= skipMinimum) {
+      skipFP = skipOut.getFilePointer();
+      //System.out.println("  skipFP=" + skipFP);
+      skipListWriter.writeSkip(skipOut);
+      //System.out.println("    numBytes=" + (skipOut.getFilePointer()-skipFP));
+    } else {
+      skipFP = -1;
+    }
+
+    lastDocID = 0;
+    df = 0;
+
+    pendingTerms.add(new PendingTerm(docIndexCopy,
+                                     freqIndexCopy,
+                                     posIndexCopy,
+                                     payloadStart,
+                                     skipFP));
+  }
+
+  @Override
+  public void flushTermsBlock(int start, int count) throws IOException {
+    //System.out.println("SEPW: flushTermsBlock: start=" + start + " count=" + count + " pendingTerms.size()=" + pendingTerms.size() + " termsOut.fp=" + termsOut.getFilePointer());
+    assert indexBytesWriter.getFilePointer() == 0;
+    final int absStart = pendingTerms.size() - start;
+    final List<PendingTerm> slice = pendingTerms.subList(absStart, absStart+count);
+
+    long lastPayloadFP = 0;
+    long lastSkipFP = 0;
+
+    if (count == 0) {
+      termsOut.writeByte((byte) 0);
+      return;
+    }
+
+    final PendingTerm firstTerm = slice.get(0);
+    final IntIndexOutput.Index docIndexFlush = firstTerm.docIndex;
+    final IntIndexOutput.Index freqIndexFlush = firstTerm.freqIndex;
+    final IntIndexOutput.Index posIndexFlush = firstTerm.posIndex;
+
+    for(int idx=0;idx<slice.size();idx++) {
+      final boolean isFirstTerm = idx == 0;
+      final PendingTerm t = slice.get(idx);
+      //System.out.println("  write idx=" + idx + " docIndex=" + t.docIndex);
+      docIndexFlush.copyFrom(t.docIndex, false);
+      docIndexFlush.write(indexBytesWriter, isFirstTerm);
+      if (indexOptions != IndexOptions.DOCS_ONLY) {
+        freqIndexFlush.copyFrom(t.freqIndex, false);
+        freqIndexFlush.write(indexBytesWriter, isFirstTerm);
+        //System.out.println("    freqIndex=" + t.freqIndex);
+        if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+          posIndexFlush.copyFrom(t.posIndex, false);
+          posIndexFlush.write(indexBytesWriter, isFirstTerm);
+          //System.out.println("    posIndex=" + t.posIndex);
+          if (storePayloads) {
+            //System.out.println("    payloadFP=" + t.payloadFP);
+            if (isFirstTerm) {
+              indexBytesWriter.writeVLong(t.payloadFP);
+            } else {
+              indexBytesWriter.writeVLong(t.payloadFP - lastPayloadFP);
+            }
+            lastPayloadFP = t.payloadFP;
+          }
+        }
+      }
+
+      if (t.skipFP != -1) {
+        if (isFirstTerm) {
+          indexBytesWriter.writeVLong(t.skipFP);
+        } else {
+          indexBytesWriter.writeVLong(t.skipFP - lastSkipFP);
+        }
+        lastSkipFP = t.skipFP;
+        //System.out.println("    skipFP=" + t.skipFP);
+      }
+    }
+
+    //System.out.println("  numBytes=" + indexBytesWriter.getFilePointer());
+    termsOut.writeVLong((int) indexBytesWriter.getFilePointer());
+    indexBytesWriter.writeTo(termsOut);
+    indexBytesWriter.reset();
+    slice.clear();
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.close(docOut, skipOut, freqOut, posOut, payloadOut);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/sep/SepSkipListReader.java b/lucene/src/java/org/apache/lucene/codecs/sep/SepSkipListReader.java
new file mode 100644
index 0000000..14a3f31
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/sep/SepSkipListReader.java
@@ -0,0 +1,209 @@
+package org.apache.lucene.codecs.sep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.codecs.MultiLevelSkipListReader;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+
+/**
+ * Implements the skip list reader for the default posting list format
+ * that stores positions and payloads.
+ *
+ * @lucene.experimental
+ */
+
+// TODO: rewrite this as recursive classes?
+class SepSkipListReader extends MultiLevelSkipListReader {
+  private boolean currentFieldStoresPayloads;
+  private IntIndexInput.Index freqIndex[];
+  private IntIndexInput.Index docIndex[];
+  private IntIndexInput.Index posIndex[];
+  private long payloadPointer[];
+  private int payloadLength[];
+
+  private final IntIndexInput.Index lastFreqIndex;
+  private final IntIndexInput.Index lastDocIndex;
+  // TODO: -- make private again
+  final IntIndexInput.Index lastPosIndex;
+  
+  private long lastPayloadPointer;
+  private int lastPayloadLength;
+                           
+  SepSkipListReader(IndexInput skipStream,
+                    IntIndexInput freqIn,
+                    IntIndexInput docIn,
+                    IntIndexInput posIn,
+                    int maxSkipLevels,
+                    int skipInterval)
+    throws IOException {
+    super(skipStream, maxSkipLevels, skipInterval);
+    if (freqIn != null) {
+      freqIndex = new IntIndexInput.Index[maxSkipLevels];
+    }
+    docIndex = new IntIndexInput.Index[maxSkipLevels];
+    if (posIn != null) {
+      posIndex = new IntIndexInput.Index[maxNumberOfSkipLevels];
+    }
+    for(int i=0;i<maxSkipLevels;i++) {
+      if (freqIn != null) {
+        freqIndex[i] = freqIn.index();
+      }
+      docIndex[i] = docIn.index();
+      if (posIn != null) {
+        posIndex[i] = posIn.index();
+      }
+    }
+    payloadPointer = new long[maxSkipLevels];
+    payloadLength = new int[maxSkipLevels];
+
+    if (freqIn != null) {
+      lastFreqIndex = freqIn.index();
+    } else {
+      lastFreqIndex = null;
+    }
+    lastDocIndex = docIn.index();
+    if (posIn != null) {
+      lastPosIndex = posIn.index();
+    } else {
+      lastPosIndex = null;
+    }
+  }
+  
+  IndexOptions indexOptions;
+
+  void setIndexOptions(IndexOptions v) {
+    indexOptions = v;
+  }
+
+  void init(long skipPointer,
+            IntIndexInput.Index docBaseIndex,
+            IntIndexInput.Index freqBaseIndex,
+            IntIndexInput.Index posBaseIndex,
+            long payloadBasePointer,
+            int df,
+            boolean storesPayloads) {
+
+    super.init(skipPointer, df);
+    this.currentFieldStoresPayloads = storesPayloads;
+
+    lastPayloadPointer = payloadBasePointer;
+
+    for(int i=0;i<maxNumberOfSkipLevels;i++) {
+      docIndex[i].set(docBaseIndex);
+      if (freqIndex != null) {
+        freqIndex[i].set(freqBaseIndex);
+      }
+      if (posBaseIndex != null) {
+        posIndex[i].set(posBaseIndex);
+      }
+    }
+    Arrays.fill(payloadPointer, payloadBasePointer);
+    Arrays.fill(payloadLength, 0);
+  }
+
+  long getPayloadPointer() {
+    return lastPayloadPointer;
+  }
+  
+  /** Returns the payload length of the payload stored just before 
+   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
+   * has skipped.  */
+  int getPayloadLength() {
+    return lastPayloadLength;
+  }
+  
+  @Override
+  protected void seekChild(int level) throws IOException {
+    super.seekChild(level);
+    payloadPointer[level] = lastPayloadPointer;
+    payloadLength[level] = lastPayloadLength;
+  }
+  
+  @Override
+  protected void setLastSkipData(int level) {
+    super.setLastSkipData(level);
+
+    lastPayloadPointer = payloadPointer[level];
+    lastPayloadLength = payloadLength[level];
+    if (freqIndex != null) {
+      lastFreqIndex.set(freqIndex[level]);
+    }
+    lastDocIndex.set(docIndex[level]);
+    if (lastPosIndex != null) {
+      lastPosIndex.set(posIndex[level]);
+    }
+
+    if (level > 0) {
+      if (freqIndex != null) {
+        freqIndex[level-1].set(freqIndex[level]);
+      }
+      docIndex[level-1].set(docIndex[level]);
+      if (posIndex != null) {
+        posIndex[level-1].set(posIndex[level]);
+      }
+    }
+  }
+
+  IntIndexInput.Index getFreqIndex() {
+    return lastFreqIndex;
+  }
+
+  IntIndexInput.Index getPosIndex() {
+    return lastPosIndex;
+  }
+
+  IntIndexInput.Index getDocIndex() {
+    return lastDocIndex;
+  }
+
+  @Override
+  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
+    int delta;
+    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !currentFieldStoresPayloads;
+    if (currentFieldStoresPayloads) {
+      // the current field stores payloads.
+      // if the doc delta is odd then we have
+      // to read the current payload length
+      // because it differs from the length of the
+      // previous payload
+      delta = skipStream.readVInt();
+      if ((delta & 1) != 0) {
+        payloadLength[level] = skipStream.readVInt();
+      }
+      delta >>>= 1;
+    } else {
+      delta = skipStream.readVInt();
+    }
+    if (indexOptions != IndexOptions.DOCS_ONLY) {
+      freqIndex[level].read(skipStream, false);
+    }
+    docIndex[level].read(skipStream, false);
+    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      posIndex[level].read(skipStream, false);
+      if (currentFieldStoresPayloads) {
+        payloadPointer[level] += skipStream.readVInt();
+      }
+    }
+    
+    return delta;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/sep/SepSkipListWriter.java b/lucene/src/java/org/apache/lucene/codecs/sep/SepSkipListWriter.java
new file mode 100644
index 0000000..66c8fa2
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/sep/SepSkipListWriter.java
@@ -0,0 +1,200 @@
+package org.apache.lucene.codecs.sep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.codecs.MultiLevelSkipListWriter;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+
+// TODO: -- skip data should somehow be more local to the
+// particular stream (doc, freq, pos, payload)
+
+/**
+ * Implements the skip list writer for the default posting list format
+ * that stores positions and payloads.
+ *
+ * @lucene.experimental
+ */
+class SepSkipListWriter extends MultiLevelSkipListWriter {
+  private int[] lastSkipDoc;
+  private int[] lastSkipPayloadLength;
+  private long[] lastSkipPayloadPointer;
+
+  private IntIndexOutput.Index[] docIndex;
+  private IntIndexOutput.Index[] freqIndex;
+  private IntIndexOutput.Index[] posIndex;
+  
+  private IntIndexOutput freqOutput;
+  // TODO: -- private again
+  IntIndexOutput posOutput;
+  // TODO: -- private again
+  IndexOutput payloadOutput;
+
+  private int curDoc;
+  private boolean curStorePayloads;
+  private int curPayloadLength;
+  private long curPayloadPointer;
+  
+  SepSkipListWriter(int skipInterval, int numberOfSkipLevels, int docCount,
+                    IntIndexOutput freqOutput,
+                    IntIndexOutput docOutput,
+                    IntIndexOutput posOutput,
+                    IndexOutput payloadOutput)
+    throws IOException {
+    super(skipInterval, numberOfSkipLevels, docCount);
+
+    this.freqOutput = freqOutput;
+    this.posOutput = posOutput;
+    this.payloadOutput = payloadOutput;
+    
+    lastSkipDoc = new int[numberOfSkipLevels];
+    lastSkipPayloadLength = new int[numberOfSkipLevels];
+    // TODO: -- also cutover normal IndexOutput to use getIndex()?
+    lastSkipPayloadPointer = new long[numberOfSkipLevels];
+
+    freqIndex = new IntIndexOutput.Index[numberOfSkipLevels];
+    docIndex = new IntIndexOutput.Index[numberOfSkipLevels];
+    posIndex = new IntIndexOutput.Index[numberOfSkipLevels];
+
+    for(int i=0;i<numberOfSkipLevels;i++) {
+      if (freqOutput != null) {
+        freqIndex[i] = freqOutput.index();
+      }
+      docIndex[i] = docOutput.index();
+      if (posOutput != null) {
+        posIndex[i] = posOutput.index();
+      }
+    }
+  }
+
+  IndexOptions indexOptions;
+
+  void setIndexOptions(IndexOptions v) {
+    indexOptions = v;
+  }
+
+  void setPosOutput(IntIndexOutput posOutput) throws IOException {
+    this.posOutput = posOutput;
+    for(int i=0;i<numberOfSkipLevels;i++) {
+      posIndex[i] = posOutput.index();
+    }
+  }
+
+  void setPayloadOutput(IndexOutput payloadOutput) {
+    this.payloadOutput = payloadOutput;
+  }
+
+  /**
+   * Sets the values for the current skip data. 
+   */
+  // Called @ every index interval (every 128th (by default)
+  // doc)
+  void setSkipData(int doc, boolean storePayloads, int payloadLength) {
+    this.curDoc = doc;
+    this.curStorePayloads = storePayloads;
+    this.curPayloadLength = payloadLength;
+    if (payloadOutput != null) {
+      this.curPayloadPointer = payloadOutput.getFilePointer();
+    }
+  }
+
+  // Called @ start of new term
+  protected void resetSkip(IntIndexOutput.Index topDocIndex, IntIndexOutput.Index topFreqIndex, IntIndexOutput.Index topPosIndex)
+    throws IOException {
+    super.resetSkip();
+
+    Arrays.fill(lastSkipDoc, 0);
+    Arrays.fill(lastSkipPayloadLength, -1);  // we don't have to write the first length in the skip list
+    for(int i=0;i<numberOfSkipLevels;i++) {
+      docIndex[i].copyFrom(topDocIndex, true);
+      if (freqOutput != null) {
+        freqIndex[i].copyFrom(topFreqIndex, true);
+      }
+      if (posOutput != null) {
+        posIndex[i].copyFrom(topPosIndex, true);
+      }
+    }
+    if (payloadOutput != null) {
+      Arrays.fill(lastSkipPayloadPointer, payloadOutput.getFilePointer());
+    }
+  }
+  
+  @Override
+  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
+    // To efficiently store payloads in the posting lists we do not store the length of
+    // every payload. Instead we omit the length for a payload if the previous payload had
+    // the same length.
+    // However, in order to support skipping the payload length at every skip point must be known.
+    // So we use the same length encoding that we use for the posting lists for the skip data as well:
+    // Case 1: current field does not store payloads
+    //           SkipDatum                 --> DocSkip, FreqSkip, ProxSkip
+    //           DocSkip,FreqSkip,ProxSkip --> VInt
+    //           DocSkip records the document number before every SkipInterval th  document in TermFreqs. 
+    //           Document numbers are represented as differences from the previous value in the sequence.
+    // Case 2: current field stores payloads
+    //           SkipDatum                 --> DocSkip, PayloadLength?, FreqSkip,ProxSkip
+    //           DocSkip,FreqSkip,ProxSkip --> VInt
+    //           PayloadLength             --> VInt    
+    //         In this case DocSkip/2 is the difference between
+    //         the current and the previous value. If DocSkip
+    //         is odd, then a PayloadLength encoded as VInt follows,
+    //         if DocSkip is even, then it is assumed that the
+    //         current payload length equals the length at the previous
+    //         skip point
+
+    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !curStorePayloads;
+
+    if (curStorePayloads) {
+      int delta = curDoc - lastSkipDoc[level];
+      if (curPayloadLength == lastSkipPayloadLength[level]) {
+        // the current payload length equals the length at the previous skip point,
+        // so we don't store the length again
+        skipBuffer.writeVInt(delta << 1);
+      } else {
+        // the payload length is different from the previous one. We shift the DocSkip, 
+        // set the lowest bit and store the current payload length as VInt.
+        skipBuffer.writeVInt(delta << 1 | 1);
+        skipBuffer.writeVInt(curPayloadLength);
+        lastSkipPayloadLength[level] = curPayloadLength;
+      }
+    } else {
+      // current field does not store payloads
+      skipBuffer.writeVInt(curDoc - lastSkipDoc[level]);
+    }
+
+    if (indexOptions != IndexOptions.DOCS_ONLY) {
+      freqIndex[level].mark();
+      freqIndex[level].write(skipBuffer, false);
+    }
+    docIndex[level].mark();
+    docIndex[level].write(skipBuffer, false);
+    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      posIndex[level].mark();
+      posIndex[level].write(skipBuffer, false);
+      if (curStorePayloads) {
+        skipBuffer.writeVInt((int) (curPayloadPointer - lastSkipPayloadPointer[level]));
+      }
+    }
+
+    lastSkipDoc[level] = curDoc;
+    lastSkipPayloadPointer[level] = curPayloadPointer;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/sep/package.html b/lucene/src/java/org/apache/lucene/codecs/sep/package.html
new file mode 100644
index 0000000..b51d910
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/sep/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Sep: base support for separate files (doc,frq,pos,skp,pyl)
+</body>
+</html>
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
new file mode 100644
index 0000000..e6a0867
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
@@ -0,0 +1,85 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
+
+/**
+ * plain text index format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public final class SimpleTextCodec extends Codec {
+  private final PostingsFormat postings = new SimpleTextPostingsFormat();
+  private final StoredFieldsFormat storedFields = new SimpleTextStoredFieldsFormat();
+  private final SegmentInfosFormat segmentInfos = new SimpleTextSegmentInfosFormat();
+  private final FieldInfosFormat fieldInfosFormat = new SimpleTextFieldInfosFormat();
+  private final TermVectorsFormat vectorsFormat = new SimpleTextTermVectorsFormat();
+  // TODO: need a plain-text impl
+  private final DocValuesFormat docValues = new Lucene40DocValuesFormat();
+  // TODO: need a plain-text impl (using the above)
+  private final NormsFormat normsFormat = new SimpleTextNormsFormat();
+  
+  public SimpleTextCodec() {
+    super("SimpleText");
+  }
+  
+  @Override
+  public PostingsFormat postingsFormat() {
+    return postings;
+  }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValues;
+  }
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return storedFields;
+  }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+
+  @Override
+  public SegmentInfosFormat segmentInfosFormat() {
+    return segmentInfos;
+  }
+
+  @Override
+  public NormsFormat normsFormat() {
+    return normsFormat;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java
new file mode 100644
index 0000000..8c6b540
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java
@@ -0,0 +1,53 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+
+/**
+ * plaintext field infos format
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextFieldInfosFormat extends FieldInfosFormat {
+  private final FieldInfosReader reader = new SimpleTextFieldInfosReader();
+  private final FieldInfosWriter writer = new SimpleTextFieldInfosWriter();
+
+  @Override
+  public FieldInfosReader getFieldInfosReader() throws IOException {
+    return reader;
+  }
+
+  @Override
+  public FieldInfosWriter getFieldInfosWriter() throws IOException {
+    return writer;
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    SimpleTextFieldInfosReader.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java
new file mode 100644
index 0000000..5d4fbeb
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java
@@ -0,0 +1,139 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextFieldInfosWriter.*;
+
+/**
+ * reads plaintext field infos files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextFieldInfosReader extends FieldInfosReader {
+
+  @Override
+  public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
+    IndexInput input = directory.openInput(fileName, iocontext);
+    BytesRef scratch = new BytesRef();
+    
+    boolean hasVectors = false;
+    boolean hasFreq = false;
+    boolean hasProx = false;
+    
+    try {
+      
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, NUMFIELDS);
+      final int size = Integer.parseInt(readString(NUMFIELDS.length, scratch));
+      FieldInfo infos[] = new FieldInfo[size];
+
+      for (int i = 0; i < size; i++) {
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, NAME);
+        String name = readString(NAME.length, scratch);
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, NUMBER);
+        int fieldNumber = Integer.parseInt(readString(NUMBER.length, scratch));
+
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, ISINDEXED);
+        boolean isIndexed = Boolean.parseBoolean(readString(ISINDEXED.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, STORETV);
+        boolean storeTermVector = Boolean.parseBoolean(readString(STORETV.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, STORETVPOS);
+        boolean storePositionsWithTermVector = Boolean.parseBoolean(readString(STORETVPOS.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, STORETVOFF);
+        boolean storeOffsetWithTermVector = Boolean.parseBoolean(readString(STORETVOFF.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, PAYLOADS);
+        boolean storePayloads = Boolean.parseBoolean(readString(PAYLOADS.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, NORMS);
+        boolean omitNorms = !Boolean.parseBoolean(readString(NORMS.length, scratch));
+
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, DOCVALUES);
+        String dvType = readString(DOCVALUES.length, scratch);
+        final DocValues.Type docValuesType;
+        
+        if ("false".equals(dvType)) {
+          docValuesType = null;
+        } else {
+          docValuesType = DocValues.Type.valueOf(dvType);
+        }
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, INDEXOPTIONS);
+        IndexOptions indexOptions = IndexOptions.valueOf(readString(INDEXOPTIONS.length, scratch));
+
+        hasVectors |= storeTermVector;
+        hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
+        
+        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
+          storePositionsWithTermVector, storeOffsetWithTermVector, 
+          omitNorms, storePayloads, indexOptions, docValuesType);
+      }
+
+      if (input.getFilePointer() != input.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
+      }
+      
+      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
+    } finally {
+      input.close();
+    }
+  }
+  
+  private String readString(int offset, BytesRef scratch) {
+    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
+  }
+  
+  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    files.add(IndexFileNames.segmentFileName(info.name, "", FIELD_INFOS_EXTENSION));
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java
new file mode 100644
index 0000000..da5452e
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java
@@ -0,0 +1,115 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * writes plaintext field infos files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextFieldInfosWriter extends FieldInfosWriter {
+  
+  /** Extension of field infos */
+  static final String FIELD_INFOS_EXTENSION = "inf";
+  
+  static final BytesRef NUMFIELDS       =  new BytesRef("number of fields ");
+  static final BytesRef NAME            =  new BytesRef("  name ");
+  static final BytesRef NUMBER          =  new BytesRef("  number ");
+  static final BytesRef ISINDEXED       =  new BytesRef("  indexed ");
+  static final BytesRef STORETV         =  new BytesRef("  term vectors ");
+  static final BytesRef STORETVPOS      =  new BytesRef("  term vector positions ");
+  static final BytesRef STORETVOFF      =  new BytesRef("  term vector offsets ");
+  static final BytesRef PAYLOADS        =  new BytesRef("  payloads ");
+  static final BytesRef NORMS           =  new BytesRef("  norms ");
+  static final BytesRef DOCVALUES       =  new BytesRef("  doc values ");
+  static final BytesRef INDEXOPTIONS    =  new BytesRef("  index options ");
+  
+  @Override
+  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
+    IndexOutput out = directory.createOutput(fileName, context);
+    BytesRef scratch = new BytesRef();
+    try {
+      SimpleTextUtil.write(out, NUMFIELDS);
+      SimpleTextUtil.write(out, Integer.toString(infos.size()), scratch);
+      SimpleTextUtil.writeNewline(out);
+      
+      for (FieldInfo fi : infos) {
+        assert fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !fi.storePayloads;
+
+        SimpleTextUtil.write(out, NAME);
+        SimpleTextUtil.write(out, fi.name, scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, NUMBER);
+        SimpleTextUtil.write(out, Integer.toString(fi.number), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, ISINDEXED);
+        SimpleTextUtil.write(out, Boolean.toString(fi.isIndexed), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, STORETV);
+        SimpleTextUtil.write(out, Boolean.toString(fi.storeTermVector), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, STORETVPOS);
+        SimpleTextUtil.write(out, Boolean.toString(fi.storePositionWithTermVector), scratch);
+        SimpleTextUtil.writeNewline(out);
+
+        SimpleTextUtil.write(out, STORETVOFF);
+        SimpleTextUtil.write(out, Boolean.toString(fi.storeOffsetWithTermVector), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, PAYLOADS);
+        SimpleTextUtil.write(out, Boolean.toString(fi.storePayloads), scratch);
+        SimpleTextUtil.writeNewline(out);
+               
+        SimpleTextUtil.write(out, NORMS);
+        SimpleTextUtil.write(out, Boolean.toString(!fi.omitNorms), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, DOCVALUES);
+        if (!fi.hasDocValues()) {
+          SimpleTextUtil.write(out, "false", scratch);
+        } else {
+          SimpleTextUtil.write(out, fi.getDocValuesType().toString(), scratch);
+        }
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, INDEXOPTIONS);
+        SimpleTextUtil.write(out, fi.indexOptions.toString(), scratch);
+        SimpleTextUtil.writeNewline(out);
+      }
+    } finally {
+      out.close();
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
new file mode 100644
index 0000000..9c540d7
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
@@ -0,0 +1,596 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.OpenBitSet;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.UnicodeUtil;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PairOutputs;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+
+class SimpleTextFieldsReader extends FieldsProducer {
+
+  private final IndexInput in;
+  private final FieldInfos fieldInfos;
+
+  final static BytesRef END     = SimpleTextFieldsWriter.END;
+  final static BytesRef FIELD   = SimpleTextFieldsWriter.FIELD;
+  final static BytesRef TERM    = SimpleTextFieldsWriter.TERM;
+  final static BytesRef DOC     = SimpleTextFieldsWriter.DOC;
+  final static BytesRef FREQ    = SimpleTextFieldsWriter.FREQ;
+  final static BytesRef POS     = SimpleTextFieldsWriter.POS;
+  final static BytesRef PAYLOAD = SimpleTextFieldsWriter.PAYLOAD;
+
+  public SimpleTextFieldsReader(SegmentReadState state) throws IOException {
+    in = state.dir.openInput(SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix), state.context);
+   
+    fieldInfos = state.fieldInfos;
+  }
+
+  private class SimpleTextFieldsEnum extends FieldsEnum {
+    private final IndexInput in;
+    private final BytesRef scratch = new BytesRef(10);
+    private String current;
+
+    public SimpleTextFieldsEnum() {
+      this.in = (IndexInput) SimpleTextFieldsReader.this.in.clone();
+    }
+
+    @Override
+    public String next() throws IOException {
+      while(true) {
+        SimpleTextUtil.readLine(in, scratch);
+        if (scratch.equals(END)) {
+          current = null;
+          return null;
+        }
+        if (StringHelper.startsWith(scratch, FIELD)) {
+          return current = new String(scratch.bytes, scratch.offset + FIELD.length, scratch.length - FIELD.length, "UTF-8");
+        }
+      }
+    }
+
+    @Override
+    public Terms terms() throws IOException {
+      return SimpleTextFieldsReader.this.terms(current);
+    }
+  }
+
+  private class SimpleTextTermsEnum extends TermsEnum {
+    private final IndexOptions indexOptions;
+    private int docFreq;
+    private long totalTermFreq;
+    private long docsStart;
+    private boolean ended;
+    private final BytesRefFSTEnum<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fstEnum;
+
+    public SimpleTextTermsEnum(FST<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fst, IndexOptions indexOptions) throws IOException {
+      this.indexOptions = indexOptions;
+      fstEnum = new BytesRefFSTEnum<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>>(fst);
+    }
+
+    @Override
+    public boolean seekExact(BytesRef text, boolean useCache /* ignored */) throws IOException {
+
+      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.seekExact(text);
+      if (result != null) {
+        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
+        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
+        docsStart = pair1.output1;
+        docFreq = pair2.output1.intValue();
+        totalTermFreq = pair2.output2;
+        return true;
+      } else {
+        return false;
+      }
+    }
+
+    @Override
+    public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
+
+      //System.out.println("seek to text=" + text.utf8ToString());
+      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.seekCeil(text);
+      if (result == null) {
+        //System.out.println("  end");
+        return SeekStatus.END;
+      } else {
+        //System.out.println("  got text=" + term.utf8ToString());
+        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
+        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
+        docsStart = pair1.output1;
+        docFreq = pair2.output1.intValue();
+        totalTermFreq = pair2.output2;
+
+        if (result.input.equals(text)) {
+          //System.out.println("  match docsStart=" + docsStart);
+          return SeekStatus.FOUND;
+        } else {
+          //System.out.println("  not match docsStart=" + docsStart);
+          return SeekStatus.NOT_FOUND;
+        }
+      }
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      assert !ended;
+      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.next();
+      if (result != null) {
+        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
+        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
+        docsStart = pair1.output1;
+        docFreq = pair2.output1.intValue();
+        totalTermFreq = pair2.output2;
+        return result.input;
+      } else {
+        return null;
+      }
+    }
+
+    @Override
+    public BytesRef term() {
+      return fstEnum.current().input;
+    }
+
+    @Override
+    public long ord() throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public void seekExact(long ord) {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public int docFreq() {
+      return docFreq;
+    }
+
+    @Override
+    public long totalTermFreq() {
+      return indexOptions == IndexOptions.DOCS_ONLY ? -1 : totalTermFreq;
+    }
+ 
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
+      if (needsFreqs && indexOptions == IndexOptions.DOCS_ONLY) {
+        return null;
+      }
+      SimpleTextDocsEnum docsEnum;
+      if (reuse != null && reuse instanceof SimpleTextDocsEnum && ((SimpleTextDocsEnum) reuse).canReuse(SimpleTextFieldsReader.this.in)) {
+        docsEnum = (SimpleTextDocsEnum) reuse;
+      } else {
+        docsEnum = new SimpleTextDocsEnum();
+      }
+      return docsEnum.reset(docsStart, liveDocs, !needsFreqs);
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
+      if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+        return null;
+      }
+
+      SimpleTextDocsAndPositionsEnum docsAndPositionsEnum;
+      if (reuse != null && reuse instanceof SimpleTextDocsAndPositionsEnum && ((SimpleTextDocsAndPositionsEnum) reuse).canReuse(SimpleTextFieldsReader.this.in)) {
+        docsAndPositionsEnum = (SimpleTextDocsAndPositionsEnum) reuse;
+      } else {
+        docsAndPositionsEnum = new SimpleTextDocsAndPositionsEnum();
+      } 
+      return docsAndPositionsEnum.reset(docsStart, liveDocs);
+    }
+    
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+
+  private class SimpleTextDocsEnum extends DocsEnum {
+    private final IndexInput inStart;
+    private final IndexInput in;
+    private boolean omitTF;
+    private int docID = -1;
+    private int tf;
+    private Bits liveDocs;
+    private final BytesRef scratch = new BytesRef(10);
+    private final CharsRef scratchUTF16 = new CharsRef(10);
+    
+    public SimpleTextDocsEnum() {
+      this.inStart = SimpleTextFieldsReader.this.in;
+      this.in = (IndexInput) this.inStart.clone();
+    }
+
+    public boolean canReuse(IndexInput in) {
+      return in == inStart;
+    }
+
+    public SimpleTextDocsEnum reset(long fp, Bits liveDocs, boolean omitTF) throws IOException {
+      this.liveDocs = liveDocs;
+      in.seek(fp);
+      this.omitTF = omitTF;
+      docID = -1;
+      return this;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int freq() {
+      assert !omitTF;
+      return tf;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      if (docID == NO_MORE_DOCS) {
+        return docID;
+      }
+      boolean first = true;
+      int termFreq = 0;
+      while(true) {
+        final long lineStart = in.getFilePointer();
+        SimpleTextUtil.readLine(in, scratch);
+        if (StringHelper.startsWith(scratch, DOC)) {
+          if (!first && (liveDocs == null || liveDocs.get(docID))) {
+            in.seek(lineStart);
+            if (!omitTF) {
+              tf = termFreq;
+            }
+            return docID;
+          }
+          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
+          docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+          termFreq = 0;
+          first = false;
+        } else if (StringHelper.startsWith(scratch, FREQ)) {
+          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+FREQ.length, scratch.length-FREQ.length, scratchUTF16);
+          termFreq = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+        } else if (StringHelper.startsWith(scratch, POS)) {
+          // skip termFreq++;
+        } else if (StringHelper.startsWith(scratch, PAYLOAD)) {
+          // skip
+        } else {
+          assert StringHelper.startsWith(scratch, TERM) || StringHelper.startsWith(scratch, FIELD) || StringHelper.startsWith(scratch, END): "scratch=" + scratch.utf8ToString();
+          if (!first && (liveDocs == null || liveDocs.get(docID))) {
+            in.seek(lineStart);
+            if (!omitTF) {
+              tf = termFreq;
+            }
+            return docID;
+          }
+          return docID = NO_MORE_DOCS;
+        }
+      }
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      // Naive -- better to index skip data
+      while(nextDoc() < target);
+      return docID;
+    }
+  }
+
+  private class SimpleTextDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private final IndexInput inStart;
+    private final IndexInput in;
+    private int docID = -1;
+    private int tf;
+    private Bits liveDocs;
+    private final BytesRef scratch = new BytesRef(10);
+    private final BytesRef scratch2 = new BytesRef(10);
+    private final CharsRef scratchUTF16 = new CharsRef(10);
+    private final CharsRef scratchUTF16_2 = new CharsRef(10);
+    private BytesRef payload;
+    private long nextDocStart;
+
+    public SimpleTextDocsAndPositionsEnum() {
+      this.inStart = SimpleTextFieldsReader.this.in;
+      this.in = (IndexInput) inStart.clone();
+    }
+
+    public boolean canReuse(IndexInput in) {
+      return in == inStart;
+    }
+
+    public SimpleTextDocsAndPositionsEnum reset(long fp, Bits liveDocs) {
+      this.liveDocs = liveDocs;
+      nextDocStart = fp;
+      docID = -1;
+      return this;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int freq() {
+      return tf;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      boolean first = true;
+      in.seek(nextDocStart);
+      long posStart = 0;
+      while(true) {
+        final long lineStart = in.getFilePointer();
+        SimpleTextUtil.readLine(in, scratch);
+        if (StringHelper.startsWith(scratch, DOC)) {
+          if (!first && (liveDocs == null || liveDocs.get(docID))) {
+            nextDocStart = lineStart;
+            in.seek(posStart);
+            return docID;
+          }
+          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
+          docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+          tf = 0;
+          first = false;
+        } else if (StringHelper.startsWith(scratch, FREQ)) {
+          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+FREQ.length, scratch.length-FREQ.length, scratchUTF16);
+          tf = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+          posStart = in.getFilePointer();
+        } else if (StringHelper.startsWith(scratch, POS)) {
+          // skip
+        } else if (StringHelper.startsWith(scratch, PAYLOAD)) {
+          // skip
+        } else {
+          assert StringHelper.startsWith(scratch, TERM) || StringHelper.startsWith(scratch, FIELD) || StringHelper.startsWith(scratch, END);
+          if (!first && (liveDocs == null || liveDocs.get(docID))) {
+            nextDocStart = lineStart;
+            in.seek(posStart);
+            return docID;
+          }
+          return docID = NO_MORE_DOCS;
+        }
+      }
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      // Naive -- better to index skip data
+      while(nextDoc() < target);
+      return docID;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      SimpleTextUtil.readLine(in, scratch);
+      assert StringHelper.startsWith(scratch, POS): "got line=" + scratch.utf8ToString();
+      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+POS.length, scratch.length-POS.length, scratchUTF16_2);
+      final int pos = ArrayUtil.parseInt(scratchUTF16_2.chars, 0, scratchUTF16_2.length);
+      final long fp = in.getFilePointer();
+      SimpleTextUtil.readLine(in, scratch);
+      if (StringHelper.startsWith(scratch, PAYLOAD)) {
+        final int len = scratch.length - PAYLOAD.length;
+        if (scratch2.bytes.length < len) {
+          scratch2.grow(len);
+        }
+        System.arraycopy(scratch.bytes, PAYLOAD.length, scratch2.bytes, 0, len);
+        scratch2.length = len;
+        payload = scratch2;
+      } else {
+        payload = null;
+        in.seek(fp);
+      }
+      return pos;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      // Some tests rely on only being able to retrieve the
+      // payload once
+      try {
+        return payload;
+      } finally {
+        payload = null;
+      }
+    }
+
+    @Override
+    public boolean hasPayload() {
+      return payload != null;
+    }
+  }
+
+  static class TermData {
+    public long docsStart;
+    public int docFreq;
+
+    public TermData(long docsStart, int docFreq) {
+      this.docsStart = docsStart;
+      this.docFreq = docFreq;
+    }
+  }
+
+  private class SimpleTextTerms extends Terms {
+    private final long termsStart;
+    private final IndexOptions indexOptions;
+    private long sumTotalTermFreq;
+    private long sumDocFreq;
+    private int docCount;
+    private FST<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fst;
+    private int termCount;
+    private final BytesRef scratch = new BytesRef(10);
+    private final CharsRef scratchUTF16 = new CharsRef(10);
+
+    public SimpleTextTerms(String field, long termsStart) throws IOException {
+      this.termsStart = termsStart;
+      indexOptions = fieldInfos.fieldInfo(field).indexOptions;
+      loadTerms();
+    }
+
+    private void loadTerms() throws IOException {
+      PositiveIntOutputs posIntOutputs = PositiveIntOutputs.getSingleton(false);
+      final Builder<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> b;
+      b = new Builder<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>>(FST.INPUT_TYPE.BYTE1,
+                                                                          new PairOutputs<Long,PairOutputs.Pair<Long,Long>>(posIntOutputs,
+                                                                                                                            new PairOutputs<Long,Long>(posIntOutputs, posIntOutputs)));
+      IndexInput in = (IndexInput) SimpleTextFieldsReader.this.in.clone();
+      in.seek(termsStart);
+      final BytesRef lastTerm = new BytesRef(10);
+      long lastDocsStart = -1;
+      int docFreq = 0;
+      long totalTermFreq = 0;
+      OpenBitSet visitedDocs = new OpenBitSet();
+      while(true) {
+        SimpleTextUtil.readLine(in, scratch);
+        if (scratch.equals(END) || StringHelper.startsWith(scratch, FIELD)) {
+          if (lastDocsStart != -1) {
+            b.add(lastTerm, new PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>(lastDocsStart,
+                                                                                   new PairOutputs.Pair<Long,Long>((long) docFreq,
+                                                                                                                   posIntOutputs.get(totalTermFreq))));
+            sumTotalTermFreq += totalTermFreq;
+          }
+          break;
+        } else if (StringHelper.startsWith(scratch, DOC)) {
+          docFreq++;
+          sumDocFreq++;
+          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
+          int docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+          visitedDocs.set(docID);
+        } else if (StringHelper.startsWith(scratch, POS)) {
+          totalTermFreq++;
+        } else if (StringHelper.startsWith(scratch, TERM)) {
+          if (lastDocsStart != -1) {
+            b.add(lastTerm, new PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>(lastDocsStart,
+                                                                                   new PairOutputs.Pair<Long,Long>((long) docFreq,
+                                                                                                                   posIntOutputs.get(totalTermFreq))));
+          }
+          lastDocsStart = in.getFilePointer();
+          final int len = scratch.length - TERM.length;
+          if (len > lastTerm.length) {
+            lastTerm.grow(len);
+          }
+          System.arraycopy(scratch.bytes, TERM.length, lastTerm.bytes, 0, len);
+          lastTerm.length = len;
+          docFreq = 0;
+          sumTotalTermFreq += totalTermFreq;
+          totalTermFreq = 0;
+          termCount++;
+        }
+      }
+      docCount = (int) visitedDocs.cardinality();
+      fst = b.finish();
+      /*
+      PrintStream ps = new PrintStream("out.dot");
+      fst.toDot(ps);
+      ps.close();
+      System.out.println("SAVED out.dot");
+      */
+      //System.out.println("FST " + fst.sizeInBytes());
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      if (fst != null) {
+        return new SimpleTextTermsEnum(fst, indexOptions);
+      } else {
+        return TermsEnum.EMPTY;
+      }
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public long getUniqueTermCount() {
+      return (long) termCount;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return indexOptions == IndexOptions.DOCS_ONLY ? -1 : sumTotalTermFreq;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return sumDocFreq;
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return docCount;
+    }
+  }
+
+  @Override
+  public FieldsEnum iterator() throws IOException {
+    return new SimpleTextFieldsEnum();
+  }
+
+  private final Map<String,Terms> termsCache = new HashMap<String,Terms>();
+
+  @Override
+  synchronized public Terms terms(String field) throws IOException {
+    Terms terms = termsCache.get(field);
+    if (terms == null) {
+      SimpleTextFieldsEnum fe = (SimpleTextFieldsEnum) iterator();
+      String fieldUpto;
+      while((fieldUpto = fe.next()) != null) {
+        if (fieldUpto.equals(field)) {
+          terms = new SimpleTextTerms(field, fe.in.getFilePointer());
+          break;
+        }
+      }
+      termsCache.put(field, terms);
+    }
+    return terms;
+  }
+
+  @Override
+  public int getUniqueFieldCount() {
+    return -1;
+  }
+
+  @Override
+  public void close() throws IOException {
+    in.close();
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java
new file mode 100644
index 0000000..ac88626
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java
@@ -0,0 +1,161 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.PostingsConsumer;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+
+import java.io.IOException;
+import java.util.Comparator;
+
+class SimpleTextFieldsWriter extends FieldsConsumer {
+  
+  private final IndexOutput out;
+  private final BytesRef scratch = new BytesRef(10);
+
+  final static BytesRef END     = new BytesRef("END");
+  final static BytesRef FIELD   = new BytesRef("field ");
+  final static BytesRef TERM    = new BytesRef("  term ");
+  final static BytesRef DOC     = new BytesRef("    doc ");
+  final static BytesRef FREQ    = new BytesRef("      freq ");
+  final static BytesRef POS     = new BytesRef("      pos ");
+  final static BytesRef PAYLOAD = new BytesRef("        payload ");
+
+  public SimpleTextFieldsWriter(SegmentWriteState state) throws IOException {
+    final String fileName = SimpleTextPostingsFormat.getPostingsFileName(state.segmentName, state.segmentSuffix);
+    out = state.directory.createOutput(fileName, state.context);
+  }
+
+  private void write(String s) throws IOException {
+    SimpleTextUtil.write(out, s, scratch);
+  }
+
+  private void write(BytesRef b) throws IOException {
+    SimpleTextUtil.write(out, b);
+  }
+
+  private void newline() throws IOException {
+    SimpleTextUtil.writeNewline(out);
+  }
+
+  @Override
+  public TermsConsumer addField(FieldInfo field) throws IOException {
+    write(FIELD);
+    write(field.name);
+    newline();
+    return new SimpleTextTermsWriter(field);
+  }
+
+  private class SimpleTextTermsWriter extends TermsConsumer {
+    private final SimpleTextPostingsWriter postingsWriter;
+    
+    public SimpleTextTermsWriter(FieldInfo field) {
+      postingsWriter = new SimpleTextPostingsWriter(field);
+    }
+
+    @Override
+    public PostingsConsumer startTerm(BytesRef term) throws IOException {
+      return postingsWriter.reset(term);
+    }
+
+    @Override
+    public void finishTerm(BytesRef term, TermStats stats) throws IOException {
+    }
+
+    @Override
+    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+
+  private class SimpleTextPostingsWriter extends PostingsConsumer {
+    private BytesRef term;
+    private boolean wroteTerm;
+    private IndexOptions indexOptions;
+
+    public SimpleTextPostingsWriter(FieldInfo field) {
+      this.indexOptions = field.indexOptions;
+    }
+
+    @Override
+    public void startDoc(int docID, int termDocFreq) throws IOException {
+      if (!wroteTerm) {
+        // we lazily do this, in case the term had zero docs
+        write(TERM);
+        write(term);
+        newline();
+        wroteTerm = true;
+      }
+
+      write(DOC);
+      write(Integer.toString(docID));
+      newline();
+      if (indexOptions != IndexOptions.DOCS_ONLY) {
+        write(FREQ);
+        write(Integer.toString(termDocFreq));
+        newline();
+      }
+    }
+    
+    
+
+    public PostingsConsumer reset(BytesRef term) {
+      this.term = term;
+      wroteTerm = false;
+      return this;
+    }
+
+    @Override
+    public void addPosition(int position, BytesRef payload) throws IOException {
+      write(POS);
+      write(Integer.toString(position));
+      newline();
+      if (payload != null && payload.length > 0) {
+        assert payload.length != 0;
+        write(PAYLOAD);
+        write(payload);
+        newline();
+      }
+    }
+
+    @Override
+    public void finishDoc() {
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      write(END);
+      newline();
+    } finally {
+      out.close();
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java
new file mode 100644
index 0000000..a0aa650
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java
@@ -0,0 +1,54 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.NormsReader;
+import org.apache.lucene.codecs.NormsWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * plain-text norms format
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextNormsFormat extends NormsFormat {
+
+  @Override
+  public NormsReader normsReader(Directory dir, SegmentInfo info, FieldInfos fields, IOContext context, Directory separateNormsDir) throws IOException {
+    return new SimpleTextNormsReader(dir, info, fields, context);
+  }
+
+  @Override
+  public NormsWriter normsWriter(SegmentWriteState state) throws IOException {
+    return new SimpleTextNormsWriter(state.directory, state.segmentName, state.context);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    SimpleTextNormsReader.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsReader.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsReader.java
new file mode 100644
index 0000000..93c058b
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsReader.java
@@ -0,0 +1,106 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.codecs.NormsReader;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextNormsWriter.*;
+
+/**
+ * Reads plain-text norms
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextNormsReader extends NormsReader {
+  private Map<String,byte[]> norms = new HashMap<String,byte[]>();
+  
+  public SimpleTextNormsReader(Directory directory, SegmentInfo si, FieldInfos fields, IOContext context) throws IOException {
+    if (fields.hasNorms()) {
+      readNorms(directory.openInput(IndexFileNames.segmentFileName(si.name, "", NORMS_EXTENSION), context), si.docCount);
+    }
+  }
+  
+  // we read in all the norms up front into a hashmap
+  private void readNorms(IndexInput in, int maxDoc) throws IOException {
+    BytesRef scratch = new BytesRef();
+    boolean success = false;
+    try {
+      SimpleTextUtil.readLine(in, scratch);
+      while (!scratch.equals(END)) {
+        assert StringHelper.startsWith(scratch, FIELD);
+        String fieldName = readString(FIELD.length, scratch);
+        byte bytes[] = new byte[maxDoc];
+        for (int i = 0; i < bytes.length; i++) {
+          SimpleTextUtil.readLine(in, scratch);
+          assert StringHelper.startsWith(scratch, DOC);
+          SimpleTextUtil.readLine(in, scratch);
+          assert StringHelper.startsWith(scratch, NORM);
+          bytes[i] = scratch.bytes[scratch.offset + NORM.length];
+        }
+        norms.put(fieldName, bytes);
+        SimpleTextUtil.readLine(in, scratch);
+        assert StringHelper.startsWith(scratch, FIELD) || scratch.equals(END);
+      }
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(in);
+      } else {
+        IOUtils.closeWhileHandlingException(in);
+      }
+    }
+  }
+  
+  @Override
+  public byte[] norms(String name) throws IOException {
+    return norms.get(name);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    norms = null;
+  }
+  
+  static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    // TODO: This is what SI always did... but we can do this cleaner?
+    // like first FI that has norms but doesn't have separate norms?
+    final String normsFileName = IndexFileNames.segmentFileName(info.name, "", SimpleTextNormsWriter.NORMS_EXTENSION);
+    if (dir.fileExists(normsFileName)) {
+      files.add(normsFileName);
+    }
+  }
+  
+  private String readString(int offset, BytesRef scratch) {
+    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsWriter.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsWriter.java
new file mode 100644
index 0000000..7b3729f
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsWriter.java
@@ -0,0 +1,114 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.NormsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Writes plain-text norms
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextNormsWriter extends NormsWriter {
+  private IndexOutput out;
+  private int docid = 0;
+    
+  /** Extension of norms file */
+  static final String NORMS_EXTENSION = "len";
+  
+  private final BytesRef scratch = new BytesRef();
+  
+  final static BytesRef END     = new BytesRef("END");
+  final static BytesRef FIELD   = new BytesRef("field ");
+  final static BytesRef DOC     = new BytesRef("  doc ");
+  final static BytesRef NORM    = new BytesRef("    norm ");
+  
+  public SimpleTextNormsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    final String normsFileName = IndexFileNames.segmentFileName(segment, "", NORMS_EXTENSION);
+    out = directory.createOutput(normsFileName, context);
+  }
+
+  @Override
+  public void startField(FieldInfo info) throws IOException {
+    assert info.omitNorms == false;
+    docid = 0;
+    write(FIELD);
+    write(info.name);
+    newLine();
+  }
+    
+  @Override
+  public void writeNorm(byte norm) throws IOException {
+    write(DOC);
+    write(Integer.toString(docid));
+    newLine();
+    
+    write(NORM);
+    write(norm);
+    newLine();
+    docid++;
+  }
+    
+  @Override
+  public void finish(int numDocs) throws IOException {
+    if (docid != numDocs) {
+      throw new RuntimeException("mergeNorms produced an invalid result: docCount is " + numDocs
+          + " but only saw " + docid + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
+    }
+    write(END);
+    newLine();
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(out);
+    } finally {
+      out = null;
+    }
+  }
+  
+  private void write(String s) throws IOException {
+    SimpleTextUtil.write(out, s, scratch);
+  }
+  
+  private void write(BytesRef bytes) throws IOException {
+    SimpleTextUtil.write(out, bytes);
+  }
+  
+  private void write(byte b) throws IOException {
+    scratch.grow(1);
+    scratch.bytes[scratch.offset] = b;
+    scratch.length = 1;
+    SimpleTextUtil.write(out, scratch);
+  }
+  
+  private void newLine() throws IOException {
+    SimpleTextUtil.writeNewline(out);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java
new file mode 100644
index 0000000..1eaf256
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java
@@ -0,0 +1,67 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+
+/** For debugging, curiosity, transparency only!!  Do not
+ *  use this codec in production.
+ *
+ *  <p>This codec stores all postings data in a single
+ *  human-readable text file (_N.pst).  You can view this in
+ *  any text editor, and even edit it to alter your index.
+ *
+ *  @lucene.experimental */
+public class SimpleTextPostingsFormat extends PostingsFormat {
+  
+  public SimpleTextPostingsFormat() {
+    super("SimpleText");
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return new SimpleTextFieldsWriter(state);
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new SimpleTextFieldsReader(state);
+  }
+
+  /** Extension of freq postings file */
+  static final String POSTINGS_EXTENSION = "pst";
+
+  static String getPostingsFileName(String segment, String segmentSuffix) {
+    return IndexFileNames.segmentFileName(segment, segmentSuffix, POSTINGS_EXTENSION);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
+    files.add(getPostingsFileName(segmentInfo.name, segmentSuffix));
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosFormat.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosFormat.java
new file mode 100644
index 0000000..6518cf4
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosFormat.java
@@ -0,0 +1,43 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.SegmentInfosReader;
+import org.apache.lucene.codecs.SegmentInfosWriter;
+
+/**
+ * plain text segments file format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfosFormat extends SegmentInfosFormat {
+  private final SegmentInfosReader reader = new SimpleTextSegmentInfosReader();
+  private final SegmentInfosWriter writer = new SimpleTextSegmentInfosWriter();
+  
+  @Override
+  public SegmentInfosReader getSegmentInfosReader() {
+    return reader;
+  }
+
+  @Override
+  public SegmentInfosWriter getSegmentInfosWriter() {
+    return writer;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosReader.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosReader.java
new file mode 100644
index 0000000..590cf6a
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosReader.java
@@ -0,0 +1,190 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.SegmentInfosReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.store.ChecksumIndexInput;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextSegmentInfosWriter.*;
+
+/**
+ * reads plaintext segments files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfosReader extends SegmentInfosReader {
+
+  @Override
+  public void read(Directory directory, String segmentsFileName, ChecksumIndexInput input, SegmentInfos infos, IOContext context) throws IOException {
+    final BytesRef scratch = new BytesRef();
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, VERSION);
+    infos.version = Long.parseLong(readString(VERSION.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, COUNTER);
+    infos.counter = Integer.parseInt(readString(COUNTER.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, NUM_USERDATA);
+    int numUserData = Integer.parseInt(readString(NUM_USERDATA.length, scratch));
+    infos.userData = new HashMap<String,String>();
+
+    for (int i = 0; i < numUserData; i++) {
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, USERDATA_KEY);
+      String key = readString(USERDATA_KEY.length, scratch);
+      
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, USERDATA_VALUE);
+      String value = readString(USERDATA_VALUE.length, scratch);
+      infos.userData.put(key, value);
+    }
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, NUM_SEGMENTS);
+    int numSegments = Integer.parseInt(readString(NUM_SEGMENTS.length, scratch));
+    
+    for (int i = 0; i < numSegments; i++) {
+      infos.add(readSegmentInfo(directory, input, scratch));
+    }
+  }
+  
+  public SegmentInfo readSegmentInfo(Directory directory, DataInput input, BytesRef scratch) throws IOException {
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_NAME);
+    final String name = readString(SI_NAME.length, scratch);
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_CODEC);
+    final Codec codec = Codec.forName(readString(SI_CODEC.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_VERSION);
+    final String version = readString(SI_VERSION.length, scratch);
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_DOCCOUNT);
+    final int docCount = Integer.parseInt(readString(SI_DOCCOUNT.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_DELCOUNT);
+    final int delCount = Integer.parseInt(readString(SI_DELCOUNT.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_HASPROX);
+    final int hasProx = readTernary(SI_HASPROX.length, scratch);
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_HASVECTORS);
+    final int hasVectors = readTernary(SI_HASVECTORS.length, scratch);
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_USECOMPOUND);
+    final boolean isCompoundFile = Boolean.parseBoolean(readString(SI_USECOMPOUND.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_DSOFFSET);
+    final int dsOffset = Integer.parseInt(readString(SI_DSOFFSET.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_DSSEGMENT);
+    final String dsSegment = readString(SI_DSSEGMENT.length, scratch);
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_DSCOMPOUND);
+    final boolean dsCompoundFile = Boolean.parseBoolean(readString(SI_DSCOMPOUND.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_DELGEN);
+    final long delGen = Long.parseLong(readString(SI_DELGEN.length, scratch));
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_NUM_NORMGEN);
+    final int numNormGen = Integer.parseInt(readString(SI_NUM_NORMGEN.length, scratch));
+    final Map<Integer,Long> normGen;
+    if (numNormGen == 0) {
+      normGen = null;
+    } else {
+      normGen = new HashMap<Integer,Long>();
+      for (int i = 0; i < numNormGen; i++) {
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_NORMGEN_KEY);
+        int key = Integer.parseInt(readString(SI_NORMGEN_KEY.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_NORMGEN_VALUE);
+        long value = Long.parseLong(readString(SI_NORMGEN_VALUE.length, scratch));
+        normGen.put(key, value);
+      }
+    }
+    
+    SimpleTextUtil.readLine(input, scratch);
+    assert StringHelper.startsWith(scratch, SI_NUM_DIAG);
+    int numDiag = Integer.parseInt(readString(SI_NUM_DIAG.length, scratch));
+    Map<String,String> diagnostics = new HashMap<String,String>();
+
+    for (int i = 0; i < numDiag; i++) {
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_DIAG_KEY);
+      String key = readString(SI_DIAG_KEY.length, scratch);
+      
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_DIAG_VALUE);
+      String value = readString(SI_DIAG_VALUE.length, scratch);
+      diagnostics.put(key, value);
+    }
+    
+    return new SegmentInfo(directory, version, name, docCount, delGen, dsOffset,
+        dsSegment, dsCompoundFile, normGen, isCompoundFile,
+        delCount, hasProx, codec, diagnostics, hasVectors);
+  }
+  
+  private String readString(int offset, BytesRef scratch) {
+    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
+  }
+  
+  private int readTernary(int offset, BytesRef scratch) throws IOException {
+    String s = readString(offset, scratch);
+    if ("check fieldinfo".equals(s)) {
+      return SegmentInfo.CHECK_FIELDINFO;
+    } else if ("true".equals(s)) {
+      return SegmentInfo.YES;
+    } else if ("false".equals(s)) {
+      return 0;
+    } else {
+      throw new CorruptIndexException("invalid ternary value: " + s);
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosWriter.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosWriter.java
new file mode 100644
index 0000000..18c80cf
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfosWriter.java
@@ -0,0 +1,234 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.lucene.codecs.SegmentInfosWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.store.ChecksumIndexOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.FlushInfo;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * writes plaintext segments files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfosWriter extends SegmentInfosWriter {
+
+  final static BytesRef VERSION             = new BytesRef("version ");
+  final static BytesRef COUNTER             = new BytesRef("counter ");
+  final static BytesRef NUM_USERDATA        = new BytesRef("user data entries ");
+  final static BytesRef USERDATA_KEY        = new BytesRef("  key ");
+  final static BytesRef USERDATA_VALUE      = new BytesRef("  value ");
+  final static BytesRef NUM_SEGMENTS        = new BytesRef("number of segments ");
+  final static BytesRef SI_NAME             = new BytesRef("  name ");
+  final static BytesRef SI_CODEC            = new BytesRef("    codec ");
+  final static BytesRef SI_VERSION          = new BytesRef("    version ");
+  final static BytesRef SI_DOCCOUNT         = new BytesRef("    number of documents ");
+  final static BytesRef SI_DELCOUNT         = new BytesRef("    number of deletions ");
+  final static BytesRef SI_HASPROX          = new BytesRef("    has prox ");
+  final static BytesRef SI_HASVECTORS       = new BytesRef("    has vectors ");
+  final static BytesRef SI_USECOMPOUND      = new BytesRef("    uses compound file ");
+  final static BytesRef SI_DSOFFSET         = new BytesRef("    docstore offset ");
+  final static BytesRef SI_DSSEGMENT        = new BytesRef("    docstore segment ");
+  final static BytesRef SI_DSCOMPOUND       = new BytesRef("    docstore is compound file ");
+  final static BytesRef SI_DELGEN           = new BytesRef("    deletion generation ");
+  final static BytesRef SI_NUM_NORMGEN      = new BytesRef("    norms generations ");
+  final static BytesRef SI_NORMGEN_KEY      = new BytesRef("      key ");
+  final static BytesRef SI_NORMGEN_VALUE    = new BytesRef("      value ");
+  final static BytesRef SI_NUM_DIAG         = new BytesRef("    diagnostics ");
+  final static BytesRef SI_DIAG_KEY         = new BytesRef("      key ");
+  final static BytesRef SI_DIAG_VALUE       = new BytesRef("      value ");
+  
+  @Override
+  public IndexOutput writeInfos(Directory dir, String segmentsFileName, String codecID, SegmentInfos infos, IOContext context) throws IOException {
+    BytesRef scratch = new BytesRef();
+    IndexOutput out = new ChecksumIndexOutput(dir.createOutput(segmentsFileName, new IOContext(new FlushInfo(infos.size(), infos.totalDocCount()))));
+    boolean success = false;
+    try {
+      // required preamble:
+      out.writeInt(SegmentInfos.FORMAT_CURRENT); // write FORMAT
+      out.writeString(codecID); // write codecID
+      // end preamble
+      
+      // version
+      SimpleTextUtil.write(out, VERSION);
+      SimpleTextUtil.write(out, Long.toString(infos.version), scratch);
+      SimpleTextUtil.writeNewline(out);
+
+      // counter
+      SimpleTextUtil.write(out, COUNTER);
+      SimpleTextUtil.write(out, Integer.toString(infos.counter), scratch);
+      SimpleTextUtil.writeNewline(out);
+
+      // user data
+      int numUserDataEntries = infos.getUserData() == null ? 0 : infos.getUserData().size();
+      SimpleTextUtil.write(out, NUM_USERDATA);
+      SimpleTextUtil.write(out, Integer.toString(numUserDataEntries), scratch);
+      SimpleTextUtil.writeNewline(out);
+      
+      if (numUserDataEntries > 0) {
+        for (Map.Entry<String,String> userEntry : infos.getUserData().entrySet()) {
+          SimpleTextUtil.write(out, USERDATA_KEY);
+          SimpleTextUtil.write(out, userEntry.getKey(), scratch);
+          SimpleTextUtil.writeNewline(out);
+          
+          SimpleTextUtil.write(out, USERDATA_VALUE);
+          SimpleTextUtil.write(out, userEntry.getValue(), scratch);
+          SimpleTextUtil.writeNewline(out);
+        }
+      }
+      
+      // infos size
+      SimpleTextUtil.write(out, NUM_SEGMENTS);
+      SimpleTextUtil.write(out, Integer.toString(infos.size()), scratch);
+      SimpleTextUtil.writeNewline(out);
+
+      for (SegmentInfo si : infos) {
+        writeInfo(out, si);
+      } 
+
+      success = true;
+      return out;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  private void writeInfo(IndexOutput output, SegmentInfo si) throws IOException {
+    assert si.getDelCount() <= si.docCount: "delCount=" + si.getDelCount() + " docCount=" + si.docCount + " segment=" + si.name;
+    BytesRef scratch = new BytesRef();
+    
+    SimpleTextUtil.write(output, SI_NAME);
+    SimpleTextUtil.write(output, si.name, scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_CODEC);
+    SimpleTextUtil.write(output, si.getCodec().getName(), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_VERSION);
+    SimpleTextUtil.write(output, si.getVersion(), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DOCCOUNT);
+    SimpleTextUtil.write(output, Integer.toString(si.docCount), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DELCOUNT);
+    SimpleTextUtil.write(output, Integer.toString(si.getDelCount()), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_HASPROX);
+    switch(si.getHasProxInternal()) {
+      case SegmentInfo.YES: SimpleTextUtil.write(output, "true", scratch); break;
+      case SegmentInfo.CHECK_FIELDINFO: SimpleTextUtil.write(output, "check fieldinfo", scratch); break;
+      // its "NO" if its 'anything but YES'... such as 0
+      default: SimpleTextUtil.write(output, "false", scratch); break;
+    }
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_HASVECTORS);
+    switch(si.getHasVectorsInternal()) {
+      case SegmentInfo.YES: SimpleTextUtil.write(output, "true", scratch); break;
+      case SegmentInfo.CHECK_FIELDINFO: SimpleTextUtil.write(output, "check fieldinfo", scratch); break;
+      // its "NO" if its 'anything but YES'... such as 0
+      default: SimpleTextUtil.write(output, "false", scratch); break;
+    }
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_USECOMPOUND);
+    SimpleTextUtil.write(output, Boolean.toString(si.getUseCompoundFile()), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DSOFFSET);
+    SimpleTextUtil.write(output, Integer.toString(si.getDocStoreOffset()), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DSSEGMENT);
+    SimpleTextUtil.write(output, si.getDocStoreSegment(), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DSCOMPOUND);
+    SimpleTextUtil.write(output, Boolean.toString(si.getDocStoreIsCompoundFile()), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    SimpleTextUtil.write(output, SI_DELGEN);
+    SimpleTextUtil.write(output, Long.toString(si.getDelGen()), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    Map<Integer,Long> normGen = si.getNormGen();
+    int numNormGen = normGen == null ? 0 : normGen.size();
+    SimpleTextUtil.write(output, SI_NUM_NORMGEN);
+    SimpleTextUtil.write(output, Integer.toString(numNormGen), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    if (numNormGen > 0) {
+      for (Entry<Integer,Long> entry : normGen.entrySet()) {
+        SimpleTextUtil.write(output, SI_NORMGEN_KEY);
+        SimpleTextUtil.write(output, Integer.toString(entry.getKey()), scratch);
+        SimpleTextUtil.writeNewline(output);
+        
+        SimpleTextUtil.write(output, SI_NORMGEN_VALUE);
+        SimpleTextUtil.write(output, Long.toString(entry.getValue()), scratch);
+        SimpleTextUtil.writeNewline(output);
+      }
+    }
+    
+    Map<String,String> diagnostics = si.getDiagnostics();
+    int numDiagnostics = diagnostics == null ? 0 : diagnostics.size();
+    SimpleTextUtil.write(output, SI_NUM_DIAG);
+    SimpleTextUtil.write(output, Integer.toString(numDiagnostics), scratch);
+    SimpleTextUtil.writeNewline(output);
+    
+    if (numDiagnostics > 0) {
+      for (Map.Entry<String,String> diagEntry : diagnostics.entrySet()) {
+        SimpleTextUtil.write(output, SI_DIAG_KEY);
+        SimpleTextUtil.write(output, diagEntry.getKey(), scratch);
+        SimpleTextUtil.writeNewline(output);
+        
+        SimpleTextUtil.write(output, SI_DIAG_VALUE);
+        SimpleTextUtil.write(output, diagEntry.getValue(), scratch);
+        SimpleTextUtil.writeNewline(output);
+      }
+    }
+  }
+
+  @Override
+  public void prepareCommit(IndexOutput out) throws IOException {
+    ((ChecksumIndexOutput)out).prepareCommit();
+  }
+
+  @Override
+  public void finishCommit(IndexOutput out) throws IOException {
+    ((ChecksumIndexOutput)out).finishCommit();
+    out.close();
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java
new file mode 100644
index 0000000..8ca53b2
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java
@@ -0,0 +1,53 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * plain text stored fields format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextStoredFieldsFormat extends StoredFieldsFormat {
+
+  @Override
+  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
+    return new SimpleTextStoredFieldsReader(directory, si, fn, context);
+  }
+
+  @Override
+  public StoredFieldsWriter fieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    return new SimpleTextStoredFieldsWriter(directory, segment, context);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    SimpleTextStoredFieldsReader.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java
new file mode 100644
index 0000000..6764292
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java
@@ -0,0 +1,198 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Set;
+
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.StoredFieldVisitor;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.UnicodeUtil;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextStoredFieldsWriter.*;
+
+/**
+ * reads plaintext stored fields
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextStoredFieldsReader extends StoredFieldsReader {
+  private ArrayList<Long> offsets; /* docid -> offset in .fld file */
+  private IndexInput in;
+  private BytesRef scratch = new BytesRef();
+  private CharsRef scratchUTF16 = new CharsRef();
+  private final FieldInfos fieldInfos;
+
+  public SimpleTextStoredFieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
+    this.fieldInfos = fn;
+    boolean success = false;
+    try {
+      in = directory.openInput(IndexFileNames.segmentFileName(si.name, "", SimpleTextStoredFieldsWriter.FIELDS_EXTENSION), context);
+      success = true;
+    } finally {
+      if (!success) {
+        close();
+      }
+    }
+    readIndex();
+  }
+  
+  // used by clone
+  SimpleTextStoredFieldsReader(ArrayList<Long> offsets, IndexInput in, FieldInfos fieldInfos) {
+    this.offsets = offsets;
+    this.in = in;
+    this.fieldInfos = fieldInfos;
+  }
+  
+  // we don't actually write a .fdx-like index, instead we read the 
+  // stored fields file in entirety up-front and save the offsets 
+  // so we can seek to the documents later.
+  private void readIndex() throws IOException {
+    offsets = new ArrayList<Long>();
+    while (!scratch.equals(END)) {
+      readLine();
+      if (StringHelper.startsWith(scratch, DOC)) {
+        offsets.add(in.getFilePointer());
+      }
+    }
+  }
+  
+  @Override
+  public void visitDocument(int n, StoredFieldVisitor visitor) throws CorruptIndexException, IOException {
+    in.seek(offsets.get(n));
+    readLine();
+    assert StringHelper.startsWith(scratch, NUM);
+    int numFields = parseIntAt(NUM.length);
+    
+    for (int i = 0; i < numFields; i++) {
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELD);
+      int fieldNumber = parseIntAt(FIELD.length);
+      FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
+      readLine();
+      assert StringHelper.startsWith(scratch, NAME);
+      readLine();
+      assert StringHelper.startsWith(scratch, TYPE);
+      
+      final BytesRef type;
+      if (equalsAt(TYPE_STRING, scratch, TYPE.length)) {
+        type = TYPE_STRING;
+      } else if (equalsAt(TYPE_BINARY, scratch, TYPE.length)) {
+        type = TYPE_BINARY;
+      } else if (equalsAt(TYPE_INT, scratch, TYPE.length)) {
+        type = TYPE_INT;
+      } else if (equalsAt(TYPE_LONG, scratch, TYPE.length)) {
+        type = TYPE_LONG;
+      } else if (equalsAt(TYPE_FLOAT, scratch, TYPE.length)) {
+        type = TYPE_FLOAT;
+      } else if (equalsAt(TYPE_DOUBLE, scratch, TYPE.length)) {
+        type = TYPE_DOUBLE;
+      } else {
+        throw new RuntimeException("unknown field type");
+      }
+      
+      switch (visitor.needsField(fieldInfo)) {
+        case YES:  
+          readField(type, fieldInfo, visitor);
+          break;
+        case NO:   
+          readLine();
+          assert StringHelper.startsWith(scratch, VALUE);
+          break;
+        case STOP: return;
+      }
+    }
+  }
+  
+  private void readField(BytesRef type, FieldInfo fieldInfo, StoredFieldVisitor visitor) throws IOException {
+    readLine();
+    assert StringHelper.startsWith(scratch, VALUE);
+    if (type == TYPE_STRING) {
+      visitor.stringField(fieldInfo, new String(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, "UTF-8"));
+    } else if (type == TYPE_BINARY) {
+      // TODO: who owns the bytes?
+      byte[] copy = new byte[scratch.length-VALUE.length];
+      System.arraycopy(scratch.bytes, scratch.offset+VALUE.length, copy, 0, copy.length);
+      visitor.binaryField(fieldInfo, copy, 0, copy.length);
+    } else if (type == TYPE_INT) {
+      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
+      visitor.intField(fieldInfo, Integer.parseInt(scratchUTF16.toString()));
+    } else if (type == TYPE_LONG) {
+      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
+      visitor.longField(fieldInfo, Long.parseLong(scratchUTF16.toString()));
+    } else if (type == TYPE_FLOAT) {
+      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
+      visitor.floatField(fieldInfo, Float.parseFloat(scratchUTF16.toString()));
+    } else if (type == TYPE_DOUBLE) {
+      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
+      visitor.doubleField(fieldInfo, Double.parseDouble(scratchUTF16.toString()));
+    }
+  }
+
+  @Override
+  public StoredFieldsReader clone() {
+    if (in == null) {
+      throw new AlreadyClosedException("this FieldsReader is closed");
+    }
+    return new SimpleTextStoredFieldsReader(offsets, (IndexInput) in.clone(), fieldInfos);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(in); 
+    } finally {
+      in = null;
+      offsets = null;
+    }
+  }
+  
+  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    files.add(IndexFileNames.segmentFileName(info.name, "", SimpleTextStoredFieldsWriter.FIELDS_EXTENSION));
+  }
+  
+  private void readLine() throws IOException {
+    SimpleTextUtil.readLine(in, scratch);
+  }
+  
+  private int parseIntAt(int offset) throws IOException {
+    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
+    return ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+  }
+  
+  private boolean equalsAt(BytesRef a, BytesRef b, int bOffset) {
+    return a.length == b.length - bOffset && 
+        ArrayUtil.equals(a.bytes, a.offset, b.bytes, b.offset + bOffset, b.length - bOffset);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java
new file mode 100644
index 0000000..9341ddc
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java
@@ -0,0 +1,204 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Writes plain-text stored fields.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextStoredFieldsWriter extends StoredFieldsWriter {
+  private int numDocsWritten = 0;
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput out;
+  
+  final static String FIELDS_EXTENSION = "fld";
+  
+  final static BytesRef TYPE_STRING = new BytesRef("string");
+  final static BytesRef TYPE_BINARY = new BytesRef("binary");
+  final static BytesRef TYPE_INT    = new BytesRef("int");
+  final static BytesRef TYPE_LONG   = new BytesRef("long");
+  final static BytesRef TYPE_FLOAT  = new BytesRef("float");
+  final static BytesRef TYPE_DOUBLE = new BytesRef("double");
+
+  final static BytesRef END     = new BytesRef("END");
+  final static BytesRef DOC     = new BytesRef("doc ");
+  final static BytesRef NUM     = new BytesRef("  numfields ");
+  final static BytesRef FIELD   = new BytesRef("  field ");
+  final static BytesRef NAME    = new BytesRef("    name ");
+  final static BytesRef TYPE    = new BytesRef("    type ");
+  final static BytesRef VALUE   = new BytesRef("    value ");
+  
+  private final BytesRef scratch = new BytesRef();
+  
+  public SimpleTextStoredFieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    this.directory = directory;
+    this.segment = segment;
+    boolean success = false;
+    try {
+      out = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
+      success = true;
+    } finally {
+      if (!success) {
+        abort();
+      }
+    }
+  }
+
+  @Override
+  public void startDocument(int numStoredFields) throws IOException {
+    write(DOC);
+    write(Integer.toString(numDocsWritten));
+    newLine();
+    
+    write(NUM);
+    write(Integer.toString(numStoredFields));
+    newLine();
+    
+    numDocsWritten++;
+  }
+
+  @Override
+  public void writeField(FieldInfo info, IndexableField field) throws IOException {
+    write(FIELD);
+    write(Integer.toString(info.number));
+    newLine();
+    
+    write(NAME);
+    write(field.name());
+    newLine();
+    
+    write(TYPE);
+    if (field.numeric()) {
+      switch (field.numericDataType()) {
+        case INT:
+          write(TYPE_INT);
+          newLine();
+          
+          write(VALUE);
+          write(Integer.toString(field.numericValue().intValue()));
+          newLine();
+          
+          break;
+        case LONG:
+          write(TYPE_LONG);
+          newLine();
+          
+          write(VALUE);
+          write(Long.toString(field.numericValue().longValue()));
+          newLine();
+          
+          break;
+        case FLOAT:
+          write(TYPE_FLOAT);
+          newLine();
+          
+          write(VALUE);
+          write(Float.toString(field.numericValue().floatValue()));
+          newLine();
+          
+          break;
+        case DOUBLE:
+          write(TYPE_DOUBLE);
+          newLine();
+          
+          write(VALUE);
+          write(Double.toString(field.numericValue().doubleValue()));
+          newLine();
+          
+          break;
+        default:
+          assert false : "Should never get here";
+      }
+    } else { 
+      BytesRef bytes = field.binaryValue();
+      if (bytes != null) {
+        write(TYPE_BINARY);
+        newLine();
+        
+        write(VALUE);
+        write(bytes);
+        newLine();
+      } else if (field.stringValue() == null) {
+        throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
+      } else {
+        write(TYPE_STRING);
+        newLine();
+        
+        write(VALUE);
+        write(field.stringValue());
+        newLine();
+      }
+    }
+  }
+
+  @Override
+  public void abort() {
+    try {
+      close();
+    } catch (IOException ignored) {}
+    try {
+      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION));
+    } catch (IOException ignored) {}
+  }
+
+  @Override
+  public void finish(int numDocs) throws IOException {
+    if (numDocsWritten != numDocs) {
+      throw new RuntimeException("mergeFields produced an invalid result: docCount is " + numDocs 
+          + " but only saw " + numDocsWritten + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
+    }
+    write(END);
+    newLine();
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(out);
+    } finally {
+      out = null;
+    }
+  }
+  
+  private void write(String s) throws IOException {
+    SimpleTextUtil.write(out, s, scratch);
+  }
+  
+  private void write(BytesRef bytes) throws IOException {
+    SimpleTextUtil.write(out, bytes);
+  }
+  
+  private void newLine() throws IOException {
+    SimpleTextUtil.writeNewline(out);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java
new file mode 100644
index 0000000..ea971b8
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java
@@ -0,0 +1,53 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * plain text term vectors format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextTermVectorsFormat extends TermVectorsFormat {
+
+  @Override
+  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
+    return new SimpleTextTermVectorsReader(directory, segmentInfo, fieldInfos, context);
+  }
+
+  @Override
+  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    return new SimpleTextTermVectorsWriter(directory, segment, context);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    SimpleTextTermVectorsReader.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
new file mode 100644
index 0000000..0b11175
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
@@ -0,0 +1,532 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.UnicodeUtil;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextTermVectorsWriter.*;
+
+/**
+ * Reads plain-text term vectors.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextTermVectorsReader extends TermVectorsReader {
+  private ArrayList<Long> offsets; /* docid -> offset in .vec file */
+  private IndexInput in;
+  private BytesRef scratch = new BytesRef();
+  private CharsRef scratchUTF16 = new CharsRef();
+  
+  public SimpleTextTermVectorsReader(Directory directory, SegmentInfo si, FieldInfos fieldInfos, IOContext context) throws IOException {
+    boolean success = false;
+    try {
+      in = directory.openInput(IndexFileNames.segmentFileName(si.name, "", VECTORS_EXTENSION), context);
+      success = true;
+    } finally {
+      if (!success) {
+        close();
+      }
+    }
+    readIndex();
+  }
+  
+  // used by clone
+  SimpleTextTermVectorsReader(ArrayList<Long> offsets, IndexInput in) {
+    this.offsets = offsets;
+    this.in = in;
+  }
+  
+  // we don't actually write a .tvx-like index, instead we read the 
+  // vectors file in entirety up-front and save the offsets 
+  // so we can seek to the data later.
+  private void readIndex() throws IOException {
+    offsets = new ArrayList<Long>();
+    while (!scratch.equals(END)) {
+      readLine();
+      if (StringHelper.startsWith(scratch, DOC)) {
+        offsets.add(in.getFilePointer());
+      }
+    }
+  }
+  
+  @Override
+  public Fields get(int doc) throws IOException {
+    // TestTV tests for this in testBadParams... but is this
+    // really guaranteed by the API?
+    if (doc < 0 || doc >= offsets.size()) {
+      throw new IllegalArgumentException("doc id out of range");
+    }
+
+    SortedMap<String,SimpleTVTerms> fields = new TreeMap<String,SimpleTVTerms>();
+    in.seek(offsets.get(doc));
+    readLine();
+    assert StringHelper.startsWith(scratch, NUMFIELDS);
+    int numFields = parseIntAt(NUMFIELDS.length);
+    if (numFields == 0) {
+      return null; // no vectors for this doc
+    }
+    for (int i = 0; i < numFields; i++) {
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELD);
+      int fieldNumber = parseIntAt(FIELD.length);
+      
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELDNAME);
+      String fieldName = readString(FIELDNAME.length, scratch);
+      
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELDPOSITIONS);
+      boolean positions = Boolean.parseBoolean(readString(FIELDPOSITIONS.length, scratch));
+      
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELDOFFSETS);
+      boolean offsets = Boolean.parseBoolean(readString(FIELDOFFSETS.length, scratch));
+      
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELDTERMCOUNT);
+      int termCount = parseIntAt(FIELDTERMCOUNT.length);
+      
+      SimpleTVTerms terms = new SimpleTVTerms();
+      fields.put(fieldName, terms);
+      
+      for (int j = 0; j < termCount; j++) {
+        readLine();
+        assert StringHelper.startsWith(scratch, TERMTEXT);
+        BytesRef term = new BytesRef();
+        int termLength = scratch.length - TERMTEXT.length;
+        term.grow(termLength);
+        term.length = termLength;
+        System.arraycopy(scratch.bytes, scratch.offset+TERMTEXT.length, term.bytes, term.offset, termLength);
+        
+        SimpleTVPostings postings = new SimpleTVPostings();
+        terms.terms.put(term, postings);
+        
+        readLine();
+        assert StringHelper.startsWith(scratch, TERMFREQ);
+        postings.freq = parseIntAt(TERMFREQ.length);
+        
+        if (positions || offsets) {
+          if (positions) {
+            postings.positions = new int[postings.freq];
+          }
+        
+          if (offsets) {
+            postings.startOffsets = new int[postings.freq];
+            postings.endOffsets = new int[postings.freq];
+          }
+          
+          for (int k = 0; k < postings.freq; k++) {
+            if (positions) {
+              readLine();
+              assert StringHelper.startsWith(scratch, POSITION);
+              postings.positions[k] = parseIntAt(POSITION.length);
+            }
+            
+            if (offsets) {
+              readLine();
+              assert StringHelper.startsWith(scratch, STARTOFFSET);
+              postings.startOffsets[k] = parseIntAt(STARTOFFSET.length);
+              
+              readLine();
+              assert StringHelper.startsWith(scratch, ENDOFFSET);
+              postings.endOffsets[k] = parseIntAt(ENDOFFSET.length);
+            }
+          }
+        }
+      }
+    }
+    return new SimpleTVFields(fields);
+  }
+
+  @Override
+  public TermVectorsReader clone() {
+    if (in == null) {
+      throw new AlreadyClosedException("this TermVectorsReader is closed");
+    }
+    return new SimpleTextTermVectorsReader(offsets, (IndexInput) in.clone());
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(in); 
+    } finally {
+      in = null;
+      offsets = null;
+    }
+  }
+  
+  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    if (info.getHasVectors()) {
+      files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_EXTENSION));
+    }
+  }
+  
+  private void readLine() throws IOException {
+    SimpleTextUtil.readLine(in, scratch);
+  }
+  
+  private int parseIntAt(int offset) throws IOException {
+    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
+    return ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+  }
+  
+  private String readString(int offset, BytesRef scratch) {
+    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
+    return scratchUTF16.toString();
+  }
+  
+  private class SimpleTVFields extends Fields {
+    private final SortedMap<String,SimpleTVTerms> fields;
+    
+    SimpleTVFields(SortedMap<String,SimpleTVTerms> fields) throws IOException {
+      this.fields = fields;
+    }
+
+    @Override
+    public FieldsEnum iterator() throws IOException {
+      return new FieldsEnum() {
+        private Iterator<Map.Entry<String,SimpleTVTerms>> iterator = fields.entrySet().iterator();
+        private Map.Entry<String,SimpleTVTerms> current = null;
+        
+        @Override
+        public String next() throws IOException {
+          if (!iterator.hasNext()) {
+            return null;
+          } else {
+            current = iterator.next();
+            return current.getKey();
+          }
+        }
+
+        @Override
+        public Terms terms() throws IOException {
+          return current.getValue();
+        }
+      };
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      return fields.get(field);
+    }
+
+    @Override
+    public int getUniqueFieldCount() throws IOException {
+      return fields.size();
+    }
+  }
+  
+  private static class SimpleTVTerms extends Terms {
+    final SortedMap<BytesRef,SimpleTVPostings> terms;
+    
+    SimpleTVTerms() {
+      terms = new TreeMap<BytesRef,SimpleTVPostings>();
+    }
+    
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      // TODO: reuse
+      return new SimpleTVTermsEnum(terms);
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() throws IOException {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public long getUniqueTermCount() throws IOException {
+      return terms.size();
+    }
+
+    @Override
+    public long getSumTotalTermFreq() throws IOException {
+      return -1;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return terms.size();
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return 1;
+    }
+  }
+  
+  private static class SimpleTVPostings {
+    private int freq;
+    private int positions[];
+    private int startOffsets[];
+    private int endOffsets[];
+  }
+  
+  private static class SimpleTVTermsEnum extends TermsEnum {
+    SortedMap<BytesRef,SimpleTVPostings> terms;
+    Iterator<Map.Entry<BytesRef,SimpleTextTermVectorsReader.SimpleTVPostings>> iterator;
+    Map.Entry<BytesRef,SimpleTextTermVectorsReader.SimpleTVPostings> current;
+    
+    SimpleTVTermsEnum(SortedMap<BytesRef,SimpleTVPostings> terms) {
+      this.terms = terms;
+      this.iterator = terms.entrySet().iterator();
+    }
+    
+    @Override
+    public SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException {
+      iterator = terms.tailMap(text).entrySet().iterator();
+      if (!iterator.hasNext()) {
+        return SeekStatus.END;
+      } else {
+        return next().equals(text) ? SeekStatus.FOUND : SeekStatus.NOT_FOUND;
+      }
+    }
+
+    @Override
+    public void seekExact(long ord) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      if (!iterator.hasNext()) {
+        return null;
+      } else {
+        current = iterator.next();
+        return current.getKey();
+      }
+    }
+
+    @Override
+    public BytesRef term() throws IOException {
+      return current.getKey();
+    }
+
+    @Override
+    public long ord() throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public int docFreq() throws IOException {
+      return 1;
+    }
+
+    @Override
+    public long totalTermFreq() throws IOException {
+      return current.getValue().freq;
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
+      // TODO: reuse
+      SimpleTVDocsEnum e = new SimpleTVDocsEnum();
+      e.reset(liveDocs, needsFreqs ? current.getValue().freq : -1);
+      return e;
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
+      SimpleTVPostings postings = current.getValue();
+      if (postings.positions == null && postings.startOffsets == null) {
+        return null;
+      }
+      // TODO: reuse
+      SimpleTVDocsAndPositionsEnum e = new SimpleTVDocsAndPositionsEnum(postings.startOffsets != null);
+      e.reset(liveDocs, postings.positions, postings.startOffsets, postings.endOffsets);
+      return e;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() throws IOException {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+  
+  // note: these two enum classes are exactly like the Default impl...
+  private static class SimpleTVDocsEnum extends DocsEnum {
+    private boolean didNext;
+    private int doc = -1;
+    private int freq;
+    private Bits liveDocs;
+
+    @Override
+    public int freq() {
+      assert freq != -1;
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return (doc = 0);
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+      if (!didNext && target == 0) {
+        return nextDoc();
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    public void reset(Bits liveDocs, int freq) {
+      this.liveDocs = liveDocs;
+      this.freq = freq;
+      this.doc = -1;
+      didNext = false;
+    }
+  }
+  
+  private static class SimpleTVDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private final OffsetAttribute offsetAtt;
+    private boolean didNext;
+    private int doc = -1;
+    private int nextPos;
+    private Bits liveDocs;
+    private int[] positions;
+    private int[] startOffsets;
+    private int[] endOffsets;
+
+    public SimpleTVDocsAndPositionsEnum(boolean storeOffsets) {
+      if (storeOffsets) {
+        offsetAtt = attributes().addAttribute(OffsetAttribute.class);
+      } else {
+        offsetAtt = null;
+      }
+    }
+
+    public boolean canReuse(boolean storeOffsets) {
+      return storeOffsets == (offsetAtt != null);
+    }
+
+    @Override
+    public int freq() {
+      if (positions != null) {
+        return positions.length;
+      } else {
+        assert startOffsets != null;
+        return startOffsets.length;
+      }
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return (doc = 0);
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+      if (!didNext && target == 0) {
+        return nextDoc();
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    public void reset(Bits liveDocs, int[] positions, int[] startOffsets, int[] endOffsets) {
+      this.liveDocs = liveDocs;
+      this.positions = positions;
+      this.startOffsets = startOffsets;
+      assert (offsetAtt != null) == (startOffsets != null);
+      this.endOffsets = endOffsets;
+      this.doc = -1;
+      didNext = false;
+      nextPos = 0;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      return null;
+    }
+
+    @Override
+    public boolean hasPayload() {
+      return false;
+    }
+
+    @Override
+    public int nextPosition() {
+      assert (positions != null && nextPos < positions.length) ||
+        startOffsets != null && nextPos < startOffsets.length;
+
+      if (startOffsets != null) {
+        offsetAtt.setOffset(startOffsets[nextPos],
+                            endOffsets[nextPos]);
+      }
+      if (positions != null) {
+        return positions[nextPos++];
+      } else {
+        nextPos++;
+        return -1;
+      }
+    }
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java
new file mode 100644
index 0000000..0a1836f
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java
@@ -0,0 +1,187 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Writes plain-text term vectors.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextTermVectorsWriter extends TermVectorsWriter {
+  
+  static final BytesRef END                = new BytesRef("END");
+  static final BytesRef DOC                = new BytesRef("doc ");
+  static final BytesRef NUMFIELDS          = new BytesRef("  numfields ");
+  static final BytesRef FIELD              = new BytesRef("  field ");
+  static final BytesRef FIELDNAME          = new BytesRef("    name ");
+  static final BytesRef FIELDPOSITIONS     = new BytesRef("    positions ");
+  static final BytesRef FIELDOFFSETS       = new BytesRef("    offsets   ");
+  static final BytesRef FIELDTERMCOUNT     = new BytesRef("    numterms ");
+  static final BytesRef TERMTEXT           = new BytesRef("    term ");
+  static final BytesRef TERMFREQ           = new BytesRef("      freq ");
+  static final BytesRef POSITION           = new BytesRef("      position ");
+  static final BytesRef STARTOFFSET        = new BytesRef("        startoffset ");
+  static final BytesRef ENDOFFSET          = new BytesRef("        endoffset ");
+
+  static final String VECTORS_EXTENSION = "vec";
+  
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput out;
+  private int numDocsWritten = 0;
+  private final BytesRef scratch = new BytesRef();
+  private boolean offsets;
+  private boolean positions;
+
+  public SimpleTextTermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    this.directory = directory;
+    this.segment = segment;
+    boolean success = false;
+    try {
+      out = directory.createOutput(IndexFileNames.segmentFileName(segment, "", VECTORS_EXTENSION), context);
+      success = true;
+    } finally {
+      if (!success) {
+        abort();
+      }
+    }
+  }
+  
+  @Override
+  public void startDocument(int numVectorFields) throws IOException {
+    write(DOC);
+    write(Integer.toString(numDocsWritten));
+    newLine();
+    
+    write(NUMFIELDS);
+    write(Integer.toString(numVectorFields));
+    newLine();
+    numDocsWritten++;
+  }
+
+  @Override
+  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets) throws IOException {  
+    write(FIELD);
+    write(Integer.toString(info.number));
+    newLine();
+    
+    write(FIELDNAME);
+    write(info.name);
+    newLine();
+    
+    write(FIELDPOSITIONS);
+    write(Boolean.toString(positions));
+    newLine();
+    
+    write(FIELDOFFSETS);
+    write(Boolean.toString(offsets));
+    newLine();
+    
+    write(FIELDTERMCOUNT);
+    write(Integer.toString(numTerms));
+    newLine();
+    
+    this.positions = positions;
+    this.offsets = offsets;
+  }
+
+  @Override
+  public void startTerm(BytesRef term, int freq) throws IOException {
+    write(TERMTEXT);
+    write(term);
+    newLine();
+    
+    write(TERMFREQ);
+    write(Integer.toString(freq));
+    newLine();
+  }
+
+  @Override
+  public void addPosition(int position, int startOffset, int endOffset) throws IOException {
+    assert positions || offsets;
+    
+    if (positions) {
+      write(POSITION);
+      write(Integer.toString(position));
+      newLine();
+    }
+    
+    if (offsets) {
+      write(STARTOFFSET);
+      write(Integer.toString(startOffset));
+      newLine();
+      
+      write(ENDOFFSET);
+      write(Integer.toString(endOffset));
+      newLine();
+    }
+  }
+
+  @Override
+  public void abort() {
+    try {
+      close();
+    } catch (IOException ignored) {}
+    
+    try {
+      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", VECTORS_EXTENSION));
+    } catch (IOException ignored) {}
+  }
+
+  @Override
+  public void finish(int numDocs) throws IOException {
+    if (numDocsWritten != numDocs) {
+      throw new RuntimeException("mergeVectors produced an invalid result: mergedDocs is " + numDocs + " but vec numDocs is " + numDocsWritten + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
+    }
+    write(END);
+    newLine();
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(out);
+    } finally {
+      out = null;
+    }
+  }
+  
+  private void write(String s) throws IOException {
+    SimpleTextUtil.write(out, s, scratch);
+  }
+  
+  private void write(BytesRef bytes) throws IOException {
+    SimpleTextUtil.write(out, bytes);
+  }
+  
+  private void newLine() throws IOException {
+    SimpleTextUtil.writeNewline(out);
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextUtil.java b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextUtil.java
new file mode 100644
index 0000000..cebc6df
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/SimpleTextUtil.java
@@ -0,0 +1,70 @@
+package org.apache.lucene.codecs.simpletext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.UnicodeUtil;
+
+class SimpleTextUtil {
+  public final static byte NEWLINE = 10;
+  public final static byte ESCAPE = 92;
+  
+  public static void write(DataOutput out, String s, BytesRef scratch) throws IOException {
+    UnicodeUtil.UTF16toUTF8(s, 0, s.length(), scratch);
+    write(out, scratch);
+  }
+
+  public static void write(DataOutput out, BytesRef b) throws IOException {
+    for(int i=0;i<b.length;i++) {
+      final byte bx = b.bytes[b.offset+i];
+      if (bx == NEWLINE || bx == ESCAPE) {
+        out.writeByte(ESCAPE);
+      }
+      out.writeByte(bx);
+    }
+  }
+
+  public static void writeNewline(DataOutput out) throws IOException {
+    out.writeByte(NEWLINE);
+  }
+  
+  public static void readLine(DataInput in, BytesRef scratch) throws IOException {
+    int upto = 0;
+    while(true) {
+      byte b = in.readByte();
+      if (scratch.bytes.length == upto) {
+        scratch.grow(1+upto);
+      }
+      if (b == ESCAPE) {
+        scratch.bytes[upto++] = in.readByte();
+      } else {
+        if (b == NEWLINE) {
+          break;
+        } else {
+          scratch.bytes[upto++] = b;
+        }
+      }
+    }
+    scratch.offset = 0;
+    scratch.length = upto;
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/codecs/simpletext/package.html b/lucene/src/java/org/apache/lucene/codecs/simpletext/package.html
new file mode 100644
index 0000000..88aad68
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/codecs/simpletext/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Simpletext Codec: writes human readable postings.
+</body>
+</html>
diff --git a/lucene/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/src/java/org/apache/lucene/index/CheckIndex.java
index 2f44530..fd668e2 100644
--- a/lucene/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/src/java/org/apache/lucene/index/CheckIndex.java
@@ -17,6 +17,8 @@ package org.apache.lucene.index;
  * limitations under the License.
  */
 
+import org.apache.lucene.codecs.BlockTreeTermsReader;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.FieldType; // for javadocs
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
@@ -27,7 +29,6 @@ import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.DocValues.SortedSource;
 import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.codecs.Codec;
 
 import java.io.File;
 import java.io.IOException;
@@ -48,9 +49,7 @@ import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.index.codecs.Codec;
 
-import org.apache.lucene.index.codecs.BlockTreeTermsReader;
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
diff --git a/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java b/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
index 90af216..ffb9869 100644
--- a/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
+++ b/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
@@ -24,12 +24,12 @@ import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Map;
 
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.codecs.PerDocConsumer;
 import org.apache.lucene.index.DocumentsWriterPerThread.DocState;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.DocValuesConsumer;
-import org.apache.lucene.index.codecs.DocValuesFormat;
-import org.apache.lucene.index.codecs.FieldInfosWriter;
-import org.apache.lucene.index.codecs.PerDocConsumer;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.ArrayUtil;
diff --git a/lucene/src/java/org/apache/lucene/index/DocValue.java b/lucene/src/java/org/apache/lucene/index/DocValue.java
index dacb7f6..4526197 100644
--- a/lucene/src/java/org/apache/lucene/index/DocValue.java
+++ b/lucene/src/java/org/apache/lucene/index/DocValue.java
@@ -18,10 +18,10 @@ package org.apache.lucene.index;
  */
 import java.util.Comparator;
 
+import org.apache.lucene.codecs.DocValuesConsumer;
 import org.apache.lucene.document.DocValuesField;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.index.DocValues.Type; // javadocs
-import org.apache.lucene.index.codecs.DocValuesConsumer; // javadocs
 import org.apache.lucene.util.BytesRef;
 
 /**
diff --git a/lucene/src/java/org/apache/lucene/index/DocValues.java b/lucene/src/java/org/apache/lucene/index/DocValues.java
index 7c5f007..2703cc1 100644
--- a/lucene/src/java/org/apache/lucene/index/DocValues.java
+++ b/lucene/src/java/org/apache/lucene/index/DocValues.java
@@ -20,8 +20,8 @@ import java.io.Closeable;
 import java.io.IOException;
 import java.util.Comparator;
 
+import org.apache.lucene.codecs.DocValuesFormat;
 import org.apache.lucene.document.DocValuesField;
-import org.apache.lucene.index.codecs.DocValuesFormat;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.packed.PackedInts;
 
diff --git a/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java b/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
index ab67e92..a084e03 100644
--- a/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -26,11 +26,11 @@ import java.util.Queue;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.DocumentsWriterPerThread.FlushedSegment;
 import org.apache.lucene.index.DocumentsWriterPerThread.IndexingChain;
 import org.apache.lucene.index.DocumentsWriterPerThreadPool.ThreadState;
 import org.apache.lucene.index.FieldInfos.FieldNumberBiMap;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.search.similarities.SimilarityProvider;
 import org.apache.lucene.store.AlreadyClosedException;
diff --git a/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java b/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
index 8f4a785..f279de0 100644
--- a/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
+++ b/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
@@ -24,8 +24,8 @@ import java.io.IOException;
 import java.text.NumberFormat;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.DocumentsWriterDeleteQueue.DeleteSlice;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.similarities.SimilarityProvider;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FlushInfo;
diff --git a/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter.java b/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
index 758123c..c68bcf0 100644
--- a/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriter.java
@@ -22,8 +22,8 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.codecs.FieldsConsumer;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CollectionUtil;
 import org.apache.lucene.util.IOUtils;
diff --git a/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java b/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java
index 2705de8..7915d27 100644
--- a/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java
+++ b/lucene/src/java/org/apache/lucene/index/FreqProxTermsWriterPerField.java
@@ -22,11 +22,11 @@ import java.util.Comparator;
 import java.util.Map;
 
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.PostingsConsumer;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsConsumer;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.PostingsConsumer;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.index.codecs.TermsConsumer;
 import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.FixedBitSet;
diff --git a/lucene/src/java/org/apache/lucene/index/IndexFileNames.java b/lucene/src/java/org/apache/lucene/index/IndexFileNames.java
index d5b3f1a..080a047 100644
--- a/lucene/src/java/org/apache/lucene/index/IndexFileNames.java
+++ b/lucene/src/java/org/apache/lucene/index/IndexFileNames.java
@@ -17,7 +17,7 @@ package org.apache.lucene.index;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.codecs.Codec;  // for javadocs
+import org.apache.lucene.codecs.Codec;
 
 // TODO: put all files under codec and remove all the static extensions here
 
diff --git a/lucene/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/src/java/org/apache/lucene/index/IndexWriter.java
index c344276..8a97cdc 100644
--- a/lucene/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/IndexWriter.java
@@ -32,12 +32,12 @@ import java.util.concurrent.atomic.AtomicInteger;
 import java.util.regex.Pattern;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.DocumentsWriterPerThread.FlushedSegment;
 import org.apache.lucene.index.FieldInfos.FieldNumberBiMap;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.index.MergeState.CheckAbort;
 import org.apache.lucene.index.PayloadProcessorProvider.DirPayloadProcessor;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.CompoundFileDirectory;
diff --git a/lucene/src/java/org/apache/lucene/index/IndexWriterConfig.java b/lucene/src/java/org/apache/lucene/index/IndexWriterConfig.java
index 5bd997c..add2fe6 100644
--- a/lucene/src/java/org/apache/lucene/index/IndexWriterConfig.java
+++ b/lucene/src/java/org/apache/lucene/index/IndexWriterConfig.java
@@ -20,9 +20,9 @@ package org.apache.lucene.index;
 import java.io.PrintStream;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.DocumentsWriterPerThread.IndexingChain;
 import org.apache.lucene.index.IndexWriter.IndexReaderWarmer;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.similarities.SimilarityProvider;
 import org.apache.lucene.util.InfoStream;
diff --git a/lucene/src/java/org/apache/lucene/index/NormsConsumer.java b/lucene/src/java/org/apache/lucene/index/NormsConsumer.java
index 48ace8f..ea20137 100644
--- a/lucene/src/java/org/apache/lucene/index/NormsConsumer.java
+++ b/lucene/src/java/org/apache/lucene/index/NormsConsumer.java
@@ -21,8 +21,8 @@ import java.io.IOException;
 import java.util.Collection;
 import java.util.Map;
 
-import org.apache.lucene.index.codecs.NormsFormat;
-import org.apache.lucene.index.codecs.NormsWriter;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.NormsWriter;
 import org.apache.lucene.util.IOUtils;
 
 // TODO FI: norms could actually be stored as doc store
diff --git a/lucene/src/java/org/apache/lucene/index/PerDocWriteState.java b/lucene/src/java/org/apache/lucene/index/PerDocWriteState.java
index bc22bbb..6b615ad 100644
--- a/lucene/src/java/org/apache/lucene/index/PerDocWriteState.java
+++ b/lucene/src/java/org/apache/lucene/index/PerDocWriteState.java
@@ -17,7 +17,7 @@ package org.apache.lucene.index;
  */
 import java.io.PrintStream;
 
-import org.apache.lucene.index.codecs.PerDocConsumer;
+import org.apache.lucene.codecs.PerDocConsumer;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.Counter;
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java b/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java
index 232496b..be9aae8 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentCoreReaders.java
@@ -22,14 +22,14 @@ import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.atomic.AtomicInteger;
 
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.NormsReader;
+import org.apache.lucene.codecs.PerDocProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.index.SegmentReader.CoreClosedListener;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.NormsReader;
-import org.apache.lucene.index.codecs.PerDocProducer;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.StoredFieldsReader;
-import org.apache.lucene.index.codecs.TermVectorsReader;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentInfo.java b/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
index 4e103cb..1da4808 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
@@ -27,8 +27,8 @@ import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
 
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.FieldInfosReader;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentInfos.java b/lucene/src/java/org/apache/lucene/index/SegmentInfos.java
index 6651e17..4014dd5 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentInfos.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentInfos.java
@@ -31,10 +31,10 @@ import java.util.List;
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.SegmentInfosReader;
+import org.apache.lucene.codecs.SegmentInfosWriter;
 import org.apache.lucene.index.FieldInfos.FieldNumberBiMap;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.SegmentInfosReader;
-import org.apache.lucene.index.codecs.SegmentInfosWriter;
 import org.apache.lucene.store.ChecksumIndexInput;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
index c338e3c..76f705b0 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -24,16 +24,16 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.NormsWriter;
+import org.apache.lucene.codecs.PerDocConsumer;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.codecs.TermVectorsWriter;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.IndexReader.FieldOption;
 import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.FieldInfosWriter;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.NormsWriter;
-import org.apache.lucene.index.codecs.StoredFieldsWriter;
-import org.apache.lucene.index.codecs.PerDocConsumer;
-import org.apache.lucene.index.codecs.TermVectorsWriter;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.Bits;
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentReader.java b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
index f7c97fb..8f0f6ad 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
@@ -26,10 +26,10 @@ import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.codecs.PerDocProducer;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.codecs.PerDocProducer;
-import org.apache.lucene.index.codecs.StoredFieldsReader;
-import org.apache.lucene.index.codecs.TermVectorsReader;
 import org.apache.lucene.search.FieldCache; // javadocs
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.BitVector;
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java b/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java
index 5402973..5898ca5 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentWriteState.java
@@ -17,7 +17,7 @@ package org.apache.lucene.index;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.BitVector;
diff --git a/lucene/src/java/org/apache/lucene/index/StoredFieldsConsumer.java b/lucene/src/java/org/apache/lucene/index/StoredFieldsConsumer.java
index 874020c..22cc289 100644
--- a/lucene/src/java/org/apache/lucene/index/StoredFieldsConsumer.java
+++ b/lucene/src/java/org/apache/lucene/index/StoredFieldsConsumer.java
@@ -19,8 +19,8 @@ package org.apache.lucene.index;
 
 import java.io.IOException;
 
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.StoredFieldsWriter;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.StoredFieldsWriter;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.RamUsageEstimator;
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorsConsumer.java b/lucene/src/java/org/apache/lucene/index/TermVectorsConsumer.java
index 93c8c02..a0655d9 100644
--- a/lucene/src/java/org/apache/lucene/index/TermVectorsConsumer.java
+++ b/lucene/src/java/org/apache/lucene/index/TermVectorsConsumer.java
@@ -20,7 +20,7 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.util.Map;
 
-import org.apache.lucene.index.codecs.TermVectorsWriter;
+import org.apache.lucene.codecs.TermVectorsWriter;
 import org.apache.lucene.store.FlushInfo;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.util.ArrayUtil;
diff --git a/lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java b/lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java
index a38b7db..4351053 100644
--- a/lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java
+++ b/lucene/src/java/org/apache/lucene/index/TermVectorsConsumerPerField.java
@@ -20,7 +20,7 @@ package org.apache.lucene.index;
 import java.io.IOException;
 
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.index.codecs.TermVectorsWriter;
+import org.apache.lucene.codecs.TermVectorsWriter;
 import org.apache.lucene.util.ByteBlockPool;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.RamUsageEstimator;
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/BlockTermState.java b/lucene/src/java/org/apache/lucene/index/codecs/BlockTermState.java
deleted file mode 100644
index 2a070f2..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/BlockTermState.java
+++ /dev/null
@@ -1,54 +0,0 @@
-package org.apache.lucene.index.codecs;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocsEnum; // javadocs
-import org.apache.lucene.index.OrdTermState;
-import org.apache.lucene.index.TermState;
-
-/**
- * Holds all state required for {@link PostingsReaderBase}
- * to produce a {@link DocsEnum} without re-seeking the
- * terms dict.
- */
-public class BlockTermState extends OrdTermState {
-  public int docFreq;            // how many docs have this term
-  public long totalTermFreq;     // total number of occurrences of this term
-
-  public int termBlockOrd;          // the term's ord in the current block
-  public long blockFilePointer;  // fp into the terms dict primary file (_X.tim) that holds this term
-
-  @Override
-  public void copyFrom(TermState _other) {
-    assert _other instanceof BlockTermState : "can not copy from " + _other.getClass().getName();
-    BlockTermState other = (BlockTermState) _other;
-    super.copyFrom(_other);
-    docFreq = other.docFreq;
-    totalTermFreq = other.totalTermFreq;
-    termBlockOrd = other.termBlockOrd;
-    blockFilePointer = other.blockFilePointer;
-
-    // NOTE: don't copy blockTermCount;
-    // it's "transient": used only by the "primary"
-    // termState, and regenerated on seek by TermState
-  }
-
-  @Override
-  public String toString() {
-    return "docFreq=" + docFreq + " totalTermFreq=" + totalTermFreq + " termBlockOrd=" + termBlockOrd + " blockFP=" + blockFilePointer;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.java
deleted file mode 100644
index 8b6c5c3..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/BlockTermsReader.java
+++ /dev/null
@@ -1,876 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.TreeMap;
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.DoubleBarrelLRUCache;
-
-/** Handles a terms dict, but decouples all details of
- *  doc/freqs/positions reading to an instance of {@link
- *  PostingsReaderBase}.  This class is reusable for
- *  codecs that use a different format for
- *  docs/freqs/positions (though codecs are also free to
- *  make their own terms dict impl).
- *
- * <p>This class also interacts with an instance of {@link
- * TermsIndexReaderBase}, to abstract away the specific
- * implementation of the terms dict index. 
- * @lucene.experimental */
-
-public class BlockTermsReader extends FieldsProducer {
-  // Open input to the main terms dict file (_X.tis)
-  private final IndexInput in;
-
-  // Reads the terms dict entries, to gather state to
-  // produce DocsEnum on demand
-  private final PostingsReaderBase postingsReader;
-
-  private final TreeMap<String,FieldReader> fields = new TreeMap<String,FieldReader>();
-
-  // Caches the most recently looked-up field + terms:
-  private final DoubleBarrelLRUCache<FieldAndTerm,BlockTermState> termsCache;
-
-  // Reads the terms index
-  private TermsIndexReaderBase indexReader;
-
-  // keeps the dirStart offset
-  protected long dirOffset;
-
-  // Used as key for the terms cache
-  private static class FieldAndTerm extends DoubleBarrelLRUCache.CloneableKey {
-    String field;
-    BytesRef term;
-
-    public FieldAndTerm() {
-    }
-
-    public FieldAndTerm(FieldAndTerm other) {
-      field = other.field;
-      term = BytesRef.deepCopyOf(other.term);
-    }
-
-    @Override
-    public boolean equals(Object _other) {
-      FieldAndTerm other = (FieldAndTerm) _other;
-      return other.field.equals(field) && term.bytesEquals(other.term);
-    }
-
-    @Override
-    public Object clone() {
-      return new FieldAndTerm(this);
-    }
-
-    @Override
-    public int hashCode() {
-      return field.hashCode() * 31 + term.hashCode();
-    }
-  }
-  
-  // private String segment;
-  
-  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, String segment, PostingsReaderBase postingsReader, IOContext context,
-                          int termsCacheSize, String segmentSuffix)
-    throws IOException {
-    
-    this.postingsReader = postingsReader;
-    termsCache = new DoubleBarrelLRUCache<FieldAndTerm,BlockTermState>(termsCacheSize);
-
-    // this.segment = segment;
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),
-                       context);
-
-    boolean success = false;
-    try {
-      readHeader(in);
-
-      // Have PostingsReader init itself
-      postingsReader.init(in);
-
-      // Read per-field details
-      seekDir(in, dirOffset);
-
-      final int numFields = in.readVInt();
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final long numTerms = in.readVLong();
-        assert numTerms >= 0;
-        final long termsStartPointer = in.readVLong();
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        final long sumTotalTermFreq = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
-        final long sumDocFreq = in.readVLong();
-        final int docCount = in.readVInt();
-        assert !fields.containsKey(fieldInfo.name);
-        fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        in.close();
-      }
-    }
-
-    this.indexReader = indexReader;
-  }
-
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, BlockTermsWriter.CODEC_NAME,
-                          BlockTermsWriter.VERSION_START,
-                          BlockTermsWriter.VERSION_CURRENT);
-    dirOffset = input.readLong();
-  }
-  
-  protected void seekDir(IndexInput input, long dirOffset)
-      throws IOException {
-    input.seek(dirOffset);
-  }
-  
-  @Override
-  public void close() throws IOException {
-    try {
-      try {
-        if (indexReader != null) {
-          indexReader.close();
-        }
-      } finally {
-        // null so if an app hangs on to us (ie, we are not
-        // GCable, despite being closed) we still free most
-        // ram
-        indexReader = null;
-        if (in != null) {
-          in.close();
-        }
-      }
-    } finally {
-      if (postingsReader != null) {
-        postingsReader.close();
-      }
-    }
-  }
-
-  public static void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) {
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION));
-  }
-
-  @Override
-  public FieldsEnum iterator() {
-    return new TermFieldsEnum();
-  }
-
-  @Override
-  public Terms terms(String field) throws IOException {
-    return fields.get(field);
-  }
-
-  @Override
-  public int getUniqueFieldCount() {
-    return fields.size();
-  }
-
-  // Iterates through all fields
-  private class TermFieldsEnum extends FieldsEnum {
-    final Iterator<FieldReader> it;
-    FieldReader current;
-
-    TermFieldsEnum() {
-      it = fields.values().iterator();
-    }
-
-    @Override
-    public String next() {
-      if (it.hasNext()) {
-        current = it.next();
-        return current.fieldInfo.name;
-      } else {
-        current = null;
-        return null;
-      }
-    }
-    
-    @Override
-    public Terms terms() throws IOException {
-      return current;
-    }
-  }
-
-  private class FieldReader extends Terms {
-    final long numTerms;
-    final FieldInfo fieldInfo;
-    final long termsStartPointer;
-    final long sumTotalTermFreq;
-    final long sumDocFreq;
-    final int docCount;
-
-    FieldReader(FieldInfo fieldInfo, long numTerms, long termsStartPointer, long sumTotalTermFreq, long sumDocFreq, int docCount) {
-      assert numTerms > 0;
-      this.fieldInfo = fieldInfo;
-      this.numTerms = numTerms;
-      this.termsStartPointer = termsStartPointer;
-      this.sumTotalTermFreq = sumTotalTermFreq;
-      this.sumDocFreq = sumDocFreq;
-      this.docCount = docCount;
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      return new SegmentTermsEnum();
-    }
-
-    @Override
-    public long getUniqueTermCount() {
-      return numTerms;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return sumTotalTermFreq;
-    }
-
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return sumDocFreq;
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return docCount;
-    }
-
-    // Iterates through terms in this field
-    private final class SegmentTermsEnum extends TermsEnum {
-      private final IndexInput in;
-      private final BlockTermState state;
-      private final boolean doOrd;
-      private final FieldAndTerm fieldTerm = new FieldAndTerm();
-      private final TermsIndexReaderBase.FieldIndexEnum indexEnum;
-      private final BytesRef term = new BytesRef();
-
-      /* This is true if indexEnum is "still" seek'd to the index term
-         for the current term. We set it to true on seeking, and then it
-         remains valid until next() is called enough times to load another
-         terms block: */
-      private boolean indexIsCurrent;
-
-      /* True if we've already called .next() on the indexEnum, to "bracket"
-         the current block of terms: */
-      private boolean didIndexNext;
-
-      /* Next index term, bracketing the current block of terms; this is
-         only valid if didIndexNext is true: */
-      private BytesRef nextIndexTerm;
-
-      /* True after seekExact(TermState), do defer seeking.  If the app then
-         calls next() (which is not "typical"), then we'll do the real seek */
-      private boolean seekPending;
-
-      /* How many blocks we've read since last seek.  Once this
-         is >= indexEnum.getDivisor() we set indexIsCurrent to false (since
-         the index can no long bracket seek-within-block). */
-      private int blocksSinceSeek;
-
-      private byte[] termSuffixes;
-      private ByteArrayDataInput termSuffixesReader = new ByteArrayDataInput();
-
-      /* Common prefix used for all terms in this block. */
-      private int termBlockPrefix;
-
-      /* How many terms in current block */
-      private int blockTermCount;
-
-      private byte[] docFreqBytes;
-      private final ByteArrayDataInput freqReader = new ByteArrayDataInput();
-      private int metaDataUpto;
-
-      public SegmentTermsEnum() throws IOException {
-        in = (IndexInput) BlockTermsReader.this.in.clone();
-        in.seek(termsStartPointer);
-        indexEnum = indexReader.getFieldEnum(fieldInfo);
-        doOrd = indexReader.supportsOrd();
-        fieldTerm.field = fieldInfo.name;
-        state = postingsReader.newTermState();
-        state.totalTermFreq = -1;
-        state.ord = -1;
-
-        termSuffixes = new byte[128];
-        docFreqBytes = new byte[64];
-        //System.out.println("BTR.enum init this=" + this + " postingsReader=" + postingsReader);
-      }
-
-      @Override
-      public Comparator<BytesRef> getComparator() {
-        return BytesRef.getUTF8SortedAsUnicodeComparator();
-      }
-
-      // TODO: we may want an alternate mode here which is
-      // "if you are about to return NOT_FOUND I won't use
-      // the terms data from that"; eg FuzzyTermsEnum will
-      // (usually) just immediately call seek again if we
-      // return NOT_FOUND so it's a waste for us to fill in
-      // the term that was actually NOT_FOUND
-      @Override
-      public SeekStatus seekCeil(final BytesRef target, final boolean useCache) throws IOException {
-
-        if (indexEnum == null) {
-          throw new IllegalStateException("terms index was not loaded");
-        }
-   
-        //System.out.println("BTR.seek seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + term().utf8ToString() + " " + term() + " useCache=" + useCache + " indexIsCurrent=" + indexIsCurrent + " didIndexNext=" + didIndexNext + " seekPending=" + seekPending + " divisor=" + indexReader.getDivisor() + " this="  + this);
-        if (didIndexNext) {
-          if (nextIndexTerm == null) {
-            //System.out.println("  nextIndexTerm=null");
-          } else {
-            //System.out.println("  nextIndexTerm=" + nextIndexTerm.utf8ToString());
-          }
-        }
-
-        // Check cache
-        if (useCache) {
-          fieldTerm.term = target;
-          // TODO: should we differentiate "frozen"
-          // TermState (ie one that was cloned and
-          // cached/returned by termState()) from the
-          // malleable (primary) one?
-          final TermState cachedState = termsCache.get(fieldTerm);
-          if (cachedState != null) {
-            seekPending = true;
-            //System.out.println("  cached!");
-            seekExact(target, cachedState);
-            //System.out.println("  term=" + term.utf8ToString());
-            return SeekStatus.FOUND;
-          }
-        }
-
-        boolean doSeek = true;
-
-        // See if we can avoid seeking, because target term
-        // is after current term but before next index term:
-        if (indexIsCurrent) {
-
-          final int cmp = BytesRef.getUTF8SortedAsUnicodeComparator().compare(term, target);
-
-          if (cmp == 0) {
-            // Already at the requested term
-            return SeekStatus.FOUND;
-          } else if (cmp < 0) {
-
-            // Target term is after current term
-            if (!didIndexNext) {
-              if (indexEnum.next() == -1) {
-                nextIndexTerm = null;
-              } else {
-                nextIndexTerm = indexEnum.term();
-              }
-              //System.out.println("  now do index next() nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
-              didIndexNext = true;
-            }
-
-            if (nextIndexTerm == null || BytesRef.getUTF8SortedAsUnicodeComparator().compare(target, nextIndexTerm) < 0) {
-              // Optimization: requested term is within the
-              // same term block we are now in; skip seeking
-              // (but do scanning):
-              doSeek = false;
-              //System.out.println("  skip seek: nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
-            }
-          }
-        }
-
-        if (doSeek) {
-          //System.out.println("  seek");
-
-          // Ask terms index to find biggest indexed term (=
-          // first term in a block) that's <= our text:
-          in.seek(indexEnum.seek(target));
-          boolean result = nextBlock();
-
-          // Block must exist since, at least, the indexed term
-          // is in the block:
-          assert result;
-
-          indexIsCurrent = true;
-          didIndexNext = false;
-          blocksSinceSeek = 0;          
-
-          if (doOrd) {
-            state.ord = indexEnum.ord()-1;
-          }
-
-          term.copyBytes(indexEnum.term());
-          //System.out.println("  seek: term=" + term.utf8ToString());
-        } else {
-          //System.out.println("  skip seek");
-          if (state.termBlockOrd == blockTermCount && !nextBlock()) {
-            indexIsCurrent = false;
-            return SeekStatus.END;
-          }
-        }
-
-        seekPending = false;
-
-        int common = 0;
-
-        // Scan within block.  We could do this by calling
-        // _next() and testing the resulting term, but this
-        // is wasteful.  Instead, we first confirm the
-        // target matches the common prefix of this block,
-        // and then we scan the term bytes directly from the
-        // termSuffixesreader's byte[], saving a copy into
-        // the BytesRef term per term.  Only when we return
-        // do we then copy the bytes into the term.
-
-        while(true) {
-
-          // First, see if target term matches common prefix
-          // in this block:
-          if (common < termBlockPrefix) {
-            final int cmp = (term.bytes[common]&0xFF) - (target.bytes[target.offset + common]&0xFF);
-            if (cmp < 0) {
-
-              // TODO: maybe we should store common prefix
-              // in block header?  (instead of relying on
-              // last term of previous block)
-
-              // Target's prefix is after the common block
-              // prefix, so term cannot be in this block
-              // but it could be in next block.  We
-              // must scan to end-of-block to set common
-              // prefix for next block:
-              if (state.termBlockOrd < blockTermCount) {
-                while(state.termBlockOrd < blockTermCount-1) {
-                  state.termBlockOrd++;
-                  state.ord++;
-                  termSuffixesReader.skipBytes(termSuffixesReader.readVInt());
-                }
-                final int suffix = termSuffixesReader.readVInt();
-                term.length = termBlockPrefix + suffix;
-                if (term.bytes.length < term.length) {
-                  term.grow(term.length);
-                }
-                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-              }
-              state.ord++;
-              
-              if (!nextBlock()) {
-                indexIsCurrent = false;
-                return SeekStatus.END;
-              }
-              common = 0;
-
-            } else if (cmp > 0) {
-              // Target's prefix is before the common prefix
-              // of this block, so we position to start of
-              // block and return NOT_FOUND:
-              assert state.termBlockOrd == 0;
-
-              final int suffix = termSuffixesReader.readVInt();
-              term.length = termBlockPrefix + suffix;
-              if (term.bytes.length < term.length) {
-                term.grow(term.length);
-              }
-              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-              return SeekStatus.NOT_FOUND;
-            } else {
-              common++;
-            }
-
-            continue;
-          }
-
-          // Test every term in this block
-          while (true) {
-            state.termBlockOrd++;
-            state.ord++;
-
-            final int suffix = termSuffixesReader.readVInt();
-            
-            // We know the prefix matches, so just compare the new suffix:
-            final int termLen = termBlockPrefix + suffix;
-            int bytePos = termSuffixesReader.getPosition();
-
-            boolean next = false;
-            final int limit = target.offset + (termLen < target.length ? termLen : target.length);
-            int targetPos = target.offset + termBlockPrefix;
-            while(targetPos < limit) {
-              final int cmp = (termSuffixes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
-              if (cmp < 0) {
-                // Current term is still before the target;
-                // keep scanning
-                next = true;
-                break;
-              } else if (cmp > 0) {
-                // Done!  Current term is after target. Stop
-                // here, fill in real term, return NOT_FOUND.
-                term.length = termBlockPrefix + suffix;
-                if (term.bytes.length < term.length) {
-                  term.grow(term.length);
-                }
-                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-                //System.out.println("  NOT_FOUND");
-                return SeekStatus.NOT_FOUND;
-              }
-            }
-
-            if (!next && target.length <= termLen) {
-              term.length = termBlockPrefix + suffix;
-              if (term.bytes.length < term.length) {
-                term.grow(term.length);
-              }
-              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-
-              if (target.length == termLen) {
-                // Done!  Exact match.  Stop here, fill in
-                // real term, return FOUND.
-                //System.out.println("  FOUND");
-
-                if (useCache) {
-                  // Store in cache
-                  decodeMetaData();
-                  //System.out.println("  cache! state=" + state);
-                  termsCache.put(new FieldAndTerm(fieldTerm), (BlockTermState) state.clone());
-                }
-
-                return SeekStatus.FOUND;
-              } else {
-                //System.out.println("  NOT_FOUND");
-                return SeekStatus.NOT_FOUND;
-              }
-            }
-
-            if (state.termBlockOrd == blockTermCount) {
-              // Must pre-fill term for next block's common prefix
-              term.length = termBlockPrefix + suffix;
-              if (term.bytes.length < term.length) {
-                term.grow(term.length);
-              }
-              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-              break;
-            } else {
-              termSuffixesReader.skipBytes(suffix);
-            }
-          }
-
-          // The purpose of the terms dict index is to seek
-          // the enum to the closest index term before the
-          // term we are looking for.  So, we should never
-          // cross another index term (besides the first
-          // one) while we are scanning:
-
-          assert indexIsCurrent;
-
-          if (!nextBlock()) {
-            //System.out.println("  END");
-            indexIsCurrent = false;
-            return SeekStatus.END;
-          }
-          common = 0;
-        }
-      }
-
-      @Override
-      public BytesRef next() throws IOException {
-        //System.out.println("BTR.next() seekPending=" + seekPending + " pendingSeekCount=" + state.termBlockOrd);
-
-        // If seek was previously called and the term was cached,
-        // usually caller is just going to pull a D/&PEnum or get
-        // docFreq, etc.  But, if they then call next(),
-        // this method catches up all internal state so next()
-        // works properly:
-        if (seekPending) {
-          assert !indexIsCurrent;
-          in.seek(state.blockFilePointer);
-          final int pendingSeekCount = state.termBlockOrd;
-          boolean result = nextBlock();
-
-          final long savOrd = state.ord;
-
-          // Block must exist since seek(TermState) was called w/ a
-          // TermState previously returned by this enum when positioned
-          // on a real term:
-          assert result;
-
-          while(state.termBlockOrd < pendingSeekCount) {
-            BytesRef nextResult = _next();
-            assert nextResult != null;
-          }
-          seekPending = false;
-          state.ord = savOrd;
-        }
-        return _next();
-      }
-
-      /* Decodes only the term bytes of the next term.  If caller then asks for
-         metadata, ie docFreq, totalTermFreq or pulls a D/&PEnum, we then (lazily)
-         decode all metadata up to the current term. */
-      private BytesRef _next() throws IOException {
-        //System.out.println("BTR._next seg=" + segment + " this=" + this + " termCount=" + state.termBlockOrd + " (vs " + blockTermCount + ")");
-        if (state.termBlockOrd == blockTermCount && !nextBlock()) {
-          //System.out.println("  eof");
-          indexIsCurrent = false;
-          return null;
-        }
-
-        // TODO: cutover to something better for these ints!  simple64?
-        final int suffix = termSuffixesReader.readVInt();
-        //System.out.println("  suffix=" + suffix);
-
-        term.length = termBlockPrefix + suffix;
-        if (term.bytes.length < term.length) {
-          term.grow(term.length);
-        }
-        termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-        state.termBlockOrd++;
-
-        // NOTE: meaningless in the non-ord case
-        state.ord++;
-
-        //System.out.println("  return term=" + fieldInfo.name + ":" + term.utf8ToString() + " " + term + " tbOrd=" + state.termBlockOrd);
-        return term;
-      }
-
-      @Override
-      public BytesRef term() {
-        return term;
-      }
-
-      @Override
-      public int docFreq() throws IOException {
-        //System.out.println("BTR.docFreq");
-        decodeMetaData();
-        //System.out.println("  return " + state.docFreq);
-        return state.docFreq;
-      }
-
-      @Override
-      public long totalTermFreq() throws IOException {
-        decodeMetaData();
-        return state.totalTermFreq;
-      }
-
-      @Override
-      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-        //System.out.println("BTR.docs this=" + this);
-        decodeMetaData();
-        //System.out.println("BTR.docs:  state.docFreq=" + state.docFreq);
-        return postingsReader.docs(fieldInfo, state, liveDocs, reuse, needsFreqs);
-      }
-
-      @Override
-      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
-        //System.out.println("BTR.d&p this=" + this);
-        decodeMetaData();
-        if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-          return null;
-        } else {
-          DocsAndPositionsEnum dpe = postingsReader.docsAndPositions(fieldInfo, state, liveDocs, reuse);
-          //System.out.println("  return d&pe=" + dpe);
-          return dpe;
-        }
-      }
-
-      @Override
-      public void seekExact(BytesRef target, TermState otherState) throws IOException {
-        //System.out.println("BTR.seekExact termState target=" + target.utf8ToString() + " " + target + " this=" + this);
-        assert otherState != null && otherState instanceof BlockTermState;
-        assert !doOrd || ((BlockTermState) otherState).ord < numTerms;
-        state.copyFrom(otherState);
-        seekPending = true;
-        indexIsCurrent = false;
-        term.copyBytes(target);
-      }
-      
-      @Override
-      public TermState termState() throws IOException {
-        //System.out.println("BTR.termState this=" + this);
-        decodeMetaData();
-        TermState ts = (TermState) state.clone();
-        //System.out.println("  return ts=" + ts);
-        return ts;
-      }
-
-      @Override
-      public void seekExact(long ord) throws IOException {
-        //System.out.println("BTR.seek by ord ord=" + ord);
-        if (indexEnum == null) {
-          throw new IllegalStateException("terms index was not loaded");
-        }
-
-        assert ord < numTerms;
-
-        // TODO: if ord is in same terms block and
-        // after current ord, we should avoid this seek just
-        // like we do in the seek(BytesRef) case
-        in.seek(indexEnum.seek(ord));
-        boolean result = nextBlock();
-
-        // Block must exist since ord < numTerms:
-        assert result;
-
-        indexIsCurrent = true;
-        didIndexNext = false;
-        blocksSinceSeek = 0;
-        seekPending = false;
-
-        state.ord = indexEnum.ord()-1;
-        assert state.ord >= -1: "ord=" + state.ord;
-        term.copyBytes(indexEnum.term());
-
-        // Now, scan:
-        int left = (int) (ord - state.ord);
-        while(left > 0) {
-          final BytesRef term = _next();
-          assert term != null;
-          left--;
-          assert indexIsCurrent;
-        }
-      }
-
-      @Override
-      public long ord() {
-        if (!doOrd) {
-          throw new UnsupportedOperationException();
-        }
-        return state.ord;
-      }
-
-      private void doPendingSeek() {
-      }
-
-      /* Does initial decode of next block of terms; this
-         doesn't actually decode the docFreq, totalTermFreq,
-         postings details (frq/prx offset, etc.) metadata;
-         it just loads them as byte[] blobs which are then      
-         decoded on-demand if the metadata is ever requested
-         for any term in this block.  This enables terms-only
-         intensive consumes (eg certain MTQs, respelling) to
-         not pay the price of decoding metadata they won't
-         use. */
-      private boolean nextBlock() throws IOException {
-
-        // TODO: we still lazy-decode the byte[] for each
-        // term (the suffix), but, if we decoded
-        // all N terms up front then seeking could do a fast
-        // bsearch w/in the block...
-
-        //System.out.println("BTR.nextBlock() fp=" + in.getFilePointer() + " this=" + this);
-        state.blockFilePointer = in.getFilePointer();
-        blockTermCount = in.readVInt();
-        //System.out.println("  blockTermCount=" + blockTermCount);
-        if (blockTermCount == 0) {
-          return false;
-        }
-        termBlockPrefix = in.readVInt();
-
-        // term suffixes:
-        int len = in.readVInt();
-        if (termSuffixes.length < len) {
-          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];
-        }
-        //System.out.println("  termSuffixes len=" + len);
-        in.readBytes(termSuffixes, 0, len);
-        termSuffixesReader.reset(termSuffixes, 0, len);
-
-        // docFreq, totalTermFreq
-        len = in.readVInt();
-        if (docFreqBytes.length < len) {
-          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];
-        }
-        //System.out.println("  freq bytes len=" + len);
-        in.readBytes(docFreqBytes, 0, len);
-        freqReader.reset(docFreqBytes, 0, len);
-        metaDataUpto = 0;
-
-        state.termBlockOrd = 0;
-
-        postingsReader.readTermsBlock(in, fieldInfo, state);
-
-        blocksSinceSeek++;
-        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());
-        //System.out.println("  indexIsCurrent=" + indexIsCurrent);
-
-        return true;
-      }
-     
-      private void decodeMetaData() throws IOException {
-        //System.out.println("BTR.decodeMetadata mdUpto=" + metaDataUpto + " vs termCount=" + state.termBlockOrd + " state=" + state);
-        if (!seekPending) {
-          // TODO: cutover to random-access API
-          // here.... really stupid that we have to decode N
-          // wasted term metadata just to get to the N+1th
-          // that we really need...
-
-          // lazily catch up on metadata decode:
-          final int limit = state.termBlockOrd;
-          // We must set/incr state.termCount because
-          // postings impl can look at this
-          state.termBlockOrd = metaDataUpto;
-          // TODO: better API would be "jump straight to term=N"???
-          while (metaDataUpto < limit) {
-            //System.out.println("  decode mdUpto=" + metaDataUpto);
-            // TODO: we could make "tiers" of metadata, ie,
-            // decode docFreq/totalTF but don't decode postings
-            // metadata; this way caller could get
-            // docFreq/totalTF w/o paying decode cost for
-            // postings
-
-            // TODO: if docFreq were bulk decoded we could
-            // just skipN here:
-            state.docFreq = freqReader.readVInt();
-            //System.out.println("    dF=" + state.docFreq);
-            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
-              state.totalTermFreq = state.docFreq + freqReader.readVLong();
-              //System.out.println("    totTF=" + state.totalTermFreq);
-            }
-
-            postingsReader.nextTerm(fieldInfo, state);
-            metaDataUpto++;
-            state.termBlockOrd++;
-          }
-        } else {
-          //System.out.println("  skip! seekPending");
-        }
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/BlockTermsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/BlockTermsWriter.java
deleted file mode 100644
index 4d5c71c..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/BlockTermsWriter.java
+++ /dev/null
@@ -1,319 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-// TODO: currently we encode all terms between two indexed
-// terms as a block; but, we could decouple the two, ie
-// allow several blocks in between two indexed terms
-
-/**
- * Writes terms dict, block-encoding (column stride) each
- * term's metadata for each set of terms between two
- * index terms.
- *
- * @lucene.experimental
- */
-
-public class BlockTermsWriter extends FieldsConsumer {
-
-  final static String CODEC_NAME = "BLOCK_TERMS_DICT";
-
-  // Initial format
-  public static final int VERSION_START = 0;
-
-  public static final int VERSION_CURRENT = VERSION_START;
-
-  /** Extension of terms file */
-  static final String TERMS_EXTENSION = "tib";
-
-  protected final IndexOutput out;
-  final PostingsWriterBase postingsWriter;
-  final FieldInfos fieldInfos;
-  FieldInfo currentField;
-  private final TermsIndexWriterBase termsIndexWriter;
-  private final List<TermsWriter> fields = new ArrayList<TermsWriter>();
-
-  // private final String segment;
-
-  public BlockTermsWriter(TermsIndexWriterBase termsIndexWriter,
-      SegmentWriteState state, PostingsWriterBase postingsWriter)
-      throws IOException {
-    final String termsFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_EXTENSION);
-    this.termsIndexWriter = termsIndexWriter;
-    out = state.directory.createOutput(termsFileName, state.context);
-    boolean success = false;
-    try {
-      fieldInfos = state.fieldInfos;
-      writeHeader(out);
-      currentField = null;
-      this.postingsWriter = postingsWriter;
-      // segment = state.segmentName;
-      
-      //System.out.println("BTW.init seg=" + state.segmentName);
-      
-      postingsWriter.start(out); // have consumer write its format/header
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); 
-
-    out.writeLong(0);                             // leave space for end index pointer    
-  }
-
-  @Override
-  public TermsConsumer addField(FieldInfo field) throws IOException {
-    //System.out.println("\nBTW.addField seg=" + segment + " field=" + field.name);
-    assert currentField == null || currentField.name.compareTo(field.name) < 0;
-    currentField = field;
-    TermsIndexWriterBase.FieldWriter fieldIndexWriter = termsIndexWriter.addField(field, out.getFilePointer());
-    final TermsWriter terms = new TermsWriter(fieldIndexWriter, field, postingsWriter);
-    fields.add(terms);
-    return terms;
-  }
-
-  @Override
-  public void close() throws IOException {
-
-    try {
-      
-      int nonZeroCount = 0;
-      for(TermsWriter field : fields) {
-        if (field.numTerms > 0) {
-          nonZeroCount++;
-        }
-      }
-
-      final long dirStart = out.getFilePointer();
-
-      out.writeVInt(nonZeroCount);
-      for(TermsWriter field : fields) {
-        if (field.numTerms > 0) {
-          out.writeVInt(field.fieldInfo.number);
-          out.writeVLong(field.numTerms);
-          out.writeVLong(field.termsStartPointer);
-          if (field.fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
-            out.writeVLong(field.sumTotalTermFreq);
-          }
-          out.writeVLong(field.sumDocFreq);
-          out.writeVInt(field.docCount);
-        }
-      }
-      writeTrailer(dirStart);
-    } finally {
-      IOUtils.close(out, postingsWriter, termsIndexWriter);
-    }
-  }
-
-  protected void writeTrailer(long dirStart) throws IOException {
-    out.seek(CodecUtil.headerLength(CODEC_NAME));
-    out.writeLong(dirStart);    
-  }
-  
-  private static class TermEntry {
-    public final BytesRef term = new BytesRef();
-    public TermStats stats;
-  }
-
-  class TermsWriter extends TermsConsumer {
-    private final FieldInfo fieldInfo;
-    private final PostingsWriterBase postingsWriter;
-    private final long termsStartPointer;
-    private long numTerms;
-    private final TermsIndexWriterBase.FieldWriter fieldIndexWriter;
-    long sumTotalTermFreq;
-    long sumDocFreq;
-    int docCount;
-
-    private TermEntry[] pendingTerms;
-
-    private int pendingCount;
-
-    TermsWriter(
-        TermsIndexWriterBase.FieldWriter fieldIndexWriter,
-        FieldInfo fieldInfo,
-        PostingsWriterBase postingsWriter) 
-    {
-      this.fieldInfo = fieldInfo;
-      this.fieldIndexWriter = fieldIndexWriter;
-      pendingTerms = new TermEntry[32];
-      for(int i=0;i<pendingTerms.length;i++) {
-        pendingTerms[i] = new TermEntry();
-      }
-      termsStartPointer = out.getFilePointer();
-      postingsWriter.setField(fieldInfo);
-      this.postingsWriter = postingsWriter;
-    }
-    
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public PostingsConsumer startTerm(BytesRef text) throws IOException {
-      //System.out.println("BTW: startTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment);
-      postingsWriter.startTerm();
-      return postingsWriter;
-    }
-
-    private final BytesRef lastPrevTerm = new BytesRef();
-
-    @Override
-    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
-
-      assert stats.docFreq > 0;
-      //System.out.println("BTW: finishTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment + " df=" + stats.docFreq);
-
-      final boolean isIndexTerm = fieldIndexWriter.checkIndexTerm(text, stats);
-
-      if (isIndexTerm) {
-        if (pendingCount > 0) {
-          // Instead of writing each term, live, we gather terms
-          // in RAM in a pending buffer, and then write the
-          // entire block in between index terms:
-          flushBlock();
-        }
-        fieldIndexWriter.add(text, stats, out.getFilePointer());
-        //System.out.println("  index term!");
-      }
-
-      if (pendingTerms.length == pendingCount) {
-        final TermEntry[] newArray = new TermEntry[ArrayUtil.oversize(pendingCount+1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-        System.arraycopy(pendingTerms, 0, newArray, 0, pendingCount);
-        for(int i=pendingCount;i<newArray.length;i++) {
-          newArray[i] = new TermEntry();
-        }
-        pendingTerms = newArray;
-      }
-      final TermEntry te = pendingTerms[pendingCount];
-      te.term.copyBytes(text);
-      te.stats = stats;
-
-      pendingCount++;
-
-      postingsWriter.finishTerm(stats);
-      numTerms++;
-    }
-
-    // Finishes all terms in this field
-    @Override
-    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
-      if (pendingCount > 0) {
-        flushBlock();
-      }
-      // EOF marker:
-      out.writeVInt(0);
-
-      this.sumTotalTermFreq = sumTotalTermFreq;
-      this.sumDocFreq = sumDocFreq;
-      this.docCount = docCount;
-      fieldIndexWriter.finish(out.getFilePointer());
-    }
-
-    private int sharedPrefix(BytesRef term1, BytesRef term2) {
-      assert term1.offset == 0;
-      assert term2.offset == 0;
-      int pos1 = 0;
-      int pos1End = pos1 + Math.min(term1.length, term2.length);
-      int pos2 = 0;
-      while(pos1 < pos1End) {
-        if (term1.bytes[pos1] != term2.bytes[pos2]) {
-          return pos1;
-        }
-        pos1++;
-        pos2++;
-      }
-      return pos1;
-    }
-
-    private final RAMOutputStream bytesWriter = new RAMOutputStream();
-
-    private void flushBlock() throws IOException {
-      //System.out.println("BTW.flushBlock seg=" + segment + " pendingCount=" + pendingCount + " fp=" + out.getFilePointer());
-
-      // First pass: compute common prefix for all terms
-      // in the block, against term before first term in
-      // this block:
-      int commonPrefix = sharedPrefix(lastPrevTerm, pendingTerms[0].term);
-      for(int termCount=1;termCount<pendingCount;termCount++) {
-        commonPrefix = Math.min(commonPrefix,
-                                sharedPrefix(lastPrevTerm,
-                                             pendingTerms[termCount].term));
-      }        
-
-      out.writeVInt(pendingCount);
-      out.writeVInt(commonPrefix);
-
-      // 2nd pass: write suffixes, as separate byte[] blob
-      for(int termCount=0;termCount<pendingCount;termCount++) {
-        final int suffix = pendingTerms[termCount].term.length - commonPrefix;
-        // TODO: cutover to better intblock codec, instead
-        // of interleaving here:
-        bytesWriter.writeVInt(suffix);
-        bytesWriter.writeBytes(pendingTerms[termCount].term.bytes, commonPrefix, suffix);
-      }
-      out.writeVInt((int) bytesWriter.getFilePointer());
-      bytesWriter.writeTo(out);
-      bytesWriter.reset();
-
-      // 3rd pass: write the freqs as byte[] blob
-      // TODO: cutover to better intblock codec.  simple64?
-      // write prefix, suffix first:
-      for(int termCount=0;termCount<pendingCount;termCount++) {
-        final TermStats stats = pendingTerms[termCount].stats;
-        assert stats != null;
-        bytesWriter.writeVInt(stats.docFreq);
-        if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
-          bytesWriter.writeVLong(stats.totalTermFreq-stats.docFreq);
-        }
-      }
-
-      out.writeVInt((int) bytesWriter.getFilePointer());
-      bytesWriter.writeTo(out);
-      bytesWriter.reset();
-
-      postingsWriter.flushTermsBlock(pendingCount, pendingCount);
-      lastPrevTerm.copyBytes(pendingTerms[pendingCount-1].term);
-      pendingCount = 0;
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsReader.java
deleted file mode 100644
index 2da3948..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsReader.java
+++ /dev/null
@@ -1,2845 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.TreeMap;
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.automaton.CompiledAutomaton;
-import org.apache.lucene.util.automaton.RunAutomaton;
-import org.apache.lucene.util.automaton.Transition;
-import org.apache.lucene.util.fst.ByteSequenceOutputs;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.Outputs;
-import org.apache.lucene.util.fst.Util;
-
-/** A block-based terms index and dictionary that assigns
- *  terms to variable length blocks according to how they
- *  share prefixes.  The terms index is a prefix trie
- *  whose leaves are term blocks.  The advantage of this
- *  approach is that seekExact is often able to
- *  determine a term cannot exist without doing any IO, and
- *  intersection with Automata is very fast.  Note that this
- *  terms dictionary has it's own fixed terms index (ie, it
- *  does not support a pluggable terms index
- *  implementation).
- *
- *  <p><b>NOTE</b>: this terms dictionary does not support
- *  index divisor when opening an IndexReader.  Instead, you
- *  can change the min/maxItemsPerBlock during indexing.</p>
- *
- *  <p>The data structure used by this implementation is very
- *  similar to a burst trie
- *  (http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3499),
- *  but with added logic to break up too-large blocks of all
- *  terms sharing a given prefix into smaller ones.</p>
- *
- *  <p>Use {@link org.apache.lucene.index.CheckIndex} with the <code>-verbose</code>
- *  option to see summary statistics on the blocks in the
- *  dictionary.
- *
- *  See {@link BlockTreeTermsWriter}.
- *
- * @lucene.experimental
- */
-
-public class BlockTreeTermsReader extends FieldsProducer {
-
-  // Open input to the main terms dict file (_X.tib)
-  private final IndexInput in;
-
-  //private static final boolean DEBUG = BlockTreeTermsWriter.DEBUG;
-
-  // Reads the terms dict entries, to gather state to
-  // produce DocsEnum on demand
-  private final PostingsReaderBase postingsReader;
-
-  private final TreeMap<String,FieldReader> fields = new TreeMap<String,FieldReader>();
-
-  // keeps the dirStart offset
-  protected long dirOffset;
-  protected long indexDirOffset;
-
-  private String segment;
-  
-  public BlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, String segment,
-                              PostingsReaderBase postingsReader, IOContext ioContext,
-                              String segmentSuffix, int indexDivisor)
-    throws IOException {
-    
-    this.postingsReader = postingsReader;
-
-    this.segment = segment;
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION),
-                       ioContext);
-
-    boolean success = false;
-    IndexInput indexIn = null;
-
-    try {
-      readHeader(in);
-      if (indexDivisor != -1) {
-        indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION),
-                                ioContext);
-        readIndexHeader(indexIn);
-      }
-
-      // Have PostingsReader init itself
-      postingsReader.init(in);
-
-      // Read per-field details
-      seekDir(in, dirOffset);
-      if (indexDivisor != -1) {
-        seekDir(indexIn, indexDirOffset);
-      }
-
-      final int numFields = in.readVInt();
-
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final long numTerms = in.readVLong();
-        assert numTerms >= 0;
-        final int numBytes = in.readVInt();
-        final BytesRef rootCode = new BytesRef(new byte[numBytes]);
-        in.readBytes(rootCode.bytes, 0, numBytes);
-        rootCode.length = numBytes;
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        assert fieldInfo != null: "field=" + field;
-        final long sumTotalTermFreq = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
-        final long sumDocFreq = in.readVLong();
-        final int docCount = in.readVInt();
-        final long indexStartFP = indexDivisor != -1 ? indexIn.readVLong() : 0;
-        assert !fields.containsKey(fieldInfo.name);
-        fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount, indexStartFP, indexIn));
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(indexIn, this);
-      } else if (indexDivisor != -1) {
-        indexIn.close();
-      }
-    }
-  }
-
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, BlockTreeTermsWriter.TERMS_CODEC_NAME,
-                          BlockTreeTermsWriter.TERMS_VERSION_START,
-                          BlockTreeTermsWriter.TERMS_VERSION_CURRENT);
-    dirOffset = input.readLong();    
-  }
-
-  protected void readIndexHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, BlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,
-                          BlockTreeTermsWriter.TERMS_INDEX_VERSION_START,
-                          BlockTreeTermsWriter.TERMS_INDEX_VERSION_CURRENT);
-    indexDirOffset = input.readLong();    
-  }
-  
-  protected void seekDir(IndexInput input, long dirOffset)
-      throws IOException {
-    input.seek(dirOffset);
-  }
-
-  // for debugging
-  // private static String toHex(int v) {
-  //   return "0x" + Integer.toHexString(v);
-  // }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(in, postingsReader);
-    } finally { 
-      // Clear so refs to terms index is GCable even if
-      // app hangs onto us:
-      fields.clear();
-    }
-  }
-
-  public static void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) {
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockTreeTermsWriter.TERMS_EXTENSION));
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockTreeTermsWriter.TERMS_INDEX_EXTENSION));
-  }
-
-  @Override
-  public FieldsEnum iterator() {
-    return new TermFieldsEnum();
-  }
-
-  @Override
-  public Terms terms(String field) throws IOException {
-    return fields.get(field);
-  }
-
-  @Override
-  public int getUniqueFieldCount() {
-    return fields.size();
-  }
-
-  // Iterates through all fields
-  private class TermFieldsEnum extends FieldsEnum {
-    final Iterator<FieldReader> it;
-    FieldReader current;
-
-    TermFieldsEnum() {
-      it = fields.values().iterator();
-    }
-
-    @Override
-    public String next() {
-      if (it.hasNext()) {
-        current = it.next();
-        return current.fieldInfo.name;
-      } else {
-        current = null;
-        return null;
-      }
-    }
-    
-    @Override
-    public Terms terms() throws IOException {
-      return current;
-    }
-  }
-
-  // for debugging
-  String brToString(BytesRef b) {
-    if (b == null) {
-      return "null";
-    } else {
-      try {
-        return b.utf8ToString() + " " + b;
-      } catch (Throwable t) {
-        // If BytesRef isn't actually UTF8, or it's eg a
-        // prefix of UTF8 that ends mid-unicode-char, we
-        // fallback to hex:
-        return b.toString();
-      }
-    }
-  }
-
-  public static class Stats {
-    public int indexNodeCount;
-    public int indexArcCount;
-    public int indexNumBytes;
-
-    public long totalTermCount;
-    public long totalTermBytes;
-
-
-    public int nonFloorBlockCount;
-    public int floorBlockCount;
-    public int floorSubBlockCount;
-    public int mixedBlockCount;
-    public int termsOnlyBlockCount;
-    public int subBlocksOnlyBlockCount;
-    public int totalBlockCount;
-
-    public int[] blockCountByPrefixLen = new int[10];
-    private int startBlockCount;
-    private int endBlockCount;
-    public long totalBlockSuffixBytes;
-    public long totalBlockStatsBytes;
-
-    // Postings impl plus the other few vInts stored in
-    // the frame:
-    public long totalBlockOtherBytes;
-
-    public final String segment;
-    public final String field;
-
-    public Stats(String segment, String field) {
-      this.segment = segment;
-      this.field = field;
-    }
-
-    void startBlock(FieldReader.SegmentTermsEnum.Frame frame, boolean isFloor) {
-      totalBlockCount++;
-      if (isFloor) {
-        if (frame.fp == frame.fpOrig) {
-          floorBlockCount++;
-        }
-        floorSubBlockCount++;
-      } else {
-        nonFloorBlockCount++;
-      }
-
-      if (blockCountByPrefixLen.length <= frame.prefix) {
-        blockCountByPrefixLen = ArrayUtil.grow(blockCountByPrefixLen, 1+frame.prefix);
-      }
-      blockCountByPrefixLen[frame.prefix]++;
-      startBlockCount++;
-      totalBlockSuffixBytes += frame.suffixesReader.length();
-      totalBlockStatsBytes += frame.statsReader.length();
-    }
-
-    void endBlock(FieldReader.SegmentTermsEnum.Frame frame) {
-      final int termCount = frame.isLeafBlock ? frame.entCount : frame.state.termBlockOrd;
-      final int subBlockCount = frame.entCount - termCount;
-      totalTermCount += termCount;
-      if (termCount != 0 && subBlockCount != 0) {
-        mixedBlockCount++;
-      } else if (termCount != 0) {
-        termsOnlyBlockCount++;
-      } else if (subBlockCount != 0) {
-        subBlocksOnlyBlockCount++;
-      } else {
-        throw new IllegalStateException();
-      }
-      endBlockCount++;
-      final long otherBytes = frame.fpEnd - frame.fp - frame.suffixesReader.length() - frame.statsReader.length();
-      assert otherBytes > 0 : "otherBytes=" + otherBytes + " frame.fp=" + frame.fp + " frame.fpEnd=" + frame.fpEnd;
-      totalBlockOtherBytes += otherBytes;
-    }
-
-    void term(BytesRef term) {
-      totalTermBytes += term.length;
-    }
-
-    void finish() {
-      assert startBlockCount == endBlockCount: "startBlockCount=" + startBlockCount + " endBlockCount=" + endBlockCount;
-      assert totalBlockCount == floorSubBlockCount + nonFloorBlockCount: "floorSubBlockCount=" + floorSubBlockCount + " nonFloorBlockCount=" + nonFloorBlockCount + " totalBlockCount=" + totalBlockCount;
-      assert totalBlockCount == mixedBlockCount + termsOnlyBlockCount + subBlocksOnlyBlockCount: "totalBlockCount=" + totalBlockCount + " mixedBlockCount=" + mixedBlockCount + " subBlocksOnlyBlockCount=" + subBlocksOnlyBlockCount + " termsOnlyBlockCount=" + termsOnlyBlockCount;
-    }
-
-    @Override
-    public String toString() {
-      final ByteArrayOutputStream bos = new ByteArrayOutputStream(1024);
-      final PrintStream out = new PrintStream(bos);
-      
-      out.println("  index FST:");
-      out.println("    " + indexNodeCount + " nodes");
-      out.println("    " + indexArcCount + " arcs");
-      out.println("    " + indexNumBytes + " bytes");
-      out.println("  terms:");
-      out.println("    " + totalTermCount + " terms");
-      out.println("    " + totalTermBytes + " bytes" + (totalTermCount != 0 ? " (" + String.format("%.1f", ((double) totalTermBytes)/totalTermCount) + " bytes/term)" : ""));
-      out.println("  blocks:");
-      out.println("    " + totalBlockCount + " blocks");
-      out.println("    " + termsOnlyBlockCount + " terms-only blocks");
-      out.println("    " + subBlocksOnlyBlockCount + " sub-block-only blocks");
-      out.println("    " + mixedBlockCount + " mixed blocks");
-      out.println("    " + floorBlockCount + " floor blocks");
-      out.println("    " + (totalBlockCount-floorSubBlockCount) + " non-floor blocks");
-      out.println("    " + floorSubBlockCount + " floor sub-blocks");
-      out.println("    " + totalBlockSuffixBytes + " term suffix bytes" + (totalBlockCount != 0 ? " (" + String.format("%.1f", ((double) totalBlockSuffixBytes)/totalBlockCount) + " suffix-bytes/block)" : ""));
-      out.println("    " + totalBlockStatsBytes + " term stats bytes" + (totalBlockCount != 0 ? " (" + String.format("%.1f", ((double) totalBlockStatsBytes)/totalBlockCount) + " stats-bytes/block)" : ""));
-      out.println("    " + totalBlockOtherBytes + " other bytes" + (totalBlockCount != 0 ? " (" + String.format("%.1f", ((double) totalBlockOtherBytes)/totalBlockCount) + " other-bytes/block)" : ""));
-      if (totalBlockCount != 0) {
-        out.println("    by prefix length:");
-        int total = 0;
-        for(int prefix=0;prefix<blockCountByPrefixLen.length;prefix++) {
-          final int blockCount = blockCountByPrefixLen[prefix];
-          total += blockCount;
-          if (blockCount != 0) {
-            out.println("      " + String.format("%2d", prefix) + ": " + blockCount);
-          }
-        }
-        assert totalBlockCount == total;
-      }
-
-      return bos.toString();
-    }
-  }
-
-  final Outputs<BytesRef> fstOutputs = ByteSequenceOutputs.getSingleton();
-  final BytesRef NO_OUTPUT = fstOutputs.getNoOutput();
-
-  public final class FieldReader extends Terms {
-    final long numTerms;
-    final FieldInfo fieldInfo;
-    final long sumTotalTermFreq;
-    final long sumDocFreq;
-    final int docCount;
-    final long indexStartFP;
-    final long rootBlockFP;
-    final BytesRef rootCode;
-    private FST<BytesRef> index;
-
-    //private boolean DEBUG;
-
-    FieldReader(FieldInfo fieldInfo, long numTerms, BytesRef rootCode, long sumTotalTermFreq, long sumDocFreq, int docCount, long indexStartFP, IndexInput indexIn) throws IOException {
-      assert numTerms > 0;
-      this.fieldInfo = fieldInfo;
-      //DEBUG = BlockTreeTermsReader.DEBUG && fieldInfo.name.equals("id");
-      this.numTerms = numTerms;
-      this.sumTotalTermFreq = sumTotalTermFreq; 
-      this.sumDocFreq = sumDocFreq; 
-      this.docCount = docCount;
-      this.indexStartFP = indexStartFP;
-      this.rootCode = rootCode;
-      // if (DEBUG) {
-      //   System.out.println("BTTR: seg=" + segment + " field=" + fieldInfo.name + " rootBlockCode=" + rootCode + " divisor=" + indexDivisor);
-      // }
-
-      rootBlockFP = (new ByteArrayDataInput(rootCode.bytes, rootCode.offset, rootCode.length)).readVLong() >>> BlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS;
-
-      if (indexIn != null) {
-        final IndexInput clone = (IndexInput) indexIn.clone();
-        //System.out.println("start=" + indexStartFP + " field=" + fieldInfo.name);
-        clone.seek(indexStartFP);
-        index = new FST<BytesRef>(clone, ByteSequenceOutputs.getSingleton());
-        
-        /*
-        if (false) {
-          final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
-          Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
-          Util.toDot(index, w, false, false);
-          System.out.println("FST INDEX: SAVED to " + dotFileName);
-          w.close();
-        }
-        */
-      }
-    }
-
-    /** For debugging -- used by CheckIndex too*/
-    // TODO: maybe push this into Terms?
-    public Stats computeStats() throws IOException {
-      return new SegmentTermsEnum().computeBlockStats();
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      return new SegmentTermsEnum();
-    }
-
-    @Override
-    public long getUniqueTermCount() {
-      return numTerms;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return sumTotalTermFreq;
-    }
-
-    @Override
-    public long getSumDocFreq() {
-      return sumDocFreq;
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return docCount;
-    }
-
-    @Override
-    public TermsEnum intersect(CompiledAutomaton compiled, BytesRef startTerm) throws IOException {
-      if (compiled.type != CompiledAutomaton.AUTOMATON_TYPE.NORMAL) {
-        throw new IllegalArgumentException("please use CompiledAutomaton.getTermsEnum instead");
-      }
-      return new IntersectEnum(compiled, startTerm);
-    }
-    
-    // NOTE: cannot seek!
-    private final class IntersectEnum extends TermsEnum {
-      private final IndexInput in;
-
-      private Frame[] stack;
-      
-      @SuppressWarnings("unchecked") private FST.Arc<BytesRef>[] arcs = new FST.Arc[5];
-
-      private final RunAutomaton runAutomaton;
-      private final CompiledAutomaton compiledAutomaton;
-
-      private Frame currentFrame;
-
-      private final BytesRef term = new BytesRef();
-
-      // TODO: can we share this with the frame in STE?
-      private final class Frame {
-        final int ord;
-        long fp;
-        long fpOrig;
-        long fpEnd;
-        long lastSubFP;
-
-        // State in automaton
-        int state;
-
-        int metaDataUpto;
-
-        byte[] suffixBytes = new byte[128];
-        final ByteArrayDataInput suffixesReader = new ByteArrayDataInput();
-
-        byte[] statBytes = new byte[64];
-        final ByteArrayDataInput statsReader = new ByteArrayDataInput();
-
-        byte[] floorData = new byte[32];
-        final ByteArrayDataInput floorDataReader = new ByteArrayDataInput();
-
-        // Length of prefix shared by all terms in this block
-        int prefix;
-
-        // Number of entries (term or sub-block) in this block
-        int entCount;
-
-        // Which term we will next read
-        int nextEnt;
-
-        // True if this block is either not a floor block,
-        // or, it's the last sub-block of a floor block
-        boolean isLastInFloor;
-
-        // True if all entries are terms
-        boolean isLeafBlock;
-
-        int numFollowFloorBlocks;
-        int nextFloorLabel;
-        
-        Transition[] transitions;
-        int curTransitionMax;
-        int transitionIndex;
-
-        FST.Arc<BytesRef> arc;
-
-        final BlockTermState termState;
-
-        // Cumulative output so far
-        BytesRef outputPrefix;
-
-        private int startBytePos;
-        private int suffix;
-
-        public Frame(int ord) throws IOException {
-          this.ord = ord;
-          termState = postingsReader.newTermState();
-          termState.totalTermFreq = -1;
-        }
-
-        void loadNextFloorBlock() throws IOException {
-          assert numFollowFloorBlocks > 0;
-          //if (DEBUG) System.out.println("    loadNextFoorBlock trans=" + transitions[transitionIndex]);
-
-          do {
-            fp = fpOrig + (floorDataReader.readVLong() >>> 1);
-            numFollowFloorBlocks--;
-            // if (DEBUG) System.out.println("    skip floor block2!  nextFloorLabel=" + (char) nextFloorLabel + " vs target=" + (char) transitions[transitionIndex].getMin() + " newFP=" + fp + " numFollowFloorBlocks=" + numFollowFloorBlocks);
-            if (numFollowFloorBlocks != 0) {
-              nextFloorLabel = floorDataReader.readByte() & 0xff;
-            } else {
-              nextFloorLabel = 256;
-            }
-            // if (DEBUG) System.out.println("    nextFloorLabel=" + (char) nextFloorLabel);
-          } while (numFollowFloorBlocks != 0 && nextFloorLabel <= transitions[transitionIndex].getMin());
-
-          load(null);
-        }
-
-        public void setState(int state) {
-          this.state = state;
-          transitionIndex = 0;
-          transitions = compiledAutomaton.sortedTransitions[state];
-          if (transitions.length != 0) {
-            curTransitionMax = transitions[0].getMax();
-          } else {
-            curTransitionMax = -1;
-          }
-        }
-
-        void load(BytesRef frameIndexData) throws IOException {
-
-          // if (DEBUG) System.out.println("    load fp=" + fp + " fpOrig=" + fpOrig + " frameIndexData=" + frameIndexData + " trans=" + (transitions.length != 0 ? transitions[0] : "n/a" + " state=" + state));
-
-          if (frameIndexData != null && transitions.length != 0) {
-            // Floor frame
-            if (floorData.length < frameIndexData.length) {
-              this.floorData = new byte[ArrayUtil.oversize(frameIndexData.length, 1)];
-            }
-            System.arraycopy(frameIndexData.bytes, frameIndexData.offset, floorData, 0, frameIndexData.length);
-            floorDataReader.reset(floorData, 0, frameIndexData.length);
-            // Skip first long -- has redundant fp, hasTerms
-            // flag, isFloor flag
-            final long code = floorDataReader.readVLong();
-            if ((code & BlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR) != 0) {
-              numFollowFloorBlocks = floorDataReader.readVInt();
-              nextFloorLabel = floorDataReader.readByte() & 0xff;
-              // if (DEBUG) System.out.println("    numFollowFloorBlocks=" + numFollowFloorBlocks + " nextFloorLabel=" + nextFloorLabel);
-
-              // If current state is accept, we must process
-              // first block in case it has empty suffix:
-              if (!runAutomaton.isAccept(state)) {
-                // Maybe skip floor blocks:
-                while (numFollowFloorBlocks != 0 && nextFloorLabel <= transitions[0].getMin()) {
-                  fp = fpOrig + (floorDataReader.readVLong() >>> 1);
-                  numFollowFloorBlocks--;
-                  // if (DEBUG) System.out.println("    skip floor block!  nextFloorLabel=" + (char) nextFloorLabel + " vs target=" + (char) transitions[0].getMin() + " newFP=" + fp + " numFollowFloorBlocks=" + numFollowFloorBlocks);
-                  if (numFollowFloorBlocks != 0) {
-                    nextFloorLabel = floorDataReader.readByte() & 0xff;
-                  } else {
-                    nextFloorLabel = 256;
-                  }
-                }
-              }
-            }
-          }
-
-          in.seek(fp);
-          int code = in.readVInt();
-          entCount = code >>> 1;
-          assert entCount > 0;
-          isLastInFloor = (code & 1) != 0;
-
-          // term suffixes:
-          code = in.readVInt();
-          isLeafBlock = (code & 1) != 0;
-          int numBytes = code >>> 1;
-          // if (DEBUG) System.out.println("      entCount=" + entCount + " lastInFloor?=" + isLastInFloor + " leafBlock?=" + isLeafBlock + " numSuffixBytes=" + numBytes);
-          if (suffixBytes.length < numBytes) {
-            suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-          }
-          in.readBytes(suffixBytes, 0, numBytes);
-          suffixesReader.reset(suffixBytes, 0, numBytes);
-
-          // stats
-          numBytes = in.readVInt();
-          if (statBytes.length < numBytes) {
-            statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-          }
-          in.readBytes(statBytes, 0, numBytes);
-          statsReader.reset(statBytes, 0, numBytes);
-          metaDataUpto = 0;
-
-          termState.termBlockOrd = 0;
-          nextEnt = 0;
-          
-          postingsReader.readTermsBlock(in, fieldInfo, termState);
-
-          if (!isLastInFloor) {
-            // Sub-blocks of a single floor block are always
-            // written one after another -- tail recurse:
-            fpEnd = in.getFilePointer();
-          }
-        }
-
-        // TODO: maybe add scanToLabel; should give perf boost
-
-        public boolean next() {
-          return isLeafBlock ? nextLeaf() : nextNonLeaf();
-        }
-
-        // Decodes next entry; returns true if it's a sub-block
-        public boolean nextLeaf() {
-          //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
-          assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
-          nextEnt++;
-          suffix = suffixesReader.readVInt();
-          startBytePos = suffixesReader.getPosition();
-          suffixesReader.skipBytes(suffix);
-          return false;
-        }
-
-        public boolean nextNonLeaf() {
-          //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
-          assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
-          nextEnt++;
-          final int code = suffixesReader.readVInt();
-          suffix = code >>> 1;
-          startBytePos = suffixesReader.getPosition();
-          suffixesReader.skipBytes(suffix);
-          if ((code & 1) == 0) {
-            // A normal term
-            termState.termBlockOrd++;
-            return false;
-          } else {
-            // A sub-block; make sub-FP absolute:
-            lastSubFP = fp - suffixesReader.readVLong();
-            return true;
-          }
-        }
-
-        public int getTermBlockOrd() {
-          return isLeafBlock ? nextEnt : termState.termBlockOrd;
-        }
-
-        public void decodeMetaData() throws IOException {
-
-          // lazily catch up on metadata decode:
-          final int limit = getTermBlockOrd();
-          assert limit > 0;
-
-          // We must set/incr state.termCount because
-          // postings impl can look at this
-          termState.termBlockOrd = metaDataUpto;
-      
-          // TODO: better API would be "jump straight to term=N"???
-          while (metaDataUpto < limit) {
-
-            // TODO: we could make "tiers" of metadata, ie,
-            // decode docFreq/totalTF but don't decode postings
-            // metadata; this way caller could get
-            // docFreq/totalTF w/o paying decode cost for
-            // postings
-
-            // TODO: if docFreq were bulk decoded we could
-            // just skipN here:
-            termState.docFreq = statsReader.readVInt();
-            //if (DEBUG) System.out.println("    dF=" + state.docFreq);
-            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
-              termState.totalTermFreq = termState.docFreq + statsReader.readVLong();
-              //if (DEBUG) System.out.println("    totTF=" + state.totalTermFreq);
-            }
-
-            postingsReader.nextTerm(fieldInfo, termState);
-            metaDataUpto++;
-            termState.termBlockOrd++;
-          }
-        }
-      }
-
-      private BytesRef savedStartTerm;
-      
-      // TODO: in some cases we can filter by length?  eg
-      // regexp foo*bar must be at least length 6 bytes
-      public IntersectEnum(CompiledAutomaton compiled, BytesRef startTerm) throws IOException {
-        // if (DEBUG) {
-        //   System.out.println("\nintEnum.init seg=" + segment + " commonSuffix=" + brToString(compiled.commonSuffixRef));
-        // }
-        runAutomaton = compiled.runAutomaton;
-        compiledAutomaton = compiled;
-        in = (IndexInput) BlockTreeTermsReader.this.in.clone();
-        stack = new Frame[5];
-        for(int idx=0;idx<stack.length;idx++) {
-          stack[idx] = new Frame(idx);
-        }
-        for(int arcIdx=0;arcIdx<arcs.length;arcIdx++) {
-          arcs[arcIdx] = new FST.Arc<BytesRef>();
-        }
-
-        // TODO: if the automaton is "smallish" we really
-        // should use the terms index to seek at least to
-        // the initial term and likely to subsequent terms
-        // (or, maybe just fallback to ATE for such cases).
-        // Else the seek cost of loading the frames will be
-        // too costly.
-
-        final FST.Arc<BytesRef> arc = index.getFirstArc(arcs[0]);
-        // Empty string prefix must have an output in the index!
-        assert arc.isFinal();
-
-        // Special pushFrame since it's the first one:
-        final Frame f = stack[0];
-        f.fp = f.fpOrig = rootBlockFP;
-        f.prefix = 0;
-        f.setState(runAutomaton.getInitialState());
-        f.arc = arc;
-        f.outputPrefix = arc.output;
-        f.load(rootCode);
-
-        // for assert:
-        assert setSavedStartTerm(startTerm);
-
-        currentFrame = f;
-        if (startTerm != null) {
-          seekToStartTerm(startTerm);
-        }
-      }
-
-      // only for assert:
-      private boolean setSavedStartTerm(BytesRef startTerm) {
-        savedStartTerm = startTerm == null ? null : BytesRef.deepCopyOf(startTerm);
-        return true;
-      }
-
-      @Override
-      public TermState termState() throws IOException {
-        currentFrame.decodeMetaData();
-        return (TermState) currentFrame.termState.clone();
-      }
-
-      private Frame getFrame(int ord) throws IOException {
-        if (ord >= stack.length) {
-          final Frame[] next = new Frame[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-          System.arraycopy(stack, 0, next, 0, stack.length);
-          for(int stackOrd=stack.length;stackOrd<next.length;stackOrd++) {
-            next[stackOrd] = new Frame(stackOrd);
-          }
-          stack = next;
-        }
-        assert stack[ord].ord == ord;
-        return stack[ord];
-      }
-
-      private FST.Arc<BytesRef> getArc(int ord) {
-        if (ord >= arcs.length) {
-          @SuppressWarnings("unchecked") final FST.Arc<BytesRef>[] next = new FST.Arc[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-          System.arraycopy(arcs, 0, next, 0, arcs.length);
-          for(int arcOrd=arcs.length;arcOrd<next.length;arcOrd++) {
-            next[arcOrd] = new FST.Arc<BytesRef>();
-          }
-          arcs = next;
-        }
-        return arcs[ord];
-      }
-
-      private Frame pushFrame(int state) throws IOException {
-        final Frame f = getFrame(currentFrame == null ? 0 : 1+currentFrame.ord);
-        
-        f.fp = f.fpOrig = currentFrame.lastSubFP;
-        f.prefix = currentFrame.prefix + currentFrame.suffix;
-        // if (DEBUG) System.out.println("    pushFrame state=" + state + " prefix=" + f.prefix);
-        f.setState(state);
-
-        // Walk the arc through the index -- we only
-        // "bother" with this so we can get the floor data
-        // from the index and skip floor blocks when
-        // possible:
-        FST.Arc<BytesRef> arc = currentFrame.arc;
-        int idx = currentFrame.prefix;
-        assert currentFrame.suffix > 0;
-        BytesRef output = currentFrame.outputPrefix;
-        while (idx < f.prefix) {
-          final int target = term.bytes[idx] & 0xff;
-          // TODO: we could be more efficient for the next()
-          // case by using current arc as starting point,
-          // passed to findTargetArc
-          arc = index.findTargetArc(target, arc, getArc(1+idx));
-          assert arc != null;
-          output = fstOutputs.add(output, arc.output);
-          idx++;
-        }
-
-        f.arc = arc;
-        f.outputPrefix = output;
-        assert arc.isFinal();
-        f.load(fstOutputs.add(output, arc.nextFinalOutput));
-        return f;
-      }
-
-      @Override
-      public BytesRef term() throws IOException {
-        return term;
-      }
-
-      @Override
-      public int docFreq() throws IOException {
-        //if (DEBUG) System.out.println("BTIR.docFreq");
-        currentFrame.decodeMetaData();
-        //if (DEBUG) System.out.println("  return " + currentFrame.termState.docFreq);
-        return currentFrame.termState.docFreq;
-      }
-
-      @Override
-      public long totalTermFreq() throws IOException {
-        currentFrame.decodeMetaData();
-        return currentFrame.termState.totalTermFreq;
-      }
-
-      @Override
-      public DocsEnum docs(Bits skipDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-        currentFrame.decodeMetaData();
-        return postingsReader.docs(fieldInfo, currentFrame.termState, skipDocs, reuse, needsFreqs);
-      }
-
-      @Override
-      public DocsAndPositionsEnum docsAndPositions(Bits skipDocs, DocsAndPositionsEnum reuse) throws IOException {
-        if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-          return null;
-        } else {
-          currentFrame.decodeMetaData();
-          return postingsReader.docsAndPositions(fieldInfo, currentFrame.termState, skipDocs, reuse);
-        }
-      }
-
-      private int getState() {
-        int state = currentFrame.state;
-        for(int idx=0;idx<currentFrame.suffix;idx++) {
-          state = runAutomaton.step(state,  currentFrame.suffixBytes[currentFrame.startBytePos+idx] & 0xff);
-          assert state != -1;
-        }
-        return state;
-      }
-
-      // NOTE: specialized to only doing the first-time
-      // seek, but we could generalize it to allow
-      // arbitrary seekExact/Ceil.  Note that this is a
-      // seekFloor!
-      private void seekToStartTerm(BytesRef target) throws IOException {
-        //if (DEBUG) System.out.println("seek to startTerm=" + target.utf8ToString());
-        assert currentFrame.ord == 0;
-        if (term.length < target.length) {
-          term.bytes = ArrayUtil.grow(term.bytes, target.length);
-        }
-        FST.Arc<BytesRef> arc = arcs[0];
-        assert arc == currentFrame.arc;
-
-        for(int idx=0;idx<=target.length;idx++) {
-
-          while (true) {
-            final int savePos = currentFrame.suffixesReader.getPosition();
-            final int saveStartBytePos = currentFrame.startBytePos;
-            final int saveSuffix = currentFrame.suffix;
-            final long saveLastSubFP = currentFrame.lastSubFP;
-            final int saveTermBlockOrd = currentFrame.termState.termBlockOrd;
-
-            final boolean isSubBlock = currentFrame.next();
-
-            //if (DEBUG) System.out.println("    cycle ent=" + currentFrame.nextEnt + " (of " + currentFrame.entCount + ") prefix=" + currentFrame.prefix + " suffix=" + currentFrame.suffix + " isBlock=" + isSubBlock + " firstLabel=" + (currentFrame.suffix == 0 ? "" : (currentFrame.suffixBytes[currentFrame.startBytePos])&0xff));
-            term.length = currentFrame.prefix + currentFrame.suffix;
-            if (term.bytes.length < term.length) {
-              term.bytes = ArrayUtil.grow(term.bytes, term.length);
-            }
-            System.arraycopy(currentFrame.suffixBytes, currentFrame.startBytePos, term.bytes, currentFrame.prefix, currentFrame.suffix);
-
-            if (isSubBlock && StringHelper.startsWith(target, term)) {
-              // Recurse
-              //if (DEBUG) System.out.println("      recurse!");
-              currentFrame = pushFrame(getState());
-              break;
-            } else {
-              final int cmp = term.compareTo(target);
-              if (cmp < 0) {
-                if (currentFrame.nextEnt == currentFrame.entCount) {
-                  if (!currentFrame.isLastInFloor) {
-                    //if (DEBUG) System.out.println("  load floorBlock");
-                    currentFrame.loadNextFloorBlock();
-                    continue;
-                  } else {
-                    //if (DEBUG) System.out.println("  return term=" + brToString(term));
-                    return;
-                  }
-                }
-                continue;
-              } else if (cmp == 0) {
-                //if (DEBUG) System.out.println("  return term=" + brToString(term));
-                return;
-              } else {
-                // Fallback to prior entry: the semantics of
-                // this method is that the first call to
-                // next() will return the term after the
-                // requested term
-                currentFrame.nextEnt--;
-                currentFrame.lastSubFP = saveLastSubFP;
-                currentFrame.startBytePos = saveStartBytePos;
-                currentFrame.suffix = saveSuffix;
-                currentFrame.suffixesReader.setPosition(savePos);
-                currentFrame.termState.termBlockOrd = saveTermBlockOrd;
-                System.arraycopy(currentFrame.suffixBytes, currentFrame.startBytePos, term.bytes, currentFrame.prefix, currentFrame.suffix);
-                term.length = currentFrame.prefix + currentFrame.suffix;
-                // If the last entry was a block we don't
-                // need to bother recursing and pushing to
-                // the last term under it because the first
-                // next() will simply skip the frame anyway
-                return;
-              }
-            }
-          }
-        }
-
-        assert false;
-      }
-
-      @Override
-      public BytesRef next() throws IOException {
-
-        // if (DEBUG) {
-        //   System.out.println("\nintEnum.next seg=" + segment);
-        //   System.out.println("  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
-        // }
-
-        nextTerm:
-        while(true) {
-          // Pop finished frames
-          while (currentFrame.nextEnt == currentFrame.entCount) {
-            if (!currentFrame.isLastInFloor) {
-              //if (DEBUG) System.out.println("    next-floor-block");
-              currentFrame.loadNextFloorBlock();
-              //if (DEBUG) System.out.println("\n  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
-            } else {
-              //if (DEBUG) System.out.println("  pop frame");
-              if (currentFrame.ord == 0) {
-                return null;
-              }
-              final long lastFP = currentFrame.fpOrig;
-              currentFrame = stack[currentFrame.ord-1];
-              assert currentFrame.lastSubFP == lastFP;
-              //if (DEBUG) System.out.println("\n  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
-            }
-          }
-
-          final boolean isSubBlock = currentFrame.next();
-          // if (DEBUG) {
-          //   final BytesRef suffixRef = new BytesRef();
-          //   suffixRef.bytes = currentFrame.suffixBytes;
-          //   suffixRef.offset = currentFrame.startBytePos;
-          //   suffixRef.length = currentFrame.suffix;
-          //   System.out.println("    " + (isSubBlock ? "sub-block" : "term") + " " + currentFrame.nextEnt + " (of " + currentFrame.entCount + ") suffix=" + brToString(suffixRef));
-          // }
-
-          if (currentFrame.suffix != 0) {
-            final int label = currentFrame.suffixBytes[currentFrame.startBytePos] & 0xff;
-            while (label > currentFrame.curTransitionMax) {
-              if (currentFrame.transitionIndex >= currentFrame.transitions.length-1) {
-                // Stop processing this frame -- no further
-                // matches are possible because we've moved
-                // beyond what the max transition will allow
-                //if (DEBUG) System.out.println("      break: trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]));
-
-                // sneaky!  forces a pop above
-                currentFrame.isLastInFloor = true;
-                currentFrame.nextEnt = currentFrame.entCount;
-                continue nextTerm;
-              }
-              currentFrame.transitionIndex++;
-              currentFrame.curTransitionMax = currentFrame.transitions[currentFrame.transitionIndex].getMax();
-              //if (DEBUG) System.out.println("      next trans=" + currentFrame.transitions[currentFrame.transitionIndex]);
-            }
-          }
-
-          // First test the common suffix, if set:
-          if (compiledAutomaton.commonSuffixRef != null && !isSubBlock) {
-            final int termLen = currentFrame.prefix + currentFrame.suffix;
-            if (termLen < compiledAutomaton.commonSuffixRef.length) {
-              // No match
-              // if (DEBUG) {
-              //   System.out.println("      skip: common suffix length");
-              // }
-              continue nextTerm;
-            }
-
-            final byte[] suffixBytes = currentFrame.suffixBytes;
-            final byte[] commonSuffixBytes = compiledAutomaton.commonSuffixRef.bytes;
-
-            final int lenInPrefix = compiledAutomaton.commonSuffixRef.length - currentFrame.suffix;
-            assert compiledAutomaton.commonSuffixRef.offset == 0;
-            int suffixBytesPos;
-            int commonSuffixBytesPos = 0;
-
-            if (lenInPrefix > 0) {
-              // A prefix of the common suffix overlaps with
-              // the suffix of the block prefix so we first
-              // test whether the prefix part matches:
-              final byte[] termBytes = term.bytes;
-              int termBytesPos = currentFrame.prefix - lenInPrefix;
-              assert termBytesPos >= 0;
-              final int termBytesPosEnd = currentFrame.prefix;
-              while (termBytesPos < termBytesPosEnd) {
-                if (termBytes[termBytesPos++] != commonSuffixBytes[commonSuffixBytesPos++]) {
-                  // if (DEBUG) {
-                  //   System.out.println("      skip: common suffix mismatch (in prefix)");
-                  // }
-                  continue nextTerm;
-                }
-              }
-              suffixBytesPos = currentFrame.startBytePos;
-            } else {
-              suffixBytesPos = currentFrame.startBytePos + currentFrame.suffix - compiledAutomaton.commonSuffixRef.length;
-            }
-
-            // Test overlapping suffix part:
-            final int commonSuffixBytesPosEnd = compiledAutomaton.commonSuffixRef.length;
-            while (commonSuffixBytesPos < commonSuffixBytesPosEnd) {
-              if (suffixBytes[suffixBytesPos++] != commonSuffixBytes[commonSuffixBytesPos++]) {
-                // if (DEBUG) {
-                //   System.out.println("      skip: common suffix mismatch");
-                // }
-                continue nextTerm;
-              }
-            }
-          }
-
-          // TODO: maybe we should do the same linear test
-          // that AutomatonTermsEnum does, so that if we
-          // reach a part of the automaton where .* is
-          // "temporarily" accepted, we just blindly .next()
-          // until the limit
-
-          // See if the term prefix matches the automaton:
-          int state = currentFrame.state;
-          for (int idx=0;idx<currentFrame.suffix;idx++) {
-            state = runAutomaton.step(state,  currentFrame.suffixBytes[currentFrame.startBytePos+idx] & 0xff);
-            if (state == -1) {
-              // No match
-              //System.out.println("    no s=" + state);
-              continue nextTerm;
-            } else {
-              //System.out.println("    c s=" + state);
-            }
-          }
-
-          if (isSubBlock) {
-            // Match!  Recurse:
-            //if (DEBUG) System.out.println("      sub-block match to state=" + state + "; recurse fp=" + currentFrame.lastSubFP);
-            copyTerm();
-            currentFrame = pushFrame(state);
-            //if (DEBUG) System.out.println("\n  frame ord=" + currentFrame.ord + " prefix=" + brToString(new BytesRef(term.bytes, term.offset, currentFrame.prefix)) + " state=" + currentFrame.state + " lastInFloor?=" + currentFrame.isLastInFloor + " fp=" + currentFrame.fp + " trans=" + (currentFrame.transitions.length == 0 ? "n/a" : currentFrame.transitions[currentFrame.transitionIndex]) + " outputPrefix=" + currentFrame.outputPrefix);
-          } else if (runAutomaton.isAccept(state)) {
-            copyTerm();
-            //if (DEBUG) System.out.println("      term match to state=" + state + "; return term=" + brToString(term));
-            assert savedStartTerm == null || term.compareTo(savedStartTerm) > 0: "saveStartTerm=" + savedStartTerm.utf8ToString() + " term=" + term.utf8ToString();
-            return term;
-          } else {
-            //System.out.println("    no s=" + state);
-          }
-        }
-      }
-
-      private void copyTerm() {
-        //System.out.println("      copyTerm cur.prefix=" + currentFrame.prefix + " cur.suffix=" + currentFrame.suffix + " first=" + (char) currentFrame.suffixBytes[currentFrame.startBytePos]);
-        final int len = currentFrame.prefix + currentFrame.suffix;
-        if (term.bytes.length < len) {
-          term.bytes = ArrayUtil.grow(term.bytes, len);
-        }
-        System.arraycopy(currentFrame.suffixBytes, currentFrame.startBytePos, term.bytes, currentFrame.prefix, currentFrame.suffix);
-        term.length = len;
-      }
-
-      @Override
-      public Comparator<BytesRef> getComparator() {
-        return BytesRef.getUTF8SortedAsUnicodeComparator();
-      }
-
-      @Override
-      public boolean seekExact(BytesRef text, boolean useCache) throws IOException {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public void seekExact(long ord) throws IOException {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public long ord() throws IOException {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException {
-        throw new UnsupportedOperationException();
-      }
-    }
-
-    // Iterates through terms in this field
-    private final class SegmentTermsEnum extends TermsEnum {
-      private IndexInput in;
-
-      private Frame[] stack;
-      private final Frame staticFrame;
-      private Frame currentFrame;
-      private boolean termExists;
-
-      private int targetBeforeCurrentLength;
-
-      private final ByteArrayDataInput scratchReader = new ByteArrayDataInput();
-
-      // What prefix of the current term was present in the index:
-      private int validIndexPrefix;
-
-      // assert only:
-      private boolean eof;
-
-      final BytesRef term = new BytesRef();
-
-      @SuppressWarnings("unchecked") private FST.Arc<BytesRef>[] arcs = new FST.Arc[1];
-
-      public SegmentTermsEnum() throws IOException {
-        //if (DEBUG) System.out.println("BTTR.init seg=" + segment);
-        stack = new Frame[0];
-        
-        // Used to hold seek by TermState, or cached seek
-        staticFrame = new Frame(-1);
-
-        // Init w/ root block; don't use index since it may
-        // not (and need not) have been loaded
-        for(int arcIdx=0;arcIdx<arcs.length;arcIdx++) {
-          arcs[arcIdx] = new FST.Arc<BytesRef>();
-        }
-
-        currentFrame = staticFrame;
-        final FST.Arc<BytesRef> arc;
-        if (index != null) {
-          arc = index.getFirstArc(arcs[0]);
-          // Empty string prefix must have an output in the index!
-          assert arc.isFinal();
-        } else {
-          arc = null;
-        }
-        currentFrame = staticFrame;
-        //currentFrame = pushFrame(arc, rootCode, 0);
-        //currentFrame.loadBlock();
-        validIndexPrefix = 0;
-        // if (DEBUG) {
-        //   System.out.println("init frame state " + currentFrame.ord);
-        //   printSeekState();
-        // }
-
-        //System.out.println();
-        // computeBlockStats().print(System.out);
-      }
-
-      private void initIndexInput() {
-        if (this.in == null) {
-          this.in = (IndexInput) BlockTreeTermsReader.this.in.clone();
-        }
-      }
-
-      /** Runs next() through the entire terms dict,
-       *  computing aggregate statistics. */
-      public Stats computeBlockStats() throws IOException {
-
-        Stats stats = new Stats(segment, fieldInfo.name);
-        if (index != null) {
-          stats.indexNodeCount = index.getNodeCount();
-          stats.indexArcCount = index.getArcCount();
-          stats.indexNumBytes = index.sizeInBytes();
-        }
-        
-        currentFrame = staticFrame;
-        FST.Arc<BytesRef> arc;
-        if (index != null) {
-          arc = index.getFirstArc(arcs[0]);
-          // Empty string prefix must have an output in the index!
-          assert arc.isFinal();
-        } else {
-          arc = null;
-        }
-
-        // Empty string prefix must have an output in the
-        // index!
-        currentFrame = pushFrame(arc, rootCode, 0);
-        currentFrame.fpOrig = currentFrame.fp;
-        currentFrame.loadBlock();
-        validIndexPrefix = 0;
-
-        stats.startBlock(currentFrame, !currentFrame.isLastInFloor);
-
-        allTerms:
-        while (true) {
-
-          // Pop finished blocks
-          while (currentFrame.nextEnt == currentFrame.entCount) {
-            stats.endBlock(currentFrame);
-            if (!currentFrame.isLastInFloor) {
-              currentFrame.loadNextFloorBlock();
-              stats.startBlock(currentFrame, true);
-            } else {
-              if (currentFrame.ord == 0) {
-                break allTerms;
-              }
-              final long lastFP = currentFrame.fpOrig;
-              currentFrame = stack[currentFrame.ord-1];
-              assert lastFP == currentFrame.lastSubFP;
-              // if (DEBUG) {
-              //   System.out.println("  reset validIndexPrefix=" + validIndexPrefix);
-              // }
-            }
-          }
-
-          while(true) {
-            if (currentFrame.next()) {
-              // Push to new block:
-              currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length);
-              currentFrame.fpOrig = currentFrame.fp;
-              // This is a "next" frame -- even if it's
-              // floor'd we must pretend it isn't so we don't
-              // try to scan to the right floor frame:
-              currentFrame.isFloor = false;
-              //currentFrame.hasTerms = true;
-              currentFrame.loadBlock();
-              stats.startBlock(currentFrame, !currentFrame.isLastInFloor);
-            } else {
-              stats.term(term);
-              break;
-            }
-          }
-        }
-
-        stats.finish();
-
-        // Put root frame back:
-        currentFrame = staticFrame;
-        if (index != null) {
-          arc = index.getFirstArc(arcs[0]);
-          // Empty string prefix must have an output in the index!
-          assert arc.isFinal();
-        } else {
-          arc = null;
-        }
-        currentFrame = pushFrame(arc, rootCode, 0);
-        currentFrame.rewind();
-        currentFrame.loadBlock();
-        validIndexPrefix = 0;
-        term.length = 0;
-
-        return stats;
-      }
-
-      private Frame getFrame(int ord) throws IOException {
-        if (ord >= stack.length) {
-          final Frame[] next = new Frame[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-          System.arraycopy(stack, 0, next, 0, stack.length);
-          for(int stackOrd=stack.length;stackOrd<next.length;stackOrd++) {
-            next[stackOrd] = new Frame(stackOrd);
-          }
-          stack = next;
-        }
-        assert stack[ord].ord == ord;
-        return stack[ord];
-      }
-
-      private FST.Arc<BytesRef> getArc(int ord) {
-        if (ord >= arcs.length) {
-          @SuppressWarnings("unchecked") final FST.Arc<BytesRef>[] next = new FST.Arc[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-          System.arraycopy(arcs, 0, next, 0, arcs.length);
-          for(int arcOrd=arcs.length;arcOrd<next.length;arcOrd++) {
-            next[arcOrd] = new FST.Arc<BytesRef>();
-          }
-          arcs = next;
-        }
-        return arcs[ord];
-      }
-
-      @Override
-      public Comparator<BytesRef> getComparator() {
-        return BytesRef.getUTF8SortedAsUnicodeComparator();
-      }
-
-      // Pushes a frame we seek'd to
-      Frame pushFrame(FST.Arc<BytesRef> arc, BytesRef frameData, int length) throws IOException {
-        scratchReader.reset(frameData.bytes, frameData.offset, frameData.length);
-        final long code = scratchReader.readVLong();
-        final long fpSeek = code >>> BlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS;
-        final Frame f = getFrame(1+currentFrame.ord);
-        f.hasTerms = (code & BlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS) != 0;
-        f.hasTermsOrig = f.hasTerms;
-        f.isFloor = (code & BlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR) != 0;
-        if (f.isFloor) {
-          f.setFloorData(scratchReader, frameData);
-        }
-        pushFrame(arc, fpSeek, length);
-
-        return f;
-      }
-
-      // Pushes next'd frame or seek'd frame; we later
-      // lazy-load the frame only when needed
-      Frame pushFrame(FST.Arc<BytesRef> arc, long fp, int length) throws IOException {
-        final Frame f = getFrame(1+currentFrame.ord);
-        f.arc = arc;
-        if (f.fpOrig == fp && f.nextEnt != -1) {
-          //if (DEBUG) System.out.println("      push reused frame ord=" + f.ord + " fp=" + f.fp + " isFloor?=" + f.isFloor + " hasTerms=" + f.hasTerms + " pref=" + term + " nextEnt=" + f.nextEnt + " targetBeforeCurrentLength=" + targetBeforeCurrentLength + " term.length=" + term.length + " vs prefix=" + f.prefix);
-          if (f.prefix > targetBeforeCurrentLength) {
-            f.rewind();
-          } else {
-            // if (DEBUG) {
-            //   System.out.println("        skip rewind!");
-            // }
-          }
-          assert length == f.prefix;
-        } else {
-          f.nextEnt = -1;
-          f.prefix = length;
-          f.state.termBlockOrd = 0;
-          f.fpOrig = f.fp = fp;
-          f.lastSubFP = -1;
-          // if (DEBUG) {
-          //   final int sav = term.length;
-          //   term.length = length;
-          //   System.out.println("      push new frame ord=" + f.ord + " fp=" + f.fp + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " pref=" + brToString(term));
-          //   term.length = sav;
-          // }
-        }
-
-        return f;
-      }
-
-      // asserts only
-      private boolean clearEOF() {
-        eof = false;
-        return true;
-      }
-
-      // asserts only
-      private boolean setEOF() {
-        eof = true;
-        return true;
-      }
-
-      @Override
-      public boolean seekExact(final BytesRef target, final boolean useCache) throws IOException {
-
-        if (index == null) {
-          throw new IllegalStateException("terms index was not loaded");
-        }
-
-        if (term.bytes.length <= target.length) {
-          term.bytes = ArrayUtil.grow(term.bytes, 1+target.length);
-        }
-
-        assert clearEOF();
-
-        // if (DEBUG) {
-        //   System.out.println("\nBTTR.seekExact seg=" + segment + " target=" + fieldInfo.name + ":" + brToString(target) + " current=" + brToString(term) + " (exists?=" + termExists + ") validIndexPrefix=" + validIndexPrefix);
-        //   printSeekState();
-        // }
-
-        FST.Arc<BytesRef> arc;
-        int targetUpto;
-        BytesRef output;
-
-        targetBeforeCurrentLength = currentFrame.ord;
-
-        if (currentFrame != staticFrame) {
-
-          // We are already seek'd; find the common
-          // prefix of new seek term vs current term and
-          // re-use the corresponding seek state.  For
-          // example, if app first seeks to foobar, then
-          // seeks to foobaz, we can re-use the seek state
-          // for the first 5 bytes.
-
-          // if (DEBUG) {
-          //   System.out.println("  re-use current seek state validIndexPrefix=" + validIndexPrefix);
-          // }
-
-          arc = arcs[0];
-          assert arc.isFinal();
-          output = arc.output;
-          targetUpto = 0;
-          
-          Frame lastFrame = stack[0];
-          assert validIndexPrefix <= term.length;
-
-          final int targetLimit = Math.min(target.length, validIndexPrefix);
-
-          int cmp = 0;
-
-          // TODO: reverse vLong byte order for better FST
-          // prefix output sharing
-
-          // First compare up to valid seek frames:
-          while (targetUpto < targetLimit) {
-            cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-            // if (DEBUG) {
-            //   System.out.println("    cycle targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")"   + " arc.output=" + arc.output + " output=" + output);
-            // }
-            if (cmp != 0) {
-              break;
-            }
-            arc = arcs[1+targetUpto];
-            //if (arc.label != (target.bytes[target.offset + targetUpto] & 0xFF)) {
-            //System.out.println("FAIL: arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF));
-            //}
-            assert arc.label == (target.bytes[target.offset + targetUpto] & 0xFF): "arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF);
-            if (arc.output != NO_OUTPUT) {
-              output = fstOutputs.add(output, arc.output);
-            }
-            if (arc.isFinal()) {
-              lastFrame = stack[1+lastFrame.ord];
-            }
-            targetUpto++;
-          }
-
-          if (cmp == 0) {
-            final int targetUptoMid = targetUpto;
-
-            // Second compare the rest of the term, but
-            // don't save arc/output/frame; we only do this
-            // to find out if the target term is before,
-            // equal or after the current term
-            final int targetLimit2 = Math.min(target.length, term.length);
-            while (targetUpto < targetLimit2) {
-              cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-              // if (DEBUG) {
-              //   System.out.println("    cycle2 targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")");
-              // }
-              if (cmp != 0) {
-                break;
-              }
-              targetUpto++;
-            }
-
-            if (cmp == 0) {
-              cmp = term.length - target.length;
-            }
-            targetUpto = targetUptoMid;
-          }
-
-          if (cmp < 0) {
-            // Common case: target term is after current
-            // term, ie, app is seeking multiple terms
-            // in sorted order
-            // if (DEBUG) {
-            //   System.out.println("  target is after current (shares prefixLen=" + targetUpto + "); frame.ord=" + lastFrame.ord);
-            // }
-            currentFrame = lastFrame;
-
-          } else if (cmp > 0) {
-            // Uncommon case: target term
-            // is before current term; this means we can
-            // keep the currentFrame but we must rewind it
-            // (so we scan from the start)
-            targetBeforeCurrentLength = 0;
-            // if (DEBUG) {
-            //   System.out.println("  target is before current (shares prefixLen=" + targetUpto + "); rewind frame ord=" + lastFrame.ord);
-            // }
-            currentFrame = lastFrame;
-            currentFrame.rewind();
-          } else {
-            // Target is exactly the same as current term
-            assert term.length == target.length;
-            if (termExists) {
-              // if (DEBUG) {
-              //   System.out.println("  target is same as current; return true");
-              // }
-              return true;
-            } else {
-              // if (DEBUG) {
-              //   System.out.println("  target is same as current but term doesn't exist");
-              // }
-            }
-            //validIndexPrefix = currentFrame.depth;
-            //term.length = target.length;
-            //return termExists;
-          }
-
-        } else {
-
-          targetBeforeCurrentLength = -1;
-          arc = index.getFirstArc(arcs[0]);
-
-          // Empty string prefix must have an output (block) in the index!
-          assert arc.isFinal();
-          assert arc.output != null;
-
-          // if (DEBUG) {
-          //   System.out.println("    no seek state; push root frame");
-          // }
-
-          output = arc.output;
-
-          currentFrame = staticFrame;
-
-          //term.length = 0;
-          targetUpto = 0;
-          currentFrame = pushFrame(arc, fstOutputs.add(output, arc.nextFinalOutput), 0);
-        }
-
-        // if (DEBUG) {
-        //   System.out.println("  start index loop targetUpto=" + targetUpto + " output=" + output + " currentFrame.ord=" + currentFrame.ord + " targetBeforeCurrentLength=" + targetBeforeCurrentLength);
-        // }
-
-        while (targetUpto < target.length) {
-
-          final int targetLabel = target.bytes[target.offset + targetUpto] & 0xFF;
-
-          final FST.Arc<BytesRef> nextArc = index.findTargetArc(targetLabel, arc, getArc(1+targetUpto));
-
-          if (nextArc == null) {
-
-            // Index is exhausted
-            // if (DEBUG) {
-            //   System.out.println("    index: index exhausted label=" + ((char) targetLabel) + " " + toHex(targetLabel));
-            // }
-            
-            validIndexPrefix = currentFrame.prefix;
-            //validIndexPrefix = targetUpto;
-
-            currentFrame.scanToFloorFrame(target);
-
-            if (!currentFrame.hasTerms) {
-              termExists = false;
-              term.bytes[targetUpto] = (byte) targetLabel;
-              term.length = 1+targetUpto;
-              // if (DEBUG) {
-              //   System.out.println("  FAST NOT_FOUND term=" + brToString(term));
-              // }
-              return false;
-            }
-
-            currentFrame.loadBlock();
-
-            final SeekStatus result = currentFrame.scanToTerm(target, true);            
-            if (result == SeekStatus.FOUND) {
-              // if (DEBUG) {
-              //   System.out.println("  return FOUND term=" + term.utf8ToString() + " " + term);
-              // }
-              return true;
-            } else {
-              // if (DEBUG) {
-              //   System.out.println("  got " + result + "; return NOT_FOUND term=" + brToString(term));
-              // }
-              return false;
-            }
-          } else {
-            // Follow this arc
-            arc = nextArc;
-            term.bytes[targetUpto] = (byte) targetLabel;
-            // Aggregate output as we go:
-            assert arc.output != null;
-            if (arc.output != NO_OUTPUT) {
-              output = fstOutputs.add(output, arc.output);
-            }
-
-            // if (DEBUG) {
-            //   System.out.println("    index: follow label=" + toHex(target.bytes[target.offset + targetUpto]&0xff) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextFinalOutput);
-            // }
-            targetUpto++;
-
-            if (arc.isFinal()) {
-              //if (DEBUG) System.out.println("    arc is final!");
-              currentFrame = pushFrame(arc, fstOutputs.add(output, arc.nextFinalOutput), targetUpto);
-              //if (DEBUG) System.out.println("    curFrame.ord=" + currentFrame.ord + " hasTerms=" + currentFrame.hasTerms);
-            }
-          }
-        }
-
-        //validIndexPrefix = targetUpto;
-        validIndexPrefix = currentFrame.prefix;
-
-        currentFrame.scanToFloorFrame(target);
-
-        // Target term is entirely contained in the index:
-        if (!currentFrame.hasTerms) {
-          termExists = false;
-          term.length = targetUpto;
-          // if (DEBUG) {
-          //   System.out.println("  FAST NOT_FOUND term=" + brToString(term));
-          // }
-          return false;
-        }
-
-        currentFrame.loadBlock();
-
-        final SeekStatus result = currentFrame.scanToTerm(target, true);            
-        if (result == SeekStatus.FOUND) {
-          // if (DEBUG) {
-          //   System.out.println("  return FOUND term=" + term.utf8ToString() + " " + term);
-          // }
-          return true;
-        } else {
-          // if (DEBUG) {
-          //   System.out.println("  got result " + result + "; return NOT_FOUND term=" + term.utf8ToString());
-          // }
-
-          return false;
-        }
-      }
-
-      @Override
-      public SeekStatus seekCeil(final BytesRef target, final boolean useCache) throws IOException {
-        if (index == null) {
-          throw new IllegalStateException("terms index was not loaded");
-        }
-   
-        if (term.bytes.length <= target.length) {
-          term.bytes = ArrayUtil.grow(term.bytes, 1+target.length);
-        }
-
-        assert clearEOF();
-
-        //if (DEBUG) {
-        //System.out.println("\nBTTR.seekCeil seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + brToString(term) + " (exists?=" + termExists + ") validIndexPrefix=  " + validIndexPrefix);
-        //printSeekState();
-        //}
-
-        FST.Arc<BytesRef> arc;
-        int targetUpto;
-        BytesRef output;
-
-        targetBeforeCurrentLength = currentFrame.ord;
-
-        if (currentFrame != staticFrame) {
-
-          // We are already seek'd; find the common
-          // prefix of new seek term vs current term and
-          // re-use the corresponding seek state.  For
-          // example, if app first seeks to foobar, then
-          // seeks to foobaz, we can re-use the seek state
-          // for the first 5 bytes.
-
-          //if (DEBUG) {
-          //System.out.println("  re-use current seek state validIndexPrefix=" + validIndexPrefix);
-          //}
-
-          arc = arcs[0];
-          assert arc.isFinal();
-          output = arc.output;
-          targetUpto = 0;
-          
-          Frame lastFrame = stack[0];
-          assert validIndexPrefix <= term.length;
-
-          final int targetLimit = Math.min(target.length, validIndexPrefix);
-
-          int cmp = 0;
-
-          // TOOD: we should write our vLong backwards (MSB
-          // first) to get better sharing from the FST
-
-          // First compare up to valid seek frames:
-          while (targetUpto < targetLimit) {
-            cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-            //if (DEBUG) {
-            //System.out.println("    cycle targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")"   + " arc.output=" + arc.output + " output=" + output);
-            //}
-            if (cmp != 0) {
-              break;
-            }
-            arc = arcs[1+targetUpto];
-            assert arc.label == (target.bytes[target.offset + targetUpto] & 0xFF): "arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF);
-            // TOOD: we could save the outputs in local
-            // byte[][] instead of making new objs ever
-            // seek; but, often the FST doesn't have any
-            // shared bytes (but this could change if we
-            // reverse vLong byte order)
-            if (arc.output != NO_OUTPUT) {
-              output = fstOutputs.add(output, arc.output);
-            }
-            if (arc.isFinal()) {
-              lastFrame = stack[1+lastFrame.ord];
-            }
-            targetUpto++;
-          }
-
-
-          if (cmp == 0) {
-            final int targetUptoMid = targetUpto;
-            // Second compare the rest of the term, but
-            // don't save arc/output/frame:
-            final int targetLimit2 = Math.min(target.length, term.length);
-            while (targetUpto < targetLimit2) {
-              cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-              //if (DEBUG) {
-              //System.out.println("    cycle2 targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")");
-              //}
-              if (cmp != 0) {
-                break;
-              }
-              targetUpto++;
-            }
-
-            if (cmp == 0) {
-              cmp = term.length - target.length;
-            }
-            targetUpto = targetUptoMid;
-          }
-
-          if (cmp < 0) {
-            // Common case: target term is after current
-            // term, ie, app is seeking multiple terms
-            // in sorted order
-            //if (DEBUG) {
-            //System.out.println("  target is after current (shares prefixLen=" + targetUpto + "); clear frame.scanned ord=" + lastFrame.ord);
-            //}
-            currentFrame = lastFrame;
-
-          } else if (cmp > 0) {
-            // Uncommon case: target term
-            // is before current term; this means we can
-            // keep the currentFrame but we must rewind it
-            // (so we scan from the start)
-            targetBeforeCurrentLength = 0;
-            //if (DEBUG) {
-            //System.out.println("  target is before current (shares prefixLen=" + targetUpto + "); rewind frame ord=" + lastFrame.ord);
-            //}
-            currentFrame = lastFrame;
-            currentFrame.rewind();
-          } else {
-            // Target is exactly the same as current term
-            assert term.length == target.length;
-            if (termExists) {
-              //if (DEBUG) {
-              //System.out.println("  target is same as current; return FOUND");
-              //}
-              return SeekStatus.FOUND;
-            } else {
-              //if (DEBUG) {
-              //System.out.println("  target is same as current but term doesn't exist");
-              //}
-            }
-          }
-
-        } else {
-
-          targetBeforeCurrentLength = -1;
-          arc = index.getFirstArc(arcs[0]);
-
-          // Empty string prefix must have an output (block) in the index!
-          assert arc.isFinal();
-          assert arc.output != null;
-
-          //if (DEBUG) {
-          //System.out.println("    no seek state; push root frame");
-          //}
-
-          output = arc.output;
-
-          currentFrame = staticFrame;
-
-          //term.length = 0;
-          targetUpto = 0;
-          currentFrame = pushFrame(arc, fstOutputs.add(output, arc.nextFinalOutput), 0);
-        }
-
-        //if (DEBUG) {
-        //System.out.println("  start index loop targetUpto=" + targetUpto + " output=" + output + " currentFrame.ord+1=" + currentFrame.ord + " targetBeforeCurrentLength=" + targetBeforeCurrentLength);
-        //}
-
-        while (targetUpto < target.length) {
-
-          final int targetLabel = target.bytes[target.offset + targetUpto] & 0xFF;
-
-          final FST.Arc<BytesRef> nextArc = index.findTargetArc(targetLabel, arc, getArc(1+targetUpto));
-
-          if (nextArc == null) {
-
-            // Index is exhausted
-            // if (DEBUG) {
-            //   System.out.println("    index: index exhausted label=" + ((char) targetLabel) + " " + toHex(targetLabel));
-            // }
-            
-            validIndexPrefix = currentFrame.prefix;
-            //validIndexPrefix = targetUpto;
-
-            currentFrame.scanToFloorFrame(target);
-
-            currentFrame.loadBlock();
-
-            final SeekStatus result = currentFrame.scanToTerm(target, false);
-            if (result == SeekStatus.END) {
-              term.copyBytes(target);
-              termExists = false;
-
-              if (next() != null) {
-                //if (DEBUG) {
-                //System.out.println("  return NOT_FOUND term=" + brToString(term) + " " + term);
-                //}
-                return SeekStatus.NOT_FOUND;
-              } else {
-                //if (DEBUG) {
-                //System.out.println("  return END");
-                //}
-                return SeekStatus.END;
-              }
-            } else {
-              //if (DEBUG) {
-              //System.out.println("  return " + result + " term=" + brToString(term) + " " + term);
-              //}
-              return result;
-            }
-          } else {
-            // Follow this arc
-            term.bytes[targetUpto] = (byte) targetLabel;
-            arc = nextArc;
-            // Aggregate output as we go:
-            assert arc.output != null;
-            if (arc.output != NO_OUTPUT) {
-              output = fstOutputs.add(output, arc.output);
-            }
-
-            //if (DEBUG) {
-            //System.out.println("    index: follow label=" + toHex(target.bytes[target.offset + targetUpto]&0xff) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextFinalOutput);
-            //}
-            targetUpto++;
-
-            if (arc.isFinal()) {
-              //if (DEBUG) System.out.println("    arc is final!");
-              currentFrame = pushFrame(arc, fstOutputs.add(output, arc.nextFinalOutput), targetUpto);
-              //if (DEBUG) System.out.println("    curFrame.ord=" + currentFrame.ord + " hasTerms=" + currentFrame.hasTerms);
-            }
-          }
-        }
-
-        //validIndexPrefix = targetUpto;
-        validIndexPrefix = currentFrame.prefix;
-
-        currentFrame.scanToFloorFrame(target);
-
-        currentFrame.loadBlock();
-
-        final SeekStatus result = currentFrame.scanToTerm(target, false);
-
-        if (result == SeekStatus.END) {
-          term.copyBytes(target);
-          termExists = false;
-          if (next() != null) {
-            //if (DEBUG) {
-            //System.out.println("  return NOT_FOUND term=" + term.utf8ToString() + " " + term);
-            //}
-            return SeekStatus.NOT_FOUND;
-          } else {
-            //if (DEBUG) {
-            //System.out.println("  return END");
-            //}
-            return SeekStatus.END;
-          }
-        } else {
-          return result;
-        }
-      }
-
-      private void printSeekState() throws IOException {
-        if (currentFrame == staticFrame) {
-          System.out.println("  no prior seek");
-        } else {
-          System.out.println("  prior seek state:");
-          int ord = 0;
-          boolean isSeekFrame = true;
-          while(true) {
-            Frame f = getFrame(ord);
-            assert f != null;
-            final BytesRef prefix = new BytesRef(term.bytes, 0, f.prefix);
-            if (f.nextEnt == -1) {
-              System.out.println("    frame " + (isSeekFrame ? "(seek)" : "(next)") + " ord=" + ord + " fp=" + f.fp + (f.isFloor ? (" (fpOrig=" + f.fpOrig + ")") : "") + " prefixLen=" + f.prefix + " prefix=" + prefix + (f.nextEnt == -1 ? "" : (" (of " + f.entCount + ")")) + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " code=" + ((f.fp<<BlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) + (f.hasTerms ? BlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) + (f.isFloor ? BlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0)) + " isLastInFloor=" + f.isLastInFloor + " mdUpto=" + f.metaDataUpto + " tbOrd=" + f.getTermBlockOrd());
-            } else {
-              System.out.println("    frame " + (isSeekFrame ? "(seek, loaded)" : "(next, loaded)") + " ord=" + ord + " fp=" + f.fp + (f.isFloor ? (" (fpOrig=" + f.fpOrig + ")") : "") + " prefixLen=" + f.prefix + " prefix=" + prefix + " nextEnt=" + f.nextEnt + (f.nextEnt == -1 ? "" : (" (of " + f.entCount + ")")) + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " code=" + ((f.fp<<BlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) + (f.hasTerms ? BlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) + (f.isFloor ? BlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0)) + " lastSubFP=" + f.lastSubFP + " isLastInFloor=" + f.isLastInFloor + " mdUpto=" + f.metaDataUpto + " tbOrd=" + f.getTermBlockOrd());
-            }
-            if (index != null) {
-              assert !isSeekFrame || f.arc != null: "isSeekFrame=" + isSeekFrame + " f.arc=" + f.arc;
-              if (f.prefix > 0 && isSeekFrame && f.arc.label != (term.bytes[f.prefix-1]&0xFF)) {
-                System.out.println("      broken seek state: arc.label=" + (char) f.arc.label + " vs term byte=" + (char) (term.bytes[f.prefix-1]&0xFF));
-                throw new RuntimeException("seek state is broken");
-              }
-              BytesRef output = Util.get(index, prefix);
-              if (output == null) {
-                System.out.println("      broken seek state: prefix is not final in index");
-                throw new RuntimeException("seek state is broken");
-              } else if (isSeekFrame && !f.isFloor) {
-                final ByteArrayDataInput reader = new ByteArrayDataInput(output.bytes, output.offset, output.length);
-                final long codeOrig = reader.readVLong();
-                final long code = (f.fp << BlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) | (f.hasTerms ? BlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) | (f.isFloor ? BlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0);
-                if (codeOrig != code) {
-                  System.out.println("      broken seek state: output code=" + codeOrig + " doesn't match frame code=" + code);
-                  throw new RuntimeException("seek state is broken");
-                }
-              }
-            }
-            if (f == currentFrame) {
-              break;
-            }
-            if (f.prefix == validIndexPrefix) {
-              isSeekFrame = false;
-            }
-            ord++;
-          }
-        }
-      }
-
-      /* Decodes only the term bytes of the next term.  If caller then asks for
-         metadata, ie docFreq, totalTermFreq or pulls a D/&PEnum, we then (lazily)
-         decode all metadata up to the current term. */
-      @Override
-      public BytesRef next() throws IOException {
-
-        if (in == null) {
-          // Fresh TermsEnum; seek to first term:
-          final FST.Arc<BytesRef> arc;
-          if (index != null) {
-            arc = index.getFirstArc(arcs[0]);
-            // Empty string prefix must have an output in the index!
-            assert arc.isFinal();
-          } else {
-            arc = null;
-          }
-          currentFrame = pushFrame(arc, rootCode, 0);
-          currentFrame.loadBlock();
-        }
-
-        targetBeforeCurrentLength = currentFrame.ord;
-
-        assert !eof;
-        //if (DEBUG) {
-        //System.out.println("\nBTTR.next seg=" + segment + " term=" + brToString(term) + " termExists?=" + termExists + " field=" + fieldInfo.name + " termBlockOrd=" + currentFrame.state.termBlockOrd + " validIndexPrefix=" + validIndexPrefix);
-        //printSeekState();
-        //}
-
-        if (currentFrame == staticFrame) {
-          // If seek was previously called and the term was
-          // cached, or seek(TermState) was called, usually
-          // caller is just going to pull a D/&PEnum or get
-          // docFreq, etc.  But, if they then call next(),
-          // this method catches up all internal state so next()
-          // works properly:
-          //if (DEBUG) System.out.println("  re-seek to pending term=" + term.utf8ToString() + " " + term);
-          final boolean result = seekExact(term, false);
-          assert result;
-        }
-
-        // Pop finished blocks
-        while (currentFrame.nextEnt == currentFrame.entCount) {
-          if (!currentFrame.isLastInFloor) {
-            currentFrame.loadNextFloorBlock();
-          } else {
-            //if (DEBUG) System.out.println("  pop frame");
-            if (currentFrame.ord == 0) {
-              //if (DEBUG) System.out.println("  return null");
-              assert setEOF();
-              term.length = 0;
-              validIndexPrefix = 0;
-              currentFrame.rewind();
-              termExists = false;
-              return null;
-            }
-            final long lastFP = currentFrame.fpOrig;
-            currentFrame = stack[currentFrame.ord-1];
-
-            if (currentFrame.nextEnt == -1 || currentFrame.lastSubFP != lastFP) {
-              // We popped into a frame that's not loaded
-              // yet or not scan'd to the right entry
-              currentFrame.scanToFloorFrame(term);
-              currentFrame.loadBlock();
-              currentFrame.scanToSubBlock(lastFP);
-            }
-
-            // Note that the seek state (last seek) has been
-            // invalidated beyond this depth
-            validIndexPrefix = Math.min(validIndexPrefix, currentFrame.prefix);
-            //if (DEBUG) {
-            //System.out.println("  reset validIndexPrefix=" + validIndexPrefix);
-            //}
-          }
-        }
-
-        while(true) {
-          if (currentFrame.next()) {
-            // Push to new block:
-            //if (DEBUG) System.out.println("  push frame");
-            currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length);
-            // This is a "next" frame -- even if it's
-            // floor'd we must pretend it isn't so we don't
-            // try to scan to the right floor frame:
-            currentFrame.isFloor = false;
-            //currentFrame.hasTerms = true;
-            currentFrame.loadBlock();
-          } else {
-            //if (DEBUG) System.out.println("  return term=" + term.utf8ToString() + " " + term + " currentFrame.ord=" + currentFrame.ord);
-            return term;
-          }
-        }
-      }
-
-      @Override
-      public BytesRef term() {
-        assert !eof;
-        return term;
-      }
-
-      @Override
-      public int docFreq() throws IOException {
-        assert !eof;
-        //if (DEBUG) System.out.println("BTR.docFreq");
-        currentFrame.decodeMetaData();
-        //if (DEBUG) System.out.println("  return " + currentFrame.state.docFreq);
-        return currentFrame.state.docFreq;
-      }
-
-      @Override
-      public long totalTermFreq() throws IOException {
-        assert !eof;
-        currentFrame.decodeMetaData();
-        return currentFrame.state.totalTermFreq;
-      }
-
-      @Override
-      public DocsEnum docs(Bits skipDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-        assert !eof;
-        //if (DEBUG) {
-        //System.out.println("BTTR.docs seg=" + segment);
-        //}
-        currentFrame.decodeMetaData();
-        //if (DEBUG) {
-        //System.out.println("  state=" + currentFrame.state);
-        //}
-        return postingsReader.docs(fieldInfo, currentFrame.state, skipDocs, reuse, needsFreqs);
-      }
-
-      @Override
-      public DocsAndPositionsEnum docsAndPositions(Bits skipDocs, DocsAndPositionsEnum reuse) throws IOException {
-        assert !eof;
-        //System.out.println("BTR.d&p this=" + this);
-        if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-          return null;
-        } else {
-          currentFrame.decodeMetaData();
-          DocsAndPositionsEnum dpe = postingsReader.docsAndPositions(fieldInfo, currentFrame.state, skipDocs, reuse);
-          //System.out.println("  return d&pe=" + dpe);
-          return dpe;
-        }
-      }
-
-      @Override
-      public void seekExact(BytesRef target, TermState otherState) throws IOException {
-        // if (DEBUG) {
-        //   System.out.println("BTTR.seekExact termState seg=" + segment + " target=" + target.utf8ToString() + " " + target + " state=" + otherState);
-        // }
-        assert clearEOF();
-        if (target.compareTo(term) != 0 || !termExists) {
-          assert otherState != null && otherState instanceof BlockTermState;
-          currentFrame = staticFrame;
-          currentFrame.state.copyFrom(otherState);
-          term.copyBytes(target);
-          currentFrame.metaDataUpto = currentFrame.getTermBlockOrd();
-          assert currentFrame.metaDataUpto > 0;
-          validIndexPrefix = 0;
-        } else {
-          // if (DEBUG) {
-          //   System.out.println("  skip seek: already on target state=" + currentFrame.state);
-          // }
-        }
-      }
-      
-      @Override
-      public TermState termState() throws IOException {
-        assert !eof;
-        currentFrame.decodeMetaData();
-        TermState ts = (TermState) currentFrame.state.clone();
-        //if (DEBUG) System.out.println("BTTR.termState seg=" + segment + " state=" + ts);
-        return ts;
-      }
-
-      @Override
-      public void seekExact(long ord) throws IOException {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public long ord() {
-        throw new UnsupportedOperationException();
-      }
-
-      // Not static -- references term, postingsReader,
-      // fieldInfo, in
-      private final class Frame {
-        // Our index in stack[]:
-        final int ord;
-
-        boolean hasTerms;
-        boolean hasTermsOrig;
-        boolean isFloor;
-
-        FST.Arc<BytesRef> arc;
-
-        // File pointer where this block was loaded from
-        long fp;
-        long fpOrig;
-        long fpEnd;
-
-        byte[] suffixBytes = new byte[128];
-        final ByteArrayDataInput suffixesReader = new ByteArrayDataInput();
-
-        byte[] statBytes = new byte[64];
-        final ByteArrayDataInput statsReader = new ByteArrayDataInput();
-
-        byte[] floorData = new byte[32];
-        final ByteArrayDataInput floorDataReader = new ByteArrayDataInput();
-
-        // Length of prefix shared by all terms in this block
-        int prefix;
-
-        // Number of entries (term or sub-block) in this block
-        int entCount;
-
-        // Which term we will next read, or -1 if the block
-        // isn't loaded yet
-        int nextEnt;
-
-        // True if this block is either not a floor block,
-        // or, it's the last sub-block of a floor block
-        boolean isLastInFloor;
-
-        // True if all entries are terms
-        boolean isLeafBlock;
-
-        long lastSubFP;
-
-        int nextFloorLabel;
-        int numFollowFloorBlocks;
-
-        // Next term to decode metaData; we decode metaData
-        // lazily so that scanning to find the matching term is
-        // fast and only if you find a match and app wants the
-        // stats or docs/positions enums, will we decode the
-        // metaData
-        int metaDataUpto;
-
-        final BlockTermState state;
-
-        public Frame(int ord) throws IOException {
-          this.ord = ord;
-          state = postingsReader.newTermState();
-          state.totalTermFreq = -1;
-        }
-
-        public void setFloorData(ByteArrayDataInput in, BytesRef source) {
-          final int numBytes = source.length - (in.getPosition() - source.offset);
-          if (numBytes > floorData.length) {
-            floorData = new byte[ArrayUtil.oversize(numBytes, 1)];
-          }
-          System.arraycopy(source.bytes, source.offset+in.getPosition(), floorData, 0, numBytes);
-          floorDataReader.reset(floorData, 0, numBytes);
-          numFollowFloorBlocks = floorDataReader.readVInt();
-          nextFloorLabel = floorDataReader.readByte() & 0xff;
-          //if (DEBUG) {
-          //System.out.println("    setFloorData fpOrig=" + fpOrig + " bytes=" + new BytesRef(source.bytes, source.offset + in.getPosition(), numBytes) + " numFollowFloorBlocks=" + numFollowFloorBlocks + " nextFloorLabel=" + toHex(nextFloorLabel));
-          //}
-        }
-
-        public int getTermBlockOrd() {
-          return isLeafBlock ? nextEnt : state.termBlockOrd;
-        }
-
-        void loadNextFloorBlock() throws IOException {
-          //if (DEBUG) {
-          //System.out.println("    loadNextFloorBlock fp=" + fp + " fpEnd=" + fpEnd);
-          //}
-          assert arc == null || isFloor: "arc=" + arc + " isFloor=" + isFloor;
-          fp = fpEnd;
-          nextEnt = -1;
-          loadBlock();
-        }
-
-        /* Does initial decode of next block of terms; this
-           doesn't actually decode the docFreq, totalTermFreq,
-           postings details (frq/prx offset, etc.) metadata;
-           it just loads them as byte[] blobs which are then      
-           decoded on-demand if the metadata is ever requested
-           for any term in this block.  This enables terms-only
-           intensive consumes (eg certain MTQs, respelling) to
-           not pay the price of decoding metadata they won't
-           use. */
-        void loadBlock() throws IOException {
-
-          // Clone the IndexInput lazily, so that consumers
-          // that just pull a TermsEnum to
-          // seekExact(TermState) don't pay this cost:
-          initIndexInput();
-
-          if (nextEnt != -1) {
-            // Already loaded
-            return;
-          }
-          //System.out.println("blc=" + blockLoadCount);
-
-          in.seek(fp);
-          int code = in.readVInt();
-          entCount = code >>> 1;
-          assert entCount > 0;
-          isLastInFloor = (code & 1) != 0;
-          assert arc == null || (isLastInFloor || isFloor);
-
-          // TODO: if suffixes were stored in random-access
-          // array structure, then we could do binary search
-          // instead of linear scan to find target term; eg
-          // we could have simple array of offsets
-
-          // term suffixes:
-          code = in.readVInt();
-          isLeafBlock = (code & 1) != 0;
-          int numBytes = code >>> 1;
-          if (suffixBytes.length < numBytes) {
-            suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-          }
-          in.readBytes(suffixBytes, 0, numBytes);
-          suffixesReader.reset(suffixBytes, 0, numBytes);
-
-          /*if (DEBUG) {
-            if (arc == null) {
-              System.out.println("    loadBlock (next) fp=" + fp + " entCount=" + entCount + " prefixLen=" + prefix + " isLastInFloor=" + isLastInFloor + " leaf?=" + isLeafBlock);
-            } else {
-              System.out.println("    loadBlock (seek) fp=" + fp + " entCount=" + entCount + " prefixLen=" + prefix + " hasTerms?=" + hasTerms + " isFloor?=" + isFloor + " isLastInFloor=" + isLastInFloor + " leaf?=" + isLeafBlock);
-            }
-            }*/
-
-          // stats
-          numBytes = in.readVInt();
-          if (statBytes.length < numBytes) {
-            statBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-          }
-          in.readBytes(statBytes, 0, numBytes);
-          statsReader.reset(statBytes, 0, numBytes);
-          metaDataUpto = 0;
-
-          state.termBlockOrd = 0;
-          nextEnt = 0;
-          lastSubFP = -1;
-
-          // TODO: we could skip this if !hasTerms; but
-          // that's rare so won't help much
-          postingsReader.readTermsBlock(in, fieldInfo, state);
-
-          // Sub-blocks of a single floor block are always
-          // written one after another -- tail recurse:
-          fpEnd = in.getFilePointer();
-          // if (DEBUG) {
-          //   System.out.println("      fpEnd=" + fpEnd);
-          // }
-        }
-
-        void rewind() throws IOException {
-
-          // Force reload:
-          fp = fpOrig;
-          nextEnt = -1;
-          hasTerms = hasTermsOrig;
-          if (isFloor) {
-            floorDataReader.rewind();
-            numFollowFloorBlocks = floorDataReader.readVInt();
-            nextFloorLabel = floorDataReader.readByte() & 0xff;
-          }
-
-          /*
-          //System.out.println("rewind");
-          // Keeps the block loaded, but rewinds its state:
-          if (nextEnt > 0 || fp != fpOrig) {
-            if (DEBUG) {
-              System.out.println("      rewind frame ord=" + ord + " fpOrig=" + fpOrig + " fp=" + fp + " hasTerms?=" + hasTerms + " isFloor?=" + isFloor + " nextEnt=" + nextEnt + " prefixLen=" + prefix);
-            }
-            if (fp != fpOrig) {
-              fp = fpOrig;
-              nextEnt = -1;
-            } else {
-              nextEnt = 0;
-            }
-            hasTerms = hasTermsOrig;
-            if (isFloor) {
-              floorDataReader.rewind();
-              numFollowFloorBlocks = floorDataReader.readVInt();
-              nextFloorLabel = floorDataReader.readByte() & 0xff;
-            }
-            assert suffixBytes != null;
-            suffixesReader.rewind();
-            assert statBytes != null;
-            statsReader.rewind();
-            metaDataUpto = 0;
-            state.termBlockOrd = 0;
-            // TODO: skip this if !hasTerms?  Then postings
-            // impl wouldn't have to write useless 0 byte
-            postingsReader.resetTermsBlock(fieldInfo, state);
-            lastSubFP = -1;
-          } else if (DEBUG) {
-            System.out.println("      skip rewind fp=" + fp + " fpOrig=" + fpOrig + " nextEnt=" + nextEnt + " ord=" + ord);
-          }
-          */
-        }
-
-        public boolean next() {
-          return isLeafBlock ? nextLeaf() : nextNonLeaf();
-        }
-
-        // Decodes next entry; returns true if it's a sub-block
-        public boolean nextLeaf() {
-          //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
-          assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
-          nextEnt++;
-          suffix = suffixesReader.readVInt();
-          startBytePos = suffixesReader.getPosition();
-          term.length = prefix + suffix;
-          if (term.bytes.length < term.length) {
-            term.grow(term.length);
-          }
-          suffixesReader.readBytes(term.bytes, prefix, suffix);
-          // A normal term
-          termExists = true;
-          return false;
-        }
-
-        public boolean nextNonLeaf() {
-          //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
-          assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
-          nextEnt++;
-          final int code = suffixesReader.readVInt();
-          suffix = code >>> 1;
-          startBytePos = suffixesReader.getPosition();
-          term.length = prefix + suffix;
-          if (term.bytes.length < term.length) {
-            term.grow(term.length);
-          }
-          suffixesReader.readBytes(term.bytes, prefix, suffix);
-          if ((code & 1) == 0) {
-            // A normal term
-            termExists = true;
-            state.termBlockOrd++;
-            return false;
-          } else {
-            // A sub-block; make sub-FP absolute:
-            termExists = false;
-            lastSubFP = fp - suffixesReader.readVLong();
-            //if (DEBUG) {
-            //System.out.println("    lastSubFP=" + lastSubFP);
-            //}
-            return true;
-          }
-        }
-        
-        // TODO: make this array'd so we can do bin search?
-        // likely not worth it?  need to measure how many
-        // floor blocks we "typically" get
-        public void scanToFloorFrame(BytesRef target) {
-
-          if (!isFloor || target.length <= prefix) {
-            // if (DEBUG) {
-            //   System.out.println("    scanToFloorFrame skip: isFloor=" + isFloor + " target.length=" + target.length + " vs prefix=" + prefix);
-            // }
-            return;
-          }
-
-          final int targetLabel = target.bytes[target.offset + prefix] & 0xFF;
-
-          // if (DEBUG) {
-          //   System.out.println("    scanToFloorFrame fpOrig=" + fpOrig + " targetLabel=" + toHex(targetLabel) + " vs nextFloorLabel=" + toHex(nextFloorLabel) + " numFollowFloorBlocks=" + numFollowFloorBlocks);
-          // }
-
-          if (targetLabel < nextFloorLabel) {
-            // if (DEBUG) {
-            //   System.out.println("      already on correct block");
-            // }
-            return;
-          }
-
-          assert numFollowFloorBlocks != 0;
-
-          long newFP = fpOrig;
-          while (true) {
-            final long code = floorDataReader.readVLong();
-            newFP = fpOrig + (code >>> 1);
-            hasTerms = (code & 1) != 0;
-            // if (DEBUG) {
-            //   System.out.println("      label=" + toHex(nextFloorLabel) + " fp=" + newFP + " hasTerms?=" + hasTerms + " numFollowFloor=" + numFollowFloorBlocks);
-            // }
-            
-            isLastInFloor = numFollowFloorBlocks == 1;
-            numFollowFloorBlocks--;
-
-            if (isLastInFloor) {
-              nextFloorLabel = 256;
-              // if (DEBUG) {
-              //   System.out.println("        stop!  last block nextFloorLabel=" + toHex(nextFloorLabel));
-              // }
-              break;
-            } else {
-              nextFloorLabel = floorDataReader.readByte() & 0xff;
-              if (targetLabel < nextFloorLabel) {
-                // if (DEBUG) {
-                //   System.out.println("        stop!  nextFloorLabel=" + toHex(nextFloorLabel));
-                // }
-                break;
-              }
-            }
-          }
-
-          if (newFP != fp) {
-            // Force re-load of the block:
-            // if (DEBUG) {
-            //   System.out.println("      force switch to fp=" + newFP + " oldFP=" + fp);
-            // }
-            nextEnt = -1;
-            fp = newFP;
-          } else {
-            // if (DEBUG) {
-            //   System.out.println("      stay on same fp=" + newFP);
-            // }
-          }
-        }
-    
-        public void decodeMetaData() throws IOException {
-
-          //if (DEBUG) System.out.println("\nBTTR.decodeMetadata seg=" + segment + " mdUpto=" + metaDataUpto + " vs termBlockOrd=" + state.termBlockOrd);
-
-          // lazily catch up on metadata decode:
-          final int limit = getTermBlockOrd();
-          assert limit > 0;
-
-          // We must set/incr state.termCount because
-          // postings impl can look at this
-          state.termBlockOrd = metaDataUpto;
-      
-          // TODO: better API would be "jump straight to term=N"???
-          while (metaDataUpto < limit) {
-
-            // TODO: we could make "tiers" of metadata, ie,
-            // decode docFreq/totalTF but don't decode postings
-            // metadata; this way caller could get
-            // docFreq/totalTF w/o paying decode cost for
-            // postings
-
-            // TODO: if docFreq were bulk decoded we could
-            // just skipN here:
-            state.docFreq = statsReader.readVInt();
-            //if (DEBUG) System.out.println("    dF=" + state.docFreq);
-            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
-              state.totalTermFreq = state.docFreq + statsReader.readVLong();
-              //if (DEBUG) System.out.println("    totTF=" + state.totalTermFreq);
-            }
-
-            postingsReader.nextTerm(fieldInfo, state);
-            metaDataUpto++;
-            state.termBlockOrd++;
-          }
-        }
-
-        // Used only by assert
-        private boolean prefixMatches(BytesRef target) {
-          for(int bytePos=0;bytePos<prefix;bytePos++) {
-            if (target.bytes[target.offset + bytePos] != term.bytes[bytePos]) {
-              return false;
-            }
-          }
-
-          return true;
-        }
-
-        // Scans to sub-block that has this target fp; only
-        // called by next(); NOTE: does not set
-        // startBytePos/suffix as a side effect
-        public void scanToSubBlock(long subFP) {
-          assert !isLeafBlock;
-          //if (DEBUG) System.out.println("  scanToSubBlock fp=" + fp + " subFP=" + subFP + " entCount=" + entCount + " lastSubFP=" + lastSubFP);
-          //assert nextEnt == 0;
-          if (lastSubFP == subFP) {
-            //if (DEBUG) System.out.println("    already positioned");
-            return;
-          }
-          assert subFP < fp : "fp=" + fp + " subFP=" + subFP;
-          final long targetSubCode = fp - subFP;
-          //if (DEBUG) System.out.println("    targetSubCode=" + targetSubCode);
-          while(true) {
-            assert nextEnt < entCount;
-            nextEnt++;
-            final int code = suffixesReader.readVInt();
-            suffixesReader.skipBytes(isLeafBlock ? code : code >>> 1);
-            //if (DEBUG) System.out.println("    " + nextEnt + " (of " + entCount + ") ent isSubBlock=" + ((code&1)==1));
-            if ((code & 1) != 0) {
-              final long subCode = suffixesReader.readVLong();
-              //if (DEBUG) System.out.println("      subCode=" + subCode);
-              if (targetSubCode == subCode) {
-                //if (DEBUG) System.out.println("        match!");
-                lastSubFP = subFP;
-                return;
-              }
-            } else {
-              state.termBlockOrd++;
-            }
-          }
-        }
-
-        // NOTE: sets startBytePos/suffix as a side effect
-        public SeekStatus scanToTerm(BytesRef target, boolean exactOnly) throws IOException {
-          return isLeafBlock ? scanToTermLeaf(target, exactOnly) : scanToTermNonLeaf(target, exactOnly);
-        }
-
-        private int startBytePos;
-        private int suffix;
-        private long subCode;
-
-        // Target's prefix matches this block's prefix; we
-        // scan the entries check if the suffix matches.
-        public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {
-
-          // if (DEBUG) System.out.println("    scanToTermLeaf: block fp=" + fp + " prefix=" + prefix + " nextEnt=" + nextEnt + " (of " + entCount + ") target=" + brToString(target) + " term=" + brToString(term));
-
-          assert nextEnt != -1;
-
-          termExists = true;
-          subCode = 0;
-
-          if (nextEnt == entCount) {
-            if (exactOnly) {
-              fillTerm();
-            }
-            return SeekStatus.END;
-          }
-
-          assert prefixMatches(target);
-
-          // Loop over each entry (term or sub-block) in this block:
-          //nextTerm: while(nextEnt < entCount) {
-          nextTerm: while (true) {
-            nextEnt++;
-
-            suffix = suffixesReader.readVInt();
-
-            // if (DEBUG) {
-            //   BytesRef suffixBytesRef = new BytesRef();
-            //   suffixBytesRef.bytes = suffixBytes;
-            //   suffixBytesRef.offset = suffixesReader.getPosition();
-            //   suffixBytesRef.length = suffix;
-            //   System.out.println("      cycle: term " + (nextEnt-1) + " (of " + entCount + ") suffix=" + brToString(suffixBytesRef));
-            // }
-
-            final int termLen = prefix + suffix;
-            startBytePos = suffixesReader.getPosition();
-            suffixesReader.skipBytes(suffix);
-
-            final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
-            int targetPos = target.offset + prefix;
-
-            // Loop over bytes in the suffix, comparing to
-            // the target
-            int bytePos = startBytePos;
-            while(true) {
-              final int cmp;
-              final boolean stop;
-              if (targetPos < targetLimit) {
-                cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
-                stop = false;
-              } else {
-                assert targetPos == targetLimit;
-                cmp = termLen - target.length;
-                stop = true;
-              }
-
-              if (cmp < 0) {
-                // Current entry is still before the target;
-                // keep scanning
-
-                if (nextEnt == entCount) {
-                  if (exactOnly) {
-                    fillTerm();
-                  }
-                  // We are done scanning this block
-                  break nextTerm;
-                } else {
-                  continue nextTerm;
-                }
-              } else if (cmp > 0) {
-
-                // Done!  Current entry is after target --
-                // return NOT_FOUND:
-                fillTerm();
-
-                if (!exactOnly && !termExists) {
-                  // We are on a sub-block, and caller wants
-                  // us to position to the next term after
-                  // the target, so we must recurse into the
-                  // sub-frame(s):
-                  currentFrame = pushFrame(null, currentFrame.lastSubFP, termLen);
-                  currentFrame.loadBlock();
-                  while (currentFrame.next()) {
-                    currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length);
-                    currentFrame.loadBlock();
-                  }
-                }
-                
-                //if (DEBUG) System.out.println("        not found");
-                return SeekStatus.NOT_FOUND;
-              } else if (stop) {
-                // Exact match!
-
-                // This cannot be a sub-block because we
-                // would have followed the index to this
-                // sub-block from the start:
-
-                assert termExists;
-                fillTerm();
-                //if (DEBUG) System.out.println("        found!");
-                return SeekStatus.FOUND;
-              }
-            }
-          }
-
-          // It is possible (and OK) that terms index pointed us
-          // at this block, but, we scanned the entire block and
-          // did not find the term to position to.  This happens
-          // when the target is after the last term in the block
-          // (but, before the next term in the index).  EG
-          // target could be foozzz, and terms index pointed us
-          // to the foo* block, but the last term in this block
-          // was fooz (and, eg, first term in the next block will
-          // bee fop).
-          //if (DEBUG) System.out.println("      block end");
-          if (exactOnly) {
-            fillTerm();
-          }
-
-          // TODO: not consistent that in the
-          // not-exact case we don't next() into the next
-          // frame here
-          return SeekStatus.END;
-        }
-
-        // Target's prefix matches this block's prefix; we
-        // scan the entries check if the suffix matches.
-        public SeekStatus scanToTermNonLeaf(BytesRef target, boolean exactOnly) throws IOException {
-
-          //if (DEBUG) System.out.println("    scanToTermNonLeaf: block fp=" + fp + " prefix=" + prefix + " nextEnt=" + nextEnt + " (of " + entCount + ") target=" + brToString(target) + " term=" + brToString(term));
-
-          assert nextEnt != -1;
-
-          if (nextEnt == entCount) {
-            if (exactOnly) {
-              fillTerm();
-              termExists = subCode == 0;
-            }
-            return SeekStatus.END;
-          }
-
-          assert prefixMatches(target);
-
-          // Loop over each entry (term or sub-block) in this block:
-          //nextTerm: while(nextEnt < entCount) {
-          nextTerm: while (true) {
-            nextEnt++;
-
-            final int code = suffixesReader.readVInt();
-            suffix = code >>> 1;
-            // if (DEBUG) {
-            //   BytesRef suffixBytesRef = new BytesRef();
-            //   suffixBytesRef.bytes = suffixBytes;
-            //   suffixBytesRef.offset = suffixesReader.getPosition();
-            //   suffixBytesRef.length = suffix;
-            //   System.out.println("      cycle: " + ((code&1)==1 ? "sub-block" : "term") + " " + (nextEnt-1) + " (of " + entCount + ") suffix=" + brToString(suffixBytesRef));
-            // }
-
-            termExists = (code & 1) == 0;
-            final int termLen = prefix + suffix;
-            startBytePos = suffixesReader.getPosition();
-            suffixesReader.skipBytes(suffix);
-            if (termExists) {
-              state.termBlockOrd++;
-              subCode = 0;
-            } else {
-              subCode = suffixesReader.readVLong();
-              lastSubFP = fp - subCode;
-            }
-
-            final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
-            int targetPos = target.offset + prefix;
-
-            // Loop over bytes in the suffix, comparing to
-            // the target
-            int bytePos = startBytePos;
-            while(true) {
-              final int cmp;
-              final boolean stop;
-              if (targetPos < targetLimit) {
-                cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
-                stop = false;
-              } else {
-                assert targetPos == targetLimit;
-                cmp = termLen - target.length;
-                stop = true;
-              }
-
-              if (cmp < 0) {
-                // Current entry is still before the target;
-                // keep scanning
-
-                if (nextEnt == entCount) {
-                  if (exactOnly) {
-                    fillTerm();
-                    //termExists = true;
-                  }
-                  // We are done scanning this block
-                  break nextTerm;
-                } else {
-                  continue nextTerm;
-                }
-              } else if (cmp > 0) {
-
-                // Done!  Current entry is after target --
-                // return NOT_FOUND:
-                fillTerm();
-
-                if (!exactOnly && !termExists) {
-                  // We are on a sub-block, and caller wants
-                  // us to position to the next term after
-                  // the target, so we must recurse into the
-                  // sub-frame(s):
-                  currentFrame = pushFrame(null, currentFrame.lastSubFP, termLen);
-                  currentFrame.loadBlock();
-                  while (currentFrame.next()) {
-                    currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length);
-                    currentFrame.loadBlock();
-                  }
-                }
-                
-                //if (DEBUG) System.out.println("        not found");
-                return SeekStatus.NOT_FOUND;
-              } else if (stop) {
-                // Exact match!
-
-                // This cannot be a sub-block because we
-                // would have followed the index to this
-                // sub-block from the start:
-
-                assert termExists;
-                fillTerm();
-                //if (DEBUG) System.out.println("        found!");
-                return SeekStatus.FOUND;
-              }
-            }
-          }
-
-          // It is possible (and OK) that terms index pointed us
-          // at this block, but, we scanned the entire block and
-          // did not find the term to position to.  This happens
-          // when the target is after the last term in the block
-          // (but, before the next term in the index).  EG
-          // target could be foozzz, and terms index pointed us
-          // to the foo* block, but the last term in this block
-          // was fooz (and, eg, first term in the next block will
-          // bee fop).
-          //if (DEBUG) System.out.println("      block end");
-          if (exactOnly) {
-            fillTerm();
-          }
-
-          // TODO: not consistent that in the
-          // not-exact case we don't next() into the next
-          // frame here
-          return SeekStatus.END;
-        }
-
-        private void fillTerm() {
-          final int termLength = prefix + suffix;
-          term.length = prefix + suffix;
-          if (term.bytes.length < termLength) {
-            term.grow(termLength);
-          }
-          System.arraycopy(suffixBytes, startBytePos, term.bytes, prefix, suffix);
-        }
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsWriter.java
deleted file mode 100644
index 86fff64..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/BlockTreeTermsWriter.java
+++ /dev/null
@@ -1,949 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.OutputStreamWriter;
-import java.io.Writer;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.ByteSequenceOutputs;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.NoOutputs;
-import org.apache.lucene.util.fst.Util;
-
-/*
-  TODO:
-  
-    - Currently there is a one-to-one mapping of indexed
-      term to term block, but we could decouple the two, ie,
-      put more terms into the index than there are blocks.
-      The index would take up more RAM but then it'd be able
-      to avoid seeking more often and could make PK/FuzzyQ
-      faster if the additional indexed terms could store
-      the offset into the terms block.
-
-    - The blocks are not written in true depth-first
-      order, meaning if you just next() the file pointer will
-      sometimes jump backwards.  For example, block foo* will
-      be written before block f* because it finished before.
-      This could possibly hurt performance if the terms dict is
-      not hot, since OSs anticipate sequential file access.  We
-      could fix the writer to re-order the blocks as a 2nd
-      pass.
-
-    - Each block encodes the term suffixes packed
-      sequentially using a separate vInt per term, which is
-      1) wasteful and 2) slow (must linear scan to find a
-      particular suffix).  We should instead 1) make
-      random-access array so we can directly access the Nth
-      suffix, and 2) bulk-encode this array using bulk int[]
-      codecs; then at search time we can binary search when
-      we seek a particular term.
-
-/**
- * Writes terms dict and index, block-encoding (column
- * stride) each term's metadata for each set of terms
- * between two index terms.
- *
- * @lucene.experimental
- */
-
-/** See {@link BlockTreeTermsReader}.
- *
- * @lucene.experimental
-*/
-
-public class BlockTreeTermsWriter extends FieldsConsumer {
-
-  public final static int DEFAULT_MIN_BLOCK_SIZE = 25;
-  public final static int DEFAULT_MAX_BLOCK_SIZE = 48;
-
-  //public final static boolean DEBUG = false;
-  public final static boolean SAVE_DOT_FILES = false;
-
-  static final int OUTPUT_FLAGS_NUM_BITS = 2;
-  static final int OUTPUT_FLAGS_MASK = 0x3;
-  static final int OUTPUT_FLAG_IS_FLOOR = 0x1;
-  static final int OUTPUT_FLAG_HAS_TERMS = 0x2;
-
-  /** Extension of terms file */
-  static final String TERMS_EXTENSION = "tim";
-  final static String TERMS_CODEC_NAME = "BLOCK_TREE_TERMS_DICT";
-  // Initial format
-  public static final int TERMS_VERSION_START = 0;
-  public static final int TERMS_VERSION_CURRENT = TERMS_VERSION_START;
-
-  /** Extension of terms index file */
-  static final String TERMS_INDEX_EXTENSION = "tip";
-  final static String TERMS_INDEX_CODEC_NAME = "BLOCK_TREE_TERMS_INDEX";
-  // Initial format
-  public static final int TERMS_INDEX_VERSION_START = 0;
-  public static final int TERMS_INDEX_VERSION_CURRENT = TERMS_INDEX_VERSION_START;
-
-  private final IndexOutput out;
-  private final IndexOutput indexOut;
-  final int minItemsInBlock;
-  final int maxItemsInBlock;
-
-  final PostingsWriterBase postingsWriter;
-  final FieldInfos fieldInfos;
-  FieldInfo currentField;
-  private final List<TermsWriter> fields = new ArrayList<TermsWriter>();
-  // private final String segment;
-
-  /** Create a new writer.  The number of items (terms or
-   *  sub-blocks) per block will aim to be between
-   *  minItemsPerBlock and maxItemsPerBlock, though in some
-   *  cases the blocks may be smaller than the min. */
-  public BlockTreeTermsWriter(
-                              SegmentWriteState state,
-                              PostingsWriterBase postingsWriter,
-                              int minItemsInBlock,
-                              int maxItemsInBlock)
-    throws IOException
-  {
-    if (minItemsInBlock <= 1) {
-      throw new IllegalArgumentException("minItemsInBlock must be >= 2; got " + minItemsInBlock);
-    }
-    if (maxItemsInBlock <= 0) {
-      throw new IllegalArgumentException("maxItemsInBlock must be >= 1; got " + maxItemsInBlock);
-    }
-    if (minItemsInBlock > maxItemsInBlock) {
-      throw new IllegalArgumentException("maxItemsInBlock must be >= minItemsInBlock; got maxItemsInBlock=" + maxItemsInBlock + " minItemsInBlock=" + minItemsInBlock);
-    }
-    if (2*(minItemsInBlock-1) > maxItemsInBlock) {
-      throw new IllegalArgumentException("maxItemsInBlock must be at least 2*(minItemsInBlock-1); got maxItemsInBlock=" + maxItemsInBlock + " minItemsInBlock=" + minItemsInBlock);
-    }
-
-    final String termsFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_EXTENSION);
-    out = state.directory.createOutput(termsFileName, state.context);
-    boolean success = false;
-    IndexOutput indexOut = null;
-    try {
-      fieldInfos = state.fieldInfos;
-      this.minItemsInBlock = minItemsInBlock;
-      this.maxItemsInBlock = maxItemsInBlock;
-      writeHeader(out);
-
-      //DEBUG = state.segmentName.equals("_4a");
-
-      final String termsIndexFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_INDEX_EXTENSION);
-      indexOut = state.directory.createOutput(termsIndexFileName, state.context);
-      writeIndexHeader(indexOut);
-
-      currentField = null;
-      this.postingsWriter = postingsWriter;
-      // segment = state.segmentName;
-
-      // System.out.println("BTW.init seg=" + state.segmentName);
-
-      postingsWriter.start(out);                          // have consumer write its format/header
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out, indexOut);
-      }
-    }
-    this.indexOut = indexOut;
-  }
-  
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, TERMS_CODEC_NAME, TERMS_VERSION_CURRENT); 
-    out.writeLong(0);                             // leave space for end index pointer    
-  }
-
-  protected void writeIndexHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, TERMS_INDEX_CODEC_NAME, TERMS_INDEX_VERSION_CURRENT); 
-    out.writeLong(0);                             // leave space for end index pointer    
-  }
-
-  protected void writeTrailer(IndexOutput out, long dirStart) throws IOException {
-    out.seek(CodecUtil.headerLength(TERMS_CODEC_NAME));
-    out.writeLong(dirStart);    
-  }
-
-  protected void writeIndexTrailer(IndexOutput indexOut, long dirStart) throws IOException {
-    indexOut.seek(CodecUtil.headerLength(TERMS_INDEX_CODEC_NAME));
-    indexOut.writeLong(dirStart);    
-  }
-  
-  @Override
-  public TermsConsumer addField(FieldInfo field) throws IOException {
-    //DEBUG = field.name.equals("id");
-    //if (DEBUG) System.out.println("\nBTTW.addField seg=" + segment + " field=" + field.name);
-    assert currentField == null || currentField.name.compareTo(field.name) < 0;
-    currentField = field;
-    final TermsWriter terms = new TermsWriter(field);
-    fields.add(terms);
-    return terms;
-  }
-
-  static long encodeOutput(long fp, boolean hasTerms, boolean isFloor) {
-    assert fp < (1L << 62);
-    return (fp << 2) | (hasTerms ? OUTPUT_FLAG_HAS_TERMS : 0) | (isFloor ? OUTPUT_FLAG_IS_FLOOR : 0);
-  }
-
-  private static class PendingEntry {
-    public final boolean isTerm;
-
-    protected PendingEntry(boolean isTerm) {
-      this.isTerm = isTerm;
-    }
-  }
-
-  private static final class PendingTerm extends PendingEntry {
-    public final BytesRef term;
-    public final TermStats stats;
-
-    public PendingTerm(BytesRef term, TermStats stats) {
-      super(true);
-      this.term = term;
-      this.stats = stats;
-    }
-
-    @Override
-    public String toString() {
-      return term.utf8ToString();
-    }
-  }
-
-  private static final class PendingBlock extends PendingEntry {
-    public final BytesRef prefix;
-    public final long fp;
-    public FST<BytesRef> index;
-    public List<FST<BytesRef>> subIndices;
-    public final boolean hasTerms;
-    public final boolean isFloor;
-    public final int floorLeadByte;
-
-    public PendingBlock(BytesRef prefix, long fp, boolean hasTerms, boolean isFloor, int floorLeadByte, List<FST<BytesRef>> subIndices) {
-      super(false);
-      this.prefix = prefix;
-      this.fp = fp;
-      this.hasTerms = hasTerms;
-      this.isFloor = isFloor;
-      this.floorLeadByte = floorLeadByte;
-      this.subIndices = subIndices;
-    }
-
-    @Override
-    public String toString() {
-      return "BLOCK: " + prefix.utf8ToString();
-    }
-
-    public void compileIndex(List<PendingBlock> floorBlocks, RAMOutputStream scratchBytes) throws IOException {
-
-      assert (isFloor && floorBlocks != null && floorBlocks.size() != 0) || (!isFloor && floorBlocks == null): "isFloor=" + isFloor + " floorBlocks=" + floorBlocks;
-
-      assert scratchBytes.getFilePointer() == 0;
-
-      // TODO: try writing the leading vLong in MSB order
-      // (opposite of what Lucene does today), for better
-      // outputs sharing in the FST
-      scratchBytes.writeVLong(encodeOutput(fp, hasTerms, isFloor));
-      if (isFloor) {
-        scratchBytes.writeVInt(floorBlocks.size());
-        for (PendingBlock sub : floorBlocks) {
-          assert sub.floorLeadByte != -1;
-          //if (DEBUG) {
-          //  System.out.println("    write floorLeadByte=" + Integer.toHexString(sub.floorLeadByte&0xff));
-          //}
-          scratchBytes.writeByte((byte) sub.floorLeadByte);
-          assert sub.fp > fp;
-          scratchBytes.writeVLong((sub.fp - fp) << 1 | (sub.hasTerms ? 1 : 0));
-        }
-      }
-
-      final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
-      final Builder<BytesRef> indexBuilder = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1,
-                                                                   0, 0, true, false, Integer.MAX_VALUE,
-                                                                   outputs, null);
-      //if (DEBUG) {
-      //  System.out.println("  compile index for prefix=" + prefix);
-      //}
-      //indexBuilder.DEBUG = false;
-      final byte[] bytes = new byte[(int) scratchBytes.getFilePointer()];
-      assert bytes.length > 0;
-      scratchBytes.writeTo(bytes, 0);
-      indexBuilder.add(prefix, new BytesRef(bytes, 0, bytes.length));
-      scratchBytes.reset();
-
-      // Copy over index for all sub-blocks
-
-      if (subIndices != null) {
-        for(FST<BytesRef> subIndex : subIndices) {
-          append(indexBuilder, subIndex);
-        }
-      }
-
-      if (floorBlocks != null) {
-        for (PendingBlock sub : floorBlocks) {
-          if (sub.subIndices != null) {
-            for(FST<BytesRef> subIndex : sub.subIndices) {
-              append(indexBuilder, subIndex);
-            }
-          }
-          sub.subIndices = null;
-        }
-      }
-
-      index = indexBuilder.finish();
-      subIndices = null;
-
-      /*
-      Writer w = new OutputStreamWriter(new FileOutputStream("out.dot"));
-      Util.toDot(index, w, false, false);
-      System.out.println("SAVED to out.dot");
-      w.close();
-      */
-    }
-
-    // TODO: maybe we could add bulk-add method to
-    // Builder?  Takes FST and unions it w/ current
-    // FST.
-    private void append(Builder<BytesRef> builder, FST<BytesRef> subIndex) throws IOException {
-      final BytesRefFSTEnum<BytesRef> subIndexEnum = new BytesRefFSTEnum<BytesRef>(subIndex);
-      BytesRefFSTEnum.InputOutput<BytesRef> indexEnt;
-      while((indexEnt = subIndexEnum.next()) != null) {
-        //if (DEBUG) {
-        //  System.out.println("      add sub=" + indexEnt.input + " " + indexEnt.input + " output=" + indexEnt.output);
-        //}
-        builder.add(indexEnt.input, indexEnt.output);
-      }
-    }
-  }
-
-  final RAMOutputStream scratchBytes = new RAMOutputStream();
-
-  class TermsWriter extends TermsConsumer {
-    private final FieldInfo fieldInfo;
-    private long numTerms;
-    long sumTotalTermFreq;
-    long sumDocFreq;
-    int docCount;
-    long indexStartFP;
-
-    // Used only to partition terms into the block tree; we
-    // don't pull an FST from this builder:
-    private final NoOutputs noOutputs;
-    private final Builder<Object> blockBuilder;
-
-    // PendingTerm or PendingBlock:
-    private final List<PendingEntry> pending = new ArrayList<PendingEntry>();
-
-    // Index into pending of most recently written block
-    private int lastBlockIndex = -1;
-
-    // Re-used when segmenting a too-large block into floor
-    // blocks:
-    private int[] subBytes = new int[10];
-    private int[] subTermCounts = new int[10];
-    private int[] subTermCountSums = new int[10];
-    private int[] subSubCounts = new int[10];
-
-    // This class assigns terms to blocks "naturally", ie,
-    // according to the number of terms under a given prefix
-    // that we encounter:
-    private class FindBlocks extends Builder.FreezeTail<Object> {
-
-      @Override
-      public void freeze(final Builder.UnCompiledNode<Object>[] frontier, int prefixLenPlus1, final IntsRef lastInput) throws IOException {
-
-        //if (DEBUG) System.out.println("  freeze prefixLenPlus1=" + prefixLenPlus1);
-
-        for(int idx=lastInput.length; idx >= prefixLenPlus1; idx--) {
-          final Builder.UnCompiledNode<Object> node = frontier[idx];
-
-          long totCount = 0;
-
-          if (node.isFinal) {
-            totCount++;
-          }
-
-          for(int arcIdx=0;arcIdx<node.numArcs;arcIdx++) {
-            @SuppressWarnings("unchecked") final Builder.UnCompiledNode<Object> target = (Builder.UnCompiledNode<Object>) node.arcs[arcIdx].target;
-            totCount += target.inputCount;
-            target.clear();
-            node.arcs[arcIdx].target = null;
-          }
-          node.numArcs = 0;
-
-          if (totCount >= minItemsInBlock || idx == 0) {
-            // We are on a prefix node that has enough
-            // entries (terms or sub-blocks) under it to let
-            // us write a new block or multiple blocks (main
-            // block + follow on floor blocks):
-            //if (DEBUG) {
-            //  if (totCount < minItemsInBlock && idx != 0) {
-            //    System.out.println("  force block has terms");
-            //  }
-            //}
-            writeBlocks(lastInput, idx, (int) totCount);
-            node.inputCount = 1;
-          } else {
-            // stragglers!  carry count upwards
-            node.inputCount = totCount;
-          }
-          frontier[idx] = new Builder.UnCompiledNode<Object>(blockBuilder, idx);
-        }
-      }
-    }
-
-    // Write the top count entries on the pending stack as
-    // one or more blocks.  Returns how many blocks were
-    // written.  If the entry count is <= maxItemsPerBlock
-    // we just write a single block; else we break into
-    // primary (initial) block and then one or more
-    // following floor blocks:
-
-    void writeBlocks(IntsRef prevTerm, int prefixLength, int count) throws IOException {
-      if (prefixLength == 0 || count <= maxItemsInBlock) {
-        // Easy case: not floor block.  Eg, prefix is "foo",
-        // and we found 30 terms/sub-blocks starting w/ that
-        // prefix, and minItemsInBlock <= 30 <=
-        // maxItemsInBlock.
-        final PendingBlock nonFloorBlock = writeBlock(prevTerm, prefixLength, prefixLength, count, count, 0, false, -1, true);
-        nonFloorBlock.compileIndex(null, scratchBytes);
-        pending.add(nonFloorBlock);
-      } else {
-        // Floor block case.  Eg, prefix is "foo" but we
-        // have 100 terms/sub-blocks starting w/ that
-        // prefix.  We segment the entries into a primary
-        // block and following floor blocks using the first
-        // label in the suffix to assign to floor blocks.
-
-        // TODO: we could store min & max suffix start byte
-        // in each block, to make floor blocks authoritative
-
-        //if (DEBUG) {
-        //  final BytesRef prefix = new BytesRef(prefixLength);
-        //  for(int m=0;m<prefixLength;m++) {
-        //    prefix.bytes[m] = (byte) prevTerm.ints[m];
-        //  }
-        //  prefix.length = prefixLength;
-        //  //System.out.println("\nWBS count=" + count + " prefix=" + prefix.utf8ToString() + " " + prefix);
-        //  System.out.println("writeBlocks: prefix=" + prefix + " " + prefix + " count=" + count + " pending.size()=" + pending.size());
-        //}
-        //System.out.println("\nwbs count=" + count);
-
-        final int savLabel = prevTerm.ints[prevTerm.offset + prefixLength];
-
-        // Count up how many items fall under
-        // each unique label after the prefix.
-        
-        // TODO: this is wasteful since the builder had
-        // already done this (partitioned these sub-terms
-        // according to their leading prefix byte)
-        
-        final List<PendingEntry> slice = pending.subList(pending.size()-count, pending.size());
-        int lastSuffixLeadLabel = -1;
-        int termCount = 0;
-        int subCount = 0;
-        int numSubs = 0;
-
-        for(PendingEntry ent : slice) {
-
-          // First byte in the suffix of this term
-          final int suffixLeadLabel;
-          if (ent.isTerm) {
-            PendingTerm term = (PendingTerm) ent;
-            if (term.term.length == prefixLength) {
-              // Suffix is 0, ie prefix 'foo' and term is
-              // 'foo' so the term has empty string suffix
-              // in this block
-              assert lastSuffixLeadLabel == -1;
-              assert numSubs == 0;
-              suffixLeadLabel = -1;
-            } else {
-              suffixLeadLabel = term.term.bytes[term.term.offset + prefixLength] & 0xff;
-            }
-          } else {
-            PendingBlock block = (PendingBlock) ent;
-            assert block.prefix.length > prefixLength;
-            suffixLeadLabel = block.prefix.bytes[block.prefix.offset + prefixLength] & 0xff;
-          }
-
-          if (suffixLeadLabel != lastSuffixLeadLabel && (termCount + subCount) != 0) {
-            if (subBytes.length == numSubs) {
-              subBytes = ArrayUtil.grow(subBytes);
-              subTermCounts = ArrayUtil.grow(subTermCounts);
-              subSubCounts = ArrayUtil.grow(subSubCounts);
-            }
-            subBytes[numSubs] = lastSuffixLeadLabel;
-            lastSuffixLeadLabel = suffixLeadLabel;
-            subTermCounts[numSubs] = termCount;
-            subSubCounts[numSubs] = subCount;
-            /*
-            if (suffixLeadLabel == -1) {
-              System.out.println("  sub " + -1 + " termCount=" + termCount + " subCount=" + subCount);
-            } else {
-              System.out.println("  sub " + Integer.toHexString(suffixLeadLabel) + " termCount=" + termCount + " subCount=" + subCount);
-            }
-            */
-            termCount = subCount = 0;
-            numSubs++;
-          }
-
-          if (ent.isTerm) {
-            termCount++;
-          } else {
-            subCount++;
-          }
-        }
-
-        if (subBytes.length == numSubs) {
-          subBytes = ArrayUtil.grow(subBytes);
-          subTermCounts = ArrayUtil.grow(subTermCounts);
-          subSubCounts = ArrayUtil.grow(subSubCounts);
-        }
-
-        subBytes[numSubs] = lastSuffixLeadLabel;
-        subTermCounts[numSubs] = termCount;
-        subSubCounts[numSubs] = subCount;
-        numSubs++;
-        /*
-        if (lastSuffixLeadLabel == -1) {
-          System.out.println("  sub " + -1 + " termCount=" + termCount + " subCount=" + subCount);
-        } else {
-          System.out.println("  sub " + Integer.toHexString(lastSuffixLeadLabel) + " termCount=" + termCount + " subCount=" + subCount);
-        }
-        */
-
-        if (subTermCountSums.length < numSubs) {
-          subTermCountSums = ArrayUtil.grow(subTermCountSums, numSubs);
-        }
-
-        // Roll up (backwards) the termCounts; postings impl
-        // needs this to know where to pull the term slice
-        // from its pending terms stack:
-        int sum = 0;
-        for(int idx=numSubs-1;idx>=0;idx--) {
-          sum += subTermCounts[idx];
-          subTermCountSums[idx] = sum;
-        }
-
-        // TODO: make a better segmenter?  It'd have to
-        // absorb the too-small end blocks backwards into
-        // the previous blocks
-
-        // Naive greedy segmentation; this is not always
-        // best (it can produce a too-small block as the
-        // last block):
-        int pendingCount = 0;
-        int startLabel = subBytes[0];
-        int curStart = count;
-        subCount = 0;
-
-        final List<PendingBlock> floorBlocks = new ArrayList<PendingBlock>();
-        PendingBlock firstBlock = null;
-
-        for(int sub=0;sub<numSubs;sub++) {
-          pendingCount += subTermCounts[sub] + subSubCounts[sub];
-          //System.out.println("  " + (subTermCounts[sub] + subSubCounts[sub]));
-          subCount++;
-
-          // Greedily make a floor block as soon as we've
-          // crossed the min count
-          if (pendingCount >= minItemsInBlock) {
-            final int curPrefixLength;
-            if (startLabel == -1) {
-              curPrefixLength = prefixLength;
-            } else {
-              curPrefixLength = 1+prefixLength;
-              // floor term:
-              prevTerm.ints[prevTerm.offset + prefixLength] = startLabel;
-            }
-            //System.out.println("  " + subCount + " subs");
-            final PendingBlock floorBlock = writeBlock(prevTerm, prefixLength, curPrefixLength, curStart, pendingCount, subTermCountSums[1+sub], true, startLabel, curStart == pendingCount);
-            if (firstBlock == null) {
-              firstBlock = floorBlock;
-            } else {
-              floorBlocks.add(floorBlock);
-            }
-            curStart -= pendingCount;
-            //System.out.println("    = " + pendingCount);
-            pendingCount = 0;
-
-            assert minItemsInBlock == 1 || subCount > 1: "minItemsInBlock=" + minItemsInBlock + " subCount=" + subCount + " sub=" + sub + " of " + numSubs + " subTermCount=" + subTermCountSums[sub] + " subSubCount=" + subSubCounts[sub] + " depth=" + prefixLength;
-            subCount = 0;
-            startLabel = subBytes[sub+1];
-
-            if (curStart == 0) {
-              break;
-            }
-
-            if (curStart <= maxItemsInBlock) {
-              // remainder is small enough to fit into a
-              // block.  NOTE that this may be too small (<
-              // minItemsInBlock); need a true segmenter
-              // here
-              assert startLabel != -1;
-              assert firstBlock != null;
-              prevTerm.ints[prevTerm.offset + prefixLength] = startLabel;
-              //System.out.println("  final " + (numSubs-sub-1) + " subs");
-              /*
-              for(sub++;sub < numSubs;sub++) {
-                System.out.println("  " + (subTermCounts[sub] + subSubCounts[sub]));
-              }
-              System.out.println("    = " + curStart);
-              if (curStart < minItemsInBlock) {
-                System.out.println("      **");
-              }
-              */
-              floorBlocks.add(writeBlock(prevTerm, prefixLength, prefixLength+1, curStart, curStart, 0, true, startLabel, true));
-              break;
-            }
-          }
-        }
-
-        prevTerm.ints[prevTerm.offset + prefixLength] = savLabel;
-
-        assert firstBlock != null;
-        firstBlock.compileIndex(floorBlocks, scratchBytes);
-
-        pending.add(firstBlock);
-        //if (DEBUG) System.out.println("  done pending.size()=" + pending.size());
-      }
-      lastBlockIndex = pending.size()-1;
-    }
-
-    // for debugging
-    private String toString(BytesRef b) {
-      try {
-        return b.utf8ToString() + " " + b;
-      } catch (Throwable t) {
-        // If BytesRef isn't actually UTF8, or it's eg a
-        // prefix of UTF8 that ends mid-unicode-char, we
-        // fallback to hex:
-        return b.toString();
-      }
-    }
-
-    // Writes all entries in the pending slice as a single
-    // block: 
-    private PendingBlock writeBlock(IntsRef prevTerm, int prefixLength, int indexPrefixLength, int startBackwards, int length,
-                                    int futureTermCount, boolean isFloor, int floorLeadByte, boolean isLastInFloor) throws IOException {
-
-      assert length > 0;
-
-      final int start = pending.size()-startBackwards;
-
-      assert start >= 0: "pending.size()=" + pending.size() + " startBackwards=" + startBackwards + " length=" + length;
-
-      final List<PendingEntry> slice = pending.subList(start, start + length);
-
-      final long startFP = out.getFilePointer();
-
-      final BytesRef prefix = new BytesRef(indexPrefixLength);
-      for(int m=0;m<indexPrefixLength;m++) {
-        prefix.bytes[m] = (byte) prevTerm.ints[m];
-      }
-      prefix.length = indexPrefixLength;
-
-      // Write block header:
-      out.writeVInt((length<<1)|(isLastInFloor ? 1:0));
-
-      // if (DEBUG) {
-      //   System.out.println("  writeBlock " + (isFloor ? "(floor) " : "") + "seg=" + segment + " pending.size()=" + pending.size() + " prefixLength=" + prefixLength + " indexPrefix=" + toString(prefix) + " entCount=" + length + " startFP=" + startFP + " futureTermCount=" + futureTermCount + (isFloor ? (" floorLeadByte=" + Integer.toHexString(floorLeadByte&0xff)) : "") + " isLastInFloor=" + isLastInFloor);
-      // }
-
-      // 1st pass: pack term suffix bytes into byte[] blob
-      // TODO: cutover to bulk int codec... simple64?
-
-      final boolean isLeafBlock;
-      if (lastBlockIndex < start) {
-        // This block definitely does not contain sub-blocks:
-        isLeafBlock = true;
-        //System.out.println("no scan true isFloor=" + isFloor);
-      } else if (!isFloor) {
-        // This block definitely does contain at least one sub-block:
-        isLeafBlock = false;
-        //System.out.println("no scan false " + lastBlockIndex + " vs start=" + start + " len=" + length);
-      } else {
-        // Must scan up-front to see if there is a sub-block
-        boolean v = true;
-        //System.out.println("scan " + lastBlockIndex + " vs start=" + start + " len=" + length);
-        for (PendingEntry ent : slice) {
-          if (!ent.isTerm) {
-            v = false;
-            break;
-          }
-        }
-        isLeafBlock = v;
-      }
-
-      final List<FST<BytesRef>> subIndices;
-
-      int termCount;
-      if (isLeafBlock) {
-        subIndices = null;
-        for (PendingEntry ent : slice) {
-          assert ent.isTerm;
-          PendingTerm term = (PendingTerm) ent;
-          final int suffix = term.term.length - prefixLength;
-          // if (DEBUG) {
-          //   BytesRef suffixBytes = new BytesRef(suffix);
-          //   System.arraycopy(term.term.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
-          //   suffixBytes.length = suffix;
-          //   System.out.println("    write term suffix=" + suffixBytes);
-          // }
-          // For leaf block we write suffix straight
-          bytesWriter.writeVInt(suffix);
-          bytesWriter.writeBytes(term.term.bytes, prefixLength, suffix);
-
-          // Write term stats, to separate byte[] blob:
-          bytesWriter2.writeVInt(term.stats.docFreq);
-          if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
-            assert term.stats.totalTermFreq >= term.stats.docFreq;
-            bytesWriter2.writeVLong(term.stats.totalTermFreq - term.stats.docFreq);
-          }
-        }
-        termCount = length;
-      } else {
-        subIndices = new ArrayList<FST<BytesRef>>();
-        termCount = 0;
-        for (PendingEntry ent : slice) {
-          if (ent.isTerm) {
-            PendingTerm term = (PendingTerm) ent;
-            final int suffix = term.term.length - prefixLength;
-            // if (DEBUG) {
-            //   BytesRef suffixBytes = new BytesRef(suffix);
-            //   System.arraycopy(term.term.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
-            //   suffixBytes.length = suffix;
-            //   System.out.println("    write term suffix=" + suffixBytes);
-            // }
-            // For non-leaf block we borrow 1 bit to record
-            // if entry is term or sub-block
-            bytesWriter.writeVInt(suffix<<1);
-            bytesWriter.writeBytes(term.term.bytes, prefixLength, suffix);
-
-            // Write term stats, to separate byte[] blob:
-            bytesWriter2.writeVInt(term.stats.docFreq);
-            if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
-              assert term.stats.totalTermFreq >= term.stats.docFreq;
-              bytesWriter2.writeVLong(term.stats.totalTermFreq - term.stats.docFreq);
-            }
-
-            termCount++;
-          } else {
-            PendingBlock block = (PendingBlock) ent;
-            final int suffix = block.prefix.length - prefixLength;
-
-            assert suffix > 0;
-
-            // For non-leaf block we borrow 1 bit to record
-            // if entry is term or sub-block
-            bytesWriter.writeVInt((suffix<<1)|1);
-            bytesWriter.writeBytes(block.prefix.bytes, prefixLength, suffix);
-            assert block.fp < startFP;
-
-            // if (DEBUG) {
-            //   BytesRef suffixBytes = new BytesRef(suffix);
-            //   System.arraycopy(block.prefix.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
-            //   suffixBytes.length = suffix;
-            //   System.out.println("    write sub-block suffix=" + toString(suffixBytes) + " subFP=" + block.fp + " subCode=" + (startFP-block.fp) + " floor=" + block.isFloor);
-            // }
-
-            bytesWriter.writeVLong(startFP - block.fp);
-            subIndices.add(block.index);
-          }
-        }
-
-        assert subIndices.size() != 0;
-      }
-
-      // TODO: we could block-write the term suffix pointers;
-      // this would take more space but would enable binary
-      // search on lookup
-
-      // Write suffixes byte[] blob to terms dict output:
-      out.writeVInt((int) (bytesWriter.getFilePointer() << 1) | (isLeafBlock ? 1:0));
-      bytesWriter.writeTo(out);
-      bytesWriter.reset();
-
-      // Write term stats byte[] blob
-      out.writeVInt((int) bytesWriter2.getFilePointer());
-      bytesWriter2.writeTo(out);
-      bytesWriter2.reset();
-
-      // Have postings writer write block
-      postingsWriter.flushTermsBlock(futureTermCount+termCount, termCount);
-
-      // Remove slice replaced by block:
-      slice.clear();
-
-      if (lastBlockIndex >= start) {
-        if (lastBlockIndex < start+length) {
-          lastBlockIndex = start;
-        } else {
-          lastBlockIndex -= length;
-        }
-      }
-
-      // if (DEBUG) {
-      //   System.out.println("      fpEnd=" + out.getFilePointer());
-      // }
-
-      return new PendingBlock(prefix, startFP, termCount != 0, isFloor, floorLeadByte, subIndices);
-    }
-
-    TermsWriter(FieldInfo fieldInfo) {
-      this.fieldInfo = fieldInfo;
-
-      noOutputs = NoOutputs.getSingleton();
-
-      // This Builder is just used transiently to fragment
-      // terms into "good" blocks; we don't save the
-      // resulting FST:
-      blockBuilder = new Builder<Object>(FST.INPUT_TYPE.BYTE1,
-                                         0, 0, true,
-                                         true, Integer.MAX_VALUE,
-                                         noOutputs,
-                                         new FindBlocks());
-
-      postingsWriter.setField(fieldInfo);
-    }
-    
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public PostingsConsumer startTerm(BytesRef text) throws IOException {
-      //if (DEBUG) System.out.println("\nBTTW.startTerm term=" + fieldInfo.name + ":" + toString(text) + " seg=" + segment);
-      postingsWriter.startTerm();
-      /*
-      if (fieldInfo.name.equals("id")) {
-        postingsWriter.termID = Integer.parseInt(text.utf8ToString());
-      } else {
-        postingsWriter.termID = -1;
-      }
-      */
-      return postingsWriter;
-    }
-
-    @Override
-    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
-
-      assert stats.docFreq > 0;
-      //if (DEBUG) System.out.println("BTTW.finishTerm term=" + fieldInfo.name + ":" + toString(text) + " seg=" + segment + " df=" + stats.docFreq);
-
-      blockBuilder.add(text, noOutputs.getNoOutput());
-      pending.add(new PendingTerm(BytesRef.deepCopyOf(text), stats));
-      postingsWriter.finishTerm(stats);
-      numTerms++;
-    }
-
-    // Finishes all terms in this field
-    @Override
-    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
-      if (numTerms > 0) {
-        blockBuilder.finish();
-
-        // We better have one final "root" block:
-        assert pending.size() == 1 && !pending.get(0).isTerm: "pending.size()=" + pending.size() + " pending=" + pending;
-        final PendingBlock root = (PendingBlock) pending.get(0);
-        assert root.prefix.length == 0;
-        assert root.index.getEmptyOutput() != null;
-
-        this.sumTotalTermFreq = sumTotalTermFreq;
-        this.sumDocFreq = sumDocFreq;
-        this.docCount = docCount;
-
-        // Write FST to index
-        indexStartFP = indexOut.getFilePointer();
-        root.index.save(indexOut);
-        //System.out.println("  write FST " + indexStartFP + " field=" + fieldInfo.name);
-
-        // if (SAVE_DOT_FILES || DEBUG) {
-        //   final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
-        //   Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
-        //   Util.toDot(root.index, w, false, false);
-        //   System.out.println("SAVED to " + dotFileName);
-        //   w.close();
-        // }
-      }
-    }
-
-    private final RAMOutputStream bytesWriter = new RAMOutputStream();
-    private final RAMOutputStream bytesWriter2 = new RAMOutputStream();
-  }
-
-  @Override
-  public void close() throws IOException {
-
-    IOException ioe = null;
-    try {
-      
-      int nonZeroCount = 0;
-      for(TermsWriter field : fields) {
-        if (field.numTerms > 0) {
-          nonZeroCount++;
-        }
-      }
-
-      final long dirStart = out.getFilePointer();
-      final long indexDirStart = indexOut.getFilePointer();
-
-      out.writeVInt(nonZeroCount);
-      
-      for(TermsWriter field : fields) {
-        if (field.numTerms > 0) {
-          //System.out.println("  field " + field.fieldInfo.name + " " + field.numTerms + " terms");
-          out.writeVInt(field.fieldInfo.number);
-          out.writeVLong(field.numTerms);
-          final BytesRef rootCode = ((PendingBlock) field.pending.get(0)).index.getEmptyOutput();
-          assert rootCode != null: "field=" + field.fieldInfo.name + " numTerms=" + field.numTerms;
-          out.writeVInt(rootCode.length);
-          out.writeBytes(rootCode.bytes, rootCode.offset, rootCode.length);
-          if (field.fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
-            out.writeVLong(field.sumTotalTermFreq);
-          }
-          out.writeVLong(field.sumDocFreq);
-          out.writeVInt(field.docCount);
-          indexOut.writeVLong(field.indexStartFP);
-        }
-      }
-      writeTrailer(out, dirStart);
-      writeIndexTrailer(indexOut, indexDirStart);
-    } catch (IOException ioe2) {
-      ioe = ioe2;
-    } finally {
-      IOUtils.closeWhileHandlingException(ioe, out, indexOut, postingsWriter);
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/Codec.java b/lucene/src/java/org/apache/lucene/index/codecs/Codec.java
deleted file mode 100644
index 82e06c5..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/Codec.java
+++ /dev/null
@@ -1,103 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.util.NamedSPILoader;
-import org.apache.lucene.store.Directory;
-
-/**
- * Encodes/decodes an inverted index segment
- */
-public abstract class Codec implements NamedSPILoader.NamedSPI {
-
-  private static final NamedSPILoader<Codec> loader =
-    new NamedSPILoader<Codec>(Codec.class);
-
-  private final String name;
-
-  public Codec(String name) {
-    this.name = name;
-  }
-  
-  @Override
-  public String getName() {
-    return name;
-  }
-  
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    postingsFormat().files(dir, info, "", files);
-    storedFieldsFormat().files(dir, info, files);
-    termVectorsFormat().files(dir, info, files);
-    fieldInfosFormat().files(dir, info, files);
-    // TODO: segmentInfosFormat should be allowed to declare additional files
-    // if it wants, in addition to segments_N
-    docValuesFormat().files(dir, info, files);
-    normsFormat().files(dir, info, files);
-  }
-  
-  /** Encodes/decodes postings */
-  public abstract PostingsFormat postingsFormat();
-  
-  /** Encodes/decodes docvalues */
-  public abstract DocValuesFormat docValuesFormat();
-  
-  /** Encodes/decodes stored fields */
-  public abstract StoredFieldsFormat storedFieldsFormat();
-  
-  /** Encodes/decodes term vectors */
-  public abstract TermVectorsFormat termVectorsFormat();
-  
-  /** Encodes/decodes field infos file */
-  public abstract FieldInfosFormat fieldInfosFormat();
-  
-  /** Encodes/decodes segments file */
-  public abstract SegmentInfosFormat segmentInfosFormat();
-  
-  /** Encodes/decodes document normalization values */
-  public abstract NormsFormat normsFormat();
-  
-  /** looks up a codec by name */
-  public static Codec forName(String name) {
-    return loader.lookup(name);
-  }
-  
-  /** returns a list of all available codec names */
-  public static Set<String> availableCodecs() {
-    return loader.availableServices();
-  }
-  
-  private static Codec defaultCodec = Codec.forName("Lucene40");
-  
-  // TODO: should we use this, or maybe a system property is better?
-  public static Codec getDefault() {
-    return defaultCodec;
-  }
-  
-  public static void setDefault(Codec codec) {
-    defaultCodec = codec;
-  }
-
-  @Override
-  public String toString() {
-    return name;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DocValuesConsumer.java b/lucene/src/java/org/apache/lucene/index/codecs/DocValuesConsumer.java
deleted file mode 100644
index 03b3f94..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/DocValuesConsumer.java
+++ /dev/null
@@ -1,134 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.DocValue;
-import org.apache.lucene.util.Bits;
-
-/**
- * Abstract API that consumes {@link DocValue}s.
- * {@link DocValuesConsumer} are always associated with a specific field and
- * segments. Concrete implementations of this API write the given
- * {@link DocValue} into a implementation specific format depending on
- * the fields meta-data.
- * 
- * @lucene.experimental
- */
-public abstract class DocValuesConsumer {
-
-  /**
-   * Adds the given {@link DocValue} instance to this
-   * {@link DocValuesConsumer}
-   * 
-   * @param docID
-   *          the document ID to add the value for. The docID must always
-   *          increase or be <tt>0</tt> if it is the first call to this method.
-   * @param docValue
-   *          the value to add
-   * @throws IOException
-   *           if an {@link IOException} occurs
-   */
-  public abstract void add(int docID, DocValue docValue)
-      throws IOException;
-
-  /**
-   * Called when the consumer of this API is doc with adding
-   * {@link DocValue} to this {@link DocValuesConsumer}
-   * 
-   * @param docCount
-   *          the total number of documents in this {@link DocValuesConsumer}.
-   *          Must be greater than or equal the last given docID to
-   *          {@link #add(int, DocValue)}.
-   * @throws IOException
-   */
-  public abstract void finish(int docCount) throws IOException;
-
-  /**
-   * Merges the given {@link org.apache.lucene.index.MergeState} into
-   * this {@link DocValuesConsumer}.
-   * 
-   * @param mergeState
-   *          the state to merge
-   * @param docValues docValues array containing one instance per reader (
-   *          {@link org.apache.lucene.index.MergeState#readers}) or <code>null</code> if the reader has
-   *          no {@link DocValues} instance.
-   * @throws IOException
-   *           if an {@link IOException} occurs
-   */
-  public void merge(MergeState mergeState, DocValues[] docValues) throws IOException {
-    assert mergeState != null;
-    boolean hasMerged = false;
-    for(int readerIDX=0;readerIDX<mergeState.readers.size();readerIDX++) {
-      final org.apache.lucene.index.MergeState.IndexReaderAndLiveDocs reader = mergeState.readers.get(readerIDX);
-      if (docValues[readerIDX] != null) {
-        hasMerged = true;
-        merge(new SingleSubMergeState(docValues[readerIDX], mergeState.docBase[readerIDX], reader.reader.maxDoc(),
-                                    reader.liveDocs));
-      }
-    }
-    // only finish if no exception is thrown!
-    if (hasMerged) {
-      finish(mergeState.mergedDocCount);
-    }
-  }
-
-  /**
-   * Merges the given {@link SingleSubMergeState} into this {@link DocValuesConsumer}.
-   * 
-   * @param mergeState
-   *          the {@link SingleSubMergeState} to merge
-   * @throws IOException
-   *           if an {@link IOException} occurs
-   */
-  // TODO: can't we have a default implementation here that merges naively with our apis?
-  // this is how stored fields and term vectors work. its a pain to have to impl merging
-  // (should be an optimization to override it)
-  protected abstract void merge(SingleSubMergeState mergeState) throws IOException;
-
-  /**
-   * Specialized auxiliary MergeState is necessary since we don't want to
-   * exploit internals up to the codecs consumer. An instance of this class is
-   * created for each merged low level {@link IndexReader} we are merging to
-   * support low level bulk copies.
-   */
-  public static class SingleSubMergeState {
-    /**
-     * the source reader for this MergeState - merged values should be read from
-     * this instance
-     */
-    public final DocValues reader;
-    /** the absolute docBase for this MergeState within the resulting segment */
-    public final int docBase;
-    /** the number of documents in this MergeState */
-    public final int docCount;
-    /** the not deleted bits for this MergeState */
-    public final Bits liveDocs;
-
-    public SingleSubMergeState(DocValues reader, int docBase, int docCount, Bits liveDocs) {
-      assert reader != null;
-      this.reader = reader;
-      this.docBase = docBase;
-      this.docCount = docCount;
-      this.liveDocs = liveDocs;
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DocValuesFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/DocValuesFormat.java
deleted file mode 100644
index 466f458..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/DocValuesFormat.java
+++ /dev/null
@@ -1,32 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.Directory;
-
-public abstract class DocValuesFormat {
-  public abstract PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException;
-  public abstract PerDocProducer docsProducer(SegmentReadState state) throws IOException;
-  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DocValuesReaderBase.java b/lucene/src/java/org/apache/lucene/index/codecs/DocValuesReaderBase.java
deleted file mode 100644
index 92f07b2..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/DocValuesReaderBase.java
+++ /dev/null
@@ -1,139 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type; // javadocs
-import org.apache.lucene.index.codecs.lucene40.values.Bytes;
-import org.apache.lucene.index.codecs.lucene40.values.Floats;
-import org.apache.lucene.index.codecs.lucene40.values.Ints;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Abstract base class for PerDocProducer implementations
- * @lucene.experimental
- */
-// TODO: this needs to go under lucene40 codec (its specific to its impl)
-public abstract class DocValuesReaderBase extends PerDocProducer {
-  
-  protected abstract void closeInternal(Collection<? extends Closeable> closeables) throws IOException;
-  protected abstract Map<String, DocValues> docValues();
-  
-  @Override
-  public void close() throws IOException {
-    closeInternal(docValues().values());
-  }
-  
-  @Override
-  public DocValues docValues(String field) throws IOException {
-    return docValues().get(field);
-  }
-  
-  public Comparator<BytesRef> getComparator() throws IOException {
-    return BytesRef.getUTF8SortedAsUnicodeComparator();
-  }
-
-  // Only opens files... doesn't actually load any values
-  protected TreeMap<String, DocValues> load(FieldInfos fieldInfos,
-      String segment, int docCount, Directory dir, IOContext context)
-      throws IOException {
-    TreeMap<String, DocValues> values = new TreeMap<String, DocValues>();
-    boolean success = false;
-    try {
-
-      for (FieldInfo fieldInfo : fieldInfos) {
-        if (fieldInfo.hasDocValues()) {
-          final String field = fieldInfo.name;
-          // TODO can we have a compound file per segment and codec for
-          // docvalues?
-          final String id = DocValuesWriterBase.docValuesId(segment,
-              fieldInfo.number);
-          values.put(field,
-              loadDocValues(docCount, dir, id, fieldInfo.getDocValuesType(), context));
-        }
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        // if we fail we must close all opened resources if there are any
-        closeInternal(values.values());
-      }
-    }
-    return values;
-  }
-  
-  /**
-   * Loads a {@link DocValues} instance depending on the given {@link Type}.
-   * Codecs that use different implementations for a certain {@link Type} can
-   * simply override this method and return their custom implementations.
-   * 
-   * @param docCount
-   *          number of documents in the segment
-   * @param dir
-   *          the {@link Directory} to load the {@link DocValues} from
-   * @param id
-   *          the unique file ID within the segment
-   * @param type
-   *          the type to load
-   * @return a {@link DocValues} instance for the given type
-   * @throws IOException
-   *           if an {@link IOException} occurs
-   * @throws IllegalArgumentException
-   *           if the given {@link Type} is not supported
-   */
-  protected DocValues loadDocValues(int docCount, Directory dir, String id,
-      DocValues.Type type, IOContext context) throws IOException {
-    switch (type) {
-    case FIXED_INTS_16:
-    case FIXED_INTS_32:
-    case FIXED_INTS_64:
-    case FIXED_INTS_8:
-    case VAR_INTS:
-      return Ints.getValues(dir, id, docCount, type, context);
-    case FLOAT_32:
-      return Floats.getValues(dir, id, docCount, context, type);
-    case FLOAT_64:
-      return Floats.getValues(dir, id, docCount, context, type);
-    case BYTES_FIXED_STRAIGHT:
-      return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, true, docCount, getComparator(), context);
-    case BYTES_FIXED_DEREF:
-      return Bytes.getValues(dir, id, Bytes.Mode.DEREF, true, docCount, getComparator(), context);
-    case BYTES_FIXED_SORTED:
-      return Bytes.getValues(dir, id, Bytes.Mode.SORTED, true, docCount, getComparator(), context);
-    case BYTES_VAR_STRAIGHT:
-      return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, false, docCount, getComparator(), context);
-    case BYTES_VAR_DEREF:
-      return Bytes.getValues(dir, id, Bytes.Mode.DEREF, false, docCount, getComparator(), context);
-    case BYTES_VAR_SORTED:
-      return Bytes.getValues(dir, id, Bytes.Mode.SORTED, false, docCount, getComparator(), context);
-    default:
-      throw new IllegalStateException("unrecognized index values mode " + type);
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/DocValuesWriterBase.java b/lucene/src/java/org/apache/lucene/index/codecs/DocValuesWriterBase.java
deleted file mode 100644
index 86261c1..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/DocValuesWriterBase.java
+++ /dev/null
@@ -1,71 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.codecs.lucene40.values.Writer;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-
-/**
- * Abstract base class for PerDocConsumer implementations
- * @lucene.experimental
- */
-//TODO: this needs to go under lucene40 codec (its specific to its impl)
-public abstract class DocValuesWriterBase extends PerDocConsumer {
-  protected final String segmentName;
-  protected final String segmentSuffix;
-  private final Counter bytesUsed;
-  protected final IOContext context;
-  
-  protected DocValuesWriterBase(PerDocWriteState state) {
-    this.segmentName = state.segmentName;
-    this.segmentSuffix = state.segmentSuffix;
-    this.bytesUsed = state.bytesUsed;
-    this.context = state.context;
-  }
-
-  protected abstract Directory getDirectory() throws IOException;
-  
-  @Override
-  public void close() throws IOException {   
-  }
-
-  @Override
-  public DocValuesConsumer addValuesField(DocValues.Type valueType, FieldInfo field) throws IOException {
-    return Writer.create(valueType,
-        docValuesId(segmentName, field.number), 
-        getDirectory(), getComparator(), bytesUsed, context);
-  }
-
-  public static String docValuesId(String segmentsName, int fieldId) {
-    return segmentsName + "_" + fieldId;
-  }
-  
-  
-  public Comparator<BytesRef> getComparator() throws IOException {
-    return BytesRef.getUTF8SortedAsUnicodeComparator();
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosFormat.java
deleted file mode 100644
index d4b66d7..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosFormat.java
+++ /dev/null
@@ -1,33 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-
-/**
- * @lucene.experimental
- */
-public abstract class FieldInfosFormat {
-  public abstract FieldInfosReader getFieldInfosReader() throws IOException;
-  public abstract FieldInfosWriter getFieldInfosWriter() throws IOException;
-  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosReader.java
deleted file mode 100644
index 151b212..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosReader.java
+++ /dev/null
@@ -1,31 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * @lucene.experimental
- */
-public abstract class FieldInfosReader {
-  public abstract FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosWriter.java
deleted file mode 100644
index be46a49..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/FieldInfosWriter.java
+++ /dev/null
@@ -1,31 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * @lucene.experimental
- */
-public abstract class FieldInfosWriter {
-  public abstract void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FieldsConsumer.java b/lucene/src/java/org/apache/lucene/index/codecs/FieldsConsumer.java
deleted file mode 100644
index 4abe89d..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/FieldsConsumer.java
+++ /dev/null
@@ -1,58 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.Terms;
-
-/** Abstract API that consumes terms, doc, freq, prox and
- *  payloads postings.  Concrete implementations of this
- *  actually do "something" with the postings (write it into
- *  the index in a specific format).
- *
- * @lucene.experimental
- */
-public abstract class FieldsConsumer implements Closeable {
-
-  /** Add a new field */
-  public abstract TermsConsumer addField(FieldInfo field) throws IOException;
-  
-  /** Called when we are done adding everything. */
-  public abstract void close() throws IOException;
-
-  public void merge(MergeState mergeState, Fields fields) throws IOException {
-    FieldsEnum fieldsEnum = fields.iterator();
-    assert fieldsEnum != null;
-    String field;
-    while((field = fieldsEnum.next()) != null) {
-      mergeState.fieldInfo = mergeState.fieldInfos.fieldInfo(field);
-      assert mergeState.fieldInfo != null : "FieldInfo for field is null: "+ field;
-      Terms terms = fieldsEnum.terms();
-      if (terms != null) {
-        final TermsConsumer termsConsumer = addField(mergeState.fieldInfo);
-        termsConsumer.merge(mergeState, terms.iterator(null));
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FieldsProducer.java b/lucene/src/java/org/apache/lucene/index/codecs/FieldsProducer.java
deleted file mode 100644
index 60f4bf5..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/FieldsProducer.java
+++ /dev/null
@@ -1,37 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.Terms;
-
-/** Abstract API that consumes terms, doc, freq, prox and
- *  payloads postings.  Concrete implementations of this
- *  actually do "something" with the postings (write it into
- *  the index in a specific format).
- *
- * @lucene.experimental
- */
-
-public abstract class FieldsProducer extends Fields implements Closeable {
-  public abstract void close() throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FixedGapTermsIndexReader.java b/lucene/src/java/org/apache/lucene/index/codecs/FixedGapTermsIndexReader.java
deleted file mode 100644
index 062179b..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/FixedGapTermsIndexReader.java
+++ /dev/null
@@ -1,407 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.PackedInts;
-
-import java.util.HashMap;
-import java.util.Collection;
-import java.util.Comparator;
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexFileNames;
-
-/** @lucene.experimental */
-public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
-
-  // NOTE: long is overkill here, since this number is 128
-  // by default and only indexDivisor * 128 if you change
-  // the indexDivisor at search time.  But, we use this in a
-  // number of places to multiply out the actual ord, and we
-  // will overflow int during those multiplies.  So to avoid
-  // having to upgrade each multiple to long in multiple
-  // places (error prone), we use long here:
-  private long totalIndexInterval;
-
-  private int indexDivisor;
-  final private int indexInterval;
-
-  // Closed if indexLoaded is true:
-  private IndexInput in;
-  private volatile boolean indexLoaded;
-
-  private final Comparator<BytesRef> termComp;
-
-  private final static int PAGED_BYTES_BITS = 15;
-
-  // all fields share this single logical byte[]
-  private final PagedBytes termBytes = new PagedBytes(PAGED_BYTES_BITS);
-  private PagedBytes.Reader termBytesReader;
-
-  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
-  
-  // start of the field info data
-  protected long dirOffset;
-
-  public FixedGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, Comparator<BytesRef> termComp, String segmentSuffix, IOContext context)
-    throws IOException {
-
-    this.termComp = termComp;
-
-    assert indexDivisor == -1 || indexDivisor > 0;
-
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION), context);
-    
-    boolean success = false;
-
-    try {
-      
-      readHeader(in);
-      indexInterval = in.readInt();
-      this.indexDivisor = indexDivisor;
-
-      if (indexDivisor < 0) {
-        totalIndexInterval = indexInterval;
-      } else {
-        // In case terms index gets loaded, later, on demand
-        totalIndexInterval = indexInterval * indexDivisor;
-      }
-      assert totalIndexInterval > 0;
-      
-      seekDir(in, dirOffset);
-
-      // Read directory
-      final int numFields = in.readVInt();      
-      //System.out.println("FGR: init seg=" + segment + " div=" + indexDivisor + " nF=" + numFields);
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final int numIndexTerms = in.readVInt();
-        final long termsStart = in.readVLong();
-        final long indexStart = in.readVLong();
-        final long packedIndexStart = in.readVLong();
-        final long packedOffsetsStart = in.readVLong();
-        assert packedIndexStart >= indexStart: "packedStart=" + packedIndexStart + " indexStart=" + indexStart + " numIndexTerms=" + numIndexTerms + " seg=" + segment;
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        fields.put(fieldInfo, new FieldIndexData(fieldInfo, numIndexTerms, indexStart, termsStart, packedIndexStart, packedOffsetsStart));
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(in);
-      }
-      if (indexDivisor > 0) {
-        in.close();
-        in = null;
-        if (success) {
-          indexLoaded = true;
-        }
-        termBytesReader = termBytes.freeze(true);
-      }
-    }
-  }
-  
-  @Override
-  public int getDivisor() {
-    return indexDivisor;
-  }
-
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, FixedGapTermsIndexWriter.CODEC_NAME,
-      FixedGapTermsIndexWriter.VERSION_START, FixedGapTermsIndexWriter.VERSION_START);
-    dirOffset = input.readLong();
-  }
-
-  private class IndexEnum extends FieldIndexEnum {
-    private final FieldIndexData.CoreFieldIndex fieldIndex;
-    private final BytesRef term = new BytesRef();
-    private long ord;
-
-    public IndexEnum(FieldIndexData.CoreFieldIndex fieldIndex) {
-      this.fieldIndex = fieldIndex;
-    }
-
-    @Override
-    public BytesRef term() {
-      return term;
-    }
-
-    @Override
-    public long seek(BytesRef target) {
-      int lo = 0;				  // binary search
-      int hi = fieldIndex.numIndexTerms - 1;
-      assert totalIndexInterval > 0 : "totalIndexInterval=" + totalIndexInterval;
-
-      while (hi >= lo) {
-        int mid = (lo + hi) >>> 1;
-
-        final long offset = fieldIndex.termOffsets.get(mid);
-        final int length = (int) (fieldIndex.termOffsets.get(1+mid) - offset);
-        termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-
-        int delta = termComp.compare(target, term);
-        if (delta < 0) {
-          hi = mid - 1;
-        } else if (delta > 0) {
-          lo = mid + 1;
-        } else {
-          assert mid >= 0;
-          ord = mid*totalIndexInterval;
-          return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(mid);
-        }
-      }
-
-      if (hi < 0) {
-        assert hi == -1;
-        hi = 0;
-      }
-
-      final long offset = fieldIndex.termOffsets.get(hi);
-      final int length = (int) (fieldIndex.termOffsets.get(1+hi) - offset);
-      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-
-      ord = hi*totalIndexInterval;
-      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(hi);
-    }
-
-    @Override
-    public long next() {
-      final int idx = 1 + (int) (ord / totalIndexInterval);
-      if (idx >= fieldIndex.numIndexTerms) {
-        return -1;
-      }
-      ord += totalIndexInterval;
-
-      final long offset = fieldIndex.termOffsets.get(idx);
-      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
-      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
-    }
-
-    @Override
-    public long ord() {
-      return ord;
-    }
-
-    @Override
-    public long seek(long ord) {
-      int idx = (int) (ord / totalIndexInterval);
-      // caller must ensure ord is in bounds
-      assert idx < fieldIndex.numIndexTerms;
-      final long offset = fieldIndex.termOffsets.get(idx);
-      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
-      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-      this.ord = idx * totalIndexInterval;
-      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
-    }
-  }
-
-  @Override
-  public boolean supportsOrd() {
-    return true;
-  }
-
-  private final class FieldIndexData {
-
-    final private FieldInfo fieldInfo;
-
-    volatile CoreFieldIndex coreIndex;
-
-    private final long indexStart;
-    private final long termsStart;
-    private final long packedIndexStart;
-    private final long packedOffsetsStart;
-
-    private final int numIndexTerms;
-
-    public FieldIndexData(FieldInfo fieldInfo, int numIndexTerms, long indexStart, long termsStart, long packedIndexStart,
-                          long packedOffsetsStart) throws IOException {
-
-      this.fieldInfo = fieldInfo;
-      this.termsStart = termsStart;
-      this.indexStart = indexStart;
-      this.packedIndexStart = packedIndexStart;
-      this.packedOffsetsStart = packedOffsetsStart;
-      this.numIndexTerms = numIndexTerms;
-
-      if (indexDivisor > 0) {
-        loadTermsIndex();
-      }
-    }
-
-    private void loadTermsIndex() throws IOException {
-      if (coreIndex == null) {
-        coreIndex = new CoreFieldIndex(indexStart, termsStart, packedIndexStart, packedOffsetsStart, numIndexTerms);
-      }
-    }
-
-    private final class CoreFieldIndex {
-
-      // where this field's terms begin in the packed byte[]
-      // data
-      final long termBytesStart;
-
-      // offset into index termBytes
-      final PackedInts.Reader termOffsets;
-
-      // index pointers into main terms dict
-      final PackedInts.Reader termsDictOffsets;
-
-      final int numIndexTerms;
-      final long termsStart;
-
-      public CoreFieldIndex(long indexStart, long termsStart, long packedIndexStart, long packedOffsetsStart, int numIndexTerms) throws IOException {
-
-        this.termsStart = termsStart;
-        termBytesStart = termBytes.getPointer();
-
-        IndexInput clone = (IndexInput) in.clone();
-        clone.seek(indexStart);
-
-        // -1 is passed to mean "don't load term index", but
-        // if we are then later loaded it's overwritten with
-        // a real value
-        assert indexDivisor > 0;
-
-        this.numIndexTerms = 1+(numIndexTerms-1) / indexDivisor;
-
-        assert this.numIndexTerms  > 0: "numIndexTerms=" + numIndexTerms + " indexDivisor=" + indexDivisor;
-
-        if (indexDivisor == 1) {
-          // Default (load all index terms) is fast -- slurp in the images from disk:
-          
-          try {
-            final long numTermBytes = packedIndexStart - indexStart;
-            termBytes.copy(clone, numTermBytes);
-
-            // records offsets into main terms dict file
-            termsDictOffsets = PackedInts.getReader(clone);
-            assert termsDictOffsets.size() == numIndexTerms;
-
-            // records offsets into byte[] term data
-            termOffsets = PackedInts.getReader(clone);
-            assert termOffsets.size() == 1+numIndexTerms;
-          } finally {
-            clone.close();
-          }
-        } else {
-          // Get packed iterators
-          final IndexInput clone1 = (IndexInput) in.clone();
-          final IndexInput clone2 = (IndexInput) in.clone();
-
-          try {
-            // Subsample the index terms
-            clone1.seek(packedIndexStart);
-            final PackedInts.ReaderIterator termsDictOffsetsIter = PackedInts.getReaderIterator(clone1);
-
-            clone2.seek(packedOffsetsStart);
-            final PackedInts.ReaderIterator termOffsetsIter = PackedInts.getReaderIterator(clone2);
-
-            // TODO: often we can get by w/ fewer bits per
-            // value, below.. .but this'd be more complex:
-            // we'd have to try @ fewer bits and then grow
-            // if we overflowed it.
-
-            PackedInts.Mutable termsDictOffsetsM = PackedInts.getMutable(this.numIndexTerms, termsDictOffsetsIter.getBitsPerValue());
-            PackedInts.Mutable termOffsetsM = PackedInts.getMutable(this.numIndexTerms+1, termOffsetsIter.getBitsPerValue());
-
-            termsDictOffsets = termsDictOffsetsM;
-            termOffsets = termOffsetsM;
-
-            int upto = 0;
-
-            long termOffsetUpto = 0;
-
-            while(upto < this.numIndexTerms) {
-              // main file offset copies straight over
-              termsDictOffsetsM.set(upto, termsDictOffsetsIter.next());
-
-              termOffsetsM.set(upto, termOffsetUpto);
-
-              long termOffset = termOffsetsIter.next();
-              long nextTermOffset = termOffsetsIter.next();
-              final int numTermBytes = (int) (nextTermOffset - termOffset);
-
-              clone.seek(indexStart + termOffset);
-              assert indexStart + termOffset < clone.length() : "indexStart=" + indexStart + " termOffset=" + termOffset + " len=" + clone.length();
-              assert indexStart + termOffset + numTermBytes < clone.length();
-
-              termBytes.copy(clone, numTermBytes);
-              termOffsetUpto += numTermBytes;
-
-              upto++;
-              if (upto == this.numIndexTerms) {
-                break;
-              }
-
-              // skip terms:
-              termsDictOffsetsIter.next();
-              for(int i=0;i<indexDivisor-2;i++) {
-                termOffsetsIter.next();
-                termsDictOffsetsIter.next();
-              }
-            }
-            termOffsetsM.set(upto, termOffsetUpto);
-
-          } finally {
-            clone1.close();
-            clone2.close();
-            clone.close();
-          }
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
-    final FieldIndexData fieldData = fields.get(fieldInfo);
-    if (fieldData.coreIndex == null) {
-      return null;
-    } else {
-      return new IndexEnum(fieldData.coreIndex);
-    }
-  }
-
-  public static void files(Directory dir, SegmentInfo info, String segmentSuffix, Collection<String> files) {
-    files.add(IndexFileNames.segmentFileName(info.name, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION));
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (in != null && !indexLoaded) {
-      in.close();
-    }
-    if (termBytesReader != null) {
-      termBytesReader.close();
-    }
-  }
-
-  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
-    input.seek(dirOffset);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FixedGapTermsIndexWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/FixedGapTermsIndexWriter.java
deleted file mode 100644
index 0e42be1..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/FixedGapTermsIndexWriter.java
+++ /dev/null
@@ -1,255 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-import java.util.List;
-import java.util.ArrayList;
-import java.io.IOException;
-
-/**
- * Selects every Nth term as and index term, and hold term
- * bytes fully expanded in memory.  This terms index
- * supports seeking by ord.  See {@link
- * VariableGapTermsIndexWriter} for a more memory efficient
- * terms index that does not support seeking by ord.
- *
- * @lucene.experimental */
-public class FixedGapTermsIndexWriter extends TermsIndexWriterBase {
-  protected final IndexOutput out;
-
-  /** Extension of terms index file */
-  static final String TERMS_INDEX_EXTENSION = "tii";
-
-  final static String CODEC_NAME = "SIMPLE_STANDARD_TERMS_INDEX";
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  final private int termIndexInterval;
-
-  private final List<SimpleFieldWriter> fields = new ArrayList<SimpleFieldWriter>();
-  private final FieldInfos fieldInfos; // unread
-
-  public FixedGapTermsIndexWriter(SegmentWriteState state) throws IOException {
-    final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_INDEX_EXTENSION);
-    termIndexInterval = state.termIndexInterval;
-    out = state.directory.createOutput(indexFileName, state.context);
-    boolean success = false;
-    try {
-      fieldInfos = state.fieldInfos;
-      writeHeader(out);
-      out.writeInt(termIndexInterval);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
-    // Placeholder for dir offset
-    out.writeLong(0);
-  }
-
-  @Override
-  public FieldWriter addField(FieldInfo field, long termsFilePointer) {
-    //System.out.println("FGW: addFfield=" + field.name);
-    SimpleFieldWriter writer = new SimpleFieldWriter(field, termsFilePointer);
-    fields.add(writer);
-    return writer;
-  }
-
-  /** NOTE: if your codec does not sort in unicode code
-   *  point order, you must override this method, to simply
-   *  return indexedTerm.length. */
-  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
-    // As long as codec sorts terms in unicode codepoint
-    // order, we can safely strip off the non-distinguishing
-    // suffix to save RAM in the loaded terms index.
-    final int idxTermOffset = indexedTerm.offset;
-    final int priorTermOffset = priorTerm.offset;
-    final int limit = Math.min(priorTerm.length, indexedTerm.length);
-    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
-      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
-        return byteIdx+1;
-      }
-    }
-    return Math.min(1+priorTerm.length, indexedTerm.length);
-  }
-
-  private class SimpleFieldWriter extends FieldWriter {
-    final FieldInfo fieldInfo;
-    int numIndexTerms;
-    final long indexStart;
-    final long termsStart;
-    long packedIndexStart;
-    long packedOffsetsStart;
-    private long numTerms;
-
-    // TODO: we could conceivably make a PackedInts wrapper
-    // that auto-grows... then we wouldn't force 6 bytes RAM
-    // per index term:
-    private short[] termLengths;
-    private int[] termsPointerDeltas;
-    private long lastTermsPointer;
-    private long totTermLength;
-
-    private final BytesRef lastTerm = new BytesRef();
-
-    SimpleFieldWriter(FieldInfo fieldInfo, long termsFilePointer) {
-      this.fieldInfo = fieldInfo;
-      indexStart = out.getFilePointer();
-      termsStart = lastTermsPointer = termsFilePointer;
-      termLengths = new short[0];
-      termsPointerDeltas = new int[0];
-    }
-
-    @Override
-    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
-      // First term is first indexed term:
-      //System.out.println("FGW: checkIndexTerm text=" + text.utf8ToString());
-      if (0 == (numTerms++ % termIndexInterval)) {
-        return true;
-      } else {
-        if (0 == numTerms % termIndexInterval) {
-          // save last term just before next index term so we
-          // can compute wasted suffix
-          lastTerm.copyBytes(text);
-        }
-        return false;
-      }
-    }
-
-    @Override
-    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
-      final int indexedTermLength = indexedTermPrefixLength(lastTerm, text);
-      //System.out.println("FGW: add text=" + text.utf8ToString() + " " + text + " fp=" + termsFilePointer);
-
-      // write only the min prefix that shows the diff
-      // against prior term
-      out.writeBytes(text.bytes, text.offset, indexedTermLength);
-
-      if (termLengths.length == numIndexTerms) {
-        termLengths = ArrayUtil.grow(termLengths);
-      }
-      if (termsPointerDeltas.length == numIndexTerms) {
-        termsPointerDeltas = ArrayUtil.grow(termsPointerDeltas);
-      }
-
-      // save delta terms pointer
-      termsPointerDeltas[numIndexTerms] = (int) (termsFilePointer - lastTermsPointer);
-      lastTermsPointer = termsFilePointer;
-
-      // save term length (in bytes)
-      assert indexedTermLength <= Short.MAX_VALUE;
-      termLengths[numIndexTerms] = (short) indexedTermLength;
-      totTermLength += indexedTermLength;
-
-      lastTerm.copyBytes(text);
-      numIndexTerms++;
-    }
-
-    @Override
-    public void finish(long termsFilePointer) throws IOException {
-
-      // write primary terms dict offsets
-      packedIndexStart = out.getFilePointer();
-
-      PackedInts.Writer w = PackedInts.getWriter(out, numIndexTerms, PackedInts.bitsRequired(termsFilePointer));
-
-      // relative to our indexStart
-      long upto = 0;
-      for(int i=0;i<numIndexTerms;i++) {
-        upto += termsPointerDeltas[i];
-        w.add(upto);
-      }
-      w.finish();
-
-      packedOffsetsStart = out.getFilePointer();
-
-      // write offsets into the byte[] terms
-      w = PackedInts.getWriter(out, 1+numIndexTerms, PackedInts.bitsRequired(totTermLength));
-      upto = 0;
-      for(int i=0;i<numIndexTerms;i++) {
-        w.add(upto);
-        upto += termLengths[i];
-      }
-      w.add(upto);
-      w.finish();
-
-      // our referrer holds onto us, while other fields are
-      // being written, so don't tie up this RAM:
-      termLengths = null;
-      termsPointerDeltas = null;
-    }
-  }
-
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      final long dirStart = out.getFilePointer();
-      final int fieldCount = fields.size();
-      
-      int nonNullFieldCount = 0;
-      for(int i=0;i<fieldCount;i++) {
-        SimpleFieldWriter field = fields.get(i);
-        if (field.numIndexTerms > 0) {
-          nonNullFieldCount++;
-        }
-      }
-      
-      out.writeVInt(nonNullFieldCount);
-      for(int i=0;i<fieldCount;i++) {
-        SimpleFieldWriter field = fields.get(i);
-        if (field.numIndexTerms > 0) {
-          out.writeVInt(field.fieldInfo.number);
-          out.writeVInt(field.numIndexTerms);
-          out.writeVLong(field.termsStart);
-          out.writeVLong(field.indexStart);
-          out.writeVLong(field.packedIndexStart);
-          out.writeVLong(field.packedOffsetsStart);
-        }
-      }
-      writeTrailer(dirStart);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(out);
-      } else {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-
-  protected void writeTrailer(long dirStart) throws IOException {
-    out.seek(CodecUtil.headerLength(CODEC_NAME));
-    out.writeLong(dirStart);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/MappingMultiDocsAndPositionsEnum.java b/lucene/src/java/org/apache/lucene/index/codecs/MappingMultiDocsAndPositionsEnum.java
deleted file mode 100644
index 039cbb7..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/MappingMultiDocsAndPositionsEnum.java
+++ /dev/null
@@ -1,120 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.MultiDocsAndPositionsEnum;
-import java.io.IOException;
-
-/**
- * Exposes flex API, merged from flex API of sub-segments,
- * remapping docIDs (this is used for segment merging).
- *
- * @lucene.experimental
- */
-
-public final class MappingMultiDocsAndPositionsEnum extends DocsAndPositionsEnum {
-  private MultiDocsAndPositionsEnum.EnumWithSlice[] subs;
-  int numSubs;
-  int upto;
-  int[] currentMap;
-  DocsAndPositionsEnum current;
-  int currentBase;
-  int doc = -1;
-  private MergeState mergeState;
-
-  MappingMultiDocsAndPositionsEnum reset(MultiDocsAndPositionsEnum postingsEnum) throws IOException {
-    this.numSubs = postingsEnum.getNumSubs();
-    this.subs = postingsEnum.getSubs();
-    upto = -1;
-    current = null;
-    return this;
-  }
-
-  public void setMergeState(MergeState mergeState) {
-    this.mergeState = mergeState;
-  }
-
-  @Override
-  public int freq() {
-    return current.freq();
-  }
-
-  @Override
-  public int docID() {
-    return doc;
-  }
-
-  @Override
-  public int advance(int target) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public int nextDoc() throws IOException {
-    while(true) {
-      if (current == null) {
-        if (upto == numSubs-1) {
-          return this.doc = NO_MORE_DOCS;
-        } else {
-          upto++;
-          final int reader = subs[upto].slice.readerIndex;
-          current = subs[upto].docsAndPositionsEnum;
-          currentBase = mergeState.docBase[reader];
-          currentMap = mergeState.docMaps[reader];
-        }
-      }
-
-      int doc = current.nextDoc();
-      if (doc != NO_MORE_DOCS) {
-        if (currentMap != null) {
-          // compact deletions
-          doc = currentMap[doc];
-          if (doc == -1) {
-            continue;
-          }
-        }
-        return this.doc = currentBase + doc;
-      } else {
-        current = null;
-      }
-    }
-  }
-
-  @Override
-  public int nextPosition() throws IOException {
-    return current.nextPosition();
-  }
-  
-  @Override
-  public BytesRef getPayload() throws IOException {
-    BytesRef payload = current.getPayload();
-    if (mergeState.currentPayloadProcessor[upto] != null) {
-      mergeState.currentPayloadProcessor[upto].processPayload(payload);
-    }
-    return payload;
-  }
-
-  @Override
-  public boolean hasPayload() {
-    return current.hasPayload();
-  }
-}
-
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/MappingMultiDocsEnum.java b/lucene/src/java/org/apache/lucene/index/codecs/MappingMultiDocsEnum.java
deleted file mode 100644
index ab26d2d..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/MappingMultiDocsEnum.java
+++ /dev/null
@@ -1,100 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.MultiDocsEnum;
-import java.io.IOException;
-
-/**
- * Exposes flex API, merged from flex API of sub-segments,
- * remapping docIDs (this is used for segment merging).
- *
- * @lucene.experimental
- */
-
-public final class MappingMultiDocsEnum extends DocsEnum {
-  private MultiDocsEnum.EnumWithSlice[] subs;
-  int numSubs;
-  int upto;
-  int[] currentMap;
-  DocsEnum current;
-  int currentBase;
-  int doc = -1;
-  private MergeState mergeState;
-
-  MappingMultiDocsEnum reset(MultiDocsEnum docsEnum) throws IOException {
-    this.numSubs = docsEnum.getNumSubs();
-    this.subs = docsEnum.getSubs();
-    upto = -1;
-    current = null;
-    return this;
-  }
-
-  public void setMergeState(MergeState mergeState) {
-    this.mergeState = mergeState;
-  }
-
-  @Override
-  public int freq() {
-    return current.freq();
-  }
-
-  @Override
-  public int docID() {
-    return doc;
-  }
-
-  @Override
-  public int advance(int target) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public int nextDoc() throws IOException {
-    while(true) {
-      if (current == null) {
-        if (upto == numSubs-1) {
-          return this.doc = NO_MORE_DOCS;
-        } else {
-          upto++;
-          final int reader = subs[upto].slice.readerIndex;
-          current = subs[upto].docsEnum;
-          currentBase = mergeState.docBase[reader];
-          currentMap = mergeState.docMaps[reader];
-        }
-      }
-
-      int doc = current.nextDoc();
-      if (doc != NO_MORE_DOCS) {
-        if (currentMap != null) {
-          // compact deletions
-          doc = currentMap[doc];
-          if (doc == -1) {
-            continue;
-          }
-        }
-        return this.doc = currentBase + doc;
-      } else {
-        current = null;
-      }
-    }
-  }
-}
-
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/MultiLevelSkipListReader.java b/lucene/src/java/org/apache/lucene/index/codecs/MultiLevelSkipListReader.java
deleted file mode 100644
index 65bce2b..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/MultiLevelSkipListReader.java
+++ /dev/null
@@ -1,296 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.BufferedIndexInput;
-import org.apache.lucene.store.IndexInput;
-
-/**
- * This abstract class reads skip lists with multiple levels.
- * 
- * See {@link MultiLevelSkipListWriter} for the information about the encoding 
- * of the multi level skip lists. 
- * 
- * Subclasses must implement the abstract method {@link #readSkipData(int, IndexInput)}
- * which defines the actual format of the skip data.
- * @lucene.experimental
- */
-
-public abstract class MultiLevelSkipListReader {
-  // the maximum number of skip levels possible for this index
-  protected int maxNumberOfSkipLevels; 
-  
-  // number of levels in this skip list
-  private int numberOfSkipLevels;
-  
-  // Expert: defines the number of top skip levels to buffer in memory.
-  // Reducing this number results in less memory usage, but possibly
-  // slower performance due to more random I/Os.
-  // Please notice that the space each level occupies is limited by
-  // the skipInterval. The top level can not contain more than
-  // skipLevel entries, the second top level can not contain more
-  // than skipLevel^2 entries and so forth.
-  private int numberOfLevelsToBuffer = 1;
-  
-  private int docCount;
-  private boolean haveSkipped;
-  
-  private IndexInput[] skipStream;    // skipStream for each level
-  private long skipPointer[];         // the start pointer of each skip level
-  private int skipInterval[];         // skipInterval of each level
-  private int[] numSkipped;           // number of docs skipped per level
-    
-  private int[] skipDoc;              // doc id of current skip entry per level 
-  private int lastDoc;                // doc id of last read skip entry with docId <= target
-  private long[] childPointer;        // child pointer of current skip entry per level
-  private long lastChildPointer;      // childPointer of last read skip entry with docId <= target
-  
-  private boolean inputIsBuffered;
-  
-  public MultiLevelSkipListReader(IndexInput skipStream, int maxSkipLevels, int skipInterval) {
-    this.skipStream = new IndexInput[maxSkipLevels];
-    this.skipPointer = new long[maxSkipLevels];
-    this.childPointer = new long[maxSkipLevels];
-    this.numSkipped = new int[maxSkipLevels];
-    this.maxNumberOfSkipLevels = maxSkipLevels;
-    this.skipInterval = new int[maxSkipLevels];
-    this.skipStream [0]= skipStream;
-    this.inputIsBuffered = (skipStream instanceof BufferedIndexInput);
-    this.skipInterval[0] = skipInterval;
-    for (int i = 1; i < maxSkipLevels; i++) {
-      // cache skip intervals
-      this.skipInterval[i] = this.skipInterval[i - 1] * skipInterval;
-    }
-    skipDoc = new int[maxSkipLevels];
-  }
-
-  
-  /** Returns the id of the doc to which the last call of {@link #skipTo(int)}
-   *  has skipped.  */
-  public int getDoc() {
-    return lastDoc;
-  }
-  
-  
-  /** Skips entries to the first beyond the current whose document number is
-   *  greater than or equal to <i>target</i>. Returns the current doc count. 
-   */
-  public int skipTo(int target) throws IOException {
-    if (!haveSkipped) {
-      // first time, load skip levels
-      loadSkipLevels();
-      haveSkipped = true;
-    }
-  
-    // walk up the levels until highest level is found that has a skip
-    // for this target
-    int level = 0;
-    while (level < numberOfSkipLevels - 1 && target > skipDoc[level + 1]) {
-      level++;
-    }    
-
-    while (level >= 0) {
-      if (target > skipDoc[level]) {
-        if (!loadNextSkip(level)) {
-          continue;
-        }
-      } else {
-        // no more skips on this level, go down one level
-        if (level > 0 && lastChildPointer > skipStream[level - 1].getFilePointer()) {
-          seekChild(level - 1);
-        } 
-        level--;
-      }
-    }
-    
-    return numSkipped[0] - skipInterval[0] - 1;
-  }
-  
-  private boolean loadNextSkip(int level) throws IOException {
-    // we have to skip, the target document is greater than the current
-    // skip list entry        
-    setLastSkipData(level);
-      
-    numSkipped[level] += skipInterval[level];
-      
-    if (numSkipped[level] > docCount) {
-      // this skip list is exhausted
-      skipDoc[level] = Integer.MAX_VALUE;
-      if (numberOfSkipLevels > level) numberOfSkipLevels = level; 
-      return false;
-    }
-
-    // read next skip entry
-    skipDoc[level] += readSkipData(level, skipStream[level]);
-    
-    if (level != 0) {
-      // read the child pointer if we are not on the leaf level
-      childPointer[level] = skipStream[level].readVLong() + skipPointer[level - 1];
-    }
-    
-    return true;
-
-  }
-  
-  /** Seeks the skip entry on the given level */
-  protected void seekChild(int level) throws IOException {
-    skipStream[level].seek(lastChildPointer);
-    numSkipped[level] = numSkipped[level + 1] - skipInterval[level + 1];
-    skipDoc[level] = lastDoc;
-    if (level > 0) {
-        childPointer[level] = skipStream[level].readVLong() + skipPointer[level - 1];
-    }
-  }
-
-  public void close() throws IOException {
-    for (int i = 1; i < skipStream.length; i++) {
-      if (skipStream[i] != null) {
-        skipStream[i].close();
-      }
-    }
-  }
-
-  /** initializes the reader */
-  public void init(long skipPointer, int df) {
-    this.skipPointer[0] = skipPointer;
-    this.docCount = df;
-    assert skipPointer >= 0 && skipPointer <= skipStream[0].length() 
-    : "invalid skip pointer: " + skipPointer + ", length=" + skipStream[0].length();
-    Arrays.fill(skipDoc, 0);
-    Arrays.fill(numSkipped, 0);
-    Arrays.fill(childPointer, 0);
-    
-    haveSkipped = false;
-    for (int i = 1; i < numberOfSkipLevels; i++) {
-      skipStream[i] = null;
-    }
-  }
-  
-  /** returns x == 0 ? 0 : Math.floor(Math.log(x) / Math.log(base)) */
-  static int log(int x, int base) {
-    assert base >= 2;
-    int ret = 0;
-    long n = base; // needs to be a long to avoid overflow
-    while (x >= n) {
-      n *= base;
-      ret++;
-    }
-    return ret;
-  }
-  
-  /** Loads the skip levels  */
-  private void loadSkipLevels() throws IOException {
-    numberOfSkipLevels = log(docCount, skipInterval[0]);
-    if (numberOfSkipLevels > maxNumberOfSkipLevels) {
-      numberOfSkipLevels = maxNumberOfSkipLevels;
-    }
-
-    skipStream[0].seek(skipPointer[0]);
-    
-    int toBuffer = numberOfLevelsToBuffer;
-    
-    for (int i = numberOfSkipLevels - 1; i > 0; i--) {
-      // the length of the current level
-      long length = skipStream[0].readVLong();
-      
-      // the start pointer of the current level
-      skipPointer[i] = skipStream[0].getFilePointer();
-      if (toBuffer > 0) {
-        // buffer this level
-        skipStream[i] = new SkipBuffer(skipStream[0], (int) length);
-        toBuffer--;
-      } else {
-        // clone this stream, it is already at the start of the current level
-        skipStream[i] = (IndexInput) skipStream[0].clone();
-        if (inputIsBuffered && length < BufferedIndexInput.BUFFER_SIZE) {
-          ((BufferedIndexInput) skipStream[i]).setBufferSize((int) length);
-        }
-        
-        // move base stream beyond the current level
-        skipStream[0].seek(skipStream[0].getFilePointer() + length);
-      }
-    }
-   
-    // use base stream for the lowest level
-    skipPointer[0] = skipStream[0].getFilePointer();
-  }
-  
-  /**
-   * Subclasses must implement the actual skip data encoding in this method.
-   *  
-   * @param level the level skip data shall be read from
-   * @param skipStream the skip stream to read from
-   */  
-  protected abstract int readSkipData(int level, IndexInput skipStream) throws IOException;
-  
-  /** Copies the values of the last read skip entry on this level */
-  protected void setLastSkipData(int level) {
-    lastDoc = skipDoc[level];
-    lastChildPointer = childPointer[level];
-  }
-
-  
-  /** used to buffer the top skip levels */
-  private final static class SkipBuffer extends IndexInput {
-    private byte[] data;
-    private long pointer;
-    private int pos;
-    
-    SkipBuffer(IndexInput input, int length) throws IOException {
-      super("SkipBuffer on " + input);
-      data = new byte[length];
-      pointer = input.getFilePointer();
-      input.readBytes(data, 0, length);
-    }
-    
-    @Override
-    public void close() throws IOException {
-      data = null;
-    }
-
-    @Override
-    public long getFilePointer() {
-      return pointer + pos;
-    }
-
-    @Override
-    public long length() {
-      return data.length;
-    }
-
-    @Override
-    public byte readByte() throws IOException {
-      return data[pos++];
-    }
-
-    @Override
-    public void readBytes(byte[] b, int offset, int len) throws IOException {
-      System.arraycopy(data, pos, b, offset, len);
-      pos += len;
-    }
-
-    @Override
-    public void seek(long pos) throws IOException {
-      this.pos =  (int) (pos - pointer);
-    }
-    
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/MultiLevelSkipListWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/MultiLevelSkipListWriter.java
deleted file mode 100644
index 476fdd2..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/MultiLevelSkipListWriter.java
+++ /dev/null
@@ -1,153 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-
-/**
- * This abstract class writes skip lists with multiple levels.
- * 
- * Example for skipInterval = 3:
- *                                                     c            (skip level 2)
- *                 c                 c                 c            (skip level 1) 
- *     x     x     x     x     x     x     x     x     x     x      (skip level 0)
- * d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d  (posting list)
- *     3     6     9     12    15    18    21    24    27    30     (df)
- * 
- * d - document
- * x - skip data
- * c - skip data with child pointer
- * 
- * Skip level i contains every skipInterval-th entry from skip level i-1.
- * Therefore the number of entries on level i is: floor(df / ((skipInterval ^ (i + 1))).
- * 
- * Each skip entry on a level i>0 contains a pointer to the corresponding skip entry in list i-1.
- * This guarantees a logarithmic amount of skips to find the target document.
- * 
- * While this class takes care of writing the different skip levels,
- * subclasses must define the actual format of the skip data.
- * @lucene.experimental
- */
-
-public abstract class MultiLevelSkipListWriter {
-  // number of levels in this skip list
-  protected int numberOfSkipLevels;
-  
-  // the skip interval in the list with level = 0
-  private int skipInterval;
-  
-  // for every skip level a different buffer is used 
-  private RAMOutputStream[] skipBuffer;
-
-  protected MultiLevelSkipListWriter(int skipInterval, int maxSkipLevels, int df) {
-    this.skipInterval = skipInterval;
-    
-    // calculate the maximum number of skip levels for this document frequency
-    numberOfSkipLevels = MultiLevelSkipListReader.log(df, skipInterval);
-    
-    // make sure it does not exceed maxSkipLevels
-    if (numberOfSkipLevels > maxSkipLevels) {
-      numberOfSkipLevels = maxSkipLevels;
-    }
-  }
-  
-  protected void init() {
-    skipBuffer = new RAMOutputStream[numberOfSkipLevels];
-    for (int i = 0; i < numberOfSkipLevels; i++) {
-      skipBuffer[i] = new RAMOutputStream();
-    }
-  }
-
-  protected void resetSkip() {
-    // creates new buffers or empties the existing ones
-    if (skipBuffer == null) {
-      init();
-    } else {
-      for (int i = 0; i < skipBuffer.length; i++) {
-        skipBuffer[i].reset();
-      }
-    }      
-  }
-
-  /**
-   * Subclasses must implement the actual skip data encoding in this method.
-   *  
-   * @param level the level skip data shall be writing for
-   * @param skipBuffer the skip buffer to write to
-   */
-  protected abstract void writeSkipData(int level, IndexOutput skipBuffer) throws IOException;
-  
-  /**
-   * Writes the current skip data to the buffers. The current document frequency determines
-   * the max level is skip data is to be written to. 
-   * 
-   * @param df the current document frequency 
-   * @throws IOException
-   */
-  public void bufferSkip(int df) throws IOException {
-    int numLevels;
-   
-    // determine max level
-    for (numLevels = 0; (df % skipInterval) == 0 && numLevels < numberOfSkipLevels; df /= skipInterval) {
-      numLevels++;
-    }
-    
-    long childPointer = 0;
-    
-    for (int level = 0; level < numLevels; level++) {
-      writeSkipData(level, skipBuffer[level]);
-      
-      long newChildPointer = skipBuffer[level].getFilePointer();
-      
-      if (level != 0) {
-        // store child pointers for all levels except the lowest
-        skipBuffer[level].writeVLong(childPointer);
-      }
-      
-      //remember the childPointer for the next level
-      childPointer = newChildPointer;
-    }
-  }
-
-  /**
-   * Writes the buffered skip lists to the given output.
-   * 
-   * @param output the IndexOutput the skip lists shall be written to 
-   * @return the pointer the skip list starts
-   */
-  public long writeSkip(IndexOutput output) throws IOException {
-    long skipPointer = output.getFilePointer();
-    //System.out.println("skipper.writeSkip fp=" + skipPointer);
-    if (skipBuffer == null || skipBuffer.length == 0) return skipPointer;
-    
-    for (int level = numberOfSkipLevels - 1; level > 0; level--) {
-      long length = skipBuffer[level].getFilePointer();
-      if (length > 0) {
-        output.writeVLong(length);
-        skipBuffer[level].writeTo(output);
-      }
-    }
-    skipBuffer[0].writeTo(output);
-    
-    return skipPointer;
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/NormsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/NormsFormat.java
deleted file mode 100644
index 5db3dfc..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/NormsFormat.java
+++ /dev/null
@@ -1,44 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * format for normalization factors
- */
-public abstract class NormsFormat {
-  /** Note: separateNormsDir should not be used! */
-  public abstract NormsReader normsReader(Directory dir, SegmentInfo info, FieldInfos fields, IOContext context, Directory separateNormsDir) throws IOException;
-  public abstract NormsWriter normsWriter(SegmentWriteState state) throws IOException;
-  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
-  
-  /** 
-   * Note: this should not be overridden! 
-   * @deprecated 
-   */
-  @Deprecated
-  public void separateFiles(Directory dir, SegmentInfo info, Set<String> files) throws IOException {};
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/NormsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/NormsReader.java
deleted file mode 100644
index 645902a..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/NormsReader.java
+++ /dev/null
@@ -1,26 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-//simple api just for now before switching to docvalues apis
-public abstract class NormsReader implements Closeable {
-  public abstract byte[] norms(String name) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/NormsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/NormsWriter.java
deleted file mode 100644
index aa1e5b5..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/NormsWriter.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.util.Bits;
-
-// simple api just for now before switching to docvalues apis
-public abstract class NormsWriter implements Closeable {
-
-  // TODO: I think IW should set info.normValueType from Similarity,
-  // and then this method just returns DocValuesConsumer
-  public abstract void startField(FieldInfo info) throws IOException;
-  public abstract void writeNorm(byte norm) throws IOException;
-  public abstract void finish(int numDocs) throws IOException;
-  
-  public int merge(MergeState mergeState) throws IOException {
-    int numMergedDocs = 0;
-    for (FieldInfo fi : mergeState.fieldInfos) {
-      if (fi.isIndexed && !fi.omitNorms) {
-        startField(fi);
-        int numMergedDocsForField = 0;
-        for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
-          final int maxDoc = reader.reader.maxDoc();
-          byte normBuffer[] = reader.reader.norms(fi.name);
-          if (normBuffer == null) {
-            // Can be null if this segment doesn't have
-            // any docs with this field
-            normBuffer = new byte[maxDoc];
-            Arrays.fill(normBuffer, (byte)0);
-          }
-          // this segment has deleted docs, so we have to
-          // check for every doc if it is deleted or not
-          final Bits liveDocs = reader.liveDocs;
-          for (int k = 0; k < maxDoc; k++) {
-            if (liveDocs == null || liveDocs.get(k)) {
-              writeNorm(normBuffer[k]);
-              numMergedDocsForField++;
-            }
-          }
-          mergeState.checkAbort.work(maxDoc);
-        }
-        assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;
-        numMergedDocs = numMergedDocsForField;
-      }
-    }
-    finish(numMergedDocs);
-    return numMergedDocs;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/PerDocConsumer.java b/lucene/src/java/org/apache/lucene/index/codecs/PerDocConsumer.java
deleted file mode 100644
index 5b99ce0..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/PerDocConsumer.java
+++ /dev/null
@@ -1,59 +0,0 @@
-package org.apache.lucene.index.codecs;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.MergeState;
-
-/**
- * Abstract API that consumes per document values. Concrete implementations of
- * this convert field values into a Codec specific format during indexing.
- * <p>
- * The {@link PerDocConsumer} API is accessible through the
- * {@link PostingsFormat} - API providing per field consumers and producers for inverted
- * data (terms, postings) as well as per-document data.
- * 
- * @lucene.experimental
- */
-public abstract class PerDocConsumer implements Closeable{
-  /** Adds a new DocValuesField */
-  public abstract DocValuesConsumer addValuesField(DocValues.Type type, FieldInfo field)
-      throws IOException;
-
-  /**
-   * Consumes and merges the given {@link PerDocProducer} producer
-   * into this consumers format.   
-   */
-  public void merge(MergeState mergeState) throws IOException {
-    final DocValues[] docValues = new DocValues[mergeState.readers.size()];
-
-    for (FieldInfo fieldInfo : mergeState.fieldInfos) {
-      mergeState.fieldInfo = fieldInfo; // set the field we are merging
-      if (fieldInfo.hasDocValues()) {
-        for (int i = 0; i < docValues.length; i++) {
-          docValues[i] = mergeState.readers.get(i).reader.docValues(fieldInfo.name);
-        }
-        final DocValuesConsumer docValuesConsumer = addValuesField(fieldInfo.getDocValuesType(), fieldInfo);
-        assert docValuesConsumer != null;
-        docValuesConsumer.merge(mergeState, docValues);
-      }
-    }
-  }  
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/PerDocProducer.java b/lucene/src/java/org/apache/lucene/index/codecs/PerDocProducer.java
deleted file mode 100644
index ff2827d..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/PerDocProducer.java
+++ /dev/null
@@ -1,48 +0,0 @@
-package org.apache.lucene.index.codecs;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.DocValues;
-
-/**
- * Abstract API that provides access to one or more per-document storage
- * features. The concrete implementations provide access to the underlying
- * storage on a per-document basis corresponding to their actual
- * {@link PerDocConsumer} counterpart.
- * <p>
- * The {@link PerDocProducer} API is accessible through the
- * {@link PostingsFormat} - API providing per field consumers and producers for inverted
- * data (terms, postings) as well as per-document data.
- * 
- * @lucene.experimental
- */
-public abstract class PerDocProducer implements Closeable {
-  /**
-   * Returns {@link DocValues} for the current field.
-   * 
-   * @param field
-   *          the field name
-   * @return the {@link DocValues} for this field or <code>null</code> if not
-   *         applicable.
-   * @throws IOException
-   */
-  public abstract DocValues docValues(String field) throws IOException;
-
-  public static final PerDocProducer[] EMPTY_ARRAY = new PerDocProducer[0];
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/PostingsBaseFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/PostingsBaseFormat.java
deleted file mode 100644
index fabb8dc..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/PostingsBaseFormat.java
+++ /dev/null
@@ -1,55 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.Directory;
-
-/** 
- * Provides a {@link PostingsReaderBase} and {@link
- * PostingsWriterBase}.
- *
- * @lucene.experimental */
-
-// TODO: find a better name; this defines the API that the
-// terms dict impls use to talk to a postings impl.
-// TermsDict + PostingsReader/WriterBase == PostingsConsumer/Producer
-
-// can we clean this up and do this some other way? 
-// refactor some of these classes and use covariant return?
-public abstract class PostingsBaseFormat {
-
-  /** Unique name that's used to retrieve this codec when
-   *  reading the index */
-  public final String name;
-  
-  protected PostingsBaseFormat(String name) {
-    this.name = name;
-  }
-
-  public abstract PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException;
-
-  public abstract PostingsWriterBase postingsWriterBase(SegmentWriteState state) throws IOException;
-
-  public abstract void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/PostingsConsumer.java b/lucene/src/java/org/apache/lucene/index/codecs/PostingsConsumer.java
deleted file mode 100644
index f988440..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/PostingsConsumer.java
+++ /dev/null
@@ -1,118 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-
-/**
- * @lucene.experimental
- */
-
-public abstract class PostingsConsumer {
-
-  /** Adds a new doc in this term.  If this field omits term
-   *  freqs & positions then termDocFreq should be ignored,
-   *  and, finishDoc will not be called. */
-  public abstract void startDoc(int docID, int termDocFreq) throws IOException;
-
-  public static class PostingsMergeState {
-    DocsEnum docsEnum;
-    int[] docMap;
-    int docBase;
-  }
-
-  /** Add a new position & payload.  A null payload means no
-   *  payload; a non-null payload with zero length also
-   *  means no payload.  Caller may reuse the {@link
-   *  BytesRef} for the payload between calls (method must
-   *  fully consume the payload). */
-  public abstract void addPosition(int position, BytesRef payload) throws IOException;
-
-  /** Called when we are done adding positions & payloads
-   *  for each doc.  Not called  when the field omits term
-   *  freq and positions. */
-  public abstract void finishDoc() throws IOException;
-
-  /** Default merge impl: append documents, mapping around
-   *  deletes */
-  public TermStats merge(final MergeState mergeState, final DocsEnum postings, final FixedBitSet visitedDocs) throws IOException {
-
-    int df = 0;
-    long totTF = 0;
-
-    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
-      while(true) {
-        final int doc = postings.nextDoc();
-        if (doc == DocIdSetIterator.NO_MORE_DOCS) {
-          break;
-        }
-        visitedDocs.set(doc);
-        this.startDoc(doc, 0);
-        this.finishDoc();
-        df++;
-      }
-      totTF = -1;
-    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {
-      while(true) {
-        final int doc = postings.nextDoc();
-        if (doc == DocIdSetIterator.NO_MORE_DOCS) {
-          break;
-        }
-        visitedDocs.set(doc);
-        final int freq = postings.freq();
-        this.startDoc(doc, freq);
-        this.finishDoc();
-        df++;
-        totTF += freq;
-      }
-    } else {
-      final DocsAndPositionsEnum postingsEnum = (DocsAndPositionsEnum) postings;
-      while(true) {
-        final int doc = postingsEnum.nextDoc();
-        if (doc == DocIdSetIterator.NO_MORE_DOCS) {
-          break;
-        }
-        visitedDocs.set(doc);
-        final int freq = postingsEnum.freq();
-        this.startDoc(doc, freq);
-        totTF += freq;
-        for(int i=0;i<freq;i++) {
-          final int position = postingsEnum.nextPosition();
-          final BytesRef payload;
-          if (postingsEnum.hasPayload()) {
-            payload = postingsEnum.getPayload();
-          } else {
-            payload = null;
-          }
-          this.addPosition(position, payload);
-        }
-        this.finishDoc();
-        df++;
-      }
-    }
-    return new TermStats(df, totTF);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/PostingsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/PostingsFormat.java
deleted file mode 100644
index 407092d..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/PostingsFormat.java
+++ /dev/null
@@ -1,84 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.util.NamedSPILoader;
-
-import org.apache.lucene.store.Directory;
-
-/** @lucene.experimental */
-public abstract class PostingsFormat implements NamedSPILoader.NamedSPI {
-
-  private static final NamedSPILoader<PostingsFormat> loader =
-    new NamedSPILoader<PostingsFormat>(PostingsFormat.class);
-
-  public static final PostingsFormat[] EMPTY = new PostingsFormat[0];
-  /** Unique name that's used to retrieve this format when
-   *  reading the index.
-   */
-  private final String name;
-  
-  protected PostingsFormat(String name) {
-    this.name = name;
-  }
-
-  @Override
-  public String getName() {
-    return name;
-  }
-  
-  /** Writes a new segment */
-  public abstract FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException;
-
-  /** Reads a segment.  NOTE: by the time this call
-   *  returns, it must hold open any files it will need to
-   *  use; else, those files may be deleted. */
-  public abstract FieldsProducer fieldsProducer(SegmentReadState state) throws IOException;
-
-  /**
-   * Gathers files associated with this segment
-   * 
-   * @param dir the {@link Directory} this segment was written to
-   * @param segmentInfo the {@link SegmentInfo} for this segment 
-   * @param segmentSuffix the format's suffix within this segment
-   * @param files the of files to add the codec files to.
-   */
-  public abstract void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException;
-
-  @Override
-  public String toString() {
-    return "PostingsFormat(name=" + name + ")";
-  }
-  
-  /** looks up a format by name */
-  public static PostingsFormat forName(String name) {
-    return loader.lookup(name);
-  }
-  
-  /** returns a list of all available format names */
-  public static Set<String> availablePostingsFormats() {
-    return loader.availableServices();
-  }
-  
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/PostingsReaderBase.java b/lucene/src/java/org/apache/lucene/index/codecs/PostingsReaderBase.java
deleted file mode 100644
index 9976881..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/PostingsReaderBase.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Closeable;
-
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Bits;
-
-/** The core terms dictionaries (BlockTermsReader,
- *  BlockTreeTermsReader) interact with a single instance
- *  of this class to manage creation of {@link DocsEnum} and
- *  {@link DocsAndPositionsEnum} instances.  It provides an
- *  IndexInput (termsIn) where this class may read any
- *  previously stored data that it had written in its
- *  corresponding {@link PostingsWriterBase} at indexing
- *  time. 
- *  @lucene.experimental */
-
-// TODO: find a better name; this defines the API that the
-// terms dict impls use to talk to a postings impl.
-// TermsDict + PostingsReader/WriterBase == PostingsConsumer/Producer
-public abstract class PostingsReaderBase implements Closeable {
-
-  public abstract void init(IndexInput termsIn) throws IOException;
-
-  /** Return a newly created empty TermState */
-  public abstract BlockTermState newTermState() throws IOException;
-
-  /** Actually decode metadata for next term */
-  public abstract void nextTerm(FieldInfo fieldInfo, BlockTermState state) throws IOException;
-
-  /** Must fully consume state, since after this call that
-   *  TermState may be reused. */
-  public abstract DocsEnum docs(FieldInfo fieldInfo, BlockTermState state, Bits skipDocs, DocsEnum reuse, boolean needsFreqs) throws IOException;
-
-  /** Must fully consume state, since after this call that
-   *  TermState may be reused. */
-  public abstract DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState state, Bits skipDocs, DocsAndPositionsEnum reuse) throws IOException;
-
-  public abstract void close() throws IOException;
-
-  /** Reads data for all terms in the next block; this
-   *  method should merely load the byte[] blob but not
-   *  decode, which is done in {@link #nextTerm}. */
-  public abstract void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState termState) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/PostingsWriterBase.java b/lucene/src/java/org/apache/lucene/index/codecs/PostingsWriterBase.java
deleted file mode 100644
index b456615..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/PostingsWriterBase.java
+++ /dev/null
@@ -1,51 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Closeable;
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.index.FieldInfo;
-
-/**
- * @lucene.experimental
- */
-
-// TODO: find a better name; this defines the API that the
-// terms dict impls use to talk to a postings impl.
-// TermsDict + PostingsReader/WriterBase == PostingsConsumer/Producer
-public abstract class PostingsWriterBase extends PostingsConsumer implements Closeable {
-
-  public abstract void start(IndexOutput termsOut) throws IOException;
-
-  public abstract void startTerm() throws IOException;
-
-  /** Flush count terms starting at start "backwards", as a
-   *  block. start is a negative offset from the end of the
-   *  terms stack, ie bigger start means further back in
-   *  the stack. */
-  public abstract void flushTermsBlock(int start, int count) throws IOException;
-
-  /** Finishes the current term */
-  public abstract void finishTerm(TermStats stats) throws IOException;
-
-  public abstract void setField(FieldInfo fieldInfo);
-
-  public abstract void close() throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/SegmentInfosFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/SegmentInfosFormat.java
deleted file mode 100644
index 1632ce1..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/SegmentInfosFormat.java
+++ /dev/null
@@ -1,37 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Expert: Controls the format of the segments file.
- * Note, this isn't a per-segment file, if you change the format, other versions
- * of lucene won't be able to read it, yackedy schmackedy
- * 
- * @lucene.experimental
- */
-// TODO: would be great to handle this situation better.
-// ideally a custom implementation could implement two-phase commit differently,
-// (e.g. atomic rename), and ideally all versions of lucene could still read it.
-// but this is just reflecting reality as it is today...
-//
-// also, perhaps the name should change (to cover all global files like .fnx?)
-// then again, maybe we can just remove that file...
-public abstract class SegmentInfosFormat {
-  public abstract SegmentInfosReader getSegmentInfosReader();
-  public abstract SegmentInfosWriter getSegmentInfosWriter();
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/SegmentInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/SegmentInfosReader.java
deleted file mode 100644
index e3f8a7c..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/SegmentInfosReader.java
+++ /dev/null
@@ -1,42 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Specifies an API for classes that can read {@link SegmentInfos} information.
- * @lucene.experimental
- */
-public abstract class SegmentInfosReader {
-
-  /**
-   * Read {@link SegmentInfos} data from a directory.
-   * @param directory directory to read from
-   * @param segmentsFileName name of the "segments_N" file
-   * @param header input of "segments_N" file after reading preamble
-   * @param infos empty instance to be populated with data
-   * @throws IOException
-   */
-  public abstract void read(Directory directory, String segmentsFileName, ChecksumIndexInput header, SegmentInfos infos, IOContext context) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/SegmentInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/SegmentInfosWriter.java
deleted file mode 100644
index d392b87..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/SegmentInfosWriter.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-
-/**
- * Specifies an API for classes that can write out {@link SegmentInfos} data.
- * @lucene.experimental
- */
-public abstract class SegmentInfosWriter {
-
-  /**
-   * Write {@link SegmentInfos} data without closing the output. The returned
-   * output will become finished only after a successful completion of
-   * "two phase commit" that first calls {@link #prepareCommit(IndexOutput)} and
-   * then {@link #finishCommit(IndexOutput)}.
-   * @param dir directory to write data to
-   * @param segmentsFileName name of the "segments_N" file to create
-   * @param infos data to write
-   * @return an instance of {@link IndexOutput} to be used in subsequent "two
-   * phase commit" operations as described above.
-   * @throws IOException
-   */
-  public abstract IndexOutput writeInfos(Directory dir, String segmentsFileName, String codecID, SegmentInfos infos, IOContext context) throws IOException;
-  
-  /**
-   * First phase of the two-phase commit - ensure that all output can be
-   * successfully written out.
-   * @param out an instance of {@link IndexOutput} returned from a previous
-   * call to {@link #writeInfos(Directory, String, String, SegmentInfos, IOContext)}.
-   * @throws IOException
-   */
-  public abstract void prepareCommit(IndexOutput out) throws IOException;
-  
-  /**
-   * Second phase of the two-phase commit. In this step the output should be
-   * finalized and closed.
-   * @param out an instance of {@link IndexOutput} returned from a previous
-   * call to {@link #writeInfos(Directory, String, String, SegmentInfos, IOContext)}.
-   * @throws IOException
-   */
-  public abstract void finishCommit(IndexOutput out) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsFormat.java
deleted file mode 100644
index de4348b..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsFormat.java
+++ /dev/null
@@ -1,35 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Controls the format of stored fields
- */
-public abstract class StoredFieldsFormat {
-  public abstract StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException;
-  public abstract StoredFieldsWriter fieldsWriter(Directory directory, String segment, IOContext context) throws IOException;
-  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsReader.java
deleted file mode 100644
index 91af63b..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsReader.java
+++ /dev/null
@@ -1,39 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Copyright 2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not
- * use this file except in compliance with the License. You may obtain a copy of
- * the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.StoredFieldVisitor;
-
-/**
- * Codec API for reading stored fields:
- * 
- * You need to implement {@link #visitDocument(int, StoredFieldVisitor)} to
- * read the stored fields for a document, implement {@link #clone()} (creating
- * clones of any IndexInputs used, etc), and {@link #close()}
- * @lucene.experimental
- */
-public abstract class StoredFieldsReader implements Cloneable, Closeable {
-  
-  /** Visit the stored fields for document <code>n</code> */
-  public abstract void visitDocument(int n, StoredFieldVisitor visitor) throws CorruptIndexException, IOException;
-
-  public abstract StoredFieldsReader clone();
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsWriter.java
deleted file mode 100644
index 47237d7..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/StoredFieldsWriter.java
+++ /dev/null
@@ -1,118 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.util.Bits;
-
-/**
- * Copyright 2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not
- * use this file except in compliance with the License. You may obtain a copy of
- * the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-/**
- * Codec API for writing stored fields:
- * <p>
- * <ol>
- *   <li>For every document, {@link #startDocument(int)} is called,
- *       informing the Codec how many fields will be written.
- *   <li>{@link #writeField(FieldInfo, IndexableField)} is called for 
- *       each field in the document.
- *   <li>After all documents have been written, {@link #finish(int)} 
- *       is called for verification/sanity-checks.
- *   <li>Finally the writer is closed ({@link #close()})
- * </ol>
- * 
- * @lucene.experimental
- */
-public abstract class StoredFieldsWriter implements Closeable {
-  
-  /** Called before writing the stored fields of the document.
-   *  {@link #writeField(FieldInfo, IndexableField)} will be called
-   *  <code>numStoredFields</code> times. Note that this is
-   *  called even if the document has no stored fields, in
-   *  this case <code>numStoredFields</code> will be zero. */
-  public abstract void startDocument(int numStoredFields) throws IOException;
-  
-  /** Writes a single stored field. */
-  public abstract void writeField(FieldInfo info, IndexableField field) throws IOException;
-
-  /** Aborts writing entirely, implementation should remove
-   *  any partially-written files, etc. */
-  public abstract void abort();
-  
-  /** Called before {@link #close()}, passing in the number
-   *  of documents that were written. Note that this is 
-   *  intentionally redundant (equivalent to the number of
-   *  calls to {@link #startDocument(int)}, but a Codec should
-   *  check that this is the case to detect the JRE bug described 
-   *  in LUCENE-1282. */
-  public abstract void finish(int numDocs) throws IOException;
-  
-  /** Merges in the stored fields from the readers in 
-   *  <code>mergeState</code>. The default implementation skips
-   *  over deleted documents, and uses {@link #startDocument(int)},
-   *  {@link #writeField(FieldInfo, IndexableField)}, and {@link #finish(int)},
-   *  returning the number of documents that were written.
-   *  Implementations can override this method for more sophisticated
-   *  merging (bulk-byte copying, etc). */
-  public int merge(MergeState mergeState) throws IOException {
-    int docCount = 0;
-    for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
-      final int maxDoc = reader.reader.maxDoc();
-      final Bits liveDocs = reader.liveDocs;
-      for (int i = 0; i < maxDoc; i++) {
-        if (liveDocs != null && !liveDocs.get(i)) {
-          // skip deleted docs
-          continue;
-        }
-        // TODO: this could be more efficient using
-        // FieldVisitor instead of loading/writing entire
-        // doc; ie we just have to renumber the field number
-        // on the fly?
-        // NOTE: it's very important to first assign to doc then pass it to
-        // fieldsWriter.addDocument; see LUCENE-1282
-        Document doc = reader.reader.document(i);
-        addDocument(doc, mergeState.fieldInfos);
-        docCount++;
-        mergeState.checkAbort.work(300);
-      }
-    }
-    finish(docCount);
-    return docCount;
-  }
-  
-  /** sugar method for startDocument() + writeField() for every stored field in the document */
-  protected final void addDocument(Iterable<? extends IndexableField> doc, FieldInfos fieldInfos) throws IOException {
-    int storedCount = 0;
-    for (IndexableField field : doc) {
-      if (field.fieldType().stored()) {
-        storedCount++;
-      }
-    }
-    
-    startDocument(storedCount);
-
-    for (IndexableField field : doc) {
-      if (field.fieldType().stored()) {
-        writeField(fieldInfos.fieldInfo(field.name()), field);
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/TermStats.java b/lucene/src/java/org/apache/lucene/index/codecs/TermStats.java
deleted file mode 100644
index bb2b6f3..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/TermStats.java
+++ /dev/null
@@ -1,28 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TermStats {
-  public final int docFreq;
-  public final long totalTermFreq;
-
-  public TermStats(int docFreq, long totalTermFreq) {
-    this.docFreq = docFreq;
-    this.totalTermFreq = totalTermFreq;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsFormat.java
deleted file mode 100644
index ecbf415..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsFormat.java
+++ /dev/null
@@ -1,35 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Controls the format of term vectors
- */
-public abstract class TermVectorsFormat {
-  public abstract TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException;
-  public abstract TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException;
-  public abstract void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsReader.java
deleted file mode 100644
index 45580c5..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsReader.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute; // javadocs
-import org.apache.lucene.index.DocsAndPositionsEnum; // javadocs
-import org.apache.lucene.index.Fields;
-
-/**
- * Codec API for reading term vectors:
- * 
- * @lucene.experimental
- */
-public abstract class TermVectorsReader implements Cloneable,Closeable {
-
-  /** Returns term vectors for this document, or null if
-   *  term vectors were not indexed. If offsets are
-   *  available they are in an {@link OffsetAttribute}
-   *  available from the {@link DocsAndPositionsEnum}. */
-  public abstract Fields get(int doc) throws IOException;
-
-  /** Create a clone that one caller at a time may use to
-   *  read term vectors. */
-  public abstract TermVectorsReader clone();
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsWriter.java
deleted file mode 100644
index 5eeaacc..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/TermVectorsWriter.java
+++ /dev/null
@@ -1,278 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Codec API for writing term vectors:
- * <p>
- * <ol>
- *   <li>For every document, {@link #startDocument(int)} is called,
- *       informing the Codec how many fields will be written.
- *   <li>{@link #startField(FieldInfo, int, boolean, boolean)} is called for 
- *       each field in the document, informing the codec how many terms
- *       will be written for that field, and whether or not positions
- *       or offsets are enabled.
- *   <li>Within each field, {@link #startTerm(BytesRef, int)} is called
- *       for each term.
- *   <li>If offsets and/or positions are enabled, then 
- *       {@link #addPosition(int, int, int)} will be called for each term
- *       occurrence.
- *   <li>After all documents have been written, {@link #finish(int)} 
- *       is called for verification/sanity-checks.
- *   <li>Finally the writer is closed ({@link #close()})
- * </ol>
- * 
- * @lucene.experimental
- */
-public abstract class TermVectorsWriter implements Closeable {
-  
-  /** Called before writing the term vectors of the document.
-   *  {@link #startField(FieldInfo, int, boolean, boolean)} will 
-   *  be called <code>numVectorFields</code> times. Note that if term 
-   *  vectors are enabled, this is called even if the document 
-   *  has no vector fields, in this case <code>numVectorFields</code> 
-   *  will be zero. */
-  public abstract void startDocument(int numVectorFields) throws IOException;
-  
-  /** Called before writing the terms of the field.
-   *  {@link #startTerm(BytesRef, int)} will be called <code>numTerms</code> times. */
-  public abstract void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets) throws IOException;
-  
-  /** Adds a term and its term frequency <code>freq</code>.
-   * If this field has positions and/or offsets enabled, then
-   * {@link #addPosition(int, int, int)} will be called 
-   * <code>freq</code> times respectively.
-   */
-  public abstract void startTerm(BytesRef term, int freq) throws IOException;
-  
-  /** Adds a term position and offsets */
-  public abstract void addPosition(int position, int startOffset, int endOffset) throws IOException;
-  
-  /** Aborts writing entirely, implementation should remove
-   *  any partially-written files, etc. */
-  public abstract void abort();
-
-  /** Called before {@link #close()}, passing in the number
-   *  of documents that were written. Note that this is 
-   *  intentionally redundant (equivalent to the number of
-   *  calls to {@link #startDocument(int)}, but a Codec should
-   *  check that this is the case to detect the JRE bug described 
-   *  in LUCENE-1282. */
-  public abstract void finish(int numDocs) throws IOException;
-  
-  /** 
-   * Called by IndexWriter when writing new segments.
-   * <p>
-   * This is an expert API that allows the codec to consume 
-   * positions and offsets directly from the indexer.
-   * <p>
-   * The default implementation calls {@link #addPosition(int, int, int)},
-   * but subclasses can override this if they want to efficiently write 
-   * all the positions, then all the offsets, for example.
-   * <p>
-   * NOTE: This API is extremely expert and subject to change or removal!!!
-   * @lucene.internal
-   */
-  // TODO: we should probably nuke this and make a more efficient 4.x format
-  // PreFlex-RW could then be slow and buffer (its only used in tests...)
-  public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
-    int position = 0;
-    int lastOffset = 0;
-
-    for (int i = 0; i < numProx; i++) {
-      final int startOffset;
-      final int endOffset;
-      
-      if (positions == null) {
-        position = -1;
-      } else {
-        position += positions.readVInt();
-      }
-      
-      if (offsets == null) {
-        startOffset = endOffset = -1;
-      } else {
-        startOffset = lastOffset + offsets.readVInt();
-        endOffset = startOffset + offsets.readVInt();
-        lastOffset = endOffset;
-      }
-      addPosition(position, startOffset, endOffset);
-    }
-  }
-  
-  /** Merges in the term vectors from the readers in 
-   *  <code>mergeState</code>. The default implementation skips
-   *  over deleted documents, and uses {@link #startDocument(int)},
-   *  {@link #startField(FieldInfo, int, boolean, boolean)}, 
-   *  {@link #startTerm(BytesRef, int)}, {@link #addPosition(int, int, int)},
-   *  and {@link #finish(int)},
-   *  returning the number of documents that were written.
-   *  Implementations can override this method for more sophisticated
-   *  merging (bulk-byte copying, etc). */
-  public int merge(MergeState mergeState) throws IOException {
-    int docCount = 0;
-    for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
-      final int maxDoc = reader.reader.maxDoc();
-      final Bits liveDocs = reader.liveDocs;
-      for (int docID = 0; docID < maxDoc; docID++) {
-        if (liveDocs != null && !liveDocs.get(docID)) {
-          // skip deleted docs
-          continue;
-        }
-        // NOTE: it's very important to first assign to vectors then pass it to
-        // termVectorsWriter.addAllDocVectors; see LUCENE-1282
-        Fields vectors = reader.reader.getTermVectors(docID);
-        addAllDocVectors(vectors, mergeState.fieldInfos);
-        docCount++;
-        mergeState.checkAbort.work(300);
-      }
-    }
-    finish(docCount);
-    return docCount;
-  }
-  
-  /** Safe (but, slowish) default method to write every
-   *  vector field in the document.  This default
-   *  implementation requires that the vectors implement
-   *  both Fields.getUniqueFieldCount and
-   *  Terms.getUniqueTermCount. */
-  protected final void addAllDocVectors(Fields vectors, FieldInfos fieldInfos) throws IOException {
-    if (vectors == null) {
-      startDocument(0);
-      return;
-    }
-
-    final int numFields = vectors.getUniqueFieldCount();
-    if (numFields == -1) {
-      throw new IllegalStateException("vectors.getUniqueFieldCount() must be implemented (it returned -1)");
-    }
-    startDocument(numFields);
-    
-    final FieldsEnum fieldsEnum = vectors.iterator();
-    String fieldName;
-    String lastFieldName = null;
-
-    while((fieldName = fieldsEnum.next()) != null) {
-      
-      final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldName);
-
-      assert lastFieldName == null || fieldName.compareTo(lastFieldName) > 0: "lastFieldName=" + lastFieldName + " fieldName=" + fieldName;
-      lastFieldName = fieldName;
-
-      final Terms terms = fieldsEnum.terms();
-      if (terms == null) {
-        // FieldsEnum shouldn't lie...
-        continue;
-      }
-      final int numTerms = (int) terms.getUniqueTermCount();
-      if (numTerms == -1) {
-        throw new IllegalStateException("vector.getUniqueTermCount() must be implemented (it returned -1)");
-      }
-
-      final boolean positions;
-
-      OffsetAttribute offsetAtt;
-
-      final TermsEnum termsEnum = terms.iterator(null);
-
-      DocsAndPositionsEnum docsAndPositionsEnum = null;
-
-      if (termsEnum.next() != null) {
-        assert numTerms > 0;
-        docsAndPositionsEnum = termsEnum.docsAndPositions(null, null);
-        if (docsAndPositionsEnum != null) {
-          // has positions
-          positions = true;
-          if (docsAndPositionsEnum.attributes().hasAttribute(OffsetAttribute.class)) {
-            offsetAtt = docsAndPositionsEnum.attributes().getAttribute(OffsetAttribute.class);
-          } else {
-            offsetAtt = null;
-          }
-        } else {
-          positions = false;
-          offsetAtt = null;
-        }
-      } else {
-        // no terms in this field (hmm why is field present
-        // then...?)
-        assert numTerms == 0;
-        positions = false;
-        offsetAtt = null;
-      }
-      
-      startField(fieldInfo, numTerms, positions, offsetAtt != null);
-
-      int termCount = 1;
-
-      // NOTE: we already .next()'d the TermsEnum above, to
-      // peek @ first term to see if positions/offsets are
-      // present
-      while(true) {
-        final int freq = (int) termsEnum.totalTermFreq();
-        startTerm(termsEnum.term(), freq);
-
-        if (positions || offsetAtt != null) {
-          DocsAndPositionsEnum dp = termsEnum.docsAndPositions(null, docsAndPositionsEnum);
-          // TODO: add startOffset()/endOffset() to d&pEnum... this is insanity
-          if (dp != docsAndPositionsEnum) {
-            // producer didnt reuse, must re-pull attributes
-            if (offsetAtt != null) {
-              assert dp.attributes().hasAttribute(OffsetAttribute.class);
-              offsetAtt = dp.attributes().getAttribute(OffsetAttribute.class);
-            }
-          }
-          docsAndPositionsEnum = dp;
-          final int docID = docsAndPositionsEnum.nextDoc();
-          assert docID != DocsEnum.NO_MORE_DOCS;
-          assert docsAndPositionsEnum.freq() == freq;
-
-          for(int posUpto=0; posUpto<freq; posUpto++) {
-            final int pos = docsAndPositionsEnum.nextPosition();
-            final int startOffset = offsetAtt == null ? -1 : offsetAtt.startOffset();
-            final int endOffset = offsetAtt == null ? -1 : offsetAtt.endOffset();
-            
-            addPosition(pos, startOffset, endOffset);
-          }
-        }
-        
-        if (termsEnum.next() == null) {
-          assert termCount == numTerms;
-          break;
-        }
-        termCount++;
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer.java b/lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer.java
deleted file mode 100644
index c39d4db..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/TermsConsumer.java
+++ /dev/null
@@ -1,160 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.MultiDocsEnum;
-import org.apache.lucene.index.MultiDocsAndPositionsEnum;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-
-/**
- * @lucene.experimental
- */
-
-public abstract class TermsConsumer {
-
-  /** Starts a new term in this field; this may be called
-   *  with no corresponding call to finish if the term had
-   *  no docs. */
-  public abstract PostingsConsumer startTerm(BytesRef text) throws IOException;
-
-  /** Finishes the current term; numDocs must be > 0. */
-  public abstract void finishTerm(BytesRef text, TermStats stats) throws IOException;
-
-  /** Called when we are done adding terms to this field */
-  public abstract void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException;
-
-  /** Return the BytesRef Comparator used to sort terms
-   *  before feeding to this API. */
-  public abstract Comparator<BytesRef> getComparator() throws IOException;
-
-  /** Default merge impl */
-  private MappingMultiDocsEnum docsEnum;
-  private MappingMultiDocsEnum docsAndFreqsEnum;
-  private MappingMultiDocsAndPositionsEnum postingsEnum;
-
-  public void merge(MergeState mergeState, TermsEnum termsEnum) throws IOException {
-
-    BytesRef term;
-    assert termsEnum != null;
-    long sumTotalTermFreq = 0;
-    long sumDocFreq = 0;
-    long sumDFsinceLastAbortCheck = 0;
-    FixedBitSet visitedDocs = new FixedBitSet(mergeState.mergedDocCount);
-
-    if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
-      if (docsEnum == null) {
-        docsEnum = new MappingMultiDocsEnum();
-      }
-      docsEnum.setMergeState(mergeState);
-
-      MultiDocsEnum docsEnumIn = null;
-
-      while((term = termsEnum.next()) != null) {
-        // We can pass null for liveDocs, because the
-        // mapping enum will skip the non-live docs:
-        docsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsEnumIn, false);
-        if (docsEnumIn != null) {
-          docsEnum.reset(docsEnumIn);
-          final PostingsConsumer postingsConsumer = startTerm(term);
-          final TermStats stats = postingsConsumer.merge(mergeState, docsEnum, visitedDocs);
-          if (stats.docFreq > 0) {
-            finishTerm(term, stats);
-            sumTotalTermFreq += stats.docFreq;
-            sumDFsinceLastAbortCheck += stats.docFreq;
-            sumDocFreq += stats.docFreq;
-            if (sumDFsinceLastAbortCheck > 60000) {
-              mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);
-              sumDFsinceLastAbortCheck = 0;
-            }
-          }
-        }
-      }
-    } else if (mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS) {
-      if (docsAndFreqsEnum == null) {
-        docsAndFreqsEnum = new MappingMultiDocsEnum();
-      }
-      docsAndFreqsEnum.setMergeState(mergeState);
-
-      MultiDocsEnum docsAndFreqsEnumIn = null;
-
-      while((term = termsEnum.next()) != null) {
-        // We can pass null for liveDocs, because the
-        // mapping enum will skip the non-live docs:
-        docsAndFreqsEnumIn = (MultiDocsEnum) termsEnum.docs(null, docsAndFreqsEnumIn, true);
-        assert docsAndFreqsEnumIn != null;
-        docsAndFreqsEnum.reset(docsAndFreqsEnumIn);
-        final PostingsConsumer postingsConsumer = startTerm(term);
-        final TermStats stats = postingsConsumer.merge(mergeState, docsAndFreqsEnum, visitedDocs);
-        if (stats.docFreq > 0) {
-          finishTerm(term, stats);
-          sumTotalTermFreq += stats.totalTermFreq;
-          sumDFsinceLastAbortCheck += stats.docFreq;
-          sumDocFreq += stats.docFreq;
-          if (sumDFsinceLastAbortCheck > 60000) {
-            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);
-            sumDFsinceLastAbortCheck = 0;
-          }
-        }
-      }
-    } else {
-      assert mergeState.fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-      if (postingsEnum == null) {
-        postingsEnum = new MappingMultiDocsAndPositionsEnum();
-      }
-      postingsEnum.setMergeState(mergeState);
-      MultiDocsAndPositionsEnum postingsEnumIn = null;
-      while((term = termsEnum.next()) != null) {
-        // We can pass null for liveDocs, because the
-        // mapping enum will skip the non-live docs:
-        postingsEnumIn = (MultiDocsAndPositionsEnum) termsEnum.docsAndPositions(null, postingsEnumIn);
-        assert postingsEnumIn != null;
-        postingsEnum.reset(postingsEnumIn);
-        // set PayloadProcessor
-        if (mergeState.payloadProcessorProvider != null) {
-          for (int i = 0; i < mergeState.readers.size(); i++) {
-            if (mergeState.dirPayloadProcessor[i] != null) {
-              mergeState.currentPayloadProcessor[i] = mergeState.dirPayloadProcessor[i].getProcessor(mergeState.fieldInfo.name, term);
-            }
-          }
-        }
-        final PostingsConsumer postingsConsumer = startTerm(term);
-        final TermStats stats = postingsConsumer.merge(mergeState, postingsEnum, visitedDocs);
-        if (stats.docFreq > 0) {
-          finishTerm(term, stats);
-          sumTotalTermFreq += stats.totalTermFreq;
-          sumDFsinceLastAbortCheck += stats.docFreq;
-          sumDocFreq += stats.docFreq;
-          if (sumDFsinceLastAbortCheck > 60000) {
-            mergeState.checkAbort.work(sumDFsinceLastAbortCheck/5.0);
-            sumDFsinceLastAbortCheck = 0;
-          }
-        }
-      }
-    }
-
-    finish(sumTotalTermFreq, sumDocFreq, visitedDocs.cardinality());
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/TermsIndexReaderBase.java b/lucene/src/java/org/apache/lucene/index/codecs/TermsIndexReaderBase.java
deleted file mode 100644
index d66999a..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/TermsIndexReaderBase.java
+++ /dev/null
@@ -1,71 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.util.BytesRef;
-
-import java.io.IOException;
-import java.io.Closeable;
-import java.util.Collection;
-
-
-// TODO
-//   - allow for non-regular index intervals?  eg with a
-//     long string of rare terms, you don't need such
-//     frequent indexing
-
-/**
- * TermsDictReader interacts with an instance of this class
- * to manage its terms index.  The writer must accept
- * indexed terms (many pairs of CharSequence text + long
- * fileOffset), and then this reader must be able to
- * retrieve the nearest index term to a provided term
- * text. 
- * @lucene.experimental */
-
-public abstract class TermsIndexReaderBase implements Closeable {
-
-  public abstract FieldIndexEnum getFieldEnum(FieldInfo fieldInfo);
-
-  public abstract void close() throws IOException;
-
-  public abstract boolean supportsOrd();
-
-  public abstract int getDivisor();
-
-  // Similar to TermsEnum, except, the only "metadata" it
-  // reports for a given indexed term is the long fileOffset
-  // into the main terms dict (_X.tis) file:
-  public static abstract class FieldIndexEnum {
-
-    /** Seeks to "largest" indexed term that's <=
-     *  term; retruns file pointer index (into the main
-     *  terms index file) for that term */
-    public abstract long seek(BytesRef term) throws IOException;
-
-    /** Returns -1 at end */
-    public abstract long next() throws IOException;
-
-    public abstract BytesRef term();
-
-    // Only impl'd if supportsOrd() returns true!
-    public abstract long seek(long ord) throws IOException;
-    public abstract long ord();
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/TermsIndexWriterBase.java b/lucene/src/java/org/apache/lucene/index/codecs/TermsIndexWriterBase.java
deleted file mode 100644
index 4b959d0..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/TermsIndexWriterBase.java
+++ /dev/null
@@ -1,36 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.util.BytesRef;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-/** @lucene.experimental */
-public abstract class TermsIndexWriterBase implements Closeable {
-
-  public abstract class FieldWriter {
-    public abstract boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException;
-    public abstract void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException;
-    public abstract void finish(long termsFilePointer) throws IOException;
-  }
-
-  public abstract FieldWriter addField(FieldInfo fieldInfo, long termsFilePointer) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/VariableGapTermsIndexReader.java b/lucene/src/java/org/apache/lucene/index/codecs/VariableGapTermsIndexReader.java
deleted file mode 100644
index de80e6d..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/VariableGapTermsIndexReader.java
+++ /dev/null
@@ -1,232 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.FileOutputStream;   // for toDot
-import java.io.OutputStreamWriter; // for toDot
-import java.io.Writer;             // for toDot
-import java.util.Collection;
-import java.util.HashMap;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util; // for toDot
-
-/** See {@link VariableGapTermsIndexWriter}
- * 
- * @lucene.experimental */
-public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
-
-  private final PositiveIntOutputs fstOutputs = PositiveIntOutputs.getSingleton(true);
-  private int indexDivisor;
-
-  // Closed if indexLoaded is true:
-  private IndexInput in;
-  private volatile boolean indexLoaded;
-
-  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
-  
-  // start of the field info data
-  protected long dirOffset;
-
-  final String segment;
-  public VariableGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, String segmentSuffix, IOContext context)
-    throws IOException {
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION), new IOContext(context, true));
-    this.segment = segment;
-    boolean success = false;
-    assert indexDivisor == -1 || indexDivisor > 0;
-
-    try {
-      
-      readHeader(in);
-      this.indexDivisor = indexDivisor;
-
-      seekDir(in, dirOffset);
-
-      // Read directory
-      final int numFields = in.readVInt();
-
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final long indexStart = in.readVLong();
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        fields.put(fieldInfo, new FieldIndexData(fieldInfo, indexStart));
-      }
-      success = true;
-    } finally {
-      if (indexDivisor > 0) {
-        in.close();
-        in = null;
-        if (success) {
-          indexLoaded = true;
-        }
-      }
-    }
-  }
-
-  @Override
-  public int getDivisor() {
-    return indexDivisor;
-  }
-  
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, VariableGapTermsIndexWriter.CODEC_NAME,
-      VariableGapTermsIndexWriter.VERSION_START, VariableGapTermsIndexWriter.VERSION_START);
-    dirOffset = input.readLong();
-  }
-
-  private static class IndexEnum extends FieldIndexEnum {
-    private final BytesRefFSTEnum<Long> fstEnum;
-    private BytesRefFSTEnum.InputOutput<Long> current;
-
-    public IndexEnum(FST<Long> fst) {
-      fstEnum = new BytesRefFSTEnum<Long>(fst);
-    }
-
-    @Override
-    public BytesRef term() {
-      if (current == null) {
-        return null;
-      } else {
-        return current.input;
-      }
-    }
-
-    @Override
-    public long seek(BytesRef target) throws IOException {
-      //System.out.println("VGR: seek field=" + fieldInfo.name + " target=" + target);
-      current = fstEnum.seekFloor(target);
-      //System.out.println("  got input=" + current.input + " output=" + current.output);
-      return current.output;
-    }
-
-    @Override
-    public long next() throws IOException {
-      //System.out.println("VGR: next field=" + fieldInfo.name);
-      current = fstEnum.next();
-      if (current == null) {
-        //System.out.println("  eof");
-        return -1;
-      } else {
-        return current.output;
-      }
-    }
-
-    @Override
-    public long ord() {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public long seek(long ord) {
-      throw new UnsupportedOperationException();
-    }
-  }
-
-  @Override
-  public boolean supportsOrd() {
-    return false;
-  }
-
-  private final class FieldIndexData {
-
-    private final long indexStart;
-    // Set only if terms index is loaded:
-    private volatile FST<Long> fst;
-
-    public FieldIndexData(FieldInfo fieldInfo, long indexStart) throws IOException {
-      this.indexStart = indexStart;
-
-      if (indexDivisor > 0) {
-        loadTermsIndex();
-      }
-    }
-
-    private void loadTermsIndex() throws IOException {
-      if (fst == null) {
-        IndexInput clone = (IndexInput) in.clone();
-        clone.seek(indexStart);
-        fst = new FST<Long>(clone, fstOutputs);
-        clone.close();
-
-        /*
-        final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
-        Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
-        Util.toDot(fst, w, false, false);
-        System.out.println("FST INDEX: SAVED to " + dotFileName);
-        w.close();
-        */
-
-        if (indexDivisor > 1) {
-          // subsample
-          final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton(true);
-          final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
-          final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst);
-          BytesRefFSTEnum.InputOutput<Long> result;
-          int count = indexDivisor;
-          while((result = fstEnum.next()) != null) {
-            if (count == indexDivisor) {
-              builder.add(result.input, result.output);
-              count = 0;
-            }
-            count++;
-          }
-          fst = builder.finish();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
-    final FieldIndexData fieldData = fields.get(fieldInfo);
-    if (fieldData.fst == null) {
-      return null;
-    } else {
-      return new IndexEnum(fieldData.fst);
-    }
-  }
-
-  public static void files(Directory dir, SegmentInfo info, String segmentSuffix, Collection<String> files) {
-    files.add(IndexFileNames.segmentFileName(info.name, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION));
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (in != null && !indexLoaded) {
-      in.close();
-    }
-  }
-
-  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
-    input.seek(dirOffset);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/VariableGapTermsIndexWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/VariableGapTermsIndexWriter.java
deleted file mode 100644
index ba4959d..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/VariableGapTermsIndexWriter.java
+++ /dev/null
@@ -1,306 +0,0 @@
-package org.apache.lucene.index.codecs;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-
-/**
- * Selects index terms according to provided pluggable
- * IndexTermPolicy, and stores them in a prefix trie that's
- * loaded entirely in RAM stored as an FST.  This terms
- * index only supports unsigned byte term sort order
- * (unicode codepoint order when the bytes are UTF8).
- *
- * @lucene.experimental */
-public class VariableGapTermsIndexWriter extends TermsIndexWriterBase {
-  protected final IndexOutput out;
-
-  /** Extension of terms index file */
-  static final String TERMS_INDEX_EXTENSION = "tiv";
-
-  final static String CODEC_NAME = "VARIABLE_GAP_TERMS_INDEX";
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  private final List<FSTFieldWriter> fields = new ArrayList<FSTFieldWriter>();
-  private final FieldInfos fieldInfos; // unread
-  private final IndexTermSelector policy;
-
-  /** @lucene.experimental */
-  public static abstract class IndexTermSelector {
-    // Called sequentially on every term being written,
-    // returning true if this term should be indexed
-    public abstract boolean isIndexTerm(BytesRef term, TermStats stats);
-    public abstract void newField(FieldInfo fieldInfo);
-  }
-
-  /** Same policy as {@link FixedGapTermsIndexWriter} */
-  public static final class EveryNTermSelector extends IndexTermSelector {
-    private int count;
-    private final int interval;
-
-    public EveryNTermSelector(int interval) {
-      this.interval = interval;
-      // First term is first indexed term:
-      count = interval;
-    }
-
-    @Override
-    public boolean isIndexTerm(BytesRef term, TermStats stats) {
-      if (count >= interval) {
-        count = 1;
-        return true;
-      } else {
-        count++;
-        return false;
-      }
-    }
-
-    @Override
-    public void newField(FieldInfo fieldInfo) {
-      count = interval;
-    }
-  }
-
-  /** Sets an index term when docFreq >= docFreqThresh, or
-   *  every interval terms.  This should reduce seek time
-   *  to high docFreq terms.  */
-  public static final class EveryNOrDocFreqTermSelector extends IndexTermSelector {
-    private int count;
-    private final int docFreqThresh;
-    private final int interval;
-
-    public EveryNOrDocFreqTermSelector(int docFreqThresh, int interval) {
-      this.interval = interval;
-      this.docFreqThresh = docFreqThresh;
-
-      // First term is first indexed term:
-      count = interval;
-    }
-
-    @Override
-    public boolean isIndexTerm(BytesRef term, TermStats stats) {
-      if (stats.docFreq >= docFreqThresh || count >= interval) {
-        count = 1;
-        return true;
-      } else {
-        count++;
-        return false;
-      }
-    }
-
-    @Override
-    public void newField(FieldInfo fieldInfo) {
-      count = interval;
-    }
-  }
-
-  // TODO: it'd be nice to let the FST builder prune based
-  // on term count of each node (the prune1/prune2 that it
-  // accepts), and build the index based on that.  This
-  // should result in a more compact terms index, more like
-  // a prefix trie than the other selectors, because it
-  // only stores enough leading bytes to get down to N
-  // terms that may complete that prefix.  It becomes
-  // "deeper" when terms are dense, and "shallow" when they
-  // are less dense.
-  //
-  // However, it's not easy to make that work this this
-  // API, because that pruning doesn't immediately know on
-  // seeing each term whether that term will be a seek point
-  // or not.  It requires some non-causality in the API, ie
-  // only on seeing some number of future terms will the
-  // builder decide which past terms are seek points.
-  // Somehow the API'd need to be able to return a "I don't
-  // know" value, eg like a Future, which only later on is
-  // flipped (frozen) to true or false.
-  //
-  // We could solve this with a 2-pass approach, where the
-  // first pass would build an FSA (no outputs) solely to
-  // determine which prefixes are the 'leaves' in the
-  // pruning. The 2nd pass would then look at this prefix
-  // trie to mark the seek points and build the FST mapping
-  // to the true output.
-  //
-  // But, one downside to this approach is that it'd result
-  // in uneven index term selection.  EG with prune1=10, the
-  // resulting index terms could be as frequent as every 10
-  // terms or as rare as every <maxArcCount> * 10 (eg 2560),
-  // in the extremes.
-
-  public VariableGapTermsIndexWriter(SegmentWriteState state, IndexTermSelector policy) throws IOException {
-    final String indexFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, TERMS_INDEX_EXTENSION);
-    out = state.directory.createOutput(indexFileName, state.context);
-    boolean success = false;
-    try {
-      fieldInfos = state.fieldInfos;
-      this.policy = policy;
-      writeHeader(out);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
-    // Placeholder for dir offset
-    out.writeLong(0);
-  }
-
-  @Override
-  public FieldWriter addField(FieldInfo field, long termsFilePointer) throws IOException {
-    ////System.out.println("VGW: field=" + field.name);
-    policy.newField(field);
-    FSTFieldWriter writer = new FSTFieldWriter(field, termsFilePointer);
-    fields.add(writer);
-    return writer;
-  }
-
-  /** NOTE: if your codec does not sort in unicode code
-   *  point order, you must override this method, to simply
-   *  return indexedTerm.length. */
-  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
-    // As long as codec sorts terms in unicode codepoint
-    // order, we can safely strip off the non-distinguishing
-    // suffix to save RAM in the loaded terms index.
-    final int idxTermOffset = indexedTerm.offset;
-    final int priorTermOffset = priorTerm.offset;
-    final int limit = Math.min(priorTerm.length, indexedTerm.length);
-    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
-      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
-        return byteIdx+1;
-      }
-    }
-    return Math.min(1+priorTerm.length, indexedTerm.length);
-  }
-
-  private class FSTFieldWriter extends FieldWriter {
-    private final Builder<Long> fstBuilder;
-    private final PositiveIntOutputs fstOutputs;
-    private final long startTermsFilePointer;
-
-    final FieldInfo fieldInfo;
-    int numIndexTerms;
-    FST<Long> fst;
-    final long indexStart;
-
-    private final BytesRef lastTerm = new BytesRef();
-    private boolean first = true;
-
-    public FSTFieldWriter(FieldInfo fieldInfo, long termsFilePointer) throws IOException {
-      this.fieldInfo = fieldInfo;
-      fstOutputs = PositiveIntOutputs.getSingleton(true);
-      fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, fstOutputs);
-      indexStart = out.getFilePointer();
-      ////System.out.println("VGW: field=" + fieldInfo.name);
-
-      // Always put empty string in
-      fstBuilder.add(new BytesRef(), fstOutputs.get(termsFilePointer));
-      startTermsFilePointer = termsFilePointer;
-    }
-
-    @Override
-    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
-      //System.out.println("VGW: index term=" + text.utf8ToString());
-      // NOTE: we must force the first term per field to be
-      // indexed, in case policy doesn't:
-      if (policy.isIndexTerm(text, stats) || first) {
-        first = false;
-        //System.out.println("  YES");
-        return true;
-      } else {
-        lastTerm.copyBytes(text);
-        return false;
-      }
-    }
-
-    @Override
-    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
-      if (text.length == 0) {
-        // We already added empty string in ctor
-        assert termsFilePointer == startTermsFilePointer;
-        return;
-      }
-      final int lengthSave = text.length;
-      text.length = indexedTermPrefixLength(lastTerm, text);
-      try {
-        fstBuilder.add(text, fstOutputs.get(termsFilePointer));
-      } finally {
-        text.length = lengthSave;
-      }
-      lastTerm.copyBytes(text);
-    }
-
-    @Override
-    public void finish(long termsFilePointer) throws IOException {
-      fst = fstBuilder.finish();
-      if (fst != null) {
-        fst.save(out);
-      }
-    }
-  }
-
-  public void close() throws IOException {
-    try {
-    final long dirStart = out.getFilePointer();
-    final int fieldCount = fields.size();
-
-    int nonNullFieldCount = 0;
-    for(int i=0;i<fieldCount;i++) {
-      FSTFieldWriter field = fields.get(i);
-      if (field.fst != null) {
-        nonNullFieldCount++;
-      }
-    }
-
-    out.writeVInt(nonNullFieldCount);
-    for(int i=0;i<fieldCount;i++) {
-      FSTFieldWriter field = fields.get(i);
-      if (field.fst != null) {
-        out.writeVInt(field.fieldInfo.number);
-        out.writeVLong(field.indexStart);
-      }
-    }
-    writeTrailer(dirStart);
-    } finally {
-    out.close();
-  }
-  }
-
-  protected void writeTrailer(long dirStart) throws IOException {
-    out.seek(CodecUtil.headerLength(CODEC_NAME));
-    out.writeLong(dirStart);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingCodec.java b/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingCodec.java
deleted file mode 100644
index a821acd..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingCodec.java
+++ /dev/null
@@ -1,88 +0,0 @@
-package org.apache.lucene.index.codecs.appending;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.DocValuesFormat;
-import org.apache.lucene.index.codecs.FieldInfosFormat;
-import org.apache.lucene.index.codecs.NormsFormat;
-import org.apache.lucene.index.codecs.StoredFieldsFormat;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.SegmentInfosFormat;
-import org.apache.lucene.index.codecs.TermVectorsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40Codec;
-import org.apache.lucene.index.codecs.lucene40.Lucene40FieldInfosFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40DocValuesFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40NormsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40StoredFieldsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40TermVectorsFormat;
-
-/**
- * This codec extends {@link Lucene40Codec} to work on append-only outputs, such
- * as plain output streams and append-only filesystems.
- * 
- * @lucene.experimental
- */
-public class AppendingCodec extends Codec {
-  public AppendingCodec() {
-    super("Appending");
-  }
-
-  private final PostingsFormat postings = new AppendingPostingsFormat();
-  private final SegmentInfosFormat infos = new AppendingSegmentInfosFormat();
-  private final StoredFieldsFormat fields = new Lucene40StoredFieldsFormat();
-  private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat();
-  private final TermVectorsFormat vectors = new Lucene40TermVectorsFormat();
-  private final DocValuesFormat docValues = new Lucene40DocValuesFormat();
-  private final NormsFormat norms = new Lucene40NormsFormat();
-  
-  @Override
-  public PostingsFormat postingsFormat() {
-    return postings;
-  }
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fields;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectors;
-  }
-
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValues;
-  }
-
-  @Override
-  public SegmentInfosFormat segmentInfosFormat() {
-    return infos;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfos;
-  }
-  
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingPostingsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingPostingsFormat.java
deleted file mode 100644
index ff96c33..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingPostingsFormat.java
+++ /dev/null
@@ -1,91 +0,0 @@
-package org.apache.lucene.index.codecs.appending;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.BlockTreeTermsReader;
-import org.apache.lucene.index.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsReader;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsWriter;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.store.Directory;
-
-/**
- * Appending postings impl
- */
-class AppendingPostingsFormat extends PostingsFormat {
-  public static String CODEC_NAME = "Appending";
-  
-  public AppendingPostingsFormat() {
-    super(CODEC_NAME);
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docsWriter = new Lucene40PostingsWriter(state);
-    boolean success = false;
-    try {
-      FieldsConsumer ret = new AppendingTermsWriter(state, docsWriter, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        docsWriter.close();
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
-    
-    boolean success = false;
-    try {
-      FieldsProducer ret = new AppendingTermsReader(
-                                                    state.dir,
-                                                    state.fieldInfos,
-                                                    state.segmentInfo.name,
-                                                    postings,
-                                                    state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        postings.close();
-      }
-    }
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files)
-          throws IOException {
-    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
-    BlockTreeTermsReader.files(dir, segmentInfo, segmentSuffix, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosFormat.java
deleted file mode 100644
index e6fcb45..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosFormat.java
+++ /dev/null
@@ -1,30 +0,0 @@
-package org.apache.lucene.index.codecs.appending;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.codecs.SegmentInfosWriter;
-import org.apache.lucene.index.codecs.lucene40.Lucene40SegmentInfosFormat;
-
-public class AppendingSegmentInfosFormat extends Lucene40SegmentInfosFormat {
-  private final SegmentInfosWriter writer = new AppendingSegmentInfosWriter();
-
-  @Override
-  public SegmentInfosWriter getSegmentInfosWriter() {
-    return writer;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosWriter.java
deleted file mode 100644
index 0bc4c33..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingSegmentInfosWriter.java
+++ /dev/null
@@ -1,32 +0,0 @@
-package org.apache.lucene.index.codecs.appending;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.codecs.lucene40.Lucene40SegmentInfosWriter;
-import org.apache.lucene.store.IndexOutput;
-
-public class AppendingSegmentInfosWriter extends Lucene40SegmentInfosWriter {
-
-  @Override
-  public void prepareCommit(IndexOutput segmentOutput) throws IOException {
-    // noop
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingTermsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingTermsReader.java
deleted file mode 100644
index 2e6292b..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingTermsReader.java
+++ /dev/null
@@ -1,61 +0,0 @@
-package org.apache.lucene.index.codecs.appending;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.codecs.BlockTreeTermsReader;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.CodecUtil;
-
-/**
- * Reads append-only terms from {@link AppendingTermsWriter}
- * @lucene.experimental
- */
-public class AppendingTermsReader extends BlockTreeTermsReader {
-
-  public AppendingTermsReader(Directory dir, FieldInfos fieldInfos, String segment, PostingsReaderBase postingsReader, 
-      IOContext ioContext, String segmentSuffix, int indexDivisor) throws IOException {
-    super(dir, fieldInfos, segment, postingsReader, ioContext, segmentSuffix, indexDivisor);
-  }
-
-  @Override
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, AppendingTermsWriter.TERMS_CODEC_NAME,
-        AppendingTermsWriter.TERMS_VERSION_START,
-        AppendingTermsWriter.TERMS_VERSION_CURRENT);  
-  }
-
-  @Override
-  protected void readIndexHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, AppendingTermsWriter.TERMS_INDEX_CODEC_NAME,
-        AppendingTermsWriter.TERMS_INDEX_VERSION_START,
-        AppendingTermsWriter.TERMS_INDEX_VERSION_CURRENT);
-  }
-  
-  @Override
-  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
-    input.seek(input.length() - Long.SIZE / 8);
-    long offset = input.readLong();
-    input.seek(offset);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingTermsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingTermsWriter.java
deleted file mode 100644
index 409e527..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/appending/AppendingTermsWriter.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.apache.lucene.index.codecs.appending;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.CodecUtil;
-
-/**
- * Append-only version of {@link BlockTreeTermsWriter}
- * @lucene.experimental
- */
-public class AppendingTermsWriter extends BlockTreeTermsWriter {
-  final static String TERMS_CODEC_NAME = "APPENDING_TERMS_DICT";
-  final static int TERMS_VERSION_START = 0;
-  final static int TERMS_VERSION_CURRENT = TERMS_VERSION_START;
-  
-  final static String TERMS_INDEX_CODEC_NAME = "APPENDING_TERMS_INDEX";
-  final static int TERMS_INDEX_VERSION_START = 0;
-  final static int TERMS_INDEX_VERSION_CURRENT = TERMS_INDEX_VERSION_START;
-  
-  public AppendingTermsWriter(SegmentWriteState state, PostingsWriterBase postingsWriter, int minItemsInBlock, int maxItemsInBlock) throws IOException {
-    super(state, postingsWriter, minItemsInBlock, maxItemsInBlock);
-  }
-
-  @Override
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, TERMS_CODEC_NAME, TERMS_VERSION_CURRENT);
-  }
-
-  @Override
-  protected void writeIndexHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, TERMS_INDEX_CODEC_NAME, TERMS_INDEX_VERSION_CURRENT);
-  }
-
-  @Override
-  protected void writeTrailer(IndexOutput out, long dirStart) throws IOException {
-    out.writeLong(dirStart);
-  }
-
-  @Override
-  protected void writeIndexTrailer(IndexOutput indexOut, long dirStart) throws IOException {
-    indexOut.writeLong(dirStart);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/intblock/FixedIntBlockIndexInput.java b/lucene/src/java/org/apache/lucene/index/codecs/intblock/FixedIntBlockIndexInput.java
deleted file mode 100644
index 0d3ef2e..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/intblock/FixedIntBlockIndexInput.java
+++ /dev/null
@@ -1,196 +0,0 @@
-package org.apache.lucene.index.codecs.intblock;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Naive int block API that writes vInts.  This is
- *  expected to give poor performance; it's really only for
- *  testing the pluggability.  One should typically use pfor instead. */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.codecs.sep.IntIndexInput;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IntsRef;
-
-/** Abstract base class that reads fixed-size blocks of ints
- *  from an IndexInput.  While this is a simple approach, a
- *  more performant approach would directly create an impl
- *  of IntIndexInput inside Directory.  Wrapping a generic
- *  IndexInput will likely cost performance.
- *
- * @lucene.experimental
- */
-public abstract class FixedIntBlockIndexInput extends IntIndexInput {
-
-  private final IndexInput in;
-  protected final int blockSize;
-  
-  public FixedIntBlockIndexInput(final IndexInput in) throws IOException {
-    this.in = in;
-    blockSize = in.readVInt();
-  }
-
-  @Override
-  public Reader reader() throws IOException {
-    final int[] buffer = new int[blockSize];
-    final IndexInput clone = (IndexInput) in.clone();
-    // TODO: can this be simplified?
-    return new Reader(clone, buffer, this.getBlockReader(clone, buffer));
-  }
-
-  @Override
-  public void close() throws IOException {
-    in.close();
-  }
-
-  @Override
-  public Index index() {
-    return new Index();
-  }
-
-  protected abstract BlockReader getBlockReader(IndexInput in, int[] buffer) throws IOException;
-
-  public interface BlockReader {
-    public void readBlock() throws IOException;
-  }
-
-  private static class Reader extends IntIndexInput.Reader {
-    private final IndexInput in;
-
-    protected final int[] pending;
-    int upto;
-
-    private boolean seekPending;
-    private long pendingFP;
-    private int pendingUpto;
-    private long lastBlockFP;
-    private final BlockReader blockReader;
-    private final int blockSize;
-    private final IntsRef bulkResult = new IntsRef();
-
-    public Reader(final IndexInput in, final int[] pending, final BlockReader blockReader)
-    throws IOException {
-      this.in = in;
-      this.pending = pending;
-      this.blockSize = pending.length;
-      bulkResult.ints = pending;
-      this.blockReader = blockReader;
-      upto = blockSize;
-    }
-
-    void seek(final long fp, final int upto) {
-      pendingFP = fp;
-      pendingUpto = upto;
-      seekPending = true;
-    }
-
-    private void maybeSeek() throws IOException {
-      if (seekPending) {
-        if (pendingFP != lastBlockFP) {
-          // need new block
-          in.seek(pendingFP);
-          lastBlockFP = pendingFP;
-          blockReader.readBlock();
-        }
-        upto = pendingUpto;
-        seekPending = false;
-      }
-    }
-
-    @Override
-    public int next() throws IOException {
-      this.maybeSeek();
-      if (upto == blockSize) {
-        lastBlockFP = in.getFilePointer();
-        blockReader.readBlock();
-        upto = 0;
-      }
-
-      return pending[upto++];
-    }
-
-    @Override
-    public IntsRef read(final int count) throws IOException {
-      this.maybeSeek();
-      if (upto == blockSize) {
-        blockReader.readBlock();
-        upto = 0;
-      }
-      bulkResult.offset = upto;
-      if (upto + count < blockSize) {
-        bulkResult.length = count;
-        upto += count;
-      } else {
-        bulkResult.length = blockSize - upto;
-        upto = blockSize;
-      }
-
-      return bulkResult;
-    }
-  }
-
-  private class Index extends IntIndexInput.Index {
-    private long fp;
-    private int upto;
-
-    @Override
-    public void read(final DataInput indexIn, final boolean absolute) throws IOException {
-      if (absolute) {
-        upto = indexIn.readVInt();
-        fp = indexIn.readVLong();
-      } else {
-        final int uptoDelta = indexIn.readVInt();
-        if ((uptoDelta & 1) == 1) {
-          // same block
-          upto += uptoDelta >>> 1;
-        } else {
-          // new block
-          upto = uptoDelta >>> 1;
-          fp += indexIn.readVLong();
-        }
-      }
-      assert upto < blockSize;
-    }
-
-    @Override
-    public void seek(final IntIndexInput.Reader other) throws IOException {
-      ((Reader) other).seek(fp, upto);
-    }
-
-    @Override
-    public void set(final IntIndexInput.Index other) {
-      final Index idx = (Index) other;
-      fp = idx.fp;
-      upto = idx.upto;
-    }
-
-    @Override
-    public Object clone() {
-      Index other = new Index();
-      other.fp = fp;
-      other.upto = upto;
-      return other;
-    }
-    
-    @Override
-    public String toString() {
-      return "fp=" + fp + " upto=" + upto;
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/intblock/FixedIntBlockIndexOutput.java b/lucene/src/java/org/apache/lucene/index/codecs/intblock/FixedIntBlockIndexOutput.java
deleted file mode 100644
index 4361908..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/intblock/FixedIntBlockIndexOutput.java
+++ /dev/null
@@ -1,127 +0,0 @@
-package org.apache.lucene.index.codecs.intblock;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Naive int block API that writes vInts.  This is
- *  expected to give poor performance; it's really only for
- *  testing the pluggability.  One should typically use pfor instead. */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.codecs.sep.IntIndexOutput;
-import org.apache.lucene.store.IndexOutput;
-
-/** Abstract base class that writes fixed-size blocks of ints
- *  to an IndexOutput.  While this is a simple approach, a
- *  more performant approach would directly create an impl
- *  of IntIndexOutput inside Directory.  Wrapping a generic
- *  IndexInput will likely cost performance.
- *
- * @lucene.experimental
- */
-public abstract class FixedIntBlockIndexOutput extends IntIndexOutput {
-
-  protected final IndexOutput out;
-  private final int blockSize;
-  protected final int[] buffer;
-  private int upto;
-
-  protected FixedIntBlockIndexOutput(IndexOutput out, int fixedBlockSize) throws IOException {
-    blockSize = fixedBlockSize;
-    this.out = out;
-    out.writeVInt(blockSize);
-    buffer = new int[blockSize];
-  }
-
-  protected abstract void flushBlock() throws IOException;
-
-  @Override
-  public Index index() throws IOException {
-    return new Index();
-  }
-
-  private class Index extends IntIndexOutput.Index {
-    long fp;
-    int upto;
-    long lastFP;
-    int lastUpto;
-
-    @Override
-    public void mark() throws IOException {
-      fp = out.getFilePointer();
-      upto = FixedIntBlockIndexOutput.this.upto;
-    }
-
-    @Override
-    public void copyFrom(IntIndexOutput.Index other, boolean copyLast) throws IOException {
-      Index idx = (Index) other;
-      fp = idx.fp;
-      upto = idx.upto;
-      if (copyLast) {
-        lastFP = fp;
-        lastUpto = upto;
-      }
-    }
-
-    @Override
-    public void write(IndexOutput indexOut, boolean absolute) throws IOException {
-      if (absolute) {
-        indexOut.writeVInt(upto);
-        indexOut.writeVLong(fp);
-      } else if (fp == lastFP) {
-        // same block
-        assert upto >= lastUpto;
-        int uptoDelta = upto - lastUpto;
-        indexOut.writeVInt(uptoDelta << 1 | 1);
-      } else {      
-        // new block
-        indexOut.writeVInt(upto << 1);
-        indexOut.writeVLong(fp - lastFP);
-      }
-      lastUpto = upto;
-      lastFP = fp;
-    }
-
-    @Override
-    public String toString() {
-      return "fp=" + fp + " upto=" + upto;
-    }
-  }
-
-  @Override
-  public void write(int v) throws IOException {
-    buffer[upto++] = v;
-    if (upto == blockSize) {
-      flushBlock();
-      upto = 0;
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      if (upto > 0) {
-        // NOTE: entries in the block after current upto are
-        // invalid
-        flushBlock();
-      }
-    } finally {
-      out.close();
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/intblock/VariableIntBlockIndexInput.java b/lucene/src/java/org/apache/lucene/index/codecs/intblock/VariableIntBlockIndexInput.java
deleted file mode 100644
index d4b7fcb..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/intblock/VariableIntBlockIndexInput.java
+++ /dev/null
@@ -1,217 +0,0 @@
-package org.apache.lucene.index.codecs.intblock;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Naive int block API that writes vInts.  This is
- *  expected to give poor performance; it's really only for
- *  testing the pluggability.  One should typically use pfor instead. */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.codecs.sep.IntIndexInput;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IntsRef;
-
-// TODO: much of this can be shared code w/ the fixed case
-
-/** Abstract base class that reads variable-size blocks of ints
- *  from an IndexInput.  While this is a simple approach, a
- *  more performant approach would directly create an impl
- *  of IntIndexInput inside Directory.  Wrapping a generic
- *  IndexInput will likely cost performance.
- *
- * @lucene.experimental
- */
-public abstract class VariableIntBlockIndexInput extends IntIndexInput {
-
-  protected final IndexInput in;
-  protected final int maxBlockSize;
-
-  protected VariableIntBlockIndexInput(final IndexInput in) throws IOException {
-    this.in = in;
-    maxBlockSize = in.readInt();
-  }
-
-  @Override
-  public Reader reader() throws IOException {
-    final int[] buffer = new int[maxBlockSize];
-    final IndexInput clone = (IndexInput) in.clone();
-    // TODO: can this be simplified?
-    return new Reader(clone, buffer, this.getBlockReader(clone, buffer));
-  }
-
-  @Override
-  public void close() throws IOException {
-    in.close();
-  }
-
-  @Override
-  public Index index() {
-    return new Index();
-  }
-
-  protected abstract BlockReader getBlockReader(IndexInput in, int[] buffer) throws IOException;
-
-  public interface BlockReader {
-    public int readBlock() throws IOException;
-    public void seek(long pos) throws IOException;
-  }
-
-  public static class Reader extends IntIndexInput.Reader {
-    private final IndexInput in;
-
-    public final int[] pending;
-    int upto;
-
-    private boolean seekPending;
-    private long pendingFP;
-    private int pendingUpto;
-    private long lastBlockFP;
-    private int blockSize;
-    private final BlockReader blockReader;
-    private final IntsRef bulkResult = new IntsRef();
-
-    public Reader(final IndexInput in, final int[] pending, final BlockReader blockReader)
-      throws IOException {
-      this.in = in;
-      this.pending = pending;
-      bulkResult.ints = pending;
-      this.blockReader = blockReader;
-    }
-
-    void seek(final long fp, final int upto) throws IOException {
-      // TODO: should we do this in real-time, not lazy?
-      pendingFP = fp;
-      pendingUpto = upto;
-      assert pendingUpto >= 0: "pendingUpto=" + pendingUpto;
-      seekPending = true;
-    }
-
-    private final void maybeSeek() throws IOException {
-      if (seekPending) {
-        if (pendingFP != lastBlockFP) {
-          // need new block
-          in.seek(pendingFP);
-          blockReader.seek(pendingFP);
-          lastBlockFP = pendingFP;
-          blockSize = blockReader.readBlock();
-        }
-        upto = pendingUpto;
-
-        // TODO: if we were more clever when writing the
-        // index, such that a seek point wouldn't be written
-        // until the int encoder "committed", we could avoid
-        // this (likely minor) inefficiency:
-
-        // This is necessary for int encoders that are
-        // non-causal, ie must see future int values to
-        // encode the current ones.
-        while(upto >= blockSize) {
-          upto -= blockSize;
-          lastBlockFP = in.getFilePointer();
-          blockSize = blockReader.readBlock();
-        }
-        seekPending = false;
-      }
-    }
-
-    @Override
-    public int next() throws IOException {
-      this.maybeSeek();
-      if (upto == blockSize) {
-        lastBlockFP = in.getFilePointer();
-        blockSize = blockReader.readBlock();
-        upto = 0;
-      }
-
-      return pending[upto++];
-    }
-
-    @Override
-    public IntsRef read(final int count) throws IOException {
-      this.maybeSeek();
-      if (upto == blockSize) {
-        lastBlockFP = in.getFilePointer();
-        blockSize = blockReader.readBlock();
-        upto = 0;
-      }
-      bulkResult.offset = upto;
-      if (upto + count < blockSize) {
-        bulkResult.length = count;
-        upto += count;
-      } else {
-        bulkResult.length = blockSize - upto;
-        upto = blockSize;
-      }
-
-      return bulkResult;
-    }
-  }
-
-  private class Index extends IntIndexInput.Index {
-    private long fp;
-    private int upto;
-
-    @Override
-    public void read(final DataInput indexIn, final boolean absolute) throws IOException {
-      if (absolute) {
-        upto = indexIn.readVInt();
-        fp = indexIn.readVLong();
-      } else {
-        final int uptoDelta = indexIn.readVInt();
-        if ((uptoDelta & 1) == 1) {
-          // same block
-          upto += uptoDelta >>> 1;
-        } else {
-          // new block
-          upto = uptoDelta >>> 1;
-          fp += indexIn.readVLong();
-        }
-      }
-      // TODO: we can't do this assert because non-causal
-      // int encoders can have upto over the buffer size
-      //assert upto < maxBlockSize: "upto=" + upto + " max=" + maxBlockSize;
-    }
-
-    @Override
-    public String toString() {
-      return "VarIntBlock.Index fp=" + fp + " upto=" + upto + " maxBlock=" + maxBlockSize;
-    }
-
-    @Override
-    public void seek(final IntIndexInput.Reader other) throws IOException {
-      ((Reader) other).seek(fp, upto);
-    }
-
-    @Override
-    public void set(final IntIndexInput.Index other) {
-      final Index idx = (Index) other;
-      fp = idx.fp;
-      upto = idx.upto;
-    }
-
-    @Override
-    public Object clone() {
-      Index other = new Index();
-      other.fp = fp;
-      other.upto = upto;
-      return other;
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/intblock/VariableIntBlockIndexOutput.java b/lucene/src/java/org/apache/lucene/index/codecs/intblock/VariableIntBlockIndexOutput.java
deleted file mode 100644
index da48ad1..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/intblock/VariableIntBlockIndexOutput.java
+++ /dev/null
@@ -1,135 +0,0 @@
-package org.apache.lucene.index.codecs.intblock;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Naive int block API that writes vInts.  This is
- *  expected to give poor performance; it's really only for
- *  testing the pluggability.  One should typically use pfor instead. */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.codecs.sep.IntIndexOutput;
-import org.apache.lucene.store.IndexOutput;
-
-// TODO: much of this can be shared code w/ the fixed case
-
-/** Abstract base class that writes variable-size blocks of ints
- *  to an IndexOutput.  While this is a simple approach, a
- *  more performant approach would directly create an impl
- *  of IntIndexOutput inside Directory.  Wrapping a generic
- *  IndexInput will likely cost performance.
- *
- * @lucene.experimental
- */
-public abstract class VariableIntBlockIndexOutput extends IntIndexOutput {
-
-  protected final IndexOutput out;
-
-  private int upto;
-  private boolean hitExcDuringWrite;
-
-  // TODO what Var-Var codecs exist in practice... and what are there blocksizes like?
-  // if its less than 128 we should set that as max and use byte?
-
-  /** NOTE: maxBlockSize must be the maximum block size 
-   *  plus the max non-causal lookahead of your codec.  EG Simple9
-   *  requires lookahead=1 because on seeing the Nth value
-   *  it knows it must now encode the N-1 values before it. */
-  protected VariableIntBlockIndexOutput(IndexOutput out, int maxBlockSize) throws IOException {
-    this.out = out;
-    out.writeInt(maxBlockSize);
-  }
-
-  /** Called one value at a time.  Return the number of
-   *  buffered input values that have been written to out. */
-  protected abstract int add(int value) throws IOException;
-
-  @Override
-  public Index index() throws IOException {
-    return new Index();
-  }
-
-  private class Index extends IntIndexOutput.Index {
-    long fp;
-    int upto;
-    long lastFP;
-    int lastUpto;
-
-    @Override
-    public void mark() throws IOException {
-      fp = out.getFilePointer();
-      upto = VariableIntBlockIndexOutput.this.upto;
-    }
-
-    @Override
-    public void copyFrom(IntIndexOutput.Index other, boolean copyLast) throws IOException {
-      Index idx = (Index) other;
-      fp = idx.fp;
-      upto = idx.upto;
-      if (copyLast) {
-        lastFP = fp;
-        lastUpto = upto;
-      }
-    }
-
-    @Override
-    public void write(IndexOutput indexOut, boolean absolute) throws IOException {
-      assert upto >= 0;
-      if (absolute) {
-        indexOut.writeVInt(upto);
-        indexOut.writeVLong(fp);
-      } else if (fp == lastFP) {
-        // same block
-        assert upto >= lastUpto;
-        int uptoDelta = upto - lastUpto;
-        indexOut.writeVInt(uptoDelta << 1 | 1);
-      } else {      
-        // new block
-        indexOut.writeVInt(upto << 1);
-        indexOut.writeVLong(fp - lastFP);
-      }
-      lastUpto = upto;
-      lastFP = fp;
-    }
-  }
-
-  @Override
-  public void write(int v) throws IOException {
-    hitExcDuringWrite = true;
-    upto -= add(v)-1;
-    hitExcDuringWrite = false;
-    assert upto >= 0;
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      if (!hitExcDuringWrite) {
-        // stuff 0s in until the "real" data is flushed:
-        int stuffed = 0;
-        while(upto > stuffed) {
-          upto -= add(0)-1;
-          assert upto >= 0;
-          stuffed += 1;
-        }
-      }
-    } finally {
-      out.close();
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/intblock/package.html b/lucene/src/java/org/apache/lucene/index/codecs/intblock/package.html
deleted file mode 100644
index 403ea1b..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/intblock/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Intblock: base support for fixed or variable length block integer encoders
-</body>
-</html>
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xCodec.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xCodec.java
deleted file mode 100644
index 4ad0b8b..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xCodec.java
+++ /dev/null
@@ -1,120 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.DocValuesFormat;
-import org.apache.lucene.index.codecs.FieldInfosFormat;
-import org.apache.lucene.index.codecs.NormsFormat;
-import org.apache.lucene.index.codecs.PerDocProducer;
-import org.apache.lucene.index.codecs.StoredFieldsFormat;
-import org.apache.lucene.index.codecs.PerDocConsumer;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.SegmentInfosFormat;
-import org.apache.lucene.index.codecs.TermVectorsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40FieldInfosFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40NormsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40SegmentInfosFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40StoredFieldsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40TermVectorsFormat;
-import org.apache.lucene.store.Directory;
-
-/**
- * Supports the Lucene 3.x index format (readonly)
- */
-public class Lucene3xCodec extends Codec {
-  public Lucene3xCodec() {
-    super("Lucene3x");
-  }
-
-  private final PostingsFormat postingsFormat = new Lucene3xPostingsFormat();
-  
-  // TODO: this should really be a different impl
-  private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat();
-  
-  // TODO: this should really be a different impl
-  private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
-  
-  // TODO: this should really be a different impl
-  private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
-
-  // TODO: this should really be a different impl
-  // also if we want preflex to *really* be read-only it should throw exception for the writer?
-  // this way IR.commit fails on delete/undelete/setNorm/etc ?
-  private final SegmentInfosFormat infosFormat = new Lucene40SegmentInfosFormat();
-  
-  // TODO: this should really be a different impl
-  private final NormsFormat normsFormat = new Lucene40NormsFormat();
-  
-  // 3.x doesn't support docvalues
-  private final DocValuesFormat docValuesFormat = new DocValuesFormat() {
-    @Override
-    public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-      return null;
-    }
-
-    @Override
-    public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
-      return null;
-    }
-
-    @Override
-    public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {}
-  };
-  
-  @Override
-  public PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-
-  @Override
-  public SegmentInfosFormat segmentInfosFormat() {
-    return infosFormat;
-  }
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xFields.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xFields.java
deleted file mode 100644
index 2140db5..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xFields.java
+++ /dev/null
@@ -1,1109 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.UnicodeUtil;
-
-/** Exposes flex API on a pre-flex index, as a codec. 
- * @lucene.experimental
- * @deprecated (4.0)
- */
-@Deprecated
-public class Lucene3xFields extends FieldsProducer {
-  
-  private static final boolean DEBUG_SURROGATES = false;
-
-  public TermInfosReader tis;
-  public final TermInfosReader tisNoIndex;
-
-  public final IndexInput freqStream;
-  public final IndexInput proxStream;
-  final private FieldInfos fieldInfos;
-  private final SegmentInfo si;
-  final TreeMap<String,FieldInfo> fields = new TreeMap<String,FieldInfo>();
-  final Map<String,Terms> preTerms = new HashMap<String,Terms>();
-  private final Directory dir;
-  private final IOContext context;
-  private Directory cfsReader;
-
-  public Lucene3xFields(Directory dir, FieldInfos fieldInfos, SegmentInfo info, IOContext context, int indexDivisor)
-    throws IOException {
-
-    si = info;
-
-    // NOTE: we must always load terms index, even for
-    // "sequential" scan during merging, because what is
-    // sequential to merger may not be to TermInfosReader
-    // since we do the surrogates dance:
-    if (indexDivisor < 0) {
-      indexDivisor = -indexDivisor;
-    }
-    
-    boolean success = false;
-    try {
-      TermInfosReader r = new TermInfosReader(dir, info.name, fieldInfos, context, indexDivisor);    
-      if (indexDivisor == -1) {
-        tisNoIndex = r;
-      } else {
-        tisNoIndex = null;
-        tis = r;
-      }
-      this.context = context;
-      this.fieldInfos = fieldInfos;
-
-      // make sure that all index files have been read or are kept open
-      // so that if an index update removes them we'll still have them
-      freqStream = dir.openInput(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.FREQ_EXTENSION), context);
-      boolean anyProx = false;
-      for (FieldInfo fi : fieldInfos) {
-        if (fi.isIndexed) {
-          fields.put(fi.name, fi);
-          preTerms.put(fi.name, new PreTerms(fi));
-          if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-            anyProx = true;
-          }
-        }
-      }
-
-      if (anyProx) {
-        proxStream = dir.openInput(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.PROX_EXTENSION), context);
-      } else {
-        proxStream = null;
-      }
-      success = true;
-    } finally {
-      // With lock-less commits, it's entirely possible (and
-      // fine) to hit a FileNotFound exception above. In
-      // this case, we want to explicitly close any subset
-      // of things that were opened so that we don't have to
-      // wait for a GC to do so.
-      if (!success) {
-        close();
-      }
-    }
-    this.dir = dir;
-  }
-
-  // If this returns, we do the surrogates dance so that the
-  // terms are sorted by unicode sort order.  This should be
-  // true when segments are used for "normal" searching;
-  // it's only false during testing, to create a pre-flex
-  // index, using the test-only PreFlexRW.
-  protected boolean sortTermsByUnicode() {
-    return true;
-  }
-
-  static void files(Directory dir, SegmentInfo info, Collection<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.TERMS_EXTENSION));
-    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION));
-    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.FREQ_EXTENSION));
-    if (info.getHasProx()) {
-      // LUCENE-1739: for certain versions of 2.9-dev,
-      // hasProx would be incorrectly computed during
-      // indexing as true, and then stored into the segments
-      // file, when it should have been false.  So we do the
-      // extra check, here:
-      final String prx = IndexFileNames.segmentFileName(info.name, "", Lucene3xPostingsFormat.PROX_EXTENSION);
-      if (dir.fileExists(prx)) {
-        files.add(prx);
-      }
-    }
-  }
-
-  @Override
-  public FieldsEnum iterator() throws IOException {
-    return new PreFlexFieldsEnum();
-  }
-
-  @Override
-  public Terms terms(String field) {
-    return preTerms.get(field);
-  }
-
-  @Override
-  public int getUniqueFieldCount() {
-    return preTerms.size();
-  }
-
-  @Override
-  public long getUniqueTermCount() throws IOException {
-    return getTermsDict().size();
-  }
-
-  synchronized private TermInfosReader getTermsDict() {
-    if (tis != null) {
-      return tis;
-    } else {
-      return tisNoIndex;
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (tis != null) {
-      tis.close();
-    }
-    if (tisNoIndex != null) {
-      tisNoIndex.close();
-    }
-    if (cfsReader != null) {
-      cfsReader.close();
-    }
-    if (freqStream != null) {
-      freqStream.close();
-    }
-    if (proxStream != null) {
-      proxStream.close();
-    }
-  }
-
-  private class PreFlexFieldsEnum extends FieldsEnum {
-    final Iterator<FieldInfo> it;
-    FieldInfo current;
-
-    public PreFlexFieldsEnum() throws IOException {
-      it = fields.values().iterator();
-    }
-
-    @Override
-    public String next() {
-      if (it.hasNext()) {
-        current = it.next();
-        return current.name;
-      } else {
-        return null;
-      }
-    }
-
-    @Override
-    public Terms terms() throws IOException {
-      return Lucene3xFields.this.terms(current.name);
-    }
-  }
-  
-  private class PreTerms extends Terms {
-    final FieldInfo fieldInfo;
-    PreTerms(FieldInfo fieldInfo) {
-      this.fieldInfo = fieldInfo;
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {    
-      PreTermsEnum termsEnum = new PreTermsEnum();
-      termsEnum.reset(fieldInfo);
-      return termsEnum;
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      // Pre-flex indexes always sorted in UTF16 order, but
-      // we remap on-the-fly to unicode order
-      if (sortTermsByUnicode()) {
-        return BytesRef.getUTF8SortedAsUnicodeComparator();
-      } else {
-        return BytesRef.getUTF8SortedAsUTF16Comparator();
-      }
-    }
-
-    @Override
-    public long getUniqueTermCount() throws IOException {
-      return -1;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return -1;
-    }
-
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return -1;
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return -1;
-    }
-  }
-
-  private class PreTermsEnum extends TermsEnum {
-    private SegmentTermEnum termEnum;
-    private FieldInfo fieldInfo;
-    private String internedFieldName;
-    private boolean skipNext;
-    private BytesRef current;
-
-    private SegmentTermEnum seekTermEnum;
-    
-    private static final byte UTF8_NON_BMP_LEAD = (byte) 0xf0;
-    private static final byte UTF8_HIGH_BMP_LEAD = (byte) 0xee;
-
-    // Returns true if the unicode char is "after" the
-    // surrogates in UTF16, ie >= U+E000 and <= U+FFFF:
-    private final boolean isHighBMPChar(byte[] b, int idx) {
-      return (b[idx] & UTF8_HIGH_BMP_LEAD) == UTF8_HIGH_BMP_LEAD;
-    }
-
-    // Returns true if the unicode char in the UTF8 byte
-    // sequence starting at idx encodes a char outside of
-    // BMP (ie what would be a surrogate pair in UTF16):
-    private final boolean isNonBMPChar(byte[] b, int idx) {
-      return (b[idx] & UTF8_NON_BMP_LEAD) == UTF8_NON_BMP_LEAD;
-    }
-
-    private final byte[] scratch = new byte[4];
-    private final BytesRef prevTerm = new BytesRef();
-    private final BytesRef scratchTerm = new BytesRef();
-    private int newSuffixStart;
-
-    // Swap in S, in place of E:
-    private boolean seekToNonBMP(SegmentTermEnum te, BytesRef term, int pos) throws IOException {
-      final int savLength = term.length;
-
-      assert term.offset == 0;
-
-      // The 3 bytes starting at downTo make up 1
-      // unicode character:
-      assert isHighBMPChar(term.bytes, pos);
-
-      // NOTE: we cannot make this assert, because
-      // AutomatonQuery legitimately sends us malformed UTF8
-      // (eg the UTF8 bytes with just 0xee)
-      // assert term.length >= pos + 3: "term.length=" + term.length + " pos+3=" + (pos+3) + " byte=" + Integer.toHexString(term.bytes[pos]) + " term=" + term.toString();
-
-      // Save the bytes && length, since we need to
-      // restore this if seek "back" finds no matching
-      // terms
-      if (term.bytes.length < 4+pos) {
-        term.grow(4+pos);
-      }
-
-      scratch[0] = term.bytes[pos];
-      scratch[1] = term.bytes[pos+1];
-      scratch[2] = term.bytes[pos+2];
-
-      term.bytes[pos] = (byte) 0xf0;
-      term.bytes[pos+1] = (byte) 0x90;
-      term.bytes[pos+2] = (byte) 0x80;
-      term.bytes[pos+3] = (byte) 0x80;
-      term.length = 4+pos;
-
-      if (DEBUG_SURROGATES) {
-        System.out.println("      try seek term=" + UnicodeUtil.toHexString(term.utf8ToString()));
-      }
-
-      // Seek "back":
-      getTermsDict().seekEnum(te, new Term(fieldInfo.name, term), true);
-
-      // Test if the term we seek'd to in fact found a
-      // surrogate pair at the same position as the E:
-      Term t2 = te.term();
-
-      // Cannot be null (or move to next field) because at
-      // "worst" it'd seek to the same term we are on now,
-      // unless we are being called from seek
-      if (t2 == null || t2.field() != internedFieldName) {
-        return false;
-      }
-
-      if (DEBUG_SURROGATES) {
-        System.out.println("      got term=" + UnicodeUtil.toHexString(t2.text()));
-      }
-
-      // Now test if prefix is identical and we found
-      // a non-BMP char at the same position:
-      BytesRef b2 = t2.bytes();
-      assert b2.offset == 0;
-
-      boolean matches;
-      if (b2.length >= term.length && isNonBMPChar(b2.bytes, pos)) {
-        matches = true;
-        for(int i=0;i<pos;i++) {
-          if (term.bytes[i] != b2.bytes[i]) {
-            matches = false;
-            break;
-          }
-        }              
-      } else {
-        matches = false;
-      }
-
-      // Restore term:
-      term.length = savLength;
-      term.bytes[pos] = scratch[0];
-      term.bytes[pos+1] = scratch[1];
-      term.bytes[pos+2] = scratch[2];
-
-      return matches;
-    }
-
-    // Seek type 2 "continue" (back to the start of the
-    // surrogates): scan the stripped suffix from the
-    // prior term, backwards. If there was an E in that
-    // part, then we try to seek back to S.  If that
-    // seek finds a matching term, we go there.
-    private boolean doContinue() throws IOException {
-
-      if (DEBUG_SURROGATES) {
-        System.out.println("  try cont");
-      }
-
-      int downTo = prevTerm.length-1;
-
-      boolean didSeek = false;
-      
-      final int limit = Math.min(newSuffixStart, scratchTerm.length-1);
-
-      while(downTo > limit) {
-
-        if (isHighBMPChar(prevTerm.bytes, downTo)) {
-
-          if (DEBUG_SURROGATES) {
-            System.out.println("    found E pos=" + downTo + " vs len=" + prevTerm.length);
-          }
-
-          if (seekToNonBMP(seekTermEnum, prevTerm, downTo)) {
-            // TODO: more efficient seek?
-            getTermsDict().seekEnum(termEnum, seekTermEnum.term(), true);
-            //newSuffixStart = downTo+4;
-            newSuffixStart = downTo;
-            scratchTerm.copyBytes(termEnum.term().bytes());
-            didSeek = true;
-            if (DEBUG_SURROGATES) {
-              System.out.println("      seek!");
-            }
-            break;
-          } else {
-            if (DEBUG_SURROGATES) {
-              System.out.println("      no seek");
-            }
-          }
-        }
-
-        // Shorten prevTerm in place so that we don't redo
-        // this loop if we come back here:
-        if ((prevTerm.bytes[downTo] & 0xc0) == 0xc0 || (prevTerm.bytes[downTo] & 0x80) == 0) {
-          prevTerm.length = downTo;
-        }
-        
-        downTo--;
-      }
-
-      return didSeek;
-    }
-
-    // Look for seek type 3 ("pop"): if the delta from
-    // prev -> current was replacing an S with an E,
-    // we must now seek to beyond that E.  This seek
-    // "finishes" the dance at this character
-    // position.
-    private boolean doPop() throws IOException {
-
-      if (DEBUG_SURROGATES) {
-        System.out.println("  try pop");
-      }
-
-      assert newSuffixStart <= prevTerm.length;
-      assert newSuffixStart < scratchTerm.length || newSuffixStart == 0;
-
-      if (prevTerm.length > newSuffixStart &&
-          isNonBMPChar(prevTerm.bytes, newSuffixStart) &&
-          isHighBMPChar(scratchTerm.bytes, newSuffixStart)) {
-
-        // Seek type 2 -- put 0xFF at this position:
-        scratchTerm.bytes[newSuffixStart] = (byte) 0xff;
-        scratchTerm.length = newSuffixStart+1;
-
-        if (DEBUG_SURROGATES) {
-          System.out.println("    seek to term=" + UnicodeUtil.toHexString(scratchTerm.utf8ToString()) + " " + scratchTerm.toString());
-        }
-          
-        // TODO: more efficient seek?  can we simply swap
-        // the enums?
-        getTermsDict().seekEnum(termEnum, new Term(fieldInfo.name, scratchTerm), true);
-
-        final Term t2 = termEnum.term();
-
-        // We could hit EOF or different field since this
-        // was a seek "forward":
-        if (t2 != null && t2.field() == internedFieldName) {
-
-          if (DEBUG_SURROGATES) {
-            System.out.println("      got term=" + UnicodeUtil.toHexString(t2.text()) + " " + t2.bytes());
-          }
-
-          final BytesRef b2 = t2.bytes();
-          assert b2.offset == 0;
-
-
-          // Set newSuffixStart -- we can't use
-          // termEnum's since the above seek may have
-          // done no scanning (eg, term was precisely
-          // and index term, or, was in the term seek
-          // cache):
-          scratchTerm.copyBytes(b2);
-          setNewSuffixStart(prevTerm, scratchTerm);
-
-          return true;
-        } else if (newSuffixStart != 0 || scratchTerm.length != 0) {
-          if (DEBUG_SURROGATES) {
-            System.out.println("      got term=null (or next field)");
-          }
-          newSuffixStart = 0;
-          scratchTerm.length = 0;
-          return true;
-        }
-      }
-
-      return false;
-    }
-
-    // Pre-flex indices store terms in UTF16 sort order, but
-    // certain queries require Unicode codepoint order; this
-    // method carefully seeks around surrogates to handle
-    // this impedance mismatch
-
-    private void surrogateDance() throws IOException {
-
-      if (!unicodeSortOrder) {
-        return;
-      }
-
-      // We are invoked after TIS.next() (by UTF16 order) to
-      // possibly seek to a different "next" (by unicode
-      // order) term.
-
-      // We scan only the "delta" from the last term to the
-      // current term, in UTF8 bytes.  We look at 1) the bytes
-      // stripped from the prior term, and then 2) the bytes
-      // appended to that prior term's prefix.
-    
-      // We don't care about specific UTF8 sequences, just
-      // the "category" of the UTF16 character.  Category S
-      // is a high/low surrogate pair (it non-BMP).
-      // Category E is any BMP char > UNI_SUR_LOW_END (and <
-      // U+FFFF). Category A is the rest (any unicode char
-      // <= UNI_SUR_HIGH_START).
-
-      // The core issue is that pre-flex indices sort the
-      // characters as ASE, while flex must sort as AES.  So
-      // when scanning, when we hit S, we must 1) seek
-      // forward to E and enum the terms there, then 2) seek
-      // back to S and enum all terms there, then 3) seek to
-      // after E.  Three different seek points (1, 2, 3).
-    
-      // We can easily detect S in UTF8: if a byte has
-      // prefix 11110 (0xf0), then that byte and the
-      // following 3 bytes encode a single unicode codepoint
-      // in S.  Similarly, we can detect E: if a byte has
-      // prefix 1110111 (0xee), then that byte and the
-      // following 2 bytes encode a single unicode codepoint
-      // in E.
-
-      // Note that this is really a recursive process --
-      // maybe the char at pos 2 needs to dance, but any
-      // point in its dance, suddenly pos 4 needs to dance
-      // so you must finish pos 4 before returning to pos
-      // 2.  But then during pos 4's dance maybe pos 7 needs
-      // to dance, etc.  However, despite being recursive,
-      // we don't need to hold any state because the state
-      // can always be derived by looking at prior term &
-      // current term.
-
-      // TODO: can we avoid this copy?
-      if (termEnum.term() == null || termEnum.term().field() != internedFieldName) {
-        scratchTerm.length = 0;
-      } else {
-        scratchTerm.copyBytes(termEnum.term().bytes());
-      }
-      
-      if (DEBUG_SURROGATES) {
-        System.out.println("  dance");
-        System.out.println("    prev=" + UnicodeUtil.toHexString(prevTerm.utf8ToString()));
-        System.out.println("         " + prevTerm.toString());
-        System.out.println("    term=" + UnicodeUtil.toHexString(scratchTerm.utf8ToString()));
-        System.out.println("         " + scratchTerm.toString());
-      }
-
-      // This code assumes TermInfosReader/SegmentTermEnum
-      // always use BytesRef.offset == 0
-      assert prevTerm.offset == 0;
-      assert scratchTerm.offset == 0;
-
-      // Need to loop here because we may need to do multiple
-      // pops, and possibly a continue in the end, ie:
-      //
-      //  cont
-      //  pop, cont
-      //  pop, pop, cont
-      //  <nothing>
-      //
-
-      while(true) {
-        if (doContinue()) {
-          break;
-        } else {
-          if (!doPop()) {
-            break;
-          }
-        }
-      }
-
-      if (DEBUG_SURROGATES) {
-        System.out.println("  finish bmp ends");
-      }
-
-      doPushes();
-    }
-
-
-    // Look for seek type 1 ("push"): if the newly added
-    // suffix contains any S, we must try to seek to the
-    // corresponding E.  If we find a match, we go there;
-    // else we keep looking for additional S's in the new
-    // suffix.  This "starts" the dance, at this character
-    // position:
-    private void doPushes() throws IOException {
-
-      int upTo = newSuffixStart;
-      if (DEBUG_SURROGATES) {
-        System.out.println("  try push newSuffixStart=" + newSuffixStart + " scratchLen=" + scratchTerm.length);
-      }
-
-      while(upTo < scratchTerm.length) {
-        if (isNonBMPChar(scratchTerm.bytes, upTo) &&
-            (upTo > newSuffixStart ||
-             (upTo >= prevTerm.length ||
-              (!isNonBMPChar(prevTerm.bytes, upTo) &&
-               !isHighBMPChar(prevTerm.bytes, upTo))))) {
-
-          // A non-BMP char (4 bytes UTF8) starts here:
-          assert scratchTerm.length >= upTo + 4;
-          
-          final int savLength = scratchTerm.length;
-          scratch[0] = scratchTerm.bytes[upTo];
-          scratch[1] = scratchTerm.bytes[upTo+1];
-          scratch[2] = scratchTerm.bytes[upTo+2];
-
-          scratchTerm.bytes[upTo] = UTF8_HIGH_BMP_LEAD;
-          scratchTerm.bytes[upTo+1] = (byte) 0x80;
-          scratchTerm.bytes[upTo+2] = (byte) 0x80;
-          scratchTerm.length = upTo+3;
-
-          if (DEBUG_SURROGATES) {
-            System.out.println("    try seek 1 pos=" + upTo + " term=" + UnicodeUtil.toHexString(scratchTerm.utf8ToString()) + " " + scratchTerm.toString() + " len=" + scratchTerm.length);
-          }
-
-          // Seek "forward":
-          // TODO: more efficient seek?
-          getTermsDict().seekEnum(seekTermEnum, new Term(fieldInfo.name, scratchTerm), true);
-
-          scratchTerm.bytes[upTo] = scratch[0];
-          scratchTerm.bytes[upTo+1] = scratch[1];
-          scratchTerm.bytes[upTo+2] = scratch[2];
-          scratchTerm.length = savLength;
-
-          // Did we find a match?
-          final Term t2 = seekTermEnum.term();
-            
-          if (DEBUG_SURROGATES) {
-            if (t2 == null) {
-              System.out.println("      hit term=null");
-            } else {
-              System.out.println("      hit term=" + UnicodeUtil.toHexString(t2.text()) + " " + (t2==null? null:t2.bytes()));
-            }
-          }
-
-          // Since this was a seek "forward", we could hit
-          // EOF or a different field:
-          boolean matches;
-
-          if (t2 != null && t2.field() == internedFieldName) {
-            final BytesRef b2 = t2.bytes();
-            assert b2.offset == 0;
-            if (b2.length >= upTo+3 && isHighBMPChar(b2.bytes, upTo)) {
-              matches = true;
-              for(int i=0;i<upTo;i++) {
-                if (scratchTerm.bytes[i] != b2.bytes[i]) {
-                  matches = false;
-                  break;
-                }
-              }              
-                
-            } else {
-              matches = false;
-            }
-          } else {
-            matches = false;
-          }
-
-          if (matches) {
-
-            if (DEBUG_SURROGATES) {
-              System.out.println("      matches!");
-            }
-
-            // OK seek "back"
-            // TODO: more efficient seek?
-            getTermsDict().seekEnum(termEnum, seekTermEnum.term(), true);
-
-            scratchTerm.copyBytes(seekTermEnum.term().bytes());
-
-            // +3 because we don't need to check the char
-            // at upTo: we know it's > BMP
-            upTo += 3;
-
-            // NOTE: we keep iterating, now, since this
-            // can easily "recurse".  Ie, after seeking
-            // forward at a certain char position, we may
-            // find another surrogate in our [new] suffix
-            // and must then do another seek (recurse)
-          } else {
-            upTo++;
-          }
-        } else {
-          upTo++;
-        }
-      }
-    }
-
-    private boolean unicodeSortOrder;
-
-    void reset(FieldInfo fieldInfo) throws IOException {
-      //System.out.println("pff.reset te=" + termEnum);
-      this.fieldInfo = fieldInfo;
-      internedFieldName = fieldInfo.name.intern();
-      final Term term = new Term(internedFieldName);
-      if (termEnum == null) {
-        termEnum = getTermsDict().terms(term);
-        seekTermEnum = getTermsDict().terms(term);
-        //System.out.println("  term=" + termEnum.term());
-      } else {
-        getTermsDict().seekEnum(termEnum, term, true);
-      }
-      skipNext = true;
-
-      unicodeSortOrder = sortTermsByUnicode();
-
-      final Term t = termEnum.term();
-      if (t != null && t.field() == internedFieldName) {
-        newSuffixStart = 0;
-        prevTerm.length = 0;
-        surrogateDance();
-      }
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      // Pre-flex indexes always sorted in UTF16 order, but
-      // we remap on-the-fly to unicode order
-      if (unicodeSortOrder) {
-        return BytesRef.getUTF8SortedAsUnicodeComparator();
-      } else {
-        return BytesRef.getUTF8SortedAsUTF16Comparator();
-      }
-    }
-
-    @Override
-    public void seekExact(long ord) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public long ord() throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef term, boolean useCache) throws IOException {
-      if (DEBUG_SURROGATES) {
-        System.out.println("TE.seek target=" + UnicodeUtil.toHexString(term.utf8ToString()));
-      }
-      skipNext = false;
-      final TermInfosReader tis = getTermsDict();
-      final Term t0 = new Term(fieldInfo.name, term);
-
-      assert termEnum != null;
-
-      tis.seekEnum(termEnum, t0, useCache);
-
-      final Term t = termEnum.term();
-
-      if (t != null && t.field() == internedFieldName && term.bytesEquals(t.bytes())) {
-        // If we found an exact match, no need to do the
-        // surrogate dance
-        if (DEBUG_SURROGATES) {
-          System.out.println("  seek exact match");
-        }
-        current = t.bytes();
-        return SeekStatus.FOUND;
-      } else if (t == null || t.field() != internedFieldName) {
-
-        // TODO: maybe we can handle this like the next()
-        // into null?  set term as prevTerm then dance?
-
-        if (DEBUG_SURROGATES) {
-          System.out.println("  seek hit EOF");
-        }
-
-        // We hit EOF; try end-case surrogate dance: if we
-        // find an E, try swapping in S, backwards:
-        scratchTerm.copyBytes(term);
-
-        assert scratchTerm.offset == 0;
-
-        for(int i=scratchTerm.length-1;i>=0;i--) {
-          if (isHighBMPChar(scratchTerm.bytes, i)) {
-            if (DEBUG_SURROGATES) {
-              System.out.println("    found E pos=" + i + "; try seek");
-            }
-
-            if (seekToNonBMP(seekTermEnum, scratchTerm, i)) {
-
-              scratchTerm.copyBytes(seekTermEnum.term().bytes());
-              getTermsDict().seekEnum(termEnum, seekTermEnum.term(), useCache);
-
-              newSuffixStart = 1+i;
-
-              doPushes();
-
-              // Found a match
-              // TODO: faster seek?
-              current = termEnum.term().bytes();
-              return SeekStatus.NOT_FOUND;
-            }
-          }
-        }
-        
-        if (DEBUG_SURROGATES) {
-          System.out.println("  seek END");
-        }
-
-        current = null;
-        return SeekStatus.END;
-      } else {
-
-        // We found a non-exact but non-null term; this one
-        // is fun -- just treat it like next, by pretending
-        // requested term was prev:
-        prevTerm.copyBytes(term);
-
-        if (DEBUG_SURROGATES) {
-          System.out.println("  seek hit non-exact term=" + UnicodeUtil.toHexString(t.text()));
-        }
-
-        final BytesRef br = t.bytes();
-        assert br.offset == 0;
-
-        setNewSuffixStart(term, br);
-
-        surrogateDance();
-
-        final Term t2 = termEnum.term();
-        if (t2 == null || t2.field() != internedFieldName) {
-          // PreFlex codec interns field names; verify:
-          assert t2 == null || !t2.field().equals(internedFieldName);
-          current = null;
-          return SeekStatus.END;
-        } else {
-          current = t2.bytes();
-          assert !unicodeSortOrder || term.compareTo(current) < 0 : "term=" + UnicodeUtil.toHexString(term.utf8ToString()) + " vs current=" + UnicodeUtil.toHexString(current.utf8ToString());
-          return SeekStatus.NOT_FOUND;
-        }
-      }
-    }
-
-    private void setNewSuffixStart(BytesRef br1, BytesRef br2) {
-      final int limit = Math.min(br1.length, br2.length);
-      int lastStart = 0;
-      for(int i=0;i<limit;i++) {
-        if ((br1.bytes[br1.offset+i] & 0xc0) == 0xc0 || (br1.bytes[br1.offset+i] & 0x80) == 0) {
-          lastStart = i;
-        }
-        if (br1.bytes[br1.offset+i] != br2.bytes[br2.offset+i]) {
-          newSuffixStart = lastStart;
-          if (DEBUG_SURROGATES) {
-            System.out.println("    set newSuffixStart=" + newSuffixStart);
-          }
-          return;
-        }
-      }
-      newSuffixStart = limit;
-      if (DEBUG_SURROGATES) {
-        System.out.println("    set newSuffixStart=" + newSuffixStart);
-      }
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      if (DEBUG_SURROGATES) {
-        System.out.println("TE.next()");
-      }
-      if (skipNext) {
-        if (DEBUG_SURROGATES) {
-          System.out.println("  skipNext=true");
-        }
-        skipNext = false;
-        if (termEnum.term() == null) {
-          return null;
-        // PreFlex codec interns field names:
-        } else if (termEnum.term().field() != internedFieldName) {
-          return null;
-        } else {
-          return current = termEnum.term().bytes();
-        }
-      }
-
-      // TODO: can we use STE's prevBuffer here?
-      prevTerm.copyBytes(termEnum.term().bytes());
-
-      if (termEnum.next() && termEnum.term().field() == internedFieldName) {
-        newSuffixStart = termEnum.newSuffixStart;
-        if (DEBUG_SURROGATES) {
-          System.out.println("  newSuffixStart=" + newSuffixStart);
-        }
-        surrogateDance();
-        final Term t = termEnum.term();
-        if (t == null || t.field() != internedFieldName) {
-          // PreFlex codec interns field names; verify:
-          assert t == null || !t.field().equals(internedFieldName);
-          current = null;
-        } else {
-          current = t.bytes();
-        }
-        return current;
-      } else {
-        // This field is exhausted, but we have to give
-        // surrogateDance a chance to seek back:
-        if (DEBUG_SURROGATES) {
-          System.out.println("  force cont");
-        }
-        //newSuffixStart = prevTerm.length;
-        newSuffixStart = 0;
-        surrogateDance();
-        
-        final Term t = termEnum.term();
-        if (t == null || t.field() != internedFieldName) {
-          // PreFlex codec interns field names; verify:
-          assert t == null || !t.field().equals(internedFieldName);
-          return null;
-        } else {
-          current = t.bytes();
-          return current;
-        }
-      }
-    }
-
-    @Override
-    public BytesRef term() {
-      return current;
-    }
-
-    @Override
-    public int docFreq() {
-      return termEnum.docFreq();
-    }
-
-    @Override
-    public long totalTermFreq() {
-      return -1;
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-      PreDocsEnum docsEnum;
-      if (needsFreqs && fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
-        return null;
-      } else if (reuse == null || !(reuse instanceof PreDocsEnum)) {
-        docsEnum = new PreDocsEnum();
-      } else {
-        docsEnum = (PreDocsEnum) reuse;
-        if (docsEnum.getFreqStream() != freqStream) {
-          docsEnum = new PreDocsEnum();
-        }
-      }
-      return docsEnum.reset(termEnum, liveDocs);
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
-      PreDocsAndPositionsEnum docsPosEnum;
-      if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-        return null;
-      } else if (reuse == null || !(reuse instanceof PreDocsAndPositionsEnum)) {
-        docsPosEnum = new PreDocsAndPositionsEnum();
-      } else {
-        docsPosEnum = (PreDocsAndPositionsEnum) reuse;
-        if (docsPosEnum.getFreqStream() != freqStream) {
-          docsPosEnum = new PreDocsAndPositionsEnum();
-        }
-      }
-      return docsPosEnum.reset(termEnum, liveDocs);        
-    }
-  }
-
-  private final class PreDocsEnum extends DocsEnum {
-    final private SegmentTermDocs docs;
-    private int docID = -1;
-    PreDocsEnum() throws IOException {
-      docs = new SegmentTermDocs(freqStream, getTermsDict(), fieldInfos);
-    }
-
-    IndexInput getFreqStream() {
-      return freqStream;
-    }
-
-    public PreDocsEnum reset(SegmentTermEnum termEnum, Bits liveDocs) throws IOException {
-      docs.setLiveDocs(liveDocs);
-      docs.seek(termEnum);
-      docID = -1;
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      if (docs.next()) {
-        return docID = docs.doc();
-      } else {
-        return docID = NO_MORE_DOCS;
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      if (docs.skipTo(target)) {
-        return docID = docs.doc();
-      } else {
-        return docID = NO_MORE_DOCS;
-      }
-    }
-
-    @Override
-    public int freq() {
-      return docs.freq();
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-  }
-
-  private final class PreDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    final private SegmentTermPositions pos;
-    private int docID = -1;
-    PreDocsAndPositionsEnum() throws IOException {
-      pos = new SegmentTermPositions(freqStream, proxStream, getTermsDict(), fieldInfos);
-    }
-
-    IndexInput getFreqStream() {
-      return freqStream;
-    }
-
-    public DocsAndPositionsEnum reset(SegmentTermEnum termEnum, Bits liveDocs) throws IOException {
-      pos.setLiveDocs(liveDocs);
-      pos.seek(termEnum);
-      docID = -1;
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      if (pos.next()) {
-        return docID = pos.doc();
-      } else {
-        return docID = NO_MORE_DOCS;
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      if (pos.skipTo(target)) {
-        return docID = pos.doc();
-      } else {
-        return docID = NO_MORE_DOCS;
-      }
-    }
-
-    @Override
-    public int freq() {
-      return pos.freq();
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      assert docID != NO_MORE_DOCS;
-      return pos.nextPosition();
-    }
-
-    @Override
-    public boolean hasPayload() {
-      assert docID != NO_MORE_DOCS;
-      return pos.isPayloadAvailable();
-    }
-
-    private BytesRef payload;
-
-    @Override
-    public BytesRef getPayload() throws IOException {
-      final int len = pos.getPayloadLength();
-      if (payload == null) {
-        payload = new BytesRef();
-        payload.bytes = new byte[len];
-      } else {
-        if (payload.bytes.length < len) {
-          payload.grow(len);
-        }
-      }
-      
-      payload.bytes = pos.getPayload(payload.bytes, 0);
-      payload.length = len;
-      return payload;
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xPostingsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xPostingsFormat.java
deleted file mode 100644
index 23da306..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xPostingsFormat.java
+++ /dev/null
@@ -1,73 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Set;
-import java.io.IOException;
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-
-/** Codec that reads the pre-flex-indexing postings
- *  format.  It does not provide a writer because newly
- *  written segments should use StandardCodec.
- *
- * @deprecated (4.0) This is only used to read indexes created
- * before 4.0.
- * @lucene.experimental
- */
-@Deprecated
-public class Lucene3xPostingsFormat extends PostingsFormat {
-
-  /** Extension of terms file */
-  public static final String TERMS_EXTENSION = "tis";
-
-  /** Extension of terms index file */
-  public static final String TERMS_INDEX_EXTENSION = "tii";
-
-  /** Extension of freq postings file */
-  public static final String FREQ_EXTENSION = "frq";
-
-  /** Extension of prox postings file */
-  public static final String PROX_EXTENSION = "prx";
-
-  public Lucene3xPostingsFormat() {
-    super("Lucene3x");
-  }
-  
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    throw new IllegalArgumentException("this codec can only be used for reading");
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new Lucene3xFields(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.termsIndexDivisor);
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, String segmentSuffix, Set<String> files) throws IOException {
-    // preflex fields have no segmentSuffix - we ignore it here
-    Lucene3xFields.files(dir, info, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/SegmentTermDocs.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/SegmentTermDocs.java
deleted file mode 100644
index f2c870e..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/SegmentTermDocs.java
+++ /dev/null
@@ -1,230 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.codecs.lucene40.Lucene40SkipListReader;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Bits;
-
-/** @deprecated (4.0)
- *  @lucene.experimental */
-@Deprecated
-public class SegmentTermDocs {
-  //protected SegmentReader parent;
-  private final FieldInfos fieldInfos;
-  private final TermInfosReader tis;
-  protected Bits liveDocs;
-  protected IndexInput freqStream;
-  protected int count;
-  protected int df;
-  int doc = 0;
-  int freq;
-
-  private int skipInterval;
-  private int maxSkipLevels;
-  private Lucene40SkipListReader skipListReader;
-  
-  private long freqBasePointer;
-  private long proxBasePointer;
-
-  private long skipPointer;
-  private boolean haveSkipped;
-  
-  protected boolean currentFieldStoresPayloads;
-  protected IndexOptions indexOptions;
-  
-  public SegmentTermDocs(IndexInput freqStream, TermInfosReader tis, FieldInfos fieldInfos) {
-    this.freqStream = (IndexInput) freqStream.clone();
-    this.tis = tis;
-    this.fieldInfos = fieldInfos;
-    skipInterval = tis.getSkipInterval();
-    maxSkipLevels = tis.getMaxSkipLevels();
-  }
-
-  public void seek(Term term) throws IOException {
-    TermInfo ti = tis.get(term);
-    seek(ti, term);
-  }
-
-  public void setLiveDocs(Bits liveDocs) {
-    this.liveDocs = liveDocs;
-  }
-
-  public void seek(SegmentTermEnum segmentTermEnum) throws IOException {
-    TermInfo ti;
-    Term term;
-    
-    // use comparison of fieldinfos to verify that termEnum belongs to the same segment as this SegmentTermDocs
-    if (segmentTermEnum.fieldInfos == fieldInfos) {        // optimized case
-      term = segmentTermEnum.term();
-      ti = segmentTermEnum.termInfo();
-    } else  {                                         // punt case
-      term = segmentTermEnum.term();
-      ti = tis.get(term); 
-    }
-    
-    seek(ti, term);
-  }
-
-  void seek(TermInfo ti, Term term) throws IOException {
-    count = 0;
-    FieldInfo fi = fieldInfos.fieldInfo(term.field());
-    this.indexOptions = (fi != null) ? fi.indexOptions : IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-    currentFieldStoresPayloads = (fi != null) ? fi.storePayloads : false;
-    if (ti == null) {
-      df = 0;
-    } else {
-      df = ti.docFreq;
-      doc = 0;
-      freqBasePointer = ti.freqPointer;
-      proxBasePointer = ti.proxPointer;
-      skipPointer = freqBasePointer + ti.skipOffset;
-      freqStream.seek(freqBasePointer);
-      haveSkipped = false;
-    }
-  }
-
-  public void close() throws IOException {
-    freqStream.close();
-    if (skipListReader != null)
-      skipListReader.close();
-  }
-
-  public final int doc() { return doc; }
-  public final int freq() {
-    assert indexOptions != IndexOptions.DOCS_ONLY;
-    return freq;
-  }
-
-  protected void skippingDoc() throws IOException {
-  }
-
-  public boolean next() throws IOException {
-    while (true) {
-      if (count == df)
-        return false;
-      final int docCode = freqStream.readVInt();
-      
-      if (indexOptions == IndexOptions.DOCS_ONLY) {
-        doc += docCode;
-      } else {
-        doc += docCode >>> 1;       // shift off low bit
-        if ((docCode & 1) != 0)       // if low bit is set
-          freq = 1;         // freq is one
-        else {
-          freq = freqStream.readVInt();     // else read freq
-          assert freq != 1;
-        }
-      }
-      
-      count++;
-
-      if (liveDocs == null || liveDocs.get(doc)) {
-        break;
-      }
-      skippingDoc();
-    }
-    return true;
-  }
-
-  /** Optimized implementation. */
-  public int read(final int[] docs, final int[] freqs)
-          throws IOException {
-    final int length = docs.length;
-    if (indexOptions == IndexOptions.DOCS_ONLY) {
-      return readNoTf(docs, freqs, length);
-    } else {
-      int i = 0;
-      while (i < length && count < df) {
-        // manually inlined call to next() for speed
-        final int docCode = freqStream.readVInt();
-        doc += docCode >>> 1;       // shift off low bit
-        if ((docCode & 1) != 0)       // if low bit is set
-          freq = 1;         // freq is one
-        else
-          freq = freqStream.readVInt();     // else read freq
-        count++;
-
-        if (liveDocs == null || liveDocs.get(doc)) {
-          docs[i] = doc;
-          freqs[i] = freq;
-          ++i;
-        }
-      }
-      return i;
-    }
-  }
-
-  private final int readNoTf(final int[] docs, final int[] freqs, final int length) throws IOException {
-    int i = 0;
-    while (i < length && count < df) {
-      // manually inlined call to next() for speed
-      doc += freqStream.readVInt();       
-      count++;
-
-      if (liveDocs == null || liveDocs.get(doc)) {
-        docs[i] = doc;
-        // Hardware freq to 1 when term freqs were not
-        // stored in the index
-        freqs[i] = 1;
-        ++i;
-      }
-    }
-    return i;
-  }
- 
-  
-  /** Overridden by SegmentTermPositions to skip in prox stream. */
-  protected void skipProx(long proxPointer, int payloadLength) throws IOException {}
-
-  /** Optimized implementation. */
-  public boolean skipTo(int target) throws IOException {
-    // don't skip if the target is close (within skipInterval docs away)
-    if ((target - skipInterval) >= doc && df >= skipInterval) {                      // optimized case
-      if (skipListReader == null)
-        skipListReader = new Lucene40SkipListReader((IndexInput) freqStream.clone(), maxSkipLevels, skipInterval); // lazily clone
-
-      if (!haveSkipped) {                          // lazily initialize skip stream
-        skipListReader.init(skipPointer, freqBasePointer, proxBasePointer, df, currentFieldStoresPayloads);
-        haveSkipped = true;
-      }
-
-      int newCount = skipListReader.skipTo(target); 
-      if (newCount > count) {
-        freqStream.seek(skipListReader.getFreqPointer());
-        skipProx(skipListReader.getProxPointer(), skipListReader.getPayloadLength());
-
-        doc = skipListReader.getDoc();
-        count = newCount;
-      }      
-    }
-
-    // done skipping, now just scan
-    do {
-      if (!next())
-        return false;
-    } while (target > doc);
-    return true;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/SegmentTermEnum.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/SegmentTermEnum.java
deleted file mode 100644
index cac23c3..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/SegmentTermEnum.java
+++ /dev/null
@@ -1,225 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.IndexFormatTooNewException;
-
-/**
- * @deprecated (4.0) No longer used with flex indexing, except for
- * reading old segments 
- * @lucene.experimental */
-
-@Deprecated
-public final class SegmentTermEnum implements Cloneable {
-  private IndexInput input;
-  FieldInfos fieldInfos;
-  long size;
-  long position = -1;
-
-  // Changed strings to true utf8 with length-in-bytes not
-  // length-in-chars
-  public static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = -4;
-
-  // NOTE: always change this if you switch to a new format!
-  // whenever you add a new format, make it 1 smaller (negative version logic)!
-  public static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
-  
-  // when removing support for old versions, leave the last supported version here
-  public static final int FORMAT_MINIMUM = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
-
-  private TermBuffer termBuffer = new TermBuffer();
-  private TermBuffer prevBuffer = new TermBuffer();
-  private TermBuffer scanBuffer = new TermBuffer(); // used for scanning
-
-  TermInfo termInfo = new TermInfo();
-
-  private int format;
-  private boolean isIndex = false;
-  long indexPointer = 0;
-  int indexInterval;
-  int skipInterval;
-  int newSuffixStart;
-  int maxSkipLevels;
-  private boolean first = true;
-
-  SegmentTermEnum(IndexInput i, FieldInfos fis, boolean isi)
-          throws CorruptIndexException, IOException {
-    input = i;
-    fieldInfos = fis;
-    isIndex = isi;
-    maxSkipLevels = 1; // use single-level skip lists for formats > -3 
-    
-    int firstInt = input.readInt();
-    if (firstInt >= 0) {
-      // original-format file, without explicit format version number
-      format = 0;
-      size = firstInt;
-
-      // back-compatible settings
-      indexInterval = 128;
-      skipInterval = Integer.MAX_VALUE; // switch off skipTo optimization
-    } else {
-      // we have a format version number
-      format = firstInt;
-
-      // check that it is a format we can understand
-      if (format > FORMAT_MINIMUM)
-        throw new IndexFormatTooOldException(input, format, FORMAT_MINIMUM, FORMAT_CURRENT);
-      if (format < FORMAT_CURRENT)
-        throw new IndexFormatTooNewException(input, format, FORMAT_MINIMUM, FORMAT_CURRENT);
-
-      size = input.readLong();                    // read the size
-      
-      indexInterval = input.readInt();
-      skipInterval = input.readInt();
-      maxSkipLevels = input.readInt();
-      assert indexInterval > 0: "indexInterval=" + indexInterval + " is negative; must be > 0";
-      assert skipInterval > 0: "skipInterval=" + skipInterval + " is negative; must be > 0";
-    }
-  }
-
-  @Override
-  protected Object clone() {
-    SegmentTermEnum clone = null;
-    try {
-      clone = (SegmentTermEnum) super.clone();
-    } catch (CloneNotSupportedException e) {}
-
-    clone.input = (IndexInput) input.clone();
-    clone.termInfo = new TermInfo(termInfo);
-
-    clone.termBuffer = (TermBuffer)termBuffer.clone();
-    clone.prevBuffer = (TermBuffer)prevBuffer.clone();
-    clone.scanBuffer = new TermBuffer();
-
-    return clone;
-  }
-
-  final void seek(long pointer, long p, Term t, TermInfo ti)
-          throws IOException {
-    input.seek(pointer);
-    position = p;
-    termBuffer.set(t);
-    prevBuffer.reset();
-    //System.out.println("  ste doSeek prev=" + prevBuffer.toTerm() + " this=" + this);
-    termInfo.set(ti);
-    first = p == -1;
-  }
-
-  /** Increments the enumeration to the next element.  True if one exists.*/
-  public final boolean next() throws IOException {
-    prevBuffer.set(termBuffer);
-    //System.out.println("  ste setPrev=" + prev() + " this=" + this);
-
-    if (position++ >= size - 1) {
-      termBuffer.reset();
-      //System.out.println("    EOF");
-      return false;
-    }
-
-    termBuffer.read(input, fieldInfos);
-    newSuffixStart = termBuffer.newSuffixStart;
-
-    termInfo.docFreq = input.readVInt();	  // read doc freq
-    termInfo.freqPointer += input.readVLong();	  // read freq pointer
-    termInfo.proxPointer += input.readVLong();	  // read prox pointer
-    
-    if (termInfo.docFreq >= skipInterval) 
-      termInfo.skipOffset = input.readVInt();
-
-    if (isIndex)
-      indexPointer += input.readVLong();	  // read index pointer
-
-    //System.out.println("  ste ret term=" + term());
-    return true;
-  }
-
-  /* Optimized scan, without allocating new terms. 
-   *  Return number of invocations to next().
-   *
-   * NOTE: LUCENE-3183: if you pass Term("", "") here then this
-   * will incorrectly return before positioning the enum,
-   * and position will be -1; caller must detect this. */
-  final int scanTo(Term term) throws IOException {
-    scanBuffer.set(term);
-    int count = 0;
-    if (first) {
-      // Always force initial next() in case term is
-      // Term("", "")
-      next();
-      first = false;
-      count++;
-    }
-    while (scanBuffer.compareTo(termBuffer) > 0 && next()) {
-      count++;
-    }
-    return count;
-  }
-
-  /** Returns the current Term in the enumeration.
-   Initially invalid, valid after next() called for the first time.*/
-  public final Term term() {
-    return termBuffer.toTerm();
-  }
-
-  /** Returns the previous Term enumerated. Initially null.*/
-  final Term prev() {
-    return prevBuffer.toTerm();
-  }
-
-  /** Returns the current TermInfo in the enumeration.
-   Initially invalid, valid after next() called for the first time.*/
-  final TermInfo termInfo() {
-    return new TermInfo(termInfo);
-  }
-
-  /** Sets the argument to the current TermInfo in the enumeration.
-   Initially invalid, valid after next() called for the first time.*/
-  final void termInfo(TermInfo ti) {
-    ti.set(termInfo);
-  }
-
-  /** Returns the docFreq from the current TermInfo in the enumeration.
-   Initially invalid, valid after next() called for the first time.*/
-  public final int docFreq() {
-    return termInfo.docFreq;
-  }
-
-  /* Returns the freqPointer from the current TermInfo in the enumeration.
-    Initially invalid, valid after next() called for the first time.*/
-  final long freqPointer() {
-    return termInfo.freqPointer;
-  }
-
-  /* Returns the proxPointer from the current TermInfo in the enumeration.
-    Initially invalid, valid after next() called for the first time.*/
-  final long proxPointer() {
-    return termInfo.proxPointer;
-  }
-
-  /** Closes the enumeration to further activity, freeing resources. */
-  public final void close() throws IOException {
-    input.close();
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/SegmentTermPositions.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/SegmentTermPositions.java
deleted file mode 100644
index 5f6921c..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/SegmentTermPositions.java
+++ /dev/null
@@ -1,219 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.store.IndexInput;
-
-/**
- * @lucene.experimental
- * @deprecated (4.0)
- */
-@Deprecated
-public final class SegmentTermPositions
-extends SegmentTermDocs  {
-  private IndexInput proxStream;
-  private IndexInput proxStreamOrig;
-  private int proxCount;
-  private int position;
-  
-  // the current payload length
-  private int payloadLength;
-  // indicates whether the payload of the current position has
-  // been read from the proxStream yet
-  private boolean needToLoadPayload;
-  
-  // these variables are being used to remember information
-  // for a lazy skip
-  private long lazySkipPointer = -1;
-  private int lazySkipProxCount = 0;
-
-  /*
-  SegmentTermPositions(SegmentReader p) {
-    super(p);
-    this.proxStream = null;  // the proxStream will be cloned lazily when nextPosition() is called for the first time
-  }
-  */
-
-  public SegmentTermPositions(IndexInput freqStream, IndexInput proxStream, TermInfosReader tis, FieldInfos fieldInfos) {
-    super(freqStream, tis, fieldInfos);
-    this.proxStreamOrig = proxStream;  // the proxStream will be cloned lazily when nextPosition() is called for the first time
-  }
-
-  @Override
-  final void seek(TermInfo ti, Term term) throws IOException {
-    super.seek(ti, term);
-    if (ti != null)
-      lazySkipPointer = ti.proxPointer;
-    
-    lazySkipProxCount = 0;
-    proxCount = 0;
-    payloadLength = 0;
-    needToLoadPayload = false;
-  }
-
-  @Override
-  public final void close() throws IOException {
-    super.close();
-    if (proxStream != null) proxStream.close();
-  }
-
-  public final int nextPosition() throws IOException {
-    if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS)
-      // This field does not store positions, payloads
-      return 0;
-    // perform lazy skips if necessary
-    lazySkip();
-    proxCount--;
-    return position += readDeltaPosition();
-  }
-
-  private final int readDeltaPosition() throws IOException {
-    int delta = proxStream.readVInt();
-    if (currentFieldStoresPayloads) {
-      // if the current field stores payloads then
-      // the position delta is shifted one bit to the left.
-      // if the LSB is set, then we have to read the current
-      // payload length
-      if ((delta & 1) != 0) {
-        payloadLength = proxStream.readVInt();
-      } 
-      delta >>>= 1;
-      needToLoadPayload = true;
-    }
-    return delta;
-  }
-  
-  @Override
-  protected final void skippingDoc() throws IOException {
-    // we remember to skip a document lazily
-    lazySkipProxCount += freq;
-  }
-
-  @Override
-  public final boolean next() throws IOException {
-    // we remember to skip the remaining positions of the current
-    // document lazily
-    lazySkipProxCount += proxCount;
-    
-    if (super.next()) {               // run super
-      proxCount = freq;               // note frequency
-      position = 0;               // reset position
-      return true;
-    }
-    return false;
-  }
-
-  @Override
-  public final int read(final int[] docs, final int[] freqs) {
-    throw new UnsupportedOperationException("TermPositions does not support processing multiple documents in one call. Use TermDocs instead.");
-  }
-
-
-  /** Called by super.skipTo(). */
-  @Override
-  protected void skipProx(long proxPointer, int payloadLength) throws IOException {
-    // we save the pointer, we might have to skip there lazily
-    lazySkipPointer = proxPointer;
-    lazySkipProxCount = 0;
-    proxCount = 0;
-    this.payloadLength = payloadLength;
-    needToLoadPayload = false;
-  }
-
-  private void skipPositions(int n) throws IOException {
-    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-    for (int f = n; f > 0; f--) {        // skip unread positions
-      readDeltaPosition();
-      skipPayload();
-    }      
-  }
-  
-  private void skipPayload() throws IOException {
-    if (needToLoadPayload && payloadLength > 0) {
-      proxStream.seek(proxStream.getFilePointer() + payloadLength);
-    }
-    needToLoadPayload = false;
-  }
-
-  // It is not always necessary to move the prox pointer
-  // to a new document after the freq pointer has been moved.
-  // Consider for example a phrase query with two terms:
-  // the freq pointer for term 1 has to move to document x
-  // to answer the question if the term occurs in that document. But
-  // only if term 2 also matches document x, the positions have to be
-  // read to figure out if term 1 and term 2 appear next
-  // to each other in document x and thus satisfy the query.
-  // So we move the prox pointer lazily to the document
-  // as soon as positions are requested.
-  private void lazySkip() throws IOException {
-    if (proxStream == null) {
-      // clone lazily
-      proxStream = (IndexInput)proxStreamOrig.clone();
-    }
-    
-    // we might have to skip the current payload
-    // if it was not read yet
-    skipPayload();
-      
-    if (lazySkipPointer != -1) {
-      proxStream.seek(lazySkipPointer);
-      lazySkipPointer = -1;
-    }
-     
-    if (lazySkipProxCount != 0) {
-      skipPositions(lazySkipProxCount);
-      lazySkipProxCount = 0;
-    }
-  }
-  
-  public int getPayloadLength() {
-    return payloadLength;
-  }
-
-  public byte[] getPayload(byte[] data, int offset) throws IOException {
-    if (!needToLoadPayload) {
-      throw new IOException("Either no payload exists at this term position or an attempt was made to load it more than once.");
-    }
-
-    // read payloads lazily
-    byte[] retArray;
-    int retOffset;
-    if (data == null || data.length - offset < payloadLength) {
-      // the array is too small to store the payload data,
-      // so we allocate a new one
-      retArray = new byte[payloadLength];
-      retOffset = 0;
-    } else {
-      retArray = data;
-      retOffset = offset;
-    }
-    proxStream.readBytes(retArray, retOffset, payloadLength);
-    needToLoadPayload = false;
-    return retArray;
-  }
-
-  public boolean isPayloadAvailable() {
-    return needToLoadPayload && payloadLength > 0;
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermBuffer.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermBuffer.java
deleted file mode 100644
index 52e8558..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermBuffer.java
+++ /dev/null
@@ -1,122 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.FieldInfos;
-
-/**
- * @lucene.experimental
- * @deprecated (4.0)
- */
-@Deprecated
-final class TermBuffer implements Cloneable {
-
-  private String field;
-  private Term term;                            // cached
-
-  private BytesRef bytes = new BytesRef(10);
-
-  // Cannot be -1 since (strangely) we write that
-  // fieldNumber into index for first indexed term:
-  private int currentFieldNumber = -2;
-
-  private static final Comparator<BytesRef> utf8AsUTF16Comparator = BytesRef.getUTF8SortedAsUTF16Comparator();
-
-  int newSuffixStart;                             // only valid right after .read is called
-
-  public int compareTo(TermBuffer other) {
-    if (field == other.field) 	  // fields are interned
-                                  // (only by PreFlex codec)
-      return utf8AsUTF16Comparator.compare(bytes, other.bytes);
-    else
-      return field.compareTo(other.field);
-  }
-
-  public void read(IndexInput input, FieldInfos fieldInfos)
-    throws IOException {
-    this.term = null;                           // invalidate cache
-    newSuffixStart = input.readVInt();
-    int length = input.readVInt();
-    int totalLength = newSuffixStart + length;
-    if (bytes.bytes.length < totalLength) {
-      bytes.grow(totalLength);
-    }
-    bytes.length = totalLength;
-    input.readBytes(bytes.bytes, newSuffixStart, length);
-    final int fieldNumber = input.readVInt();
-    if (fieldNumber != currentFieldNumber) {
-      currentFieldNumber = fieldNumber;
-      field = fieldInfos.fieldName(currentFieldNumber).intern();
-    } else {
-      assert field.equals(fieldInfos.fieldName(fieldNumber)): "currentFieldNumber=" + currentFieldNumber + " field=" + field + " vs " + fieldInfos.fieldName(fieldNumber);
-    }
-  }
-
-  public void set(Term term) {
-    if (term == null) {
-      reset();
-      return;
-    }
-    bytes.copyBytes(term.bytes());
-    field = term.field().intern();
-    currentFieldNumber = -1;
-    this.term = term;
-  }
-
-  public void set(TermBuffer other) {
-    field = other.field;
-    currentFieldNumber = other.currentFieldNumber;
-    // dangerous to copy Term over, since the underlying
-    // BytesRef could subsequently be modified:
-    term = null;
-    bytes.copyBytes(other.bytes);
-  }
-
-  public void reset() {
-    field = null;
-    term = null;
-    currentFieldNumber=  -1;
-  }
-
-  public Term toTerm() {
-    if (field == null)                            // unset
-      return null;
-
-    if (term == null) {
-      term = new Term(field, BytesRef.deepCopyOf(bytes));
-    }
-
-    return term;
-  }
-
-  @Override
-  protected Object clone() {
-    TermBuffer clone = null;
-    try {
-      clone = (TermBuffer)super.clone();
-    } catch (CloneNotSupportedException e) {}
-    clone.bytes = BytesRef.deepCopyOf(bytes);
-    return clone;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfo.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfo.java
deleted file mode 100644
index 23009cb..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfo.java
+++ /dev/null
@@ -1,63 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** A TermInfo is the record of information stored for a
- * term
- * @deprecated (4.0) This class is no longer used in flexible
- * indexing. */
-
-@Deprecated
-public class TermInfo {
-  /** The number of documents which contain the term. */
-  public int docFreq = 0;
-
-  public long freqPointer = 0;
-  public long proxPointer = 0;
-  public int skipOffset;
-
-  public TermInfo() {}
-
-  public TermInfo(int df, long fp, long pp) {
-    docFreq = df;
-    freqPointer = fp;
-    proxPointer = pp;
-  }
-
-  public TermInfo(TermInfo ti) {
-    docFreq = ti.docFreq;
-    freqPointer = ti.freqPointer;
-    proxPointer = ti.proxPointer;
-    skipOffset = ti.skipOffset;
-  }
-
-  public final void set(int docFreq,
-                 long freqPointer, long proxPointer, int skipOffset) {
-    this.docFreq = docFreq;
-    this.freqPointer = freqPointer;
-    this.proxPointer = proxPointer;
-    this.skipOffset = skipOffset;
-  }
-
-  public final void set(TermInfo ti) {
-    docFreq = ti.docFreq;
-    freqPointer = ti.freqPointer;
-    proxPointer = ti.proxPointer;
-    skipOffset = ti.skipOffset;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfosReader.java
deleted file mode 100644
index ad6e2a0..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfosReader.java
+++ /dev/null
@@ -1,347 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CloseableThreadLocal;
-import org.apache.lucene.util.DoubleBarrelLRUCache;
-
-/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
- * Directory.  Pairs are accessed either by Term or by ordinal position the
- * set
- * @deprecated (4.0) This class has been replaced by
- * FormatPostingsTermsDictReader, except for reading old segments. 
- * @lucene.experimental
- */
-@Deprecated
-public final class TermInfosReader {
-  private final Directory directory;
-  private final String segment;
-  private final FieldInfos fieldInfos;
-
-  private final CloseableThreadLocal<ThreadResources> threadResources = new CloseableThreadLocal<ThreadResources>();
-  private final SegmentTermEnum origEnum;
-  private final long size;
-
-  private final TermInfosReaderIndex index;
-  private final int indexLength;
-  
-  private final int totalIndexInterval;
-
-  private final static int DEFAULT_CACHE_SIZE = 1024;
-  
-  // Just adds term's ord to TermInfo
-  private final static class TermInfoAndOrd extends TermInfo {
-    final long termOrd;
-    public TermInfoAndOrd(TermInfo ti, long termOrd) {
-      super(ti);
-      assert termOrd >= 0;
-      this.termOrd = termOrd;
-    }
-  }
-
-  private static class CloneableTerm extends DoubleBarrelLRUCache.CloneableKey {
-    Term term;
-    public CloneableTerm(Term t) {
-      this.term = t;
-    }
-
-    @Override
-    public boolean equals(Object other) {
-      CloneableTerm t = (CloneableTerm) other;
-      return this.term.equals(t.term);
-    }
-
-    @Override
-    public int hashCode() {
-      return term.hashCode();
-    }
-
-    @Override
-    public Object clone() {
-      return new CloneableTerm(term);
-    }
-  }
-
-  private final DoubleBarrelLRUCache<CloneableTerm,TermInfoAndOrd> termsCache = new DoubleBarrelLRUCache<CloneableTerm,TermInfoAndOrd>(DEFAULT_CACHE_SIZE);
-
-  /**
-   * Per-thread resources managed by ThreadLocal
-   */
-  private static final class ThreadResources {
-    SegmentTermEnum termEnum;
-  }
-  
-  TermInfosReader(Directory dir, String seg, FieldInfos fis, IOContext context, int indexDivisor)
-       throws CorruptIndexException, IOException {
-    boolean success = false;
-
-    if (indexDivisor < 1 && indexDivisor != -1) {
-      throw new IllegalArgumentException("indexDivisor must be -1 (don't load terms index) or greater than 0: got " + indexDivisor);
-    }
-
-    try {
-      directory = dir;
-      segment = seg;
-      fieldInfos = fis;
-
-      origEnum = new SegmentTermEnum(directory.openInput(IndexFileNames.segmentFileName(segment, "", Lucene3xPostingsFormat.TERMS_EXTENSION),
-                                                         context), fieldInfos, false);
-      size = origEnum.size;
-
-
-      if (indexDivisor != -1) {
-        // Load terms index
-        totalIndexInterval = origEnum.indexInterval * indexDivisor;
-
-        final String indexFileName = IndexFileNames.segmentFileName(segment, "", Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION);
-        final SegmentTermEnum indexEnum = new SegmentTermEnum(directory.openInput(indexFileName,
-                                                                                   context), fieldInfos, true);
-
-        try {
-          index = new TermInfosReaderIndex(indexEnum, indexDivisor, dir.fileLength(indexFileName), totalIndexInterval);
-          indexLength = index.length();
-        } finally {
-          indexEnum.close();
-        }
-      } else {
-        // Do not load terms index:
-        totalIndexInterval = -1;
-        index = null;
-        indexLength = -1;
-      }
-      success = true;
-    } finally {
-      // With lock-less commits, it's entirely possible (and
-      // fine) to hit a FileNotFound exception above. In
-      // this case, we want to explicitly close any subset
-      // of things that were opened so that we don't have to
-      // wait for a GC to do so.
-      if (!success) {
-        close();
-      }
-    }
-  }
-
-  public int getSkipInterval() {
-    return origEnum.skipInterval;
-  }
-  
-  public int getMaxSkipLevels() {
-    return origEnum.maxSkipLevels;
-  }
-
-  void close() throws IOException {
-    if (origEnum != null)
-      origEnum.close();
-    threadResources.close();
-  }
-
-  /** Returns the number of term/value pairs in the set. */
-  long size() {
-    return size;
-  }
-
-  private ThreadResources getThreadResources() {
-    ThreadResources resources = threadResources.get();
-    if (resources == null) {
-      resources = new ThreadResources();
-      resources.termEnum = terms();
-      threadResources.set(resources);
-    }
-    return resources;
-  }
-  
-  private static final Comparator<BytesRef> legacyComparator = 
-    BytesRef.getUTF8SortedAsUTF16Comparator();
-
-  private final int compareAsUTF16(Term term1, Term term2) {
-    if (term1.field().equals(term2.field())) {
-      return legacyComparator.compare(term1.bytes(), term2.bytes());
-    } else {
-      return term1.field().compareTo(term2.field());
-    }
-  }
-
-  /** Returns the TermInfo for a Term in the set, or null. */
-  TermInfo get(Term term) throws IOException {
-    return get(term, false);
-  }
-  
-  /** Returns the TermInfo for a Term in the set, or null. */
-  private TermInfo get(Term term, boolean mustSeekEnum) throws IOException {
-    if (size == 0) return null;
-
-    ensureIndexIsRead();
-    TermInfoAndOrd tiOrd = termsCache.get(new CloneableTerm(term));
-    ThreadResources resources = getThreadResources();
-
-    if (!mustSeekEnum && tiOrd != null) {
-      return tiOrd;
-    }
-
-    return seekEnum(resources.termEnum, term, tiOrd, true);
-  }
-
-  public void cacheCurrentTerm(SegmentTermEnum enumerator) {
-    termsCache.put(new CloneableTerm(enumerator.term()),
-                   new TermInfoAndOrd(enumerator.termInfo,
-                                      enumerator.position));
-  }
-
-  TermInfo seekEnum(SegmentTermEnum enumerator, Term term, boolean useCache) throws IOException {
-    if (useCache) {
-      return seekEnum(enumerator, term, termsCache.get(new CloneableTerm(term)), useCache);
-    } else {
-      return seekEnum(enumerator, term, null, useCache);
-    }
-  }
-
-  TermInfo seekEnum(SegmentTermEnum enumerator, Term term, TermInfoAndOrd tiOrd, boolean useCache) throws IOException {
-    if (size == 0) {
-      return null;
-    }
-
-    // optimize sequential access: first try scanning cached enum w/o seeking
-    if (enumerator.term() != null                 // term is at or past current
-  && ((enumerator.prev() != null && compareAsUTF16(term, enumerator.prev())> 0)
-      || compareAsUTF16(term, enumerator.term()) >= 0)) {
-      int enumOffset = (int)(enumerator.position/totalIndexInterval)+1;
-      if (indexLength == enumOffset    // but before end of block
-    || index.compareTo(term, enumOffset) < 0) {
-       // no need to seek
-
-        final TermInfo ti;
-        int numScans = enumerator.scanTo(term);
-        if (enumerator.term() != null && compareAsUTF16(term, enumerator.term()) == 0) {
-          ti = enumerator.termInfo;
-          if (numScans > 1) {
-            // we only  want to put this TermInfo into the cache if
-            // scanEnum skipped more than one dictionary entry.
-            // This prevents RangeQueries or WildcardQueries to 
-            // wipe out the cache when they iterate over a large numbers
-            // of terms in order
-            if (tiOrd == null) {
-              if (useCache) {
-                termsCache.put(new CloneableTerm(term), new TermInfoAndOrd(ti, enumerator.position));
-              }
-            } else {
-              assert sameTermInfo(ti, tiOrd, enumerator);
-              assert (int) enumerator.position == tiOrd.termOrd;
-            }
-          }
-        } else {
-          ti = null;
-        }
-
-        return ti;
-      }  
-    }
-
-    // random-access: must seek
-    final int indexPos;
-    if (tiOrd != null) {
-      indexPos = (int) (tiOrd.termOrd / totalIndexInterval);
-    } else {
-      // Must do binary search:
-      indexPos = index.getIndexOffset(term);
-    }
-
-    index.seekEnum(enumerator, indexPos);
-    enumerator.scanTo(term);
-    final TermInfo ti;
-
-    if (enumerator.term() != null && compareAsUTF16(term, enumerator.term()) == 0) {
-      ti = enumerator.termInfo;
-      if (tiOrd == null) {
-        if (useCache) {
-          termsCache.put(new CloneableTerm(term), new TermInfoAndOrd(ti, enumerator.position));
-        }
-      } else {
-        assert sameTermInfo(ti, tiOrd, enumerator);
-        assert enumerator.position == tiOrd.termOrd;
-      }
-    } else {
-      ti = null;
-    }
-    return ti;
-  }
-
-  // called only from asserts
-  private boolean sameTermInfo(TermInfo ti1, TermInfo ti2, SegmentTermEnum enumerator) {
-    if (ti1.docFreq != ti2.docFreq) {
-      return false;
-    }
-    if (ti1.freqPointer != ti2.freqPointer) {
-      return false;
-    }
-    if (ti1.proxPointer != ti2.proxPointer) {
-      return false;
-    }
-    // skipOffset is only valid when docFreq >= skipInterval:
-    if (ti1.docFreq >= enumerator.skipInterval &&
-        ti1.skipOffset != ti2.skipOffset) {
-      return false;
-    }
-    return true;
-  }
-
-  private void ensureIndexIsRead() {
-    if (index == null) {
-      throw new IllegalStateException("terms index was not loaded when this reader was created");
-    }
-  }
-
-  /** Returns the position of a Term in the set or -1. */
-  long getPosition(Term term) throws IOException {
-    if (size == 0) return -1;
-
-    ensureIndexIsRead();
-    int indexOffset = index.getIndexOffset(term);
-    
-    SegmentTermEnum enumerator = getThreadResources().termEnum;
-    index.seekEnum(enumerator, indexOffset);
-
-    while(compareAsUTF16(term, enumerator.term()) > 0 && enumerator.next()) {}
-
-    if (compareAsUTF16(term, enumerator.term()) == 0)
-      return enumerator.position;
-    else
-      return -1;
-  }
-
-  /** Returns an enumeration of all the Terms and TermInfos in the set. */
-  public SegmentTermEnum terms() {
-    return (SegmentTermEnum)origEnum.clone();
-  }
-
-  /** Returns an enumeration of terms starting at or after the named term. */
-  public SegmentTermEnum terms(Term term) throws IOException {
-    get(term, true);
-    return (SegmentTermEnum)getThreadResources().termEnum.clone();
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfosReaderIndex.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfosReaderIndex.java
deleted file mode 100644
index 6dd0398..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/TermInfosReaderIndex.java
+++ /dev/null
@@ -1,252 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.util.BitUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.PagedBytes.PagedBytesDataInput;
-import org.apache.lucene.util.PagedBytes.PagedBytesDataOutput;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.GrowableWriter;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * This stores a monotonically increasing set of <Term, TermInfo> pairs in an
- * index segment. Pairs are accessed either by Term or by ordinal position the
- * set. The Terms and TermInfo are actually serialized and stored into a byte
- * array and pointers to the position of each are stored in a int array.
- */
-class TermInfosReaderIndex {
-
-  private static final int MAX_PAGE_BITS = 18; // 256 KB block
-  private Term[] fields;
-  private int totalIndexInterval;
-  private Comparator<BytesRef> comparator = BytesRef.getUTF8SortedAsUTF16Comparator();
-  private final PagedBytesDataInput dataInput;
-  private final PackedInts.Reader indexToDataOffset;
-  private final int indexSize;
-  private final int skipInterval;
-
-  /**
-   * Loads the segment information at segment load time.
-   * 
-   * @param indexEnum
-   *          the term enum.
-   * @param indexDivisor
-   *          the index divisor.
-   * @param tiiFileLength
-   *          the size of the tii file, used to approximate the size of the
-   *          buffer.
-   * @param totalIndexInterval
-   *          the total index interval.
-   */
-  TermInfosReaderIndex(SegmentTermEnum indexEnum, int indexDivisor, long tiiFileLength, int totalIndexInterval) throws IOException {
-    this.totalIndexInterval = totalIndexInterval;
-    indexSize = 1 + ((int) indexEnum.size - 1) / indexDivisor;
-    skipInterval = indexEnum.skipInterval;
-    // this is only an inital size, it will be GCed once the build is complete
-    long initialSize = (long) (tiiFileLength * 1.5) / indexDivisor;
-    PagedBytes dataPagedBytes = new PagedBytes(estimatePageBits(initialSize));
-    PagedBytesDataOutput dataOutput = dataPagedBytes.getDataOutput();
-
-    GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false);
-    String currentField = null;
-    List<String> fieldStrs = new ArrayList<String>();
-    int fieldCounter = -1;
-    for (int i = 0; indexEnum.next(); i++) {
-      Term term = indexEnum.term();
-      if (currentField == null || !currentField.equals(term.field())) {
-        currentField = term.field();
-        fieldStrs.add(currentField);
-        fieldCounter++;
-      }
-      TermInfo termInfo = indexEnum.termInfo();
-      indexToTerms.set(i, dataOutput.getPosition());
-      dataOutput.writeVInt(fieldCounter);
-      dataOutput.writeString(term.text());
-      dataOutput.writeVInt(termInfo.docFreq);
-      if (termInfo.docFreq >= skipInterval) {
-        dataOutput.writeVInt(termInfo.skipOffset);
-      }
-      dataOutput.writeVLong(termInfo.freqPointer);
-      dataOutput.writeVLong(termInfo.proxPointer);
-      dataOutput.writeVLong(indexEnum.indexPointer);
-      for (int j = 1; j < indexDivisor; j++) {
-        if (!indexEnum.next()) {
-          break;
-        }
-      }
-    }
-
-    fields = new Term[fieldStrs.size()];
-    for (int i = 0; i < fields.length; i++) {
-      fields[i] = new Term(fieldStrs.get(i));
-    }
-    
-    dataPagedBytes.freeze(true);
-    dataInput = dataPagedBytes.getDataInput();
-    indexToDataOffset = indexToTerms.getMutable();
-  }
-
-  private static int estimatePageBits(long estSize) {
-    return Math.max(Math.min(64 - BitUtil.nlz(estSize), MAX_PAGE_BITS), 4);
-  }
-
-  void seekEnum(SegmentTermEnum enumerator, int indexOffset) throws IOException {
-    PagedBytesDataInput input = (PagedBytesDataInput) dataInput.clone();
-    
-    input.setPosition(indexToDataOffset.get(indexOffset));
-
-    // read the term
-    int fieldId = input.readVInt();
-    Term field = fields[fieldId];
-    Term term = new Term(field.field(), input.readString());
-
-    // read the terminfo
-    TermInfo termInfo = new TermInfo();
-    termInfo.docFreq = input.readVInt();
-    if (termInfo.docFreq >= skipInterval) {
-      termInfo.skipOffset = input.readVInt();
-    } else {
-      termInfo.skipOffset = 0;
-    }
-    termInfo.freqPointer = input.readVLong();
-    termInfo.proxPointer = input.readVLong();
-
-    long pointer = input.readVLong();
-
-    // perform the seek
-    enumerator.seek(pointer, ((long) indexOffset * totalIndexInterval) - 1, term, termInfo);
-  }
-
-  /**
-   * Binary search for the given term.
-   * 
-   * @param term
-   *          the term to locate.
-   * @throws IOException 
-   */
-  int getIndexOffset(Term term) throws IOException {
-    int lo = 0;
-    int hi = indexSize - 1;
-    PagedBytesDataInput input = (PagedBytesDataInput) dataInput.clone();
-    BytesRef scratch = new BytesRef();
-    while (hi >= lo) {
-      int mid = (lo + hi) >>> 1;
-      int delta = compareTo(term, mid, input, scratch);
-      if (delta < 0)
-        hi = mid - 1;
-      else if (delta > 0)
-        lo = mid + 1;
-      else
-        return mid;
-    }
-    return hi;
-  }
-
-  /**
-   * Gets the term at the given position.  For testing.
-   * 
-   * @param termIndex
-   *          the position to read the term from the index.
-   * @return the term.
-   * @throws IOException
-   */
-  Term getTerm(int termIndex) throws IOException {
-    PagedBytesDataInput input = (PagedBytesDataInput) dataInput.clone();
-    input.setPosition(indexToDataOffset.get(termIndex));
-
-    // read the term
-    int fieldId = input.readVInt();
-    Term field = fields[fieldId];
-    return new Term(field.field(), input.readString());
-  }
-
-  /**
-   * Returns the number of terms.
-   * 
-   * @return int.
-   */
-  int length() {
-    return indexSize;
-  }
-
-  /**
-   * The compares the given term against the term in the index specified by the
-   * term index. ie It returns negative N when term is less than index term;
-   * 
-   * @param term
-   *          the given term.
-   * @param termIndex
-   *          the index of the of term to compare.
-   * @return int.
-   * @throws IOException 
-   */
-  int compareTo(Term term, int termIndex) throws IOException {
-    return compareTo(term, termIndex, (PagedBytesDataInput) dataInput.clone(), new BytesRef());
-  }
-
-  /**
-   * Compare the fields of the terms first, and if not equals return from
-   * compare. If equal compare terms.
-   * 
-   * @param term
-   *          the term to compare.
-   * @param termIndex
-   *          the position of the term in the input to compare
-   * @param input
-   *          the input buffer.
-   * @return int.
-   * @throws IOException 
-   */
-  private int compareTo(Term term, int termIndex, PagedBytesDataInput input, BytesRef reuse) throws IOException {
-    // if term field does not equal mid's field index, then compare fields
-    // else if they are equal, compare term's string values...
-    int c = compareField(term, termIndex, input);
-    if (c == 0) {
-      reuse.length = input.readVInt();
-      reuse.grow(reuse.length);
-      input.readBytes(reuse.bytes, 0, reuse.length);
-      return comparator.compare(term.bytes(), reuse);
-    }
-    return c;
-  }
-
-  /**
-   * Compares the fields before checking the text of the terms.
-   * 
-   * @param term
-   *          the given term.
-   * @param termIndex
-   *          the term that exists in the data block.
-   * @param input
-   *          the data block.
-   * @return int.
-   * @throws IOException 
-   */
-  private int compareField(Term term, int termIndex, PagedBytesDataInput input) throws IOException {
-    input.setPosition(indexToDataOffset.get(termIndex));
-    return term.field().compareTo(fields[input.readVInt()].field());
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/package.html b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/package.html
deleted file mode 100644
index c6c96c9..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Preflex codec: supports Lucene 3.x indexes (readonly)
-</body>
-</html>
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40Codec.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40Codec.java
deleted file mode 100644
index e75e9c5..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40Codec.java
+++ /dev/null
@@ -1,101 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.DocValuesFormat;
-import org.apache.lucene.index.codecs.FieldInfosFormat;
-import org.apache.lucene.index.codecs.NormsFormat;
-import org.apache.lucene.index.codecs.StoredFieldsFormat;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.SegmentInfosFormat;
-import org.apache.lucene.index.codecs.TermVectorsFormat;
-import org.apache.lucene.index.codecs.perfield.PerFieldPostingsFormat;
-
-/**
- * Implements the Lucene 4.0 index format, with configurable per-field postings formats.
- *
- * @lucene.experimental
- */
-// NOTE: if we make largish changes in a minor release, easier to just make Lucene42Codec or whatever
-// if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
-// (it writes a minor version, etc).
-public class Lucene40Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat();
-  private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
-  private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
-  private final DocValuesFormat docValuesFormat = new Lucene40DocValuesFormat();
-  private final SegmentInfosFormat infosFormat = new Lucene40SegmentInfosFormat();
-  private final NormsFormat normsFormat = new Lucene40NormsFormat();
-  private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() {
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      return Lucene40Codec.this.getPostingsFormatForField(field);
-    }
-  };
-
-  public Lucene40Codec() {
-    super("Lucene40");
-  }
-  
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fieldsFormat;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValuesFormat;
-  }
-
-  @Override
-  public PostingsFormat postingsFormat() {
-    return postingsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-  
-  @Override
-  public SegmentInfosFormat segmentInfosFormat() {
-    return infosFormat;
-  }
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-
-  /** Returns the postings format that should be used for writing 
-   *  new segments of <code>field</code>.
-   *  
-   *  The default implementation always returns "Lucene40"
-   */
-  public PostingsFormat getPostingsFormatForField(String field) {
-    return defaultFormat;
-  }
-  
-  private final PostingsFormat defaultFormat = PostingsFormat.forName("Lucene40");
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesConsumer.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesConsumer.java
deleted file mode 100644
index fca67b6..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesConsumer.java
+++ /dev/null
@@ -1,78 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.codecs.DocValuesWriterBase;
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.Directory;
-
-/**
- * Default PerDocConsumer implementation that uses compound file.
- * @lucene.experimental
- */
-public class Lucene40DocValuesConsumer extends DocValuesWriterBase {
-  private final Directory mainDirectory;
-  private Directory directory;
-
-  final static String DOC_VALUES_SEGMENT_SUFFIX = "dv";
-  
-  public Lucene40DocValuesConsumer(PerDocWriteState state) throws IOException {
-    super(state);
-    mainDirectory = state.directory;
-    //TODO maybe we should enable a global CFS that all codecs can pull on demand to further reduce the number of files?
-  }
-  
-  @Override
-  protected Directory getDirectory() throws IOException {
-    // lazy init
-    if (directory == null) {
-      directory = new CompoundFileDirectory(mainDirectory,
-                                            IndexFileNames.segmentFileName(segmentName, DOC_VALUES_SEGMENT_SUFFIX,
-                                                                           IndexFileNames.COMPOUND_FILE_EXTENSION), context, true);
-    }
-    return directory;
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (directory != null) {
-      directory.close();
-    }
-  }
-
-  public static void files(Directory dir, SegmentInfo segmentInfo, Set<String> files) throws IOException {
-    FieldInfos fieldInfos = segmentInfo.getFieldInfos();
-    for (FieldInfo fieldInfo : fieldInfos) {
-      if (fieldInfo.hasDocValues()) {
-        files.add(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_EXTENSION));
-        files.add(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));
-        assert dir.fileExists(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION)); 
-        assert dir.fileExists(IndexFileNames.segmentFileName(segmentInfo.name, DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_EXTENSION)); 
-        break;
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesFormat.java
deleted file mode 100644
index 28bd0b2..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesFormat.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.DocValuesFormat;
-import org.apache.lucene.index.codecs.PerDocConsumer;
-import org.apache.lucene.index.codecs.PerDocProducer;
-import org.apache.lucene.store.Directory;
-
-public class Lucene40DocValuesFormat extends DocValuesFormat {
-
-  @Override
-  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new Lucene40DocValuesConsumer(state);
-  }
-
-  @Override
-  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
-    return new Lucene40DocValuesProducer(state);
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    Lucene40DocValuesConsumer.files(dir, info, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesProducer.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesProducer.java
deleted file mode 100644
index c9baf1f..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40DocValuesProducer.java
+++ /dev/null
@@ -1,75 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.DocValuesReaderBase;
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Default PerDocProducer implementation that uses compound file.
- * @lucene.experimental
- */
-public class Lucene40DocValuesProducer extends DocValuesReaderBase {
-  protected final TreeMap<String,DocValues> docValues;
-  private final Directory cfs;
-
-  /**
-   * Creates a new {@link Lucene40DocValuesProducer} instance and loads all
-   * {@link DocValues} instances for this segment and codec.
-   */
-  public Lucene40DocValuesProducer(SegmentReadState state) throws IOException {
-    if (state.fieldInfos.anyDocValuesFields()) {
-      cfs = new CompoundFileDirectory(state.dir, 
-                                      IndexFileNames.segmentFileName(state.segmentInfo.name,
-                                                                     Lucene40DocValuesConsumer.DOC_VALUES_SEGMENT_SUFFIX, IndexFileNames.COMPOUND_FILE_EXTENSION), 
-                                      state.context, false);
-      docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.docCount, cfs, state.context);
-    } else {
-      cfs = null;
-      docValues = new TreeMap<String,DocValues>();
-    }
-  }
-  
-  @Override
-  protected Map<String,DocValues> docValues() {
-    return docValues;
-  }
-
-  @Override
-  protected void closeInternal(Collection<? extends Closeable> closeables) throws IOException {
-    if (cfs != null) {
-      final ArrayList<Closeable> list = new ArrayList<Closeable>(closeables);
-      list.add(cfs);
-      IOUtils.close(list);
-    } else {
-      IOUtils.close(closeables);
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosFormat.java
deleted file mode 100644
index 6005222..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosFormat.java
+++ /dev/null
@@ -1,50 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.codecs.FieldInfosFormat;
-import org.apache.lucene.index.codecs.FieldInfosReader;
-import org.apache.lucene.index.codecs.FieldInfosWriter;
-import org.apache.lucene.store.Directory;
-
-/**
- * @lucene.experimental
- */
-public class Lucene40FieldInfosFormat extends FieldInfosFormat {
-  private final FieldInfosReader reader = new Lucene40FieldInfosReader();
-  private final FieldInfosWriter writer = new Lucene40FieldInfosWriter();
-  
-  @Override
-  public FieldInfosReader getFieldInfosReader() throws IOException {
-    return reader;
-  }
-
-  @Override
-  public FieldInfosWriter getFieldInfosWriter() throws IOException {
-    return writer;
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    Lucene40FieldInfosReader.files(dir, info, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosReader.java
deleted file mode 100644
index 04ad825..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosReader.java
+++ /dev/null
@@ -1,167 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexFormatTooNewException;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.codecs.FieldInfosReader;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @lucene.experimental
- */
-public class Lucene40FieldInfosReader extends FieldInfosReader {
-
-  static final int FORMAT_MINIMUM = Lucene40FieldInfosWriter.FORMAT_START;
-
-  @Override
-  public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", Lucene40FieldInfosWriter.FIELD_INFOS_EXTENSION);
-    IndexInput input = directory.openInput(fileName, iocontext);
-
-    boolean hasVectors = false;
-    boolean hasFreq = false;
-    boolean hasProx = false;
-    
-    try {
-      final int format = input.readVInt();
-
-      if (format > FORMAT_MINIMUM) {
-        throw new IndexFormatTooOldException(input, format, FORMAT_MINIMUM, Lucene40FieldInfosWriter.FORMAT_CURRENT);
-      }
-      if (format < Lucene40FieldInfosWriter.FORMAT_CURRENT) {
-        throw new IndexFormatTooNewException(input, format, FORMAT_MINIMUM, Lucene40FieldInfosWriter.FORMAT_CURRENT);
-      }
-
-      final int size = input.readVInt(); //read in the size
-      FieldInfo infos[] = new FieldInfo[size];
-
-      for (int i = 0; i < size; i++) {
-        String name = input.readString();
-        final int fieldNumber = format <= Lucene40FieldInfosWriter.FORMAT_FLEX? input.readInt():i;
-        byte bits = input.readByte();
-        boolean isIndexed = (bits & Lucene40FieldInfosWriter.IS_INDEXED) != 0;
-        boolean storeTermVector = (bits & Lucene40FieldInfosWriter.STORE_TERMVECTOR) != 0;
-        boolean storePositionsWithTermVector = (bits & Lucene40FieldInfosWriter.STORE_POSITIONS_WITH_TERMVECTOR) != 0;
-        boolean storeOffsetWithTermVector = (bits & Lucene40FieldInfosWriter.STORE_OFFSET_WITH_TERMVECTOR) != 0;
-        boolean omitNorms = (bits & Lucene40FieldInfosWriter.OMIT_NORMS) != 0;
-        boolean storePayloads = (bits & Lucene40FieldInfosWriter.STORE_PAYLOADS) != 0;
-        final IndexOptions indexOptions;
-        if ((bits & Lucene40FieldInfosWriter.OMIT_TERM_FREQ_AND_POSITIONS) != 0) {
-          indexOptions = IndexOptions.DOCS_ONLY;
-        } else if ((bits & Lucene40FieldInfosWriter.OMIT_POSITIONS) != 0) {
-          if (format <= Lucene40FieldInfosWriter.FORMAT_OMIT_POSITIONS) {
-            indexOptions = IndexOptions.DOCS_AND_FREQS;
-          } else {
-            throw new CorruptIndexException("Corrupt fieldinfos, OMIT_POSITIONS set but format=" + format + " (resource: " + input + ")");
-          }
-        } else {
-          indexOptions = IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        }
-
-        // LUCENE-3027: past indices were able to write
-        // storePayloads=true when omitTFAP is also true,
-        // which is invalid.  We correct that, here:
-        if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-          storePayloads = false;
-        }
-        hasVectors |= storeTermVector;
-        hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
-        DocValues.Type docValuesType = null;
-        if (format <= Lucene40FieldInfosWriter.FORMAT_FLEX) {
-          final byte b = input.readByte();
-          switch(b) {
-            case 0:
-              docValuesType = null;
-              break;
-            case 1:
-              docValuesType = DocValues.Type.VAR_INTS;
-              break;
-            case 2:
-              docValuesType = DocValues.Type.FLOAT_32;
-              break;
-            case 3:
-              docValuesType = DocValues.Type.FLOAT_64;
-              break;
-            case 4:
-              docValuesType = DocValues.Type.BYTES_FIXED_STRAIGHT;
-              break;
-            case 5:
-              docValuesType = DocValues.Type.BYTES_FIXED_DEREF;
-              break;
-            case 6:
-              docValuesType = DocValues.Type.BYTES_VAR_STRAIGHT;
-              break;
-            case 7:
-              docValuesType = DocValues.Type.BYTES_VAR_DEREF;
-              break;
-            case 8:
-              docValuesType = DocValues.Type.FIXED_INTS_16;
-              break;
-            case 9:
-              docValuesType = DocValues.Type.FIXED_INTS_32;
-              break;
-            case 10:
-              docValuesType = DocValues.Type.FIXED_INTS_64;
-              break;
-            case 11:
-              docValuesType = DocValues.Type.FIXED_INTS_8;
-              break;
-            case 12:
-              docValuesType = DocValues.Type.BYTES_FIXED_SORTED;
-              break;
-            case 13:
-              docValuesType = DocValues.Type.BYTES_VAR_SORTED;
-              break;
-        
-            default:
-              throw new IllegalStateException("unhandled indexValues type " + b);
-          }
-        }
-        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          storePositionsWithTermVector, storeOffsetWithTermVector, 
-          omitNorms, storePayloads, indexOptions, docValuesType);
-      }
-
-      if (input.getFilePointer() != input.length()) {
-        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
-      }
-      
-      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
-    } finally {
-      input.close();
-    }
-  }
-  
-  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(info.name, "", Lucene40FieldInfosWriter.FIELD_INFOS_EXTENSION));
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosWriter.java
deleted file mode 100644
index c809fd0..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40FieldInfosWriter.java
+++ /dev/null
@@ -1,137 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.codecs.FieldInfosWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-
-/**
- * @lucene.experimental
- */
-public class Lucene40FieldInfosWriter extends FieldInfosWriter {
-  
-  /** Extension of field infos */
-  static final String FIELD_INFOS_EXTENSION = "fnm";
-  
-  // First used in 2.9; prior to 2.9 there was no format header
-  static final int FORMAT_START = -2;
-  // First used in 3.4: omit only positional information
-  static final int FORMAT_OMIT_POSITIONS = -3;
-  // per-field codec support, records index values for fields
-  static final int FORMAT_FLEX = -4;
-
-  // whenever you add a new format, make it 1 smaller (negative version logic)!
-  static final int FORMAT_CURRENT = FORMAT_FLEX;
-  
-  static final byte IS_INDEXED = 0x1;
-  static final byte STORE_TERMVECTOR = 0x2;
-  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x4;
-  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x8;
-  static final byte OMIT_NORMS = 0x10;
-  static final byte STORE_PAYLOADS = 0x20;
-  static final byte OMIT_TERM_FREQ_AND_POSITIONS = 0x40;
-  static final byte OMIT_POSITIONS = -128;
-  
-  @Override
-  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
-    IndexOutput output = directory.createOutput(fileName, context);
-    try {
-      output.writeVInt(FORMAT_CURRENT);
-      output.writeVInt(infos.size());
-      for (FieldInfo fi : infos) {
-        assert fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !fi.storePayloads;
-        byte bits = 0x0;
-        if (fi.isIndexed) bits |= IS_INDEXED;
-        if (fi.storeTermVector) bits |= STORE_TERMVECTOR;
-        if (fi.storePositionWithTermVector) bits |= STORE_POSITIONS_WITH_TERMVECTOR;
-        if (fi.storeOffsetWithTermVector) bits |= STORE_OFFSET_WITH_TERMVECTOR;
-        if (fi.omitNorms) bits |= OMIT_NORMS;
-        if (fi.storePayloads) bits |= STORE_PAYLOADS;
-        if (fi.indexOptions == IndexOptions.DOCS_ONLY)
-          bits |= OMIT_TERM_FREQ_AND_POSITIONS;
-        else if (fi.indexOptions == IndexOptions.DOCS_AND_FREQS)
-          bits |= OMIT_POSITIONS;
-        output.writeString(fi.name);
-        output.writeInt(fi.number);
-        output.writeByte(bits);
-
-        final byte b;
-
-        if (!fi.hasDocValues()) {
-          b = 0;
-        } else {
-          switch(fi.getDocValuesType()) {
-          case VAR_INTS:
-            b = 1;
-            break;
-          case FLOAT_32:
-            b = 2;
-            break;
-          case FLOAT_64:
-            b = 3;
-            break;
-          case BYTES_FIXED_STRAIGHT:
-            b = 4;
-            break;
-          case BYTES_FIXED_DEREF:
-            b = 5;
-            break;
-          case BYTES_VAR_STRAIGHT:
-            b = 6;
-            break;
-          case BYTES_VAR_DEREF:
-            b = 7;
-            break;
-          case FIXED_INTS_16:
-            b = 8;
-            break;
-          case FIXED_INTS_32:
-            b = 9;
-            break;
-          case FIXED_INTS_64:
-            b = 10;
-            break;
-          case FIXED_INTS_8:
-            b = 11;
-            break;
-          case BYTES_FIXED_SORTED:
-            b = 12;
-            break;
-          case BYTES_VAR_SORTED:
-            b = 13;
-            break;
-          default:
-            throw new IllegalStateException("unhandled indexValues type " + fi.getDocValuesType());
-          }
-        }
-        output.writeByte(b);
-      }
-    } finally {
-      output.close();
-    }
-  }
-  
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40NormsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40NormsFormat.java
deleted file mode 100644
index e3a05c8..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40NormsFormat.java
+++ /dev/null
@@ -1,53 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.NormsFormat;
-import org.apache.lucene.index.codecs.NormsReader;
-import org.apache.lucene.index.codecs.NormsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-public class Lucene40NormsFormat extends NormsFormat {
-
-  @Override
-  public NormsReader normsReader(Directory dir, SegmentInfo info, FieldInfos fields, IOContext context, Directory separateNormsDir) throws IOException {
-    return new Lucene40NormsReader(dir, info, fields, context, separateNormsDir);
-  }
-
-  @Override
-  public NormsWriter normsWriter(SegmentWriteState state) throws IOException {
-    return new Lucene40NormsWriter(state.directory, state.segmentName, state.context);
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    Lucene40NormsReader.files(dir, info, files);
-  }
-
-  @Override
-  public void separateFiles(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    Lucene40NormsReader.separateFiles(dir, info, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40NormsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40NormsReader.java
deleted file mode 100644
index a02db60..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40NormsReader.java
+++ /dev/null
@@ -1,196 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.IdentityHashMap;
-import java.util.Map;
-import java.util.Set;
-import java.util.Map.Entry;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.codecs.NormsReader;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.MapBackedSet;
-import org.apache.lucene.util.StringHelper;
-
-public class Lucene40NormsReader extends NormsReader {
-  // this would be replaced by Source/SourceCache in a dv impl.
-  // for now we have our own mini-version
-  final Map<String,Norm> norms = new HashMap<String,Norm>();
-  // any .nrm or .sNN files we have open at any time.
-  // TODO: just a list, and double-close() separate norms files?
-  final Set<IndexInput> openFiles = new MapBackedSet<IndexInput>(new IdentityHashMap<IndexInput,Boolean>());
-  // points to a singleNormFile
-  IndexInput singleNormStream;
-  final int maxdoc;
-  
-  // note: just like segmentreader in 3.x, we open up all the files here (including separate norms) up front.
-  // but we just don't do any seeks or reading yet.
-  public Lucene40NormsReader(Directory dir, SegmentInfo info, FieldInfos fields, IOContext context, Directory separateNormsDir) throws IOException {
-    maxdoc = info.docCount;
-    String segmentName = info.name;
-    Map<Integer,Long> normGen = info.getNormGen();
-    boolean success = false;
-    try {
-      long nextNormSeek = Lucene40NormsWriter.NORMS_HEADER.length; //skip header (header unused for now)
-      for (FieldInfo fi : fields) {
-        if (fi.isIndexed && !fi.omitNorms) {
-          String fileName = getNormFilename(segmentName, normGen, fi.number);
-          Directory d = hasSeparateNorms(normGen, fi.number) ? separateNormsDir : dir;
-        
-          // singleNormFile means multiple norms share this file
-          boolean singleNormFile = IndexFileNames.matchesExtension(fileName, Lucene40NormsWriter.NORMS_EXTENSION);
-          IndexInput normInput = null;
-          long normSeek;
-
-          if (singleNormFile) {
-            normSeek = nextNormSeek;
-            if (singleNormStream == null) {
-              singleNormStream = d.openInput(fileName, context);
-              openFiles.add(singleNormStream);
-            }
-            // All norms in the .nrm file can share a single IndexInput since
-            // they are only used in a synchronized context.
-            // If this were to change in the future, a clone could be done here.
-            normInput = singleNormStream;
-          } else {
-            normInput = d.openInput(fileName, context);
-            openFiles.add(normInput);
-            // if the segment was created in 3.2 or after, we wrote the header for sure,
-            // and don't need to do the sketchy file size check. otherwise, we check 
-            // if the size is exactly equal to maxDoc to detect a headerless file.
-            // NOTE: remove this check in Lucene 5.0!
-            String version = info.getVersion();
-            final boolean isUnversioned = 
-                (version == null || StringHelper.getVersionComparator().compare(version, "3.2") < 0)
-                && normInput.length() == maxdoc;
-            if (isUnversioned) {
-              normSeek = 0;
-            } else {
-              normSeek = Lucene40NormsWriter.NORMS_HEADER.length;
-            }
-          }
-
-          Norm norm = new Norm();
-          norm.file = normInput;
-          norm.offset = normSeek;
-          norms.put(fi.name, norm);
-          nextNormSeek += maxdoc; // increment also if some norms are separate
-        }
-      }
-      // TODO: change to a real check? see LUCENE-3619
-      assert singleNormStream == null || nextNormSeek == singleNormStream.length();
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(openFiles);
-      }
-    }
-  }
-  
-  @Override
-  public byte[] norms(String name) throws IOException {
-    Norm norm = norms.get(name);
-    return norm == null ? null : norm.bytes();
-  }
-  
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(openFiles);
-    } finally {
-      norms.clear();
-      openFiles.clear();
-    }
-  }
-  
-  private static String getNormFilename(String segmentName, Map<Integer,Long> normGen, int number) {
-    if (hasSeparateNorms(normGen, number)) {
-      return IndexFileNames.fileNameFromGeneration(segmentName, Lucene40NormsWriter.SEPARATE_NORMS_EXTENSION + number, normGen.get(number));
-    } else {
-      // single file for all norms
-      return IndexFileNames.fileNameFromGeneration(segmentName, Lucene40NormsWriter.NORMS_EXTENSION, SegmentInfo.WITHOUT_GEN);
-    }
-  }
-  
-  private static boolean hasSeparateNorms(Map<Integer,Long> normGen, int number) {
-    if (normGen == null) {
-      return false;
-    }
-
-    Long gen = normGen.get(number);
-    return gen != null && gen.longValue() != SegmentInfo.NO;
-  }
-  
-  class Norm {
-    IndexInput file;
-    long offset;
-    byte bytes[];
-    
-    synchronized byte[] bytes() throws IOException {
-      if (bytes == null) {
-        bytes = new byte[maxdoc];
-        // some norms share fds
-        synchronized(file) {
-          file.seek(offset);
-          file.readBytes(bytes, 0, bytes.length, false);
-        }
-        // we are done with this file
-        if (file != singleNormStream) {
-          openFiles.remove(file);
-          file.close();
-          file = null;
-        }
-      }
-      return bytes;
-    }
-  }
-  
-  static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    // TODO: This is what SI always did... but we can do this cleaner?
-    // like first FI that has norms but doesn't have separate norms?
-    final String normsFileName = IndexFileNames.segmentFileName(info.name, "", Lucene40NormsWriter.NORMS_EXTENSION);
-    if (dir.fileExists(normsFileName)) {
-      files.add(normsFileName);
-    }
-  }
-  
-  /** @deprecated */
-  @Deprecated
-  static void separateFiles(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    Map<Integer,Long> normGen = info.getNormGen();
-    if (normGen != null) {
-      for (Entry<Integer,Long> entry : normGen.entrySet()) {
-        long gen = entry.getValue();
-        if (gen >= SegmentInfo.YES) {
-          // Definitely a separate norm file, with generation:
-          files.add(IndexFileNames.fileNameFromGeneration(info.name, Lucene40NormsWriter.SEPARATE_NORMS_EXTENSION + entry.getKey(), gen));
-        }
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40NormsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40NormsWriter.java
deleted file mode 100644
index 8bb5474..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40NormsWriter.java
+++ /dev/null
@@ -1,130 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.codecs.NormsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.IOUtils;
-
-public class Lucene40NormsWriter extends NormsWriter {
-  private IndexOutput out;
-  private int normCount = 0;
-  
-  /** norms header placeholder */
-  static final byte[] NORMS_HEADER = new byte[]{'N','R','M',-1};
-  
-  /** Extension of norms file */
-  static final String NORMS_EXTENSION = "nrm";
-  
-  /** Extension of separate norms file
-   * @deprecated */
-  @Deprecated
-  static final String SEPARATE_NORMS_EXTENSION = "s";
-  
-  public Lucene40NormsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    final String normsFileName = IndexFileNames.segmentFileName(segment, "", NORMS_EXTENSION);
-    boolean success = false;
-    try {
-      out = directory.createOutput(normsFileName, context);
-      out.writeBytes(NORMS_HEADER, 0, NORMS_HEADER.length);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-
-  @Override
-  public void startField(FieldInfo info) throws IOException {
-    assert info.omitNorms == false;
-    normCount++;
-  }
-  
-  @Override
-  public void writeNorm(byte norm) throws IOException {
-    out.writeByte(norm);
-  }
-  
-  @Override
-  public void finish(int numDocs) throws IOException {
-    if (4+normCount*(long)numDocs != out.getFilePointer()) {
-      throw new RuntimeException(".nrm file size mismatch: expected=" + (4+normCount*(long)numDocs) + " actual=" + out.getFilePointer());
-    }
-  }
-  
-  /** we override merge and bulk-merge norms when there are no deletions */
-  @Override
-  public int merge(MergeState mergeState) throws IOException {
-    int numMergedDocs = 0;
-    for (FieldInfo fi : mergeState.fieldInfos) {
-      if (fi.isIndexed && !fi.omitNorms) {
-        startField(fi);
-        int numMergedDocsForField = 0;
-        for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
-          final int maxDoc = reader.reader.maxDoc();
-          byte normBuffer[] = reader.reader.norms(fi.name);
-          if (normBuffer == null) {
-            // Can be null if this segment doesn't have
-            // any docs with this field
-            normBuffer = new byte[maxDoc];
-            Arrays.fill(normBuffer, (byte)0);
-          }
-          if (reader.liveDocs == null) {
-            //optimized case for segments without deleted docs
-            out.writeBytes(normBuffer, maxDoc);
-            numMergedDocsForField += maxDoc;
-          } else {
-            // this segment has deleted docs, so we have to
-            // check for every doc if it is deleted or not
-            final Bits liveDocs = reader.liveDocs;
-            for (int k = 0; k < maxDoc; k++) {
-              if (liveDocs.get(k)) {
-                numMergedDocsForField++;
-                out.writeByte(normBuffer[k]);
-              }
-            }
-          }
-          mergeState.checkAbort.work(maxDoc);
-        }
-        assert numMergedDocs == 0 || numMergedDocs == numMergedDocsForField;
-        numMergedDocs = numMergedDocsForField;
-      }
-    }
-    finish(numMergedDocs);
-    return numMergedDocs;
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(out);
-    } finally {
-      out = null;
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsBaseFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsBaseFormat.java
deleted file mode 100644
index edd4a53..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsBaseFormat.java
+++ /dev/null
@@ -1,58 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.PostingsBaseFormat;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.store.Directory;
-
-/** 
- * Provides a {@link PostingsReaderBase} and {@link
- * PostingsWriterBase}.
- *
- * @lucene.experimental */
-
-// TODO: should these also be named / looked up via SPI?
-public final class Lucene40PostingsBaseFormat extends PostingsBaseFormat {
-
-  public Lucene40PostingsBaseFormat() {
-    super("Lucene40");
-  }
-
-  @Override
-  public PostingsReaderBase postingsReaderBase(SegmentReadState state) throws IOException {
-    return new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
-  }
-
-  @Override
-  public PostingsWriterBase postingsWriterBase(SegmentWriteState state) throws IOException {
-    return new Lucene40PostingsWriter(state);
-  }
-  
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsFormat.java
deleted file mode 100644
index 9104acb..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsFormat.java
+++ /dev/null
@@ -1,118 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.BlockTreeTermsReader;
-import org.apache.lucene.index.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.store.Directory;
-
-/** Default codec. 
- *  @lucene.experimental */
-
-// TODO: this class could be created by wrapping
-// BlockTreeTermsDict around Lucene40PostingsBaseFormat; ie
-// we should not duplicate the code from that class here:
-public class Lucene40PostingsFormat extends PostingsFormat {
-
-  private final int minBlockSize;
-  private final int maxBlockSize;
-
-  public Lucene40PostingsFormat() {
-    this(BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-  }
-
-  public Lucene40PostingsFormat(int minBlockSize, int maxBlockSize) {
-    super("Lucene40");
-    this.minBlockSize = minBlockSize;
-    assert minBlockSize > 1;
-    this.maxBlockSize = maxBlockSize;
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docs = new Lucene40PostingsWriter(state);
-
-    // TODO: should we make the terms index more easily
-    // pluggable?  Ie so that this codec would record which
-    // index impl was used, and switch on loading?
-    // Or... you must make a new Codec for this?
-    boolean success = false;
-    try {
-      FieldsConsumer ret = new BlockTreeTermsWriter(state, docs, minBlockSize, maxBlockSize);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        docs.close();
-      }
-    }
-  }
-
-  public final static int TERMS_CACHE_SIZE = 1024;
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
-
-    boolean success = false;
-    try {
-      FieldsProducer ret = new BlockTreeTermsReader(
-                                                    state.dir,
-                                                    state.fieldInfos,
-                                                    state.segmentInfo.name,
-                                                    postings,
-                                                    state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        postings.close();
-      }
-    }
-  }
-
-  /** Extension of freq postings file */
-  static final String FREQ_EXTENSION = "frq";
-
-  /** Extension of prox postings file */
-  static final String PROX_EXTENSION = "prx";
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
-    BlockTreeTermsReader.files(dir, segmentInfo, segmentSuffix, files);
-  }
-
-  @Override
-  public String toString() {
-    return getName() + "(minBlockSize=" + minBlockSize + " maxBlockSize=" + maxBlockSize + ")";
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsReader.java
deleted file mode 100644
index 75d3f0a..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsReader.java
+++ /dev/null
@@ -1,1102 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.BlockTermState;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-
-/** Concrete class that reads the current doc/freq/skip
- *  postings format. 
- *  @lucene.experimental */
-
-public class Lucene40PostingsReader extends PostingsReaderBase {
-
-  private final IndexInput freqIn;
-  private final IndexInput proxIn;
-  // public static boolean DEBUG = BlockTreeTermsWriter.DEBUG;
-
-  int skipInterval;
-  int maxSkipLevels;
-  int skipMinimum;
-
-  // private String segment;
-
-  public Lucene40PostingsReader(Directory dir, SegmentInfo segmentInfo, IOContext ioContext, String segmentSuffix) throws IOException {
-    freqIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION),
-                           ioContext);
-    // this.segment = segmentInfo.name;
-    if (segmentInfo.getHasProx()) {
-      boolean success = false;
-      try {
-        proxIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION),
-                               ioContext);
-        success = true;
-      } finally {
-        if (!success) {
-          freqIn.close();
-        }
-      }
-    } else {
-      proxIn = null;
-    }
-  }
-
-  public static void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION));
-    if (segmentInfo.getHasProx()) {
-      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION));
-    }
-  }
-
-  @Override
-  public void init(IndexInput termsIn) throws IOException {
-
-    // Make sure we are talking to the matching past writer
-    CodecUtil.checkHeader(termsIn, Lucene40PostingsWriter.CODEC,
-      Lucene40PostingsWriter.VERSION_START, Lucene40PostingsWriter.VERSION_START);
-
-    skipInterval = termsIn.readInt();
-    maxSkipLevels = termsIn.readInt();
-    skipMinimum = termsIn.readInt();
-  }
-
-  // Must keep final because we do non-standard clone
-  private final static class StandardTermState extends BlockTermState {
-    long freqOffset;
-    long proxOffset;
-    int skipOffset;
-
-    // Only used by the "primary" TermState -- clones don't
-    // copy this (basically they are "transient"):
-    ByteArrayDataInput bytesReader;  // TODO: should this NOT be in the TermState...?
-    byte[] bytes;
-
-    @Override
-    public Object clone() {
-      StandardTermState other = new StandardTermState();
-      other.copyFrom(this);
-      return other;
-    }
-
-    @Override
-    public void copyFrom(TermState _other) {
-      super.copyFrom(_other);
-      StandardTermState other = (StandardTermState) _other;
-      freqOffset = other.freqOffset;
-      proxOffset = other.proxOffset;
-      skipOffset = other.skipOffset;
-
-      // Do not copy bytes, bytesReader (else TermState is
-      // very heavy, ie drags around the entire block's
-      // byte[]).  On seek back, if next() is in fact used
-      // (rare!), they will be re-read from disk.
-    }
-
-    @Override
-    public String toString() {
-      return super.toString() + " freqFP=" + freqOffset + " proxFP=" + proxOffset + " skipOffset=" + skipOffset;
-    }
-  }
-
-  @Override
-  public BlockTermState newTermState() {
-    return new StandardTermState();
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      if (freqIn != null) {
-        freqIn.close();
-      }
-    } finally {
-      if (proxIn != null) {
-        proxIn.close();
-      }
-    }
-  }
-
-  /* Reads but does not decode the byte[] blob holding
-     metadata for the current terms block */
-  @Override
-  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
-    final StandardTermState termState = (StandardTermState) _termState;
-
-    final int len = termsIn.readVInt();
-
-    // if (DEBUG) System.out.println("  SPR.readTermsBlock bytes=" + len + " ts=" + _termState);
-    if (termState.bytes == null) {
-      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
-      termState.bytesReader = new ByteArrayDataInput();
-    } else if (termState.bytes.length < len) {
-      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
-    }
-
-    termsIn.readBytes(termState.bytes, 0, len);
-    termState.bytesReader.reset(termState.bytes, 0, len);
-  }
-
-  @Override
-  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState)
-    throws IOException {
-    final StandardTermState termState = (StandardTermState) _termState;
-    // if (DEBUG) System.out.println("SPR: nextTerm seg=" + segment + " tbOrd=" + termState.termBlockOrd + " bytesReader.fp=" + termState.bytesReader.getPosition());
-    final boolean isFirstTerm = termState.termBlockOrd == 0;
-
-    if (isFirstTerm) {
-      termState.freqOffset = termState.bytesReader.readVLong();
-    } else {
-      termState.freqOffset += termState.bytesReader.readVLong();
-    }
-    /*
-    if (DEBUG) {
-      System.out.println("  dF=" + termState.docFreq);
-      System.out.println("  freqFP=" + termState.freqOffset);
-    }
-    */
-    assert termState.freqOffset < freqIn.length();
-
-    if (termState.docFreq >= skipMinimum) {
-      termState.skipOffset = termState.bytesReader.readVInt();
-      // if (DEBUG) System.out.println("  skipOffset=" + termState.skipOffset + " vs freqIn.length=" + freqIn.length());
-      assert termState.freqOffset + termState.skipOffset < freqIn.length();
-    } else {
-      // undefined
-    }
-
-    if (fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      if (isFirstTerm) {
-        termState.proxOffset = termState.bytesReader.readVLong();
-      } else {
-        termState.proxOffset += termState.bytesReader.readVLong();
-      }
-      // if (DEBUG) System.out.println("  proxFP=" + termState.proxOffset);
-    }
-  }
-    
-  @Override
-  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-    if (needsFreqs && fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
-      return null;
-    } else if (canReuse(reuse, liveDocs)) {
-      // if (DEBUG) System.out.println("SPR.docs ts=" + termState);
-      return ((SegmentDocsEnumBase) reuse).reset(fieldInfo, (StandardTermState)termState);
-    }
-    return newDocsEnum(liveDocs, fieldInfo, (StandardTermState)termState);
-  }
-  
-  private boolean canReuse(DocsEnum reuse, Bits liveDocs) {
-    if (reuse != null && (reuse instanceof SegmentDocsEnumBase)) {
-      SegmentDocsEnumBase docsEnum = (SegmentDocsEnumBase) reuse;
-      // If you are using ParellelReader, and pass in a
-      // reused DocsEnum, it could have come from another
-      // reader also using standard codec
-      if (docsEnum.startFreqIn == freqIn) {
-        // we only reuse if the the actual the incoming enum has the same liveDocs as the given liveDocs
-        return liveDocs == docsEnum.liveDocs;
-      }
-    }
-    return false;
-  }
-  
-  private DocsEnum newDocsEnum(Bits liveDocs, FieldInfo fieldInfo, StandardTermState termState) throws IOException {
-    if (liveDocs == null) {
-      return new AllDocsSegmentDocsEnum(freqIn).reset(fieldInfo, termState);
-    } else {
-      return new LiveDocsSegmentDocsEnum(freqIn, liveDocs).reset(fieldInfo, termState);
-    }
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
-    if (fieldInfo.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      return null;
-    }
-    
-    // TODO: refactor
-    if (fieldInfo.storePayloads) {
-      SegmentDocsAndPositionsAndPayloadsEnum docsEnum;
-      if (reuse == null || !(reuse instanceof SegmentDocsAndPositionsAndPayloadsEnum)) {
-        docsEnum = new SegmentDocsAndPositionsAndPayloadsEnum(freqIn, proxIn);
-      } else {
-        docsEnum = (SegmentDocsAndPositionsAndPayloadsEnum) reuse;
-        if (docsEnum.startFreqIn != freqIn) {
-          // If you are using ParellelReader, and pass in a
-          // reused DocsEnum, it could have come from another
-          // reader also using standard codec
-          docsEnum = new SegmentDocsAndPositionsAndPayloadsEnum(freqIn, proxIn);
-        }
-      }
-      return docsEnum.reset(fieldInfo, (StandardTermState) termState, liveDocs);
-    } else {
-      SegmentDocsAndPositionsEnum docsEnum;
-      if (reuse == null || !(reuse instanceof SegmentDocsAndPositionsEnum)) {
-        docsEnum = new SegmentDocsAndPositionsEnum(freqIn, proxIn);
-      } else {
-        docsEnum = (SegmentDocsAndPositionsEnum) reuse;
-        if (docsEnum.startFreqIn != freqIn) {
-          // If you are using ParellelReader, and pass in a
-          // reused DocsEnum, it could have come from another
-          // reader also using standard codec
-          docsEnum = new SegmentDocsAndPositionsEnum(freqIn, proxIn);
-        }
-      }
-      return docsEnum.reset(fieldInfo, (StandardTermState) termState, liveDocs);
-    }
-  }
-
-  static final int BUFFERSIZE = 64;
-  
-  private abstract class SegmentDocsEnumBase extends DocsEnum {
-    
-    protected final int[] docs = new int[BUFFERSIZE];
-    protected final int[] freqs = new int[BUFFERSIZE];
-    
-    final IndexInput freqIn; // reuse
-    final IndexInput startFreqIn; // reuse
-    Lucene40SkipListReader skipper; // reuse - lazy loaded
-    
-    protected boolean indexOmitsTF;                               // does current field omit term freq?
-    protected boolean storePayloads;                        // does current field store payloads?
-
-    protected int limit;                                    // number of docs in this posting
-    protected int ord;                                      // how many docs we've read
-    protected int doc;                                 // doc we last read
-    protected int accum;                                    // accumulator for doc deltas
-    protected int freq;                                     // freq we last read
-    protected int maxBufferedDocId;
-    
-    protected int start;
-    protected int count;
-
-
-    protected long freqOffset;
-    protected int skipOffset;
-
-    protected boolean skipped;
-    protected final Bits liveDocs;
-    
-    SegmentDocsEnumBase(IndexInput startFreqIn, Bits liveDocs) throws IOException {
-      this.startFreqIn = startFreqIn;
-      this.freqIn = (IndexInput)startFreqIn.clone();
-      this.liveDocs = liveDocs;
-      
-    }
-    
-    
-    DocsEnum reset(FieldInfo fieldInfo, StandardTermState termState) throws IOException {
-      indexOmitsTF = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY;
-      storePayloads = fieldInfo.storePayloads;
-      freqOffset = termState.freqOffset;
-      skipOffset = termState.skipOffset;
-
-      // TODO: for full enum case (eg segment merging) this
-      // seek is unnecessary; maybe we can avoid in such
-      // cases
-      freqIn.seek(termState.freqOffset);
-      limit = termState.docFreq;
-      assert limit > 0;
-      ord = 0;
-      doc = -1;
-      accum = 0;
-      // if (DEBUG) System.out.println("  sde limit=" + limit + " freqFP=" + freqOffset);
-      skipped = false;
-
-      start = -1;
-      count = 0;
-      maxBufferedDocId = -1;
-      return this;
-    }
-    
-    @Override
-    public final int freq() {
-      assert !indexOmitsTF;
-      return freq;
-    }
-
-    @Override
-    public final int docID() {
-      return doc;
-    }
-    
-    @Override
-    public final int advance(int target) throws IOException {
-      // last doc in our buffer is >= target, binary search + next()
-      if (++start < count && maxBufferedDocId >= target) {
-        if ((count-start) > 32) { // 32 seemed to be a sweetspot here so use binsearch if the pending results are a lot
-          start = binarySearch(count - 1, start, target, docs);
-          return nextDoc();
-        } else {
-          return linearScan(target);
-        }
-      }
-      
-      start = count; // buffer is consumed
-      
-      return doc = skipTo(target, liveDocs);
-    }
-    
-    private final int binarySearch(int hi, int low, int target, int[] docs) {
-      while (low <= hi) {
-        int mid = (hi + low) >>> 1;
-        int doc = docs[mid];
-        if (doc < target) {
-          low = mid + 1;
-        } else if (doc > target) {
-          hi = mid - 1;
-        } else {
-          low = mid;
-          break;
-        }
-      }
-      return low-1;
-    }
-    
-    final int readFreq(final IndexInput freqIn, final int code)
-        throws IOException {
-      if ((code & 1) != 0) { // if low bit is set
-        return 1; // freq is one
-      } else {
-        return freqIn.readVInt(); // else read freq
-      }
-    }
-    
-    protected abstract int linearScan(int scanTo) throws IOException;
-    
-    protected abstract int scanTo(int target) throws IOException;
-
-    protected final int refill() throws IOException {
-      final int doc = nextUnreadDoc();
-      count = 0;
-      start = -1;
-      if (doc == NO_MORE_DOCS) {
-        return NO_MORE_DOCS;
-      }
-      final int numDocs = Math.min(docs.length, limit - ord);
-      ord += numDocs;
-      if (indexOmitsTF) {
-        count = fillDocs(numDocs);
-      } else {
-        count = fillDocsAndFreqs(numDocs);
-      }
-      maxBufferedDocId = count > 0 ? docs[count-1] : NO_MORE_DOCS;
-      return doc;
-    }
-    
-
-    protected abstract int nextUnreadDoc() throws IOException;
-
-
-    private final int fillDocs(int size) throws IOException {
-      final IndexInput freqIn = this.freqIn;
-      final int docs[] = this.docs;
-      int docAc = accum;
-      for (int i = 0; i < size; i++) {
-        docAc += freqIn.readVInt();
-        docs[i] = docAc;
-      }
-      accum = docAc;
-      return size;
-    }
-    
-    private final int fillDocsAndFreqs(int size) throws IOException {
-      final IndexInput freqIn = this.freqIn;
-      final int docs[] = this.docs;
-      final int freqs[] = this.freqs;
-      int docAc = accum;
-      for (int i = 0; i < size; i++) {
-        final int code = freqIn.readVInt();
-        docAc += code >>> 1; // shift off low bit
-        freqs[i] = readFreq(freqIn, code);
-        docs[i] = docAc;
-      }
-      accum = docAc;
-      return size;
-     
-    }
-
-    private final int skipTo(int target, Bits liveDocs) throws IOException {
-      if ((target - skipInterval) >= accum && limit >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and it isn't too close.
-
-        if (skipper == null) {
-          // This is the first time this enum has ever been used for skipping -- do lazy init
-          skipper = new Lucene40SkipListReader((IndexInput) freqIn.clone(), maxSkipLevels, skipInterval);
-        }
-
-        if (!skipped) {
-
-          // This is the first time this posting has
-          // skipped since reset() was called, so now we
-          // load the skip data for this posting
-
-          skipper.init(freqOffset + skipOffset,
-                       freqOffset, 0,
-                       limit, storePayloads);
-
-          skipped = true;
-        }
-
-        final int newOrd = skipper.skipTo(target); 
-
-        if (newOrd > ord) {
-          // Skipper moved
-
-          ord = newOrd;
-          accum = skipper.getDoc();
-          freqIn.seek(skipper.getFreqPointer());
-        }
-      }
-      return scanTo(target);
-    }
-  }
-  
-  private final class AllDocsSegmentDocsEnum extends SegmentDocsEnumBase {
-
-    AllDocsSegmentDocsEnum(IndexInput startFreqIn) throws IOException {
-      super(startFreqIn, null);
-      assert liveDocs == null;
-    }
-    
-    @Override
-    public final int nextDoc() throws IOException {
-      if (++start < count) {
-        freq = freqs[start];
-        return doc = docs[start];
-      }
-      return doc = refill();
-    }
-    
-
-    @Override
-    protected final int linearScan(int scanTo) throws IOException {
-      final int[] docs = this.docs;
-      final int upTo = count;
-      for (int i = start; i < upTo; i++) {
-        final int d = docs[i];
-        if (scanTo <= d) {
-          start = i;
-          freq = freqs[i];
-          return doc = docs[i];
-        }
-      }
-      return refill();
-    }
-
-    @Override
-    protected int scanTo(int target) throws IOException { 
-      int docAcc = accum;
-      int frq = 1;
-      final IndexInput freqIn = this.freqIn;
-      final boolean omitTF = indexOmitsTF;
-      final int loopLimit = limit;
-      for (int i = ord; i < loopLimit; i++) {
-        int code = freqIn.readVInt();
-        if (omitTF) {
-          docAcc += code;
-        } else {
-          docAcc += code >>> 1; // shift off low bit
-          frq = readFreq(freqIn, code);
-        }
-        if (docAcc >= target) {
-          freq = frq;
-          ord = i + 1;
-          return accum = docAcc;
-        }
-      }
-      ord = limit;
-      freq = frq;
-      accum = docAcc;
-      return NO_MORE_DOCS;
-    }
-
-    @Override
-    protected final int nextUnreadDoc() throws IOException {
-      if (ord++ < limit) {
-        int code = freqIn.readVInt();
-        if (indexOmitsTF) {
-          accum += code;
-        } else {
-          accum += code >>> 1; // shift off low bit
-          freq = readFreq(freqIn, code);
-        }
-        return accum;
-      } else {
-        return NO_MORE_DOCS;
-      }
-    }
-    
-  }
-  
-  private final class LiveDocsSegmentDocsEnum extends SegmentDocsEnumBase {
-
-    LiveDocsSegmentDocsEnum(IndexInput startFreqIn, Bits liveDocs) throws IOException {
-      super(startFreqIn, liveDocs);
-      assert liveDocs != null;
-    }
-    
-    @Override
-    public final int nextDoc() throws IOException {
-      final Bits liveDocs = this.liveDocs;
-      for (int i = start+1; i < count; i++) {
-        int d = docs[i];
-        if (liveDocs.get(d)) {
-          start = i;
-          freq = freqs[i];
-          return doc = d;
-        }
-      }
-      start = count;
-      return doc = refill();
-    }
-
-    @Override
-    protected final int linearScan(int scanTo) throws IOException {
-      final int[] docs = this.docs;
-      final int upTo = count;
-      final Bits liveDocs = this.liveDocs;
-      for (int i = start; i < upTo; i++) {
-        int d = docs[i];
-        if (scanTo <= d && liveDocs.get(d)) {
-          start = i;
-          freq = freqs[i];
-          return doc = docs[i];
-        }
-      }
-      return refill();
-    }
-    
-    @Override
-    protected int scanTo(int target) throws IOException { 
-      int docAcc = accum;
-      int frq = 1;
-      final IndexInput freqIn = this.freqIn;
-      final boolean omitTF = indexOmitsTF;
-      final int loopLimit = limit;
-      final Bits liveDocs = this.liveDocs;
-      for (int i = ord; i < loopLimit; i++) {
-        int code = freqIn.readVInt();
-        if (omitTF) {
-          docAcc += code;
-        } else {
-          docAcc += code >>> 1; // shift off low bit
-          frq = readFreq(freqIn, code);
-        }
-        if (docAcc >= target && liveDocs.get(docAcc)) {
-          freq = frq;
-          ord = i + 1;
-          return accum = docAcc;
-        }
-      }
-      ord = limit;
-      freq = frq;
-      accum = docAcc;
-      return NO_MORE_DOCS;
-    }
-
-    @Override
-    protected final int nextUnreadDoc() throws IOException {
-      int docAcc = accum;
-      int frq = 1;
-      final IndexInput freqIn = this.freqIn;
-      final boolean omitTF = indexOmitsTF;
-      final int loopLimit = limit;
-      final Bits liveDocs = this.liveDocs;
-      for (int i = ord; i < loopLimit; i++) {
-        int code = freqIn.readVInt();
-        if (omitTF) {
-          docAcc += code;
-        } else {
-          docAcc += code >>> 1; // shift off low bit
-          frq = readFreq(freqIn, code);
-        }
-        if (liveDocs.get(docAcc)) {
-          freq = frq;
-          ord = i + 1;
-          return accum = docAcc;
-        }
-      }
-      ord = limit;
-      freq = frq;
-      accum = docAcc;
-      return NO_MORE_DOCS;
-      
-    }
-  }
-  
-  // TODO specialize DocsAndPosEnum too
-  
-  // Decodes docs & positions. payloads are not present.
-  private final class SegmentDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    final IndexInput startFreqIn;
-    private final IndexInput freqIn;
-    private final IndexInput proxIn;
-    int limit;                                    // number of docs in this posting
-    int ord;                                      // how many docs we've read
-    int doc = -1;                                 // doc we last read
-    int accum;                                    // accumulator for doc deltas
-    int freq;                                     // freq we last read
-    int position;
-
-    Bits liveDocs;
-
-    long freqOffset;
-    int skipOffset;
-    long proxOffset;
-
-    int posPendingCount;
-
-    boolean skipped;
-    Lucene40SkipListReader skipper;
-    private long lazyProxPointer;
-
-    public SegmentDocsAndPositionsEnum(IndexInput freqIn, IndexInput proxIn) throws IOException {
-      startFreqIn = freqIn;
-      this.freqIn = (IndexInput) freqIn.clone();
-      this.proxIn = (IndexInput) proxIn.clone();
-    }
-
-    public SegmentDocsAndPositionsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
-      assert fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-      assert !fieldInfo.storePayloads;
-
-      this.liveDocs = liveDocs;
-
-      // TODO: for full enum case (eg segment merging) this
-      // seek is unnecessary; maybe we can avoid in such
-      // cases
-      freqIn.seek(termState.freqOffset);
-      lazyProxPointer = termState.proxOffset;
-
-      limit = termState.docFreq;
-      assert limit > 0;
-
-      ord = 0;
-      doc = -1;
-      accum = 0;
-      position = 0;
-
-      skipped = false;
-      posPendingCount = 0;
-
-      freqOffset = termState.freqOffset;
-      proxOffset = termState.proxOffset;
-      skipOffset = termState.skipOffset;
-      // if (DEBUG) System.out.println("StandardR.D&PE reset seg=" + segment + " limit=" + limit + " freqFP=" + freqOffset + " proxFP=" + proxOffset);
-
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      // if (DEBUG) System.out.println("SPR.nextDoc seg=" + segment + " freqIn.fp=" + freqIn.getFilePointer());
-      while(true) {
-        if (ord == limit) {
-          // if (DEBUG) System.out.println("  return END");
-          return doc = NO_MORE_DOCS;
-        }
-
-        ord++;
-
-        // Decode next doc/freq pair
-        final int code = freqIn.readVInt();
-
-        accum += code >>> 1;              // shift off low bit
-        if ((code & 1) != 0) {          // if low bit is set
-          freq = 1;                     // freq is one
-        } else {
-          freq = freqIn.readVInt();     // else read freq
-        }
-        posPendingCount += freq;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          break;
-        }
-      }
-
-      position = 0;
-
-      // if (DEBUG) System.out.println("  return doc=" + doc);
-      return (doc = accum);
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int freq() {
-      return freq;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-
-      //System.out.println("StandardR.D&PE advance target=" + target);
-
-      if ((target - skipInterval) >= doc && limit >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and it isn't too close
-
-        if (skipper == null) {
-          // This is the first time this enum has ever been used for skipping -- do lazy init
-          skipper = new Lucene40SkipListReader((IndexInput) freqIn.clone(), maxSkipLevels, skipInterval);
-        }
-
-        if (!skipped) {
-
-          // This is the first time this posting has
-          // skipped, since reset() was called, so now we
-          // load the skip data for this posting
-
-          skipper.init(freqOffset+skipOffset,
-                       freqOffset, proxOffset,
-                       limit, false);
-
-          skipped = true;
-        }
-
-        final int newOrd = skipper.skipTo(target); 
-
-        if (newOrd > ord) {
-          // Skipper moved
-          ord = newOrd;
-          doc = accum = skipper.getDoc();
-          freqIn.seek(skipper.getFreqPointer());
-          lazyProxPointer = skipper.getProxPointer();
-          posPendingCount = 0;
-          position = 0;
-        }
-      }
-        
-      // Now, linear scan for the rest:
-      do {
-        nextDoc();
-      } while (target > doc);
-
-      return doc;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-
-      if (lazyProxPointer != -1) {
-        proxIn.seek(lazyProxPointer);
-        lazyProxPointer = -1;
-      }
-
-      // scan over any docs that were iterated without their positions
-      if (posPendingCount > freq) {
-        position = 0;
-        while(posPendingCount != freq) {
-          if ((proxIn.readByte() & 0x80) == 0) {
-            posPendingCount--;
-          }
-        }
-      }
-
-      position += proxIn.readVInt();
-
-      posPendingCount--;
-
-      assert posPendingCount >= 0: "nextPosition() was called too many times (more than freq() times) posPendingCount=" + posPendingCount;
-
-      return position;
-    }
-
-    /** Returns the payload at this position, or null if no
-     *  payload was indexed. */
-    @Override
-    public BytesRef getPayload() throws IOException {
-      throw new IOException("No payloads exist for this field!");
-    }
-
-    @Override
-    public boolean hasPayload() {
-      return false;
-    }
-  }
-  
-  // Decodes docs & positions & payloads
-  private class SegmentDocsAndPositionsAndPayloadsEnum extends DocsAndPositionsEnum {
-    final IndexInput startFreqIn;
-    private final IndexInput freqIn;
-    private final IndexInput proxIn;
-
-    int limit;                                    // number of docs in this posting
-    int ord;                                      // how many docs we've read
-    int doc = -1;                                 // doc we last read
-    int accum;                                    // accumulator for doc deltas
-    int freq;                                     // freq we last read
-    int position;
-
-    Bits liveDocs;
-
-    long freqOffset;
-    int skipOffset;
-    long proxOffset;
-
-    int posPendingCount;
-    int payloadLength;
-    boolean payloadPending;
-
-    boolean skipped;
-    Lucene40SkipListReader skipper;
-    private BytesRef payload;
-    private long lazyProxPointer;
-
-    public SegmentDocsAndPositionsAndPayloadsEnum(IndexInput freqIn, IndexInput proxIn) throws IOException {
-      startFreqIn = freqIn;
-      this.freqIn = (IndexInput) freqIn.clone();
-      this.proxIn = (IndexInput) proxIn.clone();
-    }
-
-    public SegmentDocsAndPositionsAndPayloadsEnum reset(FieldInfo fieldInfo, StandardTermState termState, Bits liveDocs) throws IOException {
-      assert fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-      assert fieldInfo.storePayloads;
-      if (payload == null) {
-        payload = new BytesRef();
-        payload.bytes = new byte[1];
-      }
-
-      this.liveDocs = liveDocs;
-
-      // TODO: for full enum case (eg segment merging) this
-      // seek is unnecessary; maybe we can avoid in such
-      // cases
-      freqIn.seek(termState.freqOffset);
-      lazyProxPointer = termState.proxOffset;
-
-      limit = termState.docFreq;
-      ord = 0;
-      doc = -1;
-      accum = 0;
-      position = 0;
-
-      skipped = false;
-      posPendingCount = 0;
-      payloadPending = false;
-
-      freqOffset = termState.freqOffset;
-      proxOffset = termState.proxOffset;
-      skipOffset = termState.skipOffset;
-      //System.out.println("StandardR.D&PE reset seg=" + segment + " limit=" + limit + " freqFP=" + freqOffset + " proxFP=" + proxOffset + " this=" + this);
-
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      while(true) {
-        if (ord == limit) {
-          //System.out.println("StandardR.D&PE seg=" + segment + " nextDoc return doc=END");
-          return doc = NO_MORE_DOCS;
-        }
-
-        ord++;
-
-        // Decode next doc/freq pair
-        final int code = freqIn.readVInt();
-
-        accum += code >>> 1; // shift off low bit
-        if ((code & 1) != 0) { // if low bit is set
-          freq = 1; // freq is one
-        } else {
-          freq = freqIn.readVInt(); // else read freq
-        }
-        posPendingCount += freq;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          break;
-        }
-      }
-
-      position = 0;
-
-      //System.out.println("StandardR.D&PE nextDoc seg=" + segment + " return doc=" + doc);
-      return (doc = accum);
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int freq() {
-      return freq;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-
-      //System.out.println("StandardR.D&PE advance seg=" + segment + " target=" + target + " this=" + this);
-
-      if ((target - skipInterval) >= doc && limit >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and it isn't too close
-
-        if (skipper == null) {
-          // This is the first time this enum has ever been used for skipping -- do lazy init
-          skipper = new Lucene40SkipListReader((IndexInput) freqIn.clone(), maxSkipLevels, skipInterval);
-        }
-
-        if (!skipped) {
-
-          // This is the first time this posting has
-          // skipped, since reset() was called, so now we
-          // load the skip data for this posting
-          //System.out.println("  init skipper freqOffset=" + freqOffset + " skipOffset=" + skipOffset + " vs len=" + freqIn.length());
-          skipper.init(freqOffset+skipOffset,
-                       freqOffset, proxOffset,
-                       limit, true);
-
-          skipped = true;
-        }
-
-        final int newOrd = skipper.skipTo(target); 
-
-        if (newOrd > ord) {
-          // Skipper moved
-          ord = newOrd;
-          doc = accum = skipper.getDoc();
-          freqIn.seek(skipper.getFreqPointer());
-          lazyProxPointer = skipper.getProxPointer();
-          posPendingCount = 0;
-          position = 0;
-          payloadPending = false;
-          payloadLength = skipper.getPayloadLength();
-        }
-      }
-        
-      // Now, linear scan for the rest:
-      do {
-        nextDoc();
-      } while (target > doc);
-
-      return doc;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-
-      if (lazyProxPointer != -1) {
-        proxIn.seek(lazyProxPointer);
-        lazyProxPointer = -1;
-      }
-      
-      if (payloadPending && payloadLength > 0) {
-        // payload of last position as never retrieved -- skip it
-        proxIn.seek(proxIn.getFilePointer() + payloadLength);
-        payloadPending = false;
-      }
-
-      // scan over any docs that were iterated without their positions
-      while(posPendingCount > freq) {
-
-        final int code = proxIn.readVInt();
-
-        if ((code & 1) != 0) {
-          // new payload length
-          payloadLength = proxIn.readVInt();
-          assert payloadLength >= 0;
-        }
-        
-        assert payloadLength != -1;
-        proxIn.seek(proxIn.getFilePointer() + payloadLength);
-
-        posPendingCount--;
-        position = 0;
-        payloadPending = false;
-        //System.out.println("StandardR.D&PE skipPos");
-      }
-
-      // read next position
-      if (payloadPending && payloadLength > 0) {
-        // payload wasn't retrieved for last position
-        proxIn.seek(proxIn.getFilePointer()+payloadLength);
-      }
-
-      final int code = proxIn.readVInt();
-      if ((code & 1) != 0) {
-        // new payload length
-        payloadLength = proxIn.readVInt();
-        assert payloadLength >= 0;
-      }
-      assert payloadLength != -1;
-          
-      payloadPending = true;
-      position += code >>> 1;
-
-      posPendingCount--;
-
-      assert posPendingCount >= 0: "nextPosition() was called too many times (more than freq() times) posPendingCount=" + posPendingCount;
-
-      //System.out.println("StandardR.D&PE nextPos   return pos=" + position);
-      return position;
-    }
-
-    /** Returns the payload at this position, or null if no
-     *  payload was indexed. */
-    @Override
-    public BytesRef getPayload() throws IOException {
-      assert lazyProxPointer == -1;
-      assert posPendingCount < freq;
-      if (!payloadPending) {
-        throw new IOException("Either no payload exists at this term position or an attempt was made to load it more than once.");
-      }
-      if (payloadLength > payload.bytes.length) {
-        payload.grow(payloadLength);
-      }
-
-      proxIn.readBytes(payload.bytes, 0, payloadLength);
-      payload.length = payloadLength;
-      payloadPending = false;
-
-      return payload;
-    }
-
-    @Override
-    public boolean hasPayload() {
-      return payloadPending && payloadLength > 0;
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsWriter.java
deleted file mode 100644
index 38a6961..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsWriter.java
+++ /dev/null
@@ -1,334 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Consumes doc & freq, writing them using the current
- *  index file format */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.IOUtils;
-
-/** @lucene.experimental */
-public final class Lucene40PostingsWriter extends PostingsWriterBase {
-  final static String CODEC = "Lucene40PostingsWriter";
-
-  //private static boolean DEBUG = BlockTreeTermsWriter.DEBUG;
-  
-  // Increment version to change it:
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  final IndexOutput freqOut;
-  final IndexOutput proxOut;
-  final Lucene40SkipListWriter skipListWriter;
-  /** Expert: The fraction of TermDocs entries stored in skip tables,
-   * used to accelerate {@link DocsEnum#advance(int)}.  Larger values result in
-   * smaller indexes, greater acceleration, but fewer accelerable cases, while
-   * smaller values result in bigger indexes, less acceleration and more
-   * accelerable cases. More detailed experiments would be useful here. */
-  static final int DEFAULT_SKIP_INTERVAL = 16;
-  final int skipInterval;
-  
-  /**
-   * Expert: minimum docFreq to write any skip data at all
-   */
-  final int skipMinimum;
-
-  /** Expert: The maximum number of skip levels. Smaller values result in 
-   * slightly smaller indexes, but slower skipping in big posting lists.
-   */
-  final int maxSkipLevels = 10;
-  final int totalNumDocs;
-  IndexOutput termsOut;
-
-  IndexOptions indexOptions;
-  boolean storePayloads;
-  // Starts a new term
-  long freqStart;
-  long proxStart;
-  FieldInfo fieldInfo;
-  int lastPayloadLength;
-  int lastPosition;
-
-  // private String segment;
-
-  public Lucene40PostingsWriter(SegmentWriteState state) throws IOException {
-    this(state, DEFAULT_SKIP_INTERVAL);
-  }
-  
-  public Lucene40PostingsWriter(SegmentWriteState state, int skipInterval) throws IOException {
-    super();
-    this.skipInterval = skipInterval;
-    this.skipMinimum = skipInterval; /* set to the same for now */
-    // this.segment = state.segmentName;
-    String fileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION);
-    freqOut = state.directory.createOutput(fileName, state.context);
-    boolean success = false;
-    try {
-      if (state.fieldInfos.hasProx()) {
-        // At least one field does not omit TF, so create the
-        // prox file
-        fileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION);
-        proxOut = state.directory.createOutput(fileName, state.context);
-      } else {
-        // Every field omits TF so we will write no prox file
-        proxOut = null;
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(freqOut);
-      }
-    }
-
-    totalNumDocs = state.numDocs;
-
-    skipListWriter = new Lucene40SkipListWriter(skipInterval,
-                                               maxSkipLevels,
-                                               state.numDocs,
-                                               freqOut,
-                                               proxOut);
-  }
-
-  @Override
-  public void start(IndexOutput termsOut) throws IOException {
-    this.termsOut = termsOut;
-    CodecUtil.writeHeader(termsOut, CODEC, VERSION_CURRENT);
-    termsOut.writeInt(skipInterval);                // write skipInterval
-    termsOut.writeInt(maxSkipLevels);               // write maxSkipLevels
-    termsOut.writeInt(skipMinimum);                 // write skipMinimum
-  }
-
-  @Override
-  public void startTerm() {
-    freqStart = freqOut.getFilePointer();
-    //if (DEBUG) System.out.println("SPW: startTerm freqOut.fp=" + freqStart);
-    if (proxOut != null) {
-      proxStart = proxOut.getFilePointer();
-      // force first payload to write its length
-      lastPayloadLength = -1;
-    }
-    skipListWriter.resetSkip();
-  }
-
-  // Currently, this instance is re-used across fields, so
-  // our parent calls setField whenever the field changes
-  @Override
-  public void setField(FieldInfo fieldInfo) {
-    //System.out.println("SPW: setField");
-    /*
-    if (BlockTreeTermsWriter.DEBUG && fieldInfo.name.equals("id")) {
-      DEBUG = true;
-    } else {
-      DEBUG = false;
-    }
-    */
-    this.fieldInfo = fieldInfo;
-    indexOptions = fieldInfo.indexOptions;
-    storePayloads = fieldInfo.storePayloads;
-    //System.out.println("  set init blockFreqStart=" + freqStart);
-    //System.out.println("  set init blockProxStart=" + proxStart);
-  }
-
-  int lastDocID;
-  int df;
-  
-  /** Adds a new doc in this term.  If this returns null
-   *  then we just skip consuming positions/payloads. */
-  @Override
-  public void startDoc(int docID, int termDocFreq) throws IOException {
-    // if (DEBUG) System.out.println("SPW:   startDoc seg=" + segment + " docID=" + docID + " tf=" + termDocFreq + " freqOut.fp=" + freqOut.getFilePointer());
-
-    final int delta = docID - lastDocID;
-    
-    if (docID < 0 || (df > 0 && delta <= 0)) {
-      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " ) (freqOut: " + freqOut + ")");
-    }
-
-    if ((++df % skipInterval) == 0) {
-      skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength);
-      skipListWriter.bufferSkip(df);
-    }
-
-    assert docID < totalNumDocs: "docID=" + docID + " totalNumDocs=" + totalNumDocs;
-
-    lastDocID = docID;
-    if (indexOptions == IndexOptions.DOCS_ONLY) {
-      freqOut.writeVInt(delta);
-    } else if (1 == termDocFreq) {
-      freqOut.writeVInt((delta<<1) | 1);
-    } else {
-      freqOut.writeVInt(delta<<1);
-      freqOut.writeVInt(termDocFreq);
-    }
-
-    lastPosition = 0;
-  }
-
-  /** Add a new position & payload */
-  @Override
-  public void addPosition(int position, BytesRef payload) throws IOException {
-    //if (DEBUG) System.out.println("SPW:     addPos pos=" + position + " payload=" + (payload == null ? "null" : (payload.length + " bytes")) + " proxFP=" + proxOut.getFilePointer());
-    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS: "invalid indexOptions: " + indexOptions;
-    assert proxOut != null;
-
-    final int delta = position - lastPosition;
-    
-    assert delta >= 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
-
-    lastPosition = position;
-
-    if (storePayloads) {
-      final int payloadLength = payload == null ? 0 : payload.length;
-
-      if (payloadLength != lastPayloadLength) {
-        lastPayloadLength = payloadLength;
-        proxOut.writeVInt((delta<<1)|1);
-        proxOut.writeVInt(payloadLength);
-      } else {
-        proxOut.writeVInt(delta << 1);
-      }
-
-      if (payloadLength > 0) {
-        proxOut.writeBytes(payload.bytes, payload.offset, payloadLength);
-      }
-    } else {
-      proxOut.writeVInt(delta);
-    }
-  }
-
-  @Override
-  public void finishDoc() {
-  }
-
-  private static class PendingTerm {
-    public final long freqStart;
-    public final long proxStart;
-    public final int skipOffset;
-
-    public PendingTerm(long freqStart, long proxStart, int skipOffset) {
-      this.freqStart = freqStart;
-      this.proxStart = proxStart;
-      this.skipOffset = skipOffset;
-    }
-  }
-
-  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
-
-  /** Called when we are done adding docs to this term */
-  @Override
-  public void finishTerm(TermStats stats) throws IOException {
-
-    // if (DEBUG) System.out.println("SPW: finishTerm seg=" + segment + " freqStart=" + freqStart);
-    assert stats.docFreq > 0;
-
-    // TODO: wasteful we are counting this (counting # docs
-    // for this term) in two places?
-    assert stats.docFreq == df;
-
-    final int skipOffset;
-    if (df >= skipMinimum) {
-      skipOffset = (int) (skipListWriter.writeSkip(freqOut)-freqStart);
-    } else {
-      skipOffset = -1;
-    }
-
-    pendingTerms.add(new PendingTerm(freqStart, proxStart, skipOffset));
-
-    lastDocID = 0;
-    df = 0;
-  }
-
-  private final RAMOutputStream bytesWriter = new RAMOutputStream();
-
-  @Override
-  public void flushTermsBlock(int start, int count) throws IOException {
-    //if (DEBUG) System.out.println("SPW: flushTermsBlock start=" + start + " count=" + count + " left=" + (pendingTerms.size()-count) + " pendingTerms.size()=" + pendingTerms.size());
-
-    if (count == 0) {
-      termsOut.writeByte((byte) 0);
-      return;
-    }
-
-    assert start <= pendingTerms.size();
-    assert count <= start;
-
-    final int limit = pendingTerms.size() - start + count;
-    final PendingTerm firstTerm = pendingTerms.get(limit - count);
-    // First term in block is abs coded:
-    bytesWriter.writeVLong(firstTerm.freqStart);
-
-    if (firstTerm.skipOffset != -1) {
-      assert firstTerm.skipOffset > 0;
-      bytesWriter.writeVInt(firstTerm.skipOffset);
-    }
-    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      bytesWriter.writeVLong(firstTerm.proxStart);
-    }
-    long lastFreqStart = firstTerm.freqStart;
-    long lastProxStart = firstTerm.proxStart;
-    for(int idx=limit-count+1; idx<limit; idx++) {
-      final PendingTerm term = pendingTerms.get(idx);
-      //if (DEBUG) System.out.println("  write term freqStart=" + term.freqStart);
-      // The rest of the terms term are delta coded:
-      bytesWriter.writeVLong(term.freqStart - lastFreqStart);
-      lastFreqStart = term.freqStart;
-      if (term.skipOffset != -1) {
-        assert term.skipOffset > 0;
-        bytesWriter.writeVInt(term.skipOffset);
-      }
-      if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-        bytesWriter.writeVLong(term.proxStart - lastProxStart);
-        lastProxStart = term.proxStart;
-      }
-    }
-
-    termsOut.writeVInt((int) bytesWriter.getFilePointer());
-    bytesWriter.writeTo(termsOut);
-    bytesWriter.reset();
-
-    // Remove the terms we just wrote:
-    pendingTerms.subList(limit-count, limit).clear();
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      freqOut.close();
-    } finally {
-      if (proxOut != null) {
-        proxOut.close();
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SegmentInfosFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SegmentInfosFormat.java
deleted file mode 100644
index 76b58cc..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SegmentInfosFormat.java
+++ /dev/null
@@ -1,40 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-import org.apache.lucene.index.codecs.SegmentInfosFormat;
-import org.apache.lucene.index.codecs.SegmentInfosReader;
-import org.apache.lucene.index.codecs.SegmentInfosWriter;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @lucene.experimental
- */
-public class Lucene40SegmentInfosFormat extends SegmentInfosFormat {
-  private final SegmentInfosReader reader = new Lucene40SegmentInfosReader();
-  private final SegmentInfosWriter writer = new Lucene40SegmentInfosWriter();
-  
-  @Override
-  public SegmentInfosReader getSegmentInfosReader() {
-    return reader;
-  }
-
-  @Override
-  public SegmentInfosWriter getSegmentInfosWriter() {
-    return writer;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SegmentInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SegmentInfosReader.java
deleted file mode 100644
index d3e8a81..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SegmentInfosReader.java
+++ /dev/null
@@ -1,191 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.SegmentInfosReader;
-import org.apache.lucene.index.codecs.lucene40.Lucene40TermVectorsReader;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.CompoundFileDirectory;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * Default implementation of {@link SegmentInfosReader}.
- * @lucene.experimental
- */
-public class Lucene40SegmentInfosReader extends SegmentInfosReader {
-
-  // TODO: shove all backwards code to preflex!
-  // this is a little tricky, because of IR.commit(), two options:
-  // 1. PreFlex writes 4.x SIS format, but reads both 3.x and 4.x
-  //    (and maybe RW always only writes the 3.x one? for that to work well,
-  //     we have to move .fnx file to codec too, not too bad but more work).
-  //     or we just have crappier RW testing like today.
-  // 2. PreFlex writes 3.x SIS format, and only reads 3.x
-  //    (in this case we have to move .fnx file to codec as well)
-  @Override
-  public void read(Directory directory, String segmentsFileName, ChecksumIndexInput input, SegmentInfos infos, IOContext context) throws IOException { 
-    infos.version = input.readLong(); // read version
-    infos.counter = input.readInt(); // read counter
-    final int format = infos.getFormat();
-    for (int i = input.readInt(); i > 0; i--) { // read segmentInfos
-      SegmentInfo si = readSegmentInfo(directory, format, input);
-      if (si.getVersion() == null) {
-        // Could be a 3.0 - try to open the doc stores - if it fails, it's a
-        // 2.x segment, and an IndexFormatTooOldException will be thrown,
-        // which is what we want.
-        Directory dir = directory;
-        if (si.getDocStoreOffset() != -1) {
-          if (si.getDocStoreIsCompoundFile()) {
-            dir = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(
-                si.getDocStoreSegment(), "",
-                IndexFileNames.COMPOUND_FILE_STORE_EXTENSION), context, false);
-          }
-        } else if (si.getUseCompoundFile()) {
-          dir = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(
-              si.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION), context, false);
-        }
-
-        try {
-          Lucene40StoredFieldsReader.checkCodeVersion(dir, si.getDocStoreSegment());
-        } finally {
-          // If we opened the directory, close it
-          if (dir != directory) dir.close();
-        }
-          
-        // Above call succeeded, so it's a 3.0 segment. Upgrade it so the next
-        // time the segment is read, its version won't be null and we won't
-        // need to open FieldsReader every time for each such segment.
-        si.setVersion("3.0");
-      } else if (si.getVersion().equals("2.x")) {
-        // If it's a 3x index touched by 3.1+ code, then segments record their
-        // version, whether they are 2.x ones or not. We detect that and throw
-        // appropriate exception.
-        throw new IndexFormatTooOldException("segment " + si.name + " in resource " + input, si.getVersion());
-      }
-      infos.add(si);
-    }
-      
-    infos.userData = input.readStringStringMap();
-  }
-  
-  // if we make a preflex impl we can remove a lot of this hair...
-  public SegmentInfo readSegmentInfo(Directory dir, int format, ChecksumIndexInput input) throws IOException {
-    final String version;
-    if (format <= SegmentInfos.FORMAT_3_1) {
-      version = input.readString();
-    } else {
-      version = null;
-    }
-    final String name = input.readString();
-    final int docCount = input.readInt();
-    final long delGen = input.readLong();
-    final int docStoreOffset = input.readInt();
-    final String docStoreSegment;
-    final boolean docStoreIsCompoundFile;
-    if (docStoreOffset != -1) {
-      docStoreSegment = input.readString();
-      docStoreIsCompoundFile = input.readByte() == SegmentInfo.YES;
-    } else {
-      docStoreSegment = name;
-      docStoreIsCompoundFile = false;
-    }
-
-    if (format > SegmentInfos.FORMAT_4_0) {
-      // pre-4.0 indexes write a byte if there is a single norms file
-      byte b = input.readByte();
-      assert 1 == b;
-    }
-
-    final int numNormGen = input.readInt();
-    final Map<Integer,Long> normGen;
-    if (numNormGen == SegmentInfo.NO) {
-      normGen = null;
-    } else {
-      normGen = new HashMap<Integer, Long>();
-      for(int j=0;j<numNormGen;j++) {
-        int fieldNumber = j;
-        if (format <= SegmentInfos.FORMAT_4_0) {
-          fieldNumber = input.readInt();
-        }
-
-        normGen.put(fieldNumber, input.readLong());
-      }
-    }
-    final boolean isCompoundFile = input.readByte() == SegmentInfo.YES;
-
-    final int delCount = input.readInt();
-    assert delCount <= docCount;
-
-    final int hasProx = input.readByte();
-
-    final Codec codec;
-    // note: if the codec is not available: Codec.forName will throw an exception.
-    if (format <= SegmentInfos.FORMAT_4_0) {
-      codec = Codec.forName(input.readString());
-    } else {
-      codec = Codec.forName("Lucene3x");
-    }
-    final Map<String,String> diagnostics = input.readStringStringMap();
-
-    final int hasVectors;
-    if (format <= SegmentInfos.FORMAT_HAS_VECTORS) {
-      hasVectors = input.readByte();
-    } else {
-      final String storesSegment;
-      final String ext;
-      final boolean storeIsCompoundFile;
-      if (docStoreOffset != -1) {
-        storesSegment = docStoreSegment;
-        storeIsCompoundFile = docStoreIsCompoundFile;
-        ext = IndexFileNames.COMPOUND_FILE_STORE_EXTENSION;
-      } else {
-        storesSegment = name;
-        storeIsCompoundFile = isCompoundFile;
-        ext = IndexFileNames.COMPOUND_FILE_EXTENSION;
-      }
-      final Directory dirToTest;
-      if (storeIsCompoundFile) {
-        dirToTest = new CompoundFileDirectory(dir, IndexFileNames.segmentFileName(storesSegment, "", ext), IOContext.READONCE, false);
-      } else {
-        dirToTest = dir;
-      }
-      try {
-        // TODO: remove this manual file check or push to preflex codec
-        hasVectors = dirToTest.fileExists(IndexFileNames.segmentFileName(storesSegment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION)) ? SegmentInfo.YES : SegmentInfo.NO;
-      } finally {
-        if (isCompoundFile) {
-          dirToTest.close();
-        }
-      }
-    }
-    
-    return new SegmentInfo(dir, version, name, docCount, delGen, docStoreOffset,
-      docStoreSegment, docStoreIsCompoundFile, normGen, isCompoundFile,
-      delCount, hasProx, codec, diagnostics, hasVectors);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SegmentInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SegmentInfosWriter.java
deleted file mode 100644
index 33975cb..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SegmentInfosWriter.java
+++ /dev/null
@@ -1,115 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.index.codecs.SegmentInfosWriter;
-import org.apache.lucene.store.ChecksumIndexOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FlushInfo;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Default implementation of {@link SegmentInfosWriter}.
- * @lucene.experimental
- */
-public class Lucene40SegmentInfosWriter extends SegmentInfosWriter {
-
-  @Override
-  public IndexOutput writeInfos(Directory dir, String segmentFileName, String codecID, SegmentInfos infos, IOContext context)
-          throws IOException {
-    IndexOutput out = createOutput(dir, segmentFileName, new IOContext(new FlushInfo(infos.size(), infos.totalDocCount())));
-    boolean success = false;
-    try {
-      out.writeInt(SegmentInfos.FORMAT_CURRENT); // write FORMAT
-      out.writeString(codecID); // write codecID
-      out.writeLong(infos.version);
-      out.writeInt(infos.counter); // write counter
-      out.writeInt(infos.size()); // write infos
-      for (SegmentInfo si : infos) {
-        writeInfo(out, si);
-      }
-      out.writeStringStringMap(infos.getUserData());
-      success = true;
-      return out;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  /** Save a single segment's info. */
-  private void writeInfo(IndexOutput output, SegmentInfo si) throws IOException {
-    assert si.getDelCount() <= si.docCount: "delCount=" + si.getDelCount() + " docCount=" + si.docCount + " segment=" + si.name;
-    // Write the Lucene version that created this segment, since 3.1
-    output.writeString(si.getVersion());
-    output.writeString(si.name);
-    output.writeInt(si.docCount);
-    output.writeLong(si.getDelGen());
-
-    output.writeInt(si.getDocStoreOffset());
-    if (si.getDocStoreOffset() != -1) {
-      output.writeString(si.getDocStoreSegment());
-      output.writeByte((byte) (si.getDocStoreIsCompoundFile() ? 1:0));
-    }
-
-    Map<Integer,Long> normGen = si.getNormGen();
-    if (normGen == null) {
-      output.writeInt(SegmentInfo.NO);
-    } else {
-      output.writeInt(normGen.size());
-      for (Entry<Integer,Long> entry : normGen.entrySet()) {
-        output.writeInt(entry.getKey());
-        output.writeLong(entry.getValue());
-      }
-    }
-
-    output.writeByte((byte) (si.getUseCompoundFile() ? SegmentInfo.YES : SegmentInfo.NO));
-    output.writeInt(si.getDelCount());
-    output.writeByte((byte) (si.getHasProxInternal()));
-    output.writeString(si.getCodec().getName());
-    output.writeStringStringMap(si.getDiagnostics());
-    output.writeByte((byte) (si.getHasVectorsInternal()));
-  }
-  
-  protected IndexOutput createOutput(Directory dir, String segmentFileName, IOContext context)
-      throws IOException {
-    IndexOutput plainOut = dir.createOutput(segmentFileName, context);
-    ChecksumIndexOutput out = new ChecksumIndexOutput(plainOut);
-    return out;
-  }
-
-  @Override
-  public void prepareCommit(IndexOutput segmentOutput) throws IOException {
-    ((ChecksumIndexOutput)segmentOutput).prepareCommit();
-  }
-
-  @Override
-  public void finishCommit(IndexOutput out) throws IOException {
-    ((ChecksumIndexOutput)out).finishCommit();
-    out.close();
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SkipListReader.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SkipListReader.java
deleted file mode 100644
index 9b4a2e7..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SkipListReader.java
+++ /dev/null
@@ -1,118 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.index.codecs.MultiLevelSkipListReader;
-import org.apache.lucene.store.IndexInput;
-
-/**
- * Implements the skip list reader for the default posting list format
- * that stores positions and payloads.
- * @lucene.experimental
- */
-public class Lucene40SkipListReader extends MultiLevelSkipListReader {
-  private boolean currentFieldStoresPayloads;
-  private long freqPointer[];
-  private long proxPointer[];
-  private int payloadLength[];
-  
-  private long lastFreqPointer;
-  private long lastProxPointer;
-  private int lastPayloadLength;
-                           
-
-  public Lucene40SkipListReader(IndexInput skipStream, int maxSkipLevels, int skipInterval) {
-    super(skipStream, maxSkipLevels, skipInterval);
-    freqPointer = new long[maxSkipLevels];
-    proxPointer = new long[maxSkipLevels];
-    payloadLength = new int[maxSkipLevels];
-  }
-
-  public void init(long skipPointer, long freqBasePointer, long proxBasePointer, int df, boolean storesPayloads) {
-    super.init(skipPointer, df);
-    this.currentFieldStoresPayloads = storesPayloads;
-    lastFreqPointer = freqBasePointer;
-    lastProxPointer = proxBasePointer;
-
-    Arrays.fill(freqPointer, freqBasePointer);
-    Arrays.fill(proxPointer, proxBasePointer);
-    Arrays.fill(payloadLength, 0);
-  }
-
-  /** Returns the freq pointer of the doc to which the last call of 
-   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
-  public long getFreqPointer() {
-    return lastFreqPointer;
-  }
-
-  /** Returns the prox pointer of the doc to which the last call of 
-   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
-  public long getProxPointer() {
-    return lastProxPointer;
-  }
-  
-  /** Returns the payload length of the payload stored just before 
-   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
-   * has skipped.  */
-  public int getPayloadLength() {
-    return lastPayloadLength;
-  }
-  
-  @Override
-  protected void seekChild(int level) throws IOException {
-    super.seekChild(level);
-    freqPointer[level] = lastFreqPointer;
-    proxPointer[level] = lastProxPointer;
-    payloadLength[level] = lastPayloadLength;
-  }
-  
-  @Override
-  protected void setLastSkipData(int level) {
-    super.setLastSkipData(level);
-    lastFreqPointer = freqPointer[level];
-    lastProxPointer = proxPointer[level];
-    lastPayloadLength = payloadLength[level];
-  }
-
-
-  @Override
-  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
-    int delta;
-    if (currentFieldStoresPayloads) {
-      // the current field stores payloads.
-      // if the doc delta is odd then we have
-      // to read the current payload length
-      // because it differs from the length of the
-      // previous payload
-      delta = skipStream.readVInt();
-      if ((delta & 1) != 0) {
-        payloadLength[level] = skipStream.readVInt();
-      }
-      delta >>>= 1;
-    } else {
-      delta = skipStream.readVInt();
-    }
-    freqPointer[level] += skipStream.readVInt();
-    proxPointer[level] += skipStream.readVInt();
-    
-    return delta;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SkipListWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SkipListWriter.java
deleted file mode 100644
index 93b4f0d..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40SkipListWriter.java
+++ /dev/null
@@ -1,128 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.index.codecs.MultiLevelSkipListWriter;
-
-
-/**
- * Implements the skip list writer for the default posting list format
- * that stores positions and payloads.
- * @lucene.experimental
- */
-public class Lucene40SkipListWriter extends MultiLevelSkipListWriter {
-  private int[] lastSkipDoc;
-  private int[] lastSkipPayloadLength;
-  private long[] lastSkipFreqPointer;
-  private long[] lastSkipProxPointer;
-  
-  private IndexOutput freqOutput;
-  private IndexOutput proxOutput;
-
-  private int curDoc;
-  private boolean curStorePayloads;
-  private int curPayloadLength;
-  private long curFreqPointer;
-  private long curProxPointer;
-
-  public Lucene40SkipListWriter(int skipInterval, int numberOfSkipLevels, int docCount, IndexOutput freqOutput, IndexOutput proxOutput) {
-    super(skipInterval, numberOfSkipLevels, docCount);
-    this.freqOutput = freqOutput;
-    this.proxOutput = proxOutput;
-    
-    lastSkipDoc = new int[numberOfSkipLevels];
-    lastSkipPayloadLength = new int[numberOfSkipLevels];
-    lastSkipFreqPointer = new long[numberOfSkipLevels];
-    lastSkipProxPointer = new long[numberOfSkipLevels];
-  }
-
-  /**
-   * Sets the values for the current skip data. 
-   */
-  public void setSkipData(int doc, boolean storePayloads, int payloadLength) {
-    this.curDoc = doc;
-    this.curStorePayloads = storePayloads;
-    this.curPayloadLength = payloadLength;
-    this.curFreqPointer = freqOutput.getFilePointer();
-    if (proxOutput != null)
-      this.curProxPointer = proxOutput.getFilePointer();
-  }
-
-  @Override
-  public void resetSkip() {
-    super.resetSkip();
-    Arrays.fill(lastSkipDoc, 0);
-    Arrays.fill(lastSkipPayloadLength, -1);  // we don't have to write the first length in the skip list
-    Arrays.fill(lastSkipFreqPointer, freqOutput.getFilePointer());
-    if (proxOutput != null)
-      Arrays.fill(lastSkipProxPointer, proxOutput.getFilePointer());
-  }
-  
-  @Override
-  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
-    // To efficiently store payloads in the posting lists we do not store the length of
-    // every payload. Instead we omit the length for a payload if the previous payload had
-    // the same length.
-    // However, in order to support skipping the payload length at every skip point must be known.
-    // So we use the same length encoding that we use for the posting lists for the skip data as well:
-    // Case 1: current field does not store payloads
-    //           SkipDatum                 --> DocSkip, FreqSkip, ProxSkip
-    //           DocSkip,FreqSkip,ProxSkip --> VInt
-    //           DocSkip records the document number before every SkipInterval th  document in TermFreqs. 
-    //           Document numbers are represented as differences from the previous value in the sequence.
-    // Case 2: current field stores payloads
-    //           SkipDatum                 --> DocSkip, PayloadLength?, FreqSkip,ProxSkip
-    //           DocSkip,FreqSkip,ProxSkip --> VInt
-    //           PayloadLength             --> VInt    
-    //         In this case DocSkip/2 is the difference between
-    //         the current and the previous value. If DocSkip
-    //         is odd, then a PayloadLength encoded as VInt follows,
-    //         if DocSkip is even, then it is assumed that the
-    //         current payload length equals the length at the previous
-    //         skip point
-    if (curStorePayloads) {
-      int delta = curDoc - lastSkipDoc[level];
-      if (curPayloadLength == lastSkipPayloadLength[level]) {
-        // the current payload length equals the length at the previous skip point,
-        // so we don't store the length again
-        skipBuffer.writeVInt(delta * 2);
-      } else {
-        // the payload length is different from the previous one. We shift the DocSkip, 
-        // set the lowest bit and store the current payload length as VInt.
-        skipBuffer.writeVInt(delta * 2 + 1);
-        skipBuffer.writeVInt(curPayloadLength);
-        lastSkipPayloadLength[level] = curPayloadLength;
-      }
-    } else {
-      // current field does not store payloads
-      skipBuffer.writeVInt(curDoc - lastSkipDoc[level]);
-    }
-    skipBuffer.writeVInt((int) (curFreqPointer - lastSkipFreqPointer[level]));
-    skipBuffer.writeVInt((int) (curProxPointer - lastSkipProxPointer[level]));
-
-    lastSkipDoc[level] = curDoc;
-    
-    lastSkipFreqPointer[level] = curFreqPointer;
-    lastSkipProxPointer[level] = curProxPointer;
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40StoredFieldsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40StoredFieldsFormat.java
deleted file mode 100644
index b79c049..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40StoredFieldsFormat.java
+++ /dev/null
@@ -1,50 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.codecs.StoredFieldsFormat;
-import org.apache.lucene.index.codecs.StoredFieldsReader;
-import org.apache.lucene.index.codecs.StoredFieldsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/** @lucene.experimental */
-public class Lucene40StoredFieldsFormat extends StoredFieldsFormat {
-
-  @Override
-  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si,
-      FieldInfos fn, IOContext context) throws IOException {
-    return new Lucene40StoredFieldsReader(directory, si, fn, context);
-  }
-
-  @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, String segment,
-      IOContext context) throws IOException {
-    return new Lucene40StoredFieldsWriter(directory, segment, context);
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    Lucene40StoredFieldsReader.files(dir, info, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40StoredFieldsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40StoredFieldsReader.java
deleted file mode 100644
index ce56b08..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40StoredFieldsReader.java
+++ /dev/null
@@ -1,302 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexFormatTooNewException;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.StoredFieldVisitor;
-import org.apache.lucene.index.codecs.StoredFieldsReader;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.IOUtils;
-
-import java.io.Closeable;
-import java.nio.charset.Charset;
-import java.util.Set;
-
-/**
- * Class responsible for access to stored document fields.
- * <p/>
- * It uses &lt;segment&gt;.fdt and &lt;segment&gt;.fdx; files.
- * 
- * @lucene.internal
- */
-public final class Lucene40StoredFieldsReader extends StoredFieldsReader implements Cloneable, Closeable {
-  private final static int FORMAT_SIZE = 4;
-
-  private final FieldInfos fieldInfos;
-  private final IndexInput fieldsStream;
-  private final IndexInput indexStream;
-  private int numTotalDocs;
-  private int size;
-  private boolean closed;
-  private final int format;
-
-  // The docID offset where our docs begin in the index
-  // file.  This will be 0 if we have our own private file.
-  private int docStoreOffset;
-
-  /** Returns a cloned FieldsReader that shares open
-   *  IndexInputs with the original one.  It is the caller's
-   *  job not to close the original FieldsReader until all
-   *  clones are called (eg, currently SegmentReader manages
-   *  this logic). */
-  @Override
-  public Lucene40StoredFieldsReader clone() {
-    ensureOpen();
-    return new Lucene40StoredFieldsReader(fieldInfos, numTotalDocs, size, format, docStoreOffset, (IndexInput)fieldsStream.clone(), (IndexInput)indexStream.clone());
-  }
-
-  /** Verifies that the code version which wrote the segment is supported. */
-  public static void checkCodeVersion(Directory dir, String segment) throws IOException {
-    final String indexStreamFN = IndexFileNames.segmentFileName(segment, "", Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION);
-    IndexInput idxStream = dir.openInput(indexStreamFN, IOContext.DEFAULT);
-    
-    try {
-      int format = idxStream.readInt();
-      if (format < Lucene40StoredFieldsWriter.FORMAT_MINIMUM)
-        throw new IndexFormatTooOldException(idxStream, format, Lucene40StoredFieldsWriter.FORMAT_MINIMUM, Lucene40StoredFieldsWriter.FORMAT_CURRENT);
-      if (format > Lucene40StoredFieldsWriter.FORMAT_CURRENT)
-        throw new IndexFormatTooNewException(idxStream, format, Lucene40StoredFieldsWriter.FORMAT_MINIMUM, Lucene40StoredFieldsWriter.FORMAT_CURRENT);
-    } finally {
-      idxStream.close();
-    }
-  
-  }
-  
-  // Used only by clone
-  private Lucene40StoredFieldsReader(FieldInfos fieldInfos, int numTotalDocs, int size, int format, int docStoreOffset,
-                       IndexInput fieldsStream, IndexInput indexStream) {
-    this.fieldInfos = fieldInfos;
-    this.numTotalDocs = numTotalDocs;
-    this.size = size;
-    this.format = format;
-    this.docStoreOffset = docStoreOffset;
-    this.fieldsStream = fieldsStream;
-    this.indexStream = indexStream;
-  }
-
-  public Lucene40StoredFieldsReader(Directory d, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
-    final String segment = si.getDocStoreSegment();
-    final int docStoreOffset = si.getDocStoreOffset();
-    final int size = si.docCount;
-    boolean success = false;
-    fieldInfos = fn;
-    try {
-      fieldsStream = d.openInput(IndexFileNames.segmentFileName(segment, "", Lucene40StoredFieldsWriter.FIELDS_EXTENSION), context);
-      final String indexStreamFN = IndexFileNames.segmentFileName(segment, "", Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION);
-      indexStream = d.openInput(indexStreamFN, context);
-      
-      format = indexStream.readInt();
-
-      if (format < Lucene40StoredFieldsWriter.FORMAT_MINIMUM)
-        throw new IndexFormatTooOldException(indexStream, format, Lucene40StoredFieldsWriter.FORMAT_MINIMUM, Lucene40StoredFieldsWriter.FORMAT_CURRENT);
-      if (format > Lucene40StoredFieldsWriter.FORMAT_CURRENT)
-        throw new IndexFormatTooNewException(indexStream, format, Lucene40StoredFieldsWriter.FORMAT_MINIMUM, Lucene40StoredFieldsWriter.FORMAT_CURRENT);
-
-      final long indexSize = indexStream.length() - FORMAT_SIZE;
-      
-      if (docStoreOffset != -1) {
-        // We read only a slice out of this shared fields file
-        this.docStoreOffset = docStoreOffset;
-        this.size = size;
-
-        // Verify the file is long enough to hold all of our
-        // docs
-        assert ((int) (indexSize / 8)) >= size + this.docStoreOffset: "indexSize=" + indexSize + " size=" + size + " docStoreOffset=" + docStoreOffset;
-      } else {
-        this.docStoreOffset = 0;
-        this.size = (int) (indexSize >> 3);
-        // Verify two sources of "maxDoc" agree:
-        if (this.size != si.docCount) {
-          throw new CorruptIndexException("doc counts differ for segment " + segment + ": fieldsReader shows " + this.size + " but segmentInfo shows " + si.docCount);
-        }
-      }
-      numTotalDocs = (int) (indexSize >> 3);
-      success = true;
-    } finally {
-      // With lock-less commits, it's entirely possible (and
-      // fine) to hit a FileNotFound exception above. In
-      // this case, we want to explicitly close any subset
-      // of things that were opened so that we don't have to
-      // wait for a GC to do so.
-      if (!success) {
-        close();
-      }
-    }
-  }
-
-  /**
-   * @throws AlreadyClosedException if this FieldsReader is closed
-   */
-  private void ensureOpen() throws AlreadyClosedException {
-    if (closed) {
-      throw new AlreadyClosedException("this FieldsReader is closed");
-    }
-  }
-
-  /**
-   * Closes the underlying {@link org.apache.lucene.store.IndexInput} streams.
-   * This means that the Fields values will not be accessible.
-   *
-   * @throws IOException
-   */
-  public final void close() throws IOException {
-    if (!closed) {
-      IOUtils.close(fieldsStream, indexStream);
-      closed = true;
-    }
-  }
-
-  public final int size() {
-    return size;
-  }
-
-  private void seekIndex(int docID) throws IOException {
-    indexStream.seek(FORMAT_SIZE + (docID + docStoreOffset) * 8L);
-  }
-
-  public final void visitDocument(int n, StoredFieldVisitor visitor) throws CorruptIndexException, IOException {
-    seekIndex(n);
-    fieldsStream.seek(indexStream.readLong());
-
-    final int numFields = fieldsStream.readVInt();
-    for (int fieldIDX = 0; fieldIDX < numFields; fieldIDX++) {
-      int fieldNumber = fieldsStream.readVInt();
-      FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
-      
-      int bits = fieldsStream.readByte() & 0xFF;
-      assert bits <= (Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_MASK | Lucene40StoredFieldsWriter.FIELD_IS_BINARY): "bits=" + Integer.toHexString(bits);
-
-      switch(visitor.needsField(fieldInfo)) {
-        case YES:
-          readField(visitor, fieldInfo, bits);
-          break;
-        case NO: 
-          skipField(bits);
-          break;
-        case STOP: 
-          return;
-      }
-    }
-  }
-  
-  static final Charset UTF8 = Charset.forName("UTF-8");
-
-  private void readField(StoredFieldVisitor visitor, FieldInfo info, int bits) throws IOException {
-    final int numeric = bits & Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_MASK;
-    if (numeric != 0) {
-      switch(numeric) {
-        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_INT:
-          visitor.intField(info, fieldsStream.readInt());
-          return;
-        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_LONG:
-          visitor.longField(info, fieldsStream.readLong());
-          return;
-        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_FLOAT:
-          visitor.floatField(info, Float.intBitsToFloat(fieldsStream.readInt()));
-          return;
-        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_DOUBLE:
-          visitor.doubleField(info, Double.longBitsToDouble(fieldsStream.readLong()));
-          return;
-        default:
-          throw new CorruptIndexException("Invalid numeric type: " + Integer.toHexString(numeric));
-      }
-    } else { 
-      final int length = fieldsStream.readVInt();
-      byte bytes[] = new byte[length];
-      fieldsStream.readBytes(bytes, 0, length);
-      if ((bits & Lucene40StoredFieldsWriter.FIELD_IS_BINARY) != 0) {
-        visitor.binaryField(info, bytes, 0, bytes.length);
-      } else {
-        visitor.stringField(info, new String(bytes, 0, bytes.length, UTF8));
-      }
-    }
-  }
-  
-  private void skipField(int bits) throws IOException {
-    final int numeric = bits & Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_MASK;
-    if (numeric != 0) {
-      switch(numeric) {
-        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_INT:
-        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_FLOAT:
-          fieldsStream.readInt();
-          return;
-        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_LONG:
-        case Lucene40StoredFieldsWriter.FIELD_IS_NUMERIC_DOUBLE:
-          fieldsStream.readLong();
-          return;
-        default: 
-          throw new CorruptIndexException("Invalid numeric type: " + Integer.toHexString(numeric));
-      }
-    } else {
-      final int length = fieldsStream.readVInt();
-      fieldsStream.seek(fieldsStream.getFilePointer() + length);
-    }
-  }
-
-  /** Returns the length in bytes of each raw document in a
-   *  contiguous range of length numDocs starting with
-   *  startDocID.  Returns the IndexInput (the fieldStream),
-   *  already seeked to the starting point for startDocID.*/
-  public final IndexInput rawDocs(int[] lengths, int startDocID, int numDocs) throws IOException {
-    seekIndex(startDocID);
-    long startOffset = indexStream.readLong();
-    long lastOffset = startOffset;
-    int count = 0;
-    while (count < numDocs) {
-      final long offset;
-      final int docID = docStoreOffset + startDocID + count + 1;
-      assert docID <= numTotalDocs;
-      if (docID < numTotalDocs) 
-        offset = indexStream.readLong();
-      else
-        offset = fieldsStream.length();
-      lengths[count++] = (int) (offset-lastOffset);
-      lastOffset = offset;
-    }
-
-    fieldsStream.seek(startOffset);
-
-    return fieldsStream;
-  }
-  
-  // TODO: split into PreFlexFieldsReader so it can handle this shared docstore crap?
-  // only preflex segments refer to these?
-  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    if (info.getDocStoreOffset() != -1) {
-      assert info.getDocStoreSegment() != null;
-      if (!info.getDocStoreIsCompoundFile()) {
-        files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION));
-        files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", Lucene40StoredFieldsWriter.FIELDS_EXTENSION));
-      }
-    } else {
-      files.add(IndexFileNames.segmentFileName(info.name, "", Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION));
-      files.add(IndexFileNames.segmentFileName(info.name, "", Lucene40StoredFieldsWriter.FIELDS_EXTENSION));
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40StoredFieldsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40StoredFieldsWriter.java
deleted file mode 100644
index cca5dfa..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40StoredFieldsWriter.java
+++ /dev/null
@@ -1,336 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Copyright 2004 The Apache Software Foundation
- *
- * Licensed under the Apache License, Version 2.0 (the "License"); you may not
- * use this file except in compliance with the License. You may obtain a copy of
- * the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.SegmentReader;
-import org.apache.lucene.index.MergePolicy.MergeAbortedException;
-import org.apache.lucene.index.codecs.StoredFieldsReader;
-import org.apache.lucene.index.codecs.StoredFieldsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/** @lucene.experimental */
-public final class Lucene40StoredFieldsWriter extends StoredFieldsWriter {
-  // NOTE: bit 0 is free here!  You can steal it!
-  static final int FIELD_IS_BINARY = 1 << 1;
-
-  // the old bit 1 << 2 was compressed, is now left out
-
-  private static final int _NUMERIC_BIT_SHIFT = 3;
-  static final int FIELD_IS_NUMERIC_MASK = 0x07 << _NUMERIC_BIT_SHIFT;
-
-  static final int FIELD_IS_NUMERIC_INT = 1 << _NUMERIC_BIT_SHIFT;
-  static final int FIELD_IS_NUMERIC_LONG = 2 << _NUMERIC_BIT_SHIFT;
-  static final int FIELD_IS_NUMERIC_FLOAT = 3 << _NUMERIC_BIT_SHIFT;
-  static final int FIELD_IS_NUMERIC_DOUBLE = 4 << _NUMERIC_BIT_SHIFT;
-  // currently unused: static final int FIELD_IS_NUMERIC_SHORT = 5 << _NUMERIC_BIT_SHIFT;
-  // currently unused: static final int FIELD_IS_NUMERIC_BYTE = 6 << _NUMERIC_BIT_SHIFT;
-
-  // the next possible bits are: 1 << 6; 1 << 7
-  
-  // Lucene 3.0: Removal of compressed fields
-  static final int FORMAT_LUCENE_3_0_NO_COMPRESSED_FIELDS = 2;
-
-  // Lucene 3.2: NumericFields are stored in binary format
-  static final int FORMAT_LUCENE_3_2_NUMERIC_FIELDS = 3;
-
-  // NOTE: if you introduce a new format, make it 1 higher
-  // than the current one, and always change this if you
-  // switch to a new format!
-  static final int FORMAT_CURRENT = FORMAT_LUCENE_3_2_NUMERIC_FIELDS;
-
-  // when removing support for old versions, leave the last supported version here
-  static final int FORMAT_MINIMUM = FORMAT_LUCENE_3_0_NO_COMPRESSED_FIELDS;
-
-  /** Extension of stored fields file */
-  public static final String FIELDS_EXTENSION = "fdt";
-  
-  /** Extension of stored fields index file */
-  public static final String FIELDS_INDEX_EXTENSION = "fdx";
-
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput fieldsStream;
-  private IndexOutput indexStream;
-
-  public Lucene40StoredFieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    assert directory != null;
-    this.directory = directory;
-    this.segment = segment;
-
-    boolean success = false;
-    try {
-      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
-      indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION), context);
-
-      fieldsStream.writeInt(FORMAT_CURRENT);
-      indexStream.writeInt(FORMAT_CURRENT);
-
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
-
-  // Writes the contents of buffer into the fields stream
-  // and adds a new entry for this document into the index
-  // stream.  This assumes the buffer was already written
-  // in the correct fields format.
-  public void startDocument(int numStoredFields) throws IOException {
-    indexStream.writeLong(fieldsStream.getFilePointer());
-    fieldsStream.writeVInt(numStoredFields);
-  }
-
-  public void close() throws IOException {
-    try {
-      IOUtils.close(fieldsStream, indexStream);
-    } finally {
-      fieldsStream = indexStream = null;
-    }
-  }
-
-  public void abort() {
-    try {
-      close();
-    } catch (IOException ignored) {}
-    
-    try {
-      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION));
-    } catch (IOException ignored) {}
-    
-    try {
-      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION));
-    } catch (IOException ignored) {}
-  }
-
-  public final void writeField(FieldInfo info, IndexableField field) throws IOException {
-    fieldsStream.writeVInt(info.number);
-    int bits = 0;
-    final BytesRef bytes;
-    final String string;
-    // TODO: maybe a field should serialize itself?
-    // this way we don't bake into indexer all these
-    // specific encodings for different fields?  and apps
-    // can customize...
-    if (field.numeric()) {
-      switch (field.numericDataType()) {
-        case INT:
-          bits |= FIELD_IS_NUMERIC_INT; break;
-        case LONG:
-          bits |= FIELD_IS_NUMERIC_LONG; break;
-        case FLOAT:
-          bits |= FIELD_IS_NUMERIC_FLOAT; break;
-        case DOUBLE:
-          bits |= FIELD_IS_NUMERIC_DOUBLE; break;
-        default:
-          assert false : "Should never get here";
-      }
-      string = null;
-      bytes = null;
-    } else {
-      bytes = field.binaryValue();
-      if (bytes != null) {
-        bits |= FIELD_IS_BINARY;
-        string = null;
-      } else {
-        string = field.stringValue();
-      }
-    }
-
-    fieldsStream.writeByte((byte) bits);
-
-    if (bytes != null) {
-      fieldsStream.writeVInt(bytes.length);
-      fieldsStream.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-    } else if (string != null) {
-      fieldsStream.writeString(field.stringValue());
-    } else {
-      final Number n = field.numericValue();
-      if (n == null) {
-        throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
-      }
-      switch (field.numericDataType()) {
-        case INT:
-          fieldsStream.writeInt(n.intValue()); break;
-        case LONG:
-          fieldsStream.writeLong(n.longValue()); break;
-        case FLOAT:
-          fieldsStream.writeInt(Float.floatToIntBits(n.floatValue())); break;
-        case DOUBLE:
-          fieldsStream.writeLong(Double.doubleToLongBits(n.doubleValue())); break;
-        default:
-          assert false : "Should never get here";
-      }
-    }
-  }
-
-  /** Bulk write a contiguous series of documents.  The
-   *  lengths array is the length (in bytes) of each raw
-   *  document.  The stream IndexInput is the
-   *  fieldsStream from which we should bulk-copy all
-   *  bytes. */
-  public final void addRawDocuments(IndexInput stream, int[] lengths, int numDocs) throws IOException {
-    long position = fieldsStream.getFilePointer();
-    long start = position;
-    for(int i=0;i<numDocs;i++) {
-      indexStream.writeLong(position);
-      position += lengths[i];
-    }
-    fieldsStream.copyBytes(stream, position-start);
-    assert fieldsStream.getFilePointer() == position;
-  }
-
-  @Override
-  public void finish(int numDocs) throws IOException {
-    if (4+((long) numDocs)*8 != indexStream.getFilePointer())
-      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
-      // we detect that the bug has struck, here, and
-      // throw an exception to prevent the corruption from
-      // entering the index.  See LUCENE-1282 for
-      // details.
-      throw new RuntimeException("mergeFields produced an invalid result: docCount is " + numDocs + " but fdx file size is " + indexStream.getFilePointer() + " file=" + indexStream.toString() + "; now aborting this merge to prevent index corruption");
-  }
-  
-  @Override
-  public int merge(MergeState mergeState) throws IOException {
-    int docCount = 0;
-    // Used for bulk-reading raw bytes for stored fields
-    int rawDocLengths[] = new int[MAX_RAW_MERGE_DOCS];
-    int idx = 0;
-    
-    for (MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
-      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];
-      Lucene40StoredFieldsReader matchingFieldsReader = null;
-      if (matchingSegmentReader != null) {
-        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();
-        // we can only bulk-copy if the matching reader is also a Lucene40FieldsReader
-        if (fieldsReader != null && fieldsReader instanceof Lucene40StoredFieldsReader) {
-          matchingFieldsReader = (Lucene40StoredFieldsReader) fieldsReader;
-        }
-      }
-    
-      if (reader.liveDocs != null) {
-        docCount += copyFieldsWithDeletions(mergeState,
-                                            reader, matchingFieldsReader, rawDocLengths);
-      } else {
-        docCount += copyFieldsNoDeletions(mergeState,
-                                          reader, matchingFieldsReader, rawDocLengths);
-      }
-    }
-    finish(docCount);
-    return docCount;
-  }
-
-  /** Maximum number of contiguous documents to bulk-copy
-      when merging stored fields */
-  private final static int MAX_RAW_MERGE_DOCS = 4192;
-
-  private int copyFieldsWithDeletions(MergeState mergeState, final MergeState.IndexReaderAndLiveDocs reader,
-                                      final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])
-    throws IOException, MergeAbortedException, CorruptIndexException {
-    int docCount = 0;
-    final int maxDoc = reader.reader.maxDoc();
-    final Bits liveDocs = reader.liveDocs;
-    assert liveDocs != null;
-    if (matchingFieldsReader != null) {
-      // We can bulk-copy because the fieldInfos are "congruent"
-      for (int j = 0; j < maxDoc;) {
-        if (!liveDocs.get(j)) {
-          // skip deleted docs
-          ++j;
-          continue;
-        }
-        // We can optimize this case (doing a bulk byte copy) since the field
-        // numbers are identical
-        int start = j, numDocs = 0;
-        do {
-          j++;
-          numDocs++;
-          if (j >= maxDoc) break;
-          if (!liveDocs.get(j)) {
-            j++;
-            break;
-          }
-        } while(numDocs < MAX_RAW_MERGE_DOCS);
-
-        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);
-        addRawDocuments(stream, rawDocLengths, numDocs);
-        docCount += numDocs;
-        mergeState.checkAbort.work(300 * numDocs);
-      }
-    } else {
-      for (int j = 0; j < maxDoc; j++) {
-        if (!liveDocs.get(j)) {
-          // skip deleted docs
-          continue;
-        }
-        // TODO: this could be more efficient using
-        // FieldVisitor instead of loading/writing entire
-        // doc; ie we just have to renumber the field number
-        // on the fly?
-        // NOTE: it's very important to first assign to doc then pass it to
-        // fieldsWriter.addDocument; see LUCENE-1282
-        Document doc = reader.reader.document(j);
-        addDocument(doc, mergeState.fieldInfos);
-        docCount++;
-        mergeState.checkAbort.work(300);
-      }
-    }
-    return docCount;
-  }
-
-  private int copyFieldsNoDeletions(MergeState mergeState, final MergeState.IndexReaderAndLiveDocs reader,
-                                    final Lucene40StoredFieldsReader matchingFieldsReader, int rawDocLengths[])
-    throws IOException, MergeAbortedException, CorruptIndexException {
-    final int maxDoc = reader.reader.maxDoc();
-    int docCount = 0;
-    if (matchingFieldsReader != null) {
-      // We can bulk-copy because the fieldInfos are "congruent"
-      while (docCount < maxDoc) {
-        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);
-        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, docCount, len);
-        addRawDocuments(stream, rawDocLengths, len);
-        docCount += len;
-        mergeState.checkAbort.work(300 * len);
-      }
-    } else {
-      for (; docCount < maxDoc; docCount++) {
-        // NOTE: it's very important to first assign to doc then pass it to
-        // fieldsWriter.addDocument; see LUCENE-1282
-        Document doc = reader.reader.document(docCount);
-        addDocument(doc, mergeState.fieldInfos);
-        mergeState.checkAbort.work(300);
-      }
-    }
-    return docCount;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40TermVectorsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40TermVectorsFormat.java
deleted file mode 100644
index 20ebd28..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40TermVectorsFormat.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.codecs.TermVectorsFormat;
-import org.apache.lucene.index.codecs.TermVectorsReader;
-import org.apache.lucene.index.codecs.TermVectorsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-public class Lucene40TermVectorsFormat extends TermVectorsFormat {
-
-  @Override
-  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
-    return new Lucene40TermVectorsReader(directory, segmentInfo, fieldInfos, context);
-  }
-
-  @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    return new Lucene40TermVectorsWriter(directory, segment, context);
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    Lucene40TermVectorsReader.files(dir, info, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40TermVectorsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40TermVectorsReader.java
deleted file mode 100644
index 783c7a4..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40TermVectorsReader.java
+++ /dev/null
@@ -1,742 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexFormatTooNewException;
-import org.apache.lucene.index.IndexFormatTooOldException;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.TermVectorsReader;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-public class Lucene40TermVectorsReader extends TermVectorsReader {
-
-  // NOTE: if you make a new format, it must be larger than
-  // the current format
-
-  // Changed strings to UTF8 with length-in-bytes not length-in-chars
-  static final int FORMAT_UTF8_LENGTH_IN_BYTES = 4;
-
-  // NOTE: always change this if you switch to a new format!
-  // whenever you add a new format, make it 1 larger (positive version logic)!
-  static final int FORMAT_CURRENT = FORMAT_UTF8_LENGTH_IN_BYTES;
-  
-  // when removing support for old versions, leave the last supported version here
-  static final int FORMAT_MINIMUM = FORMAT_UTF8_LENGTH_IN_BYTES;
-
-  //The size in bytes that the FORMAT_VERSION will take up at the beginning of each file 
-  static final int FORMAT_SIZE = 4;
-
-  static final byte STORE_POSITIONS_WITH_TERMVECTOR = 0x1;
-
-  static final byte STORE_OFFSET_WITH_TERMVECTOR = 0x2;
-  
-  /** Extension of vectors fields file */
-  static final String VECTORS_FIELDS_EXTENSION = "tvf";
-
-  /** Extension of vectors documents file */
-  static final String VECTORS_DOCUMENTS_EXTENSION = "tvd";
-
-  /** Extension of vectors index file */
-  // TODO: shouldnt be visible to segments reader, preflex should do this itself somehow
-  public static final String VECTORS_INDEX_EXTENSION = "tvx";
-
-  private FieldInfos fieldInfos;
-
-  private IndexInput tvx;
-  private IndexInput tvd;
-  private IndexInput tvf;
-  private int size;
-  private int numTotalDocs;
-
-  // The docID offset where our docs begin in the index
-  // file.  This will be 0 if we have our own private file.
-  private int docStoreOffset;
-  
-  private final int format;
-
-  // used by clone
-  Lucene40TermVectorsReader(FieldInfos fieldInfos, IndexInput tvx, IndexInput tvd, IndexInput tvf, int size, int numTotalDocs, int docStoreOffset, int format) {
-    this.fieldInfos = fieldInfos;
-    this.tvx = tvx;
-    this.tvd = tvd;
-    this.tvf = tvf;
-    this.size = size;
-    this.numTotalDocs = numTotalDocs;
-    this.docStoreOffset = docStoreOffset;
-    this.format = format;
-  }
-    
-  public Lucene40TermVectorsReader(Directory d, SegmentInfo si, FieldInfos fieldInfos, IOContext context)
-    throws CorruptIndexException, IOException {
-    final String segment = si.getDocStoreSegment();
-    final int docStoreOffset = si.getDocStoreOffset();
-    final int size = si.docCount;
-    
-    boolean success = false;
-
-    try {
-      String idxName = IndexFileNames.segmentFileName(segment, "", VECTORS_INDEX_EXTENSION);
-      tvx = d.openInput(idxName, context);
-      format = checkValidFormat(tvx);
-      String fn = IndexFileNames.segmentFileName(segment, "", VECTORS_DOCUMENTS_EXTENSION);
-      tvd = d.openInput(fn, context);
-      final int tvdFormat = checkValidFormat(tvd);
-      fn = IndexFileNames.segmentFileName(segment, "", VECTORS_FIELDS_EXTENSION);
-      tvf = d.openInput(fn, context);
-      final int tvfFormat = checkValidFormat(tvf);
-
-      assert format == tvdFormat;
-      assert format == tvfFormat;
-
-      numTotalDocs = (int) (tvx.length() >> 4);
-
-      if (-1 == docStoreOffset) {
-        this.docStoreOffset = 0;
-        this.size = numTotalDocs;
-        assert size == 0 || numTotalDocs == size;
-      } else {
-        this.docStoreOffset = docStoreOffset;
-        this.size = size;
-        // Verify the file is long enough to hold all of our
-        // docs
-        assert numTotalDocs >= size + docStoreOffset: "numTotalDocs=" + numTotalDocs + " size=" + size + " docStoreOffset=" + docStoreOffset;
-      }
-
-      this.fieldInfos = fieldInfos;
-      success = true;
-    } finally {
-      // With lock-less commits, it's entirely possible (and
-      // fine) to hit a FileNotFound exception above. In
-      // this case, we want to explicitly close any subset
-      // of things that were opened so that we don't have to
-      // wait for a GC to do so.
-      if (!success) {
-        close();
-      }
-    }
-  }
-
-  // Used for bulk copy when merging
-  IndexInput getTvdStream() {
-    return tvd;
-  }
-
-  // Used for bulk copy when merging
-  IndexInput getTvfStream() {
-    return tvf;
-  }
-
-  private void seekTvx(final int docNum) throws IOException {
-    tvx.seek((docNum + docStoreOffset) * 16L + FORMAT_SIZE);
-  }
-
-  boolean canReadRawDocs() {
-    // we can always read raw docs, unless the term vectors
-    // didn't exist
-    return format != 0;
-  }
-
-  /** Retrieve the length (in bytes) of the tvd and tvf
-   *  entries for the next numDocs starting with
-   *  startDocID.  This is used for bulk copying when
-   *  merging segments, if the field numbers are
-   *  congruent.  Once this returns, the tvf & tvd streams
-   *  are seeked to the startDocID. */
-  final void rawDocs(int[] tvdLengths, int[] tvfLengths, int startDocID, int numDocs) throws IOException {
-
-    if (tvx == null) {
-      Arrays.fill(tvdLengths, 0);
-      Arrays.fill(tvfLengths, 0);
-      return;
-    }
-
-    seekTvx(startDocID);
-
-    long tvdPosition = tvx.readLong();
-    tvd.seek(tvdPosition);
-
-    long tvfPosition = tvx.readLong();
-    tvf.seek(tvfPosition);
-
-    long lastTvdPosition = tvdPosition;
-    long lastTvfPosition = tvfPosition;
-
-    int count = 0;
-    while (count < numDocs) {
-      final int docID = docStoreOffset + startDocID + count + 1;
-      assert docID <= numTotalDocs;
-      if (docID < numTotalDocs)  {
-        tvdPosition = tvx.readLong();
-        tvfPosition = tvx.readLong();
-      } else {
-        tvdPosition = tvd.length();
-        tvfPosition = tvf.length();
-        assert count == numDocs-1;
-      }
-      tvdLengths[count] = (int) (tvdPosition-lastTvdPosition);
-      tvfLengths[count] = (int) (tvfPosition-lastTvfPosition);
-      count++;
-      lastTvdPosition = tvdPosition;
-      lastTvfPosition = tvfPosition;
-    }
-  }
-
-  private int checkValidFormat(IndexInput in) throws CorruptIndexException, IOException
-  {
-    int format = in.readInt();
-    if (format < FORMAT_MINIMUM)
-      throw new IndexFormatTooOldException(in, format, FORMAT_MINIMUM, FORMAT_CURRENT);
-    if (format > FORMAT_CURRENT)
-      throw new IndexFormatTooNewException(in, format, FORMAT_MINIMUM, FORMAT_CURRENT);
-    return format;
-  }
-
-  public void close() throws IOException {
-    IOUtils.close(tvx, tvd, tvf);
-  }
-
-  /**
-   * 
-   * @return The number of documents in the reader
-   */
-  int size() {
-    return size;
-  }
-
-  private class TVFields extends Fields {
-    private final int[] fieldNumbers;
-    private final long[] fieldFPs;
-    private final Map<Integer,Integer> fieldNumberToIndex = new HashMap<Integer,Integer>();
-
-    public TVFields(int docID) throws IOException {
-      seekTvx(docID);
-      tvd.seek(tvx.readLong());
-      
-      final int fieldCount = tvd.readVInt();
-      assert fieldCount >= 0;
-      if (fieldCount != 0) {
-        fieldNumbers = new int[fieldCount];
-        fieldFPs = new long[fieldCount];
-        for(int fieldUpto=0;fieldUpto<fieldCount;fieldUpto++) {
-          final int fieldNumber = tvd.readVInt();
-          fieldNumbers[fieldUpto] = fieldNumber;
-          fieldNumberToIndex.put(fieldNumber, fieldUpto);
-        }
-
-        long position = tvx.readLong();
-        fieldFPs[0] = position;
-        for(int fieldUpto=1;fieldUpto<fieldCount;fieldUpto++) {
-          position += tvd.readVLong();
-          fieldFPs[fieldUpto] = position;
-        }
-      } else {
-        // TODO: we can improve writer here, eg write 0 into
-        // tvx file, so we know on first read from tvx that
-        // this doc has no TVs
-        fieldNumbers = null;
-        fieldFPs = null;
-      }
-    }
-    
-    @Override
-    public FieldsEnum iterator() throws IOException {
-
-      return new FieldsEnum() {
-        private int fieldUpto;
-
-        @Override
-        public String next() throws IOException {
-          if (fieldNumbers != null && fieldUpto < fieldNumbers.length) {
-            return fieldInfos.fieldName(fieldNumbers[fieldUpto++]);
-          } else {
-            return null;
-          }
-        }
-
-        @Override
-        public Terms terms() throws IOException {
-          return TVFields.this.terms(fieldInfos.fieldName(fieldNumbers[fieldUpto-1]));
-        }
-      };
-    }
-
-    @Override
-    public Terms terms(String field) throws IOException {
-      final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-      if (fieldInfo == null) {
-        // No such field
-        return null;
-      }
-
-      final Integer fieldIndex = fieldNumberToIndex.get(fieldInfo.number);
-      if (fieldIndex == null) {
-        // Term vectors were not indexed for this field
-        return null;
-      }
-
-      return new TVTerms(fieldFPs[fieldIndex]);
-    }
-
-    @Override
-    public int getUniqueFieldCount() {
-      if (fieldNumbers == null) {
-        return 0;
-      } else {
-        return fieldNumbers.length;
-      }
-    }
-  }
-
-  private class TVTerms extends Terms {
-    private final int numTerms;
-    private final long tvfFPStart;
-
-    public TVTerms(long tvfFP) throws IOException {
-      tvf.seek(tvfFP);
-      numTerms = tvf.readVInt();
-      tvfFPStart = tvf.getFilePointer();
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      TVTermsEnum termsEnum;
-      if (reuse instanceof TVTermsEnum) {
-        termsEnum = (TVTermsEnum) reuse;
-        if (!termsEnum.canReuse(tvf)) {
-          termsEnum = new TVTermsEnum();
-        }
-      } else {
-        termsEnum = new TVTermsEnum();
-      }
-      termsEnum.reset(numTerms, tvfFPStart);
-      return termsEnum;
-    }
-
-    @Override
-    public long getUniqueTermCount() {
-      return numTerms;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return -1;
-    }
-
-    @Override
-    public long getSumDocFreq() {
-      // Every term occurs in just one doc:
-      return numTerms;
-    }
-
-    @Override
-    public int getDocCount() {
-      return 1;
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      // TODO: really indexer hardwires
-      // this...?  I guess codec could buffer and re-sort...
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-  }
-
-  private class TVTermsEnum extends TermsEnum {
-    private final IndexInput origTVF;
-    private final IndexInput tvf;
-    private int numTerms;
-    private int nextTerm;
-    private int freq;
-    private BytesRef lastTerm = new BytesRef();
-    private BytesRef term = new BytesRef();
-    private boolean storePositions;
-    private boolean storeOffsets;
-    private long tvfFP;
-
-    private int[] positions;
-    private int[] startOffsets;
-    private int[] endOffsets;
-
-    // NOTE: tvf is pre-positioned by caller
-    public TVTermsEnum() throws IOException {
-      this.origTVF = Lucene40TermVectorsReader.this.tvf;
-      tvf = (IndexInput) origTVF.clone();
-    }
-
-    public boolean canReuse(IndexInput tvf) {
-      return tvf == origTVF;
-    }
-
-    public void reset(int numTerms, long tvfFPStart) throws IOException {
-      this.numTerms = numTerms;
-      nextTerm = 0;
-      tvf.seek(tvfFPStart);
-      final byte bits = tvf.readByte();
-      storePositions = (bits & STORE_POSITIONS_WITH_TERMVECTOR) != 0;
-      storeOffsets = (bits & STORE_OFFSET_WITH_TERMVECTOR) != 0;
-      tvfFP = 1+tvfFPStart;
-      positions = null;
-      startOffsets = null;
-      endOffsets = null;
-    }
-
-    // NOTE: slow!  (linear scan)
-    @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache)
-      throws IOException {
-      if (nextTerm != 0 && text.compareTo(term) < 0) {
-        nextTerm = 0;
-        tvf.seek(tvfFP);
-      }
-
-      while (next() != null) {
-        final int cmp = text.compareTo(term);
-        if (cmp < 0) {
-          return SeekStatus.NOT_FOUND;
-        } else if (cmp == 0) {
-          return SeekStatus.FOUND;
-        }
-      }
-
-      return SeekStatus.END;
-    }
-
-    @Override
-    public void seekExact(long ord) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      if (nextTerm >= numTerms) {
-        return null;
-      }
-      term.copyBytes(lastTerm);
-      final int start = tvf.readVInt();
-      final int deltaLen = tvf.readVInt();
-      term.length = start + deltaLen;
-      term.grow(term.length);
-      tvf.readBytes(term.bytes, start, deltaLen);
-      freq = tvf.readVInt();
-
-      if (storePositions) {
-        // TODO: we could maybe reuse last array, if we can
-        // somehow be careful about consumer never using two
-        // D&PEnums at once...
-        positions = new int[freq];
-        int pos = 0;
-        for(int posUpto=0;posUpto<freq;posUpto++) {
-          pos += tvf.readVInt();
-          positions[posUpto] = pos;
-        }
-      }
-
-      if (storeOffsets) {
-        startOffsets = new int[freq];
-        endOffsets = new int[freq];
-        int offset = 0;
-        for(int posUpto=0;posUpto<freq;posUpto++) {
-          startOffsets[posUpto] = offset + tvf.readVInt();
-          offset = endOffsets[posUpto] = startOffsets[posUpto] + tvf.readVInt();
-        }
-      }
-
-      lastTerm.copyBytes(term);
-      nextTerm++;
-      return term;
-    }
-
-    @Override
-    public BytesRef term() {
-      return term;
-    }
-
-    @Override
-    public long ord() {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public int docFreq() {
-      return 1;
-    }
-
-    @Override
-    public long totalTermFreq() {
-      return freq;
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs /* ignored */) throws IOException {
-      TVDocsEnum docsEnum;
-      if (reuse != null && reuse instanceof TVDocsEnum) {
-        docsEnum = (TVDocsEnum) reuse;
-      } else {
-        docsEnum = new TVDocsEnum();
-      }
-      docsEnum.reset(liveDocs, freq);
-      return docsEnum;
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
-      if (!storePositions && !storeOffsets) {
-        return null;
-      }
-      
-      TVDocsAndPositionsEnum docsAndPositionsEnum;
-      if (reuse != null) {
-        docsAndPositionsEnum = (TVDocsAndPositionsEnum) reuse;
-        if (docsAndPositionsEnum.canReuse(storeOffsets)) {
-          docsAndPositionsEnum = (TVDocsAndPositionsEnum) reuse;
-        } else {
-          docsAndPositionsEnum = new TVDocsAndPositionsEnum(storeOffsets);
-        }
-      } else {
-        docsAndPositionsEnum = new TVDocsAndPositionsEnum(storeOffsets);
-      }
-      docsAndPositionsEnum.reset(liveDocs, positions, startOffsets, endOffsets);
-      return docsAndPositionsEnum;
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      // TODO: really indexer hardwires
-      // this...?  I guess codec could buffer and re-sort...
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-  }
-
-  // NOTE: sort of a silly class, since you can get the
-  // freq() already by TermsEnum.totalTermFreq
-  private static class TVDocsEnum extends DocsEnum {
-    private boolean didNext;
-    private int doc = -1;
-    private int freq;
-    private Bits liveDocs;
-
-    @Override
-    public int freq() {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int nextDoc() {
-      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
-        didNext = true;
-        return (doc = 0);
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    @Override
-    public int advance(int target) {
-      if (!didNext && target == 0) {
-        return nextDoc();
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    public void reset(Bits liveDocs, int freq) {
-      this.liveDocs = liveDocs;
-      this.freq = freq;
-      this.doc = -1;
-      didNext = false;
-    }
-  }
-
-  private static class TVDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private final OffsetAttribute offsetAtt;
-    private boolean didNext;
-    private int doc = -1;
-    private int nextPos;
-    private Bits liveDocs;
-    private int[] positions;
-    private int[] startOffsets;
-    private int[] endOffsets;
-
-    public TVDocsAndPositionsEnum(boolean storeOffsets) {
-      if (storeOffsets) {
-        offsetAtt = attributes().addAttribute(OffsetAttribute.class);
-      } else {
-        offsetAtt = null;
-      }
-    }
-
-    public boolean canReuse(boolean storeOffsets) {
-      return storeOffsets == (offsetAtt != null);
-    }
-
-    @Override
-    public int freq() {
-      if (positions != null) {
-        return positions.length;
-      } else {
-        assert startOffsets != null;
-        return startOffsets.length;
-      }
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int nextDoc() {
-      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
-        didNext = true;
-        return (doc = 0);
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    @Override
-    public int advance(int target) {
-      if (!didNext && target == 0) {
-        return nextDoc();
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    public void reset(Bits liveDocs, int[] positions, int[] startOffsets, int[] endOffsets) {
-      this.liveDocs = liveDocs;
-      this.positions = positions;
-      this.startOffsets = startOffsets;
-      assert (offsetAtt != null) == (startOffsets != null);
-      this.endOffsets = endOffsets;
-      this.doc = -1;
-      didNext = false;
-      nextPos = 0;
-    }
-
-    @Override
-    public BytesRef getPayload() {
-      return null;
-    }
-
-    @Override
-    public boolean hasPayload() {
-      return false;
-    }
-
-    @Override
-    public int nextPosition() {
-      assert (positions != null && nextPos < positions.length) ||
-        startOffsets != null && nextPos < startOffsets.length;
-
-      if (startOffsets != null) {
-        offsetAtt.setOffset(startOffsets[nextPos],
-                            endOffsets[nextPos]);
-      }
-      if (positions != null) {
-        return positions[nextPos++];
-      } else {
-        nextPos++;
-        return -1;
-      }
-    }
-  }
-
-  @Override
-  public Fields get(int docID) throws IOException {
-    if (docID < 0 || docID >= numTotalDocs) {
-      throw new IllegalArgumentException("doID=" + docID + " is out of bounds [0.." + (numTotalDocs-1) + "]");
-    }
-    if (tvx != null) {
-      Fields fields = new TVFields(docID);
-      if (fields.getUniqueFieldCount() == 0) {
-        // TODO: we can improve writer here, eg write 0 into
-        // tvx file, so we know on first read from tvx that
-        // this doc has no TVs
-        return null;
-      } else {
-        return fields;
-      }
-    } else {
-      return null;
-    }
-  }
-
-  @Override
-  public TermVectorsReader clone() {
-    IndexInput cloneTvx = null;
-    IndexInput cloneTvd = null;
-    IndexInput cloneTvf = null;
-
-    // These are null when a TermVectorsReader was created
-    // on a segment that did not have term vectors saved
-    if (tvx != null && tvd != null && tvf != null) {
-      cloneTvx = (IndexInput) tvx.clone();
-      cloneTvd = (IndexInput) tvd.clone();
-      cloneTvf = (IndexInput) tvf.clone();
-    }
-    
-    return new Lucene40TermVectorsReader(fieldInfos, cloneTvx, cloneTvd, cloneTvf, size, numTotalDocs, docStoreOffset, format);
-  }
-  
-  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    if (info.getHasVectors()) {
-      if (info.getDocStoreOffset() != -1) {
-        assert info.getDocStoreSegment() != null;
-        if (!info.getDocStoreIsCompoundFile()) {
-          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_INDEX_EXTENSION));
-          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_FIELDS_EXTENSION));
-          files.add(IndexFileNames.segmentFileName(info.getDocStoreSegment(), "", VECTORS_DOCUMENTS_EXTENSION));
-        }
-      } else {
-        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_INDEX_EXTENSION));
-        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_FIELDS_EXTENSION));
-        files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_DOCUMENTS_EXTENSION));
-      }
-    }
-  }
-}
-
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40TermVectorsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40TermVectorsWriter.java
deleted file mode 100644
index aee247e..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40TermVectorsWriter.java
+++ /dev/null
@@ -1,377 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.MergePolicy.MergeAbortedException;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.SegmentReader;
-import org.apache.lucene.index.codecs.TermVectorsReader;
-import org.apache.lucene.index.codecs.TermVectorsWriter;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-// TODO: make a new 4.0 TV format that encodes better
-//   - use startOffset (not endOffset) as base for delta on
-//     next startOffset because today for syns or ngrams or
-//     WDF or shingles etc. we are encoding negative vints
-//     (= slow, 5 bytes per)
-//   - if doc has no term vectors, write 0 into the tvx
-//     file; saves a seek to tvd only to read a 0 vint (and
-//     saves a byte in tvd)
-
-public final class Lucene40TermVectorsWriter extends TermVectorsWriter {
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput tvx = null, tvd = null, tvf = null;
-
-  public Lucene40TermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    this.directory = directory;
-    this.segment = segment;
-    boolean success = false;
-    try {
-      // Open files for TermVector storage
-      tvx = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION), context);
-      tvx.writeInt(Lucene40TermVectorsReader.FORMAT_CURRENT);
-      tvd = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_DOCUMENTS_EXTENSION), context);
-      tvd.writeInt(Lucene40TermVectorsReader.FORMAT_CURRENT);
-      tvf = directory.createOutput(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_FIELDS_EXTENSION), context);
-      tvf.writeInt(Lucene40TermVectorsReader.FORMAT_CURRENT);
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
- 
-  @Override
-  public void startDocument(int numVectorFields) throws IOException {
-    lastFieldName = null;
-    this.numVectorFields = numVectorFields;
-    tvx.writeLong(tvd.getFilePointer());
-    tvx.writeLong(tvf.getFilePointer());
-    tvd.writeVInt(numVectorFields);
-    fieldCount = 0;
-    fps = ArrayUtil.grow(fps, numVectorFields);
-  }
-  
-  private long fps[] = new long[10]; // pointers to the tvf before writing each field 
-  private int fieldCount = 0;        // number of fields we have written so far for this document
-  private int numVectorFields = 0;   // total number of fields we will write for this document
-  private String lastFieldName;
-
-  @Override
-  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets) throws IOException {
-    assert lastFieldName == null || info.name.compareTo(lastFieldName) > 0: "fieldName=" + info.name + " lastFieldName=" + lastFieldName;
-    lastFieldName = info.name;
-    this.positions = positions;
-    this.offsets = offsets;
-    lastTerm.length = 0;
-    fps[fieldCount++] = tvf.getFilePointer();
-    tvd.writeVInt(info.number);
-    tvf.writeVInt(numTerms);
-    byte bits = 0x0;
-    if (positions)
-      bits |= Lucene40TermVectorsReader.STORE_POSITIONS_WITH_TERMVECTOR;
-    if (offsets)
-      bits |= Lucene40TermVectorsReader.STORE_OFFSET_WITH_TERMVECTOR;
-    tvf.writeByte(bits);
-    
-    assert fieldCount <= numVectorFields;
-    if (fieldCount == numVectorFields) {
-      // last field of the document
-      // this is crazy because the file format is crazy!
-      for (int i = 1; i < fieldCount; i++) {
-        tvd.writeVLong(fps[i] - fps[i-1]);
-      }
-    }
-  }
-  
-  private final BytesRef lastTerm = new BytesRef(10);
-
-  // NOTE: we override addProx, so we don't need to buffer when indexing.
-  // we also don't buffer during bulk merges.
-  private int offsetStartBuffer[] = new int[10];
-  private int offsetEndBuffer[] = new int[10];
-  private int offsetIndex = 0;
-  private int offsetFreq = 0;
-  private boolean positions = false;
-  private boolean offsets = false;
-
-  @Override
-  public void startTerm(BytesRef term, int freq) throws IOException {
-    final int prefix = StringHelper.bytesDifference(lastTerm, term);
-    final int suffix = term.length - prefix;
-    tvf.writeVInt(prefix);
-    tvf.writeVInt(suffix);
-    tvf.writeBytes(term.bytes, term.offset + prefix, suffix);
-    tvf.writeVInt(freq);
-    lastTerm.copyBytes(term);
-    lastPosition = lastOffset = 0;
-    
-    if (offsets && positions) {
-      // we might need to buffer if its a non-bulk merge
-      offsetStartBuffer = ArrayUtil.grow(offsetStartBuffer, freq);
-      offsetEndBuffer = ArrayUtil.grow(offsetEndBuffer, freq);
-      offsetIndex = 0;
-      offsetFreq = freq;
-    }
-  }
-
-  int lastPosition = 0;
-  int lastOffset = 0;
-
-  @Override
-  public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
-    // TODO: technically we could just copy bytes and not re-encode if we knew the length...
-    if (positions != null) {
-      for (int i = 0; i < numProx; i++) {
-        tvf.writeVInt(positions.readVInt());
-      }
-    }
-    
-    if (offsets != null) {
-      for (int i = 0; i < numProx; i++) {
-        tvf.writeVInt(offsets.readVInt());
-        tvf.writeVInt(offsets.readVInt());
-      }
-    }
-  }
-
-  @Override
-  public void addPosition(int position, int startOffset, int endOffset) throws IOException {
-    if (positions && offsets) {
-      // write position delta
-      tvf.writeVInt(position - lastPosition);
-      lastPosition = position;
-      
-      // buffer offsets
-      offsetStartBuffer[offsetIndex] = startOffset;
-      offsetEndBuffer[offsetIndex] = endOffset;
-      offsetIndex++;
-      
-      // dump buffer if we are done
-      if (offsetIndex == offsetFreq) {
-        for (int i = 0; i < offsetIndex; i++) {
-          tvf.writeVInt(offsetStartBuffer[i] - lastOffset);
-          tvf.writeVInt(offsetEndBuffer[i] - offsetStartBuffer[i]);
-          lastOffset = offsetEndBuffer[i];
-        }
-      }
-    } else if (positions) {
-      // write position delta
-      tvf.writeVInt(position - lastPosition);
-      lastPosition = position;
-    } else if (offsets) {
-      // write offset deltas
-      tvf.writeVInt(startOffset - lastOffset);
-      tvf.writeVInt(endOffset - startOffset);
-      lastOffset = endOffset;
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      close();
-    } catch (IOException ignored) {}
-    
-    try {
-      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_INDEX_EXTENSION));
-    } catch (IOException ignored) {}
-    
-    try {
-      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_DOCUMENTS_EXTENSION));
-    } catch (IOException ignored) {}
-    
-    try {
-      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", Lucene40TermVectorsReader.VECTORS_FIELDS_EXTENSION));
-    } catch (IOException ignored) {}
-  }
-
-  /**
-   * Do a bulk copy of numDocs documents from reader to our
-   * streams.  This is used to expedite merging, if the
-   * field numbers are congruent.
-   */
-  private void addRawDocuments(Lucene40TermVectorsReader reader, int[] tvdLengths, int[] tvfLengths, int numDocs) throws IOException {
-    long tvdPosition = tvd.getFilePointer();
-    long tvfPosition = tvf.getFilePointer();
-    long tvdStart = tvdPosition;
-    long tvfStart = tvfPosition;
-    for(int i=0;i<numDocs;i++) {
-      tvx.writeLong(tvdPosition);
-      tvdPosition += tvdLengths[i];
-      tvx.writeLong(tvfPosition);
-      tvfPosition += tvfLengths[i];
-    }
-    tvd.copyBytes(reader.getTvdStream(), tvdPosition-tvdStart);
-    tvf.copyBytes(reader.getTvfStream(), tvfPosition-tvfStart);
-    assert tvd.getFilePointer() == tvdPosition;
-    assert tvf.getFilePointer() == tvfPosition;
-  }
-
-  @Override
-  public final int merge(MergeState mergeState) throws IOException {
-    // Used for bulk-reading raw bytes for term vectors
-    int rawDocLengths[] = new int[MAX_RAW_MERGE_DOCS];
-    int rawDocLengths2[] = new int[MAX_RAW_MERGE_DOCS];
-
-    int idx = 0;
-    int numDocs = 0;
-    for (final MergeState.IndexReaderAndLiveDocs reader : mergeState.readers) {
-      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];
-      Lucene40TermVectorsReader matchingVectorsReader = null;
-      if (matchingSegmentReader != null) {
-        TermVectorsReader vectorsReader = matchingSegmentReader.getTermVectorsReader();
-
-        if (vectorsReader != null && vectorsReader instanceof Lucene40TermVectorsReader) {
-          // If the TV* files are an older format then they cannot read raw docs:
-          if (((Lucene40TermVectorsReader)vectorsReader).canReadRawDocs()) {
-            matchingVectorsReader = (Lucene40TermVectorsReader) vectorsReader;
-          }
-        }
-      }
-      if (reader.liveDocs != null) {
-        numDocs += copyVectorsWithDeletions(mergeState, matchingVectorsReader, reader, rawDocLengths, rawDocLengths2);
-      } else {
-        numDocs += copyVectorsNoDeletions(mergeState, matchingVectorsReader, reader, rawDocLengths, rawDocLengths2);
-      }
-    }
-    finish(numDocs);
-    return numDocs;
-  }
-
-  /** Maximum number of contiguous documents to bulk-copy
-      when merging term vectors */
-  private final static int MAX_RAW_MERGE_DOCS = 4192;
-
-  private int copyVectorsWithDeletions(MergeState mergeState,
-                                        final Lucene40TermVectorsReader matchingVectorsReader,
-                                        final MergeState.IndexReaderAndLiveDocs reader,
-                                        int rawDocLengths[],
-                                        int rawDocLengths2[])
-          throws IOException, MergeAbortedException {
-    final int maxDoc = reader.reader.maxDoc();
-    final Bits liveDocs = reader.liveDocs;
-    int totalNumDocs = 0;
-    if (matchingVectorsReader != null) {
-      // We can bulk-copy because the fieldInfos are "congruent"
-      for (int docNum = 0; docNum < maxDoc;) {
-        if (!liveDocs.get(docNum)) {
-          // skip deleted docs
-          ++docNum;
-          continue;
-        }
-        // We can optimize this case (doing a bulk byte copy) since the field
-        // numbers are identical
-        int start = docNum, numDocs = 0;
-        do {
-          docNum++;
-          numDocs++;
-          if (docNum >= maxDoc) break;
-          if (!liveDocs.get(docNum)) {
-            docNum++;
-            break;
-          }
-        } while(numDocs < MAX_RAW_MERGE_DOCS);
-        
-        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, start, numDocs);
-        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, numDocs);
-        totalNumDocs += numDocs;
-        mergeState.checkAbort.work(300 * numDocs);
-      }
-    } else {
-      for (int docNum = 0; docNum < maxDoc; docNum++) {
-        if (!liveDocs.get(docNum)) {
-          // skip deleted docs
-          continue;
-        }
-        
-        // NOTE: it's very important to first assign to vectors then pass it to
-        // termVectorsWriter.addAllDocVectors; see LUCENE-1282
-        Fields vectors = reader.reader.getTermVectors(docNum);
-        addAllDocVectors(vectors, mergeState.fieldInfos);
-        totalNumDocs++;
-        mergeState.checkAbort.work(300);
-      }
-    }
-    return totalNumDocs;
-  }
-  
-  private int copyVectorsNoDeletions(MergeState mergeState,
-                                      final Lucene40TermVectorsReader matchingVectorsReader,
-                                      final MergeState.IndexReaderAndLiveDocs reader,
-                                      int rawDocLengths[],
-                                      int rawDocLengths2[])
-          throws IOException, MergeAbortedException {
-    final int maxDoc = reader.reader.maxDoc();
-    if (matchingVectorsReader != null) {
-      // We can bulk-copy because the fieldInfos are "congruent"
-      int docCount = 0;
-      while (docCount < maxDoc) {
-        int len = Math.min(MAX_RAW_MERGE_DOCS, maxDoc - docCount);
-        matchingVectorsReader.rawDocs(rawDocLengths, rawDocLengths2, docCount, len);
-        addRawDocuments(matchingVectorsReader, rawDocLengths, rawDocLengths2, len);
-        docCount += len;
-        mergeState.checkAbort.work(300 * len);
-      }
-    } else {
-      for (int docNum = 0; docNum < maxDoc; docNum++) {
-        // NOTE: it's very important to first assign to vectors then pass it to
-        // termVectorsWriter.addAllDocVectors; see LUCENE-1282
-        Fields vectors = reader.reader.getTermVectors(docNum);
-        addAllDocVectors(vectors, mergeState.fieldInfos);
-        mergeState.checkAbort.work(300);
-      }
-    }
-    return maxDoc;
-  }
-  
-  @Override
-  public void finish(int numDocs) throws IOException {
-    if (4+((long) numDocs)*16 != tvx.getFilePointer())
-      // This is most likely a bug in Sun JRE 1.6.0_04/_05;
-      // we detect that the bug has struck, here, and
-      // throw an exception to prevent the corruption from
-      // entering the index.  See LUCENE-1282 for
-      // details.
-      throw new RuntimeException("mergeVectors produced an invalid result: mergedDocs is " + numDocs + " but tvx size is " + tvx.getFilePointer() + " file=" + tvx.toString() + "; now aborting this merge to prevent index corruption");
-  }
-
-  /** Close all streams. */
-  @Override
-  public void close() throws IOException {
-    // make an effort to close all streams we can but remember and re-throw
-    // the first exception encountered in this process
-    IOUtils.close(tvx, tvd, tvf);
-    tvx = tvd = tvf = null;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/package.html b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/package.html
deleted file mode 100644
index aca1dc4..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Standard Codec
-</body>
-</html>
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Bytes.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Bytes.java
deleted file mode 100644
index 789d3aa..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Bytes.java
+++ /dev/null
@@ -1,601 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Base class for specific Bytes Reader/Writer implementations */
-import java.io.IOException;
-import java.util.Comparator;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.DocValue;
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.ByteBlockPool.Allocator;
-import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefHash.TrackingDirectBytesStartArray;
-import org.apache.lucene.util.BytesRefHash;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Provides concrete Writer/Reader implementations for <tt>byte[]</tt> value per
- * document. There are 6 package-private default implementations of this, for
- * all combinations of {@link Mode#DEREF}/{@link Mode#STRAIGHT} x fixed-length/variable-length.
- * 
- * <p>
- * NOTE: Currently the total amount of byte[] data stored (across a single
- * segment) cannot exceed 2GB.
- * </p>
- * <p>
- * NOTE: Each byte[] must be <= 32768 bytes in length
- * </p>
- * 
- * @lucene.experimental
- */
-public final class Bytes {
-
-  static final String DV_SEGMENT_SUFFIX = "dv";
-
-  // TODO - add bulk copy where possible
-  private Bytes() { /* don't instantiate! */
-  }
-
-  /**
-   * Defines the {@link Writer}s store mode. The writer will either store the
-   * bytes sequentially ({@link #STRAIGHT}, dereferenced ({@link #DEREF}) or
-   * sorted ({@link #SORTED})
-   * 
-   * @lucene.experimental
-   */
-  public static enum Mode {
-    /**
-     * Mode for sequentially stored bytes
-     */
-    STRAIGHT,
-    /**
-     * Mode for dereferenced stored bytes
-     */
-    DEREF,
-    /**
-     * Mode for sorted stored bytes
-     */
-    SORTED
-  };
-
-  /**
-   * Creates a new <tt>byte[]</tt> {@link Writer} instances for the given
-   * directory.
-   * 
-   * @param dir
-   *          the directory to write the values to
-   * @param id
-   *          the id used to create a unique file name. Usually composed out of
-   *          the segment name and a unique id per segment.
-   * @param mode
-   *          the writers store mode
-   * @param fixedSize
-   *          <code>true</code> if all bytes subsequently passed to the
-   *          {@link Writer} will have the same length
-   * @param sortComparator {@link BytesRef} comparator used by sorted variants. 
-   *        If <code>null</code> {@link BytesRef#getUTF8SortedAsUnicodeComparator()}
-   *        is used instead
-   * @param bytesUsed
-   *          an {@link AtomicLong} instance to track the used bytes within the
-   *          {@link Writer}. A call to {@link Writer#finish(int)} will release
-   *          all internally used resources and frees the memory tracking
-   *          reference.
-   * @param context 
-   * @return a new {@link Writer} instance
-   * @throws IOException
-   *           if the files for the writer can not be created.
-   */
-  public static Writer getWriter(Directory dir, String id, Mode mode,
-      boolean fixedSize, Comparator<BytesRef> sortComparator, Counter bytesUsed, IOContext context)
-      throws IOException {
-    // TODO -- i shouldn't have to specify fixed? can
-    // track itself & do the write thing at write time?
-    if (sortComparator == null) {
-      sortComparator = BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    if (fixedSize) {
-      if (mode == Mode.STRAIGHT) {
-        return new FixedStraightBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.DEREF) {
-        return new FixedDerefBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.SORTED) {
-        return new FixedSortedBytesImpl.Writer(dir, id, sortComparator, bytesUsed, context);
-      }
-    } else {
-      if (mode == Mode.STRAIGHT) {
-        return new VarStraightBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.DEREF) {
-        return new VarDerefBytesImpl.Writer(dir, id, bytesUsed, context);
-      } else if (mode == Mode.SORTED) {
-        return new VarSortedBytesImpl.Writer(dir, id, sortComparator, bytesUsed, context);
-      }
-    }
-
-    throw new IllegalArgumentException("");
-  }
-
-  /**
-   * Creates a new {@link DocValues} instance that provides either memory
-   * resident or iterative access to a per-document stored <tt>byte[]</tt>
-   * value. The returned {@link DocValues} instance will be initialized without
-   * consuming a significant amount of memory.
-   * 
-   * @param dir
-   *          the directory to load the {@link DocValues} from.
-   * @param id
-   *          the file ID in the {@link Directory} to load the values from.
-   * @param mode
-   *          the mode used to store the values
-   * @param fixedSize
-   *          <code>true</code> iff the values are stored with fixed-size,
-   *          otherwise <code>false</code>
-   * @param maxDoc
-   *          the number of document values stored for the given ID
-   * @param sortComparator {@link BytesRef} comparator used by sorted variants. 
-   *        If <code>null</code> {@link BytesRef#getUTF8SortedAsUnicodeComparator()}
-   *        is used instead
-   * @return an initialized {@link DocValues} instance.
-   * @throws IOException
-   *           if an {@link IOException} occurs
-   */
-  public static DocValues getValues(Directory dir, String id, Mode mode,
-      boolean fixedSize, int maxDoc, Comparator<BytesRef> sortComparator, IOContext context) throws IOException {
-    if (sortComparator == null) {
-      sortComparator = BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-    // TODO -- I can peek @ header to determing fixed/mode?
-    if (fixedSize) {
-      if (mode == Mode.STRAIGHT) {
-        return new FixedStraightBytesImpl.FixedStraightReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.DEREF) {
-        return new FixedDerefBytesImpl.FixedDerefReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.SORTED) {
-        return new FixedSortedBytesImpl.Reader(dir, id, maxDoc, context, Type.BYTES_FIXED_SORTED, sortComparator);
-      }
-    } else {
-      if (mode == Mode.STRAIGHT) {
-        return new VarStraightBytesImpl.VarStraightReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.DEREF) {
-        return new VarDerefBytesImpl.VarDerefReader(dir, id, maxDoc, context);
-      } else if (mode == Mode.SORTED) {
-        return new VarSortedBytesImpl.Reader(dir, id, maxDoc,context, Type.BYTES_VAR_SORTED, sortComparator);
-      }
-    }
-
-    throw new IllegalArgumentException("Illegal Mode: " + mode);
-  }
-
-  // TODO open up this API?
-  static abstract class BytesSourceBase extends Source {
-    private final PagedBytes pagedBytes;
-    protected final IndexInput datIn;
-    protected final IndexInput idxIn;
-    protected final static int PAGED_BYTES_BITS = 15;
-    protected final PagedBytes.Reader data;
-    protected final long totalLengthInBytes;
-    
-
-    protected BytesSourceBase(IndexInput datIn, IndexInput idxIn,
-        PagedBytes pagedBytes, long bytesToRead, Type type) throws IOException {
-      super(type);
-      assert bytesToRead <= datIn.length() : " file size is less than the expected size diff: "
-          + (bytesToRead - datIn.length()) + " pos: " + datIn.getFilePointer();
-      this.datIn = datIn;
-      this.totalLengthInBytes = bytesToRead;
-      this.pagedBytes = pagedBytes;
-      this.pagedBytes.copy(datIn, bytesToRead);
-      data = pagedBytes.freeze(true);
-      this.idxIn = idxIn;
-    }
-  }
-  
-  // TODO: open up this API?!
-  static abstract class BytesWriterBase extends Writer {
-    private final String id;
-    private IndexOutput idxOut;
-    private IndexOutput datOut;
-    protected BytesRef bytesRef = new BytesRef();
-    private final Directory dir;
-    private final String codecName;
-    private final int version;
-    private final IOContext context;
-
-    protected BytesWriterBase(Directory dir, String id, String codecName,
-        int version, Counter bytesUsed, IOContext context) throws IOException {
-      super(bytesUsed);
-      this.id = id;
-      this.dir = dir;
-      this.codecName = codecName;
-      this.version = version;
-      this.context = context;
-    }
-    
-    protected IndexOutput getOrCreateDataOut() throws IOException {
-      if (datOut == null) {
-        boolean success = false;
-        try {
-          datOut = dir.createOutput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-              DATA_EXTENSION), context);
-          CodecUtil.writeHeader(datOut, codecName, version);
-          success = true;
-        } finally {
-          if (!success) {
-            IOUtils.closeWhileHandlingException(datOut);
-          }
-        }
-      }
-      return datOut;
-    }
-    
-    protected IndexOutput getIndexOut() {
-      return idxOut;
-    }
-    
-    protected IndexOutput getDataOut() {
-      return datOut;
-    }
-
-    protected IndexOutput getOrCreateIndexOut() throws IOException {
-      boolean success = false;
-      try {
-        if (idxOut == null) {
-          idxOut = dir.createOutput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-              INDEX_EXTENSION), context);
-          CodecUtil.writeHeader(idxOut, codecName, version);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(idxOut);
-        }
-      }
-      return idxOut;
-    }
-    /**
-     * Must be called only with increasing docIDs. It's OK for some docIDs to be
-     * skipped; they will be filled with 0 bytes.
-     */
-    @Override
-    public abstract void add(int docID, BytesRef bytes) throws IOException;
-
-    @Override
-    public abstract void finish(int docCount) throws IOException;
-
-    @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
-      add(docID, currentMergeSource.getBytes(sourceDoc, bytesRef));
-    }
-
-    @Override
-    public void add(int docID, DocValue docValue) throws IOException {
-      final BytesRef ref;
-      if ((ref = docValue.getBytes()) != null) {
-        add(docID, ref);
-      }
-    }
-  }
-
-  /**
-   * Opens all necessary files, but does not read any data in until you call
-   * {@link #load}.
-   */
-  static abstract class BytesReaderBase extends DocValues {
-    protected final IndexInput idxIn;
-    protected final IndexInput datIn;
-    protected final int version;
-    protected final String id;
-    protected final Type type;
-
-    protected BytesReaderBase(Directory dir, String id, String codecName,
-        int maxVersion, boolean doIndex, IOContext context, Type type) throws IOException {
-      IndexInput dataIn = null;
-      IndexInput indexIn = null;
-      boolean success = false;
-      try {
-        dataIn = dir.openInput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-                                                              Writer.DATA_EXTENSION), context);
-        version = CodecUtil.checkHeader(dataIn, codecName, maxVersion, maxVersion);
-        if (doIndex) {
-          indexIn = dir.openInput(IndexFileNames.segmentFileName(id, DV_SEGMENT_SUFFIX,
-                                                                 Writer.INDEX_EXTENSION), context);
-          final int version2 = CodecUtil.checkHeader(indexIn, codecName,
-                                                     maxVersion, maxVersion);
-          assert version == version2;
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(dataIn, indexIn);
-        }
-      }
-      datIn = dataIn;
-      idxIn = indexIn;
-      this.type = type;
-      this.id = id;
-    }
-
-    /**
-     * clones and returns the data {@link IndexInput}
-     */
-    protected final IndexInput cloneData() {
-      assert datIn != null;
-      return (IndexInput) datIn.clone();
-    }
-
-    /**
-     * clones and returns the indexing {@link IndexInput}
-     */
-    protected final IndexInput cloneIndex() {
-      assert idxIn != null;
-      return (IndexInput) idxIn.clone();
-    }
-
-    @Override
-    public void close() throws IOException {
-      try {
-        super.close();
-      } finally {
-         IOUtils.close(datIn, idxIn);
-      }
-    }
-
-    @Override
-    public Type type() {
-      return type;
-    }
-    
-  }
-  
-  static abstract class DerefBytesWriterBase extends BytesWriterBase {
-    protected int size = -1;
-    protected int lastDocId = -1;
-    protected int[] docToEntry;
-    protected final BytesRefHash hash;
-    protected long maxBytes = 0;
-    
-    protected DerefBytesWriterBase(Directory dir, String id, String codecName,
-        int codecVersion, Counter bytesUsed, IOContext context)
-        throws IOException {
-      this(dir, id, codecName, codecVersion, new DirectTrackingAllocator(
-          ByteBlockPool.BYTE_BLOCK_SIZE, bytesUsed), bytesUsed, context);
-    }
-
-    protected DerefBytesWriterBase(Directory dir, String id, String codecName, int codecVersion, Allocator allocator,
-        Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, codecName, codecVersion, bytesUsed, context);
-      hash = new BytesRefHash(new ByteBlockPool(allocator),
-          BytesRefHash.DEFAULT_CAPACITY, new TrackingDirectBytesStartArray(
-              BytesRefHash.DEFAULT_CAPACITY, bytesUsed));
-      docToEntry = new int[1];
-      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT);
-    }
-    
-    protected static int writePrefixLength(DataOutput datOut, BytesRef bytes)
-        throws IOException {
-      if (bytes.length < 128) {
-        datOut.writeByte((byte) bytes.length);
-        return 1;
-      } else {
-        datOut.writeByte((byte) (0x80 | (bytes.length >> 8)));
-        datOut.writeByte((byte) (bytes.length & 0xff));
-        return 2;
-      }
-    }
-
-    @Override
-    public void add(int docID, BytesRef bytes) throws IOException {
-      if (bytes.length == 0) { // default value - skip it
-        return;
-      }
-      checkSize(bytes);
-      fillDefault(docID);
-      int ord = hash.add(bytes);
-      if (ord < 0) {
-        ord = (-ord) - 1;
-      } else {
-        maxBytes += bytes.length;
-      }
-      
-      
-      docToEntry[docID] = ord;
-      lastDocId = docID;
-    }
-    
-    protected void fillDefault(int docID) {
-      if (docID >= docToEntry.length) {
-        final int size = docToEntry.length;
-        docToEntry = ArrayUtil.grow(docToEntry, 1 + docID);
-        bytesUsed.addAndGet((docToEntry.length - size)
-            * RamUsageEstimator.NUM_BYTES_INT);
-      }
-      assert size >= 0;
-      BytesRef ref = new BytesRef(size);
-      ref.length = size;
-      int ord = hash.add(ref);
-      if (ord < 0) {
-        ord = (-ord) - 1;
-      }
-      for (int i = lastDocId+1; i < docID; i++) {
-        docToEntry[i] = ord;
-      }
-    }
-    
-    protected void checkSize(BytesRef bytes) {
-      if (size == -1) {
-        size = bytes.length;
-      } else if (bytes.length != size) {
-        throw new IllegalArgumentException("expected bytes size=" + size
-            + " but got " + bytes.length);
-      }
-    }
-    
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      try {
-        finishInternal(docCount);
-        success = true;
-      } finally {
-        releaseResources();
-        if (success) {
-          IOUtils.close(getIndexOut(), getDataOut());
-        } else {
-          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
-        }
-        
-      }
-    }
-    
-    protected abstract void finishInternal(int docCount) throws IOException;
-    
-    protected void releaseResources() {
-      hash.close();
-      bytesUsed.addAndGet((-docToEntry.length) * RamUsageEstimator.NUM_BYTES_INT);
-      docToEntry = null;
-    }
-    
-    protected void writeIndex(IndexOutput idxOut, int docCount,
-        long maxValue, int[] toEntry) throws IOException {
-      writeIndex(idxOut, docCount, maxValue, (int[])null, toEntry);
-    }
-    
-    protected void writeIndex(IndexOutput idxOut, int docCount,
-        long maxValue, int[] addresses, int[] toEntry) throws IOException {
-      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,
-          PackedInts.bitsRequired(maxValue));
-      final int limit = docCount > docToEntry.length ? docToEntry.length
-          : docCount;
-      assert toEntry.length >= limit -1;
-      if (addresses != null) {
-        for (int i = 0; i < limit; i++) {
-          assert addresses[toEntry[i]] >= 0;
-          w.add(addresses[toEntry[i]]);
-        }
-      } else {
-        for (int i = 0; i < limit; i++) {
-          assert toEntry[i] >= 0;
-          w.add(toEntry[i]);
-        }
-      }
-      for (int i = limit; i < docCount; i++) {
-        w.add(0);
-      }
-      w.finish();
-    }
-    
-    protected void writeIndex(IndexOutput idxOut, int docCount,
-        long maxValue, long[] addresses, int[] toEntry) throws IOException {
-      final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount,
-          PackedInts.bitsRequired(maxValue));
-      final int limit = docCount > docToEntry.length ? docToEntry.length
-          : docCount;
-      assert toEntry.length >= limit -1;
-      if (addresses != null) {
-        for (int i = 0; i < limit; i++) {
-          assert addresses[toEntry[i]] >= 0;
-          w.add(addresses[toEntry[i]]);
-        }
-      } else {
-        for (int i = 0; i < limit; i++) {
-          assert toEntry[i] >= 0;
-          w.add(toEntry[i]);
-        }
-      }
-      for (int i = limit; i < docCount; i++) {
-        w.add(0);
-      }
-      w.finish();
-    }
-    
-  }
-  
-  static abstract class BytesSortedSourceBase extends SortedSource {
-    private final PagedBytes pagedBytes;
-    
-    protected final PackedInts.Reader docToOrdIndex;
-    protected final PackedInts.Reader ordToOffsetIndex;
-
-    protected final IndexInput datIn;
-    protected final IndexInput idxIn;
-    protected final BytesRef defaultValue = new BytesRef();
-    protected final static int PAGED_BYTES_BITS = 15;
-    protected final PagedBytes.Reader data;
-
-    protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comp, long bytesToRead, Type type, boolean hasOffsets) throws IOException {
-      this(datIn, idxIn, comp, new PagedBytes(PAGED_BYTES_BITS), bytesToRead, type, hasOffsets);
-    }
-    
-    protected BytesSortedSourceBase(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comp, PagedBytes pagedBytes, long bytesToRead, Type type, boolean hasOffsets)
-        throws IOException {
-      super(type, comp);
-      assert bytesToRead <= datIn.length() : " file size is less than the expected size diff: "
-          + (bytesToRead - datIn.length()) + " pos: " + datIn.getFilePointer();
-      this.datIn = datIn;
-      this.pagedBytes = pagedBytes;
-      this.pagedBytes.copy(datIn, bytesToRead);
-      data = pagedBytes.freeze(true);
-      this.idxIn = idxIn;
-      ordToOffsetIndex = hasOffsets ? PackedInts.getReader(idxIn) : null; 
-      docToOrdIndex = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public boolean hasPackedDocToOrd() {
-      return true;
-    }
-
-    @Override
-    public PackedInts.Reader getDocToOrd() {
-      return docToOrdIndex;
-    }
-    
-    @Override
-    public int ord(int docID) {
-      assert docToOrdIndex.get(docID) < getValueCount();
-      return (int) docToOrdIndex.get(docID);
-    }
-
-    protected void closeIndexInput() throws IOException {
-      IOUtils.close(datIn, idxIn);
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/BytesRefUtils.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/BytesRefUtils.java
deleted file mode 100644
index d0a2b90..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/BytesRefUtils.java
+++ /dev/null
@@ -1,120 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Package private BytesRefUtils - can move this into the o.a.l.utils package if
- * needed.
- * 
- * @lucene.internal
- */
-public final class BytesRefUtils {
-
-  private BytesRefUtils() {
-  }
-
-  /**
-   * Copies the given long value and encodes it as 8 byte Big-Endian.
-   * <p>
-   * NOTE: this method resets the offset to 0, length to 8 and resizes the
-   * reference array if needed.
-   */
-  public static void copyLong(BytesRef ref, long value) {
-    if (ref.bytes.length < 8) {
-      ref.bytes = new byte[8];
-    }
-    copyInternal(ref, (int) (value >> 32), ref.offset = 0);
-    copyInternal(ref, (int) value, 4);
-    ref.length = 8;
-  }
-
-  /**
-   * Copies the given int value and encodes it as 4 byte Big-Endian.
-   * <p>
-   * NOTE: this method resets the offset to 0, length to 4 and resizes the
-   * reference array if needed.
-   */
-  public static void copyInt(BytesRef ref, int value) {
-    if (ref.bytes.length < 4) {
-      ref.bytes = new byte[4];
-    }
-    copyInternal(ref, value, ref.offset = 0);
-    ref.length = 4;
-  }
-
-  /**
-   * Copies the given short value and encodes it as a 2 byte Big-Endian.
-   * <p>
-   * NOTE: this method resets the offset to 0, length to 2 and resizes the
-   * reference array if needed.
-   */
-  public static void copyShort(BytesRef ref, short value) {
-    if (ref.bytes.length < 2) {
-      ref.bytes = new byte[2];
-    }
-    ref.bytes[ref.offset] = (byte) (value >> 8);
-    ref.bytes[ref.offset + 1] = (byte) (value);
-    ref.length = 2;
-  }
-
-  private static void copyInternal(BytesRef ref, int value, int startOffset) {
-    ref.bytes[startOffset] = (byte) (value >> 24);
-    ref.bytes[startOffset + 1] = (byte) (value >> 16);
-    ref.bytes[startOffset + 2] = (byte) (value >> 8);
-    ref.bytes[startOffset + 3] = (byte) (value);
-  }
-
-  /**
-   * Converts 2 consecutive bytes from the current offset to a short. Bytes are
-   * interpreted as Big-Endian (most significant bit first)
-   * <p>
-   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
-   */
-  public static short asShort(BytesRef b) {
-    return (short) (0xFFFF & ((b.bytes[b.offset] & 0xFF) << 8) | (b.bytes[b.offset + 1] & 0xFF));
-  }
-
-  /**
-   * Converts 4 consecutive bytes from the current offset to an int. Bytes are
-   * interpreted as Big-Endian (most significant bit first)
-   * <p>
-   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
-   */
-  public static int asInt(BytesRef b) {
-    return asIntInternal(b, b.offset);
-  }
-
-  /**
-   * Converts 8 consecutive bytes from the current offset to a long. Bytes are
-   * interpreted as Big-Endian (most significant bit first)
-   * <p>
-   * NOTE: this method does <b>NOT</b> check the bounds of the referenced array.
-   */
-  public static long asLong(BytesRef b) {
-    return (((long) asIntInternal(b, b.offset) << 32) | asIntInternal(b,
-        b.offset + 4) & 0xFFFFFFFFL);
-  }
-
-  private static int asIntInternal(BytesRef b, int pos) {
-    return ((b.bytes[pos++] & 0xFF) << 24) | ((b.bytes[pos++] & 0xFF) << 16)
-        | ((b.bytes[pos++] & 0xFF) << 8) | (b.bytes[pos] & 0xFF);
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DirectSource.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DirectSource.java
deleted file mode 100644
index d5ee351..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DirectSource.java
+++ /dev/null
@@ -1,139 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Base class for disk resident source implementations
- * @lucene.internal
- */
-public abstract class DirectSource extends Source {
-
-  protected final IndexInput data;
-  private final ToNumeric toNumeric;
-  protected final long baseOffset;
-
-  public DirectSource(IndexInput input, Type type) {
-    super(type);
-    this.data = input;
-    baseOffset = input.getFilePointer();
-    switch (type) {
-    case FIXED_INTS_16:
-      toNumeric = new ShortToLong();
-      break;
-    case FLOAT_32:
-    case FIXED_INTS_32:
-      toNumeric = new IntToLong();
-      break;
-    case FIXED_INTS_8:
-      toNumeric = new ByteToLong();
-      break;
-    default:
-      toNumeric = new LongToLong();
-    }
-  }
-
-  @Override
-  public BytesRef getBytes(int docID, BytesRef ref) {
-    try {
-      final int sizeToRead = position(docID);
-      ref.grow(sizeToRead);
-      data.readBytes(ref.bytes, 0, sizeToRead);
-      ref.length = sizeToRead;
-      ref.offset = 0;
-      return ref;
-    } catch (IOException ex) {
-      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
-    }
-  }
-
-  @Override
-  public long getInt(int docID) {
-    try {
-      position(docID);
-      return toNumeric.toLong(data);
-    } catch (IOException ex) {
-      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
-    }
-  }
-
-  @Override
-  public double getFloat(int docID) {
-    try {
-      position(docID);
-      return toNumeric.toDouble(data);
-    } catch (IOException ex) {
-      throw new IllegalStateException("failed to get value for docID: " + docID, ex);
-    }
-  }
-
-  protected abstract int position(int docID) throws IOException;
-
-  private abstract static class ToNumeric {
-    abstract long toLong(IndexInput input) throws IOException;
-
-    double toDouble(IndexInput input) throws IOException {
-      return toLong(input);
-    }
-  }
-
-  private static final class ByteToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readByte();
-    }
-
-  }
-
-  private static final class ShortToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readShort();
-    }
-  }
-
-  private static final class IntToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readInt();
-    }
-
-    double toDouble(IndexInput input) throws IOException {
-      return Float.intBitsToFloat(input.readInt());
-    }
-  }
-
-  private static final class LongToLong extends ToNumeric {
-    @Override
-    long toLong(IndexInput input) throws IOException {
-      return input.readLong();
-    }
-
-    double toDouble(IndexInput input) throws IOException {
-      return Double.longBitsToDouble(input.readLong());
-    }
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DocValuesArray.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DocValuesArray.java
deleted file mode 100644
index cc3c76a..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/DocValuesArray.java
+++ /dev/null
@@ -1,306 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.EnumMap;
-import java.util.Map;
-
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-/**
- * @lucene.experimental
- */
-abstract class DocValuesArray extends Source {
-
-  static final Map<Type, DocValuesArray> TEMPLATES;
-
-  static {
-    EnumMap<Type, DocValuesArray> templates = new EnumMap<Type, DocValuesArray>(
-        Type.class);
-    templates.put(Type.FIXED_INTS_16, new ShortValues());
-    templates.put(Type.FIXED_INTS_32, new IntValues());
-    templates.put(Type.FIXED_INTS_64, new LongValues());
-    templates.put(Type.FIXED_INTS_8, new ByteValues());
-    templates.put(Type.FLOAT_32, new FloatValues());
-    templates.put(Type.FLOAT_64, new DoubleValues());
-    TEMPLATES = Collections.unmodifiableMap(templates);
-  }
-
-  protected final int bytesPerValue;
-
-  DocValuesArray(int bytesPerValue, Type type) {
-    super(type);
-    this.bytesPerValue = bytesPerValue;
-  }
-
-  public abstract DocValuesArray newFromInput(IndexInput input, int numDocs)
-      throws IOException;
-
-  @Override
-  public final boolean hasArray() {
-    return true;
-  }
-
-  void toBytes(long value, BytesRef bytesRef) {
-    BytesRefUtils.copyLong(bytesRef, value);
-  }
-
-  void toBytes(double value, BytesRef bytesRef) {
-    BytesRefUtils.copyLong(bytesRef, Double.doubleToRawLongBits(value));
-  }
-
-  final static class ByteValues extends DocValuesArray {
-    private final byte[] values;
-
-    ByteValues() {
-      super(1, Type.FIXED_INTS_8);
-      values = new byte[0];
-    }
-
-    private ByteValues(IndexInput input, int numDocs) throws IOException {
-      super(1, Type.FIXED_INTS_8);
-      values = new byte[numDocs];
-      input.readBytes(values, 0, values.length, false);
-    }
-
-    @Override
-    public byte[] getArray() {
-      return values;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public DocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new ByteValues(input, numDocs);
-    }
-
-    void toBytes(long value, BytesRef bytesRef) {
-      bytesRef.bytes[0] = (byte) (0xFFL & value);
-    }
-
-  };
-
-  final static class ShortValues extends DocValuesArray {
-    private final short[] values;
-
-    ShortValues() {
-      super(RamUsageEstimator.NUM_BYTES_SHORT, Type.FIXED_INTS_16);
-      values = new short[0];
-    }
-
-    private ShortValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_SHORT, Type.FIXED_INTS_16);
-      values = new short[numDocs];
-      for (int i = 0; i < values.length; i++) {
-        values[i] = input.readShort();
-      }
-    }
-
-    @Override
-    public short[] getArray() {
-      return values;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public DocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new ShortValues(input, numDocs);
-    }
-
-    void toBytes(long value, BytesRef bytesRef) {
-      BytesRefUtils.copyShort(bytesRef, (short) (0xFFFFL & value));
-    }
-
-  };
-
-  final static class IntValues extends DocValuesArray {
-    private final int[] values;
-
-    IntValues() {
-      super(RamUsageEstimator.NUM_BYTES_INT, Type.FIXED_INTS_32);
-      values = new int[0];
-    }
-
-    private IntValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_INT, Type.FIXED_INTS_32);
-      values = new int[numDocs];
-      for (int i = 0; i < values.length; i++) {
-        values[i] = input.readInt();
-      }
-    }
-
-    @Override
-    public int[] getArray() {
-      return values;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return 0xFFFFFFFF & values[docID];
-    }
-
-    @Override
-    public DocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new IntValues(input, numDocs);
-    }
-
-    void toBytes(long value, BytesRef bytesRef) {
-      BytesRefUtils.copyInt(bytesRef, (int) (0xFFFFFFFF & value));
-    }
-
-  };
-
-  final static class LongValues extends DocValuesArray {
-    private final long[] values;
-
-    LongValues() {
-      super(RamUsageEstimator.NUM_BYTES_LONG, Type.FIXED_INTS_64);
-      values = new long[0];
-    }
-
-    private LongValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_LONG, Type.FIXED_INTS_64);
-      values = new long[numDocs];
-      for (int i = 0; i < values.length; i++) {
-        values[i] = input.readLong();
-      }
-    }
-
-    @Override
-    public long[] getArray() {
-      return values;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public DocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new LongValues(input, numDocs);
-    }
-
-  };
-
-  final static class FloatValues extends DocValuesArray {
-    private final float[] values;
-
-    FloatValues() {
-      super(RamUsageEstimator.NUM_BYTES_FLOAT, Type.FLOAT_32);
-      values = new float[0];
-    }
-
-    private FloatValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_FLOAT, Type.FLOAT_32);
-      values = new float[numDocs];
-      /*
-       * we always read BIG_ENDIAN here since the writer serialized plain bytes
-       * we can simply read the ints / longs back in using readInt / readLong
-       */
-      for (int i = 0; i < values.length; i++) {
-        values[i] = Float.intBitsToFloat(input.readInt());
-      }
-    }
-
-    @Override
-    public float[] getArray() {
-      return values;
-    }
-
-    @Override
-    public double getFloat(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-    
-    @Override
-    void toBytes(double value, BytesRef bytesRef) {
-      BytesRefUtils.copyInt(bytesRef, Float.floatToRawIntBits((float)value));
-
-    }
-
-    @Override
-    public DocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new FloatValues(input, numDocs);
-    }
-  };
-
-  final static class DoubleValues extends DocValuesArray {
-    private final double[] values;
-
-    DoubleValues() {
-      super(RamUsageEstimator.NUM_BYTES_DOUBLE, Type.FLOAT_64);
-      values = new double[0];
-    }
-
-    private DoubleValues(IndexInput input, int numDocs) throws IOException {
-      super(RamUsageEstimator.NUM_BYTES_DOUBLE, Type.FLOAT_64);
-      values = new double[numDocs];
-      /*
-       * we always read BIG_ENDIAN here since the writer serialized plain bytes
-       * we can simply read the ints / longs back in using readInt / readLong
-       */
-      for (int i = 0; i < values.length; i++) {
-        values[i] = Double.longBitsToDouble(input.readLong());
-      }
-    }
-
-    @Override
-    public double[] getArray() {
-      return values;
-    }
-
-    @Override
-    public double getFloat(int docID) {
-      assert docID >= 0 && docID < values.length;
-      return values[docID];
-    }
-
-    @Override
-    public DocValuesArray newFromInput(IndexInput input, int numDocs)
-        throws IOException {
-      return new DoubleValues(input, numDocs);
-    }
-
-  };
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedDerefBytesImpl.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedDerefBytesImpl.java
deleted file mode 100644
index 6052ddd..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedDerefBytesImpl.java
+++ /dev/null
@@ -1,134 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSourceBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores fixed-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[]
-/**
- * @lucene.experimental
- */
-class FixedDerefBytesImpl {
-
-  static final String CODEC_NAME = "FixedDerefBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  public static class Writer extends DerefBytesWriterBase {
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-    }
-
-    @Override
-    protected void finishInternal(int docCount) throws IOException {
-      final int numValues = hash.size();
-      final IndexOutput datOut = getOrCreateDataOut();
-      datOut.writeInt(size);
-      if (size != -1) {
-        final BytesRef bytesRef = new BytesRef(size);
-        for (int i = 0; i < numValues; i++) {
-          hash.get(i, bytesRef);
-          datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-        }
-      }
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      idxOut.writeInt(numValues);
-      writeIndex(idxOut, docCount, numValues, docToEntry);
-    }
-  }
-
-  public static class FixedDerefReader extends BytesReaderBase {
-    private final int size;
-    private final int numValuesStored;
-    FixedDerefReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_START, true, context, Type.BYTES_FIXED_DEREF);
-      size = datIn.readInt();
-      numValuesStored = idxIn.readInt();
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return new FixedDerefSource(cloneData(), cloneIndex(), size, numValuesStored);
-    }
-
-    @Override
-    public Source getDirectSource()
-        throws IOException {
-      return new DirectFixedDerefSource(cloneData(), cloneIndex(), size, type());
-    }
-
-    @Override
-    public int getValueSize() {
-      return size;
-    }
-    
-  }
-  
-  static final class FixedDerefSource extends BytesSourceBase {
-    private final int size;
-    private final PackedInts.Reader addresses;
-
-    protected FixedDerefSource(IndexInput datIn, IndexInput idxIn, int size, long numValues) throws IOException {
-      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), size * numValues,
-          Type.BYTES_FIXED_DEREF);
-      this.size = size;
-      addresses = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      final int id = (int) addresses.get(docID);
-      return data.fillSlice(bytesRef, (id * size), size);
-    }
-
-  }
-  
-  final static class DirectFixedDerefSource extends DirectSource {
-    private final PackedInts.Reader index;
-    private final int size;
-
-    DirectFixedDerefSource(IndexInput data, IndexInput index, int size, Type type)
-        throws IOException {
-      super(data, type);
-      this.size = size;
-      this.index = PackedInts.getDirectReader(index);
-    }
-
-    @Override
-    protected int position(int docID) throws IOException {
-      data.seek(baseOffset + index.get(docID) * size);
-      return size;
-    }
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedSortedBytesImpl.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedSortedBytesImpl.java
deleted file mode 100644
index 0abf11e..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedSortedBytesImpl.java
+++ /dev/null
@@ -1,230 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.SortedBytesMergeUtils;
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.SortedBytesMergeUtils.MergeContext;
-import org.apache.lucene.index.SortedBytesMergeUtils.SortedSourceSlice;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSortedSourceBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores fixed-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[]
-
-/**
- * @lucene.experimental
- */
-class FixedSortedBytesImpl {
-
-  static final String CODEC_NAME = "FixedSortedBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  static final class Writer extends DerefBytesWriterBase {
-    private final Comparator<BytesRef> comp;
-
-    public Writer(Directory dir, String id, Comparator<BytesRef> comp,
-        Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      this.comp = comp;
-    }
-
-    @Override
-    public void merge(MergeState mergeState, DocValues[] docValues)
-        throws IOException {
-      boolean success = false;
-      try {
-        final MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_FIXED_SORTED, docValues, comp, mergeState);
-        List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState, docValues, ctx);
-        final IndexOutput datOut = getOrCreateDataOut();
-        datOut.writeInt(ctx.sizePerValues);
-        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, datOut, slices);
-        
-        final IndexOutput idxOut = getOrCreateIndexOut();
-        idxOut.writeInt(maxOrd);
-        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
-            PackedInts.bitsRequired(maxOrd));
-        for (SortedSourceSlice slice : slices) {
-          slice.writeOrds(ordsWriter);
-        }
-        ordsWriter.finish();
-        success = true;
-      } finally {
-        releaseResources();
-        if (success) {
-          IOUtils.close(getIndexOut(), getDataOut());
-        } else {
-          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
-        }
-
-      }
-    }
-
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finishInternal(int docCount) throws IOException {
-      fillDefault(docCount);
-      final IndexOutput datOut = getOrCreateDataOut();
-      final int count = hash.size();
-      final int[] address = new int[count];
-      datOut.writeInt(size);
-      if (size != -1) {
-        final int[] sortedEntries = hash.sort(comp);
-        // first dump bytes data, recording address as we go
-        final BytesRef spare = new BytesRef(size);
-        for (int i = 0; i < count; i++) {
-          final int e = sortedEntries[i];
-          final BytesRef bytes = hash.get(e, spare);
-          assert bytes.length == size;
-          datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-          address[e] = i;
-        }
-      }
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      idxOut.writeInt(count);
-      writeIndex(idxOut, docCount, count, address, docToEntry);
-    }
-  }
-
-  static final class Reader extends BytesReaderBase {
-    private final int size;
-    private final int valueCount;
-    private final Comparator<BytesRef> comparator;
-
-    public Reader(Directory dir, String id, int maxDoc, IOContext context,
-        Type type, Comparator<BytesRef> comparator) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_START, true, context, type);
-      size = datIn.readInt();
-      valueCount = idxIn.readInt();
-      this.comparator = comparator;
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return new FixedSortedSource(cloneData(), cloneIndex(), size, valueCount,
-          comparator);
-    }
-
-    @Override
-    public Source getDirectSource() throws IOException {
-      return new DirectFixedSortedSource(cloneData(), cloneIndex(), size,
-          valueCount, comparator, type);
-    }
-
-    @Override
-    public int getValueSize() {
-      return size;
-    }
-  }
-
-  static final class FixedSortedSource extends BytesSortedSourceBase {
-    private final int valueCount;
-    private final int size;
-
-    FixedSortedSource(IndexInput datIn, IndexInput idxIn, int size,
-        int numValues, Comparator<BytesRef> comp) throws IOException {
-      super(datIn, idxIn, comp, size * numValues, Type.BYTES_FIXED_SORTED,
-          false);
-      this.size = size;
-      this.valueCount = numValues;
-      closeIndexInput();
-    }
-
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      return data.fillSlice(bytesRef, (ord * size), size);
-    }
-  }
-
-  static final class DirectFixedSortedSource extends SortedSource {
-    final PackedInts.Reader docToOrdIndex;
-    private final IndexInput datIn;
-    private final long basePointer;
-    private final int size;
-    private final int valueCount;
-
-    DirectFixedSortedSource(IndexInput datIn, IndexInput idxIn, int size,
-        int valueCount, Comparator<BytesRef> comp, Type type)
-        throws IOException {
-      super(type, comp);
-      docToOrdIndex = PackedInts.getDirectReader(idxIn);
-      basePointer = datIn.getFilePointer();
-      this.datIn = datIn;
-      this.size = size;
-      this.valueCount = valueCount;
-    }
-
-    @Override
-    public int ord(int docID) {
-      return (int) docToOrdIndex.get(docID);
-    }
-
-    @Override
-    public boolean hasPackedDocToOrd() {
-      return true;
-    }
-
-    @Override
-    public PackedInts.Reader getDocToOrd() {
-      return docToOrdIndex;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      try {
-        datIn.seek(basePointer + size * ord);
-        bytesRef.grow(size);
-        datIn.readBytes(bytesRef.bytes, 0, size);
-        bytesRef.length = size;
-        bytesRef.offset = 0;
-        return bytesRef;
-      } catch (IOException ex) {
-        throw new IllegalStateException("failed to getByOrd", ex);
-      }
-    }
-
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-  }
-
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedStraightBytesImpl.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedStraightBytesImpl.java
deleted file mode 100644
index 2af9493..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/FixedStraightBytesImpl.java
+++ /dev/null
@@ -1,355 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSourceBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesWriterBase;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-
-// Simplest storage: stores fixed length byte[] per
-// document, with no dedup and no sorting.
-/**
- * @lucene.experimental
- */
-class FixedStraightBytesImpl {
-
-  static final String CODEC_NAME = "FixedStraightBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-  
-  static abstract class FixedBytesWriterBase extends BytesWriterBase {
-    protected int lastDocID = -1;
-    // start at -1 if the first added value is > 0
-    protected int size = -1;
-    private final int byteBlockSize = BYTE_BLOCK_SIZE;
-    private final ByteBlockPool pool;
-
-    protected FixedBytesWriterBase(Directory dir, String id, String codecName,
-        int version, Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, codecName, version, bytesUsed, context);
-      pool = new ByteBlockPool(new DirectTrackingAllocator(bytesUsed));
-      pool.nextBuffer();
-    }
-    
-    @Override
-    public void add(int docID, BytesRef bytes) throws IOException {
-      assert lastDocID < docID;
-
-      if (size == -1) {
-        if (bytes.length > BYTE_BLOCK_SIZE) {
-          throw new IllegalArgumentException("bytes arrays > " + Short.MAX_VALUE + " are not supported");
-        }
-        size = bytes.length;
-      } else if (bytes.length != size) {
-        throw new IllegalArgumentException("expected bytes size=" + size
-            + " but got " + bytes.length);
-      }
-      if (lastDocID+1 < docID) {
-        advancePool(docID);
-      }
-      pool.copy(bytes);
-      lastDocID = docID;
-    }
-    
-    private final void advancePool(int docID) {
-      long numBytes = (docID - (lastDocID+1))*size;
-      while(numBytes > 0) {
-        if (numBytes + pool.byteUpto < byteBlockSize) {
-          pool.byteUpto += numBytes;
-          numBytes = 0;
-        } else {
-          numBytes -= byteBlockSize - pool.byteUpto;
-          pool.nextBuffer();
-        }
-      }
-      assert numBytes == 0;
-    }
-    
-    protected void set(BytesRef ref, int docId) {
-      assert BYTE_BLOCK_SIZE % size == 0 : "BYTE_BLOCK_SIZE ("+ BYTE_BLOCK_SIZE + ") must be a multiple of the size: " + size;
-      ref.offset = docId*size;
-      ref.length = size;
-      pool.deref(ref);
-    }
-    
-    protected void resetPool() {
-      pool.dropBuffersAndReset();
-    }
-    
-    protected void writeData(IndexOutput out) throws IOException {
-      pool.writePool(out);
-    }
-    
-    protected void writeZeros(int num, IndexOutput out) throws IOException {
-      final byte[] zeros = new byte[size];
-      for (int i = 0; i < num; i++) {
-        out.writeBytes(zeros, zeros.length);
-      }
-    }
-  }
-
-  static class Writer extends FixedBytesWriterBase {
-    private boolean hasMerged;
-    private IndexOutput datOut;
-    
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-    }
-
-    public Writer(Directory dir, String id, String codecName, int version, Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, codecName, version, bytesUsed, context);
-    }
-
-
-    @Override
-    protected void merge(SingleSubMergeState state) throws IOException {
-      datOut = getOrCreateDataOut();
-      boolean success = false;
-      try {
-        if (!hasMerged && size != -1) {
-          datOut.writeInt(size);
-        }
-
-        if (state.liveDocs == null && tryBulkMerge(state.reader)) {
-          FixedStraightReader reader = (FixedStraightReader) state.reader;
-          final int maxDocs = reader.maxDoc;
-          if (maxDocs == 0) {
-            return;
-          }
-          if (size == -1) {
-            size = reader.size;
-            datOut.writeInt(size);
-          } else if (size != reader.size) {
-            throw new IllegalArgumentException("expected bytes size=" + size
-                + " but got " + reader.size);
-           }
-          if (lastDocID+1 < state.docBase) {
-            fill(datOut, state.docBase);
-            lastDocID = state.docBase-1;
-          }
-          // TODO should we add a transfer to API to each reader?
-          final IndexInput cloneData = reader.cloneData();
-          try {
-            datOut.copyBytes(cloneData, size * maxDocs);
-          } finally {
-            IOUtils.close(cloneData);  
-          }
-        
-          lastDocID += maxDocs;
-        } else {
-          super.merge(state);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-        hasMerged = true;
-      }
-    }
-    
-    protected boolean tryBulkMerge(DocValues docValues) {
-      return docValues instanceof FixedStraightReader;
-    }
-    
-    @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
-      assert lastDocID < docID;
-      setMergeBytes(sourceDoc);
-      if (size == -1) {
-        size = bytesRef.length;
-        datOut.writeInt(size);
-      }
-      assert size == bytesRef.length : "size: " + size + " ref: " + bytesRef.length;
-      if (lastDocID+1 < docID) {
-        fill(datOut, docID);
-      }
-      datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-      lastDocID = docID;
-    }
-    
-    protected void setMergeBytes(int sourceDoc) {
-      currentMergeSource.getBytes(sourceDoc, bytesRef);
-    }
-
-
-
-    // Fills up to but not including this docID
-    private void fill(IndexOutput datOut, int docID) throws IOException {
-      assert size >= 0;
-      writeZeros((docID - (lastDocID+1)), datOut);
-    }
-
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      try {
-        if (!hasMerged) {
-          // indexing path - no disk IO until here
-          assert datOut == null;
-          datOut = getOrCreateDataOut();
-          if (size == -1) {
-            datOut.writeInt(0);
-          } else {
-            datOut.writeInt(size);
-            writeData(datOut);
-          }
-          if (lastDocID + 1 < docCount) {
-            fill(datOut, docCount);
-          }
-        } else {
-          // merge path - datOut should be initialized
-          assert datOut != null;
-          if (size == -1) {// no data added
-            datOut.writeInt(0);
-          } else {
-            fill(datOut, docCount);
-          }
-        }
-        success = true;
-      } finally {
-        resetPool();
-        if (success) {
-          IOUtils.close(datOut);
-        } else {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-      }
-    }
-  
-  }
-  
-  public static class FixedStraightReader extends BytesReaderBase {
-    protected final int size;
-    protected final int maxDoc;
-    
-    FixedStraightReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      this(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc, context, Type.BYTES_FIXED_STRAIGHT);
-    }
-
-    protected FixedStraightReader(Directory dir, String id, String codec, int version, int maxDoc, IOContext context, Type type) throws IOException {
-      super(dir, id, codec, version, false, context, type);
-      size = datIn.readInt();
-      this.maxDoc = maxDoc;
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return size == 1 ? new SingleByteSource(cloneData(), maxDoc) : 
-        new FixedStraightSource(cloneData(), size, maxDoc, type);
-    }
-
-    @Override
-    public void close() throws IOException {
-      datIn.close();
-    }
-   
-    @Override
-    public Source getDirectSource() throws IOException {
-      return new DirectFixedStraightSource(cloneData(), size, type());
-    }
-    
-    @Override
-    public int getValueSize() {
-      return size;
-    }
-  }
-  
-  // specialized version for single bytes
-  private static final class SingleByteSource extends Source {
-    private final byte[] data;
-
-    public SingleByteSource(IndexInput datIn, int maxDoc) throws IOException {
-      super(Type.BYTES_FIXED_STRAIGHT);
-      try {
-        data = new byte[maxDoc];
-        datIn.readBytes(data, 0, data.length, false);
-      } finally {
-        IOUtils.close(datIn);
-      }
-    }
-    
-    @Override
-    public boolean hasArray() {
-      return true;
-    }
-
-    @Override
-    public Object getArray() {
-      return data;
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      bytesRef.length = 1;
-      bytesRef.bytes = data;
-      bytesRef.offset = docID;
-      return bytesRef;
-    }
-  }
-
-  
-  private final static class FixedStraightSource extends BytesSourceBase {
-    private final int size;
-
-    public FixedStraightSource(IndexInput datIn, int size, int maxDoc, Type type)
-        throws IOException {
-      super(datIn, null, new PagedBytes(PAGED_BYTES_BITS), size * maxDoc,
-          type);
-      this.size = size;
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      return data.fillSlice(bytesRef, docID * size, size);
-    }
-  }
-  
-  public final static class DirectFixedStraightSource extends DirectSource {
-    private final int size;
-
-    DirectFixedStraightSource(IndexInput input, int size, Type type) {
-      super(input, type);
-      this.size = size;
-    }
-
-    @Override
-    protected int position(int docID) throws IOException {
-      data.seek(baseOffset + size * docID);
-      return size;
-    }
-
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Floats.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Floats.java
deleted file mode 100644
index 0a55227..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Floats.java
+++ /dev/null
@@ -1,126 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValue;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Exposes {@link Writer} and reader ({@link Source}) for 32 bit and 64 bit
- * floating point values.
- * <p>
- * Current implementations store either 4 byte or 8 byte floating points with
- * full precision without any compression.
- * 
- * @lucene.experimental
- */
-public class Floats {
-  
-  protected static final String CODEC_NAME = "Floats";
-  protected static final int VERSION_START = 0;
-  protected static final int VERSION_CURRENT = VERSION_START;
-  
-  public static Writer getWriter(Directory dir, String id, Counter bytesUsed,
-      IOContext context, Type type) throws IOException {
-    return new FloatsWriter(dir, id, bytesUsed, context, type);
-  }
-
-  public static DocValues getValues(Directory dir, String id, int maxDoc, IOContext context, Type type)
-      throws IOException {
-    return new FloatsReader(dir, id, maxDoc, context, type);
-  }
-  
-  private static int typeToSize(Type type) {
-    switch (type) {
-    case FLOAT_32:
-      return 4;
-    case FLOAT_64:
-      return 8;
-    default:
-      throw new IllegalStateException("illegal type " + type);
-    }
-  }
-  
-  final static class FloatsWriter extends FixedStraightBytesImpl.Writer {
-   
-    private final int size; 
-    private final DocValuesArray template;
-    public FloatsWriter(Directory dir, String id, Counter bytesUsed,
-        IOContext context, Type type) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      size = typeToSize(type);
-      this.bytesRef = new BytesRef(size);
-      bytesRef.length = size;
-      template = DocValuesArray.TEMPLATES.get(type);
-      assert template != null;
-    }
-    
-    public void add(int docID, double v) throws IOException {
-      template.toBytes(v, bytesRef);
-      add(docID, bytesRef);
-    }
-    
-    @Override
-    public void add(int docID, DocValue docValue) throws IOException {
-      add(docID, docValue.getFloat());
-    }
-    
-    @Override
-    protected boolean tryBulkMerge(DocValues docValues) {
-      // only bulk merge if value type is the same otherwise size differs
-      return super.tryBulkMerge(docValues) && docValues.type() == template.type();
-    }
-    
-    @Override
-    protected void setMergeBytes(int sourceDoc) {
-      final double value = currentMergeSource.getFloat(sourceDoc);
-      template.toBytes(value, bytesRef);
-    }
-  }
-  
-  final static class FloatsReader extends FixedStraightBytesImpl.FixedStraightReader {
-    final DocValuesArray arrayTemplate;
-    FloatsReader(Directory dir, String id, int maxDoc, IOContext context, Type type)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc, context, type);
-      arrayTemplate = DocValuesArray.TEMPLATES.get(type);
-      assert size == 4 || size == 8;
-    }
-    
-    @Override
-    public Source load() throws IOException {
-      final IndexInput indexInput = cloneData();
-      try {
-        return arrayTemplate.newFromInput(indexInput, maxDoc);
-      } finally {
-        IOUtils.close(indexInput);
-      }
-    }
-    
-  }
-
-}
\ No newline at end of file
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Ints.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Ints.java
deleted file mode 100644
index bc5de34..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Ints.java
+++ /dev/null
@@ -1,151 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.DocValue;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Stores ints packed and fixed with fixed-bit precision.
- * 
- * @lucene.experimental
- */
-public final class Ints {
-  protected static final String CODEC_NAME = "Ints";
-  protected static final int VERSION_START = 0;
-  protected static final int VERSION_CURRENT = VERSION_START;
-
-  private Ints() {
-  }
-  
-  public static Writer getWriter(Directory dir, String id, Counter bytesUsed,
-      Type type, IOContext context) throws IOException {
-    return type == Type.VAR_INTS ? new PackedIntValues.PackedIntsWriter(dir, id,
-        bytesUsed, context) : new IntsWriter(dir, id, bytesUsed, context, type);
-  }
-
-  public static DocValues getValues(Directory dir, String id, int numDocs,
-      Type type, IOContext context) throws IOException {
-    return type == Type.VAR_INTS ? new PackedIntValues.PackedIntsReader(dir, id,
-        numDocs, context) : new IntsReader(dir, id, numDocs, context, type);
-  }
-  
-  private static Type sizeToType(int size) {
-    switch (size) {
-    case 1:
-      return Type.FIXED_INTS_8;
-    case 2:
-      return Type.FIXED_INTS_16;
-    case 4:
-      return Type.FIXED_INTS_32;
-    case 8:
-      return Type.FIXED_INTS_64;
-    default:
-      throw new IllegalStateException("illegal size " + size);
-    }
-  }
-  
-  private static int typeToSize(Type type) {
-    switch (type) {
-    case FIXED_INTS_16:
-      return 2;
-    case FIXED_INTS_32:
-      return 4;
-    case FIXED_INTS_64:
-      return 8;
-    case FIXED_INTS_8:
-      return 1;
-    default:
-      throw new IllegalStateException("illegal type " + type);
-    }
-  }
-
-
-  static class IntsWriter extends FixedStraightBytesImpl.Writer {
-    private final DocValuesArray template;
-
-    public IntsWriter(Directory dir, String id, Counter bytesUsed,
-        IOContext context, Type valueType) throws IOException {
-      this(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context, valueType);
-    }
-
-    protected IntsWriter(Directory dir, String id, String codecName,
-        int version, Counter bytesUsed, IOContext context, Type valueType) throws IOException {
-      super(dir, id, codecName, version, bytesUsed, context);
-      size = typeToSize(valueType);
-      this.bytesRef = new BytesRef(size);
-      bytesRef.length = size;
-      template = DocValuesArray.TEMPLATES.get(valueType);
-    }
-    
-    @Override
-    public void add(int docID, long v) throws IOException {
-      template.toBytes(v, bytesRef);
-      add(docID, bytesRef);
-    }
-
-    @Override
-    public void add(int docID, DocValue docValue) throws IOException {
-      add(docID, docValue.getInt());
-    }
-    
-    @Override
-    protected void setMergeBytes(int sourceDoc) {
-      final long value = currentMergeSource.getInt(sourceDoc);
-      template.toBytes(value, bytesRef);
-    }
-    
-    @Override
-    protected boolean tryBulkMerge(DocValues docValues) {
-      // only bulk merge if value type is the same otherwise size differs
-      return super.tryBulkMerge(docValues) && docValues.type() == template.type();
-    }
-  }
-  
-  final static class IntsReader extends FixedStraightBytesImpl.FixedStraightReader {
-    private final DocValuesArray arrayTemplate;
-
-    IntsReader(Directory dir, String id, int maxDoc, IOContext context, Type type)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, maxDoc,
-          context, type);
-      arrayTemplate = DocValuesArray.TEMPLATES.get(type);
-      assert arrayTemplate != null;
-      assert type == sizeToType(size);
-    }
-
-    @Override
-    public Source load() throws IOException {
-      final IndexInput indexInput = cloneData();
-      try {
-        return arrayTemplate.newFromInput(indexInput, maxDoc);
-      } finally {
-        IOUtils.close(indexInput);
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/PackedIntValues.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/PackedIntValues.java
deleted file mode 100644
index 2670510..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/PackedIntValues.java
+++ /dev/null
@@ -1,265 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.DocValue;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.codecs.lucene40.values.FixedStraightBytesImpl.FixedBytesWriterBase;
-import org.apache.lucene.index.codecs.lucene40.values.DocValuesArray.LongValues;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Stores integers using {@link PackedInts}
- * 
- * @lucene.experimental
- * */
-class PackedIntValues {
-
-  private static final String CODEC_NAME = "PackedInts";
-  private static final byte PACKED = 0x00;
-  private static final byte FIXED_64 = 0x01;
-
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  static class PackedIntsWriter extends FixedBytesWriterBase {
-
-    private long minValue;
-    private long maxValue;
-    private boolean started;
-    private int lastDocId = -1;
-
-    protected PackedIntsWriter(Directory dir, String id, Counter bytesUsed,
-        IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      bytesRef = new BytesRef(8);
-    }
-
-    @Override
-    public void add(int docID, long v) throws IOException {
-      assert lastDocId < docID;
-      if (!started) {
-        started = true;
-        minValue = maxValue = v;
-      } else {
-        if (v < minValue) {
-          minValue = v;
-        } else if (v > maxValue) {
-          maxValue = v;
-        }
-      }
-      lastDocId = docID;
-      BytesRefUtils.copyLong(bytesRef, v);
-      add(docID, bytesRef);
-    }
-
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      final IndexOutput dataOut = getOrCreateDataOut();
-      try {
-        if (!started) {
-          minValue = maxValue = 0;
-        }
-        final long delta = maxValue - minValue;
-        // if we exceed the range of positive longs we must switch to fixed
-        // ints
-        if (delta <= (maxValue >= 0 && minValue <= 0 ? Long.MAX_VALUE
-            : Long.MAX_VALUE - 1) && delta >= 0) {
-          dataOut.writeByte(PACKED);
-          writePackedInts(dataOut, docCount);
-          return; // done
-        } else {
-          dataOut.writeByte(FIXED_64);
-        }
-        writeData(dataOut);
-        writeZeros(docCount - (lastDocID + 1), dataOut);
-        success = true;
-      } finally {
-        resetPool();
-        if (success) {
-          IOUtils.close(dataOut);
-        } else {
-          IOUtils.closeWhileHandlingException(dataOut);
-        }
-      }
-    }
-
-    @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
-      assert docID > lastDocId : "docID: " + docID
-          + " must be greater than the last added doc id: " + lastDocId;
-        add(docID, currentMergeSource.getInt(sourceDoc));
-    }
-
-    private void writePackedInts(IndexOutput datOut, int docCount) throws IOException {
-      datOut.writeLong(minValue);
-      
-      // write a default value to recognize docs without a value for that
-      // field
-      final long defaultValue = maxValue >= 0 && minValue <= 0 ? 0 - minValue
-          : ++maxValue - minValue;
-      datOut.writeLong(defaultValue);
-      PackedInts.Writer w = PackedInts.getWriter(datOut, docCount,
-          PackedInts.bitsRequired(maxValue - minValue));
-      for (int i = 0; i < lastDocID + 1; i++) {
-        set(bytesRef, i);
-        byte[] bytes = bytesRef.bytes;
-        int offset = bytesRef.offset;
-        long asLong =  
-           (((long)(bytes[offset+0] & 0xff) << 56) |
-            ((long)(bytes[offset+1] & 0xff) << 48) |
-            ((long)(bytes[offset+2] & 0xff) << 40) |
-            ((long)(bytes[offset+3] & 0xff) << 32) |
-            ((long)(bytes[offset+4] & 0xff) << 24) |
-            ((long)(bytes[offset+5] & 0xff) << 16) |
-            ((long)(bytes[offset+6] & 0xff) <<  8) |
-            ((long)(bytes[offset+7] & 0xff)));
-        w.add(asLong == 0 ? defaultValue : asLong - minValue);
-      }
-      for (int i = lastDocID + 1; i < docCount; i++) {
-        w.add(defaultValue);
-      }
-      w.finish();
-    }
-
-    @Override
-    public void add(int docID, DocValue docValue) throws IOException {
-      add(docID, docValue.getInt());
-    }
-  }
-
-  /**
-   * Opens all necessary files, but does not read any data in until you call
-   * {@link #load}.
-   */
-  static class PackedIntsReader extends DocValues {
-    private final IndexInput datIn;
-    private final byte type;
-    private final int numDocs;
-    private final LongValues values;
-
-    protected PackedIntsReader(Directory dir, String id, int numDocs,
-        IOContext context) throws IOException {
-      datIn = dir.openInput(
-                IndexFileNames.segmentFileName(id, Bytes.DV_SEGMENT_SUFFIX, Writer.DATA_EXTENSION),
-          context);
-      this.numDocs = numDocs;
-      boolean success = false;
-      try {
-        CodecUtil.checkHeader(datIn, CODEC_NAME, VERSION_START, VERSION_START);
-        type = datIn.readByte();
-        values = type == FIXED_64 ? new LongValues() : null;
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(datIn);
-        }
-      }
-    }
-
-
-    /**
-     * Loads the actual values. You may call this more than once, eg if you
-     * already previously loaded but then discarded the Source.
-     */
-    @Override
-    public Source load() throws IOException {
-      boolean success = false;
-      final Source source;
-      IndexInput input = null;
-      try {
-        input = (IndexInput) datIn.clone();
-        
-        if (values == null) {
-          source = new PackedIntsSource(input, false);
-        } else {
-          source = values.newFromInput(input, numDocs);
-        }
-        success = true;
-        return source;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(input, datIn);
-        }
-      }
-    }
-
-    @Override
-    public void close() throws IOException {
-      super.close();
-      datIn.close();
-    }
-
-
-    @Override
-    public Type type() {
-      return Type.VAR_INTS;
-    }
-
-
-    @Override
-    public Source getDirectSource() throws IOException {
-      return values != null ? new FixedStraightBytesImpl.DirectFixedStraightSource((IndexInput) datIn.clone(), 8, Type.FIXED_INTS_64) : new PackedIntsSource((IndexInput) datIn.clone(), true);
-    }
-  }
-
-  
-  static class PackedIntsSource extends Source {
-    private final long minValue;
-    private final long defaultValue;
-    private final PackedInts.Reader values;
-
-    public PackedIntsSource(IndexInput dataIn, boolean direct) throws IOException {
-      super(Type.VAR_INTS);
-      minValue = dataIn.readLong();
-      defaultValue = dataIn.readLong();
-      values = direct ? PackedInts.getDirectReader(dataIn) : PackedInts.getReader(dataIn);
-    }
-    
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      ref.grow(8);
-      BytesRefUtils.copyLong(ref, getInt(docID));
-      return ref;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      // TODO -- can we somehow avoid 2X method calls
-      // on each get? must push minValue down, and make
-      // PackedInts implement Ints.Source
-      assert docID >= 0;
-      final long value = values.get(docID);
-      return value == defaultValue ? 0 : minValue + value;
-    }
-  }
-
-}
\ No newline at end of file
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarDerefBytesImpl.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarDerefBytesImpl.java
deleted file mode 100644
index 31db5fc..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarDerefBytesImpl.java
+++ /dev/null
@@ -1,151 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSourceBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores variable-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[] and both
-// docs reference that single source
-
-/**
- * @lucene.experimental
- */
-class VarDerefBytesImpl {
-
-  static final String CODEC_NAME = "VarDerefBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  /*
-   * TODO: if impls like this are merged we are bound to the amount of memory we
-   * can store into a BytesRefHash and therefore how much memory a ByteBlockPool
-   * can address. This is currently limited to 2GB. While we could extend that
-   * and use 64bit for addressing this still limits us to the existing main
-   * memory as all distinct bytes will be loaded up into main memory. We could
-   * move the byte[] writing to #finish(int) and store the bytes in sorted
-   * order and merge them in a streamed fashion. 
-   */
-  static class Writer extends DerefBytesWriterBase {
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      size = 0;
-    }
-    
-    @Override
-    protected void checkSize(BytesRef bytes) {
-      // allow var bytes sizes
-    }
-
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finishInternal(int docCount) throws IOException {
-      fillDefault(docCount);
-      final int size = hash.size();
-      final long[] addresses = new long[size];
-      final IndexOutput datOut = getOrCreateDataOut();
-      int addr = 0;
-      final BytesRef bytesRef = new BytesRef();
-      for (int i = 0; i < size; i++) {
-        hash.get(i, bytesRef);
-        addresses[i] = addr;
-        addr += writePrefixLength(datOut, bytesRef) + bytesRef.length;
-        datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-      }
-
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      // write the max address to read directly on source load
-      idxOut.writeLong(addr);
-      writeIndex(idxOut, docCount, addresses[addresses.length-1], addresses, docToEntry);
-    }
-  }
-
-  public static class VarDerefReader extends BytesReaderBase {
-    private final long totalBytes;
-    VarDerefReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_START, true, context, Type.BYTES_VAR_DEREF);
-      totalBytes = idxIn.readLong();
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return new VarDerefSource(cloneData(), cloneIndex(), totalBytes);
-    }
-   
-    @Override
-    public Source getDirectSource()
-        throws IOException {
-      return new DirectVarDerefSource(cloneData(), cloneIndex(), type());
-    }
-  }
-  
-  final static class VarDerefSource extends BytesSourceBase {
-    private final PackedInts.Reader addresses;
-
-    public VarDerefSource(IndexInput datIn, IndexInput idxIn, long totalBytes)
-        throws IOException {
-      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), totalBytes,
-          Type.BYTES_VAR_DEREF);
-      addresses = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      return data.fillSliceWithPrefix(bytesRef,
-          addresses.get(docID));
-    }
-  }
-
-  
-  final static class DirectVarDerefSource extends DirectSource {
-    private final PackedInts.Reader index;
-
-    DirectVarDerefSource(IndexInput data, IndexInput index, Type type)
-        throws IOException {
-      super(data, type);
-      this.index = PackedInts.getDirectReader(index);
-    }
-    
-    @Override
-    protected int position(int docID) throws IOException {
-      data.seek(baseOffset + index.get(docID));
-      final byte sizeByte = data.readByte();
-      if ((sizeByte & 128) == 0) {
-        // length is 1 byte
-        return sizeByte;
-      } else {
-        return ((sizeByte & 0x7f) << 8) | ((data.readByte() & 0xff));
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarSortedBytesImpl.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarSortedBytesImpl.java
deleted file mode 100644
index b50df84..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarSortedBytesImpl.java
+++ /dev/null
@@ -1,254 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.SortedBytesMergeUtils;
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.SortedBytesMergeUtils.MergeContext;
-import org.apache.lucene.index.SortedBytesMergeUtils.SortedSourceSlice;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSortedSourceBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.DerefBytesWriterBase;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-// Stores variable-length byte[] by deref, ie when two docs
-// have the same value, they store only 1 byte[] and both
-// docs reference that single source
-
-/**
- * @lucene.experimental
- */
-final class VarSortedBytesImpl {
-
-  static final String CODEC_NAME = "VarDerefBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  final static class Writer extends DerefBytesWriterBase {
-    private final Comparator<BytesRef> comp;
-
-    public Writer(Directory dir, String id, Comparator<BytesRef> comp,
-        Counter bytesUsed, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      this.comp = comp;
-      size = 0;
-    }
-    @Override
-    public void merge(MergeState mergeState, DocValues[] docValues)
-        throws IOException {
-      boolean success = false;
-      try {
-        MergeContext ctx = SortedBytesMergeUtils.init(Type.BYTES_VAR_SORTED, docValues, comp, mergeState);
-        final List<SortedSourceSlice> slices = SortedBytesMergeUtils.buildSlices(mergeState, docValues, ctx);
-        IndexOutput datOut = getOrCreateDataOut();
-        
-        ctx.offsets = new long[1];
-        final int maxOrd = SortedBytesMergeUtils.mergeRecords(ctx, datOut, slices);
-        final long[] offsets = ctx.offsets;
-        maxBytes = offsets[maxOrd-1];
-        final IndexOutput idxOut = getOrCreateIndexOut();
-        
-        idxOut.writeLong(maxBytes);
-        final PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, maxOrd+1,
-            PackedInts.bitsRequired(maxBytes));
-        offsetWriter.add(0);
-        for (int i = 0; i < maxOrd; i++) {
-          offsetWriter.add(offsets[i]);
-        }
-        offsetWriter.finish();
-        
-        final PackedInts.Writer ordsWriter = PackedInts.getWriter(idxOut, ctx.docToEntry.length,
-            PackedInts.bitsRequired(maxOrd-1));
-        for (SortedSourceSlice slice : slices) {
-          slice.writeOrds(ordsWriter);
-        }
-        ordsWriter.finish();
-        success = true;
-      } finally {
-        releaseResources();
-        if (success) {
-          IOUtils.close(getIndexOut(), getDataOut());
-        } else {
-          IOUtils.closeWhileHandlingException(getIndexOut(), getDataOut());
-        }
-
-      }
-    }
-
-    @Override
-    protected void checkSize(BytesRef bytes) {
-      // allow var bytes sizes
-    }
-
-    // Important that we get docCount, in case there were
-    // some last docs that we didn't see
-    @Override
-    public void finishInternal(int docCount) throws IOException {
-      fillDefault(docCount);
-      final int count = hash.size();
-      final IndexOutput datOut = getOrCreateDataOut();
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      long offset = 0;
-      final int[] index = new int[count];
-      final int[] sortedEntries = hash.sort(comp);
-      // total bytes of data
-      idxOut.writeLong(maxBytes);
-      PackedInts.Writer offsetWriter = PackedInts.getWriter(idxOut, count+1,
-          PackedInts.bitsRequired(maxBytes));
-      // first dump bytes data, recording index & write offset as
-      // we go
-      final BytesRef spare = new BytesRef();
-      for (int i = 0; i < count; i++) {
-        final int e = sortedEntries[i];
-        offsetWriter.add(offset);
-        index[e] = i;
-        final BytesRef bytes = hash.get(e, spare);
-        // TODO: we could prefix code...
-        datOut.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-        offset += bytes.length;
-      }
-      // write sentinel
-      offsetWriter.add(offset);
-      offsetWriter.finish();
-      // write index
-      writeIndex(idxOut, docCount, count, index, docToEntry);
-
-    }
-  }
-
-  public static class Reader extends BytesReaderBase {
-
-    private final Comparator<BytesRef> comparator;
-
-    Reader(Directory dir, String id, int maxDoc,
-        IOContext context, Type type, Comparator<BytesRef> comparator)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_START, true, context, type);
-      this.comparator = comparator;
-    }
-
-    @Override
-    public org.apache.lucene.index.DocValues.Source load()
-        throws IOException {
-      return new VarSortedSource(cloneData(), cloneIndex(), comparator);
-    }
-
-    @Override
-    public Source getDirectSource() throws IOException {
-      return new DirectSortedSource(cloneData(), cloneIndex(), comparator, type());
-    }
-    
-  }
-  private static final class VarSortedSource extends BytesSortedSourceBase {
-    private final int valueCount;
-
-    VarSortedSource(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comp) throws IOException {
-      super(datIn, idxIn, comp, idxIn.readLong(), Type.BYTES_VAR_SORTED, true);
-      valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
-      closeIndexInput();
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      final long offset = ordToOffsetIndex.get(ord);
-      final long nextOffset = ordToOffsetIndex.get(1 + ord);
-      data.fillSlice(bytesRef, offset, (int) (nextOffset - offset));
-      return bytesRef;
-    }
-
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-  }
-
-  private static final class DirectSortedSource extends SortedSource {
-    private final PackedInts.Reader docToOrdIndex;
-    private final PackedInts.Reader ordToOffsetIndex;
-    private final IndexInput datIn;
-    private final long basePointer;
-    private final int valueCount;
-    
-    DirectSortedSource(IndexInput datIn, IndexInput idxIn,
-        Comparator<BytesRef> comparator, Type type) throws IOException {
-      super(type, comparator);
-      idxIn.readLong();
-      ordToOffsetIndex = PackedInts.getDirectReader(idxIn);
-      valueCount = ordToOffsetIndex.size()-1; // the last value here is just a dummy value to get the length of the last value
-      // advance this iterator to the end and clone the stream once it points to the docToOrdIndex header
-      ordToOffsetIndex.get(valueCount);
-      docToOrdIndex = PackedInts.getDirectReader((IndexInput) idxIn.clone()); // read the ords in to prevent too many random disk seeks
-      basePointer = datIn.getFilePointer();
-      this.datIn = datIn;
-    }
-
-    @Override
-    public int ord(int docID) {
-      return (int) docToOrdIndex.get(docID);
-    }
-
-    @Override
-    public boolean hasPackedDocToOrd() {
-      return true;
-    }
-
-    @Override
-    public PackedInts.Reader getDocToOrd() {
-      return docToOrdIndex;
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      try {
-        final long offset = ordToOffsetIndex.get(ord);
-        // 1+ord is safe because we write a sentinel at the end
-        final long nextOffset = ordToOffsetIndex.get(1+ord);
-        datIn.seek(basePointer + offset);
-        final int length = (int) (nextOffset - offset);
-        bytesRef.grow(length);
-        datIn.readBytes(bytesRef.bytes, 0, length);
-        bytesRef.length = length;
-        bytesRef.offset = 0;
-        return bytesRef;
-      } catch (IOException ex) {
-        throw new IllegalStateException("failed", ex);
-      }
-    }
-    
-    @Override
-    public int getValueCount() {
-      return valueCount;
-    }
-
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarStraightBytesImpl.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarStraightBytesImpl.java
deleted file mode 100644
index e50fba6..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/VarStraightBytesImpl.java
+++ /dev/null
@@ -1,285 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesReaderBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesSourceBase;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes.BytesWriterBase;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.ByteBlockPool;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.ByteBlockPool.DirectTrackingAllocator;
-import org.apache.lucene.util.packed.PackedInts;
-import org.apache.lucene.util.packed.PackedInts.ReaderIterator;
-
-// Variable length byte[] per document, no sharing
-
-/**
- * @lucene.experimental
- */
-class VarStraightBytesImpl {
-
-  static final String CODEC_NAME = "VarStraightBytes";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-
-  static class Writer extends BytesWriterBase {
-    private long address;
-    // start at -1 if the first added value is > 0
-    private int lastDocID = -1;
-    private long[] docToAddress;
-    private final ByteBlockPool pool;
-    private IndexOutput datOut;
-    private boolean merge = false;
-    public Writer(Directory dir, String id, Counter bytesUsed, IOContext context)
-        throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_CURRENT, bytesUsed, context);
-      pool = new ByteBlockPool(new DirectTrackingAllocator(bytesUsed));
-      docToAddress = new long[1];
-      pool.nextBuffer(); // init
-      bytesUsed.addAndGet(RamUsageEstimator.NUM_BYTES_INT);
-    }
-
-    // Fills up to but not including this docID
-    private void fill(final int docID, final long nextAddress) {
-      if (docID >= docToAddress.length) {
-        int oldSize = docToAddress.length;
-        docToAddress = ArrayUtil.grow(docToAddress, 1 + docID);
-        bytesUsed.addAndGet((docToAddress.length - oldSize)
-            * RamUsageEstimator.NUM_BYTES_INT);
-      }
-      for (int i = lastDocID + 1; i < docID; i++) {
-        docToAddress[i] = nextAddress;
-      }
-    }
-
-    @Override
-    public void add(int docID, BytesRef bytes) throws IOException {
-      assert !merge;
-      if (bytes.length == 0) {
-        return; // default
-      }
-      fill(docID, address);
-      docToAddress[docID] = address;
-      pool.copy(bytes);
-      address += bytes.length;
-      lastDocID = docID;
-    }
-    
-    @Override
-    protected void merge(SingleSubMergeState state) throws IOException {
-      merge = true;
-      datOut = getOrCreateDataOut();
-      boolean success = false;
-      try {
-        if (state.liveDocs == null && state.reader instanceof VarStraightReader) {
-          // bulk merge since we don't have any deletes
-          VarStraightReader reader = (VarStraightReader) state.reader;
-          final int maxDocs = reader.maxDoc;
-          if (maxDocs == 0) {
-            return;
-          }
-          if (lastDocID+1 < state.docBase) {
-            fill(state.docBase, address);
-            lastDocID = state.docBase-1;
-          }
-          final long numDataBytes;
-          final IndexInput cloneIdx = reader.cloneIndex();
-          try {
-            numDataBytes = cloneIdx.readVLong();
-            final ReaderIterator iter = PackedInts.getReaderIterator(cloneIdx);
-            for (int i = 0; i < maxDocs; i++) {
-              long offset = iter.next();
-              ++lastDocID;
-              if (lastDocID >= docToAddress.length) {
-                int oldSize = docToAddress.length;
-                docToAddress = ArrayUtil.grow(docToAddress, 1 + lastDocID);
-                bytesUsed.addAndGet((docToAddress.length - oldSize)
-                    * RamUsageEstimator.NUM_BYTES_INT);
-              }
-              docToAddress[lastDocID] = address + offset;
-            }
-            address += numDataBytes; // this is the address after all addr pointers are updated
-            iter.close();
-          } finally {
-            IOUtils.close(cloneIdx);
-          }
-          final IndexInput cloneData = reader.cloneData();
-          try {
-            datOut.copyBytes(cloneData, numDataBytes);
-          } finally {
-            IOUtils.close(cloneData);  
-          }
-        } else {
-          super.merge(state);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-      }
-    }
-    
-    @Override
-    protected void mergeDoc(int docID, int sourceDoc) throws IOException {
-      assert merge;
-      assert lastDocID < docID;
-      currentMergeSource.getBytes(sourceDoc, bytesRef);
-      if (bytesRef.length == 0) {
-        return; // default
-      }
-      fill(docID, address);
-      datOut.writeBytes(bytesRef.bytes, bytesRef.offset, bytesRef.length);
-      docToAddress[docID] = address;
-      address += bytesRef.length;
-      lastDocID = docID;
-    }
-    
-
-    @Override
-    public void finish(int docCount) throws IOException {
-      boolean success = false;
-      assert (!merge && datOut == null) || (merge && datOut != null); 
-      final IndexOutput datOut = getOrCreateDataOut();
-      try {
-        if (!merge) {
-          // header is already written in getDataOut()
-          pool.writePool(datOut);
-        }
-        success = true;
-      } finally {
-        if (success) {
-          IOUtils.close(datOut);
-        } else {
-          IOUtils.closeWhileHandlingException(datOut);
-        }
-        pool.dropBuffersAndReset();
-      }
-
-      success = false;
-      final IndexOutput idxOut = getOrCreateIndexOut();
-      try {
-        if (lastDocID == -1) {
-          idxOut.writeVLong(0);
-          final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount+1,
-              PackedInts.bitsRequired(0));
-          // docCount+1 so we write sentinel
-          for (int i = 0; i < docCount+1; i++) {
-            w.add(0);
-          }
-          w.finish();
-        } else {
-          fill(docCount, address);
-          idxOut.writeVLong(address);
-          final PackedInts.Writer w = PackedInts.getWriter(idxOut, docCount+1,
-              PackedInts.bitsRequired(address));
-          for (int i = 0; i < docCount; i++) {
-            w.add(docToAddress[i]);
-          }
-          // write sentinel
-          w.add(address);
-          w.finish();
-        }
-        success = true;
-      } finally {
-        bytesUsed.addAndGet(-(docToAddress.length)
-            * RamUsageEstimator.NUM_BYTES_INT);
-        docToAddress = null;
-        if (success) {
-          IOUtils.close(idxOut);
-        } else {
-          IOUtils.closeWhileHandlingException(idxOut);
-        }
-      }
-    }
-
-    public long ramBytesUsed() {
-      return bytesUsed.get();
-    }
-  }
-
-  public static class VarStraightReader extends BytesReaderBase {
-    private final int maxDoc;
-
-    VarStraightReader(Directory dir, String id, int maxDoc, IOContext context) throws IOException {
-      super(dir, id, CODEC_NAME, VERSION_START, true, context, Type.BYTES_VAR_STRAIGHT);
-      this.maxDoc = maxDoc;
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return new VarStraightSource(cloneData(), cloneIndex());
-    }
-
-    @Override
-    public Source getDirectSource()
-        throws IOException {
-      return new DirectVarStraightSource(cloneData(), cloneIndex(), type());
-    }
-  }
-  
-  private static final class VarStraightSource extends BytesSourceBase {
-    private final PackedInts.Reader addresses;
-
-    public VarStraightSource(IndexInput datIn, IndexInput idxIn) throws IOException {
-      super(datIn, idxIn, new PagedBytes(PAGED_BYTES_BITS), idxIn.readVLong(),
-          Type.BYTES_VAR_STRAIGHT);
-      addresses = PackedInts.getReader(idxIn);
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef bytesRef) {
-      final long address = addresses.get(docID);
-      return data.fillSlice(bytesRef, address,
-          (int) (addresses.get(docID + 1) - address));
-    }
-  }
-  
-  public final static class DirectVarStraightSource extends DirectSource {
-
-    private final PackedInts.Reader index;
-
-    DirectVarStraightSource(IndexInput data, IndexInput index, Type type)
-        throws IOException {
-      super(data, type);
-      index.readVLong();
-      this.index = PackedInts.getDirectReader(index);
-    }
-
-    @Override
-    protected int position(int docID) throws IOException {
-      final long offset = index.get(docID);
-      data.seek(baseOffset + offset);
-      // Safe to do 1+docID because we write sentinel at the end:
-      final long nextOffset = index.get(1+docID);
-      return (int) (nextOffset - offset);
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Writer.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Writer.java
deleted file mode 100644
index 330b744..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/values/Writer.java
+++ /dev/null
@@ -1,219 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.codecs.DocValuesConsumer;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-
-/**
- * Abstract API for per-document stored primitive values of type <tt>byte[]</tt>
- * , <tt>long</tt> or <tt>double</tt>. The API accepts a single value for each
- * document. The underlying storage mechanism, file formats, data-structures and
- * representations depend on the actual implementation.
- * <p>
- * Document IDs passed to this API must always be increasing unless stated
- * otherwise.
- * </p>
- * 
- * @lucene.experimental
- */
-public abstract class Writer extends DocValuesConsumer {
-  protected Source currentMergeSource;
-  protected final Counter bytesUsed;
-
-  /**
-   * Creates a new {@link Writer}.
-   * 
-   * @param bytesUsed
-   *          bytes-usage tracking reference used by implementation to track
-   *          internally allocated memory. All tracked bytes must be released
-   *          once {@link #finish(int)} has been called.
-   */
-  protected Writer(Counter bytesUsed) {
-    this.bytesUsed = bytesUsed;
-  }
-
-  /**
-   * Filename extension for index files
-   */
-  public static final String INDEX_EXTENSION = "idx";
-  
-  /**
-   * Filename extension for data files.
-   */
-  public static final String DATA_EXTENSION = "dat";
-
-  /**
-   * Records the specified <tt>long</tt> value for the docID or throws an
-   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
-   * <tt>long</tt> values.
-   * 
-   * @throws UnsupportedOperationException
-   *           if this writer doesn't record <tt>long</tt> values
-   */
-  public void add(int docID, long value) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  /**
-   * Records the specified <tt>double</tt> value for the docID or throws an
-   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
-   * <tt>double</tt> values.
-   * 
-   * @throws UnsupportedOperationException
-   *           if this writer doesn't record <tt>double</tt> values
-   */
-  public void add(int docID, double value) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  /**
-   * Records the specified {@link BytesRef} value for the docID or throws an
-   * {@link UnsupportedOperationException} if this {@link Writer} doesn't record
-   * {@link BytesRef} values.
-   * 
-   * @throws UnsupportedOperationException
-   *           if this writer doesn't record {@link BytesRef} values
-   */
-  public void add(int docID, BytesRef value) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-
-  /**
-   * Merges a document with the given <code>docID</code>. The methods
-   * implementation obtains the value for the <i>sourceDoc</i> id from the
-   * current {@link Source} set to <i>setNextMergeSource(Source)</i>.
-   * <p>
-   * This method is used during merging to provide implementation agnostic
-   * default merge implementation.
-   * </p>
-   * <p>
-   * All documents IDs between the given ID and the previously given ID or
-   * <tt>0</tt> if the method is call the first time are filled with default
-   * values depending on the {@link Writer} implementation. The given document
-   * ID must always be greater than the previous ID or <tt>0</tt> if called the
-   * first time.
-   */
-  protected abstract void mergeDoc(int docID, int sourceDoc) throws IOException;
-
-  /**
-   * Sets the next {@link Source} to consume values from on calls to
-   * {@link #mergeDoc(int, int)}
-   * 
-   * @param mergeSource
-   *          the next {@link Source}, this must not be null
-   */
-  protected void setNextMergeSource(Source mergeSource) {
-    currentMergeSource = mergeSource;
-  }
-
-  /**
-   * Finish writing and close any files and resources used by this Writer.
-   * 
-   * @param docCount
-   *          the total number of documents for this writer. This must be
-   *          greater that or equal to the largest document id passed to one of
-   *          the add methods after the {@link Writer} was created.
-   */
-  public abstract void finish(int docCount) throws IOException;
-
-  @Override
-  protected void merge(SingleSubMergeState state) throws IOException {
-    // This enables bulk copies in subclasses per MergeState, subclasses can
-    // simply override this and decide if they want to merge
-    // segments using this generic implementation or if a bulk merge is possible
-    // / feasible.
-    final Source source = state.reader.getDirectSource();
-    assert source != null;
-    setNextMergeSource(source); // set the current enum we are working on - the
-    // impl. will get the correct reference for the type
-    // it supports
-    int docID = state.docBase;
-    final Bits liveDocs = state.liveDocs;
-    final int docCount = state.docCount;
-    for (int i = 0; i < docCount; i++) {
-      if (liveDocs == null || liveDocs.get(i)) {
-        mergeDoc(docID++, i);
-      }
-    }
-    
-  }
-
-  /**
-   * Factory method to create a {@link Writer} instance for a given type. This
-   * method returns default implementations for each of the different types
-   * defined in the {@link Type} enumeration.
-   * 
-   * @param type
-   *          the {@link Type} to create the {@link Writer} for
-   * @param id
-   *          the file name id used to create files within the writer.
-   * @param directory
-   *          the {@link Directory} to create the files from.
-   * @param bytesUsed
-   *          a byte-usage tracking reference
-   * @return a new {@link Writer} instance for the given {@link Type}
-   * @throws IOException
-   */
-  public static Writer create(Type type, String id, Directory directory,
-      Comparator<BytesRef> comp, Counter bytesUsed, IOContext context) throws IOException {
-    if (comp == null) {
-      comp = BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-    switch (type) {
-    case FIXED_INTS_16:
-    case FIXED_INTS_32:
-    case FIXED_INTS_64:
-    case FIXED_INTS_8:
-    case VAR_INTS:
-      return Ints.getWriter(directory, id, bytesUsed, type, context);
-    case FLOAT_32:
-      return Floats.getWriter(directory, id, bytesUsed, context, type);
-    case FLOAT_64:
-      return Floats.getWriter(directory, id, bytesUsed, context, type);
-    case BYTES_FIXED_STRAIGHT:
-      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, true, comp,
-          bytesUsed, context);
-    case BYTES_FIXED_DEREF:
-      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, true, comp,
-          bytesUsed, context);
-    case BYTES_FIXED_SORTED:
-      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, true, comp,
-          bytesUsed, context);
-    case BYTES_VAR_STRAIGHT:
-      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, false, comp,
-          bytesUsed, context);
-    case BYTES_VAR_DEREF:
-      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, false, comp,
-          bytesUsed, context);
-    case BYTES_VAR_SORTED:
-      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, false, comp,
-          bytesUsed, context);
-    default:
-      throw new IllegalArgumentException("Unknown Values: " + type);
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/memory/MemoryPostingsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/memory/MemoryPostingsFormat.java
deleted file mode 100644
index 7f833ea..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/memory/MemoryPostingsFormat.java
+++ /dev/null
@@ -1,809 +0,0 @@
-package org.apache.lucene.index.codecs.memory;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.Set;
-import java.util.SortedMap;
-import java.util.TreeMap;
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.PostingsConsumer;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.index.codecs.TermsConsumer;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.ByteSequenceOutputs;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.FST;
-
-// TODO: would be nice to somehow allow this to act like
-// InstantiatedIndex, by never writing to disk; ie you write
-// to this Codec in RAM only and then when you open a reader
-// it pulls the FST directly from what you wrote w/o going
-// to disk.
-
-/** Stores terms & postings (docs, positions, payloads) in
- *  RAM, using an FST.
- *
- * <p>Note that this codec implements advance as a linear
- * scan!  This means if you store large fields in here,
- * queries that rely on advance will (AND BooleanQuery,
- * PhraseQuery) will be relatively slow!
- *
- * <p><b>NOTE</b>: this codec cannot address more than ~2.1 GB
- * of postings, because the underlying FST uses an int
- * to address the underlying byte[].
- *
- * @lucene.experimental */
-
-// TODO: Maybe name this 'Cached' or something to reflect
-// the reality that it is actually written to disk, but
-// loads itself in ram?
-public class MemoryPostingsFormat extends PostingsFormat {
-  
-  public MemoryPostingsFormat() {
-    super("Memory");
-  }
-
-  private static final boolean VERBOSE = false;
-
-  private final static class TermsWriter extends TermsConsumer {
-    private final IndexOutput out;
-    private final FieldInfo field;
-    private final Builder<BytesRef> builder;
-    private final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
-    private int termCount;
-
-    public TermsWriter(IndexOutput out, FieldInfo field) {
-      this.out = out;
-      this.field = field;
-      builder = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1, outputs);
-    }
-
-    private class PostingsWriter extends PostingsConsumer {
-      private int lastDocID;
-      private int lastPos;
-      private int lastPayloadLen;
-
-      // NOTE: not private so we don't pay access check at runtime:
-      int docCount;
-      RAMOutputStream buffer = new RAMOutputStream();
-
-      @Override
-      public void startDoc(int docID, int termDocFreq) throws IOException {
-        if (VERBOSE) System.out.println("    startDoc docID=" + docID + " freq=" + termDocFreq);
-        final int delta = docID - lastDocID;
-        assert docID == 0 || delta > 0;
-        lastDocID = docID;
-        docCount++;
-
-        if (field.indexOptions == IndexOptions.DOCS_ONLY) {
-          buffer.writeVInt(delta);
-        } else if (termDocFreq == 1) {
-          buffer.writeVInt((delta<<1) | 1);
-        } else {
-          buffer.writeVInt(delta<<1);
-          assert termDocFreq > 0;
-          buffer.writeVInt(termDocFreq);
-        }
-
-        lastPos = 0;
-      }
-
-      @Override
-      public void addPosition(int pos, BytesRef payload) throws IOException {
-        assert payload == null || field.storePayloads;
-
-        if (VERBOSE) System.out.println("      addPos pos=" + pos + " payload=" + payload);
-
-        final int delta = pos - lastPos;
-        assert delta >= 0;
-        lastPos = pos;
-        
-        if (field.storePayloads) {
-          final int payloadLen = payload == null ? 0 : payload.length;
-          if (payloadLen != lastPayloadLen) {
-            lastPayloadLen = payloadLen;
-            buffer.writeVInt((delta<<1)|1);
-            buffer.writeVInt(payloadLen);
-          } else {
-            buffer.writeVInt(delta<<1);
-          }
-
-          if (payloadLen > 0) {
-            buffer.writeBytes(payload.bytes, payload.offset, payloadLen);
-          }
-        } else {
-          buffer.writeVInt(delta);
-        }
-      }
-
-      @Override
-      public void finishDoc() {
-      }
-
-      public PostingsWriter reset() {
-        assert buffer.getFilePointer() == 0;
-        lastDocID = 0;
-        docCount = 0;
-        lastPayloadLen = 0;
-        return this;
-      }
-    }
-
-    private final PostingsWriter postingsWriter = new PostingsWriter();
-
-    @Override
-    public PostingsConsumer startTerm(BytesRef text) {
-      if (VERBOSE) System.out.println("  startTerm term=" + text.utf8ToString());
-      return postingsWriter.reset();
-    }
-
-    private final RAMOutputStream buffer2 = new RAMOutputStream();
-    private final BytesRef spare = new BytesRef();
-    private byte[] finalBuffer = new byte[128];
-
-    @Override
-    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
-
-      assert postingsWriter.docCount == stats.docFreq;
-
-      assert buffer2.getFilePointer() == 0;
-
-      buffer2.writeVInt(stats.docFreq);
-      if (field.indexOptions != IndexOptions.DOCS_ONLY) {
-        buffer2.writeVLong(stats.totalTermFreq-stats.docFreq);
-      }
-      int pos = (int) buffer2.getFilePointer();
-      buffer2.writeTo(finalBuffer, 0);
-      buffer2.reset();
-
-      final int totalBytes = pos + (int) postingsWriter.buffer.getFilePointer();
-      if (totalBytes > finalBuffer.length) {
-        finalBuffer = ArrayUtil.grow(finalBuffer, totalBytes);
-      }
-      postingsWriter.buffer.writeTo(finalBuffer, pos);
-      postingsWriter.buffer.reset();
-
-      spare.bytes = finalBuffer;
-      spare.length = totalBytes;
-      if (VERBOSE) {
-        System.out.println("    finishTerm term=" + text.utf8ToString() + " " + totalBytes + " bytes totalTF=" + stats.totalTermFreq);
-        for(int i=0;i<totalBytes;i++) {
-          System.out.println("      " + Integer.toHexString(finalBuffer[i]&0xFF));
-        }
-      }
-      builder.add(text, BytesRef.deepCopyOf(spare));
-      termCount++;
-    }
-
-    @Override
-    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
-      if (termCount > 0) {
-        out.writeVInt(termCount);
-        out.writeVInt(field.number);
-        if (field.indexOptions != IndexOptions.DOCS_ONLY) {
-          out.writeVLong(sumTotalTermFreq);
-        }
-        out.writeVLong(sumDocFreq);
-        out.writeVInt(docCount);
-        builder.finish().save(out);
-        if (VERBOSE) System.out.println("finish field=" + field.name + " fp=" + out.getFilePointer());
-      }
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-  }
-
-  private static String EXTENSION = "ram";
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-
-    final String fileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, EXTENSION);
-    final IndexOutput out = state.directory.createOutput(fileName, state.context);
-    
-    return new FieldsConsumer() {
-      @Override
-      public TermsConsumer addField(FieldInfo field) {
-        if (VERBOSE) System.out.println("\naddField field=" + field.name);
-        return new TermsWriter(out, field);
-      }
-
-      @Override
-      public void close() throws IOException {
-        // EOF marker:
-        try {
-          out.writeVInt(0);
-        } finally {
-          out.close();
-        }
-      }
-    };
-  }
-
-  private final static class FSTDocsEnum extends DocsEnum {
-    private final IndexOptions indexOptions;
-    private final boolean storePayloads;
-    private byte[] buffer = new byte[16];
-    private final ByteArrayDataInput in = new ByteArrayDataInput(buffer);
-
-    private Bits liveDocs;
-    private int docUpto;
-    private int docID = -1;
-    private int accum;
-    private int freq;
-    private int payloadLen;
-    private int numDocs;
-
-    public FSTDocsEnum(IndexOptions indexOptions, boolean storePayloads) {
-      this.indexOptions = indexOptions;
-      this.storePayloads = storePayloads;
-    }
-
-    public boolean canReuse(IndexOptions indexOptions, boolean storePayloads) {
-      return indexOptions == this.indexOptions && storePayloads == this.storePayloads;
-    }
-    
-    public FSTDocsEnum reset(BytesRef bufferIn, Bits liveDocs, int numDocs) {
-      assert numDocs > 0;
-      if (buffer.length < bufferIn.length - bufferIn.offset) {
-        buffer = ArrayUtil.grow(buffer, bufferIn.length - bufferIn.offset);
-      }
-      in.reset(buffer, 0, bufferIn.length - bufferIn.offset);
-      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length - bufferIn.offset);
-      this.liveDocs = liveDocs;
-      docID = -1;
-      accum = 0;
-      docUpto = 0;
-      payloadLen = 0;
-      this.numDocs = numDocs;
-      return this;
-    }
-
-    @Override
-    public int nextDoc() {
-      while(true) {
-        if (VERBOSE) System.out.println("  nextDoc cycle docUpto=" + docUpto + " numDocs=" + numDocs + " fp=" + in.getPosition() + " this=" + this);
-        if (docUpto == numDocs) {
-          if (VERBOSE) {
-            System.out.println("    END");
-          }
-          return docID = NO_MORE_DOCS;
-        }
-        docUpto++;
-        if (indexOptions == IndexOptions.DOCS_ONLY) {
-          accum += in.readVInt();
-        } else {
-          final int code = in.readVInt();
-          accum += code >>> 1;
-          if (VERBOSE) System.out.println("  docID=" + accum + " code=" + code);
-          if ((code & 1) != 0) {
-            freq = 1;
-          } else {
-            freq = in.readVInt();
-            assert freq > 0;
-          }
-
-          if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-            // Skip positions
-            for(int posUpto=0;posUpto<freq;posUpto++) {
-              if (!storePayloads) {
-                in.readVInt();
-              } else {
-                final int posCode = in.readVInt();
-                if ((posCode & 1) != 0) {
-                  payloadLen = in.readVInt();
-                }
-                in.skipBytes(payloadLen);
-              }
-            }
-          }
-        }
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          if (VERBOSE) System.out.println("    return docID=" + accum + " freq=" + freq);
-          return (docID = accum);
-        }
-      }
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int advance(int target) {
-      // TODO: we could make more efficient version, but, it
-      // should be rare that this will matter in practice
-      // since usually apps will not store "big" fields in
-      // this codec!
-      //System.out.println("advance start docID=" + docID + " target=" + target);
-      while(nextDoc() < target) {
-      }
-      return docID;
-    }
-
-    @Override
-    public int freq() {
-      assert indexOptions != IndexOptions.DOCS_ONLY;
-      return freq;
-    }
-  }
-
-  private final static class FSTDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private final boolean storePayloads;
-    private byte[] buffer = new byte[16];
-    private final ByteArrayDataInput in = new ByteArrayDataInput(buffer);
-
-    private Bits liveDocs;
-    private int docUpto;
-    private int docID = -1;
-    private int accum;
-    private int freq;
-    private int numDocs;
-    private int posPending;
-    private int payloadLength;
-    private boolean payloadRetrieved;
-
-    private int pos;
-    private final BytesRef payload = new BytesRef();
-
-    public FSTDocsAndPositionsEnum(boolean storePayloads) {
-      this.storePayloads = storePayloads;
-    }
-
-    public boolean canReuse(boolean storePayloads) {
-      return storePayloads == this.storePayloads;
-    }
-    
-    public FSTDocsAndPositionsEnum reset(BytesRef bufferIn, Bits liveDocs, int numDocs) {
-      assert numDocs > 0;
-      if (VERBOSE) {
-        System.out.println("D&P reset bytes this=" + this);
-        for(int i=bufferIn.offset;i<bufferIn.length;i++) {
-          System.out.println("  " + Integer.toHexString(bufferIn.bytes[i]&0xFF));
-        }
-      }
-      if (buffer.length < bufferIn.length - bufferIn.offset) {
-        buffer = ArrayUtil.grow(buffer, bufferIn.length - bufferIn.offset);
-      }
-      in.reset(buffer, 0, bufferIn.length - bufferIn.offset);
-      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length - bufferIn.offset);
-      this.liveDocs = liveDocs;
-      docID = -1;
-      accum = 0;
-      docUpto = 0;
-      payload.bytes = buffer;
-      payloadLength = 0;
-      this.numDocs = numDocs;
-      posPending = 0;
-      payloadRetrieved = false;
-      return this;
-    }
-
-    @Override
-    public int nextDoc() {
-      while (posPending > 0) {
-        nextPosition();
-      }
-      while(true) {
-        if (VERBOSE) System.out.println("  nextDoc cycle docUpto=" + docUpto + " numDocs=" + numDocs + " fp=" + in.getPosition() + " this=" + this);
-        if (docUpto == numDocs) {
-          if (VERBOSE) System.out.println("    END");
-          return docID = NO_MORE_DOCS;
-        }
-        docUpto++;
-        
-        final int code = in.readVInt();
-        accum += code >>> 1;
-        if ((code & 1) != 0) {
-          freq = 1;
-        } else {
-          freq = in.readVInt();
-          assert freq > 0;
-        }
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          pos = 0;
-          posPending = freq;
-          if (VERBOSE) System.out.println("    return docID=" + accum + " freq=" + freq);
-          return (docID = accum);
-        }
-
-        // Skip positions
-        for(int posUpto=0;posUpto<freq;posUpto++) {
-          if (!storePayloads) {
-            in.readVInt();
-          } else {
-            final int skipCode = in.readVInt();
-            if ((skipCode & 1) != 0) {
-              payloadLength = in.readVInt();
-              if (VERBOSE) System.out.println("    new payloadLen=" + payloadLength);
-            }
-            in.skipBytes(payloadLength);
-          }
-        }
-      }
-    }
-
-    @Override
-    public int nextPosition() {
-      if (VERBOSE) System.out.println("    nextPos storePayloads=" + storePayloads + " this=" + this);
-      assert posPending > 0;
-      posPending--;
-      if (!storePayloads) {
-        pos += in.readVInt();
-      } else {
-        final int code = in.readVInt();
-        pos += code >>> 1;
-        if ((code & 1) != 0) {
-          payloadLength = in.readVInt();
-          //System.out.println("      new payloadLen=" + payloadLength);
-          //} else {
-          //System.out.println("      same payloadLen=" + payloadLength);
-        }
-        payload.offset = in.getPosition();
-        in.skipBytes(payloadLength);
-        payload.length = payloadLength;
-        // Necessary, in case caller changed the
-        // payload.bytes from prior call:
-        payload.bytes = buffer;
-        payloadRetrieved = false;
-      }
-
-      if (VERBOSE) System.out.println("      pos=" + pos + " payload=" + payload + " fp=" + in.getPosition());
-      return pos;
-    }
-
-    @Override
-    public BytesRef getPayload() {
-      payloadRetrieved = true;
-      return payload;
-    }
-
-    @Override
-    public boolean hasPayload() {
-      return !payloadRetrieved && payload.length > 0;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int advance(int target) {
-      // TODO: we could make more efficient version, but, it
-      // should be rare that this will matter in practice
-      // since usually apps will not store "big" fields in
-      // this codec!
-      //System.out.println("advance target=" + target);
-      while(nextDoc() < target) {
-      }
-      //System.out.println("  return " + docID);
-      return docID;
-    }
-
-    @Override
-    public int freq() {
-      return freq;
-    }
-  }
-
-  private final static class FSTTermsEnum extends TermsEnum {
-    private final FieldInfo field;
-    private final BytesRefFSTEnum<BytesRef> fstEnum;
-    private final ByteArrayDataInput buffer = new ByteArrayDataInput();
-    private boolean didDecode;
-
-    private int docFreq;
-    private long totalTermFreq;
-    private BytesRefFSTEnum.InputOutput<BytesRef> current;
-
-    public FSTTermsEnum(FieldInfo field, FST<BytesRef> fst) {
-      this.field = field;
-      fstEnum = new BytesRefFSTEnum<BytesRef>(fst);
-    }
-
-    private void decodeMetaData() throws IOException {
-      if (!didDecode) {
-        buffer.reset(current.output.bytes, 0, current.output.length);
-        docFreq = buffer.readVInt();
-        if (field.indexOptions != IndexOptions.DOCS_ONLY) {
-          totalTermFreq = docFreq + buffer.readVLong();
-        } else {
-          totalTermFreq = -1;
-        }
-        current.output.offset = buffer.getPosition();
-        if (VERBOSE) System.out.println("  df=" + docFreq + " totTF=" + totalTermFreq + " offset=" + buffer.getPosition() + " len=" + current.output.length);
-        didDecode = true;
-      }
-    }
-
-    @Override
-    public boolean seekExact(BytesRef text, boolean useCache /* ignored */) throws IOException {
-      if (VERBOSE) System.out.println("te.seekExact text=" + field.name + ":" + text.utf8ToString() + " this=" + this);
-      current = fstEnum.seekExact(text);
-      didDecode = false;
-      return current != null;
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
-      if (VERBOSE) System.out.println("te.seek text=" + field.name + ":" + text.utf8ToString() + " this=" + this);
-      current = fstEnum.seekCeil(text);
-      if (current == null) {
-        return SeekStatus.END;
-      } else {
-        if (VERBOSE) {
-          System.out.println("  got term=" + current.input.utf8ToString());
-          for(int i=0;i<current.output.length;i++) {
-            System.out.println("    " + Integer.toHexString(current.output.bytes[i]&0xFF));
-          }
-        }
-
-        didDecode = false;
-
-        if (text.equals(current.input)) {
-          if (VERBOSE) System.out.println("  found!");
-          return SeekStatus.FOUND;
-        } else {
-          if (VERBOSE) System.out.println("  not found: " + current.input.utf8ToString());
-          return SeekStatus.NOT_FOUND;
-        }
-      }
-    }
-    
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-      decodeMetaData();
-      FSTDocsEnum docsEnum;
-
-      if (needsFreqs && field.indexOptions == IndexOptions.DOCS_ONLY) {
-        return null;
-      } else if (reuse == null || !(reuse instanceof FSTDocsEnum)) {
-        docsEnum = new FSTDocsEnum(field.indexOptions, field.storePayloads);
-      } else {
-        docsEnum = (FSTDocsEnum) reuse;        
-        if (!docsEnum.canReuse(field.indexOptions, field.storePayloads)) {
-          docsEnum = new FSTDocsEnum(field.indexOptions, field.storePayloads);
-        }
-      }
-      return docsEnum.reset(current.output, liveDocs, docFreq);
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
-      if (field.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-        return null;
-      }
-      decodeMetaData();
-      FSTDocsAndPositionsEnum docsAndPositionsEnum;
-      if (reuse == null || !(reuse instanceof FSTDocsAndPositionsEnum)) {
-        docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.storePayloads);
-      } else {
-        docsAndPositionsEnum = (FSTDocsAndPositionsEnum) reuse;        
-        if (!docsAndPositionsEnum.canReuse(field.storePayloads)) {
-          docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.storePayloads);
-        }
-      }
-      if (VERBOSE) System.out.println("D&P reset this=" + this);
-      return docsAndPositionsEnum.reset(current.output, liveDocs, docFreq);
-    }
-
-    @Override
-    public BytesRef term() {
-      return current.input;
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      if (VERBOSE) System.out.println("te.next");
-      current = fstEnum.next();
-      if (current == null) {
-        if (VERBOSE) System.out.println("  END");
-        return null;
-      }
-      didDecode = false;
-      if (VERBOSE) System.out.println("  term=" + field.name + ":" + current.input.utf8ToString());
-      return current.input;
-    }
-
-    @Override
-    public int docFreq() throws IOException {
-      decodeMetaData();
-      return docFreq;
-    }
-
-    @Override
-    public long totalTermFreq() throws IOException {
-      decodeMetaData();
-      return totalTermFreq;
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public void seekExact(long ord) {
-      // NOTE: we could add this...
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public long ord() {
-      // NOTE: we could add this...
-      throw new UnsupportedOperationException();
-    }
-  }
-
-  private final static class TermsReader extends Terms {
-
-    private final long sumTotalTermFreq;
-    private final long sumDocFreq;
-    private final int docCount;
-    private final int termCount;
-    private FST<BytesRef> fst;
-    private final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
-    private final FieldInfo field;
-
-    public TermsReader(FieldInfos fieldInfos, IndexInput in, int termCount) throws IOException {
-      this.termCount = termCount;
-      final int fieldNumber = in.readVInt();
-      field = fieldInfos.fieldInfo(fieldNumber);
-      if (field.indexOptions != IndexOptions.DOCS_ONLY) {
-        sumTotalTermFreq = in.readVLong();
-      } else {
-        sumTotalTermFreq = -1;
-      }
-      sumDocFreq = in.readVLong();
-      docCount = in.readVInt();
-      
-      fst = new FST<BytesRef>(in, outputs);
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return sumTotalTermFreq;
-    }
-
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return sumDocFreq;
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return docCount;
-    }
-
-    @Override
-    public long getUniqueTermCount() throws IOException {
-      return termCount;
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) {
-      return new FSTTermsEnum(field, fst);
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, EXTENSION);
-    final IndexInput in = state.dir.openInput(fileName, IOContext.READONCE);
-
-    final SortedMap<String,TermsReader> fields = new TreeMap<String,TermsReader>();
-
-    try {
-      while(true) {
-        final int termCount = in.readVInt();
-        if (termCount == 0) {
-          break;
-        }
-        final TermsReader termsReader = new TermsReader(state.fieldInfos, in, termCount);
-        fields.put(termsReader.field.name, termsReader);
-      }
-    } finally {
-      in.close();
-    }
-
-    return new FieldsProducer() {
-      @Override
-      public FieldsEnum iterator() {
-        final Iterator<TermsReader> iter = fields.values().iterator();
-
-        return new FieldsEnum() {
-
-          private TermsReader current;
-
-          @Override
-          public String next() {
-            current = iter.next();
-            return current.field.name;
-          }
-
-          @Override
-          public Terms terms() {
-            return current;
-          }
-        };
-      }
-
-      @Override
-      public Terms terms(String field) {
-        return fields.get(field);
-      }
-      
-      @Override
-      public int getUniqueFieldCount() {
-        return fields.size();
-      }
-
-      @Override
-      public void close() {
-        // Drop ref to FST:
-        for(TermsReader termsReader : fields.values()) {
-          termsReader.fst = null;
-        }
-      }
-    };
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, EXTENSION));
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/package.html b/lucene/src/java/org/apache/lucene/index/codecs/package.html
deleted file mode 100644
index 78dcb95..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Codecs API: API for customization of the encoding and structure of the index.
-</body>
-</html>
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/perfield/PerFieldPostingsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/perfield/PerFieldPostingsFormat.java
deleted file mode 100644
index 9fa8948..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/perfield/PerFieldPostingsFormat.java
+++ /dev/null
@@ -1,340 +0,0 @@
-package org.apache.lucene.index.codecs.perfield;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.IdentityHashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Set;
-import java.util.TreeMap;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.TermsConsumer;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Enables per field format support.
- * 
- * @lucene.experimental
- */
-
-public abstract class PerFieldPostingsFormat extends PostingsFormat {
-
-  public static final String PER_FIELD_EXTENSION = "per";
-  public static final String PER_FIELD_NAME = "PerField40";
-
-  public static final int VERSION_START = 0;
-  public static final int VERSION_LATEST = VERSION_START;
-
-  public PerFieldPostingsFormat() {
-    super(PER_FIELD_NAME);
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state)
-      throws IOException {
-    return new FieldsWriter(state);
-  }
-
-  // NOTE: not private to avoid $accessN at runtime!!
-  static class FieldsConsumerAndID implements Closeable {
-    final FieldsConsumer fieldsConsumer;
-    final String segmentSuffix;
-
-    public FieldsConsumerAndID(FieldsConsumer fieldsConsumer, String segmentSuffix) {
-      this.fieldsConsumer = fieldsConsumer;
-      this.segmentSuffix = segmentSuffix;
-    }
-
-    @Override
-    public void close() throws IOException {
-      fieldsConsumer.close();
-    }
-  };
-    
-  private class FieldsWriter extends FieldsConsumer {
-
-    private final Map<PostingsFormat,FieldsConsumerAndID> formats = new IdentityHashMap<PostingsFormat,FieldsConsumerAndID>();
-
-    /** Records all fields we wrote. */
-    private final Map<String,PostingsFormat> fieldToFormat = new HashMap<String,PostingsFormat>();
-
-    private final SegmentWriteState segmentWriteState;
-
-    public FieldsWriter(SegmentWriteState state) throws IOException {
-      segmentWriteState = state;
-    }
-
-    @Override
-    public TermsConsumer addField(FieldInfo field) throws IOException {
-      final PostingsFormat format = getPostingsFormatForField(field.name);
-      if (format == null) {
-        throw new IllegalStateException("invalid null PostingsFormat for field=\"" + field.name + "\"");
-      }
-
-      assert !fieldToFormat.containsKey(field.name);
-      fieldToFormat.put(field.name, format);
-
-      FieldsConsumerAndID consumerAndId = formats.get(format);
-      if (consumerAndId == null) {
-        // First time we are seeing this format; assign
-        // next id and init it:
-        final String segmentSuffix = getFullSegmentSuffix(field.name,
-                                                          segmentWriteState.segmentSuffix,
-                                                          ""+formats.size());
-        consumerAndId = new FieldsConsumerAndID(format.fieldsConsumer(new SegmentWriteState(segmentWriteState, segmentSuffix)),
-                                                segmentSuffix);
-        formats.put(format, consumerAndId);
-      }
-
-      return consumerAndId.fieldsConsumer.addField(field);
-    }
-
-    @Override
-    public void close() throws IOException {
-
-      // Close all subs
-      IOUtils.close(formats.values());
-
-      // Write _X.per: maps field name -> format name and
-      // format name -> format id
-      final String mapFileName = IndexFileNames.segmentFileName(segmentWriteState.segmentName, segmentWriteState.segmentSuffix, PER_FIELD_EXTENSION);
-      final IndexOutput out = segmentWriteState.directory.createOutput(mapFileName, segmentWriteState.context);
-      boolean success = false;
-      try {
-        CodecUtil.writeHeader(out, PER_FIELD_NAME, VERSION_LATEST);
-
-        // format name -> int id
-        out.writeVInt(formats.size());
-        for(Map.Entry<PostingsFormat,FieldsConsumerAndID> ent : formats.entrySet()) {
-          out.writeString(ent.getValue().segmentSuffix);
-          out.writeString(ent.getKey().getName());
-        }
-
-        // field name -> format name
-        out.writeVInt(fieldToFormat.size());
-        for(Map.Entry<String,PostingsFormat> ent : fieldToFormat.entrySet()) {
-          out.writeString(ent.getKey());
-          out.writeString(ent.getValue().getName());
-        }
-
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(out);
-        } else {
-          IOUtils.close(out);
-        }
-      }
-    }
-  }
-
-  static String getFullSegmentSuffix(String fieldName, String outerSegmentSuffix, String segmentSuffix) {
-    if (outerSegmentSuffix.length() == 0) {
-      return segmentSuffix;
-    } else {
-      // TODO: support embedding; I think it should work but
-      // we need a test confirm to confirm
-      // return outerSegmentSuffix + "_" + segmentSuffix;
-      throw new IllegalStateException("cannot embed PerFieldPostingsFormat inside itself (field \"" + fieldName + "\" returned PerFieldPostingsFormat)");
-    }
-  }
-
-  private class FieldsReader extends FieldsProducer {
-
-    private final Map<String,FieldsProducer> fields = new TreeMap<String,FieldsProducer>();
-    private final Map<PostingsFormat,FieldsProducer> formats = new IdentityHashMap<PostingsFormat,FieldsProducer>();
-
-    public FieldsReader(final SegmentReadState readState) throws IOException {
-
-      // Read _X.per and init each format:
-      boolean success = false;
-      try {
-        new VisitPerFieldFile(readState.dir, readState.segmentInfo.name, readState.segmentSuffix) {
-          @Override
-          protected void visitOneFormat(String segmentSuffix, PostingsFormat postingsFormat) throws IOException {
-            formats.put(postingsFormat, postingsFormat.fieldsProducer(new SegmentReadState(readState, segmentSuffix)));
-          }
-
-          @Override
-          protected void visitOneField(String fieldName, PostingsFormat postingsFormat) throws IOException {
-            assert formats.containsKey(postingsFormat);
-            fields.put(fieldName, formats.get(postingsFormat));
-          }
-        };
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(formats.values());
-        }
-      }
-    }
-
-    private final class FieldsIterator extends FieldsEnum {
-      private final Iterator<String> it;
-      private String current;
-
-      public FieldsIterator() {
-        it = fields.keySet().iterator();
-      }
-
-      @Override
-      public String next() throws IOException {
-        if (it.hasNext()) {
-          current = it.next();
-        } else {
-          current = null;
-        }
-
-        return current;
-      }
-
-      @Override
-      public Terms terms() throws IOException {
-        return fields.get(current).terms(current);
-      }
-    }
-
-    @Override
-    public FieldsEnum iterator() throws IOException {
-      return new FieldsIterator();
-    }
-
-    @Override
-    public Terms terms(String field) throws IOException {
-      FieldsProducer fieldsProducer = fields.get(field);
-      return fieldsProducer == null ? null : fieldsProducer.terms(field);
-    }
-    
-    @Override
-    public int getUniqueFieldCount() {
-      return fields.size();
-    }
-
-    @Override
-    public void close() throws IOException {
-      IOUtils.close(formats.values());
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state)
-      throws IOException {
-    return new FieldsReader(state);
-  }
-
-  private abstract class VisitPerFieldFile {
-    public VisitPerFieldFile(Directory dir, String segmentName, String outerSegmentSuffix) throws IOException {
-      final String mapFileName = IndexFileNames.segmentFileName(segmentName, outerSegmentSuffix, PER_FIELD_EXTENSION);
-      final IndexInput in = dir.openInput(mapFileName, IOContext.READONCE);
-      boolean success = false;
-      try {
-        CodecUtil.checkHeader(in, PER_FIELD_NAME, VERSION_START, VERSION_LATEST);
-
-        // Read format name -> format id
-        final int formatCount = in.readVInt();
-        for(int formatIDX=0;formatIDX<formatCount;formatIDX++) {
-          final String segmentSuffix = in.readString();
-          final String formatName = in.readString();
-          PostingsFormat postingsFormat = PostingsFormat.forName(formatName);
-          //System.out.println("do lookup " + formatName + " -> " + postingsFormat);
-          if (postingsFormat == null) {
-            throw new IllegalStateException("unable to lookup PostingsFormat for name=\"" + formatName + "\": got null");
-          }
-
-          // Better be defined, because it was defined
-          // during indexing:
-          visitOneFormat(segmentSuffix, postingsFormat);
-        }
-
-        // Read field name -> format name
-        final int fieldCount = in.readVInt();
-        for(int fieldIDX=0;fieldIDX<fieldCount;fieldIDX++) {
-          final String fieldName = in.readString();
-          final String formatName = in.readString();
-          visitOneField(fieldName, PostingsFormat.forName(formatName));
-        }
-
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(in);
-        } else {
-          IOUtils.close(in);
-        }
-      }
-    }
-
-    // This is called first, for all formats:
-    protected abstract void visitOneFormat(String segmentSuffix, PostingsFormat format) throws IOException;
-
-    // ... then this is called, for all fields:
-    protected abstract void visitOneField(String fieldName, PostingsFormat format) throws IOException;
-  }
-
-  @Override
-  public void files(final Directory dir, final SegmentInfo info, String segmentSuffix, final Set<String> files)
-      throws IOException {
-
-    final String mapFileName = IndexFileNames.segmentFileName(info.name, segmentSuffix, PER_FIELD_EXTENSION);
-    files.add(mapFileName);
-
-    try {
-      new VisitPerFieldFile(dir, info.name, segmentSuffix) {
-        @Override
-        protected void visitOneFormat(String segmentSuffix, PostingsFormat format) throws IOException {
-          format.files(dir, info, segmentSuffix, files);
-        }
-
-        @Override
-          protected void visitOneField(String field, PostingsFormat format) {
-        }
-      };
-    } catch (FileNotFoundException fnfe) {
-      // TODO: this is somewhat shady... if we can't open
-      // the .per file then most likely someone is calling
-      // .files() after this segment was deleted, so, they
-      // wouldn't be able to do anything with the files even
-      // if we could return them, so we don't add any files
-      // in this case.
-    }
-  }
-
-  // NOTE: only called during writing; for reading we read
-  // all we need from the index (ie we save the field ->
-  // format mapping)
-  public abstract PostingsFormat getPostingsFormatForField(String field);
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/pulsing/Pulsing40PostingsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/pulsing/Pulsing40PostingsFormat.java
deleted file mode 100644
index 6d20af0..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/pulsing/Pulsing40PostingsFormat.java
+++ /dev/null
@@ -1,42 +0,0 @@
-package org.apache.lucene.index.codecs.pulsing;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsBaseFormat;
-
-/**
- * @lucene.experimental
- */
-public class Pulsing40PostingsFormat extends PulsingPostingsFormat {
-
-  /** Inlines docFreq=1 terms, otherwise uses the normal "Lucene40" format. */
-  public Pulsing40PostingsFormat() {
-    this(1);
-  }
-
-  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene40" format. */
-  public Pulsing40PostingsFormat(int freqCutoff) {
-    this(freqCutoff, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-  }
-
-  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene40" format. */
-  public Pulsing40PostingsFormat(int freqCutoff, int minBlockSize, int maxBlockSize) {
-    super("Pulsing40", new Lucene40PostingsBaseFormat(), freqCutoff, minBlockSize, maxBlockSize);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsFormat.java
deleted file mode 100644
index 0c83f20..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsFormat.java
+++ /dev/null
@@ -1,122 +0,0 @@
-package org.apache.lucene.index.codecs.pulsing;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.BlockTreeTermsReader;
-import org.apache.lucene.index.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.index.codecs.PostingsBaseFormat;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.store.Directory;
-
-/** This postings format "inlines" the postings for terms that have
- *  low docFreq.  It wraps another postings format, which is used for
- *  writing the non-inlined terms.
- *
- *  @lucene.experimental */
-
-public abstract class PulsingPostingsFormat extends PostingsFormat {
-
-  private final int freqCutoff;
-  private final int minBlockSize;
-  private final int maxBlockSize;
-  private final PostingsBaseFormat wrappedPostingsBaseFormat;
-  
-  public PulsingPostingsFormat(String name, PostingsBaseFormat wrappedPostingsBaseFormat, int freqCutoff) {
-    this(name, wrappedPostingsBaseFormat, freqCutoff, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-  }
-
-  /** Terms with freq <= freqCutoff are inlined into terms
-   *  dict. */
-  public PulsingPostingsFormat(String name, PostingsBaseFormat wrappedPostingsBaseFormat, int freqCutoff, int minBlockSize, int maxBlockSize) {
-    super(name);
-    this.freqCutoff = freqCutoff;
-    this.minBlockSize = minBlockSize;
-    assert minBlockSize > 1;
-    this.maxBlockSize = maxBlockSize;
-    this.wrappedPostingsBaseFormat = wrappedPostingsBaseFormat;
-  }
-
-  @Override
-  public String toString() {
-    return getName() + "(freqCutoff=" + freqCutoff + " minBlockSize=" + minBlockSize + " maxBlockSize=" + maxBlockSize + ")";
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docsWriter = wrappedPostingsBaseFormat.postingsWriterBase(state);
-
-    // Terms that have <= freqCutoff number of docs are
-    // "pulsed" (inlined):
-    PostingsWriterBase pulsingWriter = new PulsingPostingsWriter(freqCutoff, docsWriter);
-
-    // Terms dict
-    boolean success = false;
-    try {
-      FieldsConsumer ret = new BlockTreeTermsWriter(state, pulsingWriter, minBlockSize, maxBlockSize);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        pulsingWriter.close();
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-
-    PostingsReaderBase docsReader = wrappedPostingsBaseFormat.postingsReaderBase(state);
-    PostingsReaderBase pulsingReader = new PulsingPostingsReader(docsReader);
-
-    boolean success = false;
-    try {
-      FieldsProducer ret = new BlockTreeTermsReader(
-                                                    state.dir, state.fieldInfos, state.segmentInfo.name,
-                                                    pulsingReader,
-                                                    state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        pulsingReader.close();
-      }
-    }
-  }
-
-  public int getFreqCutoff() {
-    return freqCutoff;
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    wrappedPostingsBaseFormat.files(dir, segmentInfo, segmentSuffix, files);
-    BlockTreeTermsReader.files(dir, segmentInfo, segmentSuffix, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReader.java
deleted file mode 100644
index 8fb5761..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsReader.java
+++ /dev/null
@@ -1,596 +0,0 @@
-package org.apache.lucene.index.codecs.pulsing;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.IdentityHashMap;
-import java.util.Map;
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.BlockTermState;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Attribute;
-import org.apache.lucene.util.AttributeImpl;
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-
-/** Concrete class that reads the current doc/freq/skip
- *  postings format 
- *  @lucene.experimental */
-
-// TODO: -- should we switch "hasProx" higher up?  and
-// create two separate docs readers, one that also reads
-// prox and one that doesn't?
-
-public class PulsingPostingsReader extends PostingsReaderBase {
-
-  // Fallback reader for non-pulsed terms:
-  final PostingsReaderBase wrappedPostingsReader;
-  int maxPositions;
-
-  public PulsingPostingsReader(PostingsReaderBase wrappedPostingsReader) throws IOException {
-    this.wrappedPostingsReader = wrappedPostingsReader;
-  }
-
-  @Override
-  public void init(IndexInput termsIn) throws IOException {
-    CodecUtil.checkHeader(termsIn, PulsingPostingsWriter.CODEC,
-      PulsingPostingsWriter.VERSION_START, PulsingPostingsWriter.VERSION_START);
-    maxPositions = termsIn.readVInt();
-    wrappedPostingsReader.init(termsIn);
-  }
-
-  private static class PulsingTermState extends BlockTermState {
-    private byte[] postings;
-    private int postingsSize;                     // -1 if this term was not inlined
-    private BlockTermState wrappedTermState;
-
-    ByteArrayDataInput inlinedBytesReader;
-    private byte[] inlinedBytes;
-
-    @Override
-    public Object clone() {
-      PulsingTermState clone;
-      clone = (PulsingTermState) super.clone();
-      if (postingsSize != -1) {
-        clone.postings = new byte[postingsSize];
-        System.arraycopy(postings, 0, clone.postings, 0, postingsSize);
-      } else {
-        assert wrappedTermState != null;
-        clone.wrappedTermState = (BlockTermState) wrappedTermState.clone();
-      }
-      return clone;
-    }
-
-    @Override
-    public void copyFrom(TermState _other) {
-      super.copyFrom(_other);
-      PulsingTermState other = (PulsingTermState) _other;
-      postingsSize = other.postingsSize;
-      if (other.postingsSize != -1) {
-        if (postings == null || postings.length < other.postingsSize) {
-          postings = new byte[ArrayUtil.oversize(other.postingsSize, 1)];
-        }
-        System.arraycopy(other.postings, 0, postings, 0, other.postingsSize);
-      } else {
-        wrappedTermState.copyFrom(other.wrappedTermState);
-      }
-
-      // NOTE: we do not copy the
-      // inlinedBytes/inlinedBytesReader; these are only
-      // stored on the "primary" TermState.  They are
-      // "transient" to cloned term states.
-    }
-
-    @Override
-    public String toString() {
-      if (postingsSize == -1) {
-        return "PulsingTermState: not inlined: wrapped=" + wrappedTermState;
-      } else {
-        return "PulsingTermState: inlined size=" + postingsSize + " " + super.toString();
-      }
-    }
-  }
-
-  @Override
-  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
-    //System.out.println("PR.readTermsBlock state=" + _termState);
-    final PulsingTermState termState = (PulsingTermState) _termState;
-    if (termState.inlinedBytes == null) {
-      termState.inlinedBytes = new byte[128];
-      termState.inlinedBytesReader = new ByteArrayDataInput();
-    }
-    int len = termsIn.readVInt();
-    //System.out.println("  len=" + len + " fp=" + termsIn.getFilePointer());
-    if (termState.inlinedBytes.length < len) {
-      termState.inlinedBytes = new byte[ArrayUtil.oversize(len, 1)];
-    }
-    termsIn.readBytes(termState.inlinedBytes, 0, len);
-    termState.inlinedBytesReader.reset(termState.inlinedBytes);
-    termState.wrappedTermState.termBlockOrd = 0;
-    wrappedPostingsReader.readTermsBlock(termsIn, fieldInfo, termState.wrappedTermState);
-  }
-
-  @Override
-  public BlockTermState newTermState() throws IOException {
-    PulsingTermState state = new PulsingTermState();
-    state.wrappedTermState = wrappedPostingsReader.newTermState();
-    return state;
-  }
-
-  @Override
-  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
-    //System.out.println("PR nextTerm");
-    PulsingTermState termState = (PulsingTermState) _termState;
-
-    // if we have positions, its total TF, otherwise its computed based on docFreq.
-    long count = fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS ? termState.totalTermFreq : termState.docFreq;
-    //System.out.println("  count=" + count + " threshold=" + maxPositions);
-
-    if (count <= maxPositions) {
-
-      // Inlined into terms dict -- just read the byte[] blob in,
-      // but don't decode it now (we only decode when a DocsEnum
-      // or D&PEnum is pulled):
-      termState.postingsSize = termState.inlinedBytesReader.readVInt();
-      if (termState.postings == null || termState.postings.length < termState.postingsSize) {
-        termState.postings = new byte[ArrayUtil.oversize(termState.postingsSize, 1)];
-      }
-      // TODO: sort of silly to copy from one big byte[]
-      // (the blob holding all inlined terms' blobs for
-      // current term block) into another byte[] (just the
-      // blob for this term)...
-      termState.inlinedBytesReader.readBytes(termState.postings, 0, termState.postingsSize);
-      //System.out.println("  inlined bytes=" + termState.postingsSize);
-    } else {
-      //System.out.println("  not inlined");
-      termState.postingsSize = -1;
-      // TODO: should we do full copyFrom?  much heavier...?
-      termState.wrappedTermState.docFreq = termState.docFreq;
-      termState.wrappedTermState.totalTermFreq = termState.totalTermFreq;
-      wrappedPostingsReader.nextTerm(fieldInfo, termState.wrappedTermState);
-      termState.wrappedTermState.termBlockOrd++;
-    }
-  }
-
-  @Override
-  public DocsEnum docs(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-    if (needsFreqs && field.indexOptions == IndexOptions.DOCS_ONLY) {
-      return null;
-    }
-    PulsingTermState termState = (PulsingTermState) _termState;
-    if (termState.postingsSize != -1) {
-      PulsingDocsEnum postings;
-      if (reuse instanceof PulsingDocsEnum) {
-        postings = (PulsingDocsEnum) reuse;
-        if (!postings.canReuse(field)) {
-          postings = new PulsingDocsEnum(field);
-        }
-      } else {
-        // the 'reuse' is actually the wrapped enum
-        PulsingDocsEnum previous = (PulsingDocsEnum) getOther(reuse);
-        if (previous != null && previous.canReuse(field)) {
-          postings = previous;
-        } else {
-          postings = new PulsingDocsEnum(field);
-        }
-      }
-      if (reuse != postings) {
-        setOther(postings, reuse); // postings.other = reuse
-      }
-      return postings.reset(liveDocs, termState);
-    } else {
-      if (reuse instanceof PulsingDocsEnum) {
-        DocsEnum wrapped = wrappedPostingsReader.docs(field, termState.wrappedTermState, liveDocs, getOther(reuse), needsFreqs);
-        setOther(wrapped, reuse); // wrapped.other = reuse
-        return wrapped;
-      } else {
-        return wrappedPostingsReader.docs(field, termState.wrappedTermState, liveDocs, reuse, needsFreqs);
-      }
-    }
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
-    if (field.indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      return null;
-    }
-    //System.out.println("D&P: field=" + field.name);
-
-    final PulsingTermState termState = (PulsingTermState) _termState;
-
-    if (termState.postingsSize != -1) {
-      PulsingDocsAndPositionsEnum postings;
-      if (reuse instanceof PulsingDocsAndPositionsEnum) {
-        postings = (PulsingDocsAndPositionsEnum) reuse;
-        if (!postings.canReuse(field)) {
-          postings = new PulsingDocsAndPositionsEnum(field);
-        }
-      } else {
-        // the 'reuse' is actually the wrapped enum
-        PulsingDocsAndPositionsEnum previous = (PulsingDocsAndPositionsEnum) getOther(reuse);
-        if (previous != null && previous.canReuse(field)) {
-          postings = previous;
-        } else {
-          postings = new PulsingDocsAndPositionsEnum(field);
-        }
-      }
-      if (reuse != postings) {
-        setOther(postings, reuse); // postings.other = reuse 
-      }
-      return postings.reset(liveDocs, termState);
-    } else {
-      if (reuse instanceof PulsingDocsAndPositionsEnum) {
-        DocsAndPositionsEnum wrapped = wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, liveDocs, (DocsAndPositionsEnum) getOther(reuse));
-        setOther(wrapped, reuse); // wrapped.other = reuse
-        return wrapped;
-      } else {
-        return wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, liveDocs, reuse);
-      }
-    }
-  }
-
-  private static class PulsingDocsEnum extends DocsEnum {
-    private byte[] postingsBytes;
-    private final ByteArrayDataInput postings = new ByteArrayDataInput();
-    private final IndexOptions indexOptions;
-    private final boolean storePayloads;
-    private Bits liveDocs;
-    private int docID = -1;
-    private int accum;
-    private int freq;
-    private int payloadLength;
-
-    public PulsingDocsEnum(FieldInfo fieldInfo) {
-      indexOptions = fieldInfo.indexOptions;
-      storePayloads = fieldInfo.storePayloads;
-    }
-
-    public PulsingDocsEnum reset(Bits liveDocs, PulsingTermState termState) {
-      //System.out.println("PR docsEnum termState=" + termState + " docFreq=" + termState.docFreq);
-      assert termState.postingsSize != -1;
-
-      // Must make a copy of termState's byte[] so that if
-      // app does TermsEnum.next(), this DocsEnum is not affected
-      if (postingsBytes == null) {
-        postingsBytes = new byte[termState.postingsSize];
-      } else if (postingsBytes.length < termState.postingsSize) {
-        postingsBytes = ArrayUtil.grow(postingsBytes, termState.postingsSize);
-      }
-      System.arraycopy(termState.postings, 0, postingsBytes, 0, termState.postingsSize);
-      postings.reset(postingsBytes, 0, termState.postingsSize);
-      docID = -1;
-      accum = 0;
-      payloadLength = 0;
-      this.liveDocs = liveDocs;
-      return this;
-    }
-
-    boolean canReuse(FieldInfo fieldInfo) {
-      return indexOptions == fieldInfo.indexOptions && storePayloads == fieldInfo.storePayloads;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      //System.out.println("PR nextDoc this= "+ this);
-      while(true) {
-        if (postings.eof()) {
-          //System.out.println("PR   END");
-          return docID = NO_MORE_DOCS;
-        }
-
-        final int code = postings.readVInt();
-        //System.out.println("  read code=" + code);
-        if (indexOptions == IndexOptions.DOCS_ONLY) {
-          accum += code;
-        } else {
-          accum += code >>> 1;              // shift off low bit
-          if ((code & 1) != 0) {          // if low bit is set
-            freq = 1;                     // freq is one
-          } else {
-            freq = postings.readVInt();     // else read freq
-          }
-
-          if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-            // Skip positions
-            if (storePayloads) {
-              for(int pos=0;pos<freq;pos++) {
-                final int posCode = postings.readVInt();
-                if ((posCode & 1) != 0) {
-                  payloadLength = postings.readVInt();
-                }
-                if (payloadLength != 0) {
-                  postings.skipBytes(payloadLength);
-                }
-              }
-            } else {
-              for(int pos=0;pos<freq;pos++) {
-                // TODO: skipVInt
-                postings.readVInt();
-              }
-            }
-          }
-        }
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          return (docID = accum);
-        }
-      }
-    }
-
-    @Override
-    public int freq() {
-      assert indexOptions != IndexOptions.DOCS_ONLY;
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      int doc;
-      while((doc=nextDoc()) != NO_MORE_DOCS) {
-        if (doc >= target)
-          return doc;
-      }
-      return docID = NO_MORE_DOCS;
-    }
-  }
-
-  private static class PulsingDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private byte[] postingsBytes;
-    private final ByteArrayDataInput postings = new ByteArrayDataInput();
-    private final boolean storePayloads;
-
-    private Bits liveDocs;
-    private int docID = -1;
-    private int accum;
-    private int freq;
-    private int posPending;
-    private int position;
-    private int payloadLength;
-    private BytesRef payload;
-
-    private boolean payloadRetrieved;
-
-    public PulsingDocsAndPositionsEnum(FieldInfo fieldInfo) {
-      storePayloads = fieldInfo.storePayloads;
-    }
-
-    boolean canReuse(FieldInfo fieldInfo) {
-      return storePayloads == fieldInfo.storePayloads;
-    }
-
-    public PulsingDocsAndPositionsEnum reset(Bits liveDocs, PulsingTermState termState) {
-      assert termState.postingsSize != -1;
-      if (postingsBytes == null) {
-        postingsBytes = new byte[termState.postingsSize];
-      } else if (postingsBytes.length < termState.postingsSize) {
-        postingsBytes = ArrayUtil.grow(postingsBytes, termState.postingsSize);
-      }
-      System.arraycopy(termState.postings, 0, postingsBytes, 0, termState.postingsSize);
-      postings.reset(postingsBytes, 0, termState.postingsSize);
-      this.liveDocs = liveDocs;
-      payloadLength = 0;
-      posPending = 0;
-      docID = -1;
-      accum = 0;
-      //System.out.println("PR d&p reset storesPayloads=" + storePayloads + " bytes=" + bytes.length + " this=" + this);
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      //System.out.println("PR d&p nextDoc this=" + this);
-
-      while(true) {
-        //System.out.println("  cycle skip posPending=" + posPending);
-
-        skipPositions();
-
-        if (postings.eof()) {
-          //System.out.println("PR   END");
-          return docID = NO_MORE_DOCS;
-        }
-
-        final int code = postings.readVInt();
-        accum += code >>> 1;            // shift off low bit
-        if ((code & 1) != 0) {          // if low bit is set
-          freq = 1;                     // freq is one
-        } else {
-          freq = postings.readVInt();     // else read freq
-        }
-        posPending = freq;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          //System.out.println("  return docID=" + docID + " freq=" + freq);
-          position = 0;
-          return (docID = accum);
-        }
-      }
-    }
-
-    @Override
-    public int freq() {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      int doc;
-      while((doc=nextDoc()) != NO_MORE_DOCS) {
-        if (doc >= target) {
-          return docID = doc;
-        }
-      }
-      return docID = NO_MORE_DOCS;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      //System.out.println("PR d&p nextPosition posPending=" + posPending + " vs freq=" + freq);
-      
-      assert posPending > 0;
-      posPending--;
-
-      if (storePayloads) {
-        if (!payloadRetrieved) {
-          //System.out.println("PR     skip payload=" + payloadLength);
-          postings.skipBytes(payloadLength);
-        }
-        final int code = postings.readVInt();
-        //System.out.println("PR     code=" + code);
-        if ((code & 1) != 0) {
-          payloadLength = postings.readVInt();
-          //System.out.println("PR     new payload len=" + payloadLength);
-        }
-        position += code >> 1;
-        payloadRetrieved = false;
-      } else {
-        position += postings.readVInt();
-      }
-
-      //System.out.println("PR d&p nextPos return pos=" + position + " this=" + this);
-      return position;
-    }
-
-    private void skipPositions() throws IOException {
-      while(posPending != 0) {
-        nextPosition();
-      }
-      if (storePayloads && !payloadRetrieved) {
-        //System.out.println("  skip payload len=" + payloadLength);
-        postings.skipBytes(payloadLength);
-        payloadRetrieved = true;
-      }
-    }
-
-    @Override
-    public boolean hasPayload() {
-      return storePayloads && !payloadRetrieved && payloadLength > 0;
-    }
-
-    @Override
-    public BytesRef getPayload() throws IOException {
-      //System.out.println("PR  getPayload payloadLength=" + payloadLength + " this=" + this);
-      if (payloadRetrieved) {
-        throw new IOException("Either no payload exists at this term position or an attempt was made to load it more than once.");
-      }
-      payloadRetrieved = true;
-      if (payloadLength > 0) {
-        if (payload == null) {
-          payload = new BytesRef(payloadLength);
-        } else {
-          payload.grow(payloadLength);
-        }
-        postings.readBytes(payload.bytes, 0, payloadLength);
-        payload.length = payloadLength;
-        return payload;
-      } else {
-        return null;
-      }
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    wrappedPostingsReader.close();
-  }
-  
-  /** for a docsenum, gets the 'other' reused enum.
-   * Example: Pulsing(Standard).
-   * when doing a term range query you are switching back and forth
-   * between Pulsing and Standard
-   * 
-   * The way the reuse works is that Pulsing.other = Standard and
-   * Standard.other = Pulsing.
-   */
-  private DocsEnum getOther(DocsEnum de) {
-    if (de == null) {
-      return null;
-    } else {
-      final AttributeSource atts = de.attributes();
-      return atts.addAttribute(PulsingEnumAttribute.class).enums().get(this);
-    }
-  }
-  
-  /** 
-   * for a docsenum, sets the 'other' reused enum.
-   * see getOther for an example.
-   */
-  private DocsEnum setOther(DocsEnum de, DocsEnum other) {
-    final AttributeSource atts = de.attributes();
-    return atts.addAttribute(PulsingEnumAttribute.class).enums().put(this, other);
-  }
-
-  /** 
-   * A per-docsenum attribute that stores additional reuse information
-   * so that pulsing enums can keep a reference to their wrapped enums,
-   * and vice versa. this way we can always reuse.
-   * 
-   * @lucene.internal */
-  public static interface PulsingEnumAttribute extends Attribute {
-    public Map<PulsingPostingsReader,DocsEnum> enums();
-  }
-    
-  /** @lucene.internal */
-  public static final class PulsingEnumAttributeImpl extends AttributeImpl implements PulsingEnumAttribute {
-    // we could store 'other', but what if someone 'chained' multiple postings readers,
-    // this could cause problems?
-    // TODO: we should consider nuking this map and just making it so if you do this,
-    // you don't reuse? and maybe pulsingPostingsReader should throw an exc if it wraps
-    // another pulsing, because this is just stupid and wasteful. 
-    // we still have to be careful in case someone does Pulsing(Stomping(Pulsing(...
-    private final Map<PulsingPostingsReader,DocsEnum> enums = 
-      new IdentityHashMap<PulsingPostingsReader,DocsEnum>();
-      
-    public Map<PulsingPostingsReader,DocsEnum> enums() {
-      return enums;
-    }
-
-    @Override
-    public void clear() {
-      // our state is per-docsenum, so this makes no sense.
-      // its best not to clear, in case a wrapped enum has a per-doc attribute or something
-      // and is calling clearAttributes(), so they don't nuke the reuse information!
-    }
-
-    @Override
-    public void copyTo(AttributeImpl target) {
-      // this makes no sense for us, because our state is per-docsenum.
-      // we don't want to copy any stuff over to another docsenum ever!
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsWriter.java
deleted file mode 100644
index 84a0341..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingPostingsWriter.java
+++ /dev/null
@@ -1,389 +0,0 @@
-package org.apache.lucene.index.codecs.pulsing;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-import java.util.ArrayList;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-
-// TODO: we now inline based on total TF of the term,
-// but it might be better to inline by "net bytes used"
-// so that a term that has only 1 posting but a huge
-// payload would not be inlined.  Though this is
-// presumably rare in practice...
-
-/** @lucene.experimental */
-public final class PulsingPostingsWriter extends PostingsWriterBase {
-
-  final static String CODEC = "PulsedPostingsWriter";
-
-  // To add a new version, increment from the last one, and
-  // change VERSION_CURRENT to point to your new version:
-  final static int VERSION_START = 0;
-
-  final static int VERSION_CURRENT = VERSION_START;
-
-  private IndexOutput termsOut;
-
-  private IndexOptions indexOptions;
-  private boolean storePayloads;
-
-  private static class PendingTerm {
-    private final byte[] bytes;
-    public PendingTerm(byte[] bytes) {
-      this.bytes = bytes;
-    }
-  }
-
-  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
-
-  // one entry per position
-  private final Position[] pending;
-  private int pendingCount = 0;                           // -1 once we've hit too many positions
-  private Position currentDoc;                    // first Position entry of current doc
-
-  private static final class Position {
-    BytesRef payload;
-    int termFreq;                                 // only incremented on first position for a given doc
-    int pos;
-    int docID;
-  }
-
-  // TODO: -- lazy init this?  ie, if every single term
-  // was inlined (eg for a "primary key" field) then we
-  // never need to use this fallback?  Fallback writer for
-  // non-inlined terms:
-  final PostingsWriterBase wrappedPostingsWriter;
-
-  /** If the total number of positions (summed across all docs
-   *  for this term) is <= maxPositions, then the postings are
-   *  inlined into terms dict */
-  public PulsingPostingsWriter(int maxPositions, PostingsWriterBase wrappedPostingsWriter) throws IOException {
-    pending = new Position[maxPositions];
-    for(int i=0;i<maxPositions;i++) {
-      pending[i] = new Position();
-    }
-
-    // We simply wrap another postings writer, but only call
-    // on it when tot positions is >= the cutoff:
-    this.wrappedPostingsWriter = wrappedPostingsWriter;
-  }
-
-  @Override
-  public void start(IndexOutput termsOut) throws IOException {
-    this.termsOut = termsOut;
-    CodecUtil.writeHeader(termsOut, CODEC, VERSION_CURRENT);
-    termsOut.writeVInt(pending.length); // encode maxPositions in header
-    wrappedPostingsWriter.start(termsOut);
-  }
-
-  @Override
-  public void startTerm() {
-    if (DEBUG) System.out.println("PW   startTerm");
-    assert pendingCount == 0;
-  }
-
-  // TODO: -- should we NOT reuse across fields?  would
-  // be cleaner
-
-  // Currently, this instance is re-used across fields, so
-  // our parent calls setField whenever the field changes
-  @Override
-  public void setField(FieldInfo fieldInfo) {
-    this.indexOptions = fieldInfo.indexOptions;
-    if (DEBUG) System.out.println("PW field=" + fieldInfo.name + " indexOptions=" + indexOptions);
-    storePayloads = fieldInfo.storePayloads;
-    wrappedPostingsWriter.setField(fieldInfo);
-    //DEBUG = BlockTreeTermsWriter.DEBUG;
-  }
-
-  private boolean DEBUG;
-
-  @Override
-  public void startDoc(int docID, int termDocFreq) throws IOException {
-    assert docID >= 0: "got docID=" + docID;
-
-    /*
-    if (termID != -1) {
-      if (docID == 0) {
-        baseDocID = termID;
-      } else if (baseDocID + docID != termID) {
-        throw new RuntimeException("WRITE: baseDocID=" + baseDocID + " docID=" + docID + " termID=" + termID);
-      }
-    }
-    */
-
-    if (DEBUG) System.out.println("PW     doc=" + docID);
-
-    if (pendingCount == pending.length) {
-      push();
-      if (DEBUG) System.out.println("PW: wrapped.finishDoc");
-      wrappedPostingsWriter.finishDoc();
-    }
-
-    if (pendingCount != -1) {
-      assert pendingCount < pending.length;
-      currentDoc = pending[pendingCount];
-      currentDoc.docID = docID;
-      if (indexOptions == IndexOptions.DOCS_ONLY) {
-        pendingCount++;
-      } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) { 
-        pendingCount++;
-        currentDoc.termFreq = termDocFreq;
-      } else {
-        currentDoc.termFreq = termDocFreq;
-      }
-    } else {
-      // We've already seen too many docs for this term --
-      // just forward to our fallback writer
-      wrappedPostingsWriter.startDoc(docID, termDocFreq);
-    }
-  }
-
-  @Override
-  public void addPosition(int position, BytesRef payload) throws IOException {
-
-    if (DEBUG) System.out.println("PW       pos=" + position + " payload=" + (payload == null ? "null" : payload.length + " bytes"));
-    if (pendingCount == pending.length) {
-      push();
-    }
-
-    if (pendingCount == -1) {
-      // We've already seen too many docs for this term --
-      // just forward to our fallback writer
-      wrappedPostingsWriter.addPosition(position, payload);
-    } else {
-      // buffer up
-      final Position pos = pending[pendingCount++];
-      pos.pos = position;
-      pos.docID = currentDoc.docID;
-      if (payload != null && payload.length > 0) {
-        if (pos.payload == null) {
-          pos.payload = BytesRef.deepCopyOf(payload);
-        } else {
-          pos.payload.copyBytes(payload);
-        }
-      } else if (pos.payload != null) {
-        pos.payload.length = 0;
-      }
-    }
-  }
-
-  @Override
-  public void finishDoc() throws IOException {
-    if (DEBUG) System.out.println("PW     finishDoc");
-    if (pendingCount == -1) {
-      wrappedPostingsWriter.finishDoc();
-    }
-  }
-
-  private final RAMOutputStream buffer = new RAMOutputStream();
-
-  // private int baseDocID;
-
-  /** Called when we are done adding docs to this term */
-  @Override
-  public void finishTerm(TermStats stats) throws IOException {
-    if (DEBUG) System.out.println("PW   finishTerm docCount=" + stats.docFreq + " pendingCount=" + pendingCount + " pendingTerms.size()=" + pendingTerms.size());
-
-    assert pendingCount > 0 || pendingCount == -1;
-
-    if (pendingCount == -1) {
-      wrappedPostingsWriter.finishTerm(stats);
-      // Must add null entry to record terms that our
-      // wrapped postings impl added
-      pendingTerms.add(null);
-    } else {
-
-      // There were few enough total occurrences for this
-      // term, so we fully inline our postings data into
-      // terms dict, now:
-
-      // TODO: it'd be better to share this encoding logic
-      // in some inner codec that knows how to write a
-      // single doc / single position, etc.  This way if a
-      // given codec wants to store other interesting
-      // stuff, it could use this pulsing codec to do so
-
-      if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-        int lastDocID = 0;
-        int pendingIDX = 0;
-        int lastPayloadLength = -1;
-        while(pendingIDX < pendingCount) {
-          final Position doc = pending[pendingIDX];
-
-          final int delta = doc.docID - lastDocID;
-          lastDocID = doc.docID;
-
-          if (DEBUG) System.out.println("  write doc=" + doc.docID + " freq=" + doc.termFreq);
-
-          if (doc.termFreq == 1) {
-            buffer.writeVInt((delta<<1)|1);
-          } else {
-            buffer.writeVInt(delta<<1);
-            buffer.writeVInt(doc.termFreq);
-          }
-
-          int lastPos = 0;
-          for(int posIDX=0;posIDX<doc.termFreq;posIDX++) {
-            final Position pos = pending[pendingIDX++];
-            assert pos.docID == doc.docID;
-            final int posDelta = pos.pos - lastPos;
-            lastPos = pos.pos;
-            if (DEBUG) System.out.println("    write pos=" + pos.pos);
-            if (storePayloads) {
-              final int payloadLength = pos.payload == null ? 0 : pos.payload.length;
-              if (payloadLength != lastPayloadLength) {
-                buffer.writeVInt((posDelta << 1)|1);
-                buffer.writeVInt(payloadLength);
-                lastPayloadLength = payloadLength;
-              } else {
-                buffer.writeVInt(posDelta << 1);
-              }
-              if (payloadLength > 0) {
-                buffer.writeBytes(pos.payload.bytes, 0, pos.payload.length);
-              }
-            } else {
-              buffer.writeVInt(posDelta);
-            }
-          }
-        }
-      } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
-        int lastDocID = 0;
-        for(int posIDX=0;posIDX<pendingCount;posIDX++) {
-          final Position doc = pending[posIDX];
-          final int delta = doc.docID - lastDocID;
-          assert doc.termFreq != 0;
-          if (doc.termFreq == 1) {
-            buffer.writeVInt((delta<<1)|1);
-          } else {
-            buffer.writeVInt(delta<<1);
-            buffer.writeVInt(doc.termFreq);
-          }
-          lastDocID = doc.docID;
-        }
-      } else if (indexOptions == IndexOptions.DOCS_ONLY) {
-        int lastDocID = 0;
-        for(int posIDX=0;posIDX<pendingCount;posIDX++) {
-          final Position doc = pending[posIDX];
-          buffer.writeVInt(doc.docID - lastDocID);
-          lastDocID = doc.docID;
-        }
-      }
-
-      final byte[] bytes = new byte[(int) buffer.getFilePointer()];
-      buffer.writeTo(bytes, 0);
-      pendingTerms.add(new PendingTerm(bytes));
-      buffer.reset();
-    }
-
-    pendingCount = 0;
-  }
-
-  @Override
-  public void close() throws IOException {
-    wrappedPostingsWriter.close();
-  }
-
-  @Override
-  public void flushTermsBlock(int start, int count) throws IOException {
-    if (DEBUG) System.out.println("PW: flushTermsBlock start=" + start + " count=" + count + " pendingTerms.size()=" + pendingTerms.size());
-    int wrappedCount = 0;
-    assert buffer.getFilePointer() == 0;
-    assert start >= count;
-
-    final int limit = pendingTerms.size() - start + count;
-
-    for(int idx=pendingTerms.size()-start; idx<limit; idx++) {
-      final PendingTerm term = pendingTerms.get(idx);
-      if (term == null) {
-        wrappedCount++;
-      } else {
-        buffer.writeVInt(term.bytes.length);
-        buffer.writeBytes(term.bytes, 0, term.bytes.length);
-      }
-    }
-
-    termsOut.writeVInt((int) buffer.getFilePointer());
-    buffer.writeTo(termsOut);
-    buffer.reset();
-
-    // TDOO: this could be somewhat costly since
-    // pendingTerms.size() could be biggish?
-    int futureWrappedCount = 0;
-    final int limit2 = pendingTerms.size();
-    for(int idx=limit;idx<limit2;idx++) {
-      if (pendingTerms.get(idx) == null) {
-        futureWrappedCount++;
-      }
-    }
-
-    // Remove the terms we just wrote:
-    pendingTerms.subList(pendingTerms.size()-start, limit).clear();
-
-    if (DEBUG) System.out.println("PW:   len=" + buffer.getFilePointer() + " fp=" + termsOut.getFilePointer() + " futureWrappedCount=" + futureWrappedCount + " wrappedCount=" + wrappedCount);
-    // TODO: can we avoid calling this if all terms
-    // were inlined...?  Eg for a "primary key" field, the
-    // wrapped codec is never invoked...
-    wrappedPostingsWriter.flushTermsBlock(futureWrappedCount+wrappedCount, wrappedCount);
-  }
-
-  // Pushes pending positions to the wrapped codec
-  private void push() throws IOException {
-    if (DEBUG) System.out.println("PW now push @ " + pendingCount + " wrapped=" + wrappedPostingsWriter);
-    assert pendingCount == pending.length;
-      
-    wrappedPostingsWriter.startTerm();
-      
-    // Flush all buffered docs
-    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      Position doc = null;
-      for(Position pos : pending) {
-        if (doc == null) {
-          doc = pos;
-          if (DEBUG) System.out.println("PW: wrapped.startDoc docID=" + doc.docID + " tf=" + doc.termFreq);
-          wrappedPostingsWriter.startDoc(doc.docID, doc.termFreq);
-        } else if (doc.docID != pos.docID) {
-          assert pos.docID > doc.docID;
-          if (DEBUG) System.out.println("PW: wrapped.finishDoc");
-          wrappedPostingsWriter.finishDoc();
-          doc = pos;
-          if (DEBUG) System.out.println("PW: wrapped.startDoc docID=" + doc.docID + " tf=" + doc.termFreq);
-          wrappedPostingsWriter.startDoc(doc.docID, doc.termFreq);
-        }
-        if (DEBUG) System.out.println("PW:   wrapped.addPos pos=" + pos.pos);
-        wrappedPostingsWriter.addPosition(pos.pos, pos.payload);
-      }
-      //wrappedPostingsWriter.finishDoc();
-    } else {
-      for(Position doc : pending) {
-        wrappedPostingsWriter.startDoc(doc.docID, indexOptions == IndexOptions.DOCS_ONLY ? 0 : doc.termFreq);
-      }
-    }
-    pendingCount = -1;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/pulsing/package.html b/lucene/src/java/org/apache/lucene/index/codecs/pulsing/package.html
deleted file mode 100644
index 4216cc6..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/pulsing/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Pulsing Codec: inlines low frequency terms' postings into terms dictionary.
-</body>
-</html>
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/sep/IntIndexInput.java b/lucene/src/java/org/apache/lucene/index/codecs/sep/IntIndexInput.java
deleted file mode 100644
index 9faef71..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/sep/IntIndexInput.java
+++ /dev/null
@@ -1,76 +0,0 @@
-package org.apache.lucene.index.codecs.sep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.util.IntsRef;
-
-/** Defines basic API for writing ints to an IndexOutput.
- *  IntBlockCodec interacts with this API. @see
- *  IntBlockReader
- *
- * @lucene.experimental */
-public abstract class IntIndexInput implements Closeable {
-
-  public abstract Reader reader() throws IOException;
-
-  public abstract void close() throws IOException;
-
-  public abstract Index index() throws IOException;
-  
-  // TODO: -- can we simplify this?
-  public abstract static class Index {
-
-    public abstract void read(DataInput indexIn, boolean absolute) throws IOException;
-
-    /** Seeks primary stream to the last read offset */
-    public abstract void seek(IntIndexInput.Reader stream) throws IOException;
-
-    public abstract void set(Index other);
-    
-    @Override
-    public abstract Object clone();
-  }
-
-  public abstract static class Reader {
-
-    /** Reads next single int */
-    public abstract int next() throws IOException;
-
-    /** Reads next chunk of ints */
-    private IntsRef bulkResult;
-
-    /** Read up to count ints. */
-    public IntsRef read(int count) throws IOException {
-      if (bulkResult == null) {
-        bulkResult = new IntsRef();
-        bulkResult.ints = new int[count];
-      } else {
-        bulkResult.grow(count);
-      }
-      for(int i=0;i<count;i++) {
-        bulkResult.ints[i] = next();
-      }
-      bulkResult.length = count;
-      return bulkResult;
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/sep/IntIndexOutput.java b/lucene/src/java/org/apache/lucene/index/codecs/sep/IntIndexOutput.java
deleted file mode 100644
index 2ec7ee1..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/sep/IntIndexOutput.java
+++ /dev/null
@@ -1,59 +0,0 @@
-package org.apache.lucene.index.codecs.sep;
-
-/**
- * LICENSED to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-// TODO: we may want tighter integration w/ IndexOutput --
-// may give better perf:
-
-import org.apache.lucene.store.IndexOutput;
-
-import java.io.IOException;
-import java.io.Closeable;
-
-/** Defines basic API for writing ints to an IndexOutput.
- *  IntBlockCodec interacts with this API. @see
- *  IntBlockReader.
- *
- * <p>NOTE: block sizes could be variable
- *
- * @lucene.experimental */
-public abstract class IntIndexOutput implements Closeable {
-
-  /** Write an int to the primary file.  The value must be
-   * >= 0.  */
-  public abstract void write(int v) throws IOException;
-
-  public abstract static class Index {
-
-    /** Internally records the current location */
-    public abstract void mark() throws IOException;
-
-    /** Copies index from other */
-    public abstract void copyFrom(Index other, boolean copyLast) throws IOException;
-
-    /** Writes "location" of current output pointer of primary
-     *  output to different output (out) */
-    public abstract void write(IndexOutput indexOut, boolean absolute) throws IOException;
-  }
-
-  /** If you are indexing the primary output file, call
-   *  this and interact with the returned IndexWriter. */
-  public abstract Index index() throws IOException;
-
-  public abstract void close() throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/sep/IntStreamFactory.java b/lucene/src/java/org/apache/lucene/index/codecs/sep/IntStreamFactory.java
deleted file mode 100644
index e1cc1f5..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/sep/IntStreamFactory.java
+++ /dev/null
@@ -1,29 +0,0 @@
-package org.apache.lucene.index.codecs.sep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-import java.io.IOException;
-
-/** @lucene.experimental */
-public abstract class IntStreamFactory {
-  public abstract IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException;
-  public abstract IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException;
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesConsumer.java b/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesConsumer.java
deleted file mode 100644
index 0d28f42..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesConsumer.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.index.codecs.sep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.codecs.DocValuesWriterBase;
-import org.apache.lucene.index.codecs.lucene40.values.Writer;
-import org.apache.lucene.store.Directory;
-
-/**
- * Implementation of PerDocConsumer that uses separate files.
- * @lucene.experimental
- */
-public class SepDocValuesConsumer extends DocValuesWriterBase {
-  private final Directory directory;
-  
-  public SepDocValuesConsumer(PerDocWriteState state) throws IOException {
-    super(state);
-    this.directory = state.directory;
-  }
-  
-  @Override
-  protected Directory getDirectory() {
-    return directory;
-  }
-
-  @SuppressWarnings("fallthrough")
-  public static void files(Directory dir, SegmentInfo segmentInfo,
-      Set<String> files) throws IOException {
-    FieldInfos fieldInfos = segmentInfo.getFieldInfos();
-    for (FieldInfo fieldInfo : fieldInfos) {
-      if (fieldInfo.hasDocValues()) {
-        String filename = docValuesId(segmentInfo.name, fieldInfo.number);
-        switch (fieldInfo.getDocValuesType()) {
-          case BYTES_FIXED_DEREF:
-          case BYTES_VAR_DEREF:
-          case BYTES_VAR_STRAIGHT:
-          case BYTES_FIXED_SORTED:
-          case BYTES_VAR_SORTED:
-            files.add(IndexFileNames.segmentFileName(filename, "",
-                Writer.INDEX_EXTENSION));
-            assert dir.fileExists(IndexFileNames.segmentFileName(filename, "",
-                Writer.INDEX_EXTENSION));
-            // until here all types use an index
-          case BYTES_FIXED_STRAIGHT:
-          case FLOAT_32:
-          case FLOAT_64:
-          case VAR_INTS:
-          case FIXED_INTS_16:
-          case FIXED_INTS_32:
-          case FIXED_INTS_64:
-          case FIXED_INTS_8:
-            files.add(IndexFileNames.segmentFileName(filename, "",
-                Writer.DATA_EXTENSION));
-            assert dir.fileExists(IndexFileNames.segmentFileName(filename, "",
-                Writer.DATA_EXTENSION));
-            break;
-          default:
-            assert false;
-        }
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesProducer.java b/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesProducer.java
deleted file mode 100644
index 349417e..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepDocValuesProducer.java
+++ /dev/null
@@ -1,54 +0,0 @@
-package org.apache.lucene.index.codecs.sep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.DocValuesReaderBase;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Implementation of PerDocProducer that uses separate files.
- * @lucene.experimental
- */
-public class SepDocValuesProducer extends DocValuesReaderBase {
-  private final TreeMap<String, DocValues> docValues;
-
-  /**
-   * Creates a new {@link SepDocValuesProducer} instance and loads all
-   * {@link DocValues} instances for this segment and codec.
-   */
-  public SepDocValuesProducer(SegmentReadState state) throws IOException {
-    docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.docCount, state.dir, state.context);
-  }
-  
-  @Override
-  protected Map<String,DocValues> docValues() {
-    return docValues;
-  }
-  
-  @Override
-  protected void closeInternal(Collection<? extends Closeable> closeables) throws IOException {
-    IOUtils.close(closeables);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepPostingsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/sep/SepPostingsReader.java
deleted file mode 100644
index 7b1750d..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepPostingsReader.java
+++ /dev/null
@@ -1,749 +0,0 @@
-package org.apache.lucene.index.codecs.sep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.codecs.BlockTermState;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-
-/** Concrete class that reads the current doc/freq/skip
- *  postings format.    
- *
- * @lucene.experimental
- */
-
-// TODO: -- should we switch "hasProx" higher up?  and
-// create two separate docs readers, one that also reads
-// prox and one that doesn't?
-
-public class SepPostingsReader extends PostingsReaderBase {
-
-  final IntIndexInput freqIn;
-  final IntIndexInput docIn;
-  final IntIndexInput posIn;
-  final IndexInput payloadIn;
-  final IndexInput skipIn;
-
-  int skipInterval;
-  int maxSkipLevels;
-  int skipMinimum;
-
-  public SepPostingsReader(Directory dir, SegmentInfo segmentInfo, IOContext context, IntStreamFactory intFactory, String segmentSuffix) throws IOException {
-    boolean success = false;
-    try {
-
-      final String docFileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.DOC_EXTENSION);
-      docIn = intFactory.openInput(dir, docFileName, context);
-
-      skipIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.SKIP_EXTENSION), context);
-
-      if (segmentInfo.getFieldInfos().hasFreq()) {
-        freqIn = intFactory.openInput(dir, IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.FREQ_EXTENSION), context);        
-      } else {
-        freqIn = null;
-      }
-      if (segmentInfo.getHasProx()) {
-        posIn = intFactory.openInput(dir, IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.POS_EXTENSION), context);
-        payloadIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.PAYLOAD_EXTENSION), context);
-      } else {
-        posIn = null;
-        payloadIn = null;
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        close();
-      }
-    }
-  }
-
-  public static void files(SegmentInfo segmentInfo, String segmentSuffix, Collection<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.DOC_EXTENSION));
-    files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.SKIP_EXTENSION));
-
-    if (segmentInfo.getFieldInfos().hasFreq()) {
-      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.FREQ_EXTENSION));
-    }
-
-    if (segmentInfo.getHasProx()) {
-      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.POS_EXTENSION));
-      files.add(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.PAYLOAD_EXTENSION));
-    }
-  }
-
-  @Override
-  public void init(IndexInput termsIn) throws IOException {
-    // Make sure we are talking to the matching past writer
-    CodecUtil.checkHeader(termsIn, SepPostingsWriter.CODEC,
-      SepPostingsWriter.VERSION_START, SepPostingsWriter.VERSION_START);
-    skipInterval = termsIn.readInt();
-    maxSkipLevels = termsIn.readInt();
-    skipMinimum = termsIn.readInt();
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      if (freqIn != null)
-        freqIn.close();
-    } finally {
-      try {
-        if (docIn != null)
-          docIn.close();
-      } finally {
-        try {
-          if (skipIn != null)
-            skipIn.close();
-        } finally {
-          try {
-            if (posIn != null) {
-              posIn.close();
-            }
-          } finally {
-            if (payloadIn != null) {
-              payloadIn.close();
-            }
-          }
-        }
-      }
-    }
-  }
-
-  private static final class SepTermState extends BlockTermState {
-    // We store only the seek point to the docs file because
-    // the rest of the info (freqIndex, posIndex, etc.) is
-    // stored in the docs file:
-    IntIndexInput.Index docIndex;
-    IntIndexInput.Index posIndex;
-    IntIndexInput.Index freqIndex;
-    long payloadFP;
-    long skipFP;
-
-    // Only used for "primary" term state; these are never
-    // copied on clone:
-    
-    // TODO: these should somehow be stored per-TermsEnum
-    // not per TermState; maybe somehow the terms dict
-    // should load/manage the byte[]/DataReader for us?
-    byte[] bytes;
-    ByteArrayDataInput bytesReader;
-
-    @Override
-    public Object clone() {
-      SepTermState other = new SepTermState();
-      other.copyFrom(this);
-      return other;
-    }
-
-    @Override
-    public void copyFrom(TermState _other) {
-      super.copyFrom(_other);
-      SepTermState other = (SepTermState) _other;
-      if (docIndex == null) {
-        docIndex = (IntIndexInput.Index) other.docIndex.clone();
-      } else {
-        docIndex.set(other.docIndex);
-      }
-      if (other.freqIndex != null) {
-        if (freqIndex == null) {
-          freqIndex = (IntIndexInput.Index) other.freqIndex.clone();
-        } else {
-          freqIndex.set(other.freqIndex);
-        }
-      } else {
-        freqIndex = null;
-      }
-      if (other.posIndex != null) {
-        if (posIndex == null) {
-          posIndex = (IntIndexInput.Index) other.posIndex.clone();
-        } else {
-          posIndex.set(other.posIndex);
-        }
-      } else {
-        posIndex = null;
-      }
-      payloadFP = other.payloadFP;
-      skipFP = other.skipFP;
-    }
-
-    @Override
-    public String toString() {
-      return super.toString() + " docIndex=" + docIndex + " freqIndex=" + freqIndex + " posIndex=" + posIndex + " payloadFP=" + payloadFP + " skipFP=" + skipFP;
-    }
-  }
-
-  @Override
-  public BlockTermState newTermState() throws IOException {
-    final SepTermState state = new SepTermState();
-    state.docIndex = docIn.index();
-    if (freqIn != null) {
-      state.freqIndex = freqIn.index();
-    }
-    if (posIn != null) {
-      state.posIndex = posIn.index();
-    }
-    return state;
-  }
-
-  @Override
-  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
-    final SepTermState termState = (SepTermState) _termState;
-    //System.out.println("SEPR: readTermsBlock termsIn.fp=" + termsIn.getFilePointer());
-    final int len = termsIn.readVInt();
-    //System.out.println("  numBytes=" + len);
-    if (termState.bytes == null) {
-      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
-      termState.bytesReader = new ByteArrayDataInput(termState.bytes);
-    } else if (termState.bytes.length < len) {
-      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
-    }
-    termState.bytesReader.reset(termState.bytes, 0, len);
-    termsIn.readBytes(termState.bytes, 0, len);
-  }
-
-  @Override
-  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
-    final SepTermState termState = (SepTermState) _termState;
-    final boolean isFirstTerm = termState.termBlockOrd == 0;
-    //System.out.println("SEPR.nextTerm termCount=" + termState.termBlockOrd + " isFirstTerm=" + isFirstTerm + " bytesReader.pos=" + termState.bytesReader.getPosition());
-    //System.out.println("  docFreq=" + termState.docFreq);
-    termState.docIndex.read(termState.bytesReader, isFirstTerm);
-    //System.out.println("  docIndex=" + termState.docIndex);
-    if (fieldInfo.indexOptions != IndexOptions.DOCS_ONLY) {
-      termState.freqIndex.read(termState.bytesReader, isFirstTerm);
-      if (fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-        //System.out.println("  freqIndex=" + termState.freqIndex);
-        termState.posIndex.read(termState.bytesReader, isFirstTerm);
-        //System.out.println("  posIndex=" + termState.posIndex);
-        if (fieldInfo.storePayloads) {
-          if (isFirstTerm) {
-            termState.payloadFP = termState.bytesReader.readVLong();
-          } else {
-            termState.payloadFP += termState.bytesReader.readVLong();
-          }
-          //System.out.println("  payloadFP=" + termState.payloadFP);
-        }
-      }
-    }
-
-    if (termState.docFreq >= skipMinimum) {
-      //System.out.println("   readSkip @ " + termState.bytesReader.getPosition());
-      if (isFirstTerm) {
-        termState.skipFP = termState.bytesReader.readVLong();
-      } else {
-        termState.skipFP += termState.bytesReader.readVLong();
-      }
-      //System.out.println("  skipFP=" + termState.skipFP);
-    } else if (isFirstTerm) {
-      termState.skipFP = 0;
-    }
-  }
-
-  @Override
-  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState _termState, Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-    if (needsFreqs && fieldInfo.indexOptions == IndexOptions.DOCS_ONLY) {
-      return null;
-    }
-    final SepTermState termState = (SepTermState) _termState;
-    SepDocsEnum docsEnum;
-    if (reuse == null || !(reuse instanceof SepDocsEnum)) {
-      docsEnum = new SepDocsEnum();
-    } else {
-      docsEnum = (SepDocsEnum) reuse;
-      if (docsEnum.startDocIn != docIn) {
-        // If you are using ParellelReader, and pass in a
-        // reused DocsAndPositionsEnum, it could have come
-        // from another reader also using sep codec
-        docsEnum = new SepDocsEnum();        
-      }
-    }
-
-    return docsEnum.init(fieldInfo, termState, liveDocs);
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState _termState, Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
-    assert fieldInfo.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-    final SepTermState termState = (SepTermState) _termState;
-    SepDocsAndPositionsEnum postingsEnum;
-    if (reuse == null || !(reuse instanceof SepDocsAndPositionsEnum)) {
-      postingsEnum = new SepDocsAndPositionsEnum();
-    } else {
-      postingsEnum = (SepDocsAndPositionsEnum) reuse;
-      if (postingsEnum.startDocIn != docIn) {
-        // If you are using ParellelReader, and pass in a
-        // reused DocsAndPositionsEnum, it could have come
-        // from another reader also using sep codec
-        postingsEnum = new SepDocsAndPositionsEnum();        
-      }
-    }
-
-    return postingsEnum.init(fieldInfo, termState, liveDocs);
-  }
-
-  class SepDocsEnum extends DocsEnum {
-    int docFreq;
-    int doc = -1;
-    int accum;
-    int count;
-    int freq;
-    long freqStart;
-
-    // TODO: -- should we do omitTF with 2 different enum classes?
-    private boolean omitTF;
-    private IndexOptions indexOptions;
-    private boolean storePayloads;
-    private Bits liveDocs;
-    private final IntIndexInput.Reader docReader;
-    private final IntIndexInput.Reader freqReader;
-    private long skipFP;
-
-    private final IntIndexInput.Index docIndex;
-    private final IntIndexInput.Index freqIndex;
-    private final IntIndexInput.Index posIndex;
-    private final IntIndexInput startDocIn;
-
-    // TODO: -- should we do hasProx with 2 different enum classes?
-
-    boolean skipped;
-    SepSkipListReader skipper;
-
-    SepDocsEnum() throws IOException {
-      startDocIn = docIn;
-      docReader = docIn.reader();
-      docIndex = docIn.index();
-      if (freqIn != null) {
-        freqReader = freqIn.reader();
-        freqIndex = freqIn.index();
-      } else {
-        freqReader = null;
-        freqIndex = null;
-      }
-      if (posIn != null) {
-        posIndex = posIn.index();                 // only init this so skipper can read it
-      } else {
-        posIndex = null;
-      }
-    }
-
-    SepDocsEnum init(FieldInfo fieldInfo, SepTermState termState, Bits liveDocs) throws IOException {
-      this.liveDocs = liveDocs;
-      this.indexOptions = fieldInfo.indexOptions;
-      omitTF = indexOptions == IndexOptions.DOCS_ONLY;
-      storePayloads = fieldInfo.storePayloads;
-
-      // TODO: can't we only do this if consumer
-      // skipped consuming the previous docs?
-      docIndex.set(termState.docIndex);
-      docIndex.seek(docReader);
-
-      if (!omitTF) {
-        freqIndex.set(termState.freqIndex);
-        freqIndex.seek(freqReader);
-      }
-
-      docFreq = termState.docFreq;
-      // NOTE: unused if docFreq < skipMinimum:
-      skipFP = termState.skipFP;
-      count = 0;
-      doc = -1;
-      accum = 0;
-      skipped = false;
-
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-
-      while(true) {
-        if (count == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-
-        count++;
-
-        // Decode next doc
-        //System.out.println("decode docDelta:");
-        accum += docReader.next();
-          
-        if (!omitTF) {
-          //System.out.println("decode freq:");
-          freq = freqReader.next();
-        }
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          break;
-        }
-      }
-      return (doc = accum);
-    }
-
-    @Override
-    public int freq() {
-      assert !omitTF;
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-
-      if ((target - skipInterval) >= doc && docFreq >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and its not too close
-
-        if (skipper == null) {
-          // This DocsEnum has never done any skipping
-          skipper = new SepSkipListReader((IndexInput) skipIn.clone(),
-                                          freqIn,
-                                          docIn,
-                                          posIn,
-                                          maxSkipLevels, skipInterval);
-
-        }
-
-        if (!skipped) {
-          // We haven't yet skipped for this posting
-          skipper.init(skipFP,
-                       docIndex,
-                       freqIndex,
-                       posIndex,
-                       0,
-                       docFreq,
-                       storePayloads);
-          skipper.setIndexOptions(indexOptions);
-
-          skipped = true;
-        }
-
-        final int newCount = skipper.skipTo(target); 
-
-        if (newCount > count) {
-
-          // Skipper did move
-          if (!omitTF) {
-            skipper.getFreqIndex().seek(freqReader);
-          }
-          skipper.getDocIndex().seek(docReader);
-          count = newCount;
-          doc = accum = skipper.getDoc();
-        }
-      }
-        
-      // Now, linear scan for the rest:
-      do {
-        if (nextDoc() == NO_MORE_DOCS) {
-          return NO_MORE_DOCS;
-        }
-      } while (target > doc);
-
-      return doc;
-    }
-  }
-
-  class SepDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    int docFreq;
-    int doc = -1;
-    int accum;
-    int count;
-    int freq;
-    long freqStart;
-
-    private boolean storePayloads;
-    private Bits liveDocs;
-    private final IntIndexInput.Reader docReader;
-    private final IntIndexInput.Reader freqReader;
-    private final IntIndexInput.Reader posReader;
-    private final IndexInput payloadIn;
-    private long skipFP;
-
-    private final IntIndexInput.Index docIndex;
-    private final IntIndexInput.Index freqIndex;
-    private final IntIndexInput.Index posIndex;
-    private final IntIndexInput startDocIn;
-
-    private long payloadFP;
-
-    private int pendingPosCount;
-    private int position;
-    private int payloadLength;
-    private long pendingPayloadBytes;
-
-    private boolean skipped;
-    private SepSkipListReader skipper;
-    private boolean payloadPending;
-    private boolean posSeekPending;
-
-    SepDocsAndPositionsEnum() throws IOException {
-      startDocIn = docIn;
-      docReader = docIn.reader();
-      docIndex = docIn.index();
-      freqReader = freqIn.reader();
-      freqIndex = freqIn.index();
-      posReader = posIn.reader();
-      posIndex = posIn.index();
-      payloadIn = (IndexInput) SepPostingsReader.this.payloadIn.clone();
-    }
-
-    SepDocsAndPositionsEnum init(FieldInfo fieldInfo, SepTermState termState, Bits liveDocs) throws IOException {
-      this.liveDocs = liveDocs;
-      storePayloads = fieldInfo.storePayloads;
-      //System.out.println("Sep D&P init");
-
-      // TODO: can't we only do this if consumer
-      // skipped consuming the previous docs?
-      docIndex.set(termState.docIndex);
-      docIndex.seek(docReader);
-      //System.out.println("  docIndex=" + docIndex);
-
-      freqIndex.set(termState.freqIndex);
-      freqIndex.seek(freqReader);
-      //System.out.println("  freqIndex=" + freqIndex);
-
-      posIndex.set(termState.posIndex);
-      //System.out.println("  posIndex=" + posIndex);
-      posSeekPending = true;
-      payloadPending = false;
-
-      payloadFP = termState.payloadFP;
-      skipFP = termState.skipFP;
-      //System.out.println("  skipFP=" + skipFP);
-
-      docFreq = termState.docFreq;
-      count = 0;
-      doc = -1;
-      accum = 0;
-      pendingPosCount = 0;
-      pendingPayloadBytes = 0;
-      skipped = false;
-
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-
-      while(true) {
-        if (count == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-
-        count++;
-
-        // TODO: maybe we should do the 1-bit trick for encoding
-        // freq=1 case?
-
-        // Decode next doc
-        //System.out.println("  sep d&p read doc");
-        accum += docReader.next();
-
-        //System.out.println("  sep d&p read freq");
-        freq = freqReader.next();
-
-        pendingPosCount += freq;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          break;
-        }
-      }
-
-      position = 0;
-      return (doc = accum);
-    }
-
-    @Override
-    public int freq() {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      //System.out.println("SepD&P advance target=" + target + " vs current=" + doc + " this=" + this);
-
-      if ((target - skipInterval) >= doc && docFreq >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and its not too close
-
-        if (skipper == null) {
-          //System.out.println("  create skipper");
-          // This DocsEnum has never done any skipping
-          skipper = new SepSkipListReader((IndexInput) skipIn.clone(),
-                                          freqIn,
-                                          docIn,
-                                          posIn,
-                                          maxSkipLevels, skipInterval);
-        }
-
-        if (!skipped) {
-          //System.out.println("  init skip data skipFP=" + skipFP);
-          // We haven't yet skipped for this posting
-          skipper.init(skipFP,
-                       docIndex,
-                       freqIndex,
-                       posIndex,
-                       payloadFP,
-                       docFreq,
-                       storePayloads);
-          skipper.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
-          skipped = true;
-        }
-        final int newCount = skipper.skipTo(target); 
-        //System.out.println("  skip newCount=" + newCount + " vs " + count);
-
-        if (newCount > count) {
-
-          // Skipper did move
-          skipper.getFreqIndex().seek(freqReader);
-          skipper.getDocIndex().seek(docReader);
-          //System.out.println("  doc seek'd to " + skipper.getDocIndex());
-          // NOTE: don't seek pos here; do it lazily
-          // instead.  Eg a PhraseQuery may skip to many
-          // docs before finally asking for positions...
-          posIndex.set(skipper.getPosIndex());
-          posSeekPending = true;
-          count = newCount;
-          doc = accum = skipper.getDoc();
-          //System.out.println("    moved to doc=" + doc);
-          //payloadIn.seek(skipper.getPayloadPointer());
-          payloadFP = skipper.getPayloadPointer();
-          pendingPosCount = 0;
-          pendingPayloadBytes = 0;
-          payloadPending = false;
-          payloadLength = skipper.getPayloadLength();
-          //System.out.println("    move payloadLen=" + payloadLength);
-        }
-      }
-        
-      // Now, linear scan for the rest:
-      do {
-        if (nextDoc() == NO_MORE_DOCS) {
-          //System.out.println("  advance nextDoc=END");
-          return NO_MORE_DOCS;
-        }
-        //System.out.println("  advance nextDoc=" + doc);
-      } while (target > doc);
-
-      //System.out.println("  return doc=" + doc);
-      return doc;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      if (posSeekPending) {
-        posIndex.seek(posReader);
-        payloadIn.seek(payloadFP);
-        posSeekPending = false;
-      }
-
-      // scan over any docs that were iterated without their
-      // positions
-      while (pendingPosCount > freq) {
-        final int code = posReader.next();
-        if (storePayloads && (code & 1) != 0) {
-          // Payload length has changed
-          payloadLength = posReader.next();
-          assert payloadLength >= 0;
-        }
-        pendingPosCount--;
-        position = 0;
-        pendingPayloadBytes += payloadLength;
-      }
-
-      final int code = posReader.next();
-      assert code >= 0;
-      if (storePayloads) {
-        if ((code & 1) != 0) {
-          // Payload length has changed
-          payloadLength = posReader.next();
-          assert payloadLength >= 0;
-        }
-        position += code >> 1;
-        pendingPayloadBytes += payloadLength;
-        payloadPending = payloadLength > 0;
-      } else {
-        position += code;
-      }
-    
-      pendingPosCount--;
-      assert pendingPosCount >= 0;
-      return position;
-    }
-
-    private BytesRef payload;
-
-    @Override
-    public BytesRef getPayload() throws IOException {
-      if (!payloadPending) {
-        throw new IOException("Either no payload exists at this term position or an attempt was made to load it more than once.");
-      }
-
-      assert pendingPayloadBytes >= payloadLength;
-
-      if (pendingPayloadBytes > payloadLength) {
-        payloadIn.seek(payloadIn.getFilePointer() + (pendingPayloadBytes - payloadLength));
-      }
-
-      if (payload == null) {
-        payload = new BytesRef();
-        payload.bytes = new byte[payloadLength];
-      } else if (payload.bytes.length < payloadLength) {
-        payload.grow(payloadLength);
-      }
-
-      payloadIn.readBytes(payload.bytes, 0, payloadLength);
-      payloadPending = false;
-      payload.length = payloadLength;
-      pendingPayloadBytes = 0;
-      return payload;
-    }
-
-    @Override
-    public boolean hasPayload() {
-      return payloadPending && payloadLength > 0;
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepPostingsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/sep/SepPostingsWriter.java
deleted file mode 100644
index e48e2d9..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepPostingsWriter.java
+++ /dev/null
@@ -1,392 +0,0 @@
-package org.apache.lucene.index.codecs.sep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.IOUtils;
-
-/** Writes frq to .frq, docs to .doc, pos to .pos, payloads
- *  to .pyl, skip data to .skp
- *
- * @lucene.experimental */
-public final class SepPostingsWriter extends PostingsWriterBase {
-  final static String CODEC = "SepPostingsWriter";
-
-  final static String DOC_EXTENSION = "doc";
-  final static String SKIP_EXTENSION = "skp";
-  final static String FREQ_EXTENSION = "frq";
-  final static String POS_EXTENSION = "pos";
-  final static String PAYLOAD_EXTENSION = "pyl";
-
-  // Increment version to change it:
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  IntIndexOutput freqOut;
-  IntIndexOutput.Index freqIndex;
-
-  IntIndexOutput posOut;
-  IntIndexOutput.Index posIndex;
-
-  IntIndexOutput docOut;
-  IntIndexOutput.Index docIndex;
-
-  IndexOutput payloadOut;
-
-  IndexOutput skipOut;
-  IndexOutput termsOut;
-
-  final SepSkipListWriter skipListWriter;
-  /** Expert: The fraction of TermDocs entries stored in skip tables,
-   * used to accelerate {@link DocsEnum#advance(int)}.  Larger values result in
-   * smaller indexes, greater acceleration, but fewer accelerable cases, while
-   * smaller values result in bigger indexes, less acceleration and more
-   * accelerable cases. More detailed experiments would be useful here. */
-  final int skipInterval;
-  static final int DEFAULT_SKIP_INTERVAL = 16;
-  
-  /**
-   * Expert: minimum docFreq to write any skip data at all
-   */
-  final int skipMinimum;
-
-  /** Expert: The maximum number of skip levels. Smaller values result in 
-   * slightly smaller indexes, but slower skipping in big posting lists.
-   */
-  final int maxSkipLevels = 10;
-
-  final int totalNumDocs;
-
-  boolean storePayloads;
-  IndexOptions indexOptions;
-
-  FieldInfo fieldInfo;
-
-  int lastPayloadLength;
-  int lastPosition;
-  long payloadStart;
-  int lastDocID;
-  int df;
-
-  // Holds pending byte[] blob for the current terms block
-  private final RAMOutputStream indexBytesWriter = new RAMOutputStream();
-
-  public SepPostingsWriter(SegmentWriteState state, IntStreamFactory factory) throws IOException {
-    this(state, factory, DEFAULT_SKIP_INTERVAL);
-  }
-
-  public SepPostingsWriter(SegmentWriteState state, IntStreamFactory factory, int skipInterval) throws IOException {
-    freqOut = null;
-    freqIndex = null;
-    posOut = null;
-    posIndex = null;
-    payloadOut = null;
-    boolean success = false;
-    try {
-      this.skipInterval = skipInterval;
-      this.skipMinimum = skipInterval; /* set to the same for now */
-      final String docFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, DOC_EXTENSION);
-      docOut = factory.createOutput(state.directory, docFileName, state.context);
-      docIndex = docOut.index();
-      
-      if (state.fieldInfos.hasFreq()) {
-        final String frqFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, FREQ_EXTENSION);
-        freqOut = factory.createOutput(state.directory, frqFileName, state.context);
-        freqIndex = freqOut.index();
-      }
-
-      if (state.fieldInfos.hasProx()) {      
-        final String posFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, POS_EXTENSION);
-        posOut = factory.createOutput(state.directory, posFileName, state.context);
-        posIndex = posOut.index();
-        
-        // TODO: -- only if at least one field stores payloads?
-        final String payloadFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, PAYLOAD_EXTENSION);
-        payloadOut = state.directory.createOutput(payloadFileName, state.context);
-      }
-      
-      final String skipFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, SKIP_EXTENSION);
-      skipOut = state.directory.createOutput(skipFileName, state.context);
-      
-      totalNumDocs = state.numDocs;
-      
-      skipListWriter = new SepSkipListWriter(skipInterval,
-          maxSkipLevels,
-          state.numDocs,
-          freqOut, docOut,
-          posOut, payloadOut);
-      
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docOut, skipOut, freqOut, posOut, payloadOut);
-      }
-    }
-  }
-
-  @Override
-  public void start(IndexOutput termsOut) throws IOException {
-    this.termsOut = termsOut;
-    CodecUtil.writeHeader(termsOut, CODEC, VERSION_CURRENT);
-    // TODO: -- just ask skipper to "start" here
-    termsOut.writeInt(skipInterval);                // write skipInterval
-    termsOut.writeInt(maxSkipLevels);               // write maxSkipLevels
-    termsOut.writeInt(skipMinimum);                 // write skipMinimum
-  }
-
-  @Override
-  public void startTerm() throws IOException {
-    docIndex.mark();
-    //System.out.println("SEPW: startTerm docIndex=" + docIndex);
-
-    if (indexOptions != IndexOptions.DOCS_ONLY) {
-      freqIndex.mark();
-    }
-    
-    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      posIndex.mark();
-      payloadStart = payloadOut.getFilePointer();
-      lastPayloadLength = -1;
-    }
-
-    skipListWriter.resetSkip(docIndex, freqIndex, posIndex);
-  }
-
-  // Currently, this instance is re-used across fields, so
-  // our parent calls setField whenever the field changes
-  @Override
-  public void setField(FieldInfo fieldInfo) {
-    this.fieldInfo = fieldInfo;
-    this.indexOptions = fieldInfo.indexOptions;
-    skipListWriter.setIndexOptions(indexOptions);
-    storePayloads = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS && fieldInfo.storePayloads;
-  }
-
-  /** Adds a new doc in this term.  If this returns null
-   *  then we just skip consuming positions/payloads. */
-  @Override
-  public void startDoc(int docID, int termDocFreq) throws IOException {
-
-    final int delta = docID - lastDocID;
-    //System.out.println("SEPW: startDoc: write doc=" + docID + " delta=" + delta + " out.fp=" + docOut);
-
-    if (docID < 0 || (df > 0 && delta <= 0)) {
-      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " ) (docOut: " + docOut + ")");
-    }
-
-    if ((++df % skipInterval) == 0) {
-      // TODO: -- awkward we have to make these two
-      // separate calls to skipper
-      //System.out.println("    buffer skip lastDocID=" + lastDocID);
-      skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength);
-      skipListWriter.bufferSkip(df);
-    }
-
-    lastDocID = docID;
-    docOut.write(delta);
-    if (indexOptions != IndexOptions.DOCS_ONLY) {
-      //System.out.println("    sepw startDoc: write freq=" + termDocFreq);
-      freqOut.write(termDocFreq);
-    }
-  }
-
-  /** Add a new position & payload */
-  @Override
-  public void addPosition(int position, BytesRef payload) throws IOException {
-    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-
-    final int delta = position - lastPosition;
-    assert delta >= 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
-    lastPosition = position;
-
-    if (storePayloads) {
-      final int payloadLength = payload == null ? 0 : payload.length;
-      if (payloadLength != lastPayloadLength) {
-        lastPayloadLength = payloadLength;
-        // TODO: explore whether we get better compression
-        // by not storing payloadLength into prox stream?
-        posOut.write((delta<<1)|1);
-        posOut.write(payloadLength);
-      } else {
-        posOut.write(delta << 1);
-      }
-
-      if (payloadLength > 0) {
-        payloadOut.writeBytes(payload.bytes, payload.offset, payloadLength);
-      }
-    } else {
-      posOut.write(delta);
-    }
-
-    lastPosition = position;
-  }
-
-  /** Called when we are done adding positions & payloads */
-  @Override
-  public void finishDoc() {       
-    lastPosition = 0;
-  }
-
-  private static class PendingTerm {
-    public final IntIndexOutput.Index docIndex;
-    public final IntIndexOutput.Index freqIndex;
-    public final IntIndexOutput.Index posIndex;
-    public final long payloadFP;
-    public final long skipFP;
-
-    public PendingTerm(IntIndexOutput.Index docIndex, IntIndexOutput.Index freqIndex, IntIndexOutput.Index posIndex, long payloadFP, long skipFP) {
-      this.docIndex = docIndex;
-      this.freqIndex = freqIndex;
-      this.posIndex = posIndex;
-      this.payloadFP = payloadFP;
-      this.skipFP = skipFP;
-    }
-  }
-
-  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
-
-  /** Called when we are done adding docs to this term */
-  @Override
-  public void finishTerm(TermStats stats) throws IOException {
-    // TODO: -- wasteful we are counting this in two places?
-    assert stats.docFreq > 0;
-    assert stats.docFreq == df;
-
-    final IntIndexOutput.Index docIndexCopy = docOut.index();
-    docIndexCopy.copyFrom(docIndex, false);
-
-    final IntIndexOutput.Index freqIndexCopy;
-    final IntIndexOutput.Index posIndexCopy;
-    if (indexOptions != IndexOptions.DOCS_ONLY) {
-      freqIndexCopy = freqOut.index();
-      freqIndexCopy.copyFrom(freqIndex, false);
-      if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-        posIndexCopy = posOut.index();
-        posIndexCopy.copyFrom(posIndex, false);
-      } else {
-        posIndexCopy = null;
-      }
-    } else {
-      freqIndexCopy = null;
-      posIndexCopy = null;
-    }
-
-    final long skipFP;
-    if (df >= skipMinimum) {
-      skipFP = skipOut.getFilePointer();
-      //System.out.println("  skipFP=" + skipFP);
-      skipListWriter.writeSkip(skipOut);
-      //System.out.println("    numBytes=" + (skipOut.getFilePointer()-skipFP));
-    } else {
-      skipFP = -1;
-    }
-
-    lastDocID = 0;
-    df = 0;
-
-    pendingTerms.add(new PendingTerm(docIndexCopy,
-                                     freqIndexCopy,
-                                     posIndexCopy,
-                                     payloadStart,
-                                     skipFP));
-  }
-
-  @Override
-  public void flushTermsBlock(int start, int count) throws IOException {
-    //System.out.println("SEPW: flushTermsBlock: start=" + start + " count=" + count + " pendingTerms.size()=" + pendingTerms.size() + " termsOut.fp=" + termsOut.getFilePointer());
-    assert indexBytesWriter.getFilePointer() == 0;
-    final int absStart = pendingTerms.size() - start;
-    final List<PendingTerm> slice = pendingTerms.subList(absStart, absStart+count);
-
-    long lastPayloadFP = 0;
-    long lastSkipFP = 0;
-
-    if (count == 0) {
-      termsOut.writeByte((byte) 0);
-      return;
-    }
-
-    final PendingTerm firstTerm = slice.get(0);
-    final IntIndexOutput.Index docIndexFlush = firstTerm.docIndex;
-    final IntIndexOutput.Index freqIndexFlush = firstTerm.freqIndex;
-    final IntIndexOutput.Index posIndexFlush = firstTerm.posIndex;
-
-    for(int idx=0;idx<slice.size();idx++) {
-      final boolean isFirstTerm = idx == 0;
-      final PendingTerm t = slice.get(idx);
-      //System.out.println("  write idx=" + idx + " docIndex=" + t.docIndex);
-      docIndexFlush.copyFrom(t.docIndex, false);
-      docIndexFlush.write(indexBytesWriter, isFirstTerm);
-      if (indexOptions != IndexOptions.DOCS_ONLY) {
-        freqIndexFlush.copyFrom(t.freqIndex, false);
-        freqIndexFlush.write(indexBytesWriter, isFirstTerm);
-        //System.out.println("    freqIndex=" + t.freqIndex);
-        if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-          posIndexFlush.copyFrom(t.posIndex, false);
-          posIndexFlush.write(indexBytesWriter, isFirstTerm);
-          //System.out.println("    posIndex=" + t.posIndex);
-          if (storePayloads) {
-            //System.out.println("    payloadFP=" + t.payloadFP);
-            if (isFirstTerm) {
-              indexBytesWriter.writeVLong(t.payloadFP);
-            } else {
-              indexBytesWriter.writeVLong(t.payloadFP - lastPayloadFP);
-            }
-            lastPayloadFP = t.payloadFP;
-          }
-        }
-      }
-
-      if (t.skipFP != -1) {
-        if (isFirstTerm) {
-          indexBytesWriter.writeVLong(t.skipFP);
-        } else {
-          indexBytesWriter.writeVLong(t.skipFP - lastSkipFP);
-        }
-        lastSkipFP = t.skipFP;
-        //System.out.println("    skipFP=" + t.skipFP);
-      }
-    }
-
-    //System.out.println("  numBytes=" + indexBytesWriter.getFilePointer());
-    termsOut.writeVLong((int) indexBytesWriter.getFilePointer());
-    indexBytesWriter.writeTo(termsOut);
-    indexBytesWriter.reset();
-    slice.clear();
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.close(docOut, skipOut, freqOut, posOut, payloadOut);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepSkipListReader.java b/lucene/src/java/org/apache/lucene/index/codecs/sep/SepSkipListReader.java
deleted file mode 100644
index 355b2b6..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepSkipListReader.java
+++ /dev/null
@@ -1,209 +0,0 @@
-package org.apache.lucene.index.codecs.sep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.codecs.MultiLevelSkipListReader;
-
-/**
- * Implements the skip list reader for the default posting list format
- * that stores positions and payloads.
- *
- * @lucene.experimental
- */
-
-// TODO: rewrite this as recursive classes?
-class SepSkipListReader extends MultiLevelSkipListReader {
-  private boolean currentFieldStoresPayloads;
-  private IntIndexInput.Index freqIndex[];
-  private IntIndexInput.Index docIndex[];
-  private IntIndexInput.Index posIndex[];
-  private long payloadPointer[];
-  private int payloadLength[];
-
-  private final IntIndexInput.Index lastFreqIndex;
-  private final IntIndexInput.Index lastDocIndex;
-  // TODO: -- make private again
-  final IntIndexInput.Index lastPosIndex;
-  
-  private long lastPayloadPointer;
-  private int lastPayloadLength;
-                           
-  SepSkipListReader(IndexInput skipStream,
-                    IntIndexInput freqIn,
-                    IntIndexInput docIn,
-                    IntIndexInput posIn,
-                    int maxSkipLevels,
-                    int skipInterval)
-    throws IOException {
-    super(skipStream, maxSkipLevels, skipInterval);
-    if (freqIn != null) {
-      freqIndex = new IntIndexInput.Index[maxSkipLevels];
-    }
-    docIndex = new IntIndexInput.Index[maxSkipLevels];
-    if (posIn != null) {
-      posIndex = new IntIndexInput.Index[maxNumberOfSkipLevels];
-    }
-    for(int i=0;i<maxSkipLevels;i++) {
-      if (freqIn != null) {
-        freqIndex[i] = freqIn.index();
-      }
-      docIndex[i] = docIn.index();
-      if (posIn != null) {
-        posIndex[i] = posIn.index();
-      }
-    }
-    payloadPointer = new long[maxSkipLevels];
-    payloadLength = new int[maxSkipLevels];
-
-    if (freqIn != null) {
-      lastFreqIndex = freqIn.index();
-    } else {
-      lastFreqIndex = null;
-    }
-    lastDocIndex = docIn.index();
-    if (posIn != null) {
-      lastPosIndex = posIn.index();
-    } else {
-      lastPosIndex = null;
-    }
-  }
-  
-  IndexOptions indexOptions;
-
-  void setIndexOptions(IndexOptions v) {
-    indexOptions = v;
-  }
-
-  void init(long skipPointer,
-            IntIndexInput.Index docBaseIndex,
-            IntIndexInput.Index freqBaseIndex,
-            IntIndexInput.Index posBaseIndex,
-            long payloadBasePointer,
-            int df,
-            boolean storesPayloads) {
-
-    super.init(skipPointer, df);
-    this.currentFieldStoresPayloads = storesPayloads;
-
-    lastPayloadPointer = payloadBasePointer;
-
-    for(int i=0;i<maxNumberOfSkipLevels;i++) {
-      docIndex[i].set(docBaseIndex);
-      if (freqIndex != null) {
-        freqIndex[i].set(freqBaseIndex);
-      }
-      if (posBaseIndex != null) {
-        posIndex[i].set(posBaseIndex);
-      }
-    }
-    Arrays.fill(payloadPointer, payloadBasePointer);
-    Arrays.fill(payloadLength, 0);
-  }
-
-  long getPayloadPointer() {
-    return lastPayloadPointer;
-  }
-  
-  /** Returns the payload length of the payload stored just before 
-   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
-   * has skipped.  */
-  int getPayloadLength() {
-    return lastPayloadLength;
-  }
-  
-  @Override
-  protected void seekChild(int level) throws IOException {
-    super.seekChild(level);
-    payloadPointer[level] = lastPayloadPointer;
-    payloadLength[level] = lastPayloadLength;
-  }
-  
-  @Override
-  protected void setLastSkipData(int level) {
-    super.setLastSkipData(level);
-
-    lastPayloadPointer = payloadPointer[level];
-    lastPayloadLength = payloadLength[level];
-    if (freqIndex != null) {
-      lastFreqIndex.set(freqIndex[level]);
-    }
-    lastDocIndex.set(docIndex[level]);
-    if (lastPosIndex != null) {
-      lastPosIndex.set(posIndex[level]);
-    }
-
-    if (level > 0) {
-      if (freqIndex != null) {
-        freqIndex[level-1].set(freqIndex[level]);
-      }
-      docIndex[level-1].set(docIndex[level]);
-      if (posIndex != null) {
-        posIndex[level-1].set(posIndex[level]);
-      }
-    }
-  }
-
-  IntIndexInput.Index getFreqIndex() {
-    return lastFreqIndex;
-  }
-
-  IntIndexInput.Index getPosIndex() {
-    return lastPosIndex;
-  }
-
-  IntIndexInput.Index getDocIndex() {
-    return lastDocIndex;
-  }
-
-  @Override
-  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
-    int delta;
-    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !currentFieldStoresPayloads;
-    if (currentFieldStoresPayloads) {
-      // the current field stores payloads.
-      // if the doc delta is odd then we have
-      // to read the current payload length
-      // because it differs from the length of the
-      // previous payload
-      delta = skipStream.readVInt();
-      if ((delta & 1) != 0) {
-        payloadLength[level] = skipStream.readVInt();
-      }
-      delta >>>= 1;
-    } else {
-      delta = skipStream.readVInt();
-    }
-    if (indexOptions != IndexOptions.DOCS_ONLY) {
-      freqIndex[level].read(skipStream, false);
-    }
-    docIndex[level].read(skipStream, false);
-    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      posIndex[level].read(skipStream, false);
-      if (currentFieldStoresPayloads) {
-        payloadPointer[level] += skipStream.readVInt();
-      }
-    }
-    
-    return delta;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepSkipListWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/sep/SepSkipListWriter.java
deleted file mode 100644
index e299cbd..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepSkipListWriter.java
+++ /dev/null
@@ -1,200 +0,0 @@
-package org.apache.lucene.index.codecs.sep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.codecs.MultiLevelSkipListWriter;
-
-// TODO: -- skip data should somehow be more local to the
-// particular stream (doc, freq, pos, payload)
-
-/**
- * Implements the skip list writer for the default posting list format
- * that stores positions and payloads.
- *
- * @lucene.experimental
- */
-class SepSkipListWriter extends MultiLevelSkipListWriter {
-  private int[] lastSkipDoc;
-  private int[] lastSkipPayloadLength;
-  private long[] lastSkipPayloadPointer;
-
-  private IntIndexOutput.Index[] docIndex;
-  private IntIndexOutput.Index[] freqIndex;
-  private IntIndexOutput.Index[] posIndex;
-  
-  private IntIndexOutput freqOutput;
-  // TODO: -- private again
-  IntIndexOutput posOutput;
-  // TODO: -- private again
-  IndexOutput payloadOutput;
-
-  private int curDoc;
-  private boolean curStorePayloads;
-  private int curPayloadLength;
-  private long curPayloadPointer;
-  
-  SepSkipListWriter(int skipInterval, int numberOfSkipLevels, int docCount,
-                    IntIndexOutput freqOutput,
-                    IntIndexOutput docOutput,
-                    IntIndexOutput posOutput,
-                    IndexOutput payloadOutput)
-    throws IOException {
-    super(skipInterval, numberOfSkipLevels, docCount);
-
-    this.freqOutput = freqOutput;
-    this.posOutput = posOutput;
-    this.payloadOutput = payloadOutput;
-    
-    lastSkipDoc = new int[numberOfSkipLevels];
-    lastSkipPayloadLength = new int[numberOfSkipLevels];
-    // TODO: -- also cutover normal IndexOutput to use getIndex()?
-    lastSkipPayloadPointer = new long[numberOfSkipLevels];
-
-    freqIndex = new IntIndexOutput.Index[numberOfSkipLevels];
-    docIndex = new IntIndexOutput.Index[numberOfSkipLevels];
-    posIndex = new IntIndexOutput.Index[numberOfSkipLevels];
-
-    for(int i=0;i<numberOfSkipLevels;i++) {
-      if (freqOutput != null) {
-        freqIndex[i] = freqOutput.index();
-      }
-      docIndex[i] = docOutput.index();
-      if (posOutput != null) {
-        posIndex[i] = posOutput.index();
-      }
-    }
-  }
-
-  IndexOptions indexOptions;
-
-  void setIndexOptions(IndexOptions v) {
-    indexOptions = v;
-  }
-
-  void setPosOutput(IntIndexOutput posOutput) throws IOException {
-    this.posOutput = posOutput;
-    for(int i=0;i<numberOfSkipLevels;i++) {
-      posIndex[i] = posOutput.index();
-    }
-  }
-
-  void setPayloadOutput(IndexOutput payloadOutput) {
-    this.payloadOutput = payloadOutput;
-  }
-
-  /**
-   * Sets the values for the current skip data. 
-   */
-  // Called @ every index interval (every 128th (by default)
-  // doc)
-  void setSkipData(int doc, boolean storePayloads, int payloadLength) {
-    this.curDoc = doc;
-    this.curStorePayloads = storePayloads;
-    this.curPayloadLength = payloadLength;
-    if (payloadOutput != null) {
-      this.curPayloadPointer = payloadOutput.getFilePointer();
-    }
-  }
-
-  // Called @ start of new term
-  protected void resetSkip(IntIndexOutput.Index topDocIndex, IntIndexOutput.Index topFreqIndex, IntIndexOutput.Index topPosIndex)
-    throws IOException {
-    super.resetSkip();
-
-    Arrays.fill(lastSkipDoc, 0);
-    Arrays.fill(lastSkipPayloadLength, -1);  // we don't have to write the first length in the skip list
-    for(int i=0;i<numberOfSkipLevels;i++) {
-      docIndex[i].copyFrom(topDocIndex, true);
-      if (freqOutput != null) {
-        freqIndex[i].copyFrom(topFreqIndex, true);
-      }
-      if (posOutput != null) {
-        posIndex[i].copyFrom(topPosIndex, true);
-      }
-    }
-    if (payloadOutput != null) {
-      Arrays.fill(lastSkipPayloadPointer, payloadOutput.getFilePointer());
-    }
-  }
-  
-  @Override
-  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
-    // To efficiently store payloads in the posting lists we do not store the length of
-    // every payload. Instead we omit the length for a payload if the previous payload had
-    // the same length.
-    // However, in order to support skipping the payload length at every skip point must be known.
-    // So we use the same length encoding that we use for the posting lists for the skip data as well:
-    // Case 1: current field does not store payloads
-    //           SkipDatum                 --> DocSkip, FreqSkip, ProxSkip
-    //           DocSkip,FreqSkip,ProxSkip --> VInt
-    //           DocSkip records the document number before every SkipInterval th  document in TermFreqs. 
-    //           Document numbers are represented as differences from the previous value in the sequence.
-    // Case 2: current field stores payloads
-    //           SkipDatum                 --> DocSkip, PayloadLength?, FreqSkip,ProxSkip
-    //           DocSkip,FreqSkip,ProxSkip --> VInt
-    //           PayloadLength             --> VInt    
-    //         In this case DocSkip/2 is the difference between
-    //         the current and the previous value. If DocSkip
-    //         is odd, then a PayloadLength encoded as VInt follows,
-    //         if DocSkip is even, then it is assumed that the
-    //         current payload length equals the length at the previous
-    //         skip point
-
-    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !curStorePayloads;
-
-    if (curStorePayloads) {
-      int delta = curDoc - lastSkipDoc[level];
-      if (curPayloadLength == lastSkipPayloadLength[level]) {
-        // the current payload length equals the length at the previous skip point,
-        // so we don't store the length again
-        skipBuffer.writeVInt(delta << 1);
-      } else {
-        // the payload length is different from the previous one. We shift the DocSkip, 
-        // set the lowest bit and store the current payload length as VInt.
-        skipBuffer.writeVInt(delta << 1 | 1);
-        skipBuffer.writeVInt(curPayloadLength);
-        lastSkipPayloadLength[level] = curPayloadLength;
-      }
-    } else {
-      // current field does not store payloads
-      skipBuffer.writeVInt(curDoc - lastSkipDoc[level]);
-    }
-
-    if (indexOptions != IndexOptions.DOCS_ONLY) {
-      freqIndex[level].mark();
-      freqIndex[level].write(skipBuffer, false);
-    }
-    docIndex[level].mark();
-    docIndex[level].write(skipBuffer, false);
-    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      posIndex[level].mark();
-      posIndex[level].write(skipBuffer, false);
-      if (curStorePayloads) {
-        skipBuffer.writeVInt((int) (curPayloadPointer - lastSkipPayloadPointer[level]));
-      }
-    }
-
-    lastSkipDoc[level] = curDoc;
-    lastSkipPayloadPointer[level] = curPayloadPointer;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/sep/package.html b/lucene/src/java/org/apache/lucene/index/codecs/sep/package.html
deleted file mode 100644
index b51d910..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/sep/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Sep: base support for separate files (doc,frq,pos,skp,pyl)
-</body>
-</html>
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextCodec.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextCodec.java
deleted file mode 100644
index 15136e6..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextCodec.java
+++ /dev/null
@@ -1,85 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.DocValuesFormat;
-import org.apache.lucene.index.codecs.FieldInfosFormat;
-import org.apache.lucene.index.codecs.NormsFormat;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.SegmentInfosFormat;
-import org.apache.lucene.index.codecs.StoredFieldsFormat;
-import org.apache.lucene.index.codecs.TermVectorsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40DocValuesFormat;
-
-/**
- * plain text index format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public final class SimpleTextCodec extends Codec {
-  private final PostingsFormat postings = new SimpleTextPostingsFormat();
-  private final StoredFieldsFormat storedFields = new SimpleTextStoredFieldsFormat();
-  private final SegmentInfosFormat segmentInfos = new SimpleTextSegmentInfosFormat();
-  private final FieldInfosFormat fieldInfosFormat = new SimpleTextFieldInfosFormat();
-  private final TermVectorsFormat vectorsFormat = new SimpleTextTermVectorsFormat();
-  // TODO: need a plain-text impl
-  private final DocValuesFormat docValues = new Lucene40DocValuesFormat();
-  // TODO: need a plain-text impl (using the above)
-  private final NormsFormat normsFormat = new SimpleTextNormsFormat();
-  
-  public SimpleTextCodec() {
-    super("SimpleText");
-  }
-  
-  @Override
-  public PostingsFormat postingsFormat() {
-    return postings;
-  }
-
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValues;
-  }
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return storedFields;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-
-  @Override
-  public SegmentInfosFormat segmentInfosFormat() {
-    return segmentInfos;
-  }
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosFormat.java
deleted file mode 100644
index 731c8ec..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosFormat.java
+++ /dev/null
@@ -1,53 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.codecs.FieldInfosFormat;
-import org.apache.lucene.index.codecs.FieldInfosReader;
-import org.apache.lucene.index.codecs.FieldInfosWriter;
-import org.apache.lucene.store.Directory;
-
-/**
- * plaintext field infos format
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextFieldInfosFormat extends FieldInfosFormat {
-  private final FieldInfosReader reader = new SimpleTextFieldInfosReader();
-  private final FieldInfosWriter writer = new SimpleTextFieldInfosWriter();
-
-  @Override
-  public FieldInfosReader getFieldInfosReader() throws IOException {
-    return reader;
-  }
-
-  @Override
-  public FieldInfosWriter getFieldInfosWriter() throws IOException {
-    return writer;
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    SimpleTextFieldInfosReader.files(dir, info, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosReader.java
deleted file mode 100644
index 6f97f0c..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosReader.java
+++ /dev/null
@@ -1,139 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.codecs.FieldInfosReader;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-import static org.apache.lucene.index.codecs.simpletext.SimpleTextFieldInfosWriter.*;
-
-/**
- * reads plaintext field infos files
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextFieldInfosReader extends FieldInfosReader {
-
-  @Override
-  public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
-    IndexInput input = directory.openInput(fileName, iocontext);
-    BytesRef scratch = new BytesRef();
-    
-    boolean hasVectors = false;
-    boolean hasFreq = false;
-    boolean hasProx = false;
-    
-    try {
-      
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, NUMFIELDS);
-      final int size = Integer.parseInt(readString(NUMFIELDS.length, scratch));
-      FieldInfo infos[] = new FieldInfo[size];
-
-      for (int i = 0; i < size; i++) {
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, NAME);
-        String name = readString(NAME.length, scratch);
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, NUMBER);
-        int fieldNumber = Integer.parseInt(readString(NUMBER.length, scratch));
-
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, ISINDEXED);
-        boolean isIndexed = Boolean.parseBoolean(readString(ISINDEXED.length, scratch));
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, STORETV);
-        boolean storeTermVector = Boolean.parseBoolean(readString(STORETV.length, scratch));
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, STORETVPOS);
-        boolean storePositionsWithTermVector = Boolean.parseBoolean(readString(STORETVPOS.length, scratch));
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, STORETVOFF);
-        boolean storeOffsetWithTermVector = Boolean.parseBoolean(readString(STORETVOFF.length, scratch));
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, PAYLOADS);
-        boolean storePayloads = Boolean.parseBoolean(readString(PAYLOADS.length, scratch));
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, NORMS);
-        boolean omitNorms = !Boolean.parseBoolean(readString(NORMS.length, scratch));
-
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, DOCVALUES);
-        String dvType = readString(DOCVALUES.length, scratch);
-        final DocValues.Type docValuesType;
-        
-        if ("false".equals(dvType)) {
-          docValuesType = null;
-        } else {
-          docValuesType = DocValues.Type.valueOf(dvType);
-        }
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, INDEXOPTIONS);
-        IndexOptions indexOptions = IndexOptions.valueOf(readString(INDEXOPTIONS.length, scratch));
-
-        hasVectors |= storeTermVector;
-        hasProx |= isIndexed && indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-        hasFreq |= isIndexed && indexOptions != IndexOptions.DOCS_ONLY;
-        
-        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          storePositionsWithTermVector, storeOffsetWithTermVector, 
-          omitNorms, storePayloads, indexOptions, docValuesType);
-      }
-
-      if (input.getFilePointer() != input.length()) {
-        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
-      }
-      
-      return new FieldInfos(infos, hasFreq, hasProx, hasVectors);
-    } finally {
-      input.close();
-    }
-  }
-  
-  private String readString(int offset, BytesRef scratch) {
-    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
-  }
-  
-  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(info.name, "", FIELD_INFOS_EXTENSION));
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosWriter.java
deleted file mode 100644
index e6f5fab..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldInfosWriter.java
+++ /dev/null
@@ -1,115 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.codecs.FieldInfosWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * writes plaintext field infos files
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextFieldInfosWriter extends FieldInfosWriter {
-  
-  /** Extension of field infos */
-  static final String FIELD_INFOS_EXTENSION = "inf";
-  
-  static final BytesRef NUMFIELDS       =  new BytesRef("number of fields ");
-  static final BytesRef NAME            =  new BytesRef("  name ");
-  static final BytesRef NUMBER          =  new BytesRef("  number ");
-  static final BytesRef ISINDEXED       =  new BytesRef("  indexed ");
-  static final BytesRef STORETV         =  new BytesRef("  term vectors ");
-  static final BytesRef STORETVPOS      =  new BytesRef("  term vector positions ");
-  static final BytesRef STORETVOFF      =  new BytesRef("  term vector offsets ");
-  static final BytesRef PAYLOADS        =  new BytesRef("  payloads ");
-  static final BytesRef NORMS           =  new BytesRef("  norms ");
-  static final BytesRef DOCVALUES       =  new BytesRef("  doc values ");
-  static final BytesRef INDEXOPTIONS    =  new BytesRef("  index options ");
-  
-  @Override
-  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
-    IndexOutput out = directory.createOutput(fileName, context);
-    BytesRef scratch = new BytesRef();
-    try {
-      SimpleTextUtil.write(out, NUMFIELDS);
-      SimpleTextUtil.write(out, Integer.toString(infos.size()), scratch);
-      SimpleTextUtil.writeNewline(out);
-      
-      for (FieldInfo fi : infos) {
-        assert fi.indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !fi.storePayloads;
-
-        SimpleTextUtil.write(out, NAME);
-        SimpleTextUtil.write(out, fi.name, scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, NUMBER);
-        SimpleTextUtil.write(out, Integer.toString(fi.number), scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, ISINDEXED);
-        SimpleTextUtil.write(out, Boolean.toString(fi.isIndexed), scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, STORETV);
-        SimpleTextUtil.write(out, Boolean.toString(fi.storeTermVector), scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, STORETVPOS);
-        SimpleTextUtil.write(out, Boolean.toString(fi.storePositionWithTermVector), scratch);
-        SimpleTextUtil.writeNewline(out);
-
-        SimpleTextUtil.write(out, STORETVOFF);
-        SimpleTextUtil.write(out, Boolean.toString(fi.storeOffsetWithTermVector), scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, PAYLOADS);
-        SimpleTextUtil.write(out, Boolean.toString(fi.storePayloads), scratch);
-        SimpleTextUtil.writeNewline(out);
-               
-        SimpleTextUtil.write(out, NORMS);
-        SimpleTextUtil.write(out, Boolean.toString(!fi.omitNorms), scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, DOCVALUES);
-        if (!fi.hasDocValues()) {
-          SimpleTextUtil.write(out, "false", scratch);
-        } else {
-          SimpleTextUtil.write(out, fi.getDocValuesType().toString(), scratch);
-        }
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, INDEXOPTIONS);
-        SimpleTextUtil.write(out, fi.indexOptions.toString(), scratch);
-        SimpleTextUtil.writeNewline(out);
-      }
-    } finally {
-      out.close();
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java
deleted file mode 100644
index 958eeeb..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java
+++ /dev/null
@@ -1,596 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.OpenBitSet;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.UnicodeUtil;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PairOutputs;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-
-class SimpleTextFieldsReader extends FieldsProducer {
-
-  private final IndexInput in;
-  private final FieldInfos fieldInfos;
-
-  final static BytesRef END     = SimpleTextFieldsWriter.END;
-  final static BytesRef FIELD   = SimpleTextFieldsWriter.FIELD;
-  final static BytesRef TERM    = SimpleTextFieldsWriter.TERM;
-  final static BytesRef DOC     = SimpleTextFieldsWriter.DOC;
-  final static BytesRef FREQ    = SimpleTextFieldsWriter.FREQ;
-  final static BytesRef POS     = SimpleTextFieldsWriter.POS;
-  final static BytesRef PAYLOAD = SimpleTextFieldsWriter.PAYLOAD;
-
-  public SimpleTextFieldsReader(SegmentReadState state) throws IOException {
-    in = state.dir.openInput(SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix), state.context);
-   
-    fieldInfos = state.fieldInfos;
-  }
-
-  private class SimpleTextFieldsEnum extends FieldsEnum {
-    private final IndexInput in;
-    private final BytesRef scratch = new BytesRef(10);
-    private String current;
-
-    public SimpleTextFieldsEnum() {
-      this.in = (IndexInput) SimpleTextFieldsReader.this.in.clone();
-    }
-
-    @Override
-    public String next() throws IOException {
-      while(true) {
-        SimpleTextUtil.readLine(in, scratch);
-        if (scratch.equals(END)) {
-          current = null;
-          return null;
-        }
-        if (StringHelper.startsWith(scratch, FIELD)) {
-          return current = new String(scratch.bytes, scratch.offset + FIELD.length, scratch.length - FIELD.length, "UTF-8");
-        }
-      }
-    }
-
-    @Override
-    public Terms terms() throws IOException {
-      return SimpleTextFieldsReader.this.terms(current);
-    }
-  }
-
-  private class SimpleTextTermsEnum extends TermsEnum {
-    private final IndexOptions indexOptions;
-    private int docFreq;
-    private long totalTermFreq;
-    private long docsStart;
-    private boolean ended;
-    private final BytesRefFSTEnum<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fstEnum;
-
-    public SimpleTextTermsEnum(FST<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fst, IndexOptions indexOptions) throws IOException {
-      this.indexOptions = indexOptions;
-      fstEnum = new BytesRefFSTEnum<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>>(fst);
-    }
-
-    @Override
-    public boolean seekExact(BytesRef text, boolean useCache /* ignored */) throws IOException {
-
-      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.seekExact(text);
-      if (result != null) {
-        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
-        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
-        docsStart = pair1.output1;
-        docFreq = pair2.output1.intValue();
-        totalTermFreq = pair2.output2;
-        return true;
-      } else {
-        return false;
-      }
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
-
-      //System.out.println("seek to text=" + text.utf8ToString());
-      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.seekCeil(text);
-      if (result == null) {
-        //System.out.println("  end");
-        return SeekStatus.END;
-      } else {
-        //System.out.println("  got text=" + term.utf8ToString());
-        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
-        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
-        docsStart = pair1.output1;
-        docFreq = pair2.output1.intValue();
-        totalTermFreq = pair2.output2;
-
-        if (result.input.equals(text)) {
-          //System.out.println("  match docsStart=" + docsStart);
-          return SeekStatus.FOUND;
-        } else {
-          //System.out.println("  not match docsStart=" + docsStart);
-          return SeekStatus.NOT_FOUND;
-        }
-      }
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      assert !ended;
-      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.next();
-      if (result != null) {
-        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
-        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
-        docsStart = pair1.output1;
-        docFreq = pair2.output1.intValue();
-        totalTermFreq = pair2.output2;
-        return result.input;
-      } else {
-        return null;
-      }
-    }
-
-    @Override
-    public BytesRef term() {
-      return fstEnum.current().input;
-    }
-
-    @Override
-    public long ord() throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public void seekExact(long ord) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public int docFreq() {
-      return docFreq;
-    }
-
-    @Override
-    public long totalTermFreq() {
-      return indexOptions == IndexOptions.DOCS_ONLY ? -1 : totalTermFreq;
-    }
- 
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-      if (needsFreqs && indexOptions == IndexOptions.DOCS_ONLY) {
-        return null;
-      }
-      SimpleTextDocsEnum docsEnum;
-      if (reuse != null && reuse instanceof SimpleTextDocsEnum && ((SimpleTextDocsEnum) reuse).canReuse(SimpleTextFieldsReader.this.in)) {
-        docsEnum = (SimpleTextDocsEnum) reuse;
-      } else {
-        docsEnum = new SimpleTextDocsEnum();
-      }
-      return docsEnum.reset(docsStart, liveDocs, !needsFreqs);
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
-      if (indexOptions != IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-        return null;
-      }
-
-      SimpleTextDocsAndPositionsEnum docsAndPositionsEnum;
-      if (reuse != null && reuse instanceof SimpleTextDocsAndPositionsEnum && ((SimpleTextDocsAndPositionsEnum) reuse).canReuse(SimpleTextFieldsReader.this.in)) {
-        docsAndPositionsEnum = (SimpleTextDocsAndPositionsEnum) reuse;
-      } else {
-        docsAndPositionsEnum = new SimpleTextDocsAndPositionsEnum();
-      } 
-      return docsAndPositionsEnum.reset(docsStart, liveDocs);
-    }
-    
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-  }
-
-  private class SimpleTextDocsEnum extends DocsEnum {
-    private final IndexInput inStart;
-    private final IndexInput in;
-    private boolean omitTF;
-    private int docID = -1;
-    private int tf;
-    private Bits liveDocs;
-    private final BytesRef scratch = new BytesRef(10);
-    private final CharsRef scratchUTF16 = new CharsRef(10);
-    
-    public SimpleTextDocsEnum() {
-      this.inStart = SimpleTextFieldsReader.this.in;
-      this.in = (IndexInput) this.inStart.clone();
-    }
-
-    public boolean canReuse(IndexInput in) {
-      return in == inStart;
-    }
-
-    public SimpleTextDocsEnum reset(long fp, Bits liveDocs, boolean omitTF) throws IOException {
-      this.liveDocs = liveDocs;
-      in.seek(fp);
-      this.omitTF = omitTF;
-      docID = -1;
-      return this;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int freq() {
-      assert !omitTF;
-      return tf;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      if (docID == NO_MORE_DOCS) {
-        return docID;
-      }
-      boolean first = true;
-      int termFreq = 0;
-      while(true) {
-        final long lineStart = in.getFilePointer();
-        SimpleTextUtil.readLine(in, scratch);
-        if (StringHelper.startsWith(scratch, DOC)) {
-          if (!first && (liveDocs == null || liveDocs.get(docID))) {
-            in.seek(lineStart);
-            if (!omitTF) {
-              tf = termFreq;
-            }
-            return docID;
-          }
-          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
-          docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-          termFreq = 0;
-          first = false;
-        } else if (StringHelper.startsWith(scratch, FREQ)) {
-          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+FREQ.length, scratch.length-FREQ.length, scratchUTF16);
-          termFreq = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-        } else if (StringHelper.startsWith(scratch, POS)) {
-          // skip termFreq++;
-        } else if (StringHelper.startsWith(scratch, PAYLOAD)) {
-          // skip
-        } else {
-          assert StringHelper.startsWith(scratch, TERM) || StringHelper.startsWith(scratch, FIELD) || StringHelper.startsWith(scratch, END): "scratch=" + scratch.utf8ToString();
-          if (!first && (liveDocs == null || liveDocs.get(docID))) {
-            in.seek(lineStart);
-            if (!omitTF) {
-              tf = termFreq;
-            }
-            return docID;
-          }
-          return docID = NO_MORE_DOCS;
-        }
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      // Naive -- better to index skip data
-      while(nextDoc() < target);
-      return docID;
-    }
-  }
-
-  private class SimpleTextDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private final IndexInput inStart;
-    private final IndexInput in;
-    private int docID = -1;
-    private int tf;
-    private Bits liveDocs;
-    private final BytesRef scratch = new BytesRef(10);
-    private final BytesRef scratch2 = new BytesRef(10);
-    private final CharsRef scratchUTF16 = new CharsRef(10);
-    private final CharsRef scratchUTF16_2 = new CharsRef(10);
-    private BytesRef payload;
-    private long nextDocStart;
-
-    public SimpleTextDocsAndPositionsEnum() {
-      this.inStart = SimpleTextFieldsReader.this.in;
-      this.in = (IndexInput) inStart.clone();
-    }
-
-    public boolean canReuse(IndexInput in) {
-      return in == inStart;
-    }
-
-    public SimpleTextDocsAndPositionsEnum reset(long fp, Bits liveDocs) {
-      this.liveDocs = liveDocs;
-      nextDocStart = fp;
-      docID = -1;
-      return this;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int freq() {
-      return tf;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      boolean first = true;
-      in.seek(nextDocStart);
-      long posStart = 0;
-      while(true) {
-        final long lineStart = in.getFilePointer();
-        SimpleTextUtil.readLine(in, scratch);
-        if (StringHelper.startsWith(scratch, DOC)) {
-          if (!first && (liveDocs == null || liveDocs.get(docID))) {
-            nextDocStart = lineStart;
-            in.seek(posStart);
-            return docID;
-          }
-          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
-          docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-          tf = 0;
-          first = false;
-        } else if (StringHelper.startsWith(scratch, FREQ)) {
-          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+FREQ.length, scratch.length-FREQ.length, scratchUTF16);
-          tf = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-          posStart = in.getFilePointer();
-        } else if (StringHelper.startsWith(scratch, POS)) {
-          // skip
-        } else if (StringHelper.startsWith(scratch, PAYLOAD)) {
-          // skip
-        } else {
-          assert StringHelper.startsWith(scratch, TERM) || StringHelper.startsWith(scratch, FIELD) || StringHelper.startsWith(scratch, END);
-          if (!first && (liveDocs == null || liveDocs.get(docID))) {
-            nextDocStart = lineStart;
-            in.seek(posStart);
-            return docID;
-          }
-          return docID = NO_MORE_DOCS;
-        }
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      // Naive -- better to index skip data
-      while(nextDoc() < target);
-      return docID;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      SimpleTextUtil.readLine(in, scratch);
-      assert StringHelper.startsWith(scratch, POS): "got line=" + scratch.utf8ToString();
-      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+POS.length, scratch.length-POS.length, scratchUTF16_2);
-      final int pos = ArrayUtil.parseInt(scratchUTF16_2.chars, 0, scratchUTF16_2.length);
-      final long fp = in.getFilePointer();
-      SimpleTextUtil.readLine(in, scratch);
-      if (StringHelper.startsWith(scratch, PAYLOAD)) {
-        final int len = scratch.length - PAYLOAD.length;
-        if (scratch2.bytes.length < len) {
-          scratch2.grow(len);
-        }
-        System.arraycopy(scratch.bytes, PAYLOAD.length, scratch2.bytes, 0, len);
-        scratch2.length = len;
-        payload = scratch2;
-      } else {
-        payload = null;
-        in.seek(fp);
-      }
-      return pos;
-    }
-
-    @Override
-    public BytesRef getPayload() {
-      // Some tests rely on only being able to retrieve the
-      // payload once
-      try {
-        return payload;
-      } finally {
-        payload = null;
-      }
-    }
-
-    @Override
-    public boolean hasPayload() {
-      return payload != null;
-    }
-  }
-
-  static class TermData {
-    public long docsStart;
-    public int docFreq;
-
-    public TermData(long docsStart, int docFreq) {
-      this.docsStart = docsStart;
-      this.docFreq = docFreq;
-    }
-  }
-
-  private class SimpleTextTerms extends Terms {
-    private final long termsStart;
-    private final IndexOptions indexOptions;
-    private long sumTotalTermFreq;
-    private long sumDocFreq;
-    private int docCount;
-    private FST<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fst;
-    private int termCount;
-    private final BytesRef scratch = new BytesRef(10);
-    private final CharsRef scratchUTF16 = new CharsRef(10);
-
-    public SimpleTextTerms(String field, long termsStart) throws IOException {
-      this.termsStart = termsStart;
-      indexOptions = fieldInfos.fieldInfo(field).indexOptions;
-      loadTerms();
-    }
-
-    private void loadTerms() throws IOException {
-      PositiveIntOutputs posIntOutputs = PositiveIntOutputs.getSingleton(false);
-      final Builder<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> b;
-      b = new Builder<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>>(FST.INPUT_TYPE.BYTE1,
-                                                                          new PairOutputs<Long,PairOutputs.Pair<Long,Long>>(posIntOutputs,
-                                                                                                                            new PairOutputs<Long,Long>(posIntOutputs, posIntOutputs)));
-      IndexInput in = (IndexInput) SimpleTextFieldsReader.this.in.clone();
-      in.seek(termsStart);
-      final BytesRef lastTerm = new BytesRef(10);
-      long lastDocsStart = -1;
-      int docFreq = 0;
-      long totalTermFreq = 0;
-      OpenBitSet visitedDocs = new OpenBitSet();
-      while(true) {
-        SimpleTextUtil.readLine(in, scratch);
-        if (scratch.equals(END) || StringHelper.startsWith(scratch, FIELD)) {
-          if (lastDocsStart != -1) {
-            b.add(lastTerm, new PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>(lastDocsStart,
-                                                                                   new PairOutputs.Pair<Long,Long>((long) docFreq,
-                                                                                                                   posIntOutputs.get(totalTermFreq))));
-            sumTotalTermFreq += totalTermFreq;
-          }
-          break;
-        } else if (StringHelper.startsWith(scratch, DOC)) {
-          docFreq++;
-          sumDocFreq++;
-          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
-          int docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-          visitedDocs.set(docID);
-        } else if (StringHelper.startsWith(scratch, POS)) {
-          totalTermFreq++;
-        } else if (StringHelper.startsWith(scratch, TERM)) {
-          if (lastDocsStart != -1) {
-            b.add(lastTerm, new PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>(lastDocsStart,
-                                                                                   new PairOutputs.Pair<Long,Long>((long) docFreq,
-                                                                                                                   posIntOutputs.get(totalTermFreq))));
-          }
-          lastDocsStart = in.getFilePointer();
-          final int len = scratch.length - TERM.length;
-          if (len > lastTerm.length) {
-            lastTerm.grow(len);
-          }
-          System.arraycopy(scratch.bytes, TERM.length, lastTerm.bytes, 0, len);
-          lastTerm.length = len;
-          docFreq = 0;
-          sumTotalTermFreq += totalTermFreq;
-          totalTermFreq = 0;
-          termCount++;
-        }
-      }
-      docCount = (int) visitedDocs.cardinality();
-      fst = b.finish();
-      /*
-      PrintStream ps = new PrintStream("out.dot");
-      fst.toDot(ps);
-      ps.close();
-      System.out.println("SAVED out.dot");
-      */
-      //System.out.println("FST " + fst.sizeInBytes());
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      if (fst != null) {
-        return new SimpleTextTermsEnum(fst, indexOptions);
-      } else {
-        return TermsEnum.EMPTY;
-      }
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public long getUniqueTermCount() {
-      return (long) termCount;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return indexOptions == IndexOptions.DOCS_ONLY ? -1 : sumTotalTermFreq;
-    }
-
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return sumDocFreq;
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return docCount;
-    }
-  }
-
-  @Override
-  public FieldsEnum iterator() throws IOException {
-    return new SimpleTextFieldsEnum();
-  }
-
-  private final Map<String,Terms> termsCache = new HashMap<String,Terms>();
-
-  @Override
-  synchronized public Terms terms(String field) throws IOException {
-    Terms terms = termsCache.get(field);
-    if (terms == null) {
-      SimpleTextFieldsEnum fe = (SimpleTextFieldsEnum) iterator();
-      String fieldUpto;
-      while((fieldUpto = fe.next()) != null) {
-        if (fieldUpto.equals(field)) {
-          terms = new SimpleTextTerms(field, fe.in.getFilePointer());
-          break;
-        }
-      }
-      termsCache.put(field, terms);
-    }
-    return terms;
-  }
-
-  @Override
-  public int getUniqueFieldCount() {
-    return -1;
-  }
-
-  @Override
-  public void close() throws IOException {
-    in.close();
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsWriter.java
deleted file mode 100644
index dab584a..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsWriter.java
+++ /dev/null
@@ -1,161 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.TermsConsumer;
-import org.apache.lucene.index.codecs.PostingsConsumer;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-
-import java.io.IOException;
-import java.util.Comparator;
-
-class SimpleTextFieldsWriter extends FieldsConsumer {
-  
-  private final IndexOutput out;
-  private final BytesRef scratch = new BytesRef(10);
-
-  final static BytesRef END     = new BytesRef("END");
-  final static BytesRef FIELD   = new BytesRef("field ");
-  final static BytesRef TERM    = new BytesRef("  term ");
-  final static BytesRef DOC     = new BytesRef("    doc ");
-  final static BytesRef FREQ    = new BytesRef("      freq ");
-  final static BytesRef POS     = new BytesRef("      pos ");
-  final static BytesRef PAYLOAD = new BytesRef("        payload ");
-
-  public SimpleTextFieldsWriter(SegmentWriteState state) throws IOException {
-    final String fileName = SimpleTextPostingsFormat.getPostingsFileName(state.segmentName, state.segmentSuffix);
-    out = state.directory.createOutput(fileName, state.context);
-  }
-
-  private void write(String s) throws IOException {
-    SimpleTextUtil.write(out, s, scratch);
-  }
-
-  private void write(BytesRef b) throws IOException {
-    SimpleTextUtil.write(out, b);
-  }
-
-  private void newline() throws IOException {
-    SimpleTextUtil.writeNewline(out);
-  }
-
-  @Override
-  public TermsConsumer addField(FieldInfo field) throws IOException {
-    write(FIELD);
-    write(field.name);
-    newline();
-    return new SimpleTextTermsWriter(field);
-  }
-
-  private class SimpleTextTermsWriter extends TermsConsumer {
-    private final SimpleTextPostingsWriter postingsWriter;
-    
-    public SimpleTextTermsWriter(FieldInfo field) {
-      postingsWriter = new SimpleTextPostingsWriter(field);
-    }
-
-    @Override
-    public PostingsConsumer startTerm(BytesRef term) throws IOException {
-      return postingsWriter.reset(term);
-    }
-
-    @Override
-    public void finishTerm(BytesRef term, TermStats stats) throws IOException {
-    }
-
-    @Override
-    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-  }
-
-  private class SimpleTextPostingsWriter extends PostingsConsumer {
-    private BytesRef term;
-    private boolean wroteTerm;
-    private IndexOptions indexOptions;
-
-    public SimpleTextPostingsWriter(FieldInfo field) {
-      this.indexOptions = field.indexOptions;
-    }
-
-    @Override
-    public void startDoc(int docID, int termDocFreq) throws IOException {
-      if (!wroteTerm) {
-        // we lazily do this, in case the term had zero docs
-        write(TERM);
-        write(term);
-        newline();
-        wroteTerm = true;
-      }
-
-      write(DOC);
-      write(Integer.toString(docID));
-      newline();
-      if (indexOptions != IndexOptions.DOCS_ONLY) {
-        write(FREQ);
-        write(Integer.toString(termDocFreq));
-        newline();
-      }
-    }
-    
-    
-
-    public PostingsConsumer reset(BytesRef term) {
-      this.term = term;
-      wroteTerm = false;
-      return this;
-    }
-
-    @Override
-    public void addPosition(int position, BytesRef payload) throws IOException {
-      write(POS);
-      write(Integer.toString(position));
-      newline();
-      if (payload != null && payload.length > 0) {
-        assert payload.length != 0;
-        write(PAYLOAD);
-        write(payload);
-        newline();
-      }
-    }
-
-    @Override
-    public void finishDoc() {
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      write(END);
-      newline();
-    } finally {
-      out.close();
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextNormsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextNormsFormat.java
deleted file mode 100644
index acf0cb3..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextNormsFormat.java
+++ /dev/null
@@ -1,54 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.NormsFormat;
-import org.apache.lucene.index.codecs.NormsReader;
-import org.apache.lucene.index.codecs.NormsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * plain-text norms format
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextNormsFormat extends NormsFormat {
-
-  @Override
-  public NormsReader normsReader(Directory dir, SegmentInfo info, FieldInfos fields, IOContext context, Directory separateNormsDir) throws IOException {
-    return new SimpleTextNormsReader(dir, info, fields, context);
-  }
-
-  @Override
-  public NormsWriter normsWriter(SegmentWriteState state) throws IOException {
-    return new SimpleTextNormsWriter(state.directory, state.segmentName, state.context);
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    SimpleTextNormsReader.files(dir, info, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextNormsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextNormsReader.java
deleted file mode 100644
index db00b10..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextNormsReader.java
+++ /dev/null
@@ -1,106 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.codecs.NormsReader;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-import static org.apache.lucene.index.codecs.simpletext.SimpleTextNormsWriter.*;
-
-/**
- * Reads plain-text norms
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextNormsReader extends NormsReader {
-  private Map<String,byte[]> norms = new HashMap<String,byte[]>();
-  
-  public SimpleTextNormsReader(Directory directory, SegmentInfo si, FieldInfos fields, IOContext context) throws IOException {
-    if (fields.hasNorms()) {
-      readNorms(directory.openInput(IndexFileNames.segmentFileName(si.name, "", NORMS_EXTENSION), context), si.docCount);
-    }
-  }
-  
-  // we read in all the norms up front into a hashmap
-  private void readNorms(IndexInput in, int maxDoc) throws IOException {
-    BytesRef scratch = new BytesRef();
-    boolean success = false;
-    try {
-      SimpleTextUtil.readLine(in, scratch);
-      while (!scratch.equals(END)) {
-        assert StringHelper.startsWith(scratch, FIELD);
-        String fieldName = readString(FIELD.length, scratch);
-        byte bytes[] = new byte[maxDoc];
-        for (int i = 0; i < bytes.length; i++) {
-          SimpleTextUtil.readLine(in, scratch);
-          assert StringHelper.startsWith(scratch, DOC);
-          SimpleTextUtil.readLine(in, scratch);
-          assert StringHelper.startsWith(scratch, NORM);
-          bytes[i] = scratch.bytes[scratch.offset + NORM.length];
-        }
-        norms.put(fieldName, bytes);
-        SimpleTextUtil.readLine(in, scratch);
-        assert StringHelper.startsWith(scratch, FIELD) || scratch.equals(END);
-      }
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(in);
-      } else {
-        IOUtils.closeWhileHandlingException(in);
-      }
-    }
-  }
-  
-  @Override
-  public byte[] norms(String name) throws IOException {
-    return norms.get(name);
-  }
-  
-  @Override
-  public void close() throws IOException {
-    norms = null;
-  }
-  
-  static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    // TODO: This is what SI always did... but we can do this cleaner?
-    // like first FI that has norms but doesn't have separate norms?
-    final String normsFileName = IndexFileNames.segmentFileName(info.name, "", SimpleTextNormsWriter.NORMS_EXTENSION);
-    if (dir.fileExists(normsFileName)) {
-      files.add(normsFileName);
-    }
-  }
-  
-  private String readString(int offset, BytesRef scratch) {
-    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextNormsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextNormsWriter.java
deleted file mode 100644
index 842357a..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextNormsWriter.java
+++ /dev/null
@@ -1,114 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.codecs.NormsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Writes plain-text norms
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextNormsWriter extends NormsWriter {
-  private IndexOutput out;
-  private int docid = 0;
-    
-  /** Extension of norms file */
-  static final String NORMS_EXTENSION = "len";
-  
-  private final BytesRef scratch = new BytesRef();
-  
-  final static BytesRef END     = new BytesRef("END");
-  final static BytesRef FIELD   = new BytesRef("field ");
-  final static BytesRef DOC     = new BytesRef("  doc ");
-  final static BytesRef NORM    = new BytesRef("    norm ");
-  
-  public SimpleTextNormsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    final String normsFileName = IndexFileNames.segmentFileName(segment, "", NORMS_EXTENSION);
-    out = directory.createOutput(normsFileName, context);
-  }
-
-  @Override
-  public void startField(FieldInfo info) throws IOException {
-    assert info.omitNorms == false;
-    docid = 0;
-    write(FIELD);
-    write(info.name);
-    newLine();
-  }
-    
-  @Override
-  public void writeNorm(byte norm) throws IOException {
-    write(DOC);
-    write(Integer.toString(docid));
-    newLine();
-    
-    write(NORM);
-    write(norm);
-    newLine();
-    docid++;
-  }
-    
-  @Override
-  public void finish(int numDocs) throws IOException {
-    if (docid != numDocs) {
-      throw new RuntimeException("mergeNorms produced an invalid result: docCount is " + numDocs
-          + " but only saw " + docid + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
-    }
-    write(END);
-    newLine();
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(out);
-    } finally {
-      out = null;
-    }
-  }
-  
-  private void write(String s) throws IOException {
-    SimpleTextUtil.write(out, s, scratch);
-  }
-  
-  private void write(BytesRef bytes) throws IOException {
-    SimpleTextUtil.write(out, bytes);
-  }
-  
-  private void write(byte b) throws IOException {
-    scratch.grow(1);
-    scratch.bytes[scratch.offset] = b;
-    scratch.length = 1;
-    SimpleTextUtil.write(out, scratch);
-  }
-  
-  private void newLine() throws IOException {
-    SimpleTextUtil.writeNewline(out);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextPostingsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextPostingsFormat.java
deleted file mode 100644
index 6b47bf3..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextPostingsFormat.java
+++ /dev/null
@@ -1,67 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.store.Directory;
-
-/** For debugging, curiosity, transparency only!!  Do not
- *  use this codec in production.
- *
- *  <p>This codec stores all postings data in a single
- *  human-readable text file (_N.pst).  You can view this in
- *  any text editor, and even edit it to alter your index.
- *
- *  @lucene.experimental */
-public class SimpleTextPostingsFormat extends PostingsFormat {
-  
-  public SimpleTextPostingsFormat() {
-    super("SimpleText");
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new SimpleTextFieldsWriter(state);
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new SimpleTextFieldsReader(state);
-  }
-
-  /** Extension of freq postings file */
-  static final String POSTINGS_EXTENSION = "pst";
-
-  static String getPostingsFileName(String segment, String segmentSuffix) {
-    return IndexFileNames.segmentFileName(segment, segmentSuffix, POSTINGS_EXTENSION);
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    files.add(getPostingsFileName(segmentInfo.name, segmentSuffix));
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosFormat.java
deleted file mode 100644
index 9ed197a..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosFormat.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.codecs.SegmentInfosFormat;
-import org.apache.lucene.index.codecs.SegmentInfosReader;
-import org.apache.lucene.index.codecs.SegmentInfosWriter;
-
-/**
- * plain text segments file format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextSegmentInfosFormat extends SegmentInfosFormat {
-  private final SegmentInfosReader reader = new SimpleTextSegmentInfosReader();
-  private final SegmentInfosWriter writer = new SimpleTextSegmentInfosWriter();
-  
-  @Override
-  public SegmentInfosReader getSegmentInfosReader() {
-    return reader;
-  }
-
-  @Override
-  public SegmentInfosWriter getSegmentInfosWriter() {
-    return writer;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosReader.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosReader.java
deleted file mode 100644
index 917d698..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosReader.java
+++ /dev/null
@@ -1,190 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.SegmentInfosReader;
-import org.apache.lucene.store.ChecksumIndexInput;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-import static org.apache.lucene.index.codecs.simpletext.SimpleTextSegmentInfosWriter.*;
-
-/**
- * reads plaintext segments files
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextSegmentInfosReader extends SegmentInfosReader {
-
-  @Override
-  public void read(Directory directory, String segmentsFileName, ChecksumIndexInput input, SegmentInfos infos, IOContext context) throws IOException {
-    final BytesRef scratch = new BytesRef();
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, VERSION);
-    infos.version = Long.parseLong(readString(VERSION.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, COUNTER);
-    infos.counter = Integer.parseInt(readString(COUNTER.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, NUM_USERDATA);
-    int numUserData = Integer.parseInt(readString(NUM_USERDATA.length, scratch));
-    infos.userData = new HashMap<String,String>();
-
-    for (int i = 0; i < numUserData; i++) {
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, USERDATA_KEY);
-      String key = readString(USERDATA_KEY.length, scratch);
-      
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, USERDATA_VALUE);
-      String value = readString(USERDATA_VALUE.length, scratch);
-      infos.userData.put(key, value);
-    }
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, NUM_SEGMENTS);
-    int numSegments = Integer.parseInt(readString(NUM_SEGMENTS.length, scratch));
-    
-    for (int i = 0; i < numSegments; i++) {
-      infos.add(readSegmentInfo(directory, input, scratch));
-    }
-  }
-  
-  public SegmentInfo readSegmentInfo(Directory directory, DataInput input, BytesRef scratch) throws IOException {
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_NAME);
-    final String name = readString(SI_NAME.length, scratch);
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_CODEC);
-    final Codec codec = Codec.forName(readString(SI_CODEC.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_VERSION);
-    final String version = readString(SI_VERSION.length, scratch);
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DOCCOUNT);
-    final int docCount = Integer.parseInt(readString(SI_DOCCOUNT.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DELCOUNT);
-    final int delCount = Integer.parseInt(readString(SI_DELCOUNT.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_HASPROX);
-    final int hasProx = readTernary(SI_HASPROX.length, scratch);
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_HASVECTORS);
-    final int hasVectors = readTernary(SI_HASVECTORS.length, scratch);
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_USECOMPOUND);
-    final boolean isCompoundFile = Boolean.parseBoolean(readString(SI_USECOMPOUND.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DSOFFSET);
-    final int dsOffset = Integer.parseInt(readString(SI_DSOFFSET.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DSSEGMENT);
-    final String dsSegment = readString(SI_DSSEGMENT.length, scratch);
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DSCOMPOUND);
-    final boolean dsCompoundFile = Boolean.parseBoolean(readString(SI_DSCOMPOUND.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_DELGEN);
-    final long delGen = Long.parseLong(readString(SI_DELGEN.length, scratch));
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_NUM_NORMGEN);
-    final int numNormGen = Integer.parseInt(readString(SI_NUM_NORMGEN.length, scratch));
-    final Map<Integer,Long> normGen;
-    if (numNormGen == 0) {
-      normGen = null;
-    } else {
-      normGen = new HashMap<Integer,Long>();
-      for (int i = 0; i < numNormGen; i++) {
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, SI_NORMGEN_KEY);
-        int key = Integer.parseInt(readString(SI_NORMGEN_KEY.length, scratch));
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, SI_NORMGEN_VALUE);
-        long value = Long.parseLong(readString(SI_NORMGEN_VALUE.length, scratch));
-        normGen.put(key, value);
-      }
-    }
-    
-    SimpleTextUtil.readLine(input, scratch);
-    assert StringHelper.startsWith(scratch, SI_NUM_DIAG);
-    int numDiag = Integer.parseInt(readString(SI_NUM_DIAG.length, scratch));
-    Map<String,String> diagnostics = new HashMap<String,String>();
-
-    for (int i = 0; i < numDiag; i++) {
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, SI_DIAG_KEY);
-      String key = readString(SI_DIAG_KEY.length, scratch);
-      
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, SI_DIAG_VALUE);
-      String value = readString(SI_DIAG_VALUE.length, scratch);
-      diagnostics.put(key, value);
-    }
-    
-    return new SegmentInfo(directory, version, name, docCount, delGen, dsOffset,
-        dsSegment, dsCompoundFile, normGen, isCompoundFile,
-        delCount, hasProx, codec, diagnostics, hasVectors);
-  }
-  
-  private String readString(int offset, BytesRef scratch) {
-    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
-  }
-  
-  private int readTernary(int offset, BytesRef scratch) throws IOException {
-    String s = readString(offset, scratch);
-    if ("check fieldinfo".equals(s)) {
-      return SegmentInfo.CHECK_FIELDINFO;
-    } else if ("true".equals(s)) {
-      return SegmentInfo.YES;
-    } else if ("false".equals(s)) {
-      return 0;
-    } else {
-      throw new CorruptIndexException("invalid ternary value: " + s);
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosWriter.java
deleted file mode 100644
index b2da338..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextSegmentInfosWriter.java
+++ /dev/null
@@ -1,234 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.index.codecs.SegmentInfosWriter;
-import org.apache.lucene.store.ChecksumIndexOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FlushInfo;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * writes plaintext segments files
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextSegmentInfosWriter extends SegmentInfosWriter {
-
-  final static BytesRef VERSION             = new BytesRef("version ");
-  final static BytesRef COUNTER             = new BytesRef("counter ");
-  final static BytesRef NUM_USERDATA        = new BytesRef("user data entries ");
-  final static BytesRef USERDATA_KEY        = new BytesRef("  key ");
-  final static BytesRef USERDATA_VALUE      = new BytesRef("  value ");
-  final static BytesRef NUM_SEGMENTS        = new BytesRef("number of segments ");
-  final static BytesRef SI_NAME             = new BytesRef("  name ");
-  final static BytesRef SI_CODEC            = new BytesRef("    codec ");
-  final static BytesRef SI_VERSION          = new BytesRef("    version ");
-  final static BytesRef SI_DOCCOUNT         = new BytesRef("    number of documents ");
-  final static BytesRef SI_DELCOUNT         = new BytesRef("    number of deletions ");
-  final static BytesRef SI_HASPROX          = new BytesRef("    has prox ");
-  final static BytesRef SI_HASVECTORS       = new BytesRef("    has vectors ");
-  final static BytesRef SI_USECOMPOUND      = new BytesRef("    uses compound file ");
-  final static BytesRef SI_DSOFFSET         = new BytesRef("    docstore offset ");
-  final static BytesRef SI_DSSEGMENT        = new BytesRef("    docstore segment ");
-  final static BytesRef SI_DSCOMPOUND       = new BytesRef("    docstore is compound file ");
-  final static BytesRef SI_DELGEN           = new BytesRef("    deletion generation ");
-  final static BytesRef SI_NUM_NORMGEN      = new BytesRef("    norms generations ");
-  final static BytesRef SI_NORMGEN_KEY      = new BytesRef("      key ");
-  final static BytesRef SI_NORMGEN_VALUE    = new BytesRef("      value ");
-  final static BytesRef SI_NUM_DIAG         = new BytesRef("    diagnostics ");
-  final static BytesRef SI_DIAG_KEY         = new BytesRef("      key ");
-  final static BytesRef SI_DIAG_VALUE       = new BytesRef("      value ");
-  
-  @Override
-  public IndexOutput writeInfos(Directory dir, String segmentsFileName, String codecID, SegmentInfos infos, IOContext context) throws IOException {
-    BytesRef scratch = new BytesRef();
-    IndexOutput out = new ChecksumIndexOutput(dir.createOutput(segmentsFileName, new IOContext(new FlushInfo(infos.size(), infos.totalDocCount()))));
-    boolean success = false;
-    try {
-      // required preamble:
-      out.writeInt(SegmentInfos.FORMAT_CURRENT); // write FORMAT
-      out.writeString(codecID); // write codecID
-      // end preamble
-      
-      // version
-      SimpleTextUtil.write(out, VERSION);
-      SimpleTextUtil.write(out, Long.toString(infos.version), scratch);
-      SimpleTextUtil.writeNewline(out);
-
-      // counter
-      SimpleTextUtil.write(out, COUNTER);
-      SimpleTextUtil.write(out, Integer.toString(infos.counter), scratch);
-      SimpleTextUtil.writeNewline(out);
-
-      // user data
-      int numUserDataEntries = infos.getUserData() == null ? 0 : infos.getUserData().size();
-      SimpleTextUtil.write(out, NUM_USERDATA);
-      SimpleTextUtil.write(out, Integer.toString(numUserDataEntries), scratch);
-      SimpleTextUtil.writeNewline(out);
-      
-      if (numUserDataEntries > 0) {
-        for (Map.Entry<String,String> userEntry : infos.getUserData().entrySet()) {
-          SimpleTextUtil.write(out, USERDATA_KEY);
-          SimpleTextUtil.write(out, userEntry.getKey(), scratch);
-          SimpleTextUtil.writeNewline(out);
-          
-          SimpleTextUtil.write(out, USERDATA_VALUE);
-          SimpleTextUtil.write(out, userEntry.getValue(), scratch);
-          SimpleTextUtil.writeNewline(out);
-        }
-      }
-      
-      // infos size
-      SimpleTextUtil.write(out, NUM_SEGMENTS);
-      SimpleTextUtil.write(out, Integer.toString(infos.size()), scratch);
-      SimpleTextUtil.writeNewline(out);
-
-      for (SegmentInfo si : infos) {
-        writeInfo(out, si);
-      } 
-
-      success = true;
-      return out;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  private void writeInfo(IndexOutput output, SegmentInfo si) throws IOException {
-    assert si.getDelCount() <= si.docCount: "delCount=" + si.getDelCount() + " docCount=" + si.docCount + " segment=" + si.name;
-    BytesRef scratch = new BytesRef();
-    
-    SimpleTextUtil.write(output, SI_NAME);
-    SimpleTextUtil.write(output, si.name, scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_CODEC);
-    SimpleTextUtil.write(output, si.getCodec().getName(), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_VERSION);
-    SimpleTextUtil.write(output, si.getVersion(), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DOCCOUNT);
-    SimpleTextUtil.write(output, Integer.toString(si.docCount), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DELCOUNT);
-    SimpleTextUtil.write(output, Integer.toString(si.getDelCount()), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_HASPROX);
-    switch(si.getHasProxInternal()) {
-      case SegmentInfo.YES: SimpleTextUtil.write(output, "true", scratch); break;
-      case SegmentInfo.CHECK_FIELDINFO: SimpleTextUtil.write(output, "check fieldinfo", scratch); break;
-      // its "NO" if its 'anything but YES'... such as 0
-      default: SimpleTextUtil.write(output, "false", scratch); break;
-    }
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_HASVECTORS);
-    switch(si.getHasVectorsInternal()) {
-      case SegmentInfo.YES: SimpleTextUtil.write(output, "true", scratch); break;
-      case SegmentInfo.CHECK_FIELDINFO: SimpleTextUtil.write(output, "check fieldinfo", scratch); break;
-      // its "NO" if its 'anything but YES'... such as 0
-      default: SimpleTextUtil.write(output, "false", scratch); break;
-    }
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_USECOMPOUND);
-    SimpleTextUtil.write(output, Boolean.toString(si.getUseCompoundFile()), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DSOFFSET);
-    SimpleTextUtil.write(output, Integer.toString(si.getDocStoreOffset()), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DSSEGMENT);
-    SimpleTextUtil.write(output, si.getDocStoreSegment(), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DSCOMPOUND);
-    SimpleTextUtil.write(output, Boolean.toString(si.getDocStoreIsCompoundFile()), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    SimpleTextUtil.write(output, SI_DELGEN);
-    SimpleTextUtil.write(output, Long.toString(si.getDelGen()), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    Map<Integer,Long> normGen = si.getNormGen();
-    int numNormGen = normGen == null ? 0 : normGen.size();
-    SimpleTextUtil.write(output, SI_NUM_NORMGEN);
-    SimpleTextUtil.write(output, Integer.toString(numNormGen), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    if (numNormGen > 0) {
-      for (Entry<Integer,Long> entry : normGen.entrySet()) {
-        SimpleTextUtil.write(output, SI_NORMGEN_KEY);
-        SimpleTextUtil.write(output, Integer.toString(entry.getKey()), scratch);
-        SimpleTextUtil.writeNewline(output);
-        
-        SimpleTextUtil.write(output, SI_NORMGEN_VALUE);
-        SimpleTextUtil.write(output, Long.toString(entry.getValue()), scratch);
-        SimpleTextUtil.writeNewline(output);
-      }
-    }
-    
-    Map<String,String> diagnostics = si.getDiagnostics();
-    int numDiagnostics = diagnostics == null ? 0 : diagnostics.size();
-    SimpleTextUtil.write(output, SI_NUM_DIAG);
-    SimpleTextUtil.write(output, Integer.toString(numDiagnostics), scratch);
-    SimpleTextUtil.writeNewline(output);
-    
-    if (numDiagnostics > 0) {
-      for (Map.Entry<String,String> diagEntry : diagnostics.entrySet()) {
-        SimpleTextUtil.write(output, SI_DIAG_KEY);
-        SimpleTextUtil.write(output, diagEntry.getKey(), scratch);
-        SimpleTextUtil.writeNewline(output);
-        
-        SimpleTextUtil.write(output, SI_DIAG_VALUE);
-        SimpleTextUtil.write(output, diagEntry.getValue(), scratch);
-        SimpleTextUtil.writeNewline(output);
-      }
-    }
-  }
-
-  @Override
-  public void prepareCommit(IndexOutput out) throws IOException {
-    ((ChecksumIndexOutput)out).prepareCommit();
-  }
-
-  @Override
-  public void finishCommit(IndexOutput out) throws IOException {
-    ((ChecksumIndexOutput)out).finishCommit();
-    out.close();
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsFormat.java
deleted file mode 100644
index 0c22266..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsFormat.java
+++ /dev/null
@@ -1,53 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.codecs.StoredFieldsFormat;
-import org.apache.lucene.index.codecs.StoredFieldsReader;
-import org.apache.lucene.index.codecs.StoredFieldsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * plain text stored fields format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextStoredFieldsFormat extends StoredFieldsFormat {
-
-  @Override
-  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
-    return new SimpleTextStoredFieldsReader(directory, si, fn, context);
-  }
-
-  @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    return new SimpleTextStoredFieldsWriter(directory, segment, context);
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    SimpleTextStoredFieldsReader.files(dir, info, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsReader.java
deleted file mode 100644
index eeba98e..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsReader.java
+++ /dev/null
@@ -1,198 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Set;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.StoredFieldVisitor;
-import org.apache.lucene.index.codecs.StoredFieldsReader;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.UnicodeUtil;
-
-import static org.apache.lucene.index.codecs.simpletext.SimpleTextStoredFieldsWriter.*;
-
-/**
- * reads plaintext stored fields
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextStoredFieldsReader extends StoredFieldsReader {
-  private ArrayList<Long> offsets; /* docid -> offset in .fld file */
-  private IndexInput in;
-  private BytesRef scratch = new BytesRef();
-  private CharsRef scratchUTF16 = new CharsRef();
-  private final FieldInfos fieldInfos;
-
-  public SimpleTextStoredFieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
-    this.fieldInfos = fn;
-    boolean success = false;
-    try {
-      in = directory.openInput(IndexFileNames.segmentFileName(si.name, "", SimpleTextStoredFieldsWriter.FIELDS_EXTENSION), context);
-      success = true;
-    } finally {
-      if (!success) {
-        close();
-      }
-    }
-    readIndex();
-  }
-  
-  // used by clone
-  SimpleTextStoredFieldsReader(ArrayList<Long> offsets, IndexInput in, FieldInfos fieldInfos) {
-    this.offsets = offsets;
-    this.in = in;
-    this.fieldInfos = fieldInfos;
-  }
-  
-  // we don't actually write a .fdx-like index, instead we read the 
-  // stored fields file in entirety up-front and save the offsets 
-  // so we can seek to the documents later.
-  private void readIndex() throws IOException {
-    offsets = new ArrayList<Long>();
-    while (!scratch.equals(END)) {
-      readLine();
-      if (StringHelper.startsWith(scratch, DOC)) {
-        offsets.add(in.getFilePointer());
-      }
-    }
-  }
-  
-  @Override
-  public void visitDocument(int n, StoredFieldVisitor visitor) throws CorruptIndexException, IOException {
-    in.seek(offsets.get(n));
-    readLine();
-    assert StringHelper.startsWith(scratch, NUM);
-    int numFields = parseIntAt(NUM.length);
-    
-    for (int i = 0; i < numFields; i++) {
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELD);
-      int fieldNumber = parseIntAt(FIELD.length);
-      FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
-      readLine();
-      assert StringHelper.startsWith(scratch, NAME);
-      readLine();
-      assert StringHelper.startsWith(scratch, TYPE);
-      
-      final BytesRef type;
-      if (equalsAt(TYPE_STRING, scratch, TYPE.length)) {
-        type = TYPE_STRING;
-      } else if (equalsAt(TYPE_BINARY, scratch, TYPE.length)) {
-        type = TYPE_BINARY;
-      } else if (equalsAt(TYPE_INT, scratch, TYPE.length)) {
-        type = TYPE_INT;
-      } else if (equalsAt(TYPE_LONG, scratch, TYPE.length)) {
-        type = TYPE_LONG;
-      } else if (equalsAt(TYPE_FLOAT, scratch, TYPE.length)) {
-        type = TYPE_FLOAT;
-      } else if (equalsAt(TYPE_DOUBLE, scratch, TYPE.length)) {
-        type = TYPE_DOUBLE;
-      } else {
-        throw new RuntimeException("unknown field type");
-      }
-      
-      switch (visitor.needsField(fieldInfo)) {
-        case YES:  
-          readField(type, fieldInfo, visitor);
-          break;
-        case NO:   
-          readLine();
-          assert StringHelper.startsWith(scratch, VALUE);
-          break;
-        case STOP: return;
-      }
-    }
-  }
-  
-  private void readField(BytesRef type, FieldInfo fieldInfo, StoredFieldVisitor visitor) throws IOException {
-    readLine();
-    assert StringHelper.startsWith(scratch, VALUE);
-    if (type == TYPE_STRING) {
-      visitor.stringField(fieldInfo, new String(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, "UTF-8"));
-    } else if (type == TYPE_BINARY) {
-      // TODO: who owns the bytes?
-      byte[] copy = new byte[scratch.length-VALUE.length];
-      System.arraycopy(scratch.bytes, scratch.offset+VALUE.length, copy, 0, copy.length);
-      visitor.binaryField(fieldInfo, copy, 0, copy.length);
-    } else if (type == TYPE_INT) {
-      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
-      visitor.intField(fieldInfo, Integer.parseInt(scratchUTF16.toString()));
-    } else if (type == TYPE_LONG) {
-      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
-      visitor.longField(fieldInfo, Long.parseLong(scratchUTF16.toString()));
-    } else if (type == TYPE_FLOAT) {
-      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
-      visitor.floatField(fieldInfo, Float.parseFloat(scratchUTF16.toString()));
-    } else if (type == TYPE_DOUBLE) {
-      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
-      visitor.doubleField(fieldInfo, Double.parseDouble(scratchUTF16.toString()));
-    }
-  }
-
-  @Override
-  public StoredFieldsReader clone() {
-    if (in == null) {
-      throw new AlreadyClosedException("this FieldsReader is closed");
-    }
-    return new SimpleTextStoredFieldsReader(offsets, (IndexInput) in.clone(), fieldInfos);
-  }
-  
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(in); 
-    } finally {
-      in = null;
-      offsets = null;
-    }
-  }
-  
-  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    files.add(IndexFileNames.segmentFileName(info.name, "", SimpleTextStoredFieldsWriter.FIELDS_EXTENSION));
-  }
-  
-  private void readLine() throws IOException {
-    SimpleTextUtil.readLine(in, scratch);
-  }
-  
-  private int parseIntAt(int offset) throws IOException {
-    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
-    return ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-  }
-  
-  private boolean equalsAt(BytesRef a, BytesRef b, int bOffset) {
-    return a.length == b.length - bOffset && 
-        ArrayUtil.equals(a.bytes, a.offset, b.bytes, b.offset + bOffset, b.length - bOffset);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsWriter.java
deleted file mode 100644
index 391f814..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextStoredFieldsWriter.java
+++ /dev/null
@@ -1,204 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.codecs.StoredFieldsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Writes plain-text stored fields.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextStoredFieldsWriter extends StoredFieldsWriter {
-  private int numDocsWritten = 0;
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput out;
-  
-  final static String FIELDS_EXTENSION = "fld";
-  
-  final static BytesRef TYPE_STRING = new BytesRef("string");
-  final static BytesRef TYPE_BINARY = new BytesRef("binary");
-  final static BytesRef TYPE_INT    = new BytesRef("int");
-  final static BytesRef TYPE_LONG   = new BytesRef("long");
-  final static BytesRef TYPE_FLOAT  = new BytesRef("float");
-  final static BytesRef TYPE_DOUBLE = new BytesRef("double");
-
-  final static BytesRef END     = new BytesRef("END");
-  final static BytesRef DOC     = new BytesRef("doc ");
-  final static BytesRef NUM     = new BytesRef("  numfields ");
-  final static BytesRef FIELD   = new BytesRef("  field ");
-  final static BytesRef NAME    = new BytesRef("    name ");
-  final static BytesRef TYPE    = new BytesRef("    type ");
-  final static BytesRef VALUE   = new BytesRef("    value ");
-  
-  private final BytesRef scratch = new BytesRef();
-  
-  public SimpleTextStoredFieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    this.directory = directory;
-    this.segment = segment;
-    boolean success = false;
-    try {
-      out = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
-
-  @Override
-  public void startDocument(int numStoredFields) throws IOException {
-    write(DOC);
-    write(Integer.toString(numDocsWritten));
-    newLine();
-    
-    write(NUM);
-    write(Integer.toString(numStoredFields));
-    newLine();
-    
-    numDocsWritten++;
-  }
-
-  @Override
-  public void writeField(FieldInfo info, IndexableField field) throws IOException {
-    write(FIELD);
-    write(Integer.toString(info.number));
-    newLine();
-    
-    write(NAME);
-    write(field.name());
-    newLine();
-    
-    write(TYPE);
-    if (field.numeric()) {
-      switch (field.numericDataType()) {
-        case INT:
-          write(TYPE_INT);
-          newLine();
-          
-          write(VALUE);
-          write(Integer.toString(field.numericValue().intValue()));
-          newLine();
-          
-          break;
-        case LONG:
-          write(TYPE_LONG);
-          newLine();
-          
-          write(VALUE);
-          write(Long.toString(field.numericValue().longValue()));
-          newLine();
-          
-          break;
-        case FLOAT:
-          write(TYPE_FLOAT);
-          newLine();
-          
-          write(VALUE);
-          write(Float.toString(field.numericValue().floatValue()));
-          newLine();
-          
-          break;
-        case DOUBLE:
-          write(TYPE_DOUBLE);
-          newLine();
-          
-          write(VALUE);
-          write(Double.toString(field.numericValue().doubleValue()));
-          newLine();
-          
-          break;
-        default:
-          assert false : "Should never get here";
-      }
-    } else { 
-      BytesRef bytes = field.binaryValue();
-      if (bytes != null) {
-        write(TYPE_BINARY);
-        newLine();
-        
-        write(VALUE);
-        write(bytes);
-        newLine();
-      } else if (field.stringValue() == null) {
-        throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
-      } else {
-        write(TYPE_STRING);
-        newLine();
-        
-        write(VALUE);
-        write(field.stringValue());
-        newLine();
-      }
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      close();
-    } catch (IOException ignored) {}
-    try {
-      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION));
-    } catch (IOException ignored) {}
-  }
-
-  @Override
-  public void finish(int numDocs) throws IOException {
-    if (numDocsWritten != numDocs) {
-      throw new RuntimeException("mergeFields produced an invalid result: docCount is " + numDocs 
-          + " but only saw " + numDocsWritten + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
-    }
-    write(END);
-    newLine();
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(out);
-    } finally {
-      out = null;
-    }
-  }
-  
-  private void write(String s) throws IOException {
-    SimpleTextUtil.write(out, s, scratch);
-  }
-  
-  private void write(BytesRef bytes) throws IOException {
-    SimpleTextUtil.write(out, bytes);
-  }
-  
-  private void newLine() throws IOException {
-    SimpleTextUtil.writeNewline(out);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsFormat.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsFormat.java
deleted file mode 100644
index d78849d..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsFormat.java
+++ /dev/null
@@ -1,53 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.codecs.TermVectorsFormat;
-import org.apache.lucene.index.codecs.TermVectorsReader;
-import org.apache.lucene.index.codecs.TermVectorsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * plain text term vectors format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextTermVectorsFormat extends TermVectorsFormat {
-
-  @Override
-  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
-    return new SimpleTextTermVectorsReader(directory, segmentInfo, fieldInfos, context);
-  }
-
-  @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    return new SimpleTextTermVectorsWriter(directory, segment, context);
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    SimpleTextTermVectorsReader.files(dir, info, files);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsReader.java
deleted file mode 100644
index f8d0cb1..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsReader.java
+++ /dev/null
@@ -1,532 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Set;
-import java.util.SortedMap;
-import java.util.TreeMap;
-
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.TermVectorsReader;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.UnicodeUtil;
-
-import static org.apache.lucene.index.codecs.simpletext.SimpleTextTermVectorsWriter.*;
-
-/**
- * Reads plain-text term vectors.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextTermVectorsReader extends TermVectorsReader {
-  private ArrayList<Long> offsets; /* docid -> offset in .vec file */
-  private IndexInput in;
-  private BytesRef scratch = new BytesRef();
-  private CharsRef scratchUTF16 = new CharsRef();
-  
-  public SimpleTextTermVectorsReader(Directory directory, SegmentInfo si, FieldInfos fieldInfos, IOContext context) throws IOException {
-    boolean success = false;
-    try {
-      in = directory.openInput(IndexFileNames.segmentFileName(si.name, "", VECTORS_EXTENSION), context);
-      success = true;
-    } finally {
-      if (!success) {
-        close();
-      }
-    }
-    readIndex();
-  }
-  
-  // used by clone
-  SimpleTextTermVectorsReader(ArrayList<Long> offsets, IndexInput in) {
-    this.offsets = offsets;
-    this.in = in;
-  }
-  
-  // we don't actually write a .tvx-like index, instead we read the 
-  // vectors file in entirety up-front and save the offsets 
-  // so we can seek to the data later.
-  private void readIndex() throws IOException {
-    offsets = new ArrayList<Long>();
-    while (!scratch.equals(END)) {
-      readLine();
-      if (StringHelper.startsWith(scratch, DOC)) {
-        offsets.add(in.getFilePointer());
-      }
-    }
-  }
-  
-  @Override
-  public Fields get(int doc) throws IOException {
-    // TestTV tests for this in testBadParams... but is this
-    // really guaranteed by the API?
-    if (doc < 0 || doc >= offsets.size()) {
-      throw new IllegalArgumentException("doc id out of range");
-    }
-
-    SortedMap<String,SimpleTVTerms> fields = new TreeMap<String,SimpleTVTerms>();
-    in.seek(offsets.get(doc));
-    readLine();
-    assert StringHelper.startsWith(scratch, NUMFIELDS);
-    int numFields = parseIntAt(NUMFIELDS.length);
-    if (numFields == 0) {
-      return null; // no vectors for this doc
-    }
-    for (int i = 0; i < numFields; i++) {
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELD);
-      int fieldNumber = parseIntAt(FIELD.length);
-      
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELDNAME);
-      String fieldName = readString(FIELDNAME.length, scratch);
-      
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELDPOSITIONS);
-      boolean positions = Boolean.parseBoolean(readString(FIELDPOSITIONS.length, scratch));
-      
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELDOFFSETS);
-      boolean offsets = Boolean.parseBoolean(readString(FIELDOFFSETS.length, scratch));
-      
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELDTERMCOUNT);
-      int termCount = parseIntAt(FIELDTERMCOUNT.length);
-      
-      SimpleTVTerms terms = new SimpleTVTerms();
-      fields.put(fieldName, terms);
-      
-      for (int j = 0; j < termCount; j++) {
-        readLine();
-        assert StringHelper.startsWith(scratch, TERMTEXT);
-        BytesRef term = new BytesRef();
-        int termLength = scratch.length - TERMTEXT.length;
-        term.grow(termLength);
-        term.length = termLength;
-        System.arraycopy(scratch.bytes, scratch.offset+TERMTEXT.length, term.bytes, term.offset, termLength);
-        
-        SimpleTVPostings postings = new SimpleTVPostings();
-        terms.terms.put(term, postings);
-        
-        readLine();
-        assert StringHelper.startsWith(scratch, TERMFREQ);
-        postings.freq = parseIntAt(TERMFREQ.length);
-        
-        if (positions || offsets) {
-          if (positions) {
-            postings.positions = new int[postings.freq];
-          }
-        
-          if (offsets) {
-            postings.startOffsets = new int[postings.freq];
-            postings.endOffsets = new int[postings.freq];
-          }
-          
-          for (int k = 0; k < postings.freq; k++) {
-            if (positions) {
-              readLine();
-              assert StringHelper.startsWith(scratch, POSITION);
-              postings.positions[k] = parseIntAt(POSITION.length);
-            }
-            
-            if (offsets) {
-              readLine();
-              assert StringHelper.startsWith(scratch, STARTOFFSET);
-              postings.startOffsets[k] = parseIntAt(STARTOFFSET.length);
-              
-              readLine();
-              assert StringHelper.startsWith(scratch, ENDOFFSET);
-              postings.endOffsets[k] = parseIntAt(ENDOFFSET.length);
-            }
-          }
-        }
-      }
-    }
-    return new SimpleTVFields(fields);
-  }
-
-  @Override
-  public TermVectorsReader clone() {
-    if (in == null) {
-      throw new AlreadyClosedException("this TermVectorsReader is closed");
-    }
-    return new SimpleTextTermVectorsReader(offsets, (IndexInput) in.clone());
-  }
-  
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(in); 
-    } finally {
-      in = null;
-      offsets = null;
-    }
-  }
-  
-  public static void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    if (info.getHasVectors()) {
-      files.add(IndexFileNames.segmentFileName(info.name, "", VECTORS_EXTENSION));
-    }
-  }
-  
-  private void readLine() throws IOException {
-    SimpleTextUtil.readLine(in, scratch);
-  }
-  
-  private int parseIntAt(int offset) throws IOException {
-    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
-    return ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-  }
-  
-  private String readString(int offset, BytesRef scratch) {
-    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
-    return scratchUTF16.toString();
-  }
-  
-  private class SimpleTVFields extends Fields {
-    private final SortedMap<String,SimpleTVTerms> fields;
-    
-    SimpleTVFields(SortedMap<String,SimpleTVTerms> fields) throws IOException {
-      this.fields = fields;
-    }
-
-    @Override
-    public FieldsEnum iterator() throws IOException {
-      return new FieldsEnum() {
-        private Iterator<Map.Entry<String,SimpleTVTerms>> iterator = fields.entrySet().iterator();
-        private Map.Entry<String,SimpleTVTerms> current = null;
-        
-        @Override
-        public String next() throws IOException {
-          if (!iterator.hasNext()) {
-            return null;
-          } else {
-            current = iterator.next();
-            return current.getKey();
-          }
-        }
-
-        @Override
-        public Terms terms() throws IOException {
-          return current.getValue();
-        }
-      };
-    }
-
-    @Override
-    public Terms terms(String field) throws IOException {
-      return fields.get(field);
-    }
-
-    @Override
-    public int getUniqueFieldCount() throws IOException {
-      return fields.size();
-    }
-  }
-  
-  private static class SimpleTVTerms extends Terms {
-    final SortedMap<BytesRef,SimpleTVPostings> terms;
-    
-    SimpleTVTerms() {
-      terms = new TreeMap<BytesRef,SimpleTVPostings>();
-    }
-    
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      // TODO: reuse
-      return new SimpleTVTermsEnum(terms);
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() throws IOException {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public long getUniqueTermCount() throws IOException {
-      return terms.size();
-    }
-
-    @Override
-    public long getSumTotalTermFreq() throws IOException {
-      return -1;
-    }
-
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return terms.size();
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return 1;
-    }
-  }
-  
-  private static class SimpleTVPostings {
-    private int freq;
-    private int positions[];
-    private int startOffsets[];
-    private int endOffsets[];
-  }
-  
-  private static class SimpleTVTermsEnum extends TermsEnum {
-    SortedMap<BytesRef,SimpleTVPostings> terms;
-    Iterator<Map.Entry<BytesRef,SimpleTextTermVectorsReader.SimpleTVPostings>> iterator;
-    Map.Entry<BytesRef,SimpleTextTermVectorsReader.SimpleTVPostings> current;
-    
-    SimpleTVTermsEnum(SortedMap<BytesRef,SimpleTVPostings> terms) {
-      this.terms = terms;
-      this.iterator = terms.entrySet().iterator();
-    }
-    
-    @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException {
-      iterator = terms.tailMap(text).entrySet().iterator();
-      if (!iterator.hasNext()) {
-        return SeekStatus.END;
-      } else {
-        return next().equals(text) ? SeekStatus.FOUND : SeekStatus.NOT_FOUND;
-      }
-    }
-
-    @Override
-    public void seekExact(long ord) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      if (!iterator.hasNext()) {
-        return null;
-      } else {
-        current = iterator.next();
-        return current.getKey();
-      }
-    }
-
-    @Override
-    public BytesRef term() throws IOException {
-      return current.getKey();
-    }
-
-    @Override
-    public long ord() throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public int docFreq() throws IOException {
-      return 1;
-    }
-
-    @Override
-    public long totalTermFreq() throws IOException {
-      return current.getValue().freq;
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) throws IOException {
-      // TODO: reuse
-      SimpleTVDocsEnum e = new SimpleTVDocsEnum();
-      e.reset(liveDocs, needsFreqs ? current.getValue().freq : -1);
-      return e;
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) throws IOException {
-      SimpleTVPostings postings = current.getValue();
-      if (postings.positions == null && postings.startOffsets == null) {
-        return null;
-      }
-      // TODO: reuse
-      SimpleTVDocsAndPositionsEnum e = new SimpleTVDocsAndPositionsEnum(postings.startOffsets != null);
-      e.reset(liveDocs, postings.positions, postings.startOffsets, postings.endOffsets);
-      return e;
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() throws IOException {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-  }
-  
-  // note: these two enum classes are exactly like the Default impl...
-  private static class SimpleTVDocsEnum extends DocsEnum {
-    private boolean didNext;
-    private int doc = -1;
-    private int freq;
-    private Bits liveDocs;
-
-    @Override
-    public int freq() {
-      assert freq != -1;
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int nextDoc() {
-      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
-        didNext = true;
-        return (doc = 0);
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    @Override
-    public int advance(int target) {
-      if (!didNext && target == 0) {
-        return nextDoc();
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    public void reset(Bits liveDocs, int freq) {
-      this.liveDocs = liveDocs;
-      this.freq = freq;
-      this.doc = -1;
-      didNext = false;
-    }
-  }
-  
-  private static class SimpleTVDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private final OffsetAttribute offsetAtt;
-    private boolean didNext;
-    private int doc = -1;
-    private int nextPos;
-    private Bits liveDocs;
-    private int[] positions;
-    private int[] startOffsets;
-    private int[] endOffsets;
-
-    public SimpleTVDocsAndPositionsEnum(boolean storeOffsets) {
-      if (storeOffsets) {
-        offsetAtt = attributes().addAttribute(OffsetAttribute.class);
-      } else {
-        offsetAtt = null;
-      }
-    }
-
-    public boolean canReuse(boolean storeOffsets) {
-      return storeOffsets == (offsetAtt != null);
-    }
-
-    @Override
-    public int freq() {
-      if (positions != null) {
-        return positions.length;
-      } else {
-        assert startOffsets != null;
-        return startOffsets.length;
-      }
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int nextDoc() {
-      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
-        didNext = true;
-        return (doc = 0);
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    @Override
-    public int advance(int target) {
-      if (!didNext && target == 0) {
-        return nextDoc();
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    public void reset(Bits liveDocs, int[] positions, int[] startOffsets, int[] endOffsets) {
-      this.liveDocs = liveDocs;
-      this.positions = positions;
-      this.startOffsets = startOffsets;
-      assert (offsetAtt != null) == (startOffsets != null);
-      this.endOffsets = endOffsets;
-      this.doc = -1;
-      didNext = false;
-      nextPos = 0;
-    }
-
-    @Override
-    public BytesRef getPayload() {
-      return null;
-    }
-
-    @Override
-    public boolean hasPayload() {
-      return false;
-    }
-
-    @Override
-    public int nextPosition() {
-      assert (positions != null && nextPos < positions.length) ||
-        startOffsets != null && nextPos < startOffsets.length;
-
-      if (startOffsets != null) {
-        offsetAtt.setOffset(startOffsets[nextPos],
-                            endOffsets[nextPos]);
-      }
-      if (positions != null) {
-        return positions[nextPos++];
-      } else {
-        nextPos++;
-        return -1;
-      }
-    }
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsWriter.java
deleted file mode 100644
index 3ce645c..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextTermVectorsWriter.java
+++ /dev/null
@@ -1,187 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.codecs.TermVectorsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Writes plain-text term vectors.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextTermVectorsWriter extends TermVectorsWriter {
-  
-  static final BytesRef END                = new BytesRef("END");
-  static final BytesRef DOC                = new BytesRef("doc ");
-  static final BytesRef NUMFIELDS          = new BytesRef("  numfields ");
-  static final BytesRef FIELD              = new BytesRef("  field ");
-  static final BytesRef FIELDNAME          = new BytesRef("    name ");
-  static final BytesRef FIELDPOSITIONS     = new BytesRef("    positions ");
-  static final BytesRef FIELDOFFSETS       = new BytesRef("    offsets   ");
-  static final BytesRef FIELDTERMCOUNT     = new BytesRef("    numterms ");
-  static final BytesRef TERMTEXT           = new BytesRef("    term ");
-  static final BytesRef TERMFREQ           = new BytesRef("      freq ");
-  static final BytesRef POSITION           = new BytesRef("      position ");
-  static final BytesRef STARTOFFSET        = new BytesRef("        startoffset ");
-  static final BytesRef ENDOFFSET          = new BytesRef("        endoffset ");
-
-  static final String VECTORS_EXTENSION = "vec";
-  
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput out;
-  private int numDocsWritten = 0;
-  private final BytesRef scratch = new BytesRef();
-  private boolean offsets;
-  private boolean positions;
-
-  public SimpleTextTermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    this.directory = directory;
-    this.segment = segment;
-    boolean success = false;
-    try {
-      out = directory.createOutput(IndexFileNames.segmentFileName(segment, "", VECTORS_EXTENSION), context);
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
-  
-  @Override
-  public void startDocument(int numVectorFields) throws IOException {
-    write(DOC);
-    write(Integer.toString(numDocsWritten));
-    newLine();
-    
-    write(NUMFIELDS);
-    write(Integer.toString(numVectorFields));
-    newLine();
-    numDocsWritten++;
-  }
-
-  @Override
-  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets) throws IOException {  
-    write(FIELD);
-    write(Integer.toString(info.number));
-    newLine();
-    
-    write(FIELDNAME);
-    write(info.name);
-    newLine();
-    
-    write(FIELDPOSITIONS);
-    write(Boolean.toString(positions));
-    newLine();
-    
-    write(FIELDOFFSETS);
-    write(Boolean.toString(offsets));
-    newLine();
-    
-    write(FIELDTERMCOUNT);
-    write(Integer.toString(numTerms));
-    newLine();
-    
-    this.positions = positions;
-    this.offsets = offsets;
-  }
-
-  @Override
-  public void startTerm(BytesRef term, int freq) throws IOException {
-    write(TERMTEXT);
-    write(term);
-    newLine();
-    
-    write(TERMFREQ);
-    write(Integer.toString(freq));
-    newLine();
-  }
-
-  @Override
-  public void addPosition(int position, int startOffset, int endOffset) throws IOException {
-    assert positions || offsets;
-    
-    if (positions) {
-      write(POSITION);
-      write(Integer.toString(position));
-      newLine();
-    }
-    
-    if (offsets) {
-      write(STARTOFFSET);
-      write(Integer.toString(startOffset));
-      newLine();
-      
-      write(ENDOFFSET);
-      write(Integer.toString(endOffset));
-      newLine();
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      close();
-    } catch (IOException ignored) {}
-    
-    try {
-      directory.deleteFile(IndexFileNames.segmentFileName(segment, "", VECTORS_EXTENSION));
-    } catch (IOException ignored) {}
-  }
-
-  @Override
-  public void finish(int numDocs) throws IOException {
-    if (numDocsWritten != numDocs) {
-      throw new RuntimeException("mergeVectors produced an invalid result: mergedDocs is " + numDocs + " but vec numDocs is " + numDocsWritten + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
-    }
-    write(END);
-    newLine();
-  }
-  
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(out);
-    } finally {
-      out = null;
-    }
-  }
-  
-  private void write(String s) throws IOException {
-    SimpleTextUtil.write(out, s, scratch);
-  }
-  
-  private void write(BytesRef bytes) throws IOException {
-    SimpleTextUtil.write(out, bytes);
-  }
-  
-  private void newLine() throws IOException {
-    SimpleTextUtil.writeNewline(out);
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextUtil.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextUtil.java
deleted file mode 100644
index 94df2c8..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextUtil.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.lucene.index.codecs.simpletext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.UnicodeUtil;
-
-class SimpleTextUtil {
-  public final static byte NEWLINE = 10;
-  public final static byte ESCAPE = 92;
-  
-  public static void write(DataOutput out, String s, BytesRef scratch) throws IOException {
-    UnicodeUtil.UTF16toUTF8(s, 0, s.length(), scratch);
-    write(out, scratch);
-  }
-
-  public static void write(DataOutput out, BytesRef b) throws IOException {
-    for(int i=0;i<b.length;i++) {
-      final byte bx = b.bytes[b.offset+i];
-      if (bx == NEWLINE || bx == ESCAPE) {
-        out.writeByte(ESCAPE);
-      }
-      out.writeByte(bx);
-    }
-  }
-
-  public static void writeNewline(DataOutput out) throws IOException {
-    out.writeByte(NEWLINE);
-  }
-  
-  public static void readLine(DataInput in, BytesRef scratch) throws IOException {
-    int upto = 0;
-    while(true) {
-      byte b = in.readByte();
-      if (scratch.bytes.length == upto) {
-        scratch.grow(1+upto);
-      }
-      if (b == ESCAPE) {
-        scratch.bytes[upto++] = in.readByte();
-      } else {
-        if (b == NEWLINE) {
-          break;
-        } else {
-          scratch.bytes[upto++] = b;
-        }
-      }
-    }
-    scratch.offset = 0;
-    scratch.length = upto;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/package.html b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/package.html
deleted file mode 100644
index 88aad68..0000000
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Simpletext Codec: writes human readable postings.
-</body>
-</html>
diff --git a/lucene/src/resources/META-INF/services/org.apache.lucene.codecs.Codec b/lucene/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
new file mode 100644
index 0000000..34d1c57
--- /dev/null
+++ b/lucene/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
@@ -0,0 +1,19 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.lucene40.Lucene40Codec
+org.apache.lucene.codecs.lucene3x.Lucene3xCodec
+org.apache.lucene.codecs.simpletext.SimpleTextCodec
+org.apache.lucene.codecs.appending.AppendingCodec
diff --git a/lucene/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
new file mode 100644
index 0000000..4d847e1
--- /dev/null
+++ b/lucene/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -0,0 +1,19 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat
+org.apache.lucene.codecs.pulsing.Pulsing40PostingsFormat
+org.apache.lucene.codecs.simpletext.SimpleTextPostingsFormat
+org.apache.lucene.codecs.memory.MemoryPostingsFormat
diff --git a/lucene/src/resources/META-INF/services/org.apache.lucene.index.codecs.Codec b/lucene/src/resources/META-INF/services/org.apache.lucene.index.codecs.Codec
deleted file mode 100644
index d87ffb4..0000000
--- a/lucene/src/resources/META-INF/services/org.apache.lucene.index.codecs.Codec
+++ /dev/null
@@ -1,19 +0,0 @@
-#  Licensed to the Apache Software Foundation (ASF) under one or more
-#  contributor license agreements.  See the NOTICE file distributed with
-#  this work for additional information regarding copyright ownership.
-#  The ASF licenses this file to You under the Apache License, Version 2.0
-#  (the "License"); you may not use this file except in compliance with
-#  the License.  You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#  Unless required by applicable law or agreed to in writing, software
-#  distributed under the License is distributed on an "AS IS" BASIS,
-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#  See the License for the specific language governing permissions and
-#  limitations under the License.
-
-org.apache.lucene.index.codecs.lucene40.Lucene40Codec
-org.apache.lucene.index.codecs.lucene3x.Lucene3xCodec
-org.apache.lucene.index.codecs.simpletext.SimpleTextCodec
-org.apache.lucene.index.codecs.appending.AppendingCodec
diff --git a/lucene/src/resources/META-INF/services/org.apache.lucene.index.codecs.PostingsFormat b/lucene/src/resources/META-INF/services/org.apache.lucene.index.codecs.PostingsFormat
deleted file mode 100644
index efbc25f..0000000
--- a/lucene/src/resources/META-INF/services/org.apache.lucene.index.codecs.PostingsFormat
+++ /dev/null
@@ -1,19 +0,0 @@
-#  Licensed to the Apache Software Foundation (ASF) under one or more
-#  contributor license agreements.  See the NOTICE file distributed with
-#  this work for additional information regarding copyright ownership.
-#  The ASF licenses this file to You under the Apache License, Version 2.0
-#  (the "License"); you may not use this file except in compliance with
-#  the License.  You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#  Unless required by applicable law or agreed to in writing, software
-#  distributed under the License is distributed on an "AS IS" BASIS,
-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#  See the License for the specific language governing permissions and
-#  limitations under the License.
-
-org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat
-org.apache.lucene.index.codecs.pulsing.Pulsing40PostingsFormat
-org.apache.lucene.index.codecs.simpletext.SimpleTextPostingsFormat
-org.apache.lucene.index.codecs.memory.MemoryPostingsFormat
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene40ords/Lucene40WithOrds.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene40ords/Lucene40WithOrds.java
new file mode 100644
index 0000000..9bd0e3d
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/lucene40ords/Lucene40WithOrds.java
@@ -0,0 +1,145 @@
+package org.apache.lucene.codecs.lucene40ords;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.BlockTermsReader;
+import org.apache.lucene.codecs.BlockTermsWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.FixedGapTermsIndexReader;
+import org.apache.lucene.codecs.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermsIndexReaderBase;
+import org.apache.lucene.codecs.TermsIndexWriterBase;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsReader;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+
+// TODO: we could make separate base class that can wrapp
+// any PostingsBaseFormat and make it ord-able...
+
+public class Lucene40WithOrds extends PostingsFormat {
+    
+  public Lucene40WithOrds() {
+    super("Lucene40WithOrds");
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase docs = new Lucene40PostingsWriter(state);
+
+    // TODO: should we make the terms index more easily
+    // pluggable?  Ie so that this codec would record which
+    // index impl was used, and switch on loading?
+    // Or... you must make a new Codec for this?
+    TermsIndexWriterBase indexWriter;
+    boolean success = false;
+    try {
+      indexWriter = new FixedGapTermsIndexWriter(state);
+      success = true;
+    } finally {
+      if (!success) {
+        docs.close();
+      }
+    }
+
+    success = false;
+    try {
+      // Must use BlockTermsWriter (not BlockTree) because
+      // BlockTree doens't support ords (yet)...
+      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, docs);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          docs.close();
+        } finally {
+          indexWriter.close();
+        }
+      }
+    }
+  }
+
+  public final static int TERMS_CACHE_SIZE = 1024;
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+    TermsIndexReaderBase indexReader;
+
+    boolean success = false;
+    try {
+      indexReader = new FixedGapTermsIndexReader(state.dir,
+                                                 state.fieldInfos,
+                                                 state.segmentInfo.name,
+                                                 state.termsIndexDivisor,
+                                                 BytesRef.getUTF8SortedAsUnicodeComparator(),
+                                                 state.segmentSuffix, state.context);
+      success = true;
+    } finally {
+      if (!success) {
+        postings.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsProducer ret = new BlockTermsReader(indexReader,
+                                                state.dir,
+                                                state.fieldInfos,
+                                                state.segmentInfo.name,
+                                                postings,
+                                                state.context,
+                                                TERMS_CACHE_SIZE,
+                                                state.segmentSuffix);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postings.close();
+        } finally {
+          indexReader.close();
+        }
+      }
+    }
+  }
+
+  /** Extension of freq postings file */
+  static final String FREQ_EXTENSION = "frq";
+
+  /** Extension of prox postings file */
+  static final String PROX_EXTENSION = "prx";
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
+    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
+    BlockTermsReader.files(dir, segmentInfo, segmentSuffix, files);
+    FixedGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
new file mode 100644
index 0000000..65343275
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
@@ -0,0 +1,210 @@
+package org.apache.lucene.codecs.mockintblock;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.BlockTermsReader;
+import org.apache.lucene.codecs.BlockTermsWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.FixedGapTermsIndexReader;
+import org.apache.lucene.codecs.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermsIndexReaderBase;
+import org.apache.lucene.codecs.TermsIndexWriterBase;
+import org.apache.lucene.codecs.intblock.FixedIntBlockIndexInput;
+import org.apache.lucene.codecs.intblock.FixedIntBlockIndexOutput;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
+import org.apache.lucene.codecs.sep.IntIndexInput;
+import org.apache.lucene.codecs.sep.IntIndexOutput;
+import org.apache.lucene.codecs.sep.IntStreamFactory;
+import org.apache.lucene.codecs.sep.SepPostingsReader;
+import org.apache.lucene.codecs.sep.SepPostingsWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.*;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * A silly test codec to verify core support for fixed
+ * sized int block encoders is working.  The int encoder
+ * used here just writes each block as a series of vInt.
+ */
+
+public class MockFixedIntBlockPostingsFormat extends PostingsFormat {
+
+  private final int blockSize;
+
+  public MockFixedIntBlockPostingsFormat() {
+    this(1);
+  }
+
+  public MockFixedIntBlockPostingsFormat(int blockSize) {
+    super("MockFixedIntBlock");
+    this.blockSize = blockSize;
+  }
+
+  @Override
+  public String toString() {
+    return getName() + "(blockSize=" + blockSize + ")";
+  }
+
+  // only for testing
+  public IntStreamFactory getIntFactory() {
+    return new MockIntFactory(blockSize);
+  }
+
+  public static class MockIntFactory extends IntStreamFactory {
+    private final int blockSize;
+
+    public MockIntFactory(int blockSize) {
+      this.blockSize = blockSize;
+    }
+
+    @Override
+    public IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException {
+      return new FixedIntBlockIndexInput(dir.openInput(fileName, context)) {
+
+        @Override
+        protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) throws IOException {
+          return new BlockReader() {
+            public void seek(long pos) {}
+            public void readBlock() throws IOException {
+              for(int i=0;i<buffer.length;i++) {
+                buffer[i] = in.readVInt();
+              }
+            }
+          };
+        }
+      };
+    }
+
+    @Override
+    public IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException {
+      IndexOutput out = dir.createOutput(fileName, context);
+      boolean success = false;
+      try {
+        FixedIntBlockIndexOutput ret = new FixedIntBlockIndexOutput(out, blockSize) {
+          @Override
+          protected void flushBlock() throws IOException {
+            for(int i=0;i<buffer.length;i++) {
+              assert buffer[i] >= 0;
+              out.writeVInt(buffer[i]);
+            }
+          }
+        };
+        success = true;
+        return ret;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(out);
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase postingsWriter = new SepPostingsWriter(state, new MockIntFactory(blockSize));
+
+    boolean success = false;
+    TermsIndexWriterBase indexWriter;
+    try {
+      indexWriter = new FixedGapTermsIndexWriter(state);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsWriter.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsWriter.close();
+        } finally {
+          indexWriter.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postingsReader = new SepPostingsReader(state.dir,
+                                                              state.segmentInfo,
+                                                              state.context,
+                                                              new MockIntFactory(blockSize), state.segmentSuffix);
+
+    TermsIndexReaderBase indexReader;
+    boolean success = false;
+    try {
+      indexReader = new FixedGapTermsIndexReader(state.dir,
+                                                       state.fieldInfos,
+                                                       state.segmentInfo.name,
+                                                       state.termsIndexDivisor,
+                                                       BytesRef.getUTF8SortedAsUnicodeComparator(), state.segmentSuffix,
+                                                       IOContext.DEFAULT);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsReader.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsProducer ret = new BlockTermsReader(indexReader,
+                                                state.dir,
+                                                state.fieldInfos,
+                                                state.segmentInfo.name,
+                                                postingsReader,
+                                                state.context,
+                                                Lucene40PostingsFormat.TERMS_CACHE_SIZE,
+                                                state.segmentSuffix);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsReader.close();
+        } finally {
+          indexReader.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
+    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
+    BlockTermsReader.files(dir, segmentInfo, segmentSuffix, files);
+    FixedGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
new file mode 100644
index 0000000..470c33d
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
@@ -0,0 +1,233 @@
+package org.apache.lucene.codecs.mockintblock;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.BlockTermsReader;
+import org.apache.lucene.codecs.BlockTermsWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.FixedGapTermsIndexReader;
+import org.apache.lucene.codecs.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermsIndexReaderBase;
+import org.apache.lucene.codecs.TermsIndexWriterBase;
+import org.apache.lucene.codecs.intblock.VariableIntBlockIndexInput;
+import org.apache.lucene.codecs.intblock.VariableIntBlockIndexOutput;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
+import org.apache.lucene.codecs.sep.IntIndexInput;
+import org.apache.lucene.codecs.sep.IntIndexOutput;
+import org.apache.lucene.codecs.sep.IntStreamFactory;
+import org.apache.lucene.codecs.sep.SepPostingsReader;
+import org.apache.lucene.codecs.sep.SepPostingsWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * A silly test codec to verify core support for variable
+ * sized int block encoders is working.  The int encoder
+ * used here writes baseBlockSize ints at once, if the first
+ * int is <= 3, else 2*baseBlockSize.
+ */
+
+public class MockVariableIntBlockPostingsFormat extends PostingsFormat {
+  private final int baseBlockSize;
+  
+  public MockVariableIntBlockPostingsFormat() {
+    this(1);
+  }
+
+  public MockVariableIntBlockPostingsFormat(int baseBlockSize) {
+    super("MockVariableIntBlock");
+    this.baseBlockSize = baseBlockSize;
+  }
+
+  @Override
+  public String toString() {
+    return getName() + "(baseBlockSize="+ baseBlockSize + ")";
+  }
+
+  public static class MockIntFactory extends IntStreamFactory {
+
+    private final int baseBlockSize;
+
+    public MockIntFactory(int baseBlockSize) {
+      this.baseBlockSize = baseBlockSize;
+    }
+
+    @Override
+    public IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException {
+      final IndexInput in = dir.openInput(fileName, context);
+      final int baseBlockSize = in.readInt();
+      return new VariableIntBlockIndexInput(in) {
+
+        @Override
+        protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) throws IOException {
+          return new BlockReader() {
+            public void seek(long pos) {}
+            public int readBlock() throws IOException {
+              buffer[0] = in.readVInt();
+              final int count = buffer[0] <= 3 ? baseBlockSize-1 : 2*baseBlockSize-1;
+              assert buffer.length >= count: "buffer.length=" + buffer.length + " count=" + count;
+              for(int i=0;i<count;i++) {
+                buffer[i+1] = in.readVInt();
+              }
+              return 1+count;
+            }
+          };
+        }
+      };
+    }
+
+    @Override
+    public IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException {
+      final IndexOutput out = dir.createOutput(fileName, context);
+      boolean success = false;
+      try {
+        out.writeInt(baseBlockSize);
+        VariableIntBlockIndexOutput ret = new VariableIntBlockIndexOutput(out, 2*baseBlockSize) {
+          int pendingCount;
+          final int[] buffer = new int[2+2*baseBlockSize];
+          
+          @Override
+          protected int add(int value) throws IOException {
+            assert value >= 0;
+            buffer[pendingCount++] = value;
+            // silly variable block length int encoder: if
+            // first value <= 3, we write N vints at once;
+            // else, 2*N
+            final int flushAt = buffer[0] <= 3 ? baseBlockSize : 2*baseBlockSize;
+            
+            // intentionally be non-causal here:
+            if (pendingCount == flushAt+1) {
+              for(int i=0;i<flushAt;i++) {
+                out.writeVInt(buffer[i]);
+              }
+              buffer[0] = buffer[flushAt];
+              pendingCount = 1;
+              return flushAt;
+            } else {
+              return 0;
+            }
+          }
+        };
+        success = true;
+        return ret;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(out);
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase postingsWriter = new SepPostingsWriter(state, new MockIntFactory(baseBlockSize));
+
+    boolean success = false;
+    TermsIndexWriterBase indexWriter;
+    try {
+      indexWriter = new FixedGapTermsIndexWriter(state);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsWriter.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsWriter.close();
+        } finally {
+          indexWriter.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postingsReader = new SepPostingsReader(state.dir,
+                                                              state.segmentInfo,
+                                                              state.context,
+                                                              new MockIntFactory(baseBlockSize), state.segmentSuffix);
+
+    TermsIndexReaderBase indexReader;
+    boolean success = false;
+    try {
+      indexReader = new FixedGapTermsIndexReader(state.dir,
+                                                       state.fieldInfos,
+                                                       state.segmentInfo.name,
+                                                       state.termsIndexDivisor,
+                                                       BytesRef.getUTF8SortedAsUnicodeComparator(),
+                                                       state.segmentSuffix, state.context);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsReader.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsProducer ret = new BlockTermsReader(indexReader,
+                                                state.dir,
+                                                state.fieldInfos,
+                                                state.segmentInfo.name,
+                                                postingsReader,
+                                                state.context,
+                                                Lucene40PostingsFormat.TERMS_CACHE_SIZE,
+                                                state.segmentSuffix);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsReader.close();
+        } finally {
+          indexReader.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
+    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
+    BlockTermsReader.files(dir, segmentInfo, segmentSuffix, files);
+    FixedGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
new file mode 100644
index 0000000..6780b9d
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
@@ -0,0 +1,435 @@
+package org.apache.lucene.codecs.mockrandom;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Random;
+import java.util.Set;
+
+import org.apache.lucene.codecs.BlockTermsReader;
+import org.apache.lucene.codecs.BlockTermsWriter;
+import org.apache.lucene.codecs.BlockTreeTermsReader;
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.FixedGapTermsIndexReader;
+import org.apache.lucene.codecs.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsIndexReaderBase;
+import org.apache.lucene.codecs.TermsIndexWriterBase;
+import org.apache.lucene.codecs.VariableGapTermsIndexReader;
+import org.apache.lucene.codecs.VariableGapTermsIndexWriter;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsReader;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter;
+import org.apache.lucene.codecs.mockintblock.MockFixedIntBlockPostingsFormat;
+import org.apache.lucene.codecs.mockintblock.MockVariableIntBlockPostingsFormat;
+import org.apache.lucene.codecs.mocksep.MockSingleIntFactory;
+import org.apache.lucene.codecs.pulsing.PulsingPostingsReader;
+import org.apache.lucene.codecs.pulsing.PulsingPostingsWriter;
+import org.apache.lucene.codecs.sep.IntIndexInput;
+import org.apache.lucene.codecs.sep.IntIndexOutput;
+import org.apache.lucene.codecs.sep.IntStreamFactory;
+import org.apache.lucene.codecs.sep.SepPostingsReader;
+import org.apache.lucene.codecs.sep.SepPostingsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Randomly combines terms index impl w/ postings impls.
+ */
+
+public class MockRandomPostingsFormat extends PostingsFormat {
+  private final Random seedRandom;
+  private final String SEED_EXT = "sd";
+  
+  public MockRandomPostingsFormat() {
+    // just for reading, we are gonna setSeed from the .seed file... right?
+    this(new Random());
+  }
+  
+  public MockRandomPostingsFormat(Random random) {
+    super("MockRandom");
+    this.seedRandom = new Random(random.nextLong());
+  }
+
+  // Chooses random IntStreamFactory depending on file's extension
+  private static class MockIntStreamFactory extends IntStreamFactory {
+    private final int salt;
+    private final List<IntStreamFactory> delegates = new ArrayList<IntStreamFactory>();
+
+    public MockIntStreamFactory(Random random) {
+      salt = random.nextInt();
+      delegates.add(new MockSingleIntFactory());
+      final int blockSize = _TestUtil.nextInt(random, 1, 2000);
+      delegates.add(new MockFixedIntBlockPostingsFormat.MockIntFactory(blockSize));
+      final int baseBlockSize = _TestUtil.nextInt(random, 1, 127);
+      delegates.add(new MockVariableIntBlockPostingsFormat.MockIntFactory(baseBlockSize));
+      // TODO: others
+    }
+
+    private static String getExtension(String fileName) {
+      final int idx = fileName.indexOf('.');
+      assert idx != -1;
+      return fileName.substring(idx);
+    }
+
+    @Override
+    public IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException {
+      // Must only use extension, because IW.addIndexes can
+      // rename segment!
+      final IntStreamFactory f = delegates.get((Math.abs(salt ^ getExtension(fileName).hashCode())) % delegates.size());
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: read using int factory " + f + " from fileName=" + fileName);
+      }
+      return f.openInput(dir, fileName, context);
+    }
+
+    @Override
+    public IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException {
+      final IntStreamFactory f = delegates.get((Math.abs(salt ^ getExtension(fileName).hashCode())) % delegates.size());
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: write using int factory " + f + " to fileName=" + fileName);
+      }
+      return f.createOutput(dir, fileName, context);
+    }
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    // we pull this before the seed intentionally: because its not consumed at runtime
+    // (the skipInterval is written into postings header)
+    int skipInterval = _TestUtil.nextInt(seedRandom, 2, 10);
+    
+    if (LuceneTestCase.VERBOSE) {
+      System.out.println("MockRandomCodec: skipInterval=" + skipInterval);
+    }
+    
+    final long seed = seedRandom.nextLong();
+
+    if (LuceneTestCase.VERBOSE) {
+      System.out.println("MockRandomCodec: writing to seg=" + state.segmentName + " formatID=" + state.segmentSuffix + " seed=" + seed);
+    }
+
+    final String seedFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, SEED_EXT);
+    final IndexOutput out = state.directory.createOutput(seedFileName, state.context);
+    try {
+      out.writeLong(seed);
+    } finally {
+      out.close();
+    }
+
+    final Random random = new Random(seed);
+    
+    random.nextInt(); // consume a random for buffersize
+
+    PostingsWriterBase postingsWriter;
+    if (random.nextBoolean()) {
+      postingsWriter = new SepPostingsWriter(state, new MockIntStreamFactory(random), skipInterval);
+    } else {
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: writing Standard postings");
+      }
+      postingsWriter = new Lucene40PostingsWriter(state, skipInterval);
+    }
+
+    if (random.nextBoolean()) {
+      final int totTFCutoff = _TestUtil.nextInt(random, 1, 20);
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: writing pulsing postings with totTFCutoff=" + totTFCutoff);
+      }
+      postingsWriter = new PulsingPostingsWriter(totTFCutoff, postingsWriter);
+    }
+
+    final FieldsConsumer fields;
+
+    if (random.nextBoolean()) {
+      // Use BlockTree terms dict
+
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: writing BlockTree terms dict");
+      }
+
+      // TODO: would be nice to allow 1 but this is very
+      // slow to write
+      final int minTermsInBlock = _TestUtil.nextInt(random, 2, 100);
+      final int maxTermsInBlock = Math.max(2, (minTermsInBlock-1)*2 + random.nextInt(100));
+
+      boolean success = false;
+      try {
+        fields = new BlockTreeTermsWriter(state, postingsWriter, minTermsInBlock, maxTermsInBlock);
+        success = true;
+      } finally {
+        if (!success) {
+          postingsWriter.close();
+        }
+      }
+    } else {
+
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: writing Block terms dict");
+      }
+
+      boolean success = false;
+
+      final TermsIndexWriterBase indexWriter;
+      try {
+        if (random.nextBoolean()) {
+          state.termIndexInterval = _TestUtil.nextInt(random, 1, 100);
+          if (LuceneTestCase.VERBOSE) {
+            System.out.println("MockRandomCodec: fixed-gap terms index (tii=" + state.termIndexInterval + ")");
+          }
+          indexWriter = new FixedGapTermsIndexWriter(state);
+        } else {
+          final VariableGapTermsIndexWriter.IndexTermSelector selector;
+          final int n2 = random.nextInt(3);
+          if (n2 == 0) {
+            final int tii = _TestUtil.nextInt(random, 1, 100);
+            selector = new VariableGapTermsIndexWriter.EveryNTermSelector(tii);
+           if (LuceneTestCase.VERBOSE) {
+              System.out.println("MockRandomCodec: variable-gap terms index (tii=" + tii + ")");
+            }
+          } else if (n2 == 1) {
+            final int docFreqThresh = _TestUtil.nextInt(random, 2, 100);
+            final int tii = _TestUtil.nextInt(random, 1, 100);
+            selector = new VariableGapTermsIndexWriter.EveryNOrDocFreqTermSelector(docFreqThresh, tii);
+          } else {
+            final long seed2 = random.nextLong();
+            final int gap = _TestUtil.nextInt(random, 2, 40);
+            if (LuceneTestCase.VERBOSE) {
+             System.out.println("MockRandomCodec: random-gap terms index (max gap=" + gap + ")");
+            }
+           selector = new VariableGapTermsIndexWriter.IndexTermSelector() {
+                final Random rand = new Random(seed2);
+
+                @Override
+                public boolean isIndexTerm(BytesRef term, TermStats stats) {
+                  return rand.nextInt(gap) == gap/2;
+                }
+
+                @Override
+                  public void newField(FieldInfo fieldInfo) {
+                }
+              };
+          }
+          indexWriter = new VariableGapTermsIndexWriter(state, selector);
+        }
+        success = true;
+      } finally {
+        if (!success) {
+          postingsWriter.close();
+        }
+      }
+
+      success = false;
+      try {
+        fields = new BlockTermsWriter(indexWriter, state, postingsWriter);
+        success = true;
+      } finally {
+        if (!success) {
+          try {
+            postingsWriter.close();
+          } finally {
+            indexWriter.close();
+          }
+        }
+      }
+    }
+
+    return fields;
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+
+    final String seedFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, SEED_EXT);
+    final IndexInput in = state.dir.openInput(seedFileName, state.context);
+    final long seed = in.readLong();
+    if (LuceneTestCase.VERBOSE) {
+      System.out.println("MockRandomCodec: reading from seg=" + state.segmentInfo.name + " formatID=" + state.segmentSuffix + " seed=" + seed);
+    }
+    in.close();
+
+    final Random random = new Random(seed);
+    
+    int readBufferSize = _TestUtil.nextInt(random, 1, 4096);
+    if (LuceneTestCase.VERBOSE) {
+      System.out.println("MockRandomCodec: readBufferSize=" + readBufferSize);
+    }
+
+    PostingsReaderBase postingsReader;
+
+    if (random.nextBoolean()) {
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: reading Sep postings");
+      }
+      postingsReader = new SepPostingsReader(state.dir, state.segmentInfo,
+                                             state.context, new MockIntStreamFactory(random), state.segmentSuffix);
+    } else {
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: reading Standard postings");
+      }
+      postingsReader = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+    }
+
+    if (random.nextBoolean()) {
+      final int totTFCutoff = _TestUtil.nextInt(random, 1, 20);
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: reading pulsing postings with totTFCutoff=" + totTFCutoff);
+      }
+      postingsReader = new PulsingPostingsReader(postingsReader);
+    }
+
+    final FieldsProducer fields;
+
+    if (random.nextBoolean()) {
+      // Use BlockTree terms dict
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: reading BlockTree terms dict");
+      }
+
+      boolean success = false;
+      try {
+        fields = new BlockTreeTermsReader(state.dir,
+                                          state.fieldInfos,
+                                          state.segmentInfo.name,
+                                          postingsReader,
+                                          state.context,
+                                          state.segmentSuffix,
+                                          state.termsIndexDivisor);
+        success = true;
+      } finally {
+        if (!success) {
+          postingsReader.close();
+        }
+      }
+    } else {
+
+      if (LuceneTestCase.VERBOSE) {
+        System.out.println("MockRandomCodec: reading Block terms dict");
+      }
+      final TermsIndexReaderBase indexReader;
+      boolean success = false;
+      try {
+        final boolean doFixedGap = random.nextBoolean();
+
+        // randomness diverges from writer, here:
+        if (state.termsIndexDivisor != -1) {
+          state.termsIndexDivisor = _TestUtil.nextInt(random, 1, 10);
+        }
+
+        if (doFixedGap) {
+          // if termsIndexDivisor is set to -1, we should not touch it. It means a
+          // test explicitly instructed not to load the terms index.
+          if (LuceneTestCase.VERBOSE) {
+            System.out.println("MockRandomCodec: fixed-gap terms index (divisor=" + state.termsIndexDivisor + ")");
+          }
+          indexReader = new FixedGapTermsIndexReader(state.dir,
+                                                     state.fieldInfos,
+                                                     state.segmentInfo.name,
+                                                     state.termsIndexDivisor,
+                                                     BytesRef.getUTF8SortedAsUnicodeComparator(),
+                                                     state.segmentSuffix, state.context);
+        } else {
+          final int n2 = random.nextInt(3);
+          if (n2 == 1) {
+            random.nextInt();
+          } else if (n2 == 2) {
+            random.nextLong();
+          }
+          if (LuceneTestCase.VERBOSE) {
+            System.out.println("MockRandomCodec: variable-gap terms index (divisor=" + state.termsIndexDivisor + ")");
+          }
+          indexReader = new VariableGapTermsIndexReader(state.dir,
+                                                        state.fieldInfos,
+                                                        state.segmentInfo.name,
+                                                        state.termsIndexDivisor,
+                                                        state.segmentSuffix, state.context);
+
+        }
+
+        success = true;
+      } finally {
+        if (!success) {
+          postingsReader.close();
+        }
+      }
+
+      final int termsCacheSize = _TestUtil.nextInt(random, 1, 1024);
+
+      success = false;
+      try {
+        fields = new BlockTermsReader(indexReader,
+                                      state.dir,
+                                      state.fieldInfos,
+                                      state.segmentInfo.name,
+                                      postingsReader,
+                                      state.context,
+                                      termsCacheSize,
+                                      state.segmentSuffix);
+        success = true;
+      } finally {
+        if (!success) {
+          try {
+            postingsReader.close();
+          } finally {
+            indexReader.close();
+          }
+        }
+      }
+    }
+
+    return fields;
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
+    final String seedFileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SEED_EXT);    
+    files.add(seedFileName);
+    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
+    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
+    BlockTermsReader.files(dir, segmentInfo, segmentSuffix, files);
+    BlockTreeTermsReader.files(dir, segmentInfo, segmentSuffix, files);
+    FixedGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
+    VariableGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
+    // hackish!
+    Iterator<String> it = files.iterator();
+    while(it.hasNext()) {
+      final String file = it.next();
+      if (!dir.fileExists(file)) {
+        it.remove();
+      }
+    }
+    //System.out.println("MockRandom.files return " + files);
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSepDocValuesFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSepDocValuesFormat.java
new file mode 100644
index 0000000..e3d449f
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSepDocValuesFormat.java
@@ -0,0 +1,54 @@
+package org.apache.lucene.codecs.mocksep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.PerDocConsumer;
+import org.apache.lucene.codecs.PerDocProducer;
+import org.apache.lucene.codecs.sep.SepDocValuesConsumer;
+import org.apache.lucene.codecs.sep.SepDocValuesProducer;
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.Directory;
+
+/**
+ * Separate-file docvalues implementation
+ * @lucene.experimental
+ */
+// TODO: we could move this out of src/test ?
+public class MockSepDocValuesFormat extends DocValuesFormat {
+
+  @Override
+  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
+    return new SepDocValuesConsumer(state);
+  }
+
+  @Override
+  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
+    return new SepDocValuesProducer(state);
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
+    SepDocValuesConsumer.files(dir, info, files);
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java
new file mode 100644
index 0000000..ffd4541
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java
@@ -0,0 +1,138 @@
+package org.apache.lucene.codecs.mocksep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.BlockTermsReader;
+import org.apache.lucene.codecs.BlockTermsWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.FixedGapTermsIndexReader;
+import org.apache.lucene.codecs.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermsIndexReaderBase;
+import org.apache.lucene.codecs.TermsIndexWriterBase;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
+import org.apache.lucene.codecs.sep.SepPostingsReader;
+import org.apache.lucene.codecs.sep.SepPostingsWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * A silly codec that simply writes each file separately as
+ * single vInts.  Don't use this (performance will be poor)!
+ * This is here just to test the core sep codec
+ * classes.
+ */
+public class MockSepPostingsFormat extends PostingsFormat {
+
+  public MockSepPostingsFormat() {
+    super("MockSep");
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+
+    PostingsWriterBase postingsWriter = new SepPostingsWriter(state, new MockSingleIntFactory());
+
+    boolean success = false;
+    TermsIndexWriterBase indexWriter;
+    try {
+      indexWriter = new FixedGapTermsIndexWriter(state);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsWriter.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsWriter.close();
+        } finally {
+          indexWriter.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+
+    PostingsReaderBase postingsReader = new SepPostingsReader(state.dir, state.segmentInfo,
+        state.context, new MockSingleIntFactory(), state.segmentSuffix);
+
+    TermsIndexReaderBase indexReader;
+    boolean success = false;
+    try {
+      indexReader = new FixedGapTermsIndexReader(state.dir,
+                                                       state.fieldInfos,
+                                                       state.segmentInfo.name,
+                                                       state.termsIndexDivisor,
+                                                       BytesRef.getUTF8SortedAsUnicodeComparator(),
+                                                       state.segmentSuffix, state.context);
+      success = true;
+    } finally {
+      if (!success) {
+        postingsReader.close();
+      }
+    }
+
+    success = false;
+    try {
+      FieldsProducer ret = new BlockTermsReader(indexReader,
+                                                state.dir,
+                                                state.fieldInfos,
+                                                state.segmentInfo.name,
+                                                postingsReader,
+                                                state.context,
+                                                Lucene40PostingsFormat.TERMS_CACHE_SIZE,
+                                                state.segmentSuffix);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        try {
+          postingsReader.close();
+        } finally {
+          indexReader.close();
+        }
+      }
+    }
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
+    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
+    BlockTermsReader.files(dir, segmentInfo, segmentSuffix, files);
+    FixedGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSingleIntFactory.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSingleIntFactory.java
new file mode 100644
index 0000000..b37df0d
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSingleIntFactory.java
@@ -0,0 +1,38 @@
+package org.apache.lucene.codecs.mocksep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.codecs.sep.IntIndexInput;
+import org.apache.lucene.codecs.sep.IntIndexOutput;
+import org.apache.lucene.codecs.sep.IntStreamFactory;
+
+import java.io.IOException;
+
+/** @lucene.experimental */
+public class MockSingleIntFactory extends IntStreamFactory {
+  @Override
+  public IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException {
+    return new MockSingleIntIndexInput(dir, fileName, context);
+  }
+  @Override
+  public IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException {
+    return new MockSingleIntIndexOutput(dir, fileName, context);
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSingleIntIndexInput.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSingleIntIndexInput.java
new file mode 100644
index 0000000..5d92971
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSingleIntIndexInput.java
@@ -0,0 +1,114 @@
+package org.apache.lucene.codecs.mocksep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.sep.IntIndexInput;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.CodecUtil;
+
+/** Reads IndexInputs written with {@link
+ *  MockSingleIntIndexOutput}.  NOTE: this class is just for
+ *  demonstration puprposes (it is a very slow way to read a
+ *  block of ints).
+ *
+ * @lucene.experimental
+ */
+public class MockSingleIntIndexInput extends IntIndexInput {
+  private final IndexInput in;
+
+  public MockSingleIntIndexInput(Directory dir, String fileName, IOContext context)
+    throws IOException {
+    in = dir.openInput(fileName, context);
+    CodecUtil.checkHeader(in, MockSingleIntIndexOutput.CODEC,
+                          MockSingleIntIndexOutput.VERSION_START,
+                          MockSingleIntIndexOutput.VERSION_START);
+  }
+
+  @Override
+  public Reader reader() throws IOException {
+    return new Reader((IndexInput) in.clone());
+  }
+
+  @Override
+  public void close() throws IOException {
+    in.close();
+  }
+
+  public static class Reader extends IntIndexInput.Reader {
+    // clone:
+    private final IndexInput in;
+
+    public Reader(IndexInput in) {
+      this.in = in;
+    }
+
+    /** Reads next single int */
+    @Override
+    public int next() throws IOException {
+      //System.out.println("msii.next() fp=" + in.getFilePointer() + " vs " + in.length());
+      return in.readVInt();
+    }
+  }
+  
+  class Index extends IntIndexInput.Index {
+    private long fp;
+
+    @Override
+    public void read(DataInput indexIn, boolean absolute)
+      throws IOException {
+      if (absolute) {
+        fp = indexIn.readVLong();
+      } else {
+        fp += indexIn.readVLong();
+      }
+    }
+
+    @Override
+    public void set(IntIndexInput.Index other) {
+      fp = ((Index) other).fp;
+    }
+
+    @Override
+    public void seek(IntIndexInput.Reader other) throws IOException {
+      ((Reader) other).in.seek(fp);
+    }
+
+    @Override
+    public String toString() {
+      return Long.toString(fp);
+    }
+
+    @Override
+    public Object clone() {
+      Index other = new Index();
+      other.fp = fp;
+      return other;
+    }
+  }
+
+  @Override
+  public Index index() {
+    return new Index();
+  }
+}
+
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSingleIntIndexOutput.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSingleIntIndexOutput.java
new file mode 100644
index 0000000..3f4c22f
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/mocksep/MockSingleIntIndexOutput.java
@@ -0,0 +1,105 @@
+package org.apache.lucene.codecs.mocksep;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.codecs.sep.IntIndexOutput;
+
+import java.io.IOException;
+
+/** Writes ints directly to the file (not in blocks) as
+ *  vInt.
+ * 
+ * @lucene.experimental
+*/
+public class MockSingleIntIndexOutput extends IntIndexOutput {
+  private final IndexOutput out;
+  final static String CODEC = "SINGLE_INTS";
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  public MockSingleIntIndexOutput(Directory dir, String fileName, IOContext context) throws IOException {
+    out = dir.createOutput(fileName, context);
+    boolean success = false;
+    try {
+      CodecUtil.writeHeader(out, CODEC, VERSION_CURRENT);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+
+  /** Write an int to the primary file */
+  @Override
+  public void write(int v) throws IOException {
+    assert v >= 0;
+    out.writeVInt(v);
+  }
+
+  @Override
+  public Index index() {
+    return new Index();
+  }
+
+  @Override
+  public void close() throws IOException {
+    out.close();
+  }
+
+  @Override
+  public String toString() {
+    return "MockSingleIntIndexOutput fp=" + out.getFilePointer();
+  }
+
+  private class Index extends IntIndexOutput.Index {
+    long fp;
+    long lastFP;
+    @Override
+    public void mark() {
+      fp = out.getFilePointer();
+    }
+    @Override
+    public void copyFrom(IntIndexOutput.Index other, boolean copyLast) {
+      fp = ((Index) other).fp;
+      if (copyLast) {
+        lastFP = ((Index) other).fp;
+      }
+    }
+    @Override
+    public void write(IndexOutput indexOut, boolean absolute)
+      throws IOException {
+      if (absolute) {
+        indexOut.writeVLong(fp);
+      } else {
+        indexOut.writeVLong(fp - lastFP);
+      }
+      lastFP = fp;
+    }
+      
+    @Override
+    public String toString() {
+      return Long.toString(fp);
+    }
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java
new file mode 100644
index 0000000..4ed6afc
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/nestedpulsing/NestedPulsingPostingsFormat.java
@@ -0,0 +1,99 @@
+package org.apache.lucene.codecs.nestedpulsing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.lucene.codecs.BlockTreeTermsReader;
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsReader;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter;
+import org.apache.lucene.codecs.pulsing.PulsingPostingsReader;
+import org.apache.lucene.codecs.pulsing.PulsingPostingsWriter;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.Directory;
+
+/**
+ * Pulsing(1, Pulsing(2, Lucene40))
+ * @lucene.experimental
+ */
+// TODO: if we create PulsingPostingsBaseFormat then we
+// can simplify this? note: I don't like the *BaseFormat
+// hierarchy, maybe we can clean that up...
+public class NestedPulsingPostingsFormat extends PostingsFormat {
+  public NestedPulsingPostingsFormat() {
+    super("NestedPulsing");
+  }
+  
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase docsWriter = new Lucene40PostingsWriter(state);
+
+    PostingsWriterBase pulsingWriterInner = new PulsingPostingsWriter(2, docsWriter);
+    PostingsWriterBase pulsingWriter = new PulsingPostingsWriter(1, pulsingWriterInner);
+    
+    // Terms dict
+    boolean success = false;
+    try {
+      FieldsConsumer ret = new BlockTreeTermsWriter(state, pulsingWriter, 
+          BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        pulsingWriter.close();
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase docsReader = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
+    PostingsReaderBase pulsingReaderInner = new PulsingPostingsReader(docsReader);
+    PostingsReaderBase pulsingReader = new PulsingPostingsReader(pulsingReaderInner);
+    boolean success = false;
+    try {
+      FieldsProducer ret = new BlockTreeTermsReader(
+                                                    state.dir, state.fieldInfos, state.segmentInfo.name,
+                                                    pulsingReader,
+                                                    state.context,
+                                                    state.segmentSuffix,
+                                                    state.termsIndexDivisor);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        pulsingReader.close();
+      }
+    }
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
+    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
+    BlockTreeTermsReader.files(dir, segmentInfo, segmentSuffix, files);
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexFieldsWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexFieldsWriter.java
new file mode 100644
index 0000000..0f0e970
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexFieldsWriter.java
@@ -0,0 +1,223 @@
+package org.apache.lucene.codecs.preflexrw;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.PostingsConsumer;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.codecs.lucene3x.Lucene3xPostingsFormat;
+import org.apache.lucene.codecs.lucene3x.TermInfo;
+import org.apache.lucene.codecs.lucene40.Lucene40SkipListWriter;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+class PreFlexFieldsWriter extends FieldsConsumer {
+
+  private final TermInfosWriter termsOut;
+  private final IndexOutput freqOut;
+  private final IndexOutput proxOut;
+  private final Lucene40SkipListWriter skipListWriter;
+  private final int totalNumDocs;
+
+  public PreFlexFieldsWriter(SegmentWriteState state) throws IOException {
+    termsOut = new TermInfosWriter(state.directory,
+                                   state.segmentName,
+                                   state.fieldInfos,
+                                   state.termIndexInterval);
+
+    boolean success = false;
+    try {
+      final String freqFile = IndexFileNames.segmentFileName(state.segmentName, "", Lucene3xPostingsFormat.FREQ_EXTENSION);
+      freqOut = state.directory.createOutput(freqFile, state.context);
+      totalNumDocs = state.numDocs;
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(termsOut);
+      }
+    }
+
+    success = false;
+    try {
+      if (state.fieldInfos.hasProx()) {
+        final String proxFile = IndexFileNames.segmentFileName(state.segmentName, "", Lucene3xPostingsFormat.PROX_EXTENSION);
+        proxOut = state.directory.createOutput(proxFile, state.context);
+      } else {
+        proxOut = null;
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(termsOut, freqOut);
+      }
+    }
+
+    skipListWriter = new Lucene40SkipListWriter(termsOut.skipInterval,
+                                               termsOut.maxSkipLevels,
+                                               totalNumDocs,
+                                               freqOut,
+                                               proxOut);
+    //System.out.println("\nw start seg=" + segment);
+  }
+
+  @Override
+  public TermsConsumer addField(FieldInfo field) throws IOException {
+    assert field.number != -1;
+    //System.out.println("w field=" + field.name + " storePayload=" + field.storePayloads + " number=" + field.number);
+    return new PreFlexTermsWriter(field);
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.close(termsOut, freqOut, proxOut);
+  }
+
+  private class PreFlexTermsWriter extends TermsConsumer {
+    private final FieldInfo fieldInfo;
+    private final boolean omitTF;
+    private final boolean storePayloads;
+    
+    private final TermInfo termInfo = new TermInfo();
+    private final PostingsWriter postingsWriter = new PostingsWriter();
+
+    public PreFlexTermsWriter(FieldInfo fieldInfo) {
+      this.fieldInfo = fieldInfo;
+      omitTF = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY;
+      storePayloads = fieldInfo.storePayloads;
+    }
+
+    private class PostingsWriter extends PostingsConsumer {
+      private int lastDocID;
+      private int lastPayloadLength = -1;
+      private int lastPosition;
+      private int df;
+
+      public PostingsWriter reset() {
+        df = 0;
+        lastDocID = 0;
+        lastPayloadLength = -1;
+        return this;
+      }
+
+      @Override
+      public void startDoc(int docID, int termDocFreq) throws IOException {
+        //System.out.println("    w doc=" + docID);
+
+        final int delta = docID - lastDocID;
+        if (docID < 0 || (df > 0 && delta <= 0)) {
+          throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " )");
+        }
+
+        if ((++df % termsOut.skipInterval) == 0) {
+          skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength);
+          skipListWriter.bufferSkip(df);
+        }
+
+        lastDocID = docID;
+
+        assert docID < totalNumDocs: "docID=" + docID + " totalNumDocs=" + totalNumDocs;
+
+        if (omitTF) {
+          freqOut.writeVInt(delta);
+        } else {
+          final int code = delta << 1;
+          if (termDocFreq == 1) {
+            freqOut.writeVInt(code|1);
+          } else {
+            freqOut.writeVInt(code);
+            freqOut.writeVInt(termDocFreq);
+          }
+        }
+        lastPosition = 0;
+      }
+
+      @Override
+      public void addPosition(int position, BytesRef payload) throws IOException {
+        assert proxOut != null;
+
+        //System.out.println("      w pos=" + position + " payl=" + payload);
+        final int delta = position - lastPosition;
+        lastPosition = position;
+
+        if (storePayloads) {
+          final int payloadLength = payload == null ? 0 : payload.length;
+          if (payloadLength != lastPayloadLength) {
+            //System.out.println("        write payload len=" + payloadLength);
+            lastPayloadLength = payloadLength;
+            proxOut.writeVInt((delta<<1)|1);
+            proxOut.writeVInt(payloadLength);
+          } else {
+            proxOut.writeVInt(delta << 1);
+          }
+          if (payloadLength > 0) {
+            proxOut.writeBytes(payload.bytes, payload.offset, payload.length);
+          }
+        } else {
+          proxOut.writeVInt(delta);
+        }
+      }
+
+      @Override
+      public void finishDoc() throws IOException {
+      }
+    }
+
+    @Override
+    public PostingsConsumer startTerm(BytesRef text) throws IOException {
+      //System.out.println("  w term=" + text.utf8ToString());
+      skipListWriter.resetSkip();
+      termInfo.freqPointer = freqOut.getFilePointer();
+      if (proxOut != null) {
+        termInfo.proxPointer = proxOut.getFilePointer();
+      }
+      return postingsWriter.reset();
+    }
+
+    @Override
+    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
+      if (stats.docFreq > 0) {
+        long skipPointer = skipListWriter.writeSkip(freqOut);
+        termInfo.docFreq = stats.docFreq;
+        termInfo.skipOffset = (int) (skipPointer - termInfo.freqPointer);
+        //System.out.println("  w finish term=" + text.utf8ToString() + " fnum=" + fieldInfo.number);
+        termsOut.add(fieldInfo.number,
+                     text,
+                     termInfo);
+      }
+    }
+
+    @Override
+    public void finish(long sumTotalTermCount, long sumDocFreq, int docCount) throws IOException {
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() throws IOException {
+      return BytesRef.getUTF8SortedAsUTF16Comparator();
+    }
+  }
+}
\ No newline at end of file
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWCodec.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWCodec.java
new file mode 100644
index 0000000..df48c5c
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWCodec.java
@@ -0,0 +1,39 @@
+package org.apache.lucene.codecs.preflexrw;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.lucene3x.Lucene3xCodec;
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Writes 3.x-like indexes (not perfect emulation yet) for testing only!
+ * @lucene.experimental
+ */
+public class PreFlexRWCodec extends Lucene3xCodec {
+  private final PostingsFormat postings = new PreFlexRWPostingsFormat();
+
+  @Override
+  public PostingsFormat postingsFormat() {
+    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
+      return postings;
+    } else {
+      return super.postingsFormat();
+    }
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWPostingsFormat.java
new file mode 100644
index 0000000..9ae3f5a
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/PreFlexRWPostingsFormat.java
@@ -0,0 +1,76 @@
+package org.apache.lucene.codecs.preflexrw;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.lucene3x.Lucene3xFields;
+import org.apache.lucene.codecs.lucene3x.Lucene3xPostingsFormat;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.util.LuceneTestCase;
+
+/** Codec, only for testing, that can write and read the
+ *  pre-flex index format.
+ *
+ * @lucene.experimental
+ */
+public class PreFlexRWPostingsFormat extends Lucene3xPostingsFormat {
+
+  public PreFlexRWPostingsFormat() {
+    // NOTE: we impersonate the PreFlex codec so that it can
+    // read the segments we write!
+  }
+  
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return new PreFlexFieldsWriter(state);
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+
+    // Whenever IW opens readers, eg for merging, we have to
+    // keep terms order in UTF16:
+
+    return new Lucene3xFields(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.termsIndexDivisor) {
+      @Override
+      protected boolean sortTermsByUnicode() {
+        // We carefully peek into stack track above us: if
+        // we are part of a "merge", we must sort by UTF16:
+        boolean unicodeSortOrder = true;
+
+        StackTraceElement[] trace = new Exception().getStackTrace();
+        for (int i = 0; i < trace.length; i++) {
+          //System.out.println(trace[i].getClassName());
+          if ("merge".equals(trace[i].getMethodName())) {
+            unicodeSortOrder = false;
+            if (LuceneTestCase.VERBOSE) {
+              System.out.println("NOTE: PreFlexRW codec: forcing legacy UTF16 term sort order");
+            }
+            break;
+          }
+        }
+
+        return unicodeSortOrder;
+      }
+    };
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/TermInfosWriter.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/TermInfosWriter.java
new file mode 100644
index 0000000..56d3d93
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/preflexrw/TermInfosWriter.java
@@ -0,0 +1,276 @@
+package org.apache.lucene.codecs.preflexrw;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.codecs.lucene3x.Lucene3xPostingsFormat;
+import org.apache.lucene.codecs.lucene3x.TermInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.UnicodeUtil;
+
+
+/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
+  Directory.  A TermInfos can be written once, in order.  */
+
+final class TermInfosWriter implements Closeable {
+  /** The file format version, a negative number. */
+  public static final int FORMAT = -3;
+
+  // Changed strings to true utf8 with length-in-bytes not
+  // length-in-chars
+  public static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = -4;
+
+  // NOTE: always change this if you switch to a new format!
+  public static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
+
+  private FieldInfos fieldInfos;
+  private IndexOutput output;
+  private TermInfo lastTi = new TermInfo();
+  private long size;
+
+  // TODO: the default values for these two parameters should be settable from
+  // IndexWriter.  However, once that's done, folks will start setting them to
+  // ridiculous values and complaining that things don't work well, as with
+  // mergeFactor.  So, let's wait until a number of folks find that alternate
+  // values work better.  Note that both of these values are stored in the
+  // segment, so that it's safe to change these w/o rebuilding all indexes.
+
+  /** Expert: The fraction of terms in the "dictionary" which should be stored
+   * in RAM.  Smaller values use more memory, but make searching slightly
+   * faster, while larger values use less memory and make searching slightly
+   * slower.  Searching is typically not dominated by dictionary lookup, so
+   * tweaking this is rarely useful.*/
+  int indexInterval = 128;
+
+  /** Expert: The fraction of {@link TermDocs} entries stored in skip tables,
+   * used to accelerate {@link TermDocs#skipTo(int)}.  Larger values result in
+   * smaller indexes, greater acceleration, but fewer accelerable cases, while
+   * smaller values result in bigger indexes, less acceleration and more
+   * accelerable cases. More detailed experiments would be useful here. */
+  int skipInterval = 16;
+  
+  /** Expert: The maximum number of skip levels. Smaller values result in 
+   * slightly smaller indexes, but slower skipping in big posting lists.
+   */
+  int maxSkipLevels = 10;
+
+  private long lastIndexPointer;
+  private boolean isIndex;
+  private final BytesRef lastTerm = new BytesRef();
+  private int lastFieldNumber = -1;
+
+  private TermInfosWriter other;
+
+  TermInfosWriter(Directory directory, String segment, FieldInfos fis,
+                  int interval)
+       throws IOException {
+    initialize(directory, segment, fis, interval, false);
+    boolean success = false;
+    try {
+      other = new TermInfosWriter(directory, segment, fis, interval, true);
+      other.other = this;
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(output);
+
+        try {
+          directory.deleteFile(IndexFileNames.segmentFileName(segment, "",
+              (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
+                  : Lucene3xPostingsFormat.TERMS_EXTENSION)));
+        } catch (IOException ignored) {
+        }
+      }
+    }
+  }
+
+  private TermInfosWriter(Directory directory, String segment, FieldInfos fis,
+                          int interval, boolean isIndex) throws IOException {
+    initialize(directory, segment, fis, interval, isIndex);
+  }
+
+  private void initialize(Directory directory, String segment, FieldInfos fis,
+                          int interval, boolean isi) throws IOException {
+    indexInterval = interval;
+    fieldInfos = fis;
+    isIndex = isi;
+    output = directory.createOutput(IndexFileNames.segmentFileName(segment, "",
+        (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
+            : Lucene3xPostingsFormat.TERMS_EXTENSION)), IOContext.DEFAULT);
+    boolean success = false;
+    try {
+      output.writeInt(FORMAT_CURRENT);              // write format
+      output.writeLong(0);                          // leave space for size
+      output.writeInt(indexInterval);               // write indexInterval
+      output.writeInt(skipInterval);                // write skipInterval
+      output.writeInt(maxSkipLevels);               // write maxSkipLevels
+      assert initUTF16Results();
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(output);
+
+        try {
+          directory.deleteFile(IndexFileNames.segmentFileName(segment, "",
+              (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
+                  : Lucene3xPostingsFormat.TERMS_EXTENSION)));
+        } catch (IOException ignored) {
+        }
+      }
+    }
+  }
+
+  // Currently used only by assert statements
+  CharsRef utf16Result1;
+  CharsRef utf16Result2;
+  private final BytesRef scratchBytes = new BytesRef();
+
+  // Currently used only by assert statements
+  private boolean initUTF16Results() {
+    utf16Result1 = new CharsRef(10);
+    utf16Result2 = new CharsRef(10);
+    return true;
+  }
+
+  // Currently used only by assert statement
+  private int compareToLastTerm(int fieldNumber, BytesRef term) {
+
+    if (lastFieldNumber != fieldNumber) {
+      final int cmp = fieldInfos.fieldName(lastFieldNumber).compareTo(fieldInfos.fieldName(fieldNumber));
+      // If there is a field named "" (empty string) then we
+      // will get 0 on this comparison, yet, it's "OK".  But
+      // it's not OK if two different field numbers map to
+      // the same name.
+      if (cmp != 0 || lastFieldNumber != -1)
+        return cmp;
+    }
+
+    scratchBytes.copyBytes(term);
+    assert lastTerm.offset == 0;
+    UnicodeUtil.UTF8toUTF16(lastTerm.bytes, 0, lastTerm.length, utf16Result1);
+
+    assert scratchBytes.offset == 0;
+    UnicodeUtil.UTF8toUTF16(scratchBytes.bytes, 0, scratchBytes.length, utf16Result2);
+
+    final int len;
+    if (utf16Result1.length < utf16Result2.length)
+      len = utf16Result1.length;
+    else
+      len = utf16Result2.length;
+
+    for(int i=0;i<len;i++) {
+      final char ch1 = utf16Result1.chars[i];
+      final char ch2 = utf16Result2.chars[i];
+      if (ch1 != ch2)
+        return ch1-ch2;
+    }
+    if (utf16Result1.length == 0 && lastFieldNumber == -1) {
+      // If there is a field named "" (empty string) with a term text of "" (empty string) then we
+      // will get 0 on this comparison, yet, it's "OK". 
+      return -1;
+    }
+    return utf16Result1.length - utf16Result2.length;
+  }
+
+  /** Adds a new <<fieldNumber, termBytes>, TermInfo> pair to the set.
+    Term must be lexicographically greater than all previous Terms added.
+    TermInfo pointers must be positive and greater than all previous.*/
+  public void add(int fieldNumber, BytesRef term, TermInfo ti)
+    throws IOException {
+
+    assert compareToLastTerm(fieldNumber, term) < 0 ||
+      (isIndex && term.length == 0 && lastTerm.length == 0) :
+      "Terms are out of order: field=" + fieldInfos.fieldName(fieldNumber) + " (number " + fieldNumber + ")" +
+        " lastField=" + fieldInfos.fieldName(lastFieldNumber) + " (number " + lastFieldNumber + ")" +
+        " text=" + term.utf8ToString() + " lastText=" + lastTerm.utf8ToString();
+
+    assert ti.freqPointer >= lastTi.freqPointer: "freqPointer out of order (" + ti.freqPointer + " < " + lastTi.freqPointer + ")";
+    assert ti.proxPointer >= lastTi.proxPointer: "proxPointer out of order (" + ti.proxPointer + " < " + lastTi.proxPointer + ")";
+
+    if (!isIndex && size % indexInterval == 0)
+      other.add(lastFieldNumber, lastTerm, lastTi);                      // add an index term
+
+    writeTerm(fieldNumber, term);                        // write term
+
+    output.writeVInt(ti.docFreq);                       // write doc freq
+    output.writeVLong(ti.freqPointer - lastTi.freqPointer); // write pointers
+    output.writeVLong(ti.proxPointer - lastTi.proxPointer);
+
+    if (ti.docFreq >= skipInterval) {
+      output.writeVInt(ti.skipOffset);
+    }
+
+    if (isIndex) {
+      output.writeVLong(other.output.getFilePointer() - lastIndexPointer);
+      lastIndexPointer = other.output.getFilePointer(); // write pointer
+    }
+
+    lastFieldNumber = fieldNumber;
+    lastTi.set(ti);
+    size++;
+  }
+
+  private void writeTerm(int fieldNumber, BytesRef term)
+       throws IOException {
+
+    //System.out.println("  tiw.write field=" + fieldNumber + " term=" + term.utf8ToString());
+
+    // TODO: UTF16toUTF8 could tell us this prefix
+    // Compute prefix in common with last term:
+    int start = 0;
+    final int limit = term.length < lastTerm.length ? term.length : lastTerm.length;
+    while(start < limit) {
+      if (term.bytes[start+term.offset] != lastTerm.bytes[start+lastTerm.offset])
+        break;
+      start++;
+    }
+
+    final int length = term.length - start;
+    output.writeVInt(start);                     // write shared prefix length
+    output.writeVInt(length);                  // write delta length
+    output.writeBytes(term.bytes, start+term.offset, length);  // write delta bytes
+    output.writeVInt(fieldNumber); // write field num
+    lastTerm.copyBytes(term);
+  }
+
+  /** Called to complete TermInfos creation. */
+  public void close() throws IOException {
+    try {
+      output.seek(4);          // write size after format
+      output.writeLong(size);
+    } finally {
+      try {
+        output.close();
+      } finally {
+        if (!isIndex) {
+          other.close();
+        }
+      }
+    }
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java
new file mode 100644
index 0000000..155e7b4
--- /dev/null
+++ b/lucene/src/test-framework/java/org/apache/lucene/codecs/ramonly/RAMOnlyPostingsFormat.java
@@ -0,0 +1,580 @@
+package org.apache.lucene.codecs.ramonly;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedMap;
+import java.util.TreeMap;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsConsumer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CodecUtil;
+import org.apache.lucene.util.IOUtils;
+
+/** Stores all postings data in RAM, but writes a small
+ *  token (header + single int) to identify which "slot" the
+ *  index is using in RAM HashMap.
+ *
+ *  NOTE: this codec sorts terms by reverse-unicode-order! */
+
+public class RAMOnlyPostingsFormat extends PostingsFormat {
+
+  // For fun, test that we can override how terms are
+  // sorted, and basic things still work -- this comparator
+  // sorts in reversed unicode code point order:
+  private static final Comparator<BytesRef> reverseUnicodeComparator = new Comparator<BytesRef>() {
+      public int compare(BytesRef t1, BytesRef t2) {
+        byte[] b1 = t1.bytes;
+        byte[] b2 = t2.bytes;
+        int b1Stop;
+        int b1Upto = t1.offset;
+        int b2Upto = t2.offset;
+        if (t1.length < t2.length) {
+          b1Stop = t1.offset + t1.length;
+        } else {
+          b1Stop = t1.offset + t2.length;
+        }
+        while(b1Upto < b1Stop) {
+          final int bb1 = b1[b1Upto++] & 0xff;
+          final int bb2 = b2[b2Upto++] & 0xff;
+          if (bb1 != bb2) {
+            //System.out.println("cmp 1=" + t1 + " 2=" + t2 + " return " + (bb2-bb1));
+            return bb2 - bb1;
+          }
+        }
+
+        // One is prefix of another, or they are equal
+        return t2.length-t1.length;
+      }
+
+      @Override
+      public boolean equals(Object other) {
+        return this == other;
+      }
+    };
+
+  public RAMOnlyPostingsFormat() {
+    super("RAMOnly");
+  }
+    
+  // Postings state:
+  static class RAMPostings extends FieldsProducer {
+    final Map<String,RAMField> fieldToTerms = new TreeMap<String,RAMField>();
+
+    @Override
+    public Terms terms(String field) {
+      return fieldToTerms.get(field);
+    }
+
+    @Override
+    public int getUniqueFieldCount() {
+      return fieldToTerms.size();
+    }
+
+    @Override
+    public FieldsEnum iterator() {
+      return new RAMFieldsEnum(this);
+    }
+
+    @Override
+    public void close() {
+    }
+  } 
+
+  static class RAMField extends Terms {
+    final String field;
+    final SortedMap<String,RAMTerm> termToDocs = new TreeMap<String,RAMTerm>();
+    long sumTotalTermFreq;
+    long sumDocFreq;
+    int docCount;
+
+    RAMField(String field) {
+      this.field = field;
+    }
+
+    @Override
+    public long getUniqueTermCount() {
+      return termToDocs.size();
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return sumTotalTermFreq;
+    }
+      
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return sumDocFreq;
+    }
+      
+    @Override
+    public int getDocCount() throws IOException {
+      return docCount;
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) {
+      return new RAMTermsEnum(RAMOnlyPostingsFormat.RAMField.this);
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return reverseUnicodeComparator;
+    }
+  }
+
+  static class RAMTerm {
+    final String term;
+    long totalTermFreq;
+    final List<RAMDoc> docs = new ArrayList<RAMDoc>();
+    public RAMTerm(String term) {
+      this.term = term;
+    }
+  }
+
+  static class RAMDoc {
+    final int docID;
+    final int[] positions;
+    byte[][] payloads;
+
+    public RAMDoc(int docID, int freq) {
+      this.docID = docID;
+      positions = new int[freq];
+    }
+  }
+
+  // Classes for writing to the postings state
+  private static class RAMFieldsConsumer extends FieldsConsumer {
+
+    private final RAMPostings postings;
+    private final RAMTermsConsumer termsConsumer = new RAMTermsConsumer();
+
+    public RAMFieldsConsumer(RAMPostings postings) {
+      this.postings = postings;
+    }
+
+    @Override
+    public TermsConsumer addField(FieldInfo field) {
+      RAMField ramField = new RAMField(field.name);
+      postings.fieldToTerms.put(field.name, ramField);
+      termsConsumer.reset(ramField);
+      return termsConsumer;
+    }
+
+    @Override
+    public void close() {
+      // TODO: finalize stuff
+    }
+  }
+
+  private static class RAMTermsConsumer extends TermsConsumer {
+    private RAMField field;
+    private final RAMPostingsWriterImpl postingsWriter = new RAMPostingsWriterImpl();
+    RAMTerm current;
+      
+    void reset(RAMField field) {
+      this.field = field;
+    }
+      
+    @Override
+    public PostingsConsumer startTerm(BytesRef text) {
+      final String term = text.utf8ToString();
+      current = new RAMTerm(term);
+      postingsWriter.reset(current);
+      return postingsWriter;
+    }
+
+      
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public void finishTerm(BytesRef text, TermStats stats) {
+      assert stats.docFreq > 0;
+      assert stats.docFreq == current.docs.size();
+      current.totalTermFreq = stats.totalTermFreq;
+      field.termToDocs.put(current.term, current);
+    }
+
+    @Override
+    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) {
+      field.sumTotalTermFreq = sumTotalTermFreq;
+      field.sumDocFreq = sumDocFreq;
+      field.docCount = docCount;
+    }
+  }
+
+  public static class RAMPostingsWriterImpl extends PostingsConsumer {
+    private RAMTerm term;
+    private RAMDoc current;
+    private int posUpto = 0;
+
+    public void reset(RAMTerm term) {
+      this.term = term;
+    }
+
+    @Override
+    public void startDoc(int docID, int freq) {
+      current = new RAMDoc(docID, freq);
+      term.docs.add(current);
+      posUpto = 0;
+    }
+
+    @Override
+    public void addPosition(int position, BytesRef payload) {
+      current.positions[posUpto] = position;
+      if (payload != null && payload.length > 0) {
+        if (current.payloads == null) {
+          current.payloads = new byte[current.positions.length][];
+        }
+        byte[] bytes = current.payloads[posUpto] = new byte[payload.length];
+        System.arraycopy(payload.bytes, payload.offset, bytes, 0, payload.length);
+      }
+      posUpto++;
+    }
+
+    @Override
+    public void finishDoc() {
+      assert posUpto == current.positions.length;
+    }
+  }
+
+  // Classes for reading from the postings state
+  static class RAMFieldsEnum extends FieldsEnum {
+    private final RAMPostings postings;
+    private final Iterator<String> it;
+    private String current;
+
+    public RAMFieldsEnum(RAMPostings postings) {
+      this.postings = postings;
+      this.it = postings.fieldToTerms.keySet().iterator();
+    }
+
+    @Override
+    public String next() {
+      if (it.hasNext()) {
+        current = it.next();
+      } else {
+        current = null;
+      }
+      return current;
+    }
+
+    @Override
+    public Terms terms() {
+      return postings.fieldToTerms.get(current);
+    }
+  }
+
+  static class RAMTermsEnum extends TermsEnum {
+    Iterator<String> it;
+    String current;
+    private final RAMField ramField;
+
+    public RAMTermsEnum(RAMField field) {
+      this.ramField = field;
+    }
+      
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public BytesRef next() {
+      if (it == null) {
+        if (current == null) {
+          it = ramField.termToDocs.keySet().iterator();
+        } else {
+          it = ramField.termToDocs.tailMap(current).keySet().iterator();
+        }
+      }
+      if (it.hasNext()) {
+        current = it.next();
+        return new BytesRef(current);
+      } else {
+        return null;
+      }
+    }
+
+    @Override
+    public SeekStatus seekCeil(BytesRef term, boolean useCache) {
+      current = term.utf8ToString();
+      it = null;
+      if (ramField.termToDocs.containsKey(current)) {
+        return SeekStatus.FOUND;
+      } else {
+        if (current.compareTo(ramField.termToDocs.lastKey()) > 0) {
+          return SeekStatus.END;
+        } else {
+          return SeekStatus.NOT_FOUND;
+        }
+      }
+    }
+
+    @Override
+    public void seekExact(long ord) {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public long ord() {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public BytesRef term() {
+      // TODO: reuse BytesRef
+      return new BytesRef(current);
+    }
+
+    @Override
+    public int docFreq() {
+      return ramField.termToDocs.get(current).docs.size();
+    }
+
+    @Override
+    public long totalTermFreq() {
+      return ramField.termToDocs.get(current).totalTermFreq;
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) {
+      return new RAMDocsEnum(ramField.termToDocs.get(current), liveDocs);
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) {
+      return new RAMDocsAndPositionsEnum(ramField.termToDocs.get(current), liveDocs);
+    }
+  }
+
+  private static class RAMDocsEnum extends DocsEnum {
+    private final RAMTerm ramTerm;
+    private final Bits liveDocs;
+    private RAMDoc current;
+    int upto = -1;
+    int posUpto = 0;
+
+    public RAMDocsEnum(RAMTerm ramTerm, Bits liveDocs) {
+      this.ramTerm = ramTerm;
+      this.liveDocs = liveDocs;
+    }
+
+    @Override
+    public int advance(int targetDocID) {
+      do {
+        nextDoc();
+      } while (upto < ramTerm.docs.size() && current.docID < targetDocID);
+      return NO_MORE_DOCS;
+    }
+
+    // TODO: override bulk read, for better perf
+    @Override
+    public int nextDoc() {
+      while(true) {
+        upto++;
+        if (upto < ramTerm.docs.size()) {
+          current = ramTerm.docs.get(upto);
+          if (liveDocs == null || liveDocs.get(current.docID)) {
+            posUpto = 0;
+            return current.docID;
+          }
+        } else {
+          return NO_MORE_DOCS;
+        }
+      }
+    }
+
+    @Override
+    public int freq() {
+      return current.positions.length;
+    }
+
+    @Override
+    public int docID() {
+      return current.docID;
+    }
+  }
+
+  private static class RAMDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private final RAMTerm ramTerm;
+    private final Bits liveDocs;
+    private RAMDoc current;
+    int upto = -1;
+    int posUpto = 0;
+
+    public RAMDocsAndPositionsEnum(RAMTerm ramTerm, Bits liveDocs) {
+      this.ramTerm = ramTerm;
+      this.liveDocs = liveDocs;
+    }
+
+    @Override
+    public int advance(int targetDocID) {
+      do {
+        nextDoc();
+      } while (upto < ramTerm.docs.size() && current.docID < targetDocID);
+      return NO_MORE_DOCS;
+    }
+
+    // TODO: override bulk read, for better perf
+    @Override
+    public int nextDoc() {
+      while(true) {
+        upto++;
+        if (upto < ramTerm.docs.size()) {
+          current = ramTerm.docs.get(upto);
+          if (liveDocs == null || liveDocs.get(current.docID)) {
+            posUpto = 0;
+            return current.docID;
+          }
+        } else {
+          return NO_MORE_DOCS;
+        }
+      }
+    }
+
+    @Override
+    public int freq() {
+      return current.positions.length;
+    }
+
+    @Override
+    public int docID() {
+      return current.docID;
+    }
+
+    @Override
+    public int nextPosition() {
+      return current.positions[posUpto++];
+    }
+
+    @Override
+    public boolean hasPayload() {
+      return current.payloads != null && current.payloads[posUpto-1] != null;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      return new BytesRef(current.payloads[posUpto-1]);
+    }
+  }
+
+  // Holds all indexes created, keyed by the ID assigned in fieldsConsumer
+  private final Map<Integer,RAMPostings> state = new HashMap<Integer,RAMPostings>();
+
+  private final AtomicInteger nextID = new AtomicInteger();
+
+  private final String RAM_ONLY_NAME = "RAMOnly";
+  private final static int VERSION_START = 0;
+  private final static int VERSION_LATEST = VERSION_START;
+
+  private static final String ID_EXTENSION = "id";
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState writeState) throws IOException {
+    final int id = nextID.getAndIncrement();
+
+    // TODO -- ok to do this up front instead of
+    // on close....?  should be ok?
+    // Write our ID:
+    final String idFileName = IndexFileNames.segmentFileName(writeState.segmentName, writeState.segmentSuffix, ID_EXTENSION);
+    IndexOutput out = writeState.directory.createOutput(idFileName, writeState.context);
+    boolean success = false;
+    try {
+      CodecUtil.writeHeader(out, RAM_ONLY_NAME, VERSION_LATEST);
+      out.writeVInt(id);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      } else {
+        IOUtils.close(out);
+      }
+    }
+    
+    final RAMPostings postings = new RAMPostings();
+    final RAMFieldsConsumer consumer = new RAMFieldsConsumer(postings);
+
+    synchronized(state) {
+      state.put(id, postings);
+    }
+    return consumer;
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState readState)
+    throws IOException {
+
+    // Load our ID:
+    final String idFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, ID_EXTENSION);
+    IndexInput in = readState.dir.openInput(idFileName, readState.context);
+    boolean success = false;
+    final int id;
+    try {
+      CodecUtil.checkHeader(in, RAM_ONLY_NAME, VERSION_START, VERSION_LATEST);
+      id = in.readVInt();
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(in);
+      } else {
+        IOUtils.close(in);
+      }
+    }
+    
+    synchronized(state) {
+      return state.get(id);
+    }
+  }
+
+  @Override
+  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) {
+    final String idFileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, ID_EXTENSION);
+    files.add(idFileName);
+  }
+}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/RandomCodec.java b/lucene/src/test-framework/java/org/apache/lucene/index/RandomCodec.java
index 5d2d1af..0271efa 100644
--- a/lucene/src/test-framework/java/org/apache/lucene/index/RandomCodec.java
+++ b/lucene/src/test-framework/java/org/apache/lucene/index/RandomCodec.java
@@ -25,18 +25,18 @@ import java.util.Locale;
 import java.util.Map;
 import java.util.Random;
 
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40Codec;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat;
-import org.apache.lucene.index.codecs.lucene40ords.Lucene40WithOrds;
-import org.apache.lucene.index.codecs.memory.MemoryPostingsFormat;
-import org.apache.lucene.index.codecs.mockintblock.MockFixedIntBlockPostingsFormat;
-import org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockPostingsFormat;
-import org.apache.lucene.index.codecs.mockrandom.MockRandomPostingsFormat;
-import org.apache.lucene.index.codecs.mocksep.MockSepPostingsFormat;
-import org.apache.lucene.index.codecs.nestedpulsing.NestedPulsingPostingsFormat;
-import org.apache.lucene.index.codecs.pulsing.Pulsing40PostingsFormat;
-import org.apache.lucene.index.codecs.simpletext.SimpleTextPostingsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40Codec;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
+import org.apache.lucene.codecs.lucene40ords.Lucene40WithOrds;
+import org.apache.lucene.codecs.memory.MemoryPostingsFormat;
+import org.apache.lucene.codecs.mockintblock.MockFixedIntBlockPostingsFormat;
+import org.apache.lucene.codecs.mockintblock.MockVariableIntBlockPostingsFormat;
+import org.apache.lucene.codecs.mockrandom.MockRandomPostingsFormat;
+import org.apache.lucene.codecs.mocksep.MockSepPostingsFormat;
+import org.apache.lucene.codecs.nestedpulsing.NestedPulsingPostingsFormat;
+import org.apache.lucene.codecs.pulsing.Pulsing40PostingsFormat;
+import org.apache.lucene.codecs.simpletext.SimpleTextPostingsFormat;
 import org.apache.lucene.util._TestUtil;
 
 /**
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/RandomIndexWriter.java b/lucene/src/test-framework/java/org/apache/lucene/index/RandomIndexWriter.java
index ce6a5f1..4d126e3 100644
--- a/lucene/src/test-framework/java/org/apache/lucene/index/RandomIndexWriter.java
+++ b/lucene/src/test-framework/java/org/apache/lucene/index/RandomIndexWriter.java
@@ -24,10 +24,10 @@ import java.util.Random;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.DocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.IndexWriter; // javadoc
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.store.Directory;
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/lucene40ords/Lucene40WithOrds.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/lucene40ords/Lucene40WithOrds.java
deleted file mode 100644
index 728c361..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/lucene40ords/Lucene40WithOrds.java
+++ /dev/null
@@ -1,145 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40ords;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.BlockTermsReader;
-import org.apache.lucene.index.codecs.BlockTermsWriter;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.TermsIndexReaderBase;
-import org.apache.lucene.index.codecs.TermsIndexWriterBase;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsReader;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-
-// TODO: we could make separate base class that can wrapp
-// any PostingsBaseFormat and make it ord-able...
-
-public class Lucene40WithOrds extends PostingsFormat {
-    
-  public Lucene40WithOrds() {
-    super("Lucene40WithOrds");
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docs = new Lucene40PostingsWriter(state);
-
-    // TODO: should we make the terms index more easily
-    // pluggable?  Ie so that this codec would record which
-    // index impl was used, and switch on loading?
-    // Or... you must make a new Codec for this?
-    TermsIndexWriterBase indexWriter;
-    boolean success = false;
-    try {
-      indexWriter = new FixedGapTermsIndexWriter(state);
-      success = true;
-    } finally {
-      if (!success) {
-        docs.close();
-      }
-    }
-
-    success = false;
-    try {
-      // Must use BlockTermsWriter (not BlockTree) because
-      // BlockTree doens't support ords (yet)...
-      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, docs);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          docs.close();
-        } finally {
-          indexWriter.close();
-        }
-      }
-    }
-  }
-
-  public final static int TERMS_CACHE_SIZE = 1024;
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
-    TermsIndexReaderBase indexReader;
-
-    boolean success = false;
-    try {
-      indexReader = new FixedGapTermsIndexReader(state.dir,
-                                                 state.fieldInfos,
-                                                 state.segmentInfo.name,
-                                                 state.termsIndexDivisor,
-                                                 BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                 state.segmentSuffix, state.context);
-      success = true;
-    } finally {
-      if (!success) {
-        postings.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
-                                                state.fieldInfos,
-                                                state.segmentInfo.name,
-                                                postings,
-                                                state.context,
-                                                TERMS_CACHE_SIZE,
-                                                state.segmentSuffix);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postings.close();
-        } finally {
-          indexReader.close();
-        }
-      }
-    }
-  }
-
-  /** Extension of freq postings file */
-  static final String FREQ_EXTENSION = "frq";
-
-  /** Extension of prox postings file */
-  static final String PROX_EXTENSION = "prx";
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
-    BlockTermsReader.files(dir, segmentInfo, segmentSuffix, files);
-    FixedGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
deleted file mode 100644
index 427ebb2..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
+++ /dev/null
@@ -1,210 +0,0 @@
-package org.apache.lucene.index.codecs.mockintblock;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.sep.IntStreamFactory;
-import org.apache.lucene.index.codecs.sep.IntIndexInput;
-import org.apache.lucene.index.codecs.sep.IntIndexOutput;
-import org.apache.lucene.index.codecs.sep.SepPostingsReader;
-import org.apache.lucene.index.codecs.sep.SepPostingsWriter;
-import org.apache.lucene.index.codecs.intblock.FixedIntBlockIndexInput;
-import org.apache.lucene.index.codecs.intblock.FixedIntBlockIndexOutput;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.BlockTermsReader;
-import org.apache.lucene.index.codecs.BlockTermsWriter;
-import org.apache.lucene.index.codecs.TermsIndexReaderBase;
-import org.apache.lucene.index.codecs.TermsIndexWriterBase;
-import org.apache.lucene.store.*;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * A silly test codec to verify core support for fixed
- * sized int block encoders is working.  The int encoder
- * used here just writes each block as a series of vInt.
- */
-
-public class MockFixedIntBlockPostingsFormat extends PostingsFormat {
-
-  private final int blockSize;
-
-  public MockFixedIntBlockPostingsFormat() {
-    this(1);
-  }
-
-  public MockFixedIntBlockPostingsFormat(int blockSize) {
-    super("MockFixedIntBlock");
-    this.blockSize = blockSize;
-  }
-
-  @Override
-  public String toString() {
-    return getName() + "(blockSize=" + blockSize + ")";
-  }
-
-  // only for testing
-  public IntStreamFactory getIntFactory() {
-    return new MockIntFactory(blockSize);
-  }
-
-  public static class MockIntFactory extends IntStreamFactory {
-    private final int blockSize;
-
-    public MockIntFactory(int blockSize) {
-      this.blockSize = blockSize;
-    }
-
-    @Override
-    public IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException {
-      return new FixedIntBlockIndexInput(dir.openInput(fileName, context)) {
-
-        @Override
-        protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) throws IOException {
-          return new BlockReader() {
-            public void seek(long pos) {}
-            public void readBlock() throws IOException {
-              for(int i=0;i<buffer.length;i++) {
-                buffer[i] = in.readVInt();
-              }
-            }
-          };
-        }
-      };
-    }
-
-    @Override
-    public IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException {
-      IndexOutput out = dir.createOutput(fileName, context);
-      boolean success = false;
-      try {
-        FixedIntBlockIndexOutput ret = new FixedIntBlockIndexOutput(out, blockSize) {
-          @Override
-          protected void flushBlock() throws IOException {
-            for(int i=0;i<buffer.length;i++) {
-              assert buffer[i] >= 0;
-              out.writeVInt(buffer[i]);
-            }
-          }
-        };
-        success = true;
-        return ret;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(out);
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase postingsWriter = new SepPostingsWriter(state, new MockIntFactory(blockSize));
-
-    boolean success = false;
-    TermsIndexWriterBase indexWriter;
-    try {
-      indexWriter = new FixedGapTermsIndexWriter(state);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsWriter.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsWriter.close();
-        } finally {
-          indexWriter.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postingsReader = new SepPostingsReader(state.dir,
-                                                              state.segmentInfo,
-                                                              state.context,
-                                                              new MockIntFactory(blockSize), state.segmentSuffix);
-
-    TermsIndexReaderBase indexReader;
-    boolean success = false;
-    try {
-      indexReader = new FixedGapTermsIndexReader(state.dir,
-                                                       state.fieldInfos,
-                                                       state.segmentInfo.name,
-                                                       state.termsIndexDivisor,
-                                                       BytesRef.getUTF8SortedAsUnicodeComparator(), state.segmentSuffix,
-                                                       IOContext.DEFAULT);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsReader.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
-                                                state.fieldInfos,
-                                                state.segmentInfo.name,
-                                                postingsReader,
-                                                state.context,
-                                                Lucene40PostingsFormat.TERMS_CACHE_SIZE,
-                                                state.segmentSuffix);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsReader.close();
-        } finally {
-          indexReader.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTermsReader.files(dir, segmentInfo, segmentSuffix, files);
-    FixedGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
deleted file mode 100644
index 6e3cde7..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
+++ /dev/null
@@ -1,233 +0,0 @@
-package org.apache.lucene.index.codecs.mockintblock;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.sep.IntStreamFactory;
-import org.apache.lucene.index.codecs.sep.IntIndexInput;
-import org.apache.lucene.index.codecs.sep.IntIndexOutput;
-import org.apache.lucene.index.codecs.sep.SepPostingsReader;
-import org.apache.lucene.index.codecs.sep.SepPostingsWriter;
-import org.apache.lucene.index.codecs.intblock.VariableIntBlockIndexInput;
-import org.apache.lucene.index.codecs.intblock.VariableIntBlockIndexOutput;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.BlockTermsReader;
-import org.apache.lucene.index.codecs.BlockTermsWriter;
-import org.apache.lucene.index.codecs.TermsIndexReaderBase;
-import org.apache.lucene.index.codecs.TermsIndexWriterBase;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * A silly test codec to verify core support for variable
- * sized int block encoders is working.  The int encoder
- * used here writes baseBlockSize ints at once, if the first
- * int is <= 3, else 2*baseBlockSize.
- */
-
-public class MockVariableIntBlockPostingsFormat extends PostingsFormat {
-  private final int baseBlockSize;
-  
-  public MockVariableIntBlockPostingsFormat() {
-    this(1);
-  }
-
-  public MockVariableIntBlockPostingsFormat(int baseBlockSize) {
-    super("MockVariableIntBlock");
-    this.baseBlockSize = baseBlockSize;
-  }
-
-  @Override
-  public String toString() {
-    return getName() + "(baseBlockSize="+ baseBlockSize + ")";
-  }
-
-  public static class MockIntFactory extends IntStreamFactory {
-
-    private final int baseBlockSize;
-
-    public MockIntFactory(int baseBlockSize) {
-      this.baseBlockSize = baseBlockSize;
-    }
-
-    @Override
-    public IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException {
-      final IndexInput in = dir.openInput(fileName, context);
-      final int baseBlockSize = in.readInt();
-      return new VariableIntBlockIndexInput(in) {
-
-        @Override
-        protected BlockReader getBlockReader(final IndexInput in, final int[] buffer) throws IOException {
-          return new BlockReader() {
-            public void seek(long pos) {}
-            public int readBlock() throws IOException {
-              buffer[0] = in.readVInt();
-              final int count = buffer[0] <= 3 ? baseBlockSize-1 : 2*baseBlockSize-1;
-              assert buffer.length >= count: "buffer.length=" + buffer.length + " count=" + count;
-              for(int i=0;i<count;i++) {
-                buffer[i+1] = in.readVInt();
-              }
-              return 1+count;
-            }
-          };
-        }
-      };
-    }
-
-    @Override
-    public IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException {
-      final IndexOutput out = dir.createOutput(fileName, context);
-      boolean success = false;
-      try {
-        out.writeInt(baseBlockSize);
-        VariableIntBlockIndexOutput ret = new VariableIntBlockIndexOutput(out, 2*baseBlockSize) {
-          int pendingCount;
-          final int[] buffer = new int[2+2*baseBlockSize];
-          
-          @Override
-          protected int add(int value) throws IOException {
-            assert value >= 0;
-            buffer[pendingCount++] = value;
-            // silly variable block length int encoder: if
-            // first value <= 3, we write N vints at once;
-            // else, 2*N
-            final int flushAt = buffer[0] <= 3 ? baseBlockSize : 2*baseBlockSize;
-            
-            // intentionally be non-causal here:
-            if (pendingCount == flushAt+1) {
-              for(int i=0;i<flushAt;i++) {
-                out.writeVInt(buffer[i]);
-              }
-              buffer[0] = buffer[flushAt];
-              pendingCount = 1;
-              return flushAt;
-            } else {
-              return 0;
-            }
-          }
-        };
-        success = true;
-        return ret;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(out);
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase postingsWriter = new SepPostingsWriter(state, new MockIntFactory(baseBlockSize));
-
-    boolean success = false;
-    TermsIndexWriterBase indexWriter;
-    try {
-      indexWriter = new FixedGapTermsIndexWriter(state);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsWriter.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsWriter.close();
-        } finally {
-          indexWriter.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postingsReader = new SepPostingsReader(state.dir,
-                                                              state.segmentInfo,
-                                                              state.context,
-                                                              new MockIntFactory(baseBlockSize), state.segmentSuffix);
-
-    TermsIndexReaderBase indexReader;
-    boolean success = false;
-    try {
-      indexReader = new FixedGapTermsIndexReader(state.dir,
-                                                       state.fieldInfos,
-                                                       state.segmentInfo.name,
-                                                       state.termsIndexDivisor,
-                                                       BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                       state.segmentSuffix, state.context);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsReader.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
-                                                state.fieldInfos,
-                                                state.segmentInfo.name,
-                                                postingsReader,
-                                                state.context,
-                                                Lucene40PostingsFormat.TERMS_CACHE_SIZE,
-                                                state.segmentSuffix);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsReader.close();
-        } finally {
-          indexReader.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTermsReader.files(dir, segmentInfo, segmentSuffix, files);
-    FixedGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mockrandom/MockRandomPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mockrandom/MockRandomPostingsFormat.java
deleted file mode 100644
index 898229f..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mockrandom/MockRandomPostingsFormat.java
+++ /dev/null
@@ -1,435 +0,0 @@
-package org.apache.lucene.index.codecs.mockrandom;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Random;
-import java.util.Set;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.BlockTreeTermsReader;
-import org.apache.lucene.index.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.index.codecs.BlockTermsReader;
-import org.apache.lucene.index.codecs.BlockTermsWriter;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.index.codecs.TermsIndexReaderBase;
-import org.apache.lucene.index.codecs.TermsIndexWriterBase;
-import org.apache.lucene.index.codecs.VariableGapTermsIndexReader;
-import org.apache.lucene.index.codecs.VariableGapTermsIndexWriter;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsReader;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsWriter;
-import org.apache.lucene.index.codecs.mockintblock.MockFixedIntBlockPostingsFormat;
-import org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockPostingsFormat;
-import org.apache.lucene.index.codecs.mocksep.MockSingleIntFactory;
-import org.apache.lucene.index.codecs.pulsing.PulsingPostingsReader;
-import org.apache.lucene.index.codecs.pulsing.PulsingPostingsWriter;
-import org.apache.lucene.index.codecs.sep.IntIndexInput;
-import org.apache.lucene.index.codecs.sep.IntIndexOutput;
-import org.apache.lucene.index.codecs.sep.IntStreamFactory;
-import org.apache.lucene.index.codecs.sep.SepPostingsReader;
-import org.apache.lucene.index.codecs.sep.SepPostingsWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-
-/**
- * Randomly combines terms index impl w/ postings impls.
- */
-
-public class MockRandomPostingsFormat extends PostingsFormat {
-  private final Random seedRandom;
-  private final String SEED_EXT = "sd";
-  
-  public MockRandomPostingsFormat() {
-    // just for reading, we are gonna setSeed from the .seed file... right?
-    this(new Random());
-  }
-  
-  public MockRandomPostingsFormat(Random random) {
-    super("MockRandom");
-    this.seedRandom = new Random(random.nextLong());
-  }
-
-  // Chooses random IntStreamFactory depending on file's extension
-  private static class MockIntStreamFactory extends IntStreamFactory {
-    private final int salt;
-    private final List<IntStreamFactory> delegates = new ArrayList<IntStreamFactory>();
-
-    public MockIntStreamFactory(Random random) {
-      salt = random.nextInt();
-      delegates.add(new MockSingleIntFactory());
-      final int blockSize = _TestUtil.nextInt(random, 1, 2000);
-      delegates.add(new MockFixedIntBlockPostingsFormat.MockIntFactory(blockSize));
-      final int baseBlockSize = _TestUtil.nextInt(random, 1, 127);
-      delegates.add(new MockVariableIntBlockPostingsFormat.MockIntFactory(baseBlockSize));
-      // TODO: others
-    }
-
-    private static String getExtension(String fileName) {
-      final int idx = fileName.indexOf('.');
-      assert idx != -1;
-      return fileName.substring(idx);
-    }
-
-    @Override
-    public IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException {
-      // Must only use extension, because IW.addIndexes can
-      // rename segment!
-      final IntStreamFactory f = delegates.get((Math.abs(salt ^ getExtension(fileName).hashCode())) % delegates.size());
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: read using int factory " + f + " from fileName=" + fileName);
-      }
-      return f.openInput(dir, fileName, context);
-    }
-
-    @Override
-    public IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException {
-      final IntStreamFactory f = delegates.get((Math.abs(salt ^ getExtension(fileName).hashCode())) % delegates.size());
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: write using int factory " + f + " to fileName=" + fileName);
-      }
-      return f.createOutput(dir, fileName, context);
-    }
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    // we pull this before the seed intentionally: because its not consumed at runtime
-    // (the skipInterval is written into postings header)
-    int skipInterval = _TestUtil.nextInt(seedRandom, 2, 10);
-    
-    if (LuceneTestCase.VERBOSE) {
-      System.out.println("MockRandomCodec: skipInterval=" + skipInterval);
-    }
-    
-    final long seed = seedRandom.nextLong();
-
-    if (LuceneTestCase.VERBOSE) {
-      System.out.println("MockRandomCodec: writing to seg=" + state.segmentName + " formatID=" + state.segmentSuffix + " seed=" + seed);
-    }
-
-    final String seedFileName = IndexFileNames.segmentFileName(state.segmentName, state.segmentSuffix, SEED_EXT);
-    final IndexOutput out = state.directory.createOutput(seedFileName, state.context);
-    try {
-      out.writeLong(seed);
-    } finally {
-      out.close();
-    }
-
-    final Random random = new Random(seed);
-    
-    random.nextInt(); // consume a random for buffersize
-
-    PostingsWriterBase postingsWriter;
-    if (random.nextBoolean()) {
-      postingsWriter = new SepPostingsWriter(state, new MockIntStreamFactory(random), skipInterval);
-    } else {
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: writing Standard postings");
-      }
-      postingsWriter = new Lucene40PostingsWriter(state, skipInterval);
-    }
-
-    if (random.nextBoolean()) {
-      final int totTFCutoff = _TestUtil.nextInt(random, 1, 20);
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: writing pulsing postings with totTFCutoff=" + totTFCutoff);
-      }
-      postingsWriter = new PulsingPostingsWriter(totTFCutoff, postingsWriter);
-    }
-
-    final FieldsConsumer fields;
-
-    if (random.nextBoolean()) {
-      // Use BlockTree terms dict
-
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: writing BlockTree terms dict");
-      }
-
-      // TODO: would be nice to allow 1 but this is very
-      // slow to write
-      final int minTermsInBlock = _TestUtil.nextInt(random, 2, 100);
-      final int maxTermsInBlock = Math.max(2, (minTermsInBlock-1)*2 + random.nextInt(100));
-
-      boolean success = false;
-      try {
-        fields = new BlockTreeTermsWriter(state, postingsWriter, minTermsInBlock, maxTermsInBlock);
-        success = true;
-      } finally {
-        if (!success) {
-          postingsWriter.close();
-        }
-      }
-    } else {
-
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: writing Block terms dict");
-      }
-
-      boolean success = false;
-
-      final TermsIndexWriterBase indexWriter;
-      try {
-        if (random.nextBoolean()) {
-          state.termIndexInterval = _TestUtil.nextInt(random, 1, 100);
-          if (LuceneTestCase.VERBOSE) {
-            System.out.println("MockRandomCodec: fixed-gap terms index (tii=" + state.termIndexInterval + ")");
-          }
-          indexWriter = new FixedGapTermsIndexWriter(state);
-        } else {
-          final VariableGapTermsIndexWriter.IndexTermSelector selector;
-          final int n2 = random.nextInt(3);
-          if (n2 == 0) {
-            final int tii = _TestUtil.nextInt(random, 1, 100);
-            selector = new VariableGapTermsIndexWriter.EveryNTermSelector(tii);
-           if (LuceneTestCase.VERBOSE) {
-              System.out.println("MockRandomCodec: variable-gap terms index (tii=" + tii + ")");
-            }
-          } else if (n2 == 1) {
-            final int docFreqThresh = _TestUtil.nextInt(random, 2, 100);
-            final int tii = _TestUtil.nextInt(random, 1, 100);
-            selector = new VariableGapTermsIndexWriter.EveryNOrDocFreqTermSelector(docFreqThresh, tii);
-          } else {
-            final long seed2 = random.nextLong();
-            final int gap = _TestUtil.nextInt(random, 2, 40);
-            if (LuceneTestCase.VERBOSE) {
-             System.out.println("MockRandomCodec: random-gap terms index (max gap=" + gap + ")");
-            }
-           selector = new VariableGapTermsIndexWriter.IndexTermSelector() {
-                final Random rand = new Random(seed2);
-
-                @Override
-                public boolean isIndexTerm(BytesRef term, TermStats stats) {
-                  return rand.nextInt(gap) == gap/2;
-                }
-
-                @Override
-                  public void newField(FieldInfo fieldInfo) {
-                }
-              };
-          }
-          indexWriter = new VariableGapTermsIndexWriter(state, selector);
-        }
-        success = true;
-      } finally {
-        if (!success) {
-          postingsWriter.close();
-        }
-      }
-
-      success = false;
-      try {
-        fields = new BlockTermsWriter(indexWriter, state, postingsWriter);
-        success = true;
-      } finally {
-        if (!success) {
-          try {
-            postingsWriter.close();
-          } finally {
-            indexWriter.close();
-          }
-        }
-      }
-    }
-
-    return fields;
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-
-    final String seedFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, SEED_EXT);
-    final IndexInput in = state.dir.openInput(seedFileName, state.context);
-    final long seed = in.readLong();
-    if (LuceneTestCase.VERBOSE) {
-      System.out.println("MockRandomCodec: reading from seg=" + state.segmentInfo.name + " formatID=" + state.segmentSuffix + " seed=" + seed);
-    }
-    in.close();
-
-    final Random random = new Random(seed);
-    
-    int readBufferSize = _TestUtil.nextInt(random, 1, 4096);
-    if (LuceneTestCase.VERBOSE) {
-      System.out.println("MockRandomCodec: readBufferSize=" + readBufferSize);
-    }
-
-    PostingsReaderBase postingsReader;
-
-    if (random.nextBoolean()) {
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: reading Sep postings");
-      }
-      postingsReader = new SepPostingsReader(state.dir, state.segmentInfo,
-                                             state.context, new MockIntStreamFactory(random), state.segmentSuffix);
-    } else {
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: reading Standard postings");
-      }
-      postingsReader = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
-    }
-
-    if (random.nextBoolean()) {
-      final int totTFCutoff = _TestUtil.nextInt(random, 1, 20);
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: reading pulsing postings with totTFCutoff=" + totTFCutoff);
-      }
-      postingsReader = new PulsingPostingsReader(postingsReader);
-    }
-
-    final FieldsProducer fields;
-
-    if (random.nextBoolean()) {
-      // Use BlockTree terms dict
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: reading BlockTree terms dict");
-      }
-
-      boolean success = false;
-      try {
-        fields = new BlockTreeTermsReader(state.dir,
-                                          state.fieldInfos,
-                                          state.segmentInfo.name,
-                                          postingsReader,
-                                          state.context,
-                                          state.segmentSuffix,
-                                          state.termsIndexDivisor);
-        success = true;
-      } finally {
-        if (!success) {
-          postingsReader.close();
-        }
-      }
-    } else {
-
-      if (LuceneTestCase.VERBOSE) {
-        System.out.println("MockRandomCodec: reading Block terms dict");
-      }
-      final TermsIndexReaderBase indexReader;
-      boolean success = false;
-      try {
-        final boolean doFixedGap = random.nextBoolean();
-
-        // randomness diverges from writer, here:
-        if (state.termsIndexDivisor != -1) {
-          state.termsIndexDivisor = _TestUtil.nextInt(random, 1, 10);
-        }
-
-        if (doFixedGap) {
-          // if termsIndexDivisor is set to -1, we should not touch it. It means a
-          // test explicitly instructed not to load the terms index.
-          if (LuceneTestCase.VERBOSE) {
-            System.out.println("MockRandomCodec: fixed-gap terms index (divisor=" + state.termsIndexDivisor + ")");
-          }
-          indexReader = new FixedGapTermsIndexReader(state.dir,
-                                                     state.fieldInfos,
-                                                     state.segmentInfo.name,
-                                                     state.termsIndexDivisor,
-                                                     BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                     state.segmentSuffix, state.context);
-        } else {
-          final int n2 = random.nextInt(3);
-          if (n2 == 1) {
-            random.nextInt();
-          } else if (n2 == 2) {
-            random.nextLong();
-          }
-          if (LuceneTestCase.VERBOSE) {
-            System.out.println("MockRandomCodec: variable-gap terms index (divisor=" + state.termsIndexDivisor + ")");
-          }
-          indexReader = new VariableGapTermsIndexReader(state.dir,
-                                                        state.fieldInfos,
-                                                        state.segmentInfo.name,
-                                                        state.termsIndexDivisor,
-                                                        state.segmentSuffix, state.context);
-
-        }
-
-        success = true;
-      } finally {
-        if (!success) {
-          postingsReader.close();
-        }
-      }
-
-      final int termsCacheSize = _TestUtil.nextInt(random, 1, 1024);
-
-      success = false;
-      try {
-        fields = new BlockTermsReader(indexReader,
-                                      state.dir,
-                                      state.fieldInfos,
-                                      state.segmentInfo.name,
-                                      postingsReader,
-                                      state.context,
-                                      termsCacheSize,
-                                      state.segmentSuffix);
-        success = true;
-      } finally {
-        if (!success) {
-          try {
-            postingsReader.close();
-          } finally {
-            indexReader.close();
-          }
-        }
-      }
-    }
-
-    return fields;
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    final String seedFileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SEED_EXT);    
-    files.add(seedFileName);
-    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
-    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
-    BlockTermsReader.files(dir, segmentInfo, segmentSuffix, files);
-    BlockTreeTermsReader.files(dir, segmentInfo, segmentSuffix, files);
-    FixedGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
-    VariableGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
-    // hackish!
-    Iterator<String> it = files.iterator();
-    while(it.hasNext()) {
-      final String file = it.next();
-      if (!dir.fileExists(file)) {
-        it.remove();
-      }
-    }
-    //System.out.println("MockRandom.files return " + files);
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSepDocValuesFormat.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSepDocValuesFormat.java
deleted file mode 100644
index 595f6b6..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSepDocValuesFormat.java
+++ /dev/null
@@ -1,54 +0,0 @@
-package org.apache.lucene.index.codecs.mocksep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.DocValuesFormat;
-import org.apache.lucene.index.codecs.PerDocConsumer;
-import org.apache.lucene.index.codecs.PerDocProducer;
-import org.apache.lucene.index.codecs.sep.SepDocValuesConsumer;
-import org.apache.lucene.index.codecs.sep.SepDocValuesProducer;
-import org.apache.lucene.store.Directory;
-
-/**
- * Separate-file docvalues implementation
- * @lucene.experimental
- */
-// TODO: we could move this out of src/test ?
-public class MockSepDocValuesFormat extends DocValuesFormat {
-
-  @Override
-  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new SepDocValuesConsumer(state);
-  }
-
-  @Override
-  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
-    return new SepDocValuesProducer(state);
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo info, Set<String> files) throws IOException {
-    SepDocValuesConsumer.files(dir, info, files);
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSepPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSepPostingsFormat.java
deleted file mode 100644
index 1e8baa1..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSepPostingsFormat.java
+++ /dev/null
@@ -1,138 +0,0 @@
-package org.apache.lucene.index.codecs.mocksep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.index.codecs.FixedGapTermsIndexWriter;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.BlockTermsReader;
-import org.apache.lucene.index.codecs.BlockTermsWriter;
-import org.apache.lucene.index.codecs.TermsIndexReaderBase;
-import org.apache.lucene.index.codecs.TermsIndexWriterBase;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat;
-import org.apache.lucene.index.codecs.sep.SepPostingsWriter;
-import org.apache.lucene.index.codecs.sep.SepPostingsReader;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * A silly codec that simply writes each file separately as
- * single vInts.  Don't use this (performance will be poor)!
- * This is here just to test the core sep codec
- * classes.
- */
-public class MockSepPostingsFormat extends PostingsFormat {
-
-  public MockSepPostingsFormat() {
-    super("MockSep");
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-
-    PostingsWriterBase postingsWriter = new SepPostingsWriter(state, new MockSingleIntFactory());
-
-    boolean success = false;
-    TermsIndexWriterBase indexWriter;
-    try {
-      indexWriter = new FixedGapTermsIndexWriter(state);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsWriter.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsConsumer ret = new BlockTermsWriter(indexWriter, state, postingsWriter);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsWriter.close();
-        } finally {
-          indexWriter.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-
-    PostingsReaderBase postingsReader = new SepPostingsReader(state.dir, state.segmentInfo,
-        state.context, new MockSingleIntFactory(), state.segmentSuffix);
-
-    TermsIndexReaderBase indexReader;
-    boolean success = false;
-    try {
-      indexReader = new FixedGapTermsIndexReader(state.dir,
-                                                       state.fieldInfos,
-                                                       state.segmentInfo.name,
-                                                       state.termsIndexDivisor,
-                                                       BytesRef.getUTF8SortedAsUnicodeComparator(),
-                                                       state.segmentSuffix, state.context);
-      success = true;
-    } finally {
-      if (!success) {
-        postingsReader.close();
-      }
-    }
-
-    success = false;
-    try {
-      FieldsProducer ret = new BlockTermsReader(indexReader,
-                                                state.dir,
-                                                state.fieldInfos,
-                                                state.segmentInfo.name,
-                                                postingsReader,
-                                                state.context,
-                                                Lucene40PostingsFormat.TERMS_CACHE_SIZE,
-                                                state.segmentSuffix);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        try {
-          postingsReader.close();
-        } finally {
-          indexReader.close();
-        }
-      }
-    }
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    SepPostingsReader.files(segmentInfo, segmentSuffix, files);
-    BlockTermsReader.files(dir, segmentInfo, segmentSuffix, files);
-    FixedGapTermsIndexReader.files(dir, segmentInfo, segmentSuffix, files);
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSingleIntFactory.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSingleIntFactory.java
deleted file mode 100644
index 6dce24a..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSingleIntFactory.java
+++ /dev/null
@@ -1,38 +0,0 @@
-package org.apache.lucene.index.codecs.mocksep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.index.codecs.sep.IntStreamFactory;
-import org.apache.lucene.index.codecs.sep.IntIndexInput;
-import org.apache.lucene.index.codecs.sep.IntIndexOutput;
-
-import java.io.IOException;
-
-/** @lucene.experimental */
-public class MockSingleIntFactory extends IntStreamFactory {
-  @Override
-  public IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException {
-    return new MockSingleIntIndexInput(dir, fileName, context);
-  }
-  @Override
-  public IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException {
-    return new MockSingleIntIndexOutput(dir, fileName, context);
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexInput.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexInput.java
deleted file mode 100644
index df34e46..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexInput.java
+++ /dev/null
@@ -1,114 +0,0 @@
-package org.apache.lucene.index.codecs.mocksep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.codecs.sep.IntIndexInput;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.CodecUtil;
-
-/** Reads IndexInputs written with {@link
- *  MockSingleIntIndexOutput}.  NOTE: this class is just for
- *  demonstration puprposes (it is a very slow way to read a
- *  block of ints).
- *
- * @lucene.experimental
- */
-public class MockSingleIntIndexInput extends IntIndexInput {
-  private final IndexInput in;
-
-  public MockSingleIntIndexInput(Directory dir, String fileName, IOContext context)
-    throws IOException {
-    in = dir.openInput(fileName, context);
-    CodecUtil.checkHeader(in, MockSingleIntIndexOutput.CODEC,
-                          MockSingleIntIndexOutput.VERSION_START,
-                          MockSingleIntIndexOutput.VERSION_START);
-  }
-
-  @Override
-  public Reader reader() throws IOException {
-    return new Reader((IndexInput) in.clone());
-  }
-
-  @Override
-  public void close() throws IOException {
-    in.close();
-  }
-
-  public static class Reader extends IntIndexInput.Reader {
-    // clone:
-    private final IndexInput in;
-
-    public Reader(IndexInput in) {
-      this.in = in;
-    }
-
-    /** Reads next single int */
-    @Override
-    public int next() throws IOException {
-      //System.out.println("msii.next() fp=" + in.getFilePointer() + " vs " + in.length());
-      return in.readVInt();
-    }
-  }
-  
-  class Index extends IntIndexInput.Index {
-    private long fp;
-
-    @Override
-    public void read(DataInput indexIn, boolean absolute)
-      throws IOException {
-      if (absolute) {
-        fp = indexIn.readVLong();
-      } else {
-        fp += indexIn.readVLong();
-      }
-    }
-
-    @Override
-    public void set(IntIndexInput.Index other) {
-      fp = ((Index) other).fp;
-    }
-
-    @Override
-    public void seek(IntIndexInput.Reader other) throws IOException {
-      ((Reader) other).in.seek(fp);
-    }
-
-    @Override
-    public String toString() {
-      return Long.toString(fp);
-    }
-
-    @Override
-    public Object clone() {
-      Index other = new Index();
-      other.fp = fp;
-      return other;
-    }
-  }
-
-  @Override
-  public Index index() {
-    return new Index();
-  }
-}
-
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexOutput.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexOutput.java
deleted file mode 100644
index 83a7f3b..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/mocksep/MockSingleIntIndexOutput.java
+++ /dev/null
@@ -1,104 +0,0 @@
-package org.apache.lucene.index.codecs.mocksep;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.index.codecs.sep.IntIndexOutput;
-import java.io.IOException;
-
-/** Writes ints directly to the file (not in blocks) as
- *  vInt.
- * 
- * @lucene.experimental
-*/
-public class MockSingleIntIndexOutput extends IntIndexOutput {
-  private final IndexOutput out;
-  final static String CODEC = "SINGLE_INTS";
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  public MockSingleIntIndexOutput(Directory dir, String fileName, IOContext context) throws IOException {
-    out = dir.createOutput(fileName, context);
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(out, CODEC, VERSION_CURRENT);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-
-  /** Write an int to the primary file */
-  @Override
-  public void write(int v) throws IOException {
-    assert v >= 0;
-    out.writeVInt(v);
-  }
-
-  @Override
-  public Index index() {
-    return new Index();
-  }
-
-  @Override
-  public void close() throws IOException {
-    out.close();
-  }
-
-  @Override
-  public String toString() {
-    return "MockSingleIntIndexOutput fp=" + out.getFilePointer();
-  }
-
-  private class Index extends IntIndexOutput.Index {
-    long fp;
-    long lastFP;
-    @Override
-    public void mark() {
-      fp = out.getFilePointer();
-    }
-    @Override
-    public void copyFrom(IntIndexOutput.Index other, boolean copyLast) {
-      fp = ((Index) other).fp;
-      if (copyLast) {
-        lastFP = ((Index) other).fp;
-      }
-    }
-    @Override
-    public void write(IndexOutput indexOut, boolean absolute)
-      throws IOException {
-      if (absolute) {
-        indexOut.writeVLong(fp);
-      } else {
-        indexOut.writeVLong(fp - lastFP);
-      }
-      lastFP = fp;
-    }
-      
-    @Override
-    public String toString() {
-      return Long.toString(fp);
-    }
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/nestedpulsing/NestedPulsingPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/nestedpulsing/NestedPulsingPostingsFormat.java
deleted file mode 100644
index 659aa17..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/nestedpulsing/NestedPulsingPostingsFormat.java
+++ /dev/null
@@ -1,99 +0,0 @@
-package org.apache.lucene.index.codecs.nestedpulsing;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Set;
-
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.BlockTreeTermsReader;
-import org.apache.lucene.index.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.PostingsReaderBase;
-import org.apache.lucene.index.codecs.PostingsWriterBase;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsReader;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsWriter;
-import org.apache.lucene.index.codecs.pulsing.PulsingPostingsReader;
-import org.apache.lucene.index.codecs.pulsing.PulsingPostingsWriter;
-import org.apache.lucene.store.Directory;
-
-/**
- * Pulsing(1, Pulsing(2, Lucene40))
- * @lucene.experimental
- */
-// TODO: if we create PulsingPostingsBaseFormat then we
-// can simplify this? note: I don't like the *BaseFormat
-// hierarchy, maybe we can clean that up...
-public class NestedPulsingPostingsFormat extends PostingsFormat {
-  public NestedPulsingPostingsFormat() {
-    super("NestedPulsing");
-  }
-  
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docsWriter = new Lucene40PostingsWriter(state);
-
-    PostingsWriterBase pulsingWriterInner = new PulsingPostingsWriter(2, docsWriter);
-    PostingsWriterBase pulsingWriter = new PulsingPostingsWriter(1, pulsingWriterInner);
-    
-    // Terms dict
-    boolean success = false;
-    try {
-      FieldsConsumer ret = new BlockTreeTermsWriter(state, pulsingWriter, 
-          BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        pulsingWriter.close();
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase docsReader = new Lucene40PostingsReader(state.dir, state.segmentInfo, state.context, state.segmentSuffix);
-    PostingsReaderBase pulsingReaderInner = new PulsingPostingsReader(docsReader);
-    PostingsReaderBase pulsingReader = new PulsingPostingsReader(pulsingReaderInner);
-    boolean success = false;
-    try {
-      FieldsProducer ret = new BlockTreeTermsReader(
-                                                    state.dir, state.fieldInfos, state.segmentInfo.name,
-                                                    pulsingReader,
-                                                    state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        pulsingReader.close();
-      }
-    }
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) throws IOException {
-    Lucene40PostingsReader.files(dir, segmentInfo, segmentSuffix, files);
-    BlockTreeTermsReader.files(dir, segmentInfo, segmentSuffix, files);
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java
deleted file mode 100644
index bc34d01..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java
+++ /dev/null
@@ -1,223 +0,0 @@
-package org.apache.lucene.index.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.PostingsConsumer;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.index.codecs.TermsConsumer;
-import org.apache.lucene.index.codecs.lucene3x.Lucene3xPostingsFormat;
-import org.apache.lucene.index.codecs.lucene3x.TermInfo;
-import org.apache.lucene.index.codecs.lucene40.Lucene40SkipListWriter;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-class PreFlexFieldsWriter extends FieldsConsumer {
-
-  private final TermInfosWriter termsOut;
-  private final IndexOutput freqOut;
-  private final IndexOutput proxOut;
-  private final Lucene40SkipListWriter skipListWriter;
-  private final int totalNumDocs;
-
-  public PreFlexFieldsWriter(SegmentWriteState state) throws IOException {
-    termsOut = new TermInfosWriter(state.directory,
-                                   state.segmentName,
-                                   state.fieldInfos,
-                                   state.termIndexInterval);
-
-    boolean success = false;
-    try {
-      final String freqFile = IndexFileNames.segmentFileName(state.segmentName, "", Lucene3xPostingsFormat.FREQ_EXTENSION);
-      freqOut = state.directory.createOutput(freqFile, state.context);
-      totalNumDocs = state.numDocs;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(termsOut);
-      }
-    }
-
-    success = false;
-    try {
-      if (state.fieldInfos.hasProx()) {
-        final String proxFile = IndexFileNames.segmentFileName(state.segmentName, "", Lucene3xPostingsFormat.PROX_EXTENSION);
-        proxOut = state.directory.createOutput(proxFile, state.context);
-      } else {
-        proxOut = null;
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(termsOut, freqOut);
-      }
-    }
-
-    skipListWriter = new Lucene40SkipListWriter(termsOut.skipInterval,
-                                               termsOut.maxSkipLevels,
-                                               totalNumDocs,
-                                               freqOut,
-                                               proxOut);
-    //System.out.println("\nw start seg=" + segment);
-  }
-
-  @Override
-  public TermsConsumer addField(FieldInfo field) throws IOException {
-    assert field.number != -1;
-    //System.out.println("w field=" + field.name + " storePayload=" + field.storePayloads + " number=" + field.number);
-    return new PreFlexTermsWriter(field);
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.close(termsOut, freqOut, proxOut);
-  }
-
-  private class PreFlexTermsWriter extends TermsConsumer {
-    private final FieldInfo fieldInfo;
-    private final boolean omitTF;
-    private final boolean storePayloads;
-    
-    private final TermInfo termInfo = new TermInfo();
-    private final PostingsWriter postingsWriter = new PostingsWriter();
-
-    public PreFlexTermsWriter(FieldInfo fieldInfo) {
-      this.fieldInfo = fieldInfo;
-      omitTF = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY;
-      storePayloads = fieldInfo.storePayloads;
-    }
-
-    private class PostingsWriter extends PostingsConsumer {
-      private int lastDocID;
-      private int lastPayloadLength = -1;
-      private int lastPosition;
-      private int df;
-
-      public PostingsWriter reset() {
-        df = 0;
-        lastDocID = 0;
-        lastPayloadLength = -1;
-        return this;
-      }
-
-      @Override
-      public void startDoc(int docID, int termDocFreq) throws IOException {
-        //System.out.println("    w doc=" + docID);
-
-        final int delta = docID - lastDocID;
-        if (docID < 0 || (df > 0 && delta <= 0)) {
-          throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " )");
-        }
-
-        if ((++df % termsOut.skipInterval) == 0) {
-          skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength);
-          skipListWriter.bufferSkip(df);
-        }
-
-        lastDocID = docID;
-
-        assert docID < totalNumDocs: "docID=" + docID + " totalNumDocs=" + totalNumDocs;
-
-        if (omitTF) {
-          freqOut.writeVInt(delta);
-        } else {
-          final int code = delta << 1;
-          if (termDocFreq == 1) {
-            freqOut.writeVInt(code|1);
-          } else {
-            freqOut.writeVInt(code);
-            freqOut.writeVInt(termDocFreq);
-          }
-        }
-        lastPosition = 0;
-      }
-
-      @Override
-      public void addPosition(int position, BytesRef payload) throws IOException {
-        assert proxOut != null;
-
-        //System.out.println("      w pos=" + position + " payl=" + payload);
-        final int delta = position - lastPosition;
-        lastPosition = position;
-
-        if (storePayloads) {
-          final int payloadLength = payload == null ? 0 : payload.length;
-          if (payloadLength != lastPayloadLength) {
-            //System.out.println("        write payload len=" + payloadLength);
-            lastPayloadLength = payloadLength;
-            proxOut.writeVInt((delta<<1)|1);
-            proxOut.writeVInt(payloadLength);
-          } else {
-            proxOut.writeVInt(delta << 1);
-          }
-          if (payloadLength > 0) {
-            proxOut.writeBytes(payload.bytes, payload.offset, payload.length);
-          }
-        } else {
-          proxOut.writeVInt(delta);
-        }
-      }
-
-      @Override
-      public void finishDoc() throws IOException {
-      }
-    }
-
-    @Override
-    public PostingsConsumer startTerm(BytesRef text) throws IOException {
-      //System.out.println("  w term=" + text.utf8ToString());
-      skipListWriter.resetSkip();
-      termInfo.freqPointer = freqOut.getFilePointer();
-      if (proxOut != null) {
-        termInfo.proxPointer = proxOut.getFilePointer();
-      }
-      return postingsWriter.reset();
-    }
-
-    @Override
-    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
-      if (stats.docFreq > 0) {
-        long skipPointer = skipListWriter.writeSkip(freqOut);
-        termInfo.docFreq = stats.docFreq;
-        termInfo.skipOffset = (int) (skipPointer - termInfo.freqPointer);
-        //System.out.println("  w finish term=" + text.utf8ToString() + " fnum=" + fieldInfo.number);
-        termsOut.add(fieldInfo.number,
-                     text,
-                     termInfo);
-      }
-    }
-
-    @Override
-    public void finish(long sumTotalTermCount, long sumDocFreq, int docCount) throws IOException {
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() throws IOException {
-      return BytesRef.getUTF8SortedAsUTF16Comparator();
-    }
-  }
-}
\ No newline at end of file
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/PreFlexRWCodec.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/PreFlexRWCodec.java
deleted file mode 100644
index ff51b28..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/PreFlexRWCodec.java
+++ /dev/null
@@ -1,39 +0,0 @@
-package org.apache.lucene.index.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.lucene3x.Lucene3xCodec;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Writes 3.x-like indexes (not perfect emulation yet) for testing only!
- * @lucene.experimental
- */
-public class PreFlexRWCodec extends Lucene3xCodec {
-  private final PostingsFormat postings = new PreFlexRWPostingsFormat();
-
-  @Override
-  public PostingsFormat postingsFormat() {
-    if (LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE) {
-      return postings;
-    } else {
-      return super.postingsFormat();
-    }
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/PreFlexRWPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/PreFlexRWPostingsFormat.java
deleted file mode 100644
index 633ea47..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/PreFlexRWPostingsFormat.java
+++ /dev/null
@@ -1,76 +0,0 @@
-package org.apache.lucene.index.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.codecs.lucene3x.Lucene3xFields;
-import org.apache.lucene.index.codecs.lucene3x.Lucene3xPostingsFormat;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.util.LuceneTestCase;
-
-/** Codec, only for testing, that can write and read the
- *  pre-flex index format.
- *
- * @lucene.experimental
- */
-public class PreFlexRWPostingsFormat extends Lucene3xPostingsFormat {
-
-  public PreFlexRWPostingsFormat() {
-    // NOTE: we impersonate the PreFlex codec so that it can
-    // read the segments we write!
-  }
-  
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new PreFlexFieldsWriter(state);
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-
-    // Whenever IW opens readers, eg for merging, we have to
-    // keep terms order in UTF16:
-
-    return new Lucene3xFields(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.termsIndexDivisor) {
-      @Override
-      protected boolean sortTermsByUnicode() {
-        // We carefully peek into stack track above us: if
-        // we are part of a "merge", we must sort by UTF16:
-        boolean unicodeSortOrder = true;
-
-        StackTraceElement[] trace = new Exception().getStackTrace();
-        for (int i = 0; i < trace.length; i++) {
-          //System.out.println(trace[i].getClassName());
-          if ("merge".equals(trace[i].getMethodName())) {
-            unicodeSortOrder = false;
-            if (LuceneTestCase.VERBOSE) {
-              System.out.println("NOTE: PreFlexRW codec: forcing legacy UTF16 term sort order");
-            }
-            break;
-          }
-        }
-
-        return unicodeSortOrder;
-      }
-    };
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java
deleted file mode 100644
index 626bf84..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/preflexrw/TermInfosWriter.java
+++ /dev/null
@@ -1,276 +0,0 @@
-package org.apache.lucene.index.codecs.preflexrw;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.codecs.lucene3x.Lucene3xPostingsFormat;
-import org.apache.lucene.index.codecs.lucene3x.TermInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.UnicodeUtil;
-
-
-/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
-  Directory.  A TermInfos can be written once, in order.  */
-
-final class TermInfosWriter implements Closeable {
-  /** The file format version, a negative number. */
-  public static final int FORMAT = -3;
-
-  // Changed strings to true utf8 with length-in-bytes not
-  // length-in-chars
-  public static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = -4;
-
-  // NOTE: always change this if you switch to a new format!
-  public static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
-
-  private FieldInfos fieldInfos;
-  private IndexOutput output;
-  private TermInfo lastTi = new TermInfo();
-  private long size;
-
-  // TODO: the default values for these two parameters should be settable from
-  // IndexWriter.  However, once that's done, folks will start setting them to
-  // ridiculous values and complaining that things don't work well, as with
-  // mergeFactor.  So, let's wait until a number of folks find that alternate
-  // values work better.  Note that both of these values are stored in the
-  // segment, so that it's safe to change these w/o rebuilding all indexes.
-
-  /** Expert: The fraction of terms in the "dictionary" which should be stored
-   * in RAM.  Smaller values use more memory, but make searching slightly
-   * faster, while larger values use less memory and make searching slightly
-   * slower.  Searching is typically not dominated by dictionary lookup, so
-   * tweaking this is rarely useful.*/
-  int indexInterval = 128;
-
-  /** Expert: The fraction of {@link TermDocs} entries stored in skip tables,
-   * used to accelerate {@link TermDocs#skipTo(int)}.  Larger values result in
-   * smaller indexes, greater acceleration, but fewer accelerable cases, while
-   * smaller values result in bigger indexes, less acceleration and more
-   * accelerable cases. More detailed experiments would be useful here. */
-  int skipInterval = 16;
-  
-  /** Expert: The maximum number of skip levels. Smaller values result in 
-   * slightly smaller indexes, but slower skipping in big posting lists.
-   */
-  int maxSkipLevels = 10;
-
-  private long lastIndexPointer;
-  private boolean isIndex;
-  private final BytesRef lastTerm = new BytesRef();
-  private int lastFieldNumber = -1;
-
-  private TermInfosWriter other;
-
-  TermInfosWriter(Directory directory, String segment, FieldInfos fis,
-                  int interval)
-       throws IOException {
-    initialize(directory, segment, fis, interval, false);
-    boolean success = false;
-    try {
-      other = new TermInfosWriter(directory, segment, fis, interval, true);
-      other.other = this;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(output);
-
-        try {
-          directory.deleteFile(IndexFileNames.segmentFileName(segment, "",
-              (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
-                  : Lucene3xPostingsFormat.TERMS_EXTENSION)));
-        } catch (IOException ignored) {
-        }
-      }
-    }
-  }
-
-  private TermInfosWriter(Directory directory, String segment, FieldInfos fis,
-                          int interval, boolean isIndex) throws IOException {
-    initialize(directory, segment, fis, interval, isIndex);
-  }
-
-  private void initialize(Directory directory, String segment, FieldInfos fis,
-                          int interval, boolean isi) throws IOException {
-    indexInterval = interval;
-    fieldInfos = fis;
-    isIndex = isi;
-    output = directory.createOutput(IndexFileNames.segmentFileName(segment, "",
-        (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
-            : Lucene3xPostingsFormat.TERMS_EXTENSION)), IOContext.DEFAULT);
-    boolean success = false;
-    try {
-      output.writeInt(FORMAT_CURRENT);              // write format
-      output.writeLong(0);                          // leave space for size
-      output.writeInt(indexInterval);               // write indexInterval
-      output.writeInt(skipInterval);                // write skipInterval
-      output.writeInt(maxSkipLevels);               // write maxSkipLevels
-      assert initUTF16Results();
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(output);
-
-        try {
-          directory.deleteFile(IndexFileNames.segmentFileName(segment, "",
-              (isIndex ? Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION
-                  : Lucene3xPostingsFormat.TERMS_EXTENSION)));
-        } catch (IOException ignored) {
-        }
-      }
-    }
-  }
-
-  // Currently used only by assert statements
-  CharsRef utf16Result1;
-  CharsRef utf16Result2;
-  private final BytesRef scratchBytes = new BytesRef();
-
-  // Currently used only by assert statements
-  private boolean initUTF16Results() {
-    utf16Result1 = new CharsRef(10);
-    utf16Result2 = new CharsRef(10);
-    return true;
-  }
-
-  // Currently used only by assert statement
-  private int compareToLastTerm(int fieldNumber, BytesRef term) {
-
-    if (lastFieldNumber != fieldNumber) {
-      final int cmp = fieldInfos.fieldName(lastFieldNumber).compareTo(fieldInfos.fieldName(fieldNumber));
-      // If there is a field named "" (empty string) then we
-      // will get 0 on this comparison, yet, it's "OK".  But
-      // it's not OK if two different field numbers map to
-      // the same name.
-      if (cmp != 0 || lastFieldNumber != -1)
-        return cmp;
-    }
-
-    scratchBytes.copyBytes(term);
-    assert lastTerm.offset == 0;
-    UnicodeUtil.UTF8toUTF16(lastTerm.bytes, 0, lastTerm.length, utf16Result1);
-
-    assert scratchBytes.offset == 0;
-    UnicodeUtil.UTF8toUTF16(scratchBytes.bytes, 0, scratchBytes.length, utf16Result2);
-
-    final int len;
-    if (utf16Result1.length < utf16Result2.length)
-      len = utf16Result1.length;
-    else
-      len = utf16Result2.length;
-
-    for(int i=0;i<len;i++) {
-      final char ch1 = utf16Result1.chars[i];
-      final char ch2 = utf16Result2.chars[i];
-      if (ch1 != ch2)
-        return ch1-ch2;
-    }
-    if (utf16Result1.length == 0 && lastFieldNumber == -1) {
-      // If there is a field named "" (empty string) with a term text of "" (empty string) then we
-      // will get 0 on this comparison, yet, it's "OK". 
-      return -1;
-    }
-    return utf16Result1.length - utf16Result2.length;
-  }
-
-  /** Adds a new <<fieldNumber, termBytes>, TermInfo> pair to the set.
-    Term must be lexicographically greater than all previous Terms added.
-    TermInfo pointers must be positive and greater than all previous.*/
-  public void add(int fieldNumber, BytesRef term, TermInfo ti)
-    throws IOException {
-
-    assert compareToLastTerm(fieldNumber, term) < 0 ||
-      (isIndex && term.length == 0 && lastTerm.length == 0) :
-      "Terms are out of order: field=" + fieldInfos.fieldName(fieldNumber) + " (number " + fieldNumber + ")" +
-        " lastField=" + fieldInfos.fieldName(lastFieldNumber) + " (number " + lastFieldNumber + ")" +
-        " text=" + term.utf8ToString() + " lastText=" + lastTerm.utf8ToString();
-
-    assert ti.freqPointer >= lastTi.freqPointer: "freqPointer out of order (" + ti.freqPointer + " < " + lastTi.freqPointer + ")";
-    assert ti.proxPointer >= lastTi.proxPointer: "proxPointer out of order (" + ti.proxPointer + " < " + lastTi.proxPointer + ")";
-
-    if (!isIndex && size % indexInterval == 0)
-      other.add(lastFieldNumber, lastTerm, lastTi);                      // add an index term
-
-    writeTerm(fieldNumber, term);                        // write term
-
-    output.writeVInt(ti.docFreq);                       // write doc freq
-    output.writeVLong(ti.freqPointer - lastTi.freqPointer); // write pointers
-    output.writeVLong(ti.proxPointer - lastTi.proxPointer);
-
-    if (ti.docFreq >= skipInterval) {
-      output.writeVInt(ti.skipOffset);
-    }
-
-    if (isIndex) {
-      output.writeVLong(other.output.getFilePointer() - lastIndexPointer);
-      lastIndexPointer = other.output.getFilePointer(); // write pointer
-    }
-
-    lastFieldNumber = fieldNumber;
-    lastTi.set(ti);
-    size++;
-  }
-
-  private void writeTerm(int fieldNumber, BytesRef term)
-       throws IOException {
-
-    //System.out.println("  tiw.write field=" + fieldNumber + " term=" + term.utf8ToString());
-
-    // TODO: UTF16toUTF8 could tell us this prefix
-    // Compute prefix in common with last term:
-    int start = 0;
-    final int limit = term.length < lastTerm.length ? term.length : lastTerm.length;
-    while(start < limit) {
-      if (term.bytes[start+term.offset] != lastTerm.bytes[start+lastTerm.offset])
-        break;
-      start++;
-    }
-
-    final int length = term.length - start;
-    output.writeVInt(start);                     // write shared prefix length
-    output.writeVInt(length);                  // write delta length
-    output.writeBytes(term.bytes, start+term.offset, length);  // write delta bytes
-    output.writeVInt(fieldNumber); // write field num
-    lastTerm.copyBytes(term);
-  }
-
-  /** Called to complete TermInfos creation. */
-  public void close() throws IOException {
-    try {
-      output.seek(4);          // write size after format
-      output.writeLong(size);
-    } finally {
-      try {
-        output.close();
-      } finally {
-        if (!isIndex) {
-          other.close();
-        }
-      }
-    }
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/ramonly/RAMOnlyPostingsFormat.java b/lucene/src/test-framework/java/org/apache/lucene/index/codecs/ramonly/RAMOnlyPostingsFormat.java
deleted file mode 100644
index 43e3b41..0000000
--- a/lucene/src/test-framework/java/org/apache/lucene/index/codecs/ramonly/RAMOnlyPostingsFormat.java
+++ /dev/null
@@ -1,580 +0,0 @@
-package org.apache.lucene.index.codecs.ramonly;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.SortedMap;
-import java.util.TreeMap;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.PostingsConsumer;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.index.codecs.TermsConsumer;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CodecUtil;
-import org.apache.lucene.util.IOUtils;
-
-/** Stores all postings data in RAM, but writes a small
- *  token (header + single int) to identify which "slot" the
- *  index is using in RAM HashMap.
- *
- *  NOTE: this codec sorts terms by reverse-unicode-order! */
-
-public class RAMOnlyPostingsFormat extends PostingsFormat {
-
-  // For fun, test that we can override how terms are
-  // sorted, and basic things still work -- this comparator
-  // sorts in reversed unicode code point order:
-  private static final Comparator<BytesRef> reverseUnicodeComparator = new Comparator<BytesRef>() {
-      public int compare(BytesRef t1, BytesRef t2) {
-        byte[] b1 = t1.bytes;
-        byte[] b2 = t2.bytes;
-        int b1Stop;
-        int b1Upto = t1.offset;
-        int b2Upto = t2.offset;
-        if (t1.length < t2.length) {
-          b1Stop = t1.offset + t1.length;
-        } else {
-          b1Stop = t1.offset + t2.length;
-        }
-        while(b1Upto < b1Stop) {
-          final int bb1 = b1[b1Upto++] & 0xff;
-          final int bb2 = b2[b2Upto++] & 0xff;
-          if (bb1 != bb2) {
-            //System.out.println("cmp 1=" + t1 + " 2=" + t2 + " return " + (bb2-bb1));
-            return bb2 - bb1;
-          }
-        }
-
-        // One is prefix of another, or they are equal
-        return t2.length-t1.length;
-      }
-
-      @Override
-      public boolean equals(Object other) {
-        return this == other;
-      }
-    };
-
-  public RAMOnlyPostingsFormat() {
-    super("RAMOnly");
-  }
-    
-  // Postings state:
-  static class RAMPostings extends FieldsProducer {
-    final Map<String,RAMField> fieldToTerms = new TreeMap<String,RAMField>();
-
-    @Override
-    public Terms terms(String field) {
-      return fieldToTerms.get(field);
-    }
-
-    @Override
-    public int getUniqueFieldCount() {
-      return fieldToTerms.size();
-    }
-
-    @Override
-    public FieldsEnum iterator() {
-      return new RAMFieldsEnum(this);
-    }
-
-    @Override
-    public void close() {
-    }
-  } 
-
-  static class RAMField extends Terms {
-    final String field;
-    final SortedMap<String,RAMTerm> termToDocs = new TreeMap<String,RAMTerm>();
-    long sumTotalTermFreq;
-    long sumDocFreq;
-    int docCount;
-
-    RAMField(String field) {
-      this.field = field;
-    }
-
-    @Override
-    public long getUniqueTermCount() {
-      return termToDocs.size();
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return sumTotalTermFreq;
-    }
-      
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return sumDocFreq;
-    }
-      
-    @Override
-    public int getDocCount() throws IOException {
-      return docCount;
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) {
-      return new RAMTermsEnum(RAMOnlyPostingsFormat.RAMField.this);
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return reverseUnicodeComparator;
-    }
-  }
-
-  static class RAMTerm {
-    final String term;
-    long totalTermFreq;
-    final List<RAMDoc> docs = new ArrayList<RAMDoc>();
-    public RAMTerm(String term) {
-      this.term = term;
-    }
-  }
-
-  static class RAMDoc {
-    final int docID;
-    final int[] positions;
-    byte[][] payloads;
-
-    public RAMDoc(int docID, int freq) {
-      this.docID = docID;
-      positions = new int[freq];
-    }
-  }
-
-  // Classes for writing to the postings state
-  private static class RAMFieldsConsumer extends FieldsConsumer {
-
-    private final RAMPostings postings;
-    private final RAMTermsConsumer termsConsumer = new RAMTermsConsumer();
-
-    public RAMFieldsConsumer(RAMPostings postings) {
-      this.postings = postings;
-    }
-
-    @Override
-    public TermsConsumer addField(FieldInfo field) {
-      RAMField ramField = new RAMField(field.name);
-      postings.fieldToTerms.put(field.name, ramField);
-      termsConsumer.reset(ramField);
-      return termsConsumer;
-    }
-
-    @Override
-    public void close() {
-      // TODO: finalize stuff
-    }
-  }
-
-  private static class RAMTermsConsumer extends TermsConsumer {
-    private RAMField field;
-    private final RAMPostingsWriterImpl postingsWriter = new RAMPostingsWriterImpl();
-    RAMTerm current;
-      
-    void reset(RAMField field) {
-      this.field = field;
-    }
-      
-    @Override
-    public PostingsConsumer startTerm(BytesRef text) {
-      final String term = text.utf8ToString();
-      current = new RAMTerm(term);
-      postingsWriter.reset(current);
-      return postingsWriter;
-    }
-
-      
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public void finishTerm(BytesRef text, TermStats stats) {
-      assert stats.docFreq > 0;
-      assert stats.docFreq == current.docs.size();
-      current.totalTermFreq = stats.totalTermFreq;
-      field.termToDocs.put(current.term, current);
-    }
-
-    @Override
-    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) {
-      field.sumTotalTermFreq = sumTotalTermFreq;
-      field.sumDocFreq = sumDocFreq;
-      field.docCount = docCount;
-    }
-  }
-
-  public static class RAMPostingsWriterImpl extends PostingsConsumer {
-    private RAMTerm term;
-    private RAMDoc current;
-    private int posUpto = 0;
-
-    public void reset(RAMTerm term) {
-      this.term = term;
-    }
-
-    @Override
-    public void startDoc(int docID, int freq) {
-      current = new RAMDoc(docID, freq);
-      term.docs.add(current);
-      posUpto = 0;
-    }
-
-    @Override
-    public void addPosition(int position, BytesRef payload) {
-      current.positions[posUpto] = position;
-      if (payload != null && payload.length > 0) {
-        if (current.payloads == null) {
-          current.payloads = new byte[current.positions.length][];
-        }
-        byte[] bytes = current.payloads[posUpto] = new byte[payload.length];
-        System.arraycopy(payload.bytes, payload.offset, bytes, 0, payload.length);
-      }
-      posUpto++;
-    }
-
-    @Override
-    public void finishDoc() {
-      assert posUpto == current.positions.length;
-    }
-  }
-
-  // Classes for reading from the postings state
-  static class RAMFieldsEnum extends FieldsEnum {
-    private final RAMPostings postings;
-    private final Iterator<String> it;
-    private String current;
-
-    public RAMFieldsEnum(RAMPostings postings) {
-      this.postings = postings;
-      this.it = postings.fieldToTerms.keySet().iterator();
-    }
-
-    @Override
-    public String next() {
-      if (it.hasNext()) {
-        current = it.next();
-      } else {
-        current = null;
-      }
-      return current;
-    }
-
-    @Override
-    public Terms terms() {
-      return postings.fieldToTerms.get(current);
-    }
-  }
-
-  static class RAMTermsEnum extends TermsEnum {
-    Iterator<String> it;
-    String current;
-    private final RAMField ramField;
-
-    public RAMTermsEnum(RAMField field) {
-      this.ramField = field;
-    }
-      
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public BytesRef next() {
-      if (it == null) {
-        if (current == null) {
-          it = ramField.termToDocs.keySet().iterator();
-        } else {
-          it = ramField.termToDocs.tailMap(current).keySet().iterator();
-        }
-      }
-      if (it.hasNext()) {
-        current = it.next();
-        return new BytesRef(current);
-      } else {
-        return null;
-      }
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef term, boolean useCache) {
-      current = term.utf8ToString();
-      it = null;
-      if (ramField.termToDocs.containsKey(current)) {
-        return SeekStatus.FOUND;
-      } else {
-        if (current.compareTo(ramField.termToDocs.lastKey()) > 0) {
-          return SeekStatus.END;
-        } else {
-          return SeekStatus.NOT_FOUND;
-        }
-      }
-    }
-
-    @Override
-    public void seekExact(long ord) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public long ord() {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public BytesRef term() {
-      // TODO: reuse BytesRef
-      return new BytesRef(current);
-    }
-
-    @Override
-    public int docFreq() {
-      return ramField.termToDocs.get(current).docs.size();
-    }
-
-    @Override
-    public long totalTermFreq() {
-      return ramField.termToDocs.get(current).totalTermFreq;
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, boolean needsFreqs) {
-      return new RAMDocsEnum(ramField.termToDocs.get(current), liveDocs);
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse) {
-      return new RAMDocsAndPositionsEnum(ramField.termToDocs.get(current), liveDocs);
-    }
-  }
-
-  private static class RAMDocsEnum extends DocsEnum {
-    private final RAMTerm ramTerm;
-    private final Bits liveDocs;
-    private RAMDoc current;
-    int upto = -1;
-    int posUpto = 0;
-
-    public RAMDocsEnum(RAMTerm ramTerm, Bits liveDocs) {
-      this.ramTerm = ramTerm;
-      this.liveDocs = liveDocs;
-    }
-
-    @Override
-    public int advance(int targetDocID) {
-      do {
-        nextDoc();
-      } while (upto < ramTerm.docs.size() && current.docID < targetDocID);
-      return NO_MORE_DOCS;
-    }
-
-    // TODO: override bulk read, for better perf
-    @Override
-    public int nextDoc() {
-      while(true) {
-        upto++;
-        if (upto < ramTerm.docs.size()) {
-          current = ramTerm.docs.get(upto);
-          if (liveDocs == null || liveDocs.get(current.docID)) {
-            posUpto = 0;
-            return current.docID;
-          }
-        } else {
-          return NO_MORE_DOCS;
-        }
-      }
-    }
-
-    @Override
-    public int freq() {
-      return current.positions.length;
-    }
-
-    @Override
-    public int docID() {
-      return current.docID;
-    }
-  }
-
-  private static class RAMDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private final RAMTerm ramTerm;
-    private final Bits liveDocs;
-    private RAMDoc current;
-    int upto = -1;
-    int posUpto = 0;
-
-    public RAMDocsAndPositionsEnum(RAMTerm ramTerm, Bits liveDocs) {
-      this.ramTerm = ramTerm;
-      this.liveDocs = liveDocs;
-    }
-
-    @Override
-    public int advance(int targetDocID) {
-      do {
-        nextDoc();
-      } while (upto < ramTerm.docs.size() && current.docID < targetDocID);
-      return NO_MORE_DOCS;
-    }
-
-    // TODO: override bulk read, for better perf
-    @Override
-    public int nextDoc() {
-      while(true) {
-        upto++;
-        if (upto < ramTerm.docs.size()) {
-          current = ramTerm.docs.get(upto);
-          if (liveDocs == null || liveDocs.get(current.docID)) {
-            posUpto = 0;
-            return current.docID;
-          }
-        } else {
-          return NO_MORE_DOCS;
-        }
-      }
-    }
-
-    @Override
-    public int freq() {
-      return current.positions.length;
-    }
-
-    @Override
-    public int docID() {
-      return current.docID;
-    }
-
-    @Override
-    public int nextPosition() {
-      return current.positions[posUpto++];
-    }
-
-    @Override
-    public boolean hasPayload() {
-      return current.payloads != null && current.payloads[posUpto-1] != null;
-    }
-
-    @Override
-    public BytesRef getPayload() {
-      return new BytesRef(current.payloads[posUpto-1]);
-    }
-  }
-
-  // Holds all indexes created, keyed by the ID assigned in fieldsConsumer
-  private final Map<Integer,RAMPostings> state = new HashMap<Integer,RAMPostings>();
-
-  private final AtomicInteger nextID = new AtomicInteger();
-
-  private final String RAM_ONLY_NAME = "RAMOnly";
-  private final static int VERSION_START = 0;
-  private final static int VERSION_LATEST = VERSION_START;
-
-  private static final String ID_EXTENSION = "id";
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState writeState) throws IOException {
-    final int id = nextID.getAndIncrement();
-
-    // TODO -- ok to do this up front instead of
-    // on close....?  should be ok?
-    // Write our ID:
-    final String idFileName = IndexFileNames.segmentFileName(writeState.segmentName, writeState.segmentSuffix, ID_EXTENSION);
-    IndexOutput out = writeState.directory.createOutput(idFileName, writeState.context);
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(out, RAM_ONLY_NAME, VERSION_LATEST);
-      out.writeVInt(id);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      } else {
-        IOUtils.close(out);
-      }
-    }
-    
-    final RAMPostings postings = new RAMPostings();
-    final RAMFieldsConsumer consumer = new RAMFieldsConsumer(postings);
-
-    synchronized(state) {
-      state.put(id, postings);
-    }
-    return consumer;
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState readState)
-    throws IOException {
-
-    // Load our ID:
-    final String idFileName = IndexFileNames.segmentFileName(readState.segmentInfo.name, readState.segmentSuffix, ID_EXTENSION);
-    IndexInput in = readState.dir.openInput(idFileName, readState.context);
-    boolean success = false;
-    final int id;
-    try {
-      CodecUtil.checkHeader(in, RAM_ONLY_NAME, VERSION_START, VERSION_LATEST);
-      id = in.readVInt();
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(in);
-      } else {
-        IOUtils.close(in);
-      }
-    }
-    
-    synchronized(state) {
-      return state.get(id);
-    }
-  }
-
-  @Override
-  public void files(Directory dir, SegmentInfo segmentInfo, String segmentSuffix, Set<String> files) {
-    final String idFileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, ID_EXTENSION);
-    files.add(idFileName);
-  }
-}
diff --git a/lucene/src/test-framework/java/org/apache/lucene/util/LuceneTestCase.java b/lucene/src/test-framework/java/org/apache/lucene/util/LuceneTestCase.java
index 6cf75dd..6d64c4e 100644
--- a/lucene/src/test-framework/java/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/src/test-framework/java/org/apache/lucene/util/LuceneTestCase.java
@@ -33,16 +33,16 @@ import java.util.concurrent.Executors;
 import java.util.concurrent.TimeUnit;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.appending.AppendingCodec;
+import org.apache.lucene.codecs.lucene40.Lucene40Codec;
+import org.apache.lucene.codecs.preflexrw.PreFlexRWCodec;
+import org.apache.lucene.codecs.simpletext.SimpleTextCodec;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.index.*;
 import org.apache.lucene.index.IndexReader.ReaderClosedListener;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.appending.AppendingCodec;
-import org.apache.lucene.index.codecs.lucene40.Lucene40Codec;
-import org.apache.lucene.index.codecs.preflexrw.PreFlexRWCodec;
-import org.apache.lucene.index.codecs.simpletext.SimpleTextCodec;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.FieldCache.CacheEntry;
diff --git a/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java b/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java
index f37b9c5..dbb5220 100644
--- a/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java
+++ b/lucene/src/test-framework/java/org/apache/lucene/util/_TestUtil.java
@@ -33,6 +33,10 @@ import java.util.Random;
 import java.util.zip.ZipEntry;
 import java.util.zip.ZipFile;
 
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40Codec;
+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.index.CheckIndex;
@@ -50,10 +54,6 @@ import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.TieredMergePolicy;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40Codec;
-import org.apache.lucene.index.codecs.perfield.PerFieldPostingsFormat;
 import org.apache.lucene.search.FieldDoc;
 import org.apache.lucene.search.ScoreDoc;
 import org.apache.lucene.search.TopDocs;
diff --git a/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.codecs.Codec b/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.codecs.Codec
new file mode 100644
index 0000000..be7693b
--- /dev/null
+++ b/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.codecs.Codec
@@ -0,0 +1,16 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.preflexrw.PreFlexRWCodec
diff --git a/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
new file mode 100644
index 0000000..5242e64
--- /dev/null
+++ b/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -0,0 +1,22 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.mockintblock.MockFixedIntBlockPostingsFormat
+org.apache.lucene.codecs.mockintblock.MockVariableIntBlockPostingsFormat
+org.apache.lucene.codecs.mockrandom.MockRandomPostingsFormat
+org.apache.lucene.codecs.mocksep.MockSepPostingsFormat
+org.apache.lucene.codecs.nestedpulsing.NestedPulsingPostingsFormat
+org.apache.lucene.codecs.ramonly.RAMOnlyPostingsFormat
+org.apache.lucene.codecs.lucene40ords.Lucene40WithOrds
diff --git a/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.index.codecs.Codec b/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.index.codecs.Codec
deleted file mode 100644
index fdd6f8e..0000000
--- a/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.index.codecs.Codec
+++ /dev/null
@@ -1,16 +0,0 @@
-#  Licensed to the Apache Software Foundation (ASF) under one or more
-#  contributor license agreements.  See the NOTICE file distributed with
-#  this work for additional information regarding copyright ownership.
-#  The ASF licenses this file to You under the Apache License, Version 2.0
-#  (the "License"); you may not use this file except in compliance with
-#  the License.  You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#  Unless required by applicable law or agreed to in writing, software
-#  distributed under the License is distributed on an "AS IS" BASIS,
-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#  See the License for the specific language governing permissions and
-#  limitations under the License.
-
-org.apache.lucene.index.codecs.preflexrw.PreFlexRWCodec
diff --git a/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.index.codecs.PostingsFormat b/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.index.codecs.PostingsFormat
deleted file mode 100644
index 505d010..0000000
--- a/lucene/src/test-framework/resources/META-INF/services/org.apache.lucene.index.codecs.PostingsFormat
+++ /dev/null
@@ -1,22 +0,0 @@
-#  Licensed to the Apache Software Foundation (ASF) under one or more
-#  contributor license agreements.  See the NOTICE file distributed with
-#  this work for additional information regarding copyright ownership.
-#  The ASF licenses this file to You under the Apache License, Version 2.0
-#  (the "License"); you may not use this file except in compliance with
-#  the License.  You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#  Unless required by applicable law or agreed to in writing, software
-#  distributed under the License is distributed on an "AS IS" BASIS,
-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#  See the License for the specific language governing permissions and
-#  limitations under the License.
-
-org.apache.lucene.index.codecs.mockintblock.MockFixedIntBlockPostingsFormat
-org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockPostingsFormat
-org.apache.lucene.index.codecs.mockrandom.MockRandomPostingsFormat
-org.apache.lucene.index.codecs.mocksep.MockSepPostingsFormat
-org.apache.lucene.index.codecs.nestedpulsing.NestedPulsingPostingsFormat
-org.apache.lucene.index.codecs.ramonly.RAMOnlyPostingsFormat
-org.apache.lucene.index.codecs.lucene40ords.Lucene40WithOrds
diff --git a/lucene/src/test/org/apache/lucene/TestExternalCodecs.java b/lucene/src/test/org/apache/lucene/TestExternalCodecs.java
index b6cd018..978a214 100644
--- a/lucene/src/test/org/apache/lucene/TestExternalCodecs.java
+++ b/lucene/src/test/org/apache/lucene/TestExternalCodecs.java
@@ -21,10 +21,10 @@ import java.io.*;
 import java.util.*;
 
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.codecs.*;
+import org.apache.lucene.codecs.lucene40.Lucene40Codec;
 import org.apache.lucene.document.*;
 import org.apache.lucene.index.*;
-import org.apache.lucene.index.codecs.*;
-import org.apache.lucene.index.codecs.lucene40.Lucene40Codec;
 import org.apache.lucene.search.*;
 import org.apache.lucene.store.*;
 import org.apache.lucene.util.*;
diff --git a/lucene/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java b/lucene/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java
new file mode 100644
index 0000000..df9bf80
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java
@@ -0,0 +1,166 @@
+package org.apache.lucene.codecs.appending;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.appending.AppendingCodec;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.TieredMergePolicy;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.Version;
+
+public class TestAppendingCodec extends LuceneTestCase {
+  
+    private static class AppendingIndexOutputWrapper extends IndexOutput {
+    IndexOutput wrapped;
+    
+    public AppendingIndexOutputWrapper(IndexOutput wrapped) {
+      this.wrapped = wrapped;
+    }
+
+    @Override
+    public void close() throws IOException {
+      wrapped.close();
+    }
+
+    @Override
+    public void flush() throws IOException {
+      wrapped.flush();
+    }
+
+    @Override
+    public long getFilePointer() {
+      return wrapped.getFilePointer();
+    }
+
+    @Override
+    public long length() throws IOException {
+      return wrapped.length();
+    }
+
+    @Override
+    public void seek(long pos) throws IOException {
+      throw new UnsupportedOperationException("seek() is unsupported");
+    }
+
+    @Override
+    public void writeByte(byte b) throws IOException {
+      wrapped.writeByte(b);
+    }
+
+    @Override
+    public void writeBytes(byte[] b, int offset, int length) throws IOException {
+      wrapped.writeBytes(b, offset, length);
+    }
+    
+  }
+  
+  @SuppressWarnings("serial")
+  private static class AppendingRAMDirectory extends MockDirectoryWrapper {
+
+    public AppendingRAMDirectory(Random random, Directory delegate) {
+      super(random, delegate);
+    }
+
+    @Override
+    public IndexOutput createOutput(String name, IOContext context) throws IOException {
+      return new AppendingIndexOutputWrapper(super.createOutput(name, context));
+    }
+    
+  }
+  
+  private static final String text = "the quick brown fox jumped over the lazy dog";
+
+  public void testCodec() throws Exception {
+    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());
+    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));
+    
+    cfg.setCodec(new AppendingCodec());
+    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);
+    IndexWriter writer = new IndexWriter(dir, cfg);
+    Document doc = new Document();
+    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);
+    storedTextType.setStoreTermVectors(true);
+    storedTextType.setStoreTermVectorPositions(true);
+    storedTextType.setStoreTermVectorOffsets(true);
+    doc.add(newField("f", text, storedTextType));
+    writer.addDocument(doc);
+    writer.commit();
+    writer.addDocument(doc);
+    writer.forceMerge(1);
+    writer.close();
+    IndexReader reader = IndexReader.open(dir, 1);
+    assertEquals(2, reader.numDocs());
+    Document doc2 = reader.document(0);
+    assertEquals(text, doc2.get("f"));
+    Fields fields = MultiFields.getFields(reader);
+    Terms terms = fields.terms("f");
+    assertNotNull(terms);
+    TermsEnum te = terms.iterator(null);
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("quick")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("brown")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("fox")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("jumped")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("over")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("lazy")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("dog")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("the")));
+    DocsEnum de = te.docs(null, null, true);
+    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);
+    assertEquals(2, de.freq());
+    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);
+    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);
+    reader.close();
+  }
+  
+  public void testCompoundFile() throws Exception {
+    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());
+    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));
+    TieredMergePolicy mp = new TieredMergePolicy();
+    mp.setUseCompoundFile(true);
+    mp.setNoCFSRatio(1.0);
+    cfg.setMergePolicy(mp);
+    cfg.setCodec(new AppendingCodec());
+    IndexWriter writer = new IndexWriter(dir, cfg);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    writer.close();
+    assertTrue(dir.fileExists("_0.cfs"));
+    dir.close();
+  }
+}
diff --git a/lucene/src/test/org/apache/lucene/codecs/intblock/TestIntBlockCodec.java b/lucene/src/test/org/apache/lucene/codecs/intblock/TestIntBlockCodec.java
new file mode 100644
index 0000000..b85f7fd
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/codecs/intblock/TestIntBlockCodec.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.codecs.intblock;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.store.*;
+import org.apache.lucene.codecs.mockintblock.*;
+import org.apache.lucene.codecs.sep.*;
+
+public class TestIntBlockCodec extends LuceneTestCase {
+
+  public void testSimpleIntBlocks() throws Exception {
+    Directory dir = newDirectory();
+
+    IntStreamFactory f = new MockFixedIntBlockPostingsFormat(128).getIntFactory();
+
+    IntIndexOutput out = f.createOutput(dir, "test", newIOContext(random));
+    for(int i=0;i<11777;i++) {
+      out.write(i);
+    }
+    out.close();
+
+    IntIndexInput in = f.openInput(dir, "test", newIOContext(random));
+    IntIndexInput.Reader r = in.reader();
+
+    for(int i=0;i<11777;i++) {
+      assertEquals(i, r.next());
+    }
+    in.close();
+    
+    dir.close();
+  }
+
+  public void testEmptySimpleIntBlocks() throws Exception {
+    Directory dir = newDirectory();
+
+    IntStreamFactory f = new MockFixedIntBlockPostingsFormat(128).getIntFactory();
+    IntIndexOutput out = f.createOutput(dir, "test", newIOContext(random));
+
+    // write no ints
+    out.close();
+
+    IntIndexInput in = f.openInput(dir, "test", newIOContext(random));
+    in.reader();
+    // read no ints
+    in.close();
+    dir.close();
+  }
+}
diff --git a/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestImpersonation.java b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestImpersonation.java
new file mode 100644
index 0000000..a0d486a
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestImpersonation.java
@@ -0,0 +1,34 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.preflexrw.PreFlexRWCodec;
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Test that the SPI magic is returning "PreFlexRWCodec" for Lucene3x
+ * 
+ * @lucene.experimental
+ */
+public class TestImpersonation extends LuceneTestCase {
+  public void test() throws Exception {
+    Codec codec = Codec.forName("Lucene3x");
+    assertTrue(codec instanceof PreFlexRWCodec);
+  }
+}
diff --git a/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestSurrogates.java b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestSurrogates.java
new file mode 100644
index 0000000..0de48c7
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestSurrogates.java
@@ -0,0 +1,356 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.*;
+import org.apache.lucene.codecs.preflexrw.PreFlexRWCodec;
+import org.apache.lucene.document.*;
+import org.apache.lucene.analysis.*;
+import org.apache.lucene.index.*;
+import org.apache.lucene.util.*;
+
+import java.util.*;
+import java.io.IOException;
+
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestSurrogates extends LuceneTestCase {
+  /** we will manually instantiate preflex-rw here */
+  @BeforeClass
+  public static void beforeClass() {
+    LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE = true;
+  }
+
+  private static String makeDifficultRandomUnicodeString(Random r) {
+    final int end = r.nextInt(20);
+    if (end == 0) {
+      // allow 0 length
+      return "";
+    }
+    final char[] buffer = new char[end];
+    for (int i = 0; i < end; i++) {
+      int t = r.nextInt(5);
+
+      if (0 == t && i < end - 1) {
+        // hi
+        buffer[i++] = (char) (0xd800 + r.nextInt(2));
+        // lo
+        buffer[i] = (char) (0xdc00 + r.nextInt(2));
+      } else if (t <= 3) {
+        buffer[i] = (char) ('a' + r.nextInt(2));
+      }  else if (4 == t) {
+        buffer[i] = (char) (0xe000 + r.nextInt(2));
+      }
+    }
+
+    return new String(buffer, 0, end);
+  }
+
+  private String toHexString(Term t) {
+    return t.field() + ":" + UnicodeUtil.toHexString(t.text());
+  }
+
+  private String getRandomString(Random r) {
+    String s;
+    if (r.nextInt(5) == 1) {
+      if (r.nextInt(3) == 1) {
+        s = makeDifficultRandomUnicodeString(r);
+      } else {
+        s = _TestUtil.randomUnicodeString(r);
+      }
+    } else {
+      s = _TestUtil.randomRealisticUnicodeString(r);
+    }
+    return s;
+  }
+
+  private static class SortTermAsUTF16Comparator implements Comparator<Term> {
+    private static final Comparator<BytesRef> legacyComparator = 
+      BytesRef.getUTF8SortedAsUTF16Comparator();
+
+    public int compare(Term term1, Term term2) {
+      if (term1.field().equals(term2.field())) {
+        return legacyComparator.compare(term1.bytes(), term2.bytes());
+      } else {
+        return term1.field().compareTo(term2.field());
+      }
+    }
+  }
+
+  private static final SortTermAsUTF16Comparator termAsUTF16Comparator = new SortTermAsUTF16Comparator();
+
+  // single straight enum
+  private void doTestStraightEnum(List<Term> fieldTerms, IndexReader reader, int uniqueTermCount) throws IOException {
+
+    if (VERBOSE) {
+      System.out.println("\nTEST: top now enum reader=" + reader);
+    }
+    FieldsEnum fieldsEnum = MultiFields.getFields(reader).iterator();
+
+    {
+      // Test straight enum:
+      String field;
+      int termCount = 0;
+      while((field = fieldsEnum.next()) != null) {
+        Terms terms = fieldsEnum.terms();
+        assertNotNull(terms);
+        TermsEnum termsEnum = terms.iterator(null);
+        BytesRef text;
+        BytesRef lastText = null;
+        while((text = termsEnum.next()) != null) {
+          Term exp = fieldTerms.get(termCount);
+          if (VERBOSE) {
+            System.out.println("  got term=" + field + ":" + UnicodeUtil.toHexString(text.utf8ToString()));
+            System.out.println("       exp=" + exp.field() + ":" + UnicodeUtil.toHexString(exp.text().toString()));
+            System.out.println();
+          }
+          if (lastText == null) {
+            lastText = BytesRef.deepCopyOf(text);
+          } else {
+            assertTrue(lastText.compareTo(text) < 0);
+            lastText.copyBytes(text);
+          }
+          assertEquals(exp.field(), field);
+          assertEquals(exp.bytes(), text);
+          termCount++;
+        }
+        if (VERBOSE) {
+          System.out.println("  no more terms for field=" + field);
+        }
+      }
+      assertEquals(uniqueTermCount, termCount);
+    }
+  }
+
+  // randomly seeks to term that we know exists, then next's
+  // from there
+  private void doTestSeekExists(Random r, List<Term> fieldTerms, IndexReader reader) throws IOException {
+
+    final Map<String,TermsEnum> tes = new HashMap<String,TermsEnum>();
+
+    // Test random seek to existing term, then enum:
+    if (VERBOSE) {
+      System.out.println("\nTEST: top now seek");
+    }
+
+    int num = atLeast(100);
+    for (int iter = 0; iter < num; iter++) {
+
+      // pick random field+term
+      int spot = r.nextInt(fieldTerms.size());
+      Term term = fieldTerms.get(spot);
+      String field = term.field();
+
+      if (VERBOSE) {
+        System.out.println("TEST: exist seek field=" + field + " term=" + UnicodeUtil.toHexString(term.text()));
+      }
+
+      // seek to it
+      TermsEnum te = tes.get(field);
+      if (te == null) {
+        te = MultiFields.getTerms(reader, field).iterator(null);
+        tes.put(field, te);
+      }
+
+      if (VERBOSE) {
+        System.out.println("  done get enum");
+      }
+
+      // seek should find the term
+      assertEquals(TermsEnum.SeekStatus.FOUND,
+                   te.seekCeil(term.bytes()));
+      
+      // now .next() this many times:
+      int ct = _TestUtil.nextInt(r, 5, 100);
+      for(int i=0;i<ct;i++) {
+        if (VERBOSE) {
+          System.out.println("TEST: now next()");
+        }
+        if (1+spot+i >= fieldTerms.size()) {
+          break;
+        }
+        term = fieldTerms.get(1+spot+i);
+        if (!term.field().equals(field)) {
+          assertNull(te.next());
+          break;
+        } else {
+          BytesRef t = te.next();
+
+          if (VERBOSE) {
+            System.out.println("  got term=" + (t == null ? null : UnicodeUtil.toHexString(t.utf8ToString())));
+            System.out.println("       exp=" + UnicodeUtil.toHexString(term.text().toString()));
+          }
+
+          assertEquals(term.bytes(), t);
+        }
+      }
+    }
+  }
+
+  private void doTestSeekDoesNotExist(Random r, int numField, List<Term> fieldTerms, Term[] fieldTermsArray, IndexReader reader) throws IOException {
+
+    final Map<String,TermsEnum> tes = new HashMap<String,TermsEnum>();
+
+    if (VERBOSE) {
+      System.out.println("TEST: top random seeks");
+    }
+
+    {
+      int num = atLeast(100);
+      for (int iter = 0; iter < num; iter++) {
+      
+        // seek to random spot
+        String field = ("f" + r.nextInt(numField)).intern();
+        Term tx = new Term(field, getRandomString(r));
+
+        int spot = Arrays.binarySearch(fieldTermsArray, tx);
+
+        if (spot < 0) {
+          if (VERBOSE) {
+            System.out.println("TEST: non-exist seek to " + field + ":" + UnicodeUtil.toHexString(tx.text()));
+          }
+
+          // term does not exist:
+          TermsEnum te = tes.get(field);
+          if (te == null) {
+            te = MultiFields.getTerms(reader, field).iterator(null);
+            tes.put(field, te);
+          }
+
+          if (VERBOSE) {
+            System.out.println("  got enum");
+          }
+
+          spot = -spot - 1;
+
+          if (spot == fieldTerms.size() || !fieldTerms.get(spot).field().equals(field)) {
+            assertEquals(TermsEnum.SeekStatus.END, te.seekCeil(tx.bytes()));
+          } else {
+            assertEquals(TermsEnum.SeekStatus.NOT_FOUND, te.seekCeil(tx.bytes()));
+
+            if (VERBOSE) {
+              System.out.println("  got term=" + UnicodeUtil.toHexString(te.term().utf8ToString()));
+              System.out.println("  exp term=" + UnicodeUtil.toHexString(fieldTerms.get(spot).text()));
+            }
+
+            assertEquals(fieldTerms.get(spot).bytes(),
+                         te.term());
+
+            // now .next() this many times:
+            int ct = _TestUtil.nextInt(r, 5, 100);
+            for(int i=0;i<ct;i++) {
+              if (VERBOSE) {
+                System.out.println("TEST: now next()");
+              }
+              if (1+spot+i >= fieldTerms.size()) {
+                break;
+              }
+              Term term = fieldTerms.get(1+spot+i);
+              if (!term.field().equals(field)) {
+                assertNull(te.next());
+                break;
+              } else {
+                BytesRef t = te.next();
+
+                if (VERBOSE) {
+                  System.out.println("  got term=" + (t == null ? null : UnicodeUtil.toHexString(t.utf8ToString())));
+                  System.out.println("       exp=" + UnicodeUtil.toHexString(term.text().toString()));
+                }
+
+                assertEquals(term.bytes(), t);
+              }
+            }
+
+          }
+        }
+      }
+    }
+  }
+
+
+  @Test
+  public void testSurrogatesOrder() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random,
+                                                dir,
+                                                newIndexWriterConfig( TEST_VERSION_CURRENT,
+                                                                      new MockAnalyzer(random)).setCodec(new PreFlexRWCodec()));
+
+    final int numField = _TestUtil.nextInt(random, 2, 5);
+
+    int uniqueTermCount = 0;
+
+    int tc = 0;
+
+    List<Term> fieldTerms = new ArrayList<Term>();
+
+    for(int f=0;f<numField;f++) {
+      String field = "f" + f;
+      final int numTerms = atLeast(1000);
+
+      final Set<String> uniqueTerms = new HashSet<String>();
+
+      for(int i=0;i<numTerms;i++) {
+        String term = getRandomString(random) + "_ " + (tc++);
+        uniqueTerms.add(term);
+        fieldTerms.add(new Term(field, term));
+        Document doc = new Document();
+        doc.add(newField(field, term, StringField.TYPE_UNSTORED));
+        w.addDocument(doc);
+      }
+      uniqueTermCount += uniqueTerms.size();
+    }
+
+    IndexReader reader = w.getReader();
+
+    if (VERBOSE) {
+      Collections.sort(fieldTerms, termAsUTF16Comparator);
+
+      System.out.println("\nTEST: UTF16 order");
+      for(Term t: fieldTerms) {
+        System.out.println("  " + toHexString(t));
+      }
+    }
+
+    // sorts in code point order:
+    Collections.sort(fieldTerms);
+
+    if (VERBOSE) {
+      System.out.println("\nTEST: codepoint order");
+      for(Term t: fieldTerms) {
+        System.out.println("  " + toHexString(t));
+      }
+    }
+
+    Term[] fieldTermsArray = fieldTerms.toArray(new Term[fieldTerms.size()]);
+
+    //SegmentInfo si = makePreFlexSegment(r, "_0", dir, fieldInfos, codec, fieldTerms);
+
+    //FieldsProducer fields = codec.fieldsProducer(new SegmentReadState(dir, si, fieldInfos, 1024, 1));
+    //assertNotNull(fields);
+
+    doTestStraightEnum(fieldTerms, reader, uniqueTermCount);
+    doTestSeekExists(random, fieldTerms, reader);
+    doTestSeekDoesNotExist(random, numField, fieldTerms, fieldTermsArray, reader);
+
+    reader.close();
+    w.close();
+    dir.close();
+  }
+}
diff --git a/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestTermInfosReaderIndex.java b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestTermInfosReaderIndex.java
new file mode 100644
index 0000000..6bc1021
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/codecs/lucene3x/TestTermInfosReaderIndex.java
@@ -0,0 +1,201 @@
+package org.apache.lucene.codecs.lucene3x;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.codecs.lucene3x.Lucene3xPostingsFormat;
+import org.apache.lucene.codecs.lucene3x.SegmentTermEnum;
+import org.apache.lucene.codecs.lucene3x.TermInfosReaderIndex;
+import org.apache.lucene.codecs.preflexrw.PreFlexRWCodec;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LogMergePolicy;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SegmentReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.junit.BeforeClass;
+
+public class TestTermInfosReaderIndex extends LuceneTestCase {
+  
+  private static final int NUMBER_OF_DOCUMENTS = 1000;
+  private static final int NUMBER_OF_FIELDS = 100;
+  private TermInfosReaderIndex index;
+  private Directory directory;
+  private SegmentTermEnum termEnum;
+  private int indexDivisor;
+  private int termIndexInterval;
+  private IndexReader reader;
+  private List<Term> sampleTerms;
+  
+  /** we will manually instantiate preflex-rw here */
+  @BeforeClass
+  public static void beforeClass() {
+    LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE = true;
+  }
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    indexDivisor = _TestUtil.nextInt(random, 1, 10);
+    directory = newDirectory();
+    termIndexInterval = populate(directory);
+
+    IndexReader r0 = IndexReader.open(directory);
+    SegmentReader r = (SegmentReader) r0.getSequentialSubReaders()[0];
+    String segment = r.getSegmentName();
+    r.close();
+
+    FieldInfosReader infosReader = new PreFlexRWCodec().fieldInfosFormat().getFieldInfosReader();
+    FieldInfos fieldInfos = infosReader.read(directory, segment, IOContext.READONCE);
+    String segmentFileName = IndexFileNames.segmentFileName(segment, "", Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION);
+    long tiiFileLength = directory.fileLength(segmentFileName);
+    IndexInput input = directory.openInput(segmentFileName, newIOContext(random));
+    termEnum = new SegmentTermEnum(directory.openInput(IndexFileNames.segmentFileName(segment, "", Lucene3xPostingsFormat.TERMS_EXTENSION), newIOContext(random)), fieldInfos, false);
+    int totalIndexInterval = termEnum.indexInterval * indexDivisor;
+    
+    SegmentTermEnum indexEnum = new SegmentTermEnum(input, fieldInfos, true);
+    index = new TermInfosReaderIndex(indexEnum, indexDivisor, tiiFileLength, totalIndexInterval);
+    indexEnum.close();
+    input.close();
+    
+    reader = IndexReader.open(directory);
+    sampleTerms = sample(reader,1000);
+    
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    termEnum.close();
+    reader.close();
+    directory.close();
+    super.tearDown();
+  }
+  
+  public void testSeekEnum() throws CorruptIndexException, IOException {
+    int indexPosition = 3;
+    SegmentTermEnum clone = (SegmentTermEnum) termEnum.clone();
+    Term term = findTermThatWouldBeAtIndex(clone, indexPosition);
+    SegmentTermEnum enumerator = clone;
+    index.seekEnum(enumerator, indexPosition);
+    assertEquals(term, enumerator.term());
+    clone.close();
+  }
+  
+  public void testCompareTo() throws IOException {
+    Term term = new Term("field" + random.nextInt(NUMBER_OF_FIELDS) ,getText());
+    for (int i = 0; i < index.length(); i++) {
+      Term t = index.getTerm(i);
+      int compareTo = term.compareTo(t);
+      assertEquals(compareTo, index.compareTo(term, i));
+    }
+  }
+  
+  public void testRandomSearchPerformance() throws CorruptIndexException, IOException {
+    IndexSearcher searcher = new IndexSearcher(reader);
+    for (Term t : sampleTerms) {
+      TermQuery query = new TermQuery(t);
+      TopDocs topDocs = searcher.search(query, 10);
+      assertTrue(topDocs.totalHits > 0);
+    }
+  }
+
+  private List<Term> sample(IndexReader reader, int size) throws IOException {
+    List<Term> sample = new ArrayList<Term>();
+    Random random = new Random();
+    FieldsEnum fieldsEnum = MultiFields.getFields(reader).iterator();
+    String field;
+    while((field = fieldsEnum.next()) != null) {
+      Terms terms = fieldsEnum.terms();
+      assertNotNull(terms);
+      TermsEnum termsEnum = terms.iterator(null);
+      while (termsEnum.next() != null) {
+        if (sample.size() >= size) {
+          int pos = random.nextInt(size);
+          sample.set(pos, new Term(field, termsEnum.term()));
+        } else {
+          sample.add(new Term(field, termsEnum.term()));
+        }
+      }
+    }
+    Collections.shuffle(sample);
+    return sample;
+  }
+
+  private Term findTermThatWouldBeAtIndex(SegmentTermEnum termEnum, int index) throws IOException {
+    int termPosition = index * termIndexInterval * indexDivisor;
+    for (int i = 0; i < termPosition; i++) {
+      if (!termEnum.next()) {
+        fail("Should not have run out of terms.");
+      }
+    }
+    return termEnum.term();
+  }
+
+  private int populate(Directory directory) throws CorruptIndexException, LockObtainFailedException, IOException {
+    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT, 
+        new MockAnalyzer(random, MockTokenizer.KEYWORD, false));
+    config.setCodec(new PreFlexRWCodec());
+    // turn off compound file, this test will open some index files directly.
+    LogMergePolicy mp = newLogMergePolicy();
+    mp.setUseCompoundFile(false);
+    config.setMergePolicy(mp);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random, directory, config);
+    for (int i = 0; i < NUMBER_OF_DOCUMENTS; i++) {
+      Document document = new Document();
+      for (int f = 0; f < NUMBER_OF_FIELDS; f++) {
+        document.add(newField("field" + f, getText(), StringField.TYPE_UNSTORED));
+      }
+      writer.addDocument(document);
+    }
+    writer.forceMerge(1);
+    writer.close();
+    return config.getTermIndexInterval();
+  }
+  
+  private String getText() {
+    return Long.toString(random.nextLong(),Character.MAX_RADIX);
+  }
+}
diff --git a/lucene/src/test/org/apache/lucene/codecs/lucene40/TestDocValues.java b/lucene/src/test/org/apache/lucene/codecs/lucene40/TestDocValues.java
new file mode 100644
index 0000000..b5eda74
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/codecs/lucene40/TestDocValues.java
@@ -0,0 +1,413 @@
+package org.apache.lucene.codecs.lucene40;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.codecs.lucene40.values.Bytes;
+import org.apache.lucene.codecs.lucene40.values.Floats;
+import org.apache.lucene.codecs.lucene40.values.Ints;
+import org.apache.lucene.codecs.lucene40.values.Writer;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.SortedSource;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.Counter;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.UnicodeUtil;
+import org.apache.lucene.util._TestUtil;
+
+// TODO: some of this should be under lucene40 codec tests? is talking to codec directly?f
+public class TestDocValues extends LuceneTestCase {
+  private static final Comparator<BytesRef> COMP = BytesRef.getUTF8SortedAsUnicodeComparator();
+  // TODO -- for sorted test, do our own Sort of the
+  // values and verify it's identical
+
+  public void testBytesStraight() throws IOException {
+    runTestBytes(Bytes.Mode.STRAIGHT, true);
+    runTestBytes(Bytes.Mode.STRAIGHT, false);
+  }
+
+  public void testBytesDeref() throws IOException {
+    runTestBytes(Bytes.Mode.DEREF, true);
+    runTestBytes(Bytes.Mode.DEREF, false);
+  }
+  
+  public void testBytesSorted() throws IOException {
+    runTestBytes(Bytes.Mode.SORTED, true);
+    runTestBytes(Bytes.Mode.SORTED, false);
+  }
+
+  public void runTestBytes(final Bytes.Mode mode, final boolean fixedSize)
+      throws IOException {
+
+    final BytesRef bytesRef = new BytesRef();
+
+    Directory dir = newDirectory();
+    final Counter trackBytes = Counter.newCounter();
+    Writer w = Bytes.getWriter(dir, "test", mode, fixedSize, COMP, trackBytes, newIOContext(random));
+    int maxDoc = 220;
+    final String[] values = new String[maxDoc];
+    final int fixedLength = 1 + atLeast(50);
+    for (int i = 0; i < 100; i++) {
+      final String s;
+      if (i > 0 && random.nextInt(5) <= 2) {
+        // use prior value
+        s = values[2 * random.nextInt(i)];
+      } else {
+        s = _TestUtil.randomFixedByteLengthUnicodeString(random, fixedSize? fixedLength : 1 + random.nextInt(39));
+      }
+      values[2 * i] = s;
+
+      UnicodeUtil.UTF16toUTF8(s, 0, s.length(), bytesRef);
+      w.add(2 * i, bytesRef);
+    }
+    w.finish(maxDoc);
+    assertEquals(0, trackBytes.get());
+
+    DocValues r = Bytes.getValues(dir, "test", mode, fixedSize, maxDoc, COMP, newIOContext(random));
+
+    // Verify we can load source twice:
+    for (int iter = 0; iter < 2; iter++) {
+      Source s;
+      DocValues.SortedSource ss;
+      if (mode == Bytes.Mode.SORTED) {
+        // default is unicode so we can simply pass null here
+        s = ss = getSortedSource(r);  
+      } else {
+        s = getSource(r);
+        ss = null;
+      }
+      for (int i = 0; i < 100; i++) {
+        final int idx = 2 * i;
+        assertNotNull("doc " + idx + "; value=" + values[idx], s.getBytes(idx,
+            bytesRef));
+        assertEquals("doc " + idx, values[idx], s.getBytes(idx, bytesRef)
+            .utf8ToString());
+        if (ss != null) {
+          assertEquals("doc " + idx, values[idx], ss.getByOrd(ss.ord(idx),
+              bytesRef).utf8ToString());
+         int ord = ss
+              .getByValue(new BytesRef(values[idx]), new BytesRef());
+          assertTrue(ord >= 0);
+          assertEquals(ss.ord(idx), ord);
+        }
+      }
+
+      // Lookup random strings:
+      if (mode == Bytes.Mode.SORTED) {
+        final int valueCount = ss.getValueCount();
+        for (int i = 0; i < 1000; i++) {
+          BytesRef bytesValue = new BytesRef(_TestUtil.randomFixedByteLengthUnicodeString(random, fixedSize? fixedLength : 1 + random.nextInt(39)));
+          int ord = ss.getByValue(bytesValue, new BytesRef());
+          if (ord >= 0) {
+            assertTrue(bytesValue
+                .bytesEquals(ss.getByOrd(ord, bytesRef)));
+            int count = 0;
+            for (int k = 0; k < 100; k++) {
+              if (bytesValue.utf8ToString().equals(values[2 * k])) {
+                assertEquals(ss.ord(2 * k), ord);
+                count++;
+              }
+            }
+            assertTrue(count > 0);
+          } else {
+            assert ord < 0;
+            int insertIndex = (-ord)-1;
+            if (insertIndex == 0) {
+              final BytesRef firstRef = ss.getByOrd(1, bytesRef);
+              // random string was before our first
+              assertTrue(firstRef.compareTo(bytesValue) > 0);
+            } else if (insertIndex == valueCount) {
+              final BytesRef lastRef = ss.getByOrd(valueCount-1, bytesRef);
+              // random string was after our last
+              assertTrue(lastRef.compareTo(bytesValue) < 0);
+            } else {
+              // TODO: I don't think this actually needs a deep copy?
+              final BytesRef before = BytesRef.deepCopyOf(ss.getByOrd(insertIndex-1, bytesRef));
+              BytesRef after = ss.getByOrd(insertIndex, bytesRef);
+              assertTrue(COMP.compare(before, bytesValue) < 0);
+              assertTrue(COMP.compare(bytesValue, after) < 0);
+            }
+          }
+        }
+      }
+    }
+
+
+    r.close();
+    dir.close();
+  }
+
+  public void testVariableIntsLimits() throws IOException {
+    long[][] minMax = new long[][] { { Long.MIN_VALUE, Long.MAX_VALUE },
+        { Long.MIN_VALUE + 1, 1 }, { -1, Long.MAX_VALUE },
+        { Long.MIN_VALUE, -1 }, { 1, Long.MAX_VALUE },
+        { -1, Long.MAX_VALUE - 1 }, { Long.MIN_VALUE + 2, 1 }, };
+    Type[] expectedTypes = new Type[] { Type.FIXED_INTS_64,
+        Type.FIXED_INTS_64, Type.FIXED_INTS_64,
+        Type.FIXED_INTS_64, Type.VAR_INTS, Type.VAR_INTS,
+        Type.VAR_INTS, };
+    for (int i = 0; i < minMax.length; i++) {
+      Directory dir = newDirectory();
+      final Counter trackBytes = Counter.newCounter();
+      Writer w = Ints.getWriter(dir, "test", trackBytes, Type.VAR_INTS, newIOContext(random));
+      w.add(0, minMax[i][0]);
+      w.add(1, minMax[i][1]);
+      w.finish(2);
+      assertEquals(0, trackBytes.get());
+      DocValues r = Ints.getValues(dir, "test", 2,  Type.VAR_INTS, newIOContext(random));
+      Source source = getSource(r);
+      assertEquals(i + " with min: " + minMax[i][0] + " max: " + minMax[i][1],
+          expectedTypes[i], source.type());
+      assertEquals(minMax[i][0], source.getInt(0));
+      assertEquals(minMax[i][1], source.getInt(1));
+
+      r.close();
+      dir.close();
+    }
+  }
+  
+  public void testVInts() throws IOException {
+    testInts(Type.VAR_INTS, 63);
+  }
+  
+  public void testFixedInts() throws IOException {
+    testInts(Type.FIXED_INTS_64, 63);
+    testInts(Type.FIXED_INTS_32, 31);
+    testInts(Type.FIXED_INTS_16, 15);
+    testInts(Type.FIXED_INTS_8, 7);
+
+  }
+  
+  public void testGetInt8Array() throws IOException {
+    byte[] sourceArray = new byte[] {1,2,3};
+    Directory dir = newDirectory();
+    final Counter trackBytes = Counter.newCounter();
+    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_8, newIOContext(random));
+    for (int i = 0; i < sourceArray.length; i++) {
+      w.add(i, (long) sourceArray[i]);
+    }
+    w.finish(sourceArray.length);
+    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_8, newIOContext(random));
+    Source source = r.getSource();
+    assertTrue(source.hasArray());
+    byte[] loaded = ((byte[])source.getArray());
+    assertEquals(loaded.length, sourceArray.length);
+    for (int i = 0; i < loaded.length; i++) {
+      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
+    }
+    r.close();
+    dir.close();
+  }
+  
+  public void testGetInt16Array() throws IOException {
+    short[] sourceArray = new short[] {1,2,3};
+    Directory dir = newDirectory();
+    final Counter trackBytes = Counter.newCounter();
+    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_16, newIOContext(random));
+    for (int i = 0; i < sourceArray.length; i++) {
+      w.add(i, (long) sourceArray[i]);
+    }
+    w.finish(sourceArray.length);
+    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_16, newIOContext(random));
+    Source source = r.getSource();
+    assertTrue(source.hasArray());
+    short[] loaded = ((short[])source.getArray());
+    assertEquals(loaded.length, sourceArray.length);
+    for (int i = 0; i < loaded.length; i++) {
+      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
+    }
+    r.close();
+    dir.close();
+  }
+  
+  public void testGetInt64Array() throws IOException {
+    long[] sourceArray = new long[] {1,2,3};
+    Directory dir = newDirectory();
+    final Counter trackBytes = Counter.newCounter();
+    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_64, newIOContext(random));
+    for (int i = 0; i < sourceArray.length; i++) {
+      w.add(i, sourceArray[i]);
+    }
+    w.finish(sourceArray.length);
+    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_64, newIOContext(random));
+    Source source = r.getSource();
+    assertTrue(source.hasArray());
+    long[] loaded = ((long[])source.getArray());
+    assertEquals(loaded.length, sourceArray.length);
+    for (int i = 0; i < loaded.length; i++) {
+      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
+    }
+    r.close();
+    dir.close();
+  }
+  
+  public void testGetInt32Array() throws IOException {
+    int[] sourceArray = new int[] {1,2,3};
+    Directory dir = newDirectory();
+    final Counter trackBytes = Counter.newCounter();
+    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_32, newIOContext(random));
+    for (int i = 0; i < sourceArray.length; i++) {
+      w.add(i, (long) sourceArray[i]);
+    }
+    w.finish(sourceArray.length);
+    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_32, newIOContext(random));
+    Source source = r.getSource();
+    assertTrue(source.hasArray());
+    int[] loaded = ((int[])source.getArray());
+    assertEquals(loaded.length, sourceArray.length);
+    for (int i = 0; i < loaded.length; i++) {
+      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
+    }
+    r.close();
+    dir.close();
+  }
+  
+  public void testGetFloat32Array() throws IOException {
+    float[] sourceArray = new float[] {1,2,3};
+    Directory dir = newDirectory();
+    final Counter trackBytes = Counter.newCounter();
+    Writer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random), Type.FLOAT_32);
+    for (int i = 0; i < sourceArray.length; i++) {
+      w.add(i, sourceArray[i]);
+    }
+    w.finish(sourceArray.length);
+    DocValues r = Floats.getValues(dir, "test", 3, newIOContext(random), Type.FLOAT_32);
+    Source source = r.getSource();
+    assertTrue(source.hasArray());
+    float[] loaded = ((float[])source.getArray());
+    assertEquals(loaded.length, sourceArray.length);
+    for (int i = 0; i < loaded.length; i++) {
+      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i], 0.0f);
+    }
+    r.close();
+    dir.close();
+  }
+  
+  public void testGetFloat64Array() throws IOException {
+    double[] sourceArray = new double[] {1,2,3};
+    Directory dir = newDirectory();
+    final Counter trackBytes = Counter.newCounter();
+    Writer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random), Type.FLOAT_64);
+    for (int i = 0; i < sourceArray.length; i++) {
+      w.add(i, sourceArray[i]);
+    }
+    w.finish(sourceArray.length);
+    DocValues r = Floats.getValues(dir, "test", 3, newIOContext(random), Type.FLOAT_64);
+    Source source = r.getSource();
+    assertTrue(source.hasArray());
+    double[] loaded = ((double[])source.getArray());
+    assertEquals(loaded.length, sourceArray.length);
+    for (int i = 0; i < loaded.length; i++) {
+      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i], 0.0d);
+    }
+    r.close();
+    dir.close();
+  }
+
+  private void testInts(Type type, int maxBit) throws IOException {
+    long maxV = 1;
+    final int NUM_VALUES = 333 + random.nextInt(333);
+    final long[] values = new long[NUM_VALUES];
+    for (int rx = 1; rx < maxBit; rx++, maxV *= 2) {
+      Directory dir = newDirectory();
+      final Counter trackBytes = Counter.newCounter();
+      Writer w = Ints.getWriter(dir, "test", trackBytes, type, newIOContext(random));
+      for (int i = 0; i < NUM_VALUES; i++) {
+        final long v = random.nextLong() % (1 + maxV);
+        values[i] = v;
+        w.add(i, v);
+      }
+      final int additionalDocs = 1 + random.nextInt(9);
+      w.finish(NUM_VALUES + additionalDocs);
+      assertEquals(0, trackBytes.get());
+
+      DocValues r = Ints.getValues(dir, "test", NUM_VALUES + additionalDocs, type, newIOContext(random));
+      for (int iter = 0; iter < 2; iter++) {
+        Source s = getSource(r);
+        assertEquals(type, s.type());
+        for (int i = 0; i < NUM_VALUES; i++) {
+          final long v = s.getInt(i);
+          assertEquals("index " + i, values[i], v);
+        }
+      }
+
+      r.close();
+      dir.close();
+    }
+  }
+
+  public void testFloats4() throws IOException {
+    runTestFloats(Type.FLOAT_32, 0.00001);
+  }
+
+  private void runTestFloats(Type type, double delta) throws IOException {
+    Directory dir = newDirectory();
+    final Counter trackBytes = Counter.newCounter();
+    Writer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random), type);
+    final int NUM_VALUES = 777 + random.nextInt(777);;
+    final double[] values = new double[NUM_VALUES];
+    for (int i = 0; i < NUM_VALUES; i++) {
+      final double v = type == Type.FLOAT_32 ? random.nextFloat() : random
+          .nextDouble();
+      values[i] = v;
+      w.add(i, v);
+    }
+    final int additionalValues = 1 + random.nextInt(10);
+    w.finish(NUM_VALUES + additionalValues);
+    assertEquals(0, trackBytes.get());
+
+    DocValues r = Floats.getValues(dir, "test", NUM_VALUES + additionalValues, newIOContext(random), type);
+    for (int iter = 0; iter < 2; iter++) {
+      Source s = getSource(r);
+      for (int i = 0; i < NUM_VALUES; i++) {
+        assertEquals("" + i, values[i], s.getFloat(i), 0.0f);
+      }
+    }
+    r.close();
+    dir.close();
+  }
+
+  public void testFloats8() throws IOException {
+    runTestFloats(Type.FLOAT_64, 0.0);
+  }
+  
+
+  private Source getSource(DocValues values) throws IOException {
+    // getSource uses cache internally
+    switch(random.nextInt(5)) {
+    case 3:
+      return values.load();
+    case 2:
+      return values.getDirectSource();
+    case 1:
+      return values.getSource();
+    default:
+      return values.getSource();
+    }
+  }
+  
+  private SortedSource getSortedSource(DocValues values) throws IOException {
+    return getSource(values).asSortedSource();
+  }
+  
+}
diff --git a/lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java b/lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
new file mode 100644
index 0000000..d3e778d
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/codecs/lucene40/TestReuseDocsEnum.java
@@ -0,0 +1,174 @@
+package org.apache.lucene.codecs.lucene40;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.IdentityHashMap;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.Bits.MatchNoBits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.LineFileDocs;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+public class TestReuseDocsEnum extends LuceneTestCase {
+
+  public void testReuseDocsEnumNoReuse() throws IOException {
+    Directory dir = newDirectory();
+    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());
+    RandomIndexWriter writer = new RandomIndexWriter(random, dir,
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
+    int numdocs = atLeast(20);
+    createRandomIndex(numdocs, writer, random);
+    writer.commit();
+
+    IndexReader open = IndexReader.open(dir);
+    IndexReader[] sequentialSubReaders = open.getSequentialSubReaders();
+    for (IndexReader indexReader : sequentialSubReaders) {
+      Terms terms = indexReader.terms("body");
+      TermsEnum iterator = terms.iterator(null);
+      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
+      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());
+      while ((iterator.next()) != null) {
+        DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(open.maxDoc()), null, random.nextBoolean());
+        enums.put(docs, true);
+      }
+      
+      assertEquals(terms.getUniqueTermCount(), enums.size());  
+    }
+    IOUtils.close(writer, open, dir);
+  }
+  
+  // tests for reuse only if bits are the same either null or the same instance
+  public void testReuseDocsEnumSameBitsOrNull() throws IOException {
+    Directory dir = newDirectory();
+    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());
+    RandomIndexWriter writer = new RandomIndexWriter(random, dir,
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
+    int numdocs = atLeast(20);
+    createRandomIndex(numdocs, writer, random);
+    writer.commit();
+
+    IndexReader open = IndexReader.open(dir);
+    IndexReader[] sequentialSubReaders = open.getSequentialSubReaders();
+    for (IndexReader indexReader : sequentialSubReaders) {
+      Terms terms = indexReader.terms("body");
+      TermsEnum iterator = terms.iterator(null);
+      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
+      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());
+      DocsEnum docs = null;
+      while ((iterator.next()) != null) {
+        docs = iterator.docs(bits, docs, random.nextBoolean());
+        enums.put(docs, true);
+      }
+      
+      assertEquals(1, enums.size());
+      enums.clear();
+      iterator = terms.iterator(null);
+      docs = null;
+      while ((iterator.next()) != null) {
+        docs = iterator.docs(new Bits.MatchNoBits(open.maxDoc()), docs, random.nextBoolean());
+        enums.put(docs, true);
+      }
+      assertEquals(terms.getUniqueTermCount(), enums.size());  
+      
+      enums.clear();
+      iterator = terms.iterator(null);
+      docs = null;
+      while ((iterator.next()) != null) {
+        docs = iterator.docs(null, docs, random.nextBoolean());
+        enums.put(docs, true);
+      }
+      assertEquals(1, enums.size());  
+    }
+    IOUtils.close(writer, open, dir);
+  }
+  
+  // make sure we never reuse from another reader even if it is the same field & codec etc
+  public void testReuseDocsEnumDifferentReader() throws IOException {
+    Directory dir = newDirectory();
+    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());
+    RandomIndexWriter writer = new RandomIndexWriter(random, dir,
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
+    int numdocs = atLeast(20);
+    createRandomIndex(numdocs, writer, random);
+    writer.commit();
+
+    IndexReader firstReader = IndexReader.open(dir);
+    IndexReader secondReader = IndexReader.open(dir);
+    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();
+    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();
+    
+    for (IndexReader indexReader : sequentialSubReaders) {
+      Terms terms = indexReader.terms("body");
+      TermsEnum iterator = terms.iterator(null);
+      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
+      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());
+      iterator = terms.iterator(null);
+      DocsEnum docs = null;
+      BytesRef term = null;
+      while ((term = iterator.next()) != null) {
+        docs = iterator.docs(null, randomDocsEnum("body", term, sequentialSubReaders2, bits), random.nextBoolean());
+        enums.put(docs, true);
+      }
+      assertEquals(terms.getUniqueTermCount(), enums.size());  
+      
+      iterator = terms.iterator(null);
+      enums.clear();
+      docs = null;
+      while ((term = iterator.next()) != null) {
+        docs = iterator.docs(bits, randomDocsEnum("body", term, sequentialSubReaders2, bits), random.nextBoolean());
+        enums.put(docs, true);
+      }
+      assertEquals(terms.getUniqueTermCount(), enums.size());  
+    }
+    IOUtils.close(writer, firstReader, secondReader, dir);
+  }
+  
+  public DocsEnum randomDocsEnum(String field, BytesRef term, IndexReader[] readers, Bits bits) throws IOException {
+    if (random.nextInt(10) == 0) {
+      return null;
+    }
+    IndexReader indexReader = readers[random.nextInt(readers.length)];
+    return indexReader.termDocsEnum(bits, field, term, random.nextBoolean());
+  }
+
+  /**
+   * populates a writer with random stuff. this must be fully reproducable with
+   * the seed!
+   */
+  public static void createRandomIndex(int numdocs, RandomIndexWriter writer,
+      Random random) throws IOException {
+    LineFileDocs lineFileDocs = new LineFileDocs(random);
+
+    for (int i = 0; i < numdocs; i++) {
+      writer.addDocument(lineFileDocs.nextDoc());
+    }
+  }
+
+}
diff --git a/lucene/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat.java b/lucene/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat.java
new file mode 100644
index 0000000..1b8e1d1
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/codecs/perfield/TestPerFieldPostingsFormat.java
@@ -0,0 +1,267 @@
+package org.apache.lucene.codecs.perfield;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40Codec;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
+import org.apache.lucene.codecs.mocksep.MockSepPostingsFormat;
+import org.apache.lucene.codecs.simpletext.SimpleTextPostingsFormat;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LogDocMergePolicy;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Test;
+
+/**
+ * 
+ *
+ */
+//TODO: would be better in this test to pull termsenums and instanceof or something?
+// this way we can verify PFPF is doing the right thing.
+// for now we do termqueries.
+public class TestPerFieldPostingsFormat extends LuceneTestCase {
+
+  private IndexWriter newWriter(Directory dir, IndexWriterConfig conf)
+      throws IOException {
+    LogDocMergePolicy logByteSizeMergePolicy = new LogDocMergePolicy();
+    logByteSizeMergePolicy.setUseCompoundFile(false); // make sure we use plain
+    // files
+    conf.setMergePolicy(logByteSizeMergePolicy);
+
+    final IndexWriter writer = new IndexWriter(dir, conf);
+    return writer;
+  }
+
+  private void addDocs(IndexWriter writer, int numDocs) throws IOException {
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      doc.add(newField("content", "aaa", TextField.TYPE_UNSTORED));
+      writer.addDocument(doc);
+    }
+  }
+
+  private void addDocs2(IndexWriter writer, int numDocs) throws IOException {
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      doc.add(newField("content", "bbb", TextField.TYPE_UNSTORED));
+      writer.addDocument(doc);
+    }
+  }
+
+  private void addDocs3(IndexWriter writer, int numDocs) throws IOException {
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      doc.add(newField("content", "ccc", TextField.TYPE_UNSTORED));
+      doc.add(newField("id", "" + i, StringField.TYPE_STORED));
+      writer.addDocument(doc);
+    }
+  }
+
+  /*
+   * Test that heterogeneous index segments are merge successfully
+   */
+  @Test
+  public void testMergeUnusedPerFieldCodec() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT,
+        new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE).setCodec(new MockCodec());
+    IndexWriter writer = newWriter(dir, iwconf);
+    addDocs(writer, 10);
+    writer.commit();
+    addDocs3(writer, 10);
+    writer.commit();
+    addDocs2(writer, 10);
+    writer.commit();
+    assertEquals(30, writer.maxDoc());
+    _TestUtil.checkIndex(dir);
+    writer.forceMerge(1);
+    assertEquals(30, writer.maxDoc());
+    writer.close();
+    dir.close();
+  }
+
+  /*
+   * Test that heterogeneous index segments are merged sucessfully
+   */
+  // TODO: not sure this test is that great, we should probably peek inside PerFieldPostingsFormat or something?!
+  @Test
+  public void testChangeCodecAndMerge() throws IOException {
+    Directory dir = newDirectory();
+    if (VERBOSE) {
+      System.out.println("TEST: make new index");
+    }
+    IndexWriterConfig iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT,
+             new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE).setCodec(new MockCodec());
+    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);
+    IndexWriter writer = newWriter(dir, iwconf);
+
+    addDocs(writer, 10);
+    writer.commit();
+    assertQuery(new Term("content", "aaa"), dir, 10);
+    if (VERBOSE) {
+      System.out.println("TEST: addDocs3");
+    }
+    addDocs3(writer, 10);
+    writer.commit();
+    writer.close();
+
+    assertQuery(new Term("content", "ccc"), dir, 10);
+    assertQuery(new Term("content", "aaa"), dir, 10);
+    Lucene40Codec codec = (Lucene40Codec)iwconf.getCodec();
+
+    iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))
+        .setOpenMode(OpenMode.APPEND).setCodec(codec);
+    //((LogMergePolicy) iwconf.getMergePolicy()).setUseCompoundFile(false);
+    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);
+    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);
+
+    iwconf.setCodec(new MockCodec2()); // uses standard for field content
+    writer = newWriter(dir, iwconf);
+    // swap in new codec for currently written segments
+    if (VERBOSE) {
+      System.out.println("TEST: add docs w/ Standard codec for content field");
+    }
+    addDocs2(writer, 10);
+    writer.commit();
+    codec = (Lucene40Codec)iwconf.getCodec();
+    assertEquals(30, writer.maxDoc());
+    assertQuery(new Term("content", "bbb"), dir, 10);
+    assertQuery(new Term("content", "ccc"), dir, 10);   ////
+    assertQuery(new Term("content", "aaa"), dir, 10);
+
+    if (VERBOSE) {
+      System.out.println("TEST: add more docs w/ new codec");
+    }
+    addDocs2(writer, 10);
+    writer.commit();
+    assertQuery(new Term("content", "ccc"), dir, 10);
+    assertQuery(new Term("content", "bbb"), dir, 20);
+    assertQuery(new Term("content", "aaa"), dir, 10);
+    assertEquals(40, writer.maxDoc());
+
+    if (VERBOSE) {
+      System.out.println("TEST: now optimize");
+    }
+    writer.forceMerge(1);
+    assertEquals(40, writer.maxDoc());
+    writer.close();
+    assertQuery(new Term("content", "ccc"), dir, 10);
+    assertQuery(new Term("content", "bbb"), dir, 20);
+    assertQuery(new Term("content", "aaa"), dir, 10);
+
+    dir.close();
+  }
+
+  public void assertQuery(Term t, Directory dir, int num)
+      throws CorruptIndexException, IOException {
+    if (VERBOSE) {
+      System.out.println("\nTEST: assertQuery " + t);
+    }
+    IndexReader reader = IndexReader.open(dir, 1);
+    IndexSearcher searcher = newSearcher(reader);
+    TopDocs search = searcher.search(new TermQuery(t), num + 10);
+    assertEquals(num, search.totalHits);
+    reader.close();
+
+  }
+
+  public static class MockCodec extends Lucene40Codec {
+    final PostingsFormat lucene40 = new Lucene40PostingsFormat();
+    final PostingsFormat simpleText = new SimpleTextPostingsFormat();
+    final PostingsFormat mockSep = new MockSepPostingsFormat();
+    
+    @Override
+    public PostingsFormat getPostingsFormatForField(String field) {
+      if (field.equals("id")) {
+        return simpleText;
+      } else if (field.equals("content")) {
+        return mockSep;
+      } else {
+        return lucene40;
+      }
+    }
+  }
+
+  public static class MockCodec2 extends Lucene40Codec {
+    final PostingsFormat lucene40 = new Lucene40PostingsFormat();
+    final PostingsFormat simpleText = new SimpleTextPostingsFormat();
+    
+    @Override
+    public PostingsFormat getPostingsFormatForField(String field) {
+      if (field.equals("id")) {
+        return simpleText;
+      } else {
+        return lucene40;
+      }
+    }
+  }
+
+  /*
+   * Test per field codec support - adding fields with random codecs
+   */
+  @Test
+  public void testStressPerFieldCodec() throws IOException {
+    Directory dir = newDirectory(random);
+    final int docsPerRound = 97;
+    int numRounds = atLeast(1);
+    for (int i = 0; i < numRounds; i++) {
+      int num = _TestUtil.nextInt(random, 30, 60);
+      IndexWriterConfig config = newIndexWriterConfig(random,
+          TEST_VERSION_CURRENT, new MockAnalyzer(random));
+      config.setOpenMode(OpenMode.CREATE_OR_APPEND);
+      IndexWriter writer = newWriter(dir, config);
+      for (int j = 0; j < docsPerRound; j++) {
+        final Document doc = new Document();
+        for (int k = 0; k < num; k++) {
+          FieldType customType = new FieldType(TextField.TYPE_UNSTORED);
+          customType.setTokenized(random.nextBoolean());
+          customType.setOmitNorms(random.nextBoolean());
+          Field field = newField("" + k, _TestUtil
+              .randomRealisticUnicodeString(random, 128), customType);
+          doc.add(field);
+        }
+        writer.addDocument(doc);
+      }
+      if (random.nextBoolean()) {
+        writer.forceMerge(1);
+      }
+      writer.commit();
+      assertEquals((i + 1) * docsPerRound, writer.maxDoc());
+      writer.close();
+    }
+    dir.close();
+  }
+}
diff --git a/lucene/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java b/lucene/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java
new file mode 100644
index 0000000..50c2eb9
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java
@@ -0,0 +1,156 @@
+package org.apache.lucene.codecs.pulsing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.text.DecimalFormat;
+import java.text.DecimalFormatSymbols;
+import java.text.NumberFormat;
+import java.util.Locale;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.pulsing.Pulsing40PostingsFormat;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Pulses 10k terms/docs, 
+ * originally designed to find JRE bugs (https://issues.apache.org/jira/browse/LUCENE-3335)
+ * 
+ * @lucene.experimental
+ */
+public class Test10KPulsings extends LuceneTestCase {
+  public void test10kPulsed() throws Exception {
+    // we always run this test with pulsing codec.
+    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));
+    
+    File f = _TestUtil.getTempDir("10kpulsed");
+    MockDirectoryWrapper dir = newFSDirectory(f);
+    dir.setCheckIndexOnClose(false); // we do this ourselves explicitly
+    RandomIndexWriter iw = new RandomIndexWriter(random, dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
+    
+    Document document = new Document();
+    FieldType ft = new FieldType(TextField.TYPE_STORED);
+    
+    switch(_TestUtil.nextInt(random, 0, 2)) {
+      case 0: ft.setIndexOptions(IndexOptions.DOCS_ONLY); break;
+      case 1: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS); break;
+      default: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); break; 
+    }
+
+    Field field = newField("field", "", ft);
+    document.add(field);
+    
+    NumberFormat df = new DecimalFormat("00000", new DecimalFormatSymbols(Locale.ENGLISH));
+
+    for (int i = 0; i < 10050; i++) {
+      field.setValue(df.format(i));
+      iw.addDocument(document);
+    }
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+
+    TermsEnum te = MultiFields.getTerms(ir, "field").iterator(null);
+    DocsEnum de = null;
+    
+    for (int i = 0; i < 10050; i++) {
+      String expected = df.format(i);
+      assertEquals(expected, te.next().utf8ToString());
+      de = _TestUtil.docs(random, te, null, de, false);
+      assertTrue(de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
+      assertEquals(DocIdSetIterator.NO_MORE_DOCS, de.nextDoc());
+    }
+    ir.close();
+
+    _TestUtil.checkIndex(dir);
+    dir.close();
+  }
+  
+  /** a variant, that uses pulsing, but uses a high TF to force pass thru to the underlying codec
+   */
+  public void test10kNotPulsed() throws Exception {
+    // we always run this test with pulsing codec.
+    int freqCutoff = _TestUtil.nextInt(random, 1, 10);
+    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(freqCutoff));
+    
+    File f = _TestUtil.getTempDir("10knotpulsed");
+    MockDirectoryWrapper dir = newFSDirectory(f);
+    dir.setCheckIndexOnClose(false); // we do this ourselves explicitly
+    RandomIndexWriter iw = new RandomIndexWriter(random, dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
+    
+    Document document = new Document();
+    FieldType ft = new FieldType(TextField.TYPE_STORED);
+    
+    switch(_TestUtil.nextInt(random, 0, 2)) {
+      case 0: ft.setIndexOptions(IndexOptions.DOCS_ONLY); break;
+      case 1: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS); break;
+      default: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); break; 
+    }
+
+    Field field = newField("field", "", ft);
+    document.add(field);
+    
+    NumberFormat df = new DecimalFormat("00000", new DecimalFormatSymbols(Locale.ENGLISH));
+
+    final int freq = freqCutoff + 1;
+    
+    for (int i = 0; i < 10050; i++) {
+      StringBuilder sb = new StringBuilder();
+      for (int j = 0; j < freq; j++) {
+        sb.append(df.format(i));
+        sb.append(' '); // whitespace
+      }
+      field.setValue(sb.toString());
+      iw.addDocument(document);
+    }
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+
+    TermsEnum te = MultiFields.getTerms(ir, "field").iterator(null);
+    DocsEnum de = null;
+    
+    for (int i = 0; i < 10050; i++) {
+      String expected = df.format(i);
+      assertEquals(expected, te.next().utf8ToString());
+      de = _TestUtil.docs(random, te, null, de, false);
+      assertTrue(de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
+      assertEquals(DocIdSetIterator.NO_MORE_DOCS, de.nextDoc());
+    }
+    ir.close();
+
+    _TestUtil.checkIndex(dir);
+    dir.close();
+  }
+}
diff --git a/lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java b/lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java
new file mode 100644
index 0000000..8170863
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java
@@ -0,0 +1,126 @@
+package org.apache.lucene.codecs.pulsing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.IdentityHashMap;
+import java.util.Map;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.nestedpulsing.NestedPulsingPostingsFormat;
+import org.apache.lucene.codecs.pulsing.Pulsing40PostingsFormat;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.CheckIndex;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Tests that pulsing codec reuses its enums and wrapped enums
+ */
+public class TestPulsingReuse extends LuceneTestCase {
+  // TODO: this is a basic test. this thing is complicated, add more
+  public void testSophisticatedReuse() throws Exception {
+    // we always run this test with pulsing codec.
+    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));
+    Directory dir = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random, dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
+    Document doc = new Document();
+    doc.add(new Field("foo", "a b b c c c d e f g g h i i j j k", TextField.TYPE_UNSTORED));
+    iw.addDocument(doc);
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexReader segment = ir.getSequentialSubReaders()[0];
+    DocsEnum reuse = null;
+    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
+    TermsEnum te = segment.terms("foo").iterator(null);
+    while (te.next() != null) {
+      reuse = te.docs(null, reuse, false);
+      allEnums.put(reuse, true);
+    }
+    
+    assertEquals(2, allEnums.size());
+    
+    allEnums.clear();
+    DocsAndPositionsEnum posReuse = null;
+    te = segment.terms("foo").iterator(null);
+    while (te.next() != null) {
+      posReuse = te.docsAndPositions(null, posReuse);
+      allEnums.put(posReuse, true);
+    }
+    
+    assertEquals(2, allEnums.size());
+    
+    ir.close();
+    dir.close();
+  }
+  
+  /** tests reuse with Pulsing1(Pulsing2(Standard)) */
+  public void testNestedPulsing() throws Exception {
+    // we always run this test with pulsing codec.
+    Codec cp = _TestUtil.alwaysPostingsFormat(new NestedPulsingPostingsFormat());
+    MockDirectoryWrapper dir = newDirectory();
+    dir.setCheckIndexOnClose(false); // will do this ourselves, custom codec
+    RandomIndexWriter iw = new RandomIndexWriter(random, dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
+    Document doc = new Document();
+    doc.add(new Field("foo", "a b b c c c d e f g g g h i i j j k l l m m m", TextField.TYPE_UNSTORED));
+    // note: the reuse is imperfect, here we would have 4 enums (lost reuse when we get an enum for 'm')
+    // this is because we only track the 'last' enum we reused (not all).
+    // but this seems 'good enough' for now.
+    iw.addDocument(doc);
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexReader segment = ir.getSequentialSubReaders()[0];
+    DocsEnum reuse = null;
+    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
+    TermsEnum te = segment.terms("foo").iterator(null);
+    while (te.next() != null) {
+      reuse = te.docs(null, reuse, false);
+      allEnums.put(reuse, true);
+    }
+    
+    assertEquals(4, allEnums.size());
+    
+    allEnums.clear();
+    DocsAndPositionsEnum posReuse = null;
+    te = segment.terms("foo").iterator(null);
+    while (te.next() != null) {
+      posReuse = te.docsAndPositions(null, posReuse);
+      allEnums.put(posReuse, true);
+    }
+    
+    assertEquals(4, allEnums.size());
+    
+    ir.close();
+    CheckIndex ci = new CheckIndex(dir);
+    ci.checkIndex(null);
+    dir.close();
+  }
+}
diff --git a/lucene/src/test/org/apache/lucene/index/Test2BTerms.java b/lucene/src/test/org/apache/lucene/index/Test2BTerms.java
index 41648a0..11ebbef 100644
--- a/lucene/src/test/org/apache/lucene/index/Test2BTerms.java
+++ b/lucene/src/test/org/apache/lucene/index/Test2BTerms.java
@@ -22,9 +22,9 @@ import org.apache.lucene.store.*;
 import org.apache.lucene.search.*;
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.analysis.tokenattributes.*;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.*;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.codecs.Codec;
 
 import java.io.IOException;
 import java.util.ArrayList;
diff --git a/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java b/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
index 0ab5a28..6a7b360 100755
--- a/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
+++ b/lucene/src/test/org/apache/lucene/index/TestAddIndexes.java
@@ -24,6 +24,22 @@ import java.util.Arrays;
 import java.util.List;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfosFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40Codec;
+import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
+import org.apache.lucene.codecs.pulsing.Pulsing40PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -31,22 +47,6 @@ import org.apache.lucene.document.DocValuesField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.DocValuesFormat;
-import org.apache.lucene.index.codecs.FieldInfosFormat;
-import org.apache.lucene.index.codecs.NormsFormat;
-import org.apache.lucene.index.codecs.StoredFieldsFormat;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.SegmentInfosFormat;
-import org.apache.lucene.index.codecs.TermVectorsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40Codec;
-import org.apache.lucene.index.codecs.lucene40.Lucene40FieldInfosFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40DocValuesFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40NormsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40SegmentInfosFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40StoredFieldsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40TermVectorsFormat;
-import org.apache.lucene.index.codecs.pulsing.Pulsing40PostingsFormat;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.PhraseQuery;
 import org.apache.lucene.store.AlreadyClosedException;
diff --git a/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java b/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
index aca8a0e..91392d9 100644
--- a/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
+++ b/lucene/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
@@ -27,6 +27,8 @@ import java.util.List;
 import java.util.Random;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -35,8 +37,6 @@ import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.FieldInfosReader;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
diff --git a/lucene/src/test/org/apache/lucene/index/TestBinaryTerms.java b/lucene/src/test/org/apache/lucene/index/TestBinaryTerms.java
index 2bd2bcd..360e872 100644
--- a/lucene/src/test/org/apache/lucene/index/TestBinaryTerms.java
+++ b/lucene/src/test/org/apache/lucene/index/TestBinaryTerms.java
@@ -19,11 +19,11 @@ package org.apache.lucene.index;
 
 import java.io.IOException;
 
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.TopDocs;
diff --git a/lucene/src/test/org/apache/lucene/index/TestCodecs.java b/lucene/src/test/org/apache/lucene/index/TestCodecs.java
index 2e7763c..5f7dc09 100644
--- a/lucene/src/test/org/apache/lucene/index/TestCodecs.java
+++ b/lucene/src/test/org/apache/lucene/index/TestCodecs.java
@@ -22,18 +22,18 @@ import java.util.Arrays;
 import java.util.HashSet;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsConsumer;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.codecs.lucene3x.Lucene3xCodec;
+import org.apache.lucene.codecs.mocksep.MockSepPostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.FieldsConsumer;
-import org.apache.lucene.index.codecs.FieldsProducer;
-import org.apache.lucene.index.codecs.PostingsConsumer;
-import org.apache.lucene.index.codecs.TermStats;
-import org.apache.lucene.index.codecs.TermsConsumer;
-import org.apache.lucene.index.codecs.lucene3x.Lucene3xCodec;
-import org.apache.lucene.index.codecs.mocksep.MockSepPostingsFormat;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.PhraseQuery;
diff --git a/lucene/src/test/org/apache/lucene/index/TestDoc.java b/lucene/src/test/org/apache/lucene/index/TestDoc.java
index 1c9af88..dd15419 100644
--- a/lucene/src/test/org/apache/lucene/index/TestDoc.java
+++ b/lucene/src/test/org/apache/lucene/index/TestDoc.java
@@ -30,10 +30,10 @@ import junit.framework.TestSuite;
 import junit.textui.TestRunner;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
diff --git a/lucene/src/test/org/apache/lucene/index/TestDocCount.java b/lucene/src/test/org/apache/lucene/index/TestDocCount.java
index 223bb0d..9609b42 100644
--- a/lucene/src/test/org/apache/lucene/index/TestDocCount.java
+++ b/lucene/src/test/org/apache/lucene/index/TestDocCount.java
@@ -17,9 +17,9 @@ package org.apache.lucene.index;
  * limitations under the License.
  */
 
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.FixedBitSet;
diff --git a/lucene/src/test/org/apache/lucene/index/TestDocTermOrds.java b/lucene/src/test/org/apache/lucene/index/TestDocTermOrds.java
index 2c4fa7a..7fe0df8 100644
--- a/lucene/src/test/org/apache/lucene/index/TestDocTermOrds.java
+++ b/lucene/src/test/org/apache/lucene/index/TestDocTermOrds.java
@@ -24,14 +24,14 @@ import java.util.HashSet;
 import java.util.Set;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.NumericField;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.DocTermOrds.TermOrdsIterator;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.PostingsFormat;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.MockDirectoryWrapper;
diff --git a/lucene/src/test/org/apache/lucene/index/TestDocValuesIndexing.java b/lucene/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
new file mode 100644
index 0000000..30bd027
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/index/TestDocValuesIndexing.java
@@ -0,0 +1,591 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.EnumSet;
+import java.util.List;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.PerDocProducer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.DocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.LogDocMergePolicy;
+import org.apache.lucene.index.LogMergePolicy;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.search.*;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Before;
+
+/**
+ * 
+ * Tests DocValues integration into IndexWriter & Codecs
+ * 
+ */
+public class TestDocValuesIndexing extends LuceneTestCase {
+  /*
+   * - add test for multi segment case with deletes
+   * - add multithreaded tests / integrate into stress indexing?
+   */
+
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+    assumeFalse("cannot work with preflex codec", Codec.getDefault().getName().equals("Lucene3x"));
+  }
+  
+  /*
+   * Simple test case to show how to use the API
+   */
+  public void testDocValuesSimple() throws CorruptIndexException, IOException {
+    Directory dir = newDirectory();
+    IndexWriter writer = new IndexWriter(dir, writerConfig(false));
+    for (int i = 0; i < 5; i++) {
+      Document doc = new Document();
+      DocValuesField valuesField = new DocValuesField("docId");
+      valuesField.setInt(i);
+      doc.add(valuesField);
+      doc.add(new TextField("docId", "" + i));
+      writer.addDocument(doc);
+    }
+    writer.commit();
+    writer.forceMerge(1, true);
+
+    writer.close(true);
+
+    IndexReader reader = IndexReader.open(dir, 1);
+    assertEquals(1, reader.getSequentialSubReaders().length);
+
+    IndexSearcher searcher = new IndexSearcher(reader);
+
+    BooleanQuery query = new BooleanQuery();
+    query.add(new TermQuery(new Term("docId", "0")), BooleanClause.Occur.SHOULD);
+    query.add(new TermQuery(new Term("docId", "1")), BooleanClause.Occur.SHOULD);
+    query.add(new TermQuery(new Term("docId", "2")), BooleanClause.Occur.SHOULD);
+    query.add(new TermQuery(new Term("docId", "3")), BooleanClause.Occur.SHOULD);
+    query.add(new TermQuery(new Term("docId", "4")), BooleanClause.Occur.SHOULD);
+
+    TopDocs search = searcher.search(query, 10);
+    assertEquals(5, search.totalHits);
+    ScoreDoc[] scoreDocs = search.scoreDocs;
+    DocValues docValues = MultiDocValues.getDocValues(reader, "docId");
+    Source source = docValues.getSource();
+    for (int i = 0; i < scoreDocs.length; i++) {
+      assertEquals(i, scoreDocs[i].doc);
+      assertEquals(i, source.getInt(scoreDocs[i].doc));
+    }
+    reader.close();
+    dir.close();
+  }
+
+  public void testIndexBytesNoDeletes() throws IOException {
+    runTestIndexBytes(writerConfig(random.nextBoolean()), false);
+  }
+
+  public void testIndexBytesDeletes() throws IOException {
+    runTestIndexBytes(writerConfig(random.nextBoolean()), true);
+  }
+
+  public void testIndexNumericsNoDeletes() throws IOException {
+    runTestNumerics(writerConfig(random.nextBoolean()), false);
+  }
+
+  public void testIndexNumericsDeletes() throws IOException {
+    runTestNumerics(writerConfig(random.nextBoolean()), true);
+  }
+
+  public void testAddIndexes() throws IOException {
+    int valuesPerIndex = 10;
+    List<Type> values = Arrays.asList(Type.values());
+    Collections.shuffle(values, random);
+    Type first = values.get(0);
+    Type second = values.get(1);
+    // index first index
+    Directory d_1 = newDirectory();
+    IndexWriter w_1 = new IndexWriter(d_1, writerConfig(random.nextBoolean()));
+    indexValues(w_1, valuesPerIndex, first, values, false, 7);
+    w_1.commit();
+    assertEquals(valuesPerIndex, w_1.maxDoc());
+    _TestUtil.checkIndex(d_1);
+
+    // index second index
+    Directory d_2 = newDirectory();
+    IndexWriter w_2 = new IndexWriter(d_2, writerConfig(random.nextBoolean()));
+    indexValues(w_2, valuesPerIndex, second, values, false, 7);
+    w_2.commit();
+    assertEquals(valuesPerIndex, w_2.maxDoc());
+    _TestUtil.checkIndex(d_2);
+
+    Directory target = newDirectory();
+    IndexWriter w = new IndexWriter(target, writerConfig(random.nextBoolean()));
+    IndexReader r_1 = IndexReader.open(w_1, true);
+    IndexReader r_2 = IndexReader.open(w_2, true);
+    if (random.nextBoolean()) {
+      w.addIndexes(d_1, d_2);
+    } else {
+      w.addIndexes(r_1, r_2);
+    }
+    w.forceMerge(1, true);
+    w.commit();
+    
+    _TestUtil.checkIndex(target);
+    assertEquals(valuesPerIndex * 2, w.maxDoc());
+
+    // check values
+    
+    IndexReader merged = IndexReader.open(w, true);
+    Source source_1 = getSource(getDocValues(r_1, first.name()));
+    Source source_2 = getSource(getDocValues(r_2, second.name()));
+    Source source_1_merged = getSource(getDocValues(merged, first.name()));
+    Source source_2_merged = getSource(getDocValues(merged, second
+        .name()));
+    for (int i = 0; i < r_1.maxDoc(); i++) {
+      switch (first) {
+      case BYTES_FIXED_DEREF:
+      case BYTES_FIXED_STRAIGHT:
+      case BYTES_VAR_DEREF:
+      case BYTES_VAR_STRAIGHT:
+      case BYTES_FIXED_SORTED:
+      case BYTES_VAR_SORTED:
+        assertEquals(source_1.getBytes(i, new BytesRef()),
+            source_1_merged.getBytes(i, new BytesRef()));
+        break;
+      case FIXED_INTS_16:
+      case FIXED_INTS_32:
+      case FIXED_INTS_64:
+      case FIXED_INTS_8:
+      case VAR_INTS:
+        assertEquals(source_1.getInt(i), source_1_merged.getInt(i));
+        break;
+      case FLOAT_32:
+      case FLOAT_64:
+        assertEquals(source_1.getFloat(i), source_1_merged.getFloat(i), 0.0d);
+        break;
+      default:
+        fail("unkonwn " + first);
+      }
+    }
+
+    for (int i = r_1.maxDoc(); i < merged.maxDoc(); i++) {
+      switch (second) {
+      case BYTES_FIXED_DEREF:
+      case BYTES_FIXED_STRAIGHT:
+      case BYTES_VAR_DEREF:
+      case BYTES_VAR_STRAIGHT:
+      case BYTES_FIXED_SORTED:
+      case BYTES_VAR_SORTED:
+        assertEquals(source_2.getBytes(i - r_1.maxDoc(), new BytesRef()),
+            source_2_merged.getBytes(i, new BytesRef()));
+        break;
+      case FIXED_INTS_16:
+      case FIXED_INTS_32:
+      case FIXED_INTS_64:
+      case FIXED_INTS_8:
+      case VAR_INTS:
+        assertEquals(source_2.getInt(i - r_1.maxDoc()),
+            source_2_merged.getInt(i));
+        break;
+      case FLOAT_32:
+      case FLOAT_64:
+        assertEquals(source_2.getFloat(i - r_1.maxDoc()),
+            source_2_merged.getFloat(i), 0.0d);
+        break;
+      default:
+        fail("unkonwn " + first);
+      }
+    }
+    // close resources
+    r_1.close();
+    r_2.close();
+    merged.close();
+    w_1.close(true);
+    w_2.close(true);
+    w.close(true);
+    d_1.close();
+    d_2.close();
+    target.close();
+  }
+
+  private IndexWriterConfig writerConfig(boolean useCompoundFile) {
+    final IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT,
+        new MockAnalyzer(random));
+    cfg.setMergePolicy(newLogMergePolicy(random));
+    LogMergePolicy policy = new LogDocMergePolicy();
+    cfg.setMergePolicy(policy);
+    policy.setUseCompoundFile(useCompoundFile);
+    return cfg;
+  }
+
+  @SuppressWarnings("fallthrough")
+  public void runTestNumerics(IndexWriterConfig cfg, boolean withDeletions)
+      throws IOException {
+    Directory d = newDirectory();
+    IndexWriter w = new IndexWriter(d, cfg);
+    final int numValues = 50 + atLeast(10);
+    final List<Type> numVariantList = new ArrayList<Type>(NUMERICS);
+
+    // run in random order to test if fill works correctly during merges
+    Collections.shuffle(numVariantList, random);
+    for (Type val : numVariantList) {
+      FixedBitSet deleted = indexValues(w, numValues, val, numVariantList,
+          withDeletions, 7);
+      List<Closeable> closeables = new ArrayList<Closeable>();
+      IndexReader r = IndexReader.open(w, true);
+      final int numRemainingValues = numValues - deleted.cardinality();
+      final int base = r.numDocs() - numRemainingValues;
+      // for FIXED_INTS_8 we use value mod 128 - to enable testing in 
+      // one go we simply use numValues as the mod for all other INT types
+      int mod = numValues;
+      switch (val) {
+      case FIXED_INTS_8:
+        mod = 128;
+      case FIXED_INTS_16:
+      case FIXED_INTS_32:
+      case FIXED_INTS_64:
+      case VAR_INTS: {
+        DocValues intsReader = getDocValues(r, val.name());
+        assertNotNull(intsReader);
+
+        Source ints = getSource(intsReader);
+
+        for (int i = 0; i < base; i++) {
+          long value = ints.getInt(i);
+          assertEquals("index " + i, 0, value);
+        }
+
+        int expected = 0;
+        for (int i = base; i < r.numDocs(); i++, expected++) {
+          while (deleted.get(expected)) {
+            expected++;
+          }
+          assertEquals(val + " mod: " + mod + " index: " +  i, expected%mod, ints.getInt(i));
+        }
+      }
+        break;
+      case FLOAT_32:
+      case FLOAT_64: {
+        DocValues floatReader = getDocValues(r, val.name());
+        assertNotNull(floatReader);
+        Source floats = getSource(floatReader);
+        for (int i = 0; i < base; i++) {
+          double value = floats.getFloat(i);
+          assertEquals(val + " failed for doc: " + i + " base: " + base,
+              0.0d, value, 0.0d);
+        }
+        int expected = 0;
+        for (int i = base; i < r.numDocs(); i++, expected++) {
+          while (deleted.get(expected)) {
+            expected++;
+          }
+          assertEquals("index " + i, 2.0 * expected, floats.getFloat(i),
+              0.00001);
+        }
+      }
+        break;
+      default:
+        fail("unexpected value " + val);
+      }
+
+      closeables.add(r);
+      for (Closeable toClose : closeables) {
+        toClose.close();
+      }
+    }
+    w.close();
+    d.close();
+  }
+  
+  public void runTestIndexBytes(IndexWriterConfig cfg, boolean withDeletions)
+      throws CorruptIndexException, LockObtainFailedException, IOException {
+    final Directory d = newDirectory();
+    IndexWriter w = new IndexWriter(d, cfg);
+    final List<Type> byteVariantList = new ArrayList<Type>(BYTES);
+    // run in random order to test if fill works correctly during merges
+    Collections.shuffle(byteVariantList, random);
+    final int numValues = 50 + atLeast(10);
+    for (Type byteIndexValue : byteVariantList) {
+      List<Closeable> closeables = new ArrayList<Closeable>();
+      final int bytesSize = 1 + atLeast(50);
+      FixedBitSet deleted = indexValues(w, numValues, byteIndexValue,
+          byteVariantList, withDeletions, bytesSize);
+      final IndexReader r = IndexReader.open(w, withDeletions);
+      assertEquals(0, r.numDeletedDocs());
+      final int numRemainingValues = numValues - deleted.cardinality();
+      final int base = r.numDocs() - numRemainingValues;
+      DocValues bytesReader = getDocValues(r, byteIndexValue.name());
+      assertNotNull("field " + byteIndexValue.name()
+          + " returned null reader - maybe merged failed", bytesReader);
+      Source bytes = getSource(bytesReader);
+      byte upto = 0;
+
+      // test the filled up slots for correctness
+      for (int i = 0; i < base; i++) {
+
+        BytesRef br = bytes.getBytes(i, new BytesRef());
+        String msg = " field: " + byteIndexValue.name() + " at index: " + i
+            + " base: " + base + " numDocs:" + r.numDocs();
+        switch (byteIndexValue) {
+        case BYTES_VAR_STRAIGHT:
+        case BYTES_FIXED_STRAIGHT:
+        case BYTES_FIXED_DEREF:
+        case BYTES_FIXED_SORTED:
+          // fixed straight returns bytesref with zero bytes all of fixed
+          // length
+          assertNotNull("expected none null - " + msg, br);
+          if (br.length != 0) {
+            assertEquals("expected zero bytes of length " + bytesSize + " - "
+                + msg + br.utf8ToString(), bytesSize, br.length);
+            for (int j = 0; j < br.length; j++) {
+              assertEquals("Byte at index " + j + " doesn't match - " + msg, 0,
+                  br.bytes[br.offset + j]);
+            }
+          }
+          break;
+        default:
+          assertNotNull("expected none null - " + msg, br);
+          assertEquals(byteIndexValue + "", 0, br.length);
+          // make sure we advance at least until base
+        }
+      }
+
+      // test the actual doc values added in this iteration
+      assertEquals(base + numRemainingValues, r.numDocs());
+      int v = 0;
+      for (int i = base; i < r.numDocs(); i++) {
+        String msg = " field: " + byteIndexValue.name() + " at index: " + i
+            + " base: " + base + " numDocs:" + r.numDocs() + " bytesSize: "
+            + bytesSize + " src: " + bytes;
+        while (withDeletions && deleted.get(v++)) {
+          upto += bytesSize;
+        }
+        BytesRef br = bytes.getBytes(i, new BytesRef());
+        assertTrue(msg, br.length > 0);
+        for (int j = 0; j < br.length; j++, upto++) {
+          if (!(br.bytes.length > br.offset + j))
+            br = bytes.getBytes(i, new BytesRef());
+          assertTrue("BytesRef index exceeded [" + msg + "] offset: "
+              + br.offset + " length: " + br.length + " index: "
+              + (br.offset + j), br.bytes.length > br.offset + j);
+          assertEquals("SourceRef Byte at index " + j + " doesn't match - "
+              + msg, upto, br.bytes[br.offset + j]);
+        }
+      }
+
+      // clean up
+      closeables.add(r);
+      for (Closeable toClose : closeables) {
+        toClose.close();
+      }
+    }
+
+    w.close();
+    d.close();
+  }
+
+  private DocValues getDocValues(IndexReader reader, String field) throws IOException {
+    return MultiDocValues.getDocValues(reader, field);
+  }
+
+  private Source getSource(DocValues values) throws IOException {
+    // getSource uses cache internally
+    switch(random.nextInt(5)) {
+    case 3:
+      return values.load();
+    case 2:
+      return values.getDirectSource();
+    case 1:
+      return values.getSource();
+    default:
+      return values.getSource();
+    }
+  }
+
+
+  private static EnumSet<Type> BYTES = EnumSet.of(Type.BYTES_FIXED_DEREF,
+      Type.BYTES_FIXED_STRAIGHT, Type.BYTES_VAR_DEREF,
+      Type.BYTES_VAR_STRAIGHT, Type.BYTES_FIXED_SORTED, Type.BYTES_VAR_SORTED);
+
+  private static EnumSet<Type> NUMERICS = EnumSet.of(Type.VAR_INTS,
+      Type.FIXED_INTS_16, Type.FIXED_INTS_32,
+      Type.FIXED_INTS_64, 
+      Type.FIXED_INTS_8,
+      Type.FLOAT_32,
+      Type.FLOAT_64);
+
+  private FixedBitSet indexValues(IndexWriter w, int numValues, Type value,
+      List<Type> valueVarList, boolean withDeletions, int bytesSize)
+      throws CorruptIndexException, IOException {
+    final boolean isNumeric = NUMERICS.contains(value);
+    FixedBitSet deleted = new FixedBitSet(numValues);
+    Document doc = new Document();
+    DocValuesField valField = new DocValuesField(value.name());
+    doc.add(valField);
+    final BytesRef bytesRef = new BytesRef();
+
+    final String idBase = value.name() + "_";
+    final byte[] b = new byte[bytesSize];
+    if (bytesRef != null) {
+      bytesRef.bytes = b;
+      bytesRef.length = b.length;
+      bytesRef.offset = 0;
+    }
+    byte upto = 0;
+    for (int i = 0; i < numValues; i++) {
+      if (isNumeric) {
+        switch (value) {
+        case VAR_INTS:
+          valField.setInt((long)i);
+          break;
+        case FIXED_INTS_16:
+          valField.setInt((short)i, random.nextInt(10) != 0);
+          break;
+        case FIXED_INTS_32:
+          valField.setInt(i, random.nextInt(10) != 0);
+          break;
+        case FIXED_INTS_64:
+          valField.setInt((long)i, random.nextInt(10) != 0);
+          break;
+        case FIXED_INTS_8:
+          valField.setInt((byte)(0xFF & (i % 128)), random.nextInt(10) != 0);
+          break;
+        case FLOAT_32:
+          valField.setFloat(2.0f * i);
+          break;
+        case FLOAT_64:
+          valField.setFloat(2.0d * i);
+          break;
+       
+        default:
+          fail("unexpected value " + value);
+        }
+      } else {
+        for (int j = 0; j < b.length; j++) {
+          b[j] = upto++;
+        }
+        if (bytesRef != null) {
+          valField.setBytes(bytesRef, value);
+        }
+      }
+      doc.removeFields("id");
+      doc.add(new Field("id", idBase + i, StringField.TYPE_STORED));
+      w.addDocument(doc);
+
+      if (i % 7 == 0) {
+        if (withDeletions && random.nextBoolean()) {
+          Type val = valueVarList.get(random.nextInt(1 + valueVarList
+              .indexOf(value)));
+          final int randInt = val == value ? random.nextInt(1 + i) : random
+              .nextInt(numValues);
+          w.deleteDocuments(new Term("id", val.name() + "_" + randInt));
+          if (val == value) {
+            deleted.set(randInt);
+          }
+        }
+        if (random.nextInt(10) == 0) {
+          w.commit();
+        }
+      }
+    }
+    w.commit();
+
+    // TODO test multi seg with deletions
+    if (withDeletions || random.nextBoolean()) {
+      w.forceMerge(1, true);
+    }
+    return deleted;
+  }
+
+  public void testMultiValuedDocValuesField() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random, d);
+    Document doc = new Document();
+    DocValuesField f = new DocValuesField("field");
+    f.setInt(17);
+    // Index doc values are single-valued so we should not
+    // be able to add same field more than once:
+    doc.add(f);
+    doc.add(f);
+    try {
+      w.addDocument(doc);
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    doc = new Document();
+    doc.add(f);
+    w.addDocument(doc);
+    w.forceMerge(1);
+    IndexReader r = w.getReader();
+    w.close();
+    assertEquals(17, r.getSequentialSubReaders()[0].docValues("field").load().getInt(0));
+    r.close();
+    d.close();
+  }
+
+  public void testDifferentTypedDocValuesField() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random, d);
+    Document doc = new Document();
+    DocValuesField f = new DocValuesField("field");
+    f.setInt(17);
+    // Index doc values are single-valued so we should not
+    // be able to add same field more than once:
+    doc.add(f);
+    DocValuesField f2 = new DocValuesField("field");
+    f2.setFloat(22.0);
+    doc.add(f2);
+    try {
+      w.addDocument(doc);
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    doc = new Document();
+    doc.add(f);
+    w.addDocument(doc);
+    w.forceMerge(1);
+    IndexReader r = w.getReader();
+    w.close();
+    assertEquals(17, r.getSequentialSubReaders()[0].docValues("field").load().getInt(0));
+    r.close();
+    d.close();
+  }
+}
diff --git a/lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java b/lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java
index 57d98a2..7e45618 100644
--- a/lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java
+++ b/lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java
@@ -25,9 +25,9 @@ import java.util.Set;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.IndexReader.FieldOption;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
diff --git a/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java b/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java
index 726729e..ff05b53 100644
--- a/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java
+++ b/lucene/src/test/org/apache/lucene/index/TestFieldInfos.java
@@ -19,11 +19,11 @@ package org.apache.lucene.index;
 
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.codecs.FieldInfosWriter;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.FieldInfosReader;
-import org.apache.lucene.index.codecs.FieldInfosWriter;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexOutput;
diff --git a/lucene/src/test/org/apache/lucene/index/TestFlex.java b/lucene/src/test/org/apache/lucene/index/TestFlex.java
index 484547c..727d3f6 100644
--- a/lucene/src/test/org/apache/lucene/index/TestFlex.java
+++ b/lucene/src/test/org/apache/lucene/index/TestFlex.java
@@ -19,8 +19,8 @@ package org.apache.lucene.index;
 
 import org.apache.lucene.store.*;
 import org.apache.lucene.analysis.*;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.document.*;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.util.*;
 
 public class TestFlex extends LuceneTestCase {
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java b/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
index 511b54c..74b0963 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
@@ -21,12 +21,12 @@ import java.io.*;
 import java.util.*;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.FieldInfosReader;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.FieldInfosReader;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.store.CompoundFileDirectory;
 import org.apache.lucene.store.Directory;
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexReader.java b/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
index 7d43d7f..64c5f60 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexReader.java
@@ -29,6 +29,7 @@ import java.util.Random;
 import java.util.Set;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.document.BinaryField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -37,7 +38,6 @@ import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexReader.FieldOption;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.store.Directory;
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
index 2ed2aa4..bc6054c 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -35,6 +35,8 @@ import java.util.Set;
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.simpletext.SimpleTextCodec;
 import org.apache.lucene.document.BinaryField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
@@ -42,8 +44,6 @@ import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.simpletext.SimpleTextCodec;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.IndexSearcher;
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexWriterConfig.java b/lucene/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
index d04dc96..d968b28 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
@@ -24,9 +24,9 @@ import java.util.HashSet;
 import java.util.Set;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.DocumentsWriterPerThread.IndexingChain;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.similarities.DefaultSimilarityProvider;
 import org.apache.lucene.util.InfoStream;
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java b/lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java
index aeb22d4..f49ae3c 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexWriterReader.java
@@ -24,11 +24,11 @@ import java.util.Random;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
diff --git a/lucene/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java b/lucene/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
index 81c9cb4..9a52a25 100644
--- a/lucene/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
+++ b/lucene/src/test/org/apache/lucene/index/TestMultiLevelSkipList.java
@@ -23,9 +23,9 @@ import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.analysis.*;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IOContext;
 import org.apache.lucene.store.IndexInput;
diff --git a/lucene/src/test/org/apache/lucene/index/TestRollingUpdates.java b/lucene/src/test/org/apache/lucene/index/TestRollingUpdates.java
index 97e822a..8fbfa32 100644
--- a/lucene/src/test/org/apache/lucene/index/TestRollingUpdates.java
+++ b/lucene/src/test/org/apache/lucene/index/TestRollingUpdates.java
@@ -18,9 +18,9 @@ package org.apache.lucene.index;
  */
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.memory.MemoryPostingsFormat;
 import org.apache.lucene.document.*;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.memory.MemoryPostingsFormat;
 import org.apache.lucene.store.*;
 import org.apache.lucene.util.*;
 import org.junit.Test;
diff --git a/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java b/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
index 69be684..080a0e1 100644
--- a/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
+++ b/lucene/src/test/org/apache/lucene/index/TestSegmentMerger.java
@@ -21,10 +21,10 @@ import java.io.IOException;
 import java.util.Collection;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.InfoStream;
diff --git a/lucene/src/test/org/apache/lucene/index/TestSegmentTermEnum.java b/lucene/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
index 189dbb2..259e5d6 100644
--- a/lucene/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
+++ b/lucene/src/test/org/apache/lucene/index/TestSegmentTermEnum.java
@@ -23,10 +23,10 @@ import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util._TestUtil;
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.store.Directory;
 
 
diff --git a/lucene/src/test/org/apache/lucene/index/TestTermVectorsReader.java b/lucene/src/test/org/apache/lucene/index/TestTermVectorsReader.java
index 7edda9c..dd4dfd1 100644
--- a/lucene/src/test/org/apache/lucene/index/TestTermVectorsReader.java
+++ b/lucene/src/test/org/apache/lucene/index/TestTermVectorsReader.java
@@ -27,12 +27,12 @@ import org.apache.lucene.analysis.*;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
 import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.TermVectorsReader;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.TermVectorsReader;
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
diff --git a/lucene/src/test/org/apache/lucene/index/TestTermsEnum2.java b/lucene/src/test/org/apache/lucene/index/TestTermsEnum2.java
index 70c12f7..bda74bf 100644
--- a/lucene/src/test/org/apache/lucene/index/TestTermsEnum2.java
+++ b/lucene/src/test/org/apache/lucene/index/TestTermsEnum2.java
@@ -25,11 +25,11 @@ import java.util.TreeSet;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.AutomatonQuery;
 import org.apache.lucene.search.CheckHits;
 import org.apache.lucene.search.IndexSearcher;
diff --git a/lucene/src/test/org/apache/lucene/index/TestTypePromotion.java b/lucene/src/test/org/apache/lucene/index/TestTypePromotion.java
new file mode 100644
index 0000000..8b71be3
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/index/TestTypePromotion.java
@@ -0,0 +1,320 @@
+package org.apache.lucene.index;
+
+import java.io.IOException;
+import java.util.EnumSet;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.lucene40.values.BytesRefUtils;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.DocValuesField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexReader.ReaderContext;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.SlowMultiReaderWrapper;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Before;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+public class TestTypePromotion extends LuceneTestCase {
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+    assumeFalse("cannot work with preflex codec", Codec.getDefault().getName().equals("Lucene3x"));
+  }
+
+  private static EnumSet<Type> INTEGERS = EnumSet.of(Type.VAR_INTS,
+      Type.FIXED_INTS_16, Type.FIXED_INTS_32,
+      Type.FIXED_INTS_64, Type.FIXED_INTS_8);
+
+  private static EnumSet<Type> FLOATS = EnumSet.of(Type.FLOAT_32,
+      Type.FLOAT_64);
+
+  private static EnumSet<Type> UNSORTED_BYTES = EnumSet.of(
+      Type.BYTES_FIXED_DEREF, Type.BYTES_FIXED_STRAIGHT,
+      Type.BYTES_VAR_STRAIGHT, Type.BYTES_VAR_DEREF);
+
+  private static EnumSet<Type> SORTED_BYTES = EnumSet.of(
+      Type.BYTES_FIXED_SORTED, Type.BYTES_VAR_SORTED);
+  
+  public Type randomValueType(EnumSet<Type> typeEnum, Random random) {
+    Type[] array = typeEnum.toArray(new Type[0]);
+    return array[random.nextInt(array.length)];
+  }
+  
+  private static enum TestType {
+    Int, Float, Byte
+  }
+
+  private void runTest(EnumSet<Type> types, TestType type)
+      throws CorruptIndexException, IOException {
+    Directory dir = newDirectory();
+    IndexWriter writer = new IndexWriter(dir,
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
+    int num_1 = atLeast(200);
+    int num_2 = atLeast(200);
+    int num_3 = atLeast(200);
+    long[] values = new long[num_1 + num_2 + num_3];
+    index(writer, new DocValuesField("promote"),
+        randomValueType(types, random), values, 0, num_1);
+    writer.commit();
+    
+    index(writer, new DocValuesField("promote"),
+        randomValueType(types, random), values, num_1, num_2);
+    writer.commit();
+    
+    if (random.nextInt(4) == 0) {
+      // once in a while use addIndexes
+      writer.forceMerge(1);
+      
+      Directory dir_2 = newDirectory() ;
+      IndexWriter writer_2 = new IndexWriter(dir_2,
+          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
+      index(writer_2, new DocValuesField("promote"),
+          randomValueType(types, random), values, num_1 + num_2, num_3);
+      writer_2.commit();
+      writer_2.close();
+      if (random.nextBoolean()) {
+        writer.addIndexes(dir_2);
+      } else {
+        // do a real merge here
+        IndexReader open = IndexReader.open(dir_2);
+        // we cannot use SlowMR for sorted bytes, because it returns a null sortedsource
+        boolean useSlowMRWrapper = types != SORTED_BYTES && random.nextBoolean();
+        writer.addIndexes(useSlowMRWrapper ? new SlowMultiReaderWrapper(open) : open);
+        open.close();
+      }
+      dir_2.close();
+    } else {
+      index(writer, new DocValuesField("promote"),
+          randomValueType(types, random), values, num_1 + num_2, num_3);
+    }
+
+    writer.forceMerge(1);
+    writer.close();
+    assertValues(type, dir, values);
+    dir.close();
+  }
+
+  
+  private void assertValues(TestType type, Directory dir, long[] values)
+      throws CorruptIndexException, IOException {
+    IndexReader reader = IndexReader.open(dir);
+    assertEquals(1, reader.getSequentialSubReaders().length);
+    ReaderContext topReaderContext = reader.getTopReaderContext();
+    ReaderContext[] children = topReaderContext.children();
+    DocValues docValues = children[0].reader.docValues("promote");
+    assertEquals(1, children.length);
+    Source directSource = docValues.getDirectSource();
+    for (int i = 0; i < values.length; i++) {
+      int id = Integer.parseInt(reader.document(i).get("id"));
+      String msg = "id: " + id + " doc: " + i;
+      switch (type) {
+      case Byte:
+        BytesRef bytes = directSource.getBytes(i, new BytesRef());
+        long value = 0;
+        switch(bytes.length) {
+        case 1:
+          value = bytes.bytes[bytes.offset];
+          break;
+        case 2:
+          value = BytesRefUtils.asShort(bytes);
+          break;
+        case 4:
+          value = BytesRefUtils.asInt(bytes);
+          break;
+        case 8:
+          value = BytesRefUtils.asLong(bytes);
+          break;
+          
+        default:
+          fail(msg + " bytessize: " + bytes.length);
+        }
+        
+        assertEquals(msg  + " byteSize: " + bytes.length, values[id], value);
+        break;
+      case Float:
+          assertEquals(msg, values[id], Double.doubleToRawLongBits(directSource.getFloat(i)));
+        break;
+      case Int:
+        assertEquals(msg, values[id], directSource.getInt(i));
+        break;
+      default:
+        break;
+      }
+
+    }
+    docValues.close();
+    reader.close();
+  }
+
+  public void index(IndexWriter writer, DocValuesField valField,
+      Type valueType, long[] values, int offset, int num)
+      throws CorruptIndexException, IOException {
+    BytesRef ref = new BytesRef(new byte[] { 1, 2, 3, 4 });
+    for (int i = offset; i < offset + num; i++) {
+      Document doc = new Document();
+      doc.add(new Field("id", i + "", TextField.TYPE_STORED));
+      switch (valueType) {
+      case VAR_INTS:
+        values[i] = random.nextInt();
+        valField.setInt(values[i]);
+        break;
+      case FIXED_INTS_16:
+        values[i] = random.nextInt(Short.MAX_VALUE);
+        valField.setInt((short) values[i], true);
+        break;
+      case FIXED_INTS_32:
+        values[i] = random.nextInt();
+        valField.setInt((int) values[i], true);
+        break;
+      case FIXED_INTS_64:
+        values[i] = random.nextLong();
+        valField.setInt(values[i], true);
+        break;
+      case FLOAT_64:
+        double nextDouble = random.nextDouble();
+        values[i] = Double.doubleToRawLongBits(nextDouble);
+        valField.setFloat(nextDouble);
+        break;
+      case FLOAT_32:
+        final float nextFloat = random.nextFloat();
+        values[i] = Double.doubleToRawLongBits(nextFloat);
+        valField.setFloat(nextFloat);
+        break;
+      case FIXED_INTS_8:
+         values[i] = (byte) i;
+        valField.setInt((byte)values[i], true);
+        break;
+      case BYTES_FIXED_DEREF:
+      case BYTES_FIXED_SORTED:
+      case BYTES_FIXED_STRAIGHT:
+        values[i] = random.nextLong();
+        BytesRefUtils.copyLong(ref, values[i]);
+        valField.setBytes(ref, valueType);
+        break;
+      case BYTES_VAR_DEREF:
+      case BYTES_VAR_SORTED:
+      case BYTES_VAR_STRAIGHT:
+        if (random.nextBoolean()) {
+          BytesRefUtils.copyInt(ref, random.nextInt());
+          values[i] = BytesRefUtils.asInt(ref);
+        } else {
+          BytesRefUtils.copyLong(ref, random.nextLong());
+          values[i] = BytesRefUtils.asLong(ref);
+        }
+        valField.setBytes(ref, valueType);
+        break;
+
+      default:
+        fail("unexpected value " + valueType);
+
+      }
+      doc.add(valField);
+      writer.addDocument(doc);
+      if (random.nextInt(10) == 0) {
+        writer.commit();
+      }
+    }
+  }
+
+  public void testPromoteBytes() throws IOException {
+    runTest(UNSORTED_BYTES, TestType.Byte);
+  }
+  
+  public void testSortedPromoteBytes() throws IOException {
+    runTest(SORTED_BYTES, TestType.Byte);
+  }
+
+  public void testPromotInteger() throws IOException {
+    runTest(INTEGERS, TestType.Int);
+  }
+
+  public void testPromotFloatingPoint() throws CorruptIndexException,
+      IOException {
+    runTest(FLOATS, TestType.Float);
+  }
+  
+  public void testMergeIncompatibleTypes() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));
+    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values
+    IndexWriter writer = new IndexWriter(dir, writerConfig);
+    int num_1 = atLeast(200);
+    int num_2 = atLeast(200);
+    long[] values = new long[num_1 + num_2];
+    index(writer, new DocValuesField("promote"),
+        randomValueType(INTEGERS, random), values, 0, num_1);
+    writer.commit();
+    
+    if (random.nextInt(4) == 0) {
+      // once in a while use addIndexes
+      Directory dir_2 = newDirectory() ;
+      IndexWriter writer_2 = new IndexWriter(dir_2,
+          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
+      index(writer_2, new DocValuesField("promote"),
+          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);
+      writer_2.commit();
+      writer_2.close();
+      if (random.nextBoolean()) {
+        writer.addIndexes(dir_2);
+      } else {
+        // do a real merge here
+        IndexReader open = IndexReader.open(dir_2);
+        writer.addIndexes(open);
+        open.close();
+      }
+      dir_2.close();
+    } else {
+      index(writer, new DocValuesField("promote"),
+          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);
+      writer.commit();
+    }
+    writer.close();
+    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));
+    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {
+      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)
+    }
+    writer = new IndexWriter(dir, writerConfig);
+    // now merge
+    writer.forceMerge(1);
+    writer.close();
+    IndexReader reader = IndexReader.open(dir);
+    assertEquals(1, reader.getSequentialSubReaders().length);
+    ReaderContext topReaderContext = reader.getTopReaderContext();
+    ReaderContext[] children = topReaderContext.children();
+    DocValues docValues = children[0].reader.docValues("promote");
+    assertNotNull(docValues);
+    assertValues(TestType.Byte, dir, values);
+    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());
+    reader.close();
+    dir.close();
+  }
+
+}
\ No newline at end of file
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java b/lucene/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
deleted file mode 100644
index 866fc76..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/appending/TestAppendingCodec.java
+++ /dev/null
@@ -1,165 +0,0 @@
-package org.apache.lucene.index.codecs.appending;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.TieredMergePolicy;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
-
-public class TestAppendingCodec extends LuceneTestCase {
-  
-    private static class AppendingIndexOutputWrapper extends IndexOutput {
-    IndexOutput wrapped;
-    
-    public AppendingIndexOutputWrapper(IndexOutput wrapped) {
-      this.wrapped = wrapped;
-    }
-
-    @Override
-    public void close() throws IOException {
-      wrapped.close();
-    }
-
-    @Override
-    public void flush() throws IOException {
-      wrapped.flush();
-    }
-
-    @Override
-    public long getFilePointer() {
-      return wrapped.getFilePointer();
-    }
-
-    @Override
-    public long length() throws IOException {
-      return wrapped.length();
-    }
-
-    @Override
-    public void seek(long pos) throws IOException {
-      throw new UnsupportedOperationException("seek() is unsupported");
-    }
-
-    @Override
-    public void writeByte(byte b) throws IOException {
-      wrapped.writeByte(b);
-    }
-
-    @Override
-    public void writeBytes(byte[] b, int offset, int length) throws IOException {
-      wrapped.writeBytes(b, offset, length);
-    }
-    
-  }
-  
-  @SuppressWarnings("serial")
-  private static class AppendingRAMDirectory extends MockDirectoryWrapper {
-
-    public AppendingRAMDirectory(Random random, Directory delegate) {
-      super(random, delegate);
-    }
-
-    @Override
-    public IndexOutput createOutput(String name, IOContext context) throws IOException {
-      return new AppendingIndexOutputWrapper(super.createOutput(name, context));
-    }
-    
-  }
-  
-  private static final String text = "the quick brown fox jumped over the lazy dog";
-
-  public void testCodec() throws Exception {
-    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());
-    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));
-    
-    cfg.setCodec(new AppendingCodec());
-    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);
-    IndexWriter writer = new IndexWriter(dir, cfg);
-    Document doc = new Document();
-    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);
-    storedTextType.setStoreTermVectors(true);
-    storedTextType.setStoreTermVectorPositions(true);
-    storedTextType.setStoreTermVectorOffsets(true);
-    doc.add(newField("f", text, storedTextType));
-    writer.addDocument(doc);
-    writer.commit();
-    writer.addDocument(doc);
-    writer.forceMerge(1);
-    writer.close();
-    IndexReader reader = IndexReader.open(dir, 1);
-    assertEquals(2, reader.numDocs());
-    Document doc2 = reader.document(0);
-    assertEquals(text, doc2.get("f"));
-    Fields fields = MultiFields.getFields(reader);
-    Terms terms = fields.terms("f");
-    assertNotNull(terms);
-    TermsEnum te = terms.iterator(null);
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("quick")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("brown")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("fox")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("jumped")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("over")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("lazy")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("dog")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("the")));
-    DocsEnum de = te.docs(null, null, true);
-    assertTrue(de.advance(0) != DocsEnum.NO_MORE_DOCS);
-    assertEquals(2, de.freq());
-    assertTrue(de.advance(1) != DocsEnum.NO_MORE_DOCS);
-    assertTrue(de.advance(2) == DocsEnum.NO_MORE_DOCS);
-    reader.close();
-  }
-  
-  public void testCompoundFile() throws Exception {
-    Directory dir = new AppendingRAMDirectory(random, new RAMDirectory());
-    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random));
-    TieredMergePolicy mp = new TieredMergePolicy();
-    mp.setUseCompoundFile(true);
-    mp.setNoCFSRatio(1.0);
-    cfg.setMergePolicy(mp);
-    cfg.setCodec(new AppendingCodec());
-    IndexWriter writer = new IndexWriter(dir, cfg);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    writer.close();
-    assertTrue(dir.fileExists("_0.cfs"));
-    dir.close();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/intblock/TestIntBlockCodec.java b/lucene/src/test/org/apache/lucene/index/codecs/intblock/TestIntBlockCodec.java
deleted file mode 100644
index 2762aae..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/intblock/TestIntBlockCodec.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.apache.lucene.index.codecs.intblock;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.store.*;
-import org.apache.lucene.index.codecs.sep.*;
-import org.apache.lucene.index.codecs.mockintblock.*;
-
-public class TestIntBlockCodec extends LuceneTestCase {
-
-  public void testSimpleIntBlocks() throws Exception {
-    Directory dir = newDirectory();
-
-    IntStreamFactory f = new MockFixedIntBlockPostingsFormat(128).getIntFactory();
-
-    IntIndexOutput out = f.createOutput(dir, "test", newIOContext(random));
-    for(int i=0;i<11777;i++) {
-      out.write(i);
-    }
-    out.close();
-
-    IntIndexInput in = f.openInput(dir, "test", newIOContext(random));
-    IntIndexInput.Reader r = in.reader();
-
-    for(int i=0;i<11777;i++) {
-      assertEquals(i, r.next());
-    }
-    in.close();
-    
-    dir.close();
-  }
-
-  public void testEmptySimpleIntBlocks() throws Exception {
-    Directory dir = newDirectory();
-
-    IntStreamFactory f = new MockFixedIntBlockPostingsFormat(128).getIntFactory();
-    IntIndexOutput out = f.createOutput(dir, "test", newIOContext(random));
-
-    // write no ints
-    out.close();
-
-    IntIndexInput in = f.openInput(dir, "test", newIOContext(random));
-    in.reader();
-    // read no ints
-    in.close();
-    dir.close();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestImpersonation.java b/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestImpersonation.java
deleted file mode 100644
index 25d4331..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestImpersonation.java
+++ /dev/null
@@ -1,34 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.preflexrw.PreFlexRWCodec;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Test that the SPI magic is returning "PreFlexRWCodec" for Lucene3x
- * 
- * @lucene.experimental
- */
-public class TestImpersonation extends LuceneTestCase {
-  public void test() throws Exception {
-    Codec codec = Codec.forName("Lucene3x");
-    assertTrue(codec instanceof PreFlexRWCodec);
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestSurrogates.java b/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestSurrogates.java
deleted file mode 100644
index 2fe1cf5..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestSurrogates.java
+++ /dev/null
@@ -1,356 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.*;
-import org.apache.lucene.document.*;
-import org.apache.lucene.analysis.*;
-import org.apache.lucene.index.*;
-import org.apache.lucene.index.codecs.preflexrw.PreFlexRWCodec;
-import org.apache.lucene.util.*;
-
-import java.util.*;
-import java.io.IOException;
-
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestSurrogates extends LuceneTestCase {
-  /** we will manually instantiate preflex-rw here */
-  @BeforeClass
-  public static void beforeClass() {
-    LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE = true;
-  }
-
-  private static String makeDifficultRandomUnicodeString(Random r) {
-    final int end = r.nextInt(20);
-    if (end == 0) {
-      // allow 0 length
-      return "";
-    }
-    final char[] buffer = new char[end];
-    for (int i = 0; i < end; i++) {
-      int t = r.nextInt(5);
-
-      if (0 == t && i < end - 1) {
-        // hi
-        buffer[i++] = (char) (0xd800 + r.nextInt(2));
-        // lo
-        buffer[i] = (char) (0xdc00 + r.nextInt(2));
-      } else if (t <= 3) {
-        buffer[i] = (char) ('a' + r.nextInt(2));
-      }  else if (4 == t) {
-        buffer[i] = (char) (0xe000 + r.nextInt(2));
-      }
-    }
-
-    return new String(buffer, 0, end);
-  }
-
-  private String toHexString(Term t) {
-    return t.field() + ":" + UnicodeUtil.toHexString(t.text());
-  }
-
-  private String getRandomString(Random r) {
-    String s;
-    if (r.nextInt(5) == 1) {
-      if (r.nextInt(3) == 1) {
-        s = makeDifficultRandomUnicodeString(r);
-      } else {
-        s = _TestUtil.randomUnicodeString(r);
-      }
-    } else {
-      s = _TestUtil.randomRealisticUnicodeString(r);
-    }
-    return s;
-  }
-
-  private static class SortTermAsUTF16Comparator implements Comparator<Term> {
-    private static final Comparator<BytesRef> legacyComparator = 
-      BytesRef.getUTF8SortedAsUTF16Comparator();
-
-    public int compare(Term term1, Term term2) {
-      if (term1.field().equals(term2.field())) {
-        return legacyComparator.compare(term1.bytes(), term2.bytes());
-      } else {
-        return term1.field().compareTo(term2.field());
-      }
-    }
-  }
-
-  private static final SortTermAsUTF16Comparator termAsUTF16Comparator = new SortTermAsUTF16Comparator();
-
-  // single straight enum
-  private void doTestStraightEnum(List<Term> fieldTerms, IndexReader reader, int uniqueTermCount) throws IOException {
-
-    if (VERBOSE) {
-      System.out.println("\nTEST: top now enum reader=" + reader);
-    }
-    FieldsEnum fieldsEnum = MultiFields.getFields(reader).iterator();
-
-    {
-      // Test straight enum:
-      String field;
-      int termCount = 0;
-      while((field = fieldsEnum.next()) != null) {
-        Terms terms = fieldsEnum.terms();
-        assertNotNull(terms);
-        TermsEnum termsEnum = terms.iterator(null);
-        BytesRef text;
-        BytesRef lastText = null;
-        while((text = termsEnum.next()) != null) {
-          Term exp = fieldTerms.get(termCount);
-          if (VERBOSE) {
-            System.out.println("  got term=" + field + ":" + UnicodeUtil.toHexString(text.utf8ToString()));
-            System.out.println("       exp=" + exp.field() + ":" + UnicodeUtil.toHexString(exp.text().toString()));
-            System.out.println();
-          }
-          if (lastText == null) {
-            lastText = BytesRef.deepCopyOf(text);
-          } else {
-            assertTrue(lastText.compareTo(text) < 0);
-            lastText.copyBytes(text);
-          }
-          assertEquals(exp.field(), field);
-          assertEquals(exp.bytes(), text);
-          termCount++;
-        }
-        if (VERBOSE) {
-          System.out.println("  no more terms for field=" + field);
-        }
-      }
-      assertEquals(uniqueTermCount, termCount);
-    }
-  }
-
-  // randomly seeks to term that we know exists, then next's
-  // from there
-  private void doTestSeekExists(Random r, List<Term> fieldTerms, IndexReader reader) throws IOException {
-
-    final Map<String,TermsEnum> tes = new HashMap<String,TermsEnum>();
-
-    // Test random seek to existing term, then enum:
-    if (VERBOSE) {
-      System.out.println("\nTEST: top now seek");
-    }
-
-    int num = atLeast(100);
-    for (int iter = 0; iter < num; iter++) {
-
-      // pick random field+term
-      int spot = r.nextInt(fieldTerms.size());
-      Term term = fieldTerms.get(spot);
-      String field = term.field();
-
-      if (VERBOSE) {
-        System.out.println("TEST: exist seek field=" + field + " term=" + UnicodeUtil.toHexString(term.text()));
-      }
-
-      // seek to it
-      TermsEnum te = tes.get(field);
-      if (te == null) {
-        te = MultiFields.getTerms(reader, field).iterator(null);
-        tes.put(field, te);
-      }
-
-      if (VERBOSE) {
-        System.out.println("  done get enum");
-      }
-
-      // seek should find the term
-      assertEquals(TermsEnum.SeekStatus.FOUND,
-                   te.seekCeil(term.bytes()));
-      
-      // now .next() this many times:
-      int ct = _TestUtil.nextInt(r, 5, 100);
-      for(int i=0;i<ct;i++) {
-        if (VERBOSE) {
-          System.out.println("TEST: now next()");
-        }
-        if (1+spot+i >= fieldTerms.size()) {
-          break;
-        }
-        term = fieldTerms.get(1+spot+i);
-        if (!term.field().equals(field)) {
-          assertNull(te.next());
-          break;
-        } else {
-          BytesRef t = te.next();
-
-          if (VERBOSE) {
-            System.out.println("  got term=" + (t == null ? null : UnicodeUtil.toHexString(t.utf8ToString())));
-            System.out.println("       exp=" + UnicodeUtil.toHexString(term.text().toString()));
-          }
-
-          assertEquals(term.bytes(), t);
-        }
-      }
-    }
-  }
-
-  private void doTestSeekDoesNotExist(Random r, int numField, List<Term> fieldTerms, Term[] fieldTermsArray, IndexReader reader) throws IOException {
-
-    final Map<String,TermsEnum> tes = new HashMap<String,TermsEnum>();
-
-    if (VERBOSE) {
-      System.out.println("TEST: top random seeks");
-    }
-
-    {
-      int num = atLeast(100);
-      for (int iter = 0; iter < num; iter++) {
-      
-        // seek to random spot
-        String field = ("f" + r.nextInt(numField)).intern();
-        Term tx = new Term(field, getRandomString(r));
-
-        int spot = Arrays.binarySearch(fieldTermsArray, tx);
-
-        if (spot < 0) {
-          if (VERBOSE) {
-            System.out.println("TEST: non-exist seek to " + field + ":" + UnicodeUtil.toHexString(tx.text()));
-          }
-
-          // term does not exist:
-          TermsEnum te = tes.get(field);
-          if (te == null) {
-            te = MultiFields.getTerms(reader, field).iterator(null);
-            tes.put(field, te);
-          }
-
-          if (VERBOSE) {
-            System.out.println("  got enum");
-          }
-
-          spot = -spot - 1;
-
-          if (spot == fieldTerms.size() || !fieldTerms.get(spot).field().equals(field)) {
-            assertEquals(TermsEnum.SeekStatus.END, te.seekCeil(tx.bytes()));
-          } else {
-            assertEquals(TermsEnum.SeekStatus.NOT_FOUND, te.seekCeil(tx.bytes()));
-
-            if (VERBOSE) {
-              System.out.println("  got term=" + UnicodeUtil.toHexString(te.term().utf8ToString()));
-              System.out.println("  exp term=" + UnicodeUtil.toHexString(fieldTerms.get(spot).text()));
-            }
-
-            assertEquals(fieldTerms.get(spot).bytes(),
-                         te.term());
-
-            // now .next() this many times:
-            int ct = _TestUtil.nextInt(r, 5, 100);
-            for(int i=0;i<ct;i++) {
-              if (VERBOSE) {
-                System.out.println("TEST: now next()");
-              }
-              if (1+spot+i >= fieldTerms.size()) {
-                break;
-              }
-              Term term = fieldTerms.get(1+spot+i);
-              if (!term.field().equals(field)) {
-                assertNull(te.next());
-                break;
-              } else {
-                BytesRef t = te.next();
-
-                if (VERBOSE) {
-                  System.out.println("  got term=" + (t == null ? null : UnicodeUtil.toHexString(t.utf8ToString())));
-                  System.out.println("       exp=" + UnicodeUtil.toHexString(term.text().toString()));
-                }
-
-                assertEquals(term.bytes(), t);
-              }
-            }
-
-          }
-        }
-      }
-    }
-  }
-
-
-  @Test
-  public void testSurrogatesOrder() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random,
-                                                dir,
-                                                newIndexWriterConfig( TEST_VERSION_CURRENT,
-                                                                      new MockAnalyzer(random)).setCodec(new PreFlexRWCodec()));
-
-    final int numField = _TestUtil.nextInt(random, 2, 5);
-
-    int uniqueTermCount = 0;
-
-    int tc = 0;
-
-    List<Term> fieldTerms = new ArrayList<Term>();
-
-    for(int f=0;f<numField;f++) {
-      String field = "f" + f;
-      final int numTerms = atLeast(1000);
-
-      final Set<String> uniqueTerms = new HashSet<String>();
-
-      for(int i=0;i<numTerms;i++) {
-        String term = getRandomString(random) + "_ " + (tc++);
-        uniqueTerms.add(term);
-        fieldTerms.add(new Term(field, term));
-        Document doc = new Document();
-        doc.add(newField(field, term, StringField.TYPE_UNSTORED));
-        w.addDocument(doc);
-      }
-      uniqueTermCount += uniqueTerms.size();
-    }
-
-    IndexReader reader = w.getReader();
-
-    if (VERBOSE) {
-      Collections.sort(fieldTerms, termAsUTF16Comparator);
-
-      System.out.println("\nTEST: UTF16 order");
-      for(Term t: fieldTerms) {
-        System.out.println("  " + toHexString(t));
-      }
-    }
-
-    // sorts in code point order:
-    Collections.sort(fieldTerms);
-
-    if (VERBOSE) {
-      System.out.println("\nTEST: codepoint order");
-      for(Term t: fieldTerms) {
-        System.out.println("  " + toHexString(t));
-      }
-    }
-
-    Term[] fieldTermsArray = fieldTerms.toArray(new Term[fieldTerms.size()]);
-
-    //SegmentInfo si = makePreFlexSegment(r, "_0", dir, fieldInfos, codec, fieldTerms);
-
-    //FieldsProducer fields = codec.fieldsProducer(new SegmentReadState(dir, si, fieldInfos, 1024, 1));
-    //assertNotNull(fields);
-
-    doTestStraightEnum(fieldTerms, reader, uniqueTermCount);
-    doTestSeekExists(random, fieldTerms, reader);
-    doTestSeekDoesNotExist(random, numField, fieldTerms, fieldTermsArray, reader);
-
-    reader.close();
-    w.close();
-    dir.close();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestTermInfosReaderIndex.java b/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestTermInfosReaderIndex.java
deleted file mode 100644
index ce2af1e..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/lucene3x/TestTermInfosReaderIndex.java
+++ /dev/null
@@ -1,198 +0,0 @@
-package org.apache.lucene.index.codecs.lucene3x;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldsEnum;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LogMergePolicy;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SegmentReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.FieldInfosReader;
-import org.apache.lucene.index.codecs.preflexrw.PreFlexRWCodec;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.LockObtainFailedException;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-import org.junit.BeforeClass;
-
-public class TestTermInfosReaderIndex extends LuceneTestCase {
-  
-  private static final int NUMBER_OF_DOCUMENTS = 1000;
-  private static final int NUMBER_OF_FIELDS = 100;
-  private TermInfosReaderIndex index;
-  private Directory directory;
-  private SegmentTermEnum termEnum;
-  private int indexDivisor;
-  private int termIndexInterval;
-  private IndexReader reader;
-  private List<Term> sampleTerms;
-  
-  /** we will manually instantiate preflex-rw here */
-  @BeforeClass
-  public static void beforeClass() {
-    LuceneTestCase.PREFLEX_IMPERSONATION_IS_ACTIVE = true;
-  }
-
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    indexDivisor = _TestUtil.nextInt(random, 1, 10);
-    directory = newDirectory();
-    termIndexInterval = populate(directory);
-
-    IndexReader r0 = IndexReader.open(directory);
-    SegmentReader r = (SegmentReader) r0.getSequentialSubReaders()[0];
-    String segment = r.getSegmentName();
-    r.close();
-
-    FieldInfosReader infosReader = new PreFlexRWCodec().fieldInfosFormat().getFieldInfosReader();
-    FieldInfos fieldInfos = infosReader.read(directory, segment, IOContext.READONCE);
-    String segmentFileName = IndexFileNames.segmentFileName(segment, "", Lucene3xPostingsFormat.TERMS_INDEX_EXTENSION);
-    long tiiFileLength = directory.fileLength(segmentFileName);
-    IndexInput input = directory.openInput(segmentFileName, newIOContext(random));
-    termEnum = new SegmentTermEnum(directory.openInput(IndexFileNames.segmentFileName(segment, "", Lucene3xPostingsFormat.TERMS_EXTENSION), newIOContext(random)), fieldInfos, false);
-    int totalIndexInterval = termEnum.indexInterval * indexDivisor;
-    
-    SegmentTermEnum indexEnum = new SegmentTermEnum(input, fieldInfos, true);
-    index = new TermInfosReaderIndex(indexEnum, indexDivisor, tiiFileLength, totalIndexInterval);
-    indexEnum.close();
-    input.close();
-    
-    reader = IndexReader.open(directory);
-    sampleTerms = sample(reader,1000);
-    
-  }
-  
-  @Override
-  public void tearDown() throws Exception {
-    termEnum.close();
-    reader.close();
-    directory.close();
-    super.tearDown();
-  }
-  
-  public void testSeekEnum() throws CorruptIndexException, IOException {
-    int indexPosition = 3;
-    SegmentTermEnum clone = (SegmentTermEnum) termEnum.clone();
-    Term term = findTermThatWouldBeAtIndex(clone, indexPosition);
-    SegmentTermEnum enumerator = clone;
-    index.seekEnum(enumerator, indexPosition);
-    assertEquals(term, enumerator.term());
-    clone.close();
-  }
-  
-  public void testCompareTo() throws IOException {
-    Term term = new Term("field" + random.nextInt(NUMBER_OF_FIELDS) ,getText());
-    for (int i = 0; i < index.length(); i++) {
-      Term t = index.getTerm(i);
-      int compareTo = term.compareTo(t);
-      assertEquals(compareTo, index.compareTo(term, i));
-    }
-  }
-  
-  public void testRandomSearchPerformance() throws CorruptIndexException, IOException {
-    IndexSearcher searcher = new IndexSearcher(reader);
-    for (Term t : sampleTerms) {
-      TermQuery query = new TermQuery(t);
-      TopDocs topDocs = searcher.search(query, 10);
-      assertTrue(topDocs.totalHits > 0);
-    }
-  }
-
-  private List<Term> sample(IndexReader reader, int size) throws IOException {
-    List<Term> sample = new ArrayList<Term>();
-    Random random = new Random();
-    FieldsEnum fieldsEnum = MultiFields.getFields(reader).iterator();
-    String field;
-    while((field = fieldsEnum.next()) != null) {
-      Terms terms = fieldsEnum.terms();
-      assertNotNull(terms);
-      TermsEnum termsEnum = terms.iterator(null);
-      while (termsEnum.next() != null) {
-        if (sample.size() >= size) {
-          int pos = random.nextInt(size);
-          sample.set(pos, new Term(field, termsEnum.term()));
-        } else {
-          sample.add(new Term(field, termsEnum.term()));
-        }
-      }
-    }
-    Collections.shuffle(sample);
-    return sample;
-  }
-
-  private Term findTermThatWouldBeAtIndex(SegmentTermEnum termEnum, int index) throws IOException {
-    int termPosition = index * termIndexInterval * indexDivisor;
-    for (int i = 0; i < termPosition; i++) {
-      if (!termEnum.next()) {
-        fail("Should not have run out of terms.");
-      }
-    }
-    return termEnum.term();
-  }
-
-  private int populate(Directory directory) throws CorruptIndexException, LockObtainFailedException, IOException {
-    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT, 
-        new MockAnalyzer(random, MockTokenizer.KEYWORD, false));
-    config.setCodec(new PreFlexRWCodec());
-    // turn off compound file, this test will open some index files directly.
-    LogMergePolicy mp = newLogMergePolicy();
-    mp.setUseCompoundFile(false);
-    config.setMergePolicy(mp);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random, directory, config);
-    for (int i = 0; i < NUMBER_OF_DOCUMENTS; i++) {
-      Document document = new Document();
-      for (int f = 0; f < NUMBER_OF_FIELDS; f++) {
-        document.add(newField("field" + f, getText(), StringField.TYPE_UNSTORED));
-      }
-      writer.addDocument(document);
-    }
-    writer.forceMerge(1);
-    writer.close();
-    return config.getTermIndexInterval();
-  }
-  
-  private String getText() {
-    return Long.toString(random.nextLong(),Character.MAX_RADIX);
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/lucene40/TestReuseDocsEnum.java b/lucene/src/test/org/apache/lucene/index/codecs/lucene40/TestReuseDocsEnum.java
deleted file mode 100644
index 65aa441..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/lucene40/TestReuseDocsEnum.java
+++ /dev/null
@@ -1,173 +0,0 @@
-package org.apache.lucene.index.codecs.lucene40;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.IdentityHashMap;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.Bits.MatchNoBits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.LineFileDocs;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-
-public class TestReuseDocsEnum extends LuceneTestCase {
-
-  public void testReuseDocsEnumNoReuse() throws IOException {
-    Directory dir = newDirectory();
-    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());
-    RandomIndexWriter writer = new RandomIndexWriter(random, dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
-    int numdocs = atLeast(20);
-    createRandomIndex(numdocs, writer, random);
-    writer.commit();
-
-    IndexReader open = IndexReader.open(dir);
-    IndexReader[] sequentialSubReaders = open.getSequentialSubReaders();
-    for (IndexReader indexReader : sequentialSubReaders) {
-      Terms terms = indexReader.terms("body");
-      TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
-      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());
-      while ((iterator.next()) != null) {
-        DocsEnum docs = iterator.docs(random.nextBoolean() ? bits : new Bits.MatchNoBits(open.maxDoc()), null, random.nextBoolean());
-        enums.put(docs, true);
-      }
-      
-      assertEquals(terms.getUniqueTermCount(), enums.size());  
-    }
-    IOUtils.close(writer, open, dir);
-  }
-  
-  // tests for reuse only if bits are the same either null or the same instance
-  public void testReuseDocsEnumSameBitsOrNull() throws IOException {
-    Directory dir = newDirectory();
-    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());
-    RandomIndexWriter writer = new RandomIndexWriter(random, dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
-    int numdocs = atLeast(20);
-    createRandomIndex(numdocs, writer, random);
-    writer.commit();
-
-    IndexReader open = IndexReader.open(dir);
-    IndexReader[] sequentialSubReaders = open.getSequentialSubReaders();
-    for (IndexReader indexReader : sequentialSubReaders) {
-      Terms terms = indexReader.terms("body");
-      TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
-      MatchNoBits bits = new Bits.MatchNoBits(open.maxDoc());
-      DocsEnum docs = null;
-      while ((iterator.next()) != null) {
-        docs = iterator.docs(bits, docs, random.nextBoolean());
-        enums.put(docs, true);
-      }
-      
-      assertEquals(1, enums.size());
-      enums.clear();
-      iterator = terms.iterator(null);
-      docs = null;
-      while ((iterator.next()) != null) {
-        docs = iterator.docs(new Bits.MatchNoBits(open.maxDoc()), docs, random.nextBoolean());
-        enums.put(docs, true);
-      }
-      assertEquals(terms.getUniqueTermCount(), enums.size());  
-      
-      enums.clear();
-      iterator = terms.iterator(null);
-      docs = null;
-      while ((iterator.next()) != null) {
-        docs = iterator.docs(null, docs, random.nextBoolean());
-        enums.put(docs, true);
-      }
-      assertEquals(1, enums.size());  
-    }
-    IOUtils.close(writer, open, dir);
-  }
-  
-  // make sure we never reuse from another reader even if it is the same field & codec etc
-  public void testReuseDocsEnumDifferentReader() throws IOException {
-    Directory dir = newDirectory();
-    Codec cp = _TestUtil.alwaysPostingsFormat(new Lucene40PostingsFormat());
-    RandomIndexWriter writer = new RandomIndexWriter(random, dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
-    int numdocs = atLeast(20);
-    createRandomIndex(numdocs, writer, random);
-    writer.commit();
-
-    IndexReader firstReader = IndexReader.open(dir);
-    IndexReader secondReader = IndexReader.open(dir);
-    IndexReader[] sequentialSubReaders = firstReader.getSequentialSubReaders();
-    IndexReader[] sequentialSubReaders2 = secondReader.getSequentialSubReaders();
-    
-    for (IndexReader indexReader : sequentialSubReaders) {
-      Terms terms = indexReader.terms("body");
-      TermsEnum iterator = terms.iterator(null);
-      IdentityHashMap<DocsEnum, Boolean> enums = new IdentityHashMap<DocsEnum, Boolean>();
-      MatchNoBits bits = new Bits.MatchNoBits(firstReader.maxDoc());
-      iterator = terms.iterator(null);
-      DocsEnum docs = null;
-      BytesRef term = null;
-      while ((term = iterator.next()) != null) {
-        docs = iterator.docs(null, randomDocsEnum("body", term, sequentialSubReaders2, bits), random.nextBoolean());
-        enums.put(docs, true);
-      }
-      assertEquals(terms.getUniqueTermCount(), enums.size());  
-      
-      iterator = terms.iterator(null);
-      enums.clear();
-      docs = null;
-      while ((term = iterator.next()) != null) {
-        docs = iterator.docs(bits, randomDocsEnum("body", term, sequentialSubReaders2, bits), random.nextBoolean());
-        enums.put(docs, true);
-      }
-      assertEquals(terms.getUniqueTermCount(), enums.size());  
-    }
-    IOUtils.close(writer, firstReader, secondReader, dir);
-  }
-  
-  public DocsEnum randomDocsEnum(String field, BytesRef term, IndexReader[] readers, Bits bits) throws IOException {
-    if (random.nextInt(10) == 0) {
-      return null;
-    }
-    IndexReader indexReader = readers[random.nextInt(readers.length)];
-    return indexReader.termDocsEnum(bits, field, term, random.nextBoolean());
-  }
-
-  /**
-   * populates a writer with random stuff. this must be fully reproducable with
-   * the seed!
-   */
-  public static void createRandomIndex(int numdocs, RandomIndexWriter writer,
-      Random random) throws IOException {
-    LineFileDocs lineFileDocs = new LineFileDocs(random);
-
-    for (int i = 0; i < numdocs; i++) {
-      writer.addDocument(lineFileDocs.nextDoc());
-    }
-  }
-
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/perfield/TestPerFieldPostingsFormat.java b/lucene/src/test/org/apache/lucene/index/codecs/perfield/TestPerFieldPostingsFormat.java
deleted file mode 100644
index f763804..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/perfield/TestPerFieldPostingsFormat.java
+++ /dev/null
@@ -1,267 +0,0 @@
-package org.apache.lucene.index.codecs.perfield;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LogDocMergePolicy;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40Codec;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat;
-import org.apache.lucene.index.codecs.mocksep.MockSepPostingsFormat;
-import org.apache.lucene.index.codecs.simpletext.SimpleTextPostingsFormat;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Test;
-
-/**
- * 
- *
- */
-//TODO: would be better in this test to pull termsenums and instanceof or something?
-// this way we can verify PFPF is doing the right thing.
-// for now we do termqueries.
-public class TestPerFieldPostingsFormat extends LuceneTestCase {
-
-  private IndexWriter newWriter(Directory dir, IndexWriterConfig conf)
-      throws IOException {
-    LogDocMergePolicy logByteSizeMergePolicy = new LogDocMergePolicy();
-    logByteSizeMergePolicy.setUseCompoundFile(false); // make sure we use plain
-    // files
-    conf.setMergePolicy(logByteSizeMergePolicy);
-
-    final IndexWriter writer = new IndexWriter(dir, conf);
-    return writer;
-  }
-
-  private void addDocs(IndexWriter writer, int numDocs) throws IOException {
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      doc.add(newField("content", "aaa", TextField.TYPE_UNSTORED));
-      writer.addDocument(doc);
-    }
-  }
-
-  private void addDocs2(IndexWriter writer, int numDocs) throws IOException {
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      doc.add(newField("content", "bbb", TextField.TYPE_UNSTORED));
-      writer.addDocument(doc);
-    }
-  }
-
-  private void addDocs3(IndexWriter writer, int numDocs) throws IOException {
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      doc.add(newField("content", "ccc", TextField.TYPE_UNSTORED));
-      doc.add(newField("id", "" + i, StringField.TYPE_STORED));
-      writer.addDocument(doc);
-    }
-  }
-
-  /*
-   * Test that heterogeneous index segments are merge successfully
-   */
-  @Test
-  public void testMergeUnusedPerFieldCodec() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT,
-        new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE).setCodec(new MockCodec());
-    IndexWriter writer = newWriter(dir, iwconf);
-    addDocs(writer, 10);
-    writer.commit();
-    addDocs3(writer, 10);
-    writer.commit();
-    addDocs2(writer, 10);
-    writer.commit();
-    assertEquals(30, writer.maxDoc());
-    _TestUtil.checkIndex(dir);
-    writer.forceMerge(1);
-    assertEquals(30, writer.maxDoc());
-    writer.close();
-    dir.close();
-  }
-
-  /*
-   * Test that heterogeneous index segments are merged sucessfully
-   */
-  // TODO: not sure this test is that great, we should probably peek inside PerFieldPostingsFormat or something?!
-  @Test
-  public void testChangeCodecAndMerge() throws IOException {
-    Directory dir = newDirectory();
-    if (VERBOSE) {
-      System.out.println("TEST: make new index");
-    }
-    IndexWriterConfig iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT,
-             new MockAnalyzer(random)).setOpenMode(OpenMode.CREATE).setCodec(new MockCodec());
-    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);
-    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);
-    IndexWriter writer = newWriter(dir, iwconf);
-
-    addDocs(writer, 10);
-    writer.commit();
-    assertQuery(new Term("content", "aaa"), dir, 10);
-    if (VERBOSE) {
-      System.out.println("TEST: addDocs3");
-    }
-    addDocs3(writer, 10);
-    writer.commit();
-    writer.close();
-
-    assertQuery(new Term("content", "ccc"), dir, 10);
-    assertQuery(new Term("content", "aaa"), dir, 10);
-    Lucene40Codec codec = (Lucene40Codec)iwconf.getCodec();
-
-    iwconf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random))
-        .setOpenMode(OpenMode.APPEND).setCodec(codec);
-    //((LogMergePolicy) iwconf.getMergePolicy()).setUseCompoundFile(false);
-    //((LogMergePolicy) iwconf.getMergePolicy()).setMergeFactor(10);
-    iwconf.setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH);
-
-    iwconf.setCodec(new MockCodec2()); // uses standard for field content
-    writer = newWriter(dir, iwconf);
-    // swap in new codec for currently written segments
-    if (VERBOSE) {
-      System.out.println("TEST: add docs w/ Standard codec for content field");
-    }
-    addDocs2(writer, 10);
-    writer.commit();
-    codec = (Lucene40Codec)iwconf.getCodec();
-    assertEquals(30, writer.maxDoc());
-    assertQuery(new Term("content", "bbb"), dir, 10);
-    assertQuery(new Term("content", "ccc"), dir, 10);   ////
-    assertQuery(new Term("content", "aaa"), dir, 10);
-
-    if (VERBOSE) {
-      System.out.println("TEST: add more docs w/ new codec");
-    }
-    addDocs2(writer, 10);
-    writer.commit();
-    assertQuery(new Term("content", "ccc"), dir, 10);
-    assertQuery(new Term("content", "bbb"), dir, 20);
-    assertQuery(new Term("content", "aaa"), dir, 10);
-    assertEquals(40, writer.maxDoc());
-
-    if (VERBOSE) {
-      System.out.println("TEST: now optimize");
-    }
-    writer.forceMerge(1);
-    assertEquals(40, writer.maxDoc());
-    writer.close();
-    assertQuery(new Term("content", "ccc"), dir, 10);
-    assertQuery(new Term("content", "bbb"), dir, 20);
-    assertQuery(new Term("content", "aaa"), dir, 10);
-
-    dir.close();
-  }
-
-  public void assertQuery(Term t, Directory dir, int num)
-      throws CorruptIndexException, IOException {
-    if (VERBOSE) {
-      System.out.println("\nTEST: assertQuery " + t);
-    }
-    IndexReader reader = IndexReader.open(dir, 1);
-    IndexSearcher searcher = newSearcher(reader);
-    TopDocs search = searcher.search(new TermQuery(t), num + 10);
-    assertEquals(num, search.totalHits);
-    reader.close();
-
-  }
-
-  public static class MockCodec extends Lucene40Codec {
-    final PostingsFormat lucene40 = new Lucene40PostingsFormat();
-    final PostingsFormat simpleText = new SimpleTextPostingsFormat();
-    final PostingsFormat mockSep = new MockSepPostingsFormat();
-    
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      if (field.equals("id")) {
-        return simpleText;
-      } else if (field.equals("content")) {
-        return mockSep;
-      } else {
-        return lucene40;
-      }
-    }
-  }
-
-  public static class MockCodec2 extends Lucene40Codec {
-    final PostingsFormat lucene40 = new Lucene40PostingsFormat();
-    final PostingsFormat simpleText = new SimpleTextPostingsFormat();
-    
-    @Override
-    public PostingsFormat getPostingsFormatForField(String field) {
-      if (field.equals("id")) {
-        return simpleText;
-      } else {
-        return lucene40;
-      }
-    }
-  }
-
-  /*
-   * Test per field codec support - adding fields with random codecs
-   */
-  @Test
-  public void testStressPerFieldCodec() throws IOException {
-    Directory dir = newDirectory(random);
-    final int docsPerRound = 97;
-    int numRounds = atLeast(1);
-    for (int i = 0; i < numRounds; i++) {
-      int num = _TestUtil.nextInt(random, 30, 60);
-      IndexWriterConfig config = newIndexWriterConfig(random,
-          TEST_VERSION_CURRENT, new MockAnalyzer(random));
-      config.setOpenMode(OpenMode.CREATE_OR_APPEND);
-      IndexWriter writer = newWriter(dir, config);
-      for (int j = 0; j < docsPerRound; j++) {
-        final Document doc = new Document();
-        for (int k = 0; k < num; k++) {
-          FieldType customType = new FieldType(TextField.TYPE_UNSTORED);
-          customType.setTokenized(random.nextBoolean());
-          customType.setOmitNorms(random.nextBoolean());
-          Field field = newField("" + k, _TestUtil
-              .randomRealisticUnicodeString(random, 128), customType);
-          doc.add(field);
-        }
-        writer.addDocument(doc);
-      }
-      if (random.nextBoolean()) {
-        writer.forceMerge(1);
-      }
-      writer.commit();
-      assertEquals((i + 1) * docsPerRound, writer.maxDoc());
-      writer.close();
-    }
-    dir.close();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/pulsing/Test10KPulsings.java b/lucene/src/test/org/apache/lucene/index/codecs/pulsing/Test10KPulsings.java
deleted file mode 100644
index 01a8298..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/pulsing/Test10KPulsings.java
+++ /dev/null
@@ -1,155 +0,0 @@
-package org.apache.lucene.index.codecs.pulsing;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.text.DecimalFormat;
-import java.text.DecimalFormatSymbols;
-import java.text.NumberFormat;
-import java.util.Locale;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-
-/**
- * Pulses 10k terms/docs, 
- * originally designed to find JRE bugs (https://issues.apache.org/jira/browse/LUCENE-3335)
- * 
- * @lucene.experimental
- */
-public class Test10KPulsings extends LuceneTestCase {
-  public void test10kPulsed() throws Exception {
-    // we always run this test with pulsing codec.
-    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));
-    
-    File f = _TestUtil.getTempDir("10kpulsed");
-    MockDirectoryWrapper dir = newFSDirectory(f);
-    dir.setCheckIndexOnClose(false); // we do this ourselves explicitly
-    RandomIndexWriter iw = new RandomIndexWriter(random, dir, 
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
-    
-    Document document = new Document();
-    FieldType ft = new FieldType(TextField.TYPE_STORED);
-    
-    switch(_TestUtil.nextInt(random, 0, 2)) {
-      case 0: ft.setIndexOptions(IndexOptions.DOCS_ONLY); break;
-      case 1: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS); break;
-      default: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); break; 
-    }
-
-    Field field = newField("field", "", ft);
-    document.add(field);
-    
-    NumberFormat df = new DecimalFormat("00000", new DecimalFormatSymbols(Locale.ENGLISH));
-
-    for (int i = 0; i < 10050; i++) {
-      field.setValue(df.format(i));
-      iw.addDocument(document);
-    }
-    
-    IndexReader ir = iw.getReader();
-    iw.close();
-
-    TermsEnum te = MultiFields.getTerms(ir, "field").iterator(null);
-    DocsEnum de = null;
-    
-    for (int i = 0; i < 10050; i++) {
-      String expected = df.format(i);
-      assertEquals(expected, te.next().utf8ToString());
-      de = _TestUtil.docs(random, te, null, de, false);
-      assertTrue(de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
-      assertEquals(DocIdSetIterator.NO_MORE_DOCS, de.nextDoc());
-    }
-    ir.close();
-
-    _TestUtil.checkIndex(dir);
-    dir.close();
-  }
-  
-  /** a variant, that uses pulsing, but uses a high TF to force pass thru to the underlying codec
-   */
-  public void test10kNotPulsed() throws Exception {
-    // we always run this test with pulsing codec.
-    int freqCutoff = _TestUtil.nextInt(random, 1, 10);
-    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(freqCutoff));
-    
-    File f = _TestUtil.getTempDir("10knotpulsed");
-    MockDirectoryWrapper dir = newFSDirectory(f);
-    dir.setCheckIndexOnClose(false); // we do this ourselves explicitly
-    RandomIndexWriter iw = new RandomIndexWriter(random, dir, 
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
-    
-    Document document = new Document();
-    FieldType ft = new FieldType(TextField.TYPE_STORED);
-    
-    switch(_TestUtil.nextInt(random, 0, 2)) {
-      case 0: ft.setIndexOptions(IndexOptions.DOCS_ONLY); break;
-      case 1: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS); break;
-      default: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); break; 
-    }
-
-    Field field = newField("field", "", ft);
-    document.add(field);
-    
-    NumberFormat df = new DecimalFormat("00000", new DecimalFormatSymbols(Locale.ENGLISH));
-
-    final int freq = freqCutoff + 1;
-    
-    for (int i = 0; i < 10050; i++) {
-      StringBuilder sb = new StringBuilder();
-      for (int j = 0; j < freq; j++) {
-        sb.append(df.format(i));
-        sb.append(' '); // whitespace
-      }
-      field.setValue(sb.toString());
-      iw.addDocument(document);
-    }
-    
-    IndexReader ir = iw.getReader();
-    iw.close();
-
-    TermsEnum te = MultiFields.getTerms(ir, "field").iterator(null);
-    DocsEnum de = null;
-    
-    for (int i = 0; i < 10050; i++) {
-      String expected = df.format(i);
-      assertEquals(expected, te.next().utf8ToString());
-      de = _TestUtil.docs(random, te, null, de, false);
-      assertTrue(de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
-      assertEquals(DocIdSetIterator.NO_MORE_DOCS, de.nextDoc());
-    }
-    ir.close();
-
-    _TestUtil.checkIndex(dir);
-    dir.close();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse.java b/lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse.java
deleted file mode 100644
index dc6198e..0000000
--- a/lucene/src/test/org/apache/lucene/index/codecs/pulsing/TestPulsingReuse.java
+++ /dev/null
@@ -1,125 +0,0 @@
-package org.apache.lucene.index.codecs.pulsing;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.IdentityHashMap;
-import java.util.Map;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.CheckIndex;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.nestedpulsing.NestedPulsingPostingsFormat;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-
-/**
- * Tests that pulsing codec reuses its enums and wrapped enums
- */
-public class TestPulsingReuse extends LuceneTestCase {
-  // TODO: this is a basic test. this thing is complicated, add more
-  public void testSophisticatedReuse() throws Exception {
-    // we always run this test with pulsing codec.
-    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));
-    Directory dir = newDirectory();
-    RandomIndexWriter iw = new RandomIndexWriter(random, dir, 
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
-    Document doc = new Document();
-    doc.add(new Field("foo", "a b b c c c d e f g g h i i j j k", TextField.TYPE_UNSTORED));
-    iw.addDocument(doc);
-    IndexReader ir = iw.getReader();
-    iw.close();
-    
-    IndexReader segment = ir.getSequentialSubReaders()[0];
-    DocsEnum reuse = null;
-    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
-    TermsEnum te = segment.terms("foo").iterator(null);
-    while (te.next() != null) {
-      reuse = te.docs(null, reuse, false);
-      allEnums.put(reuse, true);
-    }
-    
-    assertEquals(2, allEnums.size());
-    
-    allEnums.clear();
-    DocsAndPositionsEnum posReuse = null;
-    te = segment.terms("foo").iterator(null);
-    while (te.next() != null) {
-      posReuse = te.docsAndPositions(null, posReuse);
-      allEnums.put(posReuse, true);
-    }
-    
-    assertEquals(2, allEnums.size());
-    
-    ir.close();
-    dir.close();
-  }
-  
-  /** tests reuse with Pulsing1(Pulsing2(Standard)) */
-  public void testNestedPulsing() throws Exception {
-    // we always run this test with pulsing codec.
-    Codec cp = _TestUtil.alwaysPostingsFormat(new NestedPulsingPostingsFormat());
-    MockDirectoryWrapper dir = newDirectory();
-    dir.setCheckIndexOnClose(false); // will do this ourselves, custom codec
-    RandomIndexWriter iw = new RandomIndexWriter(random, dir, 
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setCodec(cp));
-    Document doc = new Document();
-    doc.add(new Field("foo", "a b b c c c d e f g g g h i i j j k l l m m m", TextField.TYPE_UNSTORED));
-    // note: the reuse is imperfect, here we would have 4 enums (lost reuse when we get an enum for 'm')
-    // this is because we only track the 'last' enum we reused (not all).
-    // but this seems 'good enough' for now.
-    iw.addDocument(doc);
-    IndexReader ir = iw.getReader();
-    iw.close();
-    
-    IndexReader segment = ir.getSequentialSubReaders()[0];
-    DocsEnum reuse = null;
-    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
-    TermsEnum te = segment.terms("foo").iterator(null);
-    while (te.next() != null) {
-      reuse = te.docs(null, reuse, false);
-      allEnums.put(reuse, true);
-    }
-    
-    assertEquals(4, allEnums.size());
-    
-    allEnums.clear();
-    DocsAndPositionsEnum posReuse = null;
-    te = segment.terms("foo").iterator(null);
-    while (te.next() != null) {
-      posReuse = te.docsAndPositions(null, posReuse);
-      allEnums.put(posReuse, true);
-    }
-    
-    assertEquals(4, allEnums.size());
-    
-    ir.close();
-    CheckIndex ci = new CheckIndex(dir);
-    ci.checkIndex(null);
-    dir.close();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/values/TestDocValues.java b/lucene/src/test/org/apache/lucene/index/values/TestDocValues.java
deleted file mode 100644
index ff0aa7b..0000000
--- a/lucene/src/test/org/apache/lucene/index/values/TestDocValues.java
+++ /dev/null
@@ -1,413 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.codecs.lucene40.values.Bytes;
-import org.apache.lucene.index.codecs.lucene40.values.Floats;
-import org.apache.lucene.index.codecs.lucene40.values.Ints;
-import org.apache.lucene.index.codecs.lucene40.values.Writer;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.Counter;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.UnicodeUtil;
-import org.apache.lucene.util._TestUtil;
-
-// TODO: some of this should be under lucene40 codec tests? is talking to codec directly?f
-public class TestDocValues extends LuceneTestCase {
-  private static final Comparator<BytesRef> COMP = BytesRef.getUTF8SortedAsUnicodeComparator();
-  // TODO -- for sorted test, do our own Sort of the
-  // values and verify it's identical
-
-  public void testBytesStraight() throws IOException {
-    runTestBytes(Bytes.Mode.STRAIGHT, true);
-    runTestBytes(Bytes.Mode.STRAIGHT, false);
-  }
-
-  public void testBytesDeref() throws IOException {
-    runTestBytes(Bytes.Mode.DEREF, true);
-    runTestBytes(Bytes.Mode.DEREF, false);
-  }
-  
-  public void testBytesSorted() throws IOException {
-    runTestBytes(Bytes.Mode.SORTED, true);
-    runTestBytes(Bytes.Mode.SORTED, false);
-  }
-
-  public void runTestBytes(final Bytes.Mode mode, final boolean fixedSize)
-      throws IOException {
-
-    final BytesRef bytesRef = new BytesRef();
-
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    Writer w = Bytes.getWriter(dir, "test", mode, fixedSize, COMP, trackBytes, newIOContext(random));
-    int maxDoc = 220;
-    final String[] values = new String[maxDoc];
-    final int fixedLength = 1 + atLeast(50);
-    for (int i = 0; i < 100; i++) {
-      final String s;
-      if (i > 0 && random.nextInt(5) <= 2) {
-        // use prior value
-        s = values[2 * random.nextInt(i)];
-      } else {
-        s = _TestUtil.randomFixedByteLengthUnicodeString(random, fixedSize? fixedLength : 1 + random.nextInt(39));
-      }
-      values[2 * i] = s;
-
-      UnicodeUtil.UTF16toUTF8(s, 0, s.length(), bytesRef);
-      w.add(2 * i, bytesRef);
-    }
-    w.finish(maxDoc);
-    assertEquals(0, trackBytes.get());
-
-    DocValues r = Bytes.getValues(dir, "test", mode, fixedSize, maxDoc, COMP, newIOContext(random));
-
-    // Verify we can load source twice:
-    for (int iter = 0; iter < 2; iter++) {
-      Source s;
-      DocValues.SortedSource ss;
-      if (mode == Bytes.Mode.SORTED) {
-        // default is unicode so we can simply pass null here
-        s = ss = getSortedSource(r);  
-      } else {
-        s = getSource(r);
-        ss = null;
-      }
-      for (int i = 0; i < 100; i++) {
-        final int idx = 2 * i;
-        assertNotNull("doc " + idx + "; value=" + values[idx], s.getBytes(idx,
-            bytesRef));
-        assertEquals("doc " + idx, values[idx], s.getBytes(idx, bytesRef)
-            .utf8ToString());
-        if (ss != null) {
-          assertEquals("doc " + idx, values[idx], ss.getByOrd(ss.ord(idx),
-              bytesRef).utf8ToString());
-         int ord = ss
-              .getByValue(new BytesRef(values[idx]), new BytesRef());
-          assertTrue(ord >= 0);
-          assertEquals(ss.ord(idx), ord);
-        }
-      }
-
-      // Lookup random strings:
-      if (mode == Bytes.Mode.SORTED) {
-        final int valueCount = ss.getValueCount();
-        for (int i = 0; i < 1000; i++) {
-          BytesRef bytesValue = new BytesRef(_TestUtil.randomFixedByteLengthUnicodeString(random, fixedSize? fixedLength : 1 + random.nextInt(39)));
-          int ord = ss.getByValue(bytesValue, new BytesRef());
-          if (ord >= 0) {
-            assertTrue(bytesValue
-                .bytesEquals(ss.getByOrd(ord, bytesRef)));
-            int count = 0;
-            for (int k = 0; k < 100; k++) {
-              if (bytesValue.utf8ToString().equals(values[2 * k])) {
-                assertEquals(ss.ord(2 * k), ord);
-                count++;
-              }
-            }
-            assertTrue(count > 0);
-          } else {
-            assert ord < 0;
-            int insertIndex = (-ord)-1;
-            if (insertIndex == 0) {
-              final BytesRef firstRef = ss.getByOrd(1, bytesRef);
-              // random string was before our first
-              assertTrue(firstRef.compareTo(bytesValue) > 0);
-            } else if (insertIndex == valueCount) {
-              final BytesRef lastRef = ss.getByOrd(valueCount-1, bytesRef);
-              // random string was after our last
-              assertTrue(lastRef.compareTo(bytesValue) < 0);
-            } else {
-              // TODO: I don't think this actually needs a deep copy?
-              final BytesRef before = BytesRef.deepCopyOf(ss.getByOrd(insertIndex-1, bytesRef));
-              BytesRef after = ss.getByOrd(insertIndex, bytesRef);
-              assertTrue(COMP.compare(before, bytesValue) < 0);
-              assertTrue(COMP.compare(bytesValue, after) < 0);
-            }
-          }
-        }
-      }
-    }
-
-
-    r.close();
-    dir.close();
-  }
-
-  public void testVariableIntsLimits() throws IOException {
-    long[][] minMax = new long[][] { { Long.MIN_VALUE, Long.MAX_VALUE },
-        { Long.MIN_VALUE + 1, 1 }, { -1, Long.MAX_VALUE },
-        { Long.MIN_VALUE, -1 }, { 1, Long.MAX_VALUE },
-        { -1, Long.MAX_VALUE - 1 }, { Long.MIN_VALUE + 2, 1 }, };
-    Type[] expectedTypes = new Type[] { Type.FIXED_INTS_64,
-        Type.FIXED_INTS_64, Type.FIXED_INTS_64,
-        Type.FIXED_INTS_64, Type.VAR_INTS, Type.VAR_INTS,
-        Type.VAR_INTS, };
-    for (int i = 0; i < minMax.length; i++) {
-      Directory dir = newDirectory();
-      final Counter trackBytes = Counter.newCounter();
-      Writer w = Ints.getWriter(dir, "test", trackBytes, Type.VAR_INTS, newIOContext(random));
-      w.add(0, minMax[i][0]);
-      w.add(1, minMax[i][1]);
-      w.finish(2);
-      assertEquals(0, trackBytes.get());
-      DocValues r = Ints.getValues(dir, "test", 2,  Type.VAR_INTS, newIOContext(random));
-      Source source = getSource(r);
-      assertEquals(i + " with min: " + minMax[i][0] + " max: " + minMax[i][1],
-          expectedTypes[i], source.type());
-      assertEquals(minMax[i][0], source.getInt(0));
-      assertEquals(minMax[i][1], source.getInt(1));
-
-      r.close();
-      dir.close();
-    }
-  }
-  
-  public void testVInts() throws IOException {
-    testInts(Type.VAR_INTS, 63);
-  }
-  
-  public void testFixedInts() throws IOException {
-    testInts(Type.FIXED_INTS_64, 63);
-    testInts(Type.FIXED_INTS_32, 31);
-    testInts(Type.FIXED_INTS_16, 15);
-    testInts(Type.FIXED_INTS_8, 7);
-
-  }
-  
-  public void testGetInt8Array() throws IOException {
-    byte[] sourceArray = new byte[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_8, newIOContext(random));
-    for (int i = 0; i < sourceArray.length; i++) {
-      w.add(i, (long) sourceArray[i]);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_8, newIOContext(random));
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    byte[] loaded = ((byte[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
-    }
-    r.close();
-    dir.close();
-  }
-  
-  public void testGetInt16Array() throws IOException {
-    short[] sourceArray = new short[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_16, newIOContext(random));
-    for (int i = 0; i < sourceArray.length; i++) {
-      w.add(i, (long) sourceArray[i]);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_16, newIOContext(random));
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    short[] loaded = ((short[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
-    }
-    r.close();
-    dir.close();
-  }
-  
-  public void testGetInt64Array() throws IOException {
-    long[] sourceArray = new long[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_64, newIOContext(random));
-    for (int i = 0; i < sourceArray.length; i++) {
-      w.add(i, sourceArray[i]);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_64, newIOContext(random));
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    long[] loaded = ((long[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
-    }
-    r.close();
-    dir.close();
-  }
-  
-  public void testGetInt32Array() throws IOException {
-    int[] sourceArray = new int[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    Writer w = Ints.getWriter(dir, "test", trackBytes, Type.FIXED_INTS_32, newIOContext(random));
-    for (int i = 0; i < sourceArray.length; i++) {
-      w.add(i, (long) sourceArray[i]);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Ints.getValues(dir, "test", sourceArray.length, Type.FIXED_INTS_32, newIOContext(random));
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    int[] loaded = ((int[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i]);
-    }
-    r.close();
-    dir.close();
-  }
-  
-  public void testGetFloat32Array() throws IOException {
-    float[] sourceArray = new float[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    Writer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random), Type.FLOAT_32);
-    for (int i = 0; i < sourceArray.length; i++) {
-      w.add(i, sourceArray[i]);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Floats.getValues(dir, "test", 3, newIOContext(random), Type.FLOAT_32);
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    float[] loaded = ((float[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i], 0.0f);
-    }
-    r.close();
-    dir.close();
-  }
-  
-  public void testGetFloat64Array() throws IOException {
-    double[] sourceArray = new double[] {1,2,3};
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    Writer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random), Type.FLOAT_64);
-    for (int i = 0; i < sourceArray.length; i++) {
-      w.add(i, sourceArray[i]);
-    }
-    w.finish(sourceArray.length);
-    DocValues r = Floats.getValues(dir, "test", 3, newIOContext(random), Type.FLOAT_64);
-    Source source = r.getSource();
-    assertTrue(source.hasArray());
-    double[] loaded = ((double[])source.getArray());
-    assertEquals(loaded.length, sourceArray.length);
-    for (int i = 0; i < loaded.length; i++) {
-      assertEquals("value didn't match at index " + i, sourceArray[i], loaded[i], 0.0d);
-    }
-    r.close();
-    dir.close();
-  }
-
-  private void testInts(Type type, int maxBit) throws IOException {
-    long maxV = 1;
-    final int NUM_VALUES = 333 + random.nextInt(333);
-    final long[] values = new long[NUM_VALUES];
-    for (int rx = 1; rx < maxBit; rx++, maxV *= 2) {
-      Directory dir = newDirectory();
-      final Counter trackBytes = Counter.newCounter();
-      Writer w = Ints.getWriter(dir, "test", trackBytes, type, newIOContext(random));
-      for (int i = 0; i < NUM_VALUES; i++) {
-        final long v = random.nextLong() % (1 + maxV);
-        values[i] = v;
-        w.add(i, v);
-      }
-      final int additionalDocs = 1 + random.nextInt(9);
-      w.finish(NUM_VALUES + additionalDocs);
-      assertEquals(0, trackBytes.get());
-
-      DocValues r = Ints.getValues(dir, "test", NUM_VALUES + additionalDocs, type, newIOContext(random));
-      for (int iter = 0; iter < 2; iter++) {
-        Source s = getSource(r);
-        assertEquals(type, s.type());
-        for (int i = 0; i < NUM_VALUES; i++) {
-          final long v = s.getInt(i);
-          assertEquals("index " + i, values[i], v);
-        }
-      }
-
-      r.close();
-      dir.close();
-    }
-  }
-
-  public void testFloats4() throws IOException {
-    runTestFloats(Type.FLOAT_32, 0.00001);
-  }
-
-  private void runTestFloats(Type type, double delta) throws IOException {
-    Directory dir = newDirectory();
-    final Counter trackBytes = Counter.newCounter();
-    Writer w = Floats.getWriter(dir, "test", trackBytes, newIOContext(random), type);
-    final int NUM_VALUES = 777 + random.nextInt(777);;
-    final double[] values = new double[NUM_VALUES];
-    for (int i = 0; i < NUM_VALUES; i++) {
-      final double v = type == Type.FLOAT_32 ? random.nextFloat() : random
-          .nextDouble();
-      values[i] = v;
-      w.add(i, v);
-    }
-    final int additionalValues = 1 + random.nextInt(10);
-    w.finish(NUM_VALUES + additionalValues);
-    assertEquals(0, trackBytes.get());
-
-    DocValues r = Floats.getValues(dir, "test", NUM_VALUES + additionalValues, newIOContext(random), type);
-    for (int iter = 0; iter < 2; iter++) {
-      Source s = getSource(r);
-      for (int i = 0; i < NUM_VALUES; i++) {
-        assertEquals("" + i, values[i], s.getFloat(i), 0.0f);
-      }
-    }
-    r.close();
-    dir.close();
-  }
-
-  public void testFloats8() throws IOException {
-    runTestFloats(Type.FLOAT_64, 0.0);
-  }
-  
-
-  private Source getSource(DocValues values) throws IOException {
-    // getSource uses cache internally
-    switch(random.nextInt(5)) {
-    case 3:
-      return values.load();
-    case 2:
-      return values.getDirectSource();
-    case 1:
-      return values.getSource();
-    default:
-      return values.getSource();
-    }
-  }
-  
-  private SortedSource getSortedSource(DocValues values) throws IOException {
-    return getSource(values).asSortedSource();
-  }
-  
-}
diff --git a/lucene/src/test/org/apache/lucene/index/values/TestDocValuesIndexing.java b/lucene/src/test/org/apache/lucene/index/values/TestDocValuesIndexing.java
deleted file mode 100644
index 35fa90f..0000000
--- a/lucene/src/test/org/apache/lucene/index/values/TestDocValuesIndexing.java
+++ /dev/null
@@ -1,591 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.EnumSet;
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.DocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.LogDocMergePolicy;
-import org.apache.lucene.index.LogMergePolicy;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.PerDocProducer;
-import org.apache.lucene.search.*;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.LockObtainFailedException;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Before;
-
-/**
- * 
- * Tests DocValues integration into IndexWriter & Codecs
- * 
- */
-public class TestDocValuesIndexing extends LuceneTestCase {
-  /*
-   * - add test for multi segment case with deletes
-   * - add multithreaded tests / integrate into stress indexing?
-   */
-
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    assumeFalse("cannot work with preflex codec", Codec.getDefault().getName().equals("Lucene3x"));
-  }
-  
-  /*
-   * Simple test case to show how to use the API
-   */
-  public void testDocValuesSimple() throws CorruptIndexException, IOException {
-    Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter(dir, writerConfig(false));
-    for (int i = 0; i < 5; i++) {
-      Document doc = new Document();
-      DocValuesField valuesField = new DocValuesField("docId");
-      valuesField.setInt(i);
-      doc.add(valuesField);
-      doc.add(new TextField("docId", "" + i));
-      writer.addDocument(doc);
-    }
-    writer.commit();
-    writer.forceMerge(1, true);
-
-    writer.close(true);
-
-    IndexReader reader = IndexReader.open(dir, 1);
-    assertEquals(1, reader.getSequentialSubReaders().length);
-
-    IndexSearcher searcher = new IndexSearcher(reader);
-
-    BooleanQuery query = new BooleanQuery();
-    query.add(new TermQuery(new Term("docId", "0")), BooleanClause.Occur.SHOULD);
-    query.add(new TermQuery(new Term("docId", "1")), BooleanClause.Occur.SHOULD);
-    query.add(new TermQuery(new Term("docId", "2")), BooleanClause.Occur.SHOULD);
-    query.add(new TermQuery(new Term("docId", "3")), BooleanClause.Occur.SHOULD);
-    query.add(new TermQuery(new Term("docId", "4")), BooleanClause.Occur.SHOULD);
-
-    TopDocs search = searcher.search(query, 10);
-    assertEquals(5, search.totalHits);
-    ScoreDoc[] scoreDocs = search.scoreDocs;
-    DocValues docValues = MultiDocValues.getDocValues(reader, "docId");
-    Source source = docValues.getSource();
-    for (int i = 0; i < scoreDocs.length; i++) {
-      assertEquals(i, scoreDocs[i].doc);
-      assertEquals(i, source.getInt(scoreDocs[i].doc));
-    }
-    reader.close();
-    dir.close();
-  }
-
-  public void testIndexBytesNoDeletes() throws IOException {
-    runTestIndexBytes(writerConfig(random.nextBoolean()), false);
-  }
-
-  public void testIndexBytesDeletes() throws IOException {
-    runTestIndexBytes(writerConfig(random.nextBoolean()), true);
-  }
-
-  public void testIndexNumericsNoDeletes() throws IOException {
-    runTestNumerics(writerConfig(random.nextBoolean()), false);
-  }
-
-  public void testIndexNumericsDeletes() throws IOException {
-    runTestNumerics(writerConfig(random.nextBoolean()), true);
-  }
-
-  public void testAddIndexes() throws IOException {
-    int valuesPerIndex = 10;
-    List<Type> values = Arrays.asList(Type.values());
-    Collections.shuffle(values, random);
-    Type first = values.get(0);
-    Type second = values.get(1);
-    // index first index
-    Directory d_1 = newDirectory();
-    IndexWriter w_1 = new IndexWriter(d_1, writerConfig(random.nextBoolean()));
-    indexValues(w_1, valuesPerIndex, first, values, false, 7);
-    w_1.commit();
-    assertEquals(valuesPerIndex, w_1.maxDoc());
-    _TestUtil.checkIndex(d_1);
-
-    // index second index
-    Directory d_2 = newDirectory();
-    IndexWriter w_2 = new IndexWriter(d_2, writerConfig(random.nextBoolean()));
-    indexValues(w_2, valuesPerIndex, second, values, false, 7);
-    w_2.commit();
-    assertEquals(valuesPerIndex, w_2.maxDoc());
-    _TestUtil.checkIndex(d_2);
-
-    Directory target = newDirectory();
-    IndexWriter w = new IndexWriter(target, writerConfig(random.nextBoolean()));
-    IndexReader r_1 = IndexReader.open(w_1, true);
-    IndexReader r_2 = IndexReader.open(w_2, true);
-    if (random.nextBoolean()) {
-      w.addIndexes(d_1, d_2);
-    } else {
-      w.addIndexes(r_1, r_2);
-    }
-    w.forceMerge(1, true);
-    w.commit();
-    
-    _TestUtil.checkIndex(target);
-    assertEquals(valuesPerIndex * 2, w.maxDoc());
-
-    // check values
-    
-    IndexReader merged = IndexReader.open(w, true);
-    Source source_1 = getSource(getDocValues(r_1, first.name()));
-    Source source_2 = getSource(getDocValues(r_2, second.name()));
-    Source source_1_merged = getSource(getDocValues(merged, first.name()));
-    Source source_2_merged = getSource(getDocValues(merged, second
-        .name()));
-    for (int i = 0; i < r_1.maxDoc(); i++) {
-      switch (first) {
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_FIXED_SORTED:
-      case BYTES_VAR_SORTED:
-        assertEquals(source_1.getBytes(i, new BytesRef()),
-            source_1_merged.getBytes(i, new BytesRef()));
-        break;
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case FIXED_INTS_8:
-      case VAR_INTS:
-        assertEquals(source_1.getInt(i), source_1_merged.getInt(i));
-        break;
-      case FLOAT_32:
-      case FLOAT_64:
-        assertEquals(source_1.getFloat(i), source_1_merged.getFloat(i), 0.0d);
-        break;
-      default:
-        fail("unkonwn " + first);
-      }
-    }
-
-    for (int i = r_1.maxDoc(); i < merged.maxDoc(); i++) {
-      switch (second) {
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_FIXED_SORTED:
-      case BYTES_VAR_SORTED:
-        assertEquals(source_2.getBytes(i - r_1.maxDoc(), new BytesRef()),
-            source_2_merged.getBytes(i, new BytesRef()));
-        break;
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case FIXED_INTS_8:
-      case VAR_INTS:
-        assertEquals(source_2.getInt(i - r_1.maxDoc()),
-            source_2_merged.getInt(i));
-        break;
-      case FLOAT_32:
-      case FLOAT_64:
-        assertEquals(source_2.getFloat(i - r_1.maxDoc()),
-            source_2_merged.getFloat(i), 0.0d);
-        break;
-      default:
-        fail("unkonwn " + first);
-      }
-    }
-    // close resources
-    r_1.close();
-    r_2.close();
-    merged.close();
-    w_1.close(true);
-    w_2.close(true);
-    w.close(true);
-    d_1.close();
-    d_2.close();
-    target.close();
-  }
-
-  private IndexWriterConfig writerConfig(boolean useCompoundFile) {
-    final IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT,
-        new MockAnalyzer(random));
-    cfg.setMergePolicy(newLogMergePolicy(random));
-    LogMergePolicy policy = new LogDocMergePolicy();
-    cfg.setMergePolicy(policy);
-    policy.setUseCompoundFile(useCompoundFile);
-    return cfg;
-  }
-
-  @SuppressWarnings("fallthrough")
-  public void runTestNumerics(IndexWriterConfig cfg, boolean withDeletions)
-      throws IOException {
-    Directory d = newDirectory();
-    IndexWriter w = new IndexWriter(d, cfg);
-    final int numValues = 50 + atLeast(10);
-    final List<Type> numVariantList = new ArrayList<Type>(NUMERICS);
-
-    // run in random order to test if fill works correctly during merges
-    Collections.shuffle(numVariantList, random);
-    for (Type val : numVariantList) {
-      FixedBitSet deleted = indexValues(w, numValues, val, numVariantList,
-          withDeletions, 7);
-      List<Closeable> closeables = new ArrayList<Closeable>();
-      IndexReader r = IndexReader.open(w, true);
-      final int numRemainingValues = numValues - deleted.cardinality();
-      final int base = r.numDocs() - numRemainingValues;
-      // for FIXED_INTS_8 we use value mod 128 - to enable testing in 
-      // one go we simply use numValues as the mod for all other INT types
-      int mod = numValues;
-      switch (val) {
-      case FIXED_INTS_8:
-        mod = 128;
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case VAR_INTS: {
-        DocValues intsReader = getDocValues(r, val.name());
-        assertNotNull(intsReader);
-
-        Source ints = getSource(intsReader);
-
-        for (int i = 0; i < base; i++) {
-          long value = ints.getInt(i);
-          assertEquals("index " + i, 0, value);
-        }
-
-        int expected = 0;
-        for (int i = base; i < r.numDocs(); i++, expected++) {
-          while (deleted.get(expected)) {
-            expected++;
-          }
-          assertEquals(val + " mod: " + mod + " index: " +  i, expected%mod, ints.getInt(i));
-        }
-      }
-        break;
-      case FLOAT_32:
-      case FLOAT_64: {
-        DocValues floatReader = getDocValues(r, val.name());
-        assertNotNull(floatReader);
-        Source floats = getSource(floatReader);
-        for (int i = 0; i < base; i++) {
-          double value = floats.getFloat(i);
-          assertEquals(val + " failed for doc: " + i + " base: " + base,
-              0.0d, value, 0.0d);
-        }
-        int expected = 0;
-        for (int i = base; i < r.numDocs(); i++, expected++) {
-          while (deleted.get(expected)) {
-            expected++;
-          }
-          assertEquals("index " + i, 2.0 * expected, floats.getFloat(i),
-              0.00001);
-        }
-      }
-        break;
-      default:
-        fail("unexpected value " + val);
-      }
-
-      closeables.add(r);
-      for (Closeable toClose : closeables) {
-        toClose.close();
-      }
-    }
-    w.close();
-    d.close();
-  }
-  
-  public void runTestIndexBytes(IndexWriterConfig cfg, boolean withDeletions)
-      throws CorruptIndexException, LockObtainFailedException, IOException {
-    final Directory d = newDirectory();
-    IndexWriter w = new IndexWriter(d, cfg);
-    final List<Type> byteVariantList = new ArrayList<Type>(BYTES);
-    // run in random order to test if fill works correctly during merges
-    Collections.shuffle(byteVariantList, random);
-    final int numValues = 50 + atLeast(10);
-    for (Type byteIndexValue : byteVariantList) {
-      List<Closeable> closeables = new ArrayList<Closeable>();
-      final int bytesSize = 1 + atLeast(50);
-      FixedBitSet deleted = indexValues(w, numValues, byteIndexValue,
-          byteVariantList, withDeletions, bytesSize);
-      final IndexReader r = IndexReader.open(w, withDeletions);
-      assertEquals(0, r.numDeletedDocs());
-      final int numRemainingValues = numValues - deleted.cardinality();
-      final int base = r.numDocs() - numRemainingValues;
-      DocValues bytesReader = getDocValues(r, byteIndexValue.name());
-      assertNotNull("field " + byteIndexValue.name()
-          + " returned null reader - maybe merged failed", bytesReader);
-      Source bytes = getSource(bytesReader);
-      byte upto = 0;
-
-      // test the filled up slots for correctness
-      for (int i = 0; i < base; i++) {
-
-        BytesRef br = bytes.getBytes(i, new BytesRef());
-        String msg = " field: " + byteIndexValue.name() + " at index: " + i
-            + " base: " + base + " numDocs:" + r.numDocs();
-        switch (byteIndexValue) {
-        case BYTES_VAR_STRAIGHT:
-        case BYTES_FIXED_STRAIGHT:
-        case BYTES_FIXED_DEREF:
-        case BYTES_FIXED_SORTED:
-          // fixed straight returns bytesref with zero bytes all of fixed
-          // length
-          assertNotNull("expected none null - " + msg, br);
-          if (br.length != 0) {
-            assertEquals("expected zero bytes of length " + bytesSize + " - "
-                + msg + br.utf8ToString(), bytesSize, br.length);
-            for (int j = 0; j < br.length; j++) {
-              assertEquals("Byte at index " + j + " doesn't match - " + msg, 0,
-                  br.bytes[br.offset + j]);
-            }
-          }
-          break;
-        default:
-          assertNotNull("expected none null - " + msg, br);
-          assertEquals(byteIndexValue + "", 0, br.length);
-          // make sure we advance at least until base
-        }
-      }
-
-      // test the actual doc values added in this iteration
-      assertEquals(base + numRemainingValues, r.numDocs());
-      int v = 0;
-      for (int i = base; i < r.numDocs(); i++) {
-        String msg = " field: " + byteIndexValue.name() + " at index: " + i
-            + " base: " + base + " numDocs:" + r.numDocs() + " bytesSize: "
-            + bytesSize + " src: " + bytes;
-        while (withDeletions && deleted.get(v++)) {
-          upto += bytesSize;
-        }
-        BytesRef br = bytes.getBytes(i, new BytesRef());
-        assertTrue(msg, br.length > 0);
-        for (int j = 0; j < br.length; j++, upto++) {
-          if (!(br.bytes.length > br.offset + j))
-            br = bytes.getBytes(i, new BytesRef());
-          assertTrue("BytesRef index exceeded [" + msg + "] offset: "
-              + br.offset + " length: " + br.length + " index: "
-              + (br.offset + j), br.bytes.length > br.offset + j);
-          assertEquals("SourceRef Byte at index " + j + " doesn't match - "
-              + msg, upto, br.bytes[br.offset + j]);
-        }
-      }
-
-      // clean up
-      closeables.add(r);
-      for (Closeable toClose : closeables) {
-        toClose.close();
-      }
-    }
-
-    w.close();
-    d.close();
-  }
-
-  private DocValues getDocValues(IndexReader reader, String field) throws IOException {
-    return MultiDocValues.getDocValues(reader, field);
-  }
-
-  private Source getSource(DocValues values) throws IOException {
-    // getSource uses cache internally
-    switch(random.nextInt(5)) {
-    case 3:
-      return values.load();
-    case 2:
-      return values.getDirectSource();
-    case 1:
-      return values.getSource();
-    default:
-      return values.getSource();
-    }
-  }
-
-
-  private static EnumSet<Type> BYTES = EnumSet.of(Type.BYTES_FIXED_DEREF,
-      Type.BYTES_FIXED_STRAIGHT, Type.BYTES_VAR_DEREF,
-      Type.BYTES_VAR_STRAIGHT, Type.BYTES_FIXED_SORTED, Type.BYTES_VAR_SORTED);
-
-  private static EnumSet<Type> NUMERICS = EnumSet.of(Type.VAR_INTS,
-      Type.FIXED_INTS_16, Type.FIXED_INTS_32,
-      Type.FIXED_INTS_64, 
-      Type.FIXED_INTS_8,
-      Type.FLOAT_32,
-      Type.FLOAT_64);
-
-  private FixedBitSet indexValues(IndexWriter w, int numValues, Type value,
-      List<Type> valueVarList, boolean withDeletions, int bytesSize)
-      throws CorruptIndexException, IOException {
-    final boolean isNumeric = NUMERICS.contains(value);
-    FixedBitSet deleted = new FixedBitSet(numValues);
-    Document doc = new Document();
-    DocValuesField valField = new DocValuesField(value.name());
-    doc.add(valField);
-    final BytesRef bytesRef = new BytesRef();
-
-    final String idBase = value.name() + "_";
-    final byte[] b = new byte[bytesSize];
-    if (bytesRef != null) {
-      bytesRef.bytes = b;
-      bytesRef.length = b.length;
-      bytesRef.offset = 0;
-    }
-    byte upto = 0;
-    for (int i = 0; i < numValues; i++) {
-      if (isNumeric) {
-        switch (value) {
-        case VAR_INTS:
-          valField.setInt((long)i);
-          break;
-        case FIXED_INTS_16:
-          valField.setInt((short)i, random.nextInt(10) != 0);
-          break;
-        case FIXED_INTS_32:
-          valField.setInt(i, random.nextInt(10) != 0);
-          break;
-        case FIXED_INTS_64:
-          valField.setInt((long)i, random.nextInt(10) != 0);
-          break;
-        case FIXED_INTS_8:
-          valField.setInt((byte)(0xFF & (i % 128)), random.nextInt(10) != 0);
-          break;
-        case FLOAT_32:
-          valField.setFloat(2.0f * i);
-          break;
-        case FLOAT_64:
-          valField.setFloat(2.0d * i);
-          break;
-       
-        default:
-          fail("unexpected value " + value);
-        }
-      } else {
-        for (int j = 0; j < b.length; j++) {
-          b[j] = upto++;
-        }
-        if (bytesRef != null) {
-          valField.setBytes(bytesRef, value);
-        }
-      }
-      doc.removeFields("id");
-      doc.add(new Field("id", idBase + i, StringField.TYPE_STORED));
-      w.addDocument(doc);
-
-      if (i % 7 == 0) {
-        if (withDeletions && random.nextBoolean()) {
-          Type val = valueVarList.get(random.nextInt(1 + valueVarList
-              .indexOf(value)));
-          final int randInt = val == value ? random.nextInt(1 + i) : random
-              .nextInt(numValues);
-          w.deleteDocuments(new Term("id", val.name() + "_" + randInt));
-          if (val == value) {
-            deleted.set(randInt);
-          }
-        }
-        if (random.nextInt(10) == 0) {
-          w.commit();
-        }
-      }
-    }
-    w.commit();
-
-    // TODO test multi seg with deletions
-    if (withDeletions || random.nextBoolean()) {
-      w.forceMerge(1, true);
-    }
-    return deleted;
-  }
-
-  public void testMultiValuedDocValuesField() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random, d);
-    Document doc = new Document();
-    DocValuesField f = new DocValuesField("field");
-    f.setInt(17);
-    // Index doc values are single-valued so we should not
-    // be able to add same field more than once:
-    doc.add(f);
-    doc.add(f);
-    try {
-      w.addDocument(doc);
-      fail("didn't hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    doc = new Document();
-    doc.add(f);
-    w.addDocument(doc);
-    w.forceMerge(1);
-    IndexReader r = w.getReader();
-    w.close();
-    assertEquals(17, r.getSequentialSubReaders()[0].docValues("field").load().getInt(0));
-    r.close();
-    d.close();
-  }
-
-  public void testDifferentTypedDocValuesField() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random, d);
-    Document doc = new Document();
-    DocValuesField f = new DocValuesField("field");
-    f.setInt(17);
-    // Index doc values are single-valued so we should not
-    // be able to add same field more than once:
-    doc.add(f);
-    DocValuesField f2 = new DocValuesField("field");
-    f2.setFloat(22.0);
-    doc.add(f2);
-    try {
-      w.addDocument(doc);
-      fail("didn't hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    doc = new Document();
-    doc.add(f);
-    w.addDocument(doc);
-    w.forceMerge(1);
-    IndexReader r = w.getReader();
-    w.close();
-    assertEquals(17, r.getSequentialSubReaders()[0].docValues("field").load().getInt(0));
-    r.close();
-    d.close();
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/index/values/TestTypePromotion.java b/lucene/src/test/org/apache/lucene/index/values/TestTypePromotion.java
deleted file mode 100644
index 412f8cd..0000000
--- a/lucene/src/test/org/apache/lucene/index/values/TestTypePromotion.java
+++ /dev/null
@@ -1,320 +0,0 @@
-package org.apache.lucene.index.values;
-
-import java.io.IOException;
-import java.util.EnumSet;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.DocValuesField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReader.ReaderContext;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.SlowMultiReaderWrapper;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.lucene40.values.BytesRefUtils;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.junit.Before;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-public class TestTypePromotion extends LuceneTestCase {
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    assumeFalse("cannot work with preflex codec", Codec.getDefault().getName().equals("Lucene3x"));
-  }
-
-  private static EnumSet<Type> INTEGERS = EnumSet.of(Type.VAR_INTS,
-      Type.FIXED_INTS_16, Type.FIXED_INTS_32,
-      Type.FIXED_INTS_64, Type.FIXED_INTS_8);
-
-  private static EnumSet<Type> FLOATS = EnumSet.of(Type.FLOAT_32,
-      Type.FLOAT_64);
-
-  private static EnumSet<Type> UNSORTED_BYTES = EnumSet.of(
-      Type.BYTES_FIXED_DEREF, Type.BYTES_FIXED_STRAIGHT,
-      Type.BYTES_VAR_STRAIGHT, Type.BYTES_VAR_DEREF);
-
-  private static EnumSet<Type> SORTED_BYTES = EnumSet.of(
-      Type.BYTES_FIXED_SORTED, Type.BYTES_VAR_SORTED);
-  
-  public Type randomValueType(EnumSet<Type> typeEnum, Random random) {
-    Type[] array = typeEnum.toArray(new Type[0]);
-    return array[random.nextInt(array.length)];
-  }
-  
-  private static enum TestType {
-    Int, Float, Byte
-  }
-
-  private void runTest(EnumSet<Type> types, TestType type)
-      throws CorruptIndexException, IOException {
-    Directory dir = newDirectory();
-    IndexWriter writer = new IndexWriter(dir,
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-    int num_1 = atLeast(200);
-    int num_2 = atLeast(200);
-    int num_3 = atLeast(200);
-    long[] values = new long[num_1 + num_2 + num_3];
-    index(writer, new DocValuesField("promote"),
-        randomValueType(types, random), values, 0, num_1);
-    writer.commit();
-    
-    index(writer, new DocValuesField("promote"),
-        randomValueType(types, random), values, num_1, num_2);
-    writer.commit();
-    
-    if (random.nextInt(4) == 0) {
-      // once in a while use addIndexes
-      writer.forceMerge(1);
-      
-      Directory dir_2 = newDirectory() ;
-      IndexWriter writer_2 = new IndexWriter(dir_2,
-          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-      index(writer_2, new DocValuesField("promote"),
-          randomValueType(types, random), values, num_1 + num_2, num_3);
-      writer_2.commit();
-      writer_2.close();
-      if (random.nextBoolean()) {
-        writer.addIndexes(dir_2);
-      } else {
-        // do a real merge here
-        IndexReader open = IndexReader.open(dir_2);
-        // we cannot use SlowMR for sorted bytes, because it returns a null sortedsource
-        boolean useSlowMRWrapper = types != SORTED_BYTES && random.nextBoolean();
-        writer.addIndexes(useSlowMRWrapper ? new SlowMultiReaderWrapper(open) : open);
-        open.close();
-      }
-      dir_2.close();
-    } else {
-      index(writer, new DocValuesField("promote"),
-          randomValueType(types, random), values, num_1 + num_2, num_3);
-    }
-
-    writer.forceMerge(1);
-    writer.close();
-    assertValues(type, dir, values);
-    dir.close();
-  }
-
-  
-  private void assertValues(TestType type, Directory dir, long[] values)
-      throws CorruptIndexException, IOException {
-    IndexReader reader = IndexReader.open(dir);
-    assertEquals(1, reader.getSequentialSubReaders().length);
-    ReaderContext topReaderContext = reader.getTopReaderContext();
-    ReaderContext[] children = topReaderContext.children();
-    DocValues docValues = children[0].reader.docValues("promote");
-    assertEquals(1, children.length);
-    Source directSource = docValues.getDirectSource();
-    for (int i = 0; i < values.length; i++) {
-      int id = Integer.parseInt(reader.document(i).get("id"));
-      String msg = "id: " + id + " doc: " + i;
-      switch (type) {
-      case Byte:
-        BytesRef bytes = directSource.getBytes(i, new BytesRef());
-        long value = 0;
-        switch(bytes.length) {
-        case 1:
-          value = bytes.bytes[bytes.offset];
-          break;
-        case 2:
-          value = BytesRefUtils.asShort(bytes);
-          break;
-        case 4:
-          value = BytesRefUtils.asInt(bytes);
-          break;
-        case 8:
-          value = BytesRefUtils.asLong(bytes);
-          break;
-          
-        default:
-          fail(msg + " bytessize: " + bytes.length);
-        }
-        
-        assertEquals(msg  + " byteSize: " + bytes.length, values[id], value);
-        break;
-      case Float:
-          assertEquals(msg, values[id], Double.doubleToRawLongBits(directSource.getFloat(i)));
-        break;
-      case Int:
-        assertEquals(msg, values[id], directSource.getInt(i));
-        break;
-      default:
-        break;
-      }
-
-    }
-    docValues.close();
-    reader.close();
-  }
-
-  public void index(IndexWriter writer, DocValuesField valField,
-      Type valueType, long[] values, int offset, int num)
-      throws CorruptIndexException, IOException {
-    BytesRef ref = new BytesRef(new byte[] { 1, 2, 3, 4 });
-    for (int i = offset; i < offset + num; i++) {
-      Document doc = new Document();
-      doc.add(new Field("id", i + "", TextField.TYPE_STORED));
-      switch (valueType) {
-      case VAR_INTS:
-        values[i] = random.nextInt();
-        valField.setInt(values[i]);
-        break;
-      case FIXED_INTS_16:
-        values[i] = random.nextInt(Short.MAX_VALUE);
-        valField.setInt((short) values[i], true);
-        break;
-      case FIXED_INTS_32:
-        values[i] = random.nextInt();
-        valField.setInt((int) values[i], true);
-        break;
-      case FIXED_INTS_64:
-        values[i] = random.nextLong();
-        valField.setInt(values[i], true);
-        break;
-      case FLOAT_64:
-        double nextDouble = random.nextDouble();
-        values[i] = Double.doubleToRawLongBits(nextDouble);
-        valField.setFloat(nextDouble);
-        break;
-      case FLOAT_32:
-        final float nextFloat = random.nextFloat();
-        values[i] = Double.doubleToRawLongBits(nextFloat);
-        valField.setFloat(nextFloat);
-        break;
-      case FIXED_INTS_8:
-         values[i] = (byte) i;
-        valField.setInt((byte)values[i], true);
-        break;
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-        values[i] = random.nextLong();
-        BytesRefUtils.copyLong(ref, values[i]);
-        valField.setBytes(ref, valueType);
-        break;
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        if (random.nextBoolean()) {
-          BytesRefUtils.copyInt(ref, random.nextInt());
-          values[i] = BytesRefUtils.asInt(ref);
-        } else {
-          BytesRefUtils.copyLong(ref, random.nextLong());
-          values[i] = BytesRefUtils.asLong(ref);
-        }
-        valField.setBytes(ref, valueType);
-        break;
-
-      default:
-        fail("unexpected value " + valueType);
-
-      }
-      doc.add(valField);
-      writer.addDocument(doc);
-      if (random.nextInt(10) == 0) {
-        writer.commit();
-      }
-    }
-  }
-
-  public void testPromoteBytes() throws IOException {
-    runTest(UNSORTED_BYTES, TestType.Byte);
-  }
-  
-  public void testSortedPromoteBytes() throws IOException {
-    runTest(SORTED_BYTES, TestType.Byte);
-  }
-
-  public void testPromotInteger() throws IOException {
-    runTest(INTEGERS, TestType.Int);
-  }
-
-  public void testPromotFloatingPoint() throws CorruptIndexException,
-      IOException {
-    runTest(FLOATS, TestType.Float);
-  }
-  
-  public void testMergeIncompatibleTypes() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriterConfig writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));
-    writerConfig.setMergePolicy(NoMergePolicy.NO_COMPOUND_FILES); // no merges until we are done with adding values
-    IndexWriter writer = new IndexWriter(dir, writerConfig);
-    int num_1 = atLeast(200);
-    int num_2 = atLeast(200);
-    long[] values = new long[num_1 + num_2];
-    index(writer, new DocValuesField("promote"),
-        randomValueType(INTEGERS, random), values, 0, num_1);
-    writer.commit();
-    
-    if (random.nextInt(4) == 0) {
-      // once in a while use addIndexes
-      Directory dir_2 = newDirectory() ;
-      IndexWriter writer_2 = new IndexWriter(dir_2,
-          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)));
-      index(writer_2, new DocValuesField("promote"),
-          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);
-      writer_2.commit();
-      writer_2.close();
-      if (random.nextBoolean()) {
-        writer.addIndexes(dir_2);
-      } else {
-        // do a real merge here
-        IndexReader open = IndexReader.open(dir_2);
-        writer.addIndexes(open);
-        open.close();
-      }
-      dir_2.close();
-    } else {
-      index(writer, new DocValuesField("promote"),
-          randomValueType(random.nextBoolean() ? UNSORTED_BYTES : SORTED_BYTES, random), values, num_1, num_2);
-      writer.commit();
-    }
-    writer.close();
-    writerConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));
-    if (writerConfig.getMergePolicy() instanceof NoMergePolicy) {
-      writerConfig.setMergePolicy(newLogMergePolicy()); // make sure we merge to one segment (merge everything together)
-    }
-    writer = new IndexWriter(dir, writerConfig);
-    // now merge
-    writer.forceMerge(1);
-    writer.close();
-    IndexReader reader = IndexReader.open(dir);
-    assertEquals(1, reader.getSequentialSubReaders().length);
-    ReaderContext topReaderContext = reader.getTopReaderContext();
-    ReaderContext[] children = topReaderContext.children();
-    DocValues docValues = children[0].reader.docValues("promote");
-    assertNotNull(docValues);
-    assertValues(TestType.Byte, dir, values);
-    assertEquals(Type.BYTES_VAR_STRAIGHT, docValues.type());
-    reader.close();
-    dir.close();
-  }
-
-}
\ No newline at end of file
diff --git a/lucene/src/test/org/apache/lucene/search/TestDocValuesScoring.java b/lucene/src/test/org/apache/lucene/search/TestDocValuesScoring.java
index 49fddbe..03dfef0 100644
--- a/lucene/src/test/org/apache/lucene/search/TestDocValuesScoring.java
+++ b/lucene/src/test/org/apache/lucene/search/TestDocValuesScoring.java
@@ -19,6 +19,7 @@ package org.apache.lucene.search;
 
 import java.io.IOException;
 
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.DocValuesField;
@@ -29,7 +30,6 @@ import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.DocValues.Source;
 import org.apache.lucene.index.IndexReader.AtomicReaderContext;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.similarities.DefaultSimilarityProvider;
 import org.apache.lucene.search.similarities.Similarity;
 import org.apache.lucene.search.similarities.SimilarityProvider;
diff --git a/lucene/src/test/org/apache/lucene/search/TestPrefixRandom.java b/lucene/src/test/org/apache/lucene/search/TestPrefixRandom.java
index d55b2d7..629a9d9 100644
--- a/lucene/src/test/org/apache/lucene/search/TestPrefixRandom.java
+++ b/lucene/src/test/org/apache/lucene/search/TestPrefixRandom.java
@@ -21,6 +21,7 @@ import java.io.IOException;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
@@ -30,7 +31,6 @@ import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.BytesRef;
diff --git a/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java b/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
index a8aa421..44a7801 100644
--- a/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
+++ b/lucene/src/test/org/apache/lucene/search/TestRegexpRandom2.java
@@ -24,6 +24,7 @@ import java.util.List;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
@@ -33,7 +34,6 @@ import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.BytesRef;
diff --git a/lucene/src/test/org/apache/lucene/search/TestSort.java b/lucene/src/test/org/apache/lucene/search/TestSort.java
index 32a2a6a..ce1a5d5 100644
--- a/lucene/src/test/org/apache/lucene/search/TestSort.java
+++ b/lucene/src/test/org/apache/lucene/search/TestSort.java
@@ -25,6 +25,7 @@ import java.util.concurrent.Executors;
 import java.util.concurrent.TimeUnit;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -41,7 +42,6 @@ import org.apache.lucene.index.MultiReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.SlowMultiReaderWrapper;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.index.DocValues;
 import org.apache.lucene.search.BooleanClause.Occur;
 import org.apache.lucene.search.FieldValueHitQueue.Entry;
diff --git a/lucene/src/test/org/apache/lucene/search/similarities/TestSimilarityBase.java b/lucene/src/test/org/apache/lucene/search/similarities/TestSimilarityBase.java
index d3fc279..09d7b0b 100644
--- a/lucene/src/test/org/apache/lucene/search/similarities/TestSimilarityBase.java
+++ b/lucene/src/test/org/apache/lucene/search/similarities/TestSimilarityBase.java
@@ -21,6 +21,7 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
@@ -29,7 +30,6 @@ import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.OrdTermState;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.CollectionStatistics;
 import org.apache.lucene.search.Explanation;
 import org.apache.lucene.search.IndexSearcher;
diff --git a/lucene/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java b/lucene/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
index 998c5b7..0e82e45 100644
--- a/lucene/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
+++ b/lucene/src/test/org/apache/lucene/store/TestFileSwitchDirectory.java
@@ -24,12 +24,12 @@ import java.util.HashSet;
 import java.util.Set;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.TestIndexWriterReader;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.lucene40.Lucene40StoredFieldsWriter;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util._TestUtil;
 
diff --git a/lucene/src/test/org/apache/lucene/util/TestNamedSPILoader.java b/lucene/src/test/org/apache/lucene/util/TestNamedSPILoader.java
index 60c2a49..aea0e82 100644
--- a/lucene/src/test/org/apache/lucene/util/TestNamedSPILoader.java
+++ b/lucene/src/test/org/apache/lucene/util/TestNamedSPILoader.java
@@ -2,7 +2,7 @@ package org.apache.lucene.util;
 
 import java.util.Set;
 
-import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.codecs.Codec;
 
 /**
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/src/test/org/apache/lucene/util/fst/TestFSTs.java b/lucene/src/test/org/apache/lucene/util/fst/TestFSTs.java
index 451edb0..5f38b52 100644
--- a/lucene/src/test/org/apache/lucene/util/fst/TestFSTs.java
+++ b/lucene/src/test/org/apache/lucene/util/fst/TestFSTs.java
@@ -29,6 +29,8 @@ import java.io.Writer;
 import java.util.*;
 
 import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.StringField;
@@ -40,8 +42,6 @@ import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.index.Term;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.lucene40.Lucene40PostingsFormat;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
diff --git a/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java b/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
index 00c035b..94b9b1f 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
@@ -20,7 +20,7 @@ package org.apache.lucene.collation;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CollationTestBase;
-import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.util.BytesRef;
 
 import java.text.Collator;
diff --git a/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java b/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java
index 681290a..198b043 100644
--- a/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java
+++ b/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java
@@ -22,7 +22,7 @@ import com.ibm.icu.text.Collator;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.CollationTestBase;
-import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.util.BytesRef;
 
 import java.util.Locale;
diff --git a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
index 9351875..887330f 100644
--- a/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
+++ b/modules/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
@@ -19,6 +19,7 @@ package org.apache.lucene.benchmark.byTask.tasks;
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
 import org.apache.lucene.benchmark.byTask.utils.Config;
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.CorruptIndexException;
 import org.apache.lucene.index.IndexCommit;
 import org.apache.lucene.index.IndexDeletionPolicy;
@@ -33,7 +34,6 @@ import org.apache.lucene.index.NoDeletionPolicy;
 import org.apache.lucene.index.NoMergePolicy;
 import org.apache.lucene.index.NoMergeScheduler;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.util.Version;
 
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationField.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationField.java
index 8d2ebbe..88a44e6 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationField.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationField.java
@@ -22,7 +22,7 @@ import java.io.FileOutputStream;
 
 import org.apache.commons.io.FileUtils;
 import org.apache.commons.io.IOUtils;
-import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.codecs.Codec;
 import org.apache.solr.SolrTestCaseJ4;
 import org.junit.BeforeClass;
 
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationFieldOptions.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationFieldOptions.java
index d96e3c1..78ce18a 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationFieldOptions.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationFieldOptions.java
@@ -17,7 +17,7 @@ package org.apache.solr.schema;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.codecs.Codec;
 import org.apache.solr.SolrTestCaseJ4;
 import org.junit.BeforeClass;
 
diff --git a/solr/core/src/java/org/apache/solr/core/CodecFactory.java b/solr/core/src/java/org/apache/solr/core/CodecFactory.java
index e047181..68b9089 100644
--- a/solr/core/src/java/org/apache/solr/core/CodecFactory.java
+++ b/solr/core/src/java/org/apache/solr/core/CodecFactory.java
@@ -17,7 +17,7 @@ package org.apache.solr.core;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.codecs.Codec;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.schema.IndexSchema;
 import org.apache.solr.util.plugin.NamedListInitializedPlugin;
diff --git a/solr/core/src/java/org/apache/solr/core/DefaultCodecFactory.java b/solr/core/src/java/org/apache/solr/core/DefaultCodecFactory.java
index 602bfca..432d63f 100644
--- a/solr/core/src/java/org/apache/solr/core/DefaultCodecFactory.java
+++ b/solr/core/src/java/org/apache/solr/core/DefaultCodecFactory.java
@@ -17,9 +17,9 @@ package org.apache.solr.core;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.PostingsFormat;
-import org.apache.lucene.index.codecs.lucene40.Lucene40Codec;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40Codec;
 import org.apache.solr.schema.IndexSchema;
 import org.apache.solr.schema.SchemaField;
 
diff --git a/solr/core/src/java/org/apache/solr/core/SolrCore.java b/solr/core/src/java/org/apache/solr/core/SolrCore.java
index 812e8ef..910f707 100644
--- a/solr/core/src/java/org/apache/solr/core/SolrCore.java
+++ b/solr/core/src/java/org/apache/solr/core/SolrCore.java
@@ -17,10 +17,10 @@
 
 package org.apache.solr.core;
 
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.IndexDeletionPolicy;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.BooleanQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.solr.common.SolrException;
diff --git a/solr/core/src/java/org/apache/solr/update/SolrIndexWriter.java b/solr/core/src/java/org/apache/solr/update/SolrIndexWriter.java
index 3217307..c1aa1e5 100644
--- a/solr/core/src/java/org/apache/solr/update/SolrIndexWriter.java
+++ b/solr/core/src/java/org/apache/solr/update/SolrIndexWriter.java
@@ -26,10 +26,10 @@ import java.text.DateFormat;
 import java.util.Date;
 import java.util.concurrent.atomic.AtomicLong;
 
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.IndexDeletionPolicy;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.InfoStream;
 import org.apache.lucene.util.PrintStreamInfoStream;
diff --git a/solr/core/src/test/org/apache/solr/core/TestCodecSupport.java b/solr/core/src/test/org/apache/solr/core/TestCodecSupport.java
index 459d765..75860bb 100644
--- a/solr/core/src/test/org/apache/solr/core/TestCodecSupport.java
+++ b/solr/core/src/test/org/apache/solr/core/TestCodecSupport.java
@@ -19,8 +19,8 @@ package org.apache.solr.core;
 
 import java.util.Map;
 
-import org.apache.lucene.index.codecs.Codec;
-import org.apache.lucene.index.codecs.perfield.PerFieldPostingsFormat;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 import org.apache.solr.SolrTestCaseJ4;
 import org.apache.solr.schema.SchemaField;
 import org.junit.BeforeClass;
diff --git a/solr/core/src/test/org/apache/solr/schema/TestCollationField.java b/solr/core/src/test/org/apache/solr/schema/TestCollationField.java
index 175b0f8..7bdda60 100644
--- a/solr/core/src/test/org/apache/solr/schema/TestCollationField.java
+++ b/solr/core/src/test/org/apache/solr/schema/TestCollationField.java
@@ -25,7 +25,7 @@ import java.util.Locale;
 
 import org.apache.commons.io.FileUtils;
 import org.apache.commons.io.IOUtils;
-import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.codecs.Codec;
 import org.apache.solr.SolrTestCaseJ4;
 import org.junit.BeforeClass;
 
diff --git a/solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java b/solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java
index fff65ec..41cb7a1 100755
--- a/solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java
+++ b/solr/core/src/test/org/apache/solr/search/function/TestFunctionQuery.java
@@ -17,8 +17,8 @@
 
 package org.apache.solr.search.function;
 
+import org.apache.lucene.codecs.Codec;
 import org.apache.lucene.index.FieldInvertState;
-import org.apache.lucene.index.codecs.Codec;
 import org.apache.lucene.search.FieldCache;
 import org.apache.lucene.search.similarities.DefaultSimilarity;
 import org.apache.lucene.search.similarities.TFIDFSimilarity;

