GitDiffStart: 36b65637fcb2614619a7930159477364e57b3738 | Wed Oct 28 17:49:53 2009 +0000
diff --git a/contrib/CHANGES.txt b/contrib/CHANGES.txt
index d0ad850..3474a3e 100644
--- a/contrib/CHANGES.txt
+++ b/contrib/CHANGES.txt
@@ -89,6 +89,9 @@ Documentation
 
 Build
 
+ * LUCENE-1904: Moved wordnet-based synonym support from contrib/memory
+   into contrib/wordnet.  (Robert Muir)
+
 Test Cases
 ======================= Release 2.9.0 2009-09-23 =======================
 
diff --git a/contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil.java b/contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil.java
deleted file mode 100644
index d3a5810..0000000
--- a/contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil.java
+++ /dev/null
@@ -1,456 +0,0 @@
-package org.apache.lucene.index.memory;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.PrintStream;
-import java.io.Reader;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.regex.Pattern;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.PorterStemFilter;
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.util.AttributeSource;
-
-/**
- * Various fulltext analysis utilities avoiding redundant code in several
- * classes.
- *
- */
-public class AnalyzerUtil {
-  
-  private AnalyzerUtil() {};
-
-  /**
-   * Returns a simple analyzer wrapper that logs all tokens produced by the
-   * underlying child analyzer to the given log stream (typically System.err);
-   * Otherwise behaves exactly like the child analyzer, delivering the very
-   * same tokens; useful for debugging purposes on custom indexing and/or
-   * querying.
-   * 
-   * @param child
-   *            the underlying child analyzer
-   * @param log
-   *            the print stream to log to (typically System.err)
-   * @param logName
-   *            a name for this logger (typically "log" or similar)
-   * @return a logging analyzer
-   */
-  public static Analyzer getLoggingAnalyzer(final Analyzer child, 
-      final PrintStream log, final String logName) {
-    
-    if (child == null) 
-      throw new IllegalArgumentException("child analyzer must not be null");
-    if (log == null) 
-      throw new IllegalArgumentException("logStream must not be null");
-
-    return new Analyzer() {
-      public TokenStream tokenStream(final String fieldName, Reader reader) {
-        return new TokenFilter(child.tokenStream(fieldName, reader)) {
-          private int position = -1;
-          private TermAttribute termAtt = addAttribute(TermAttribute.class);
-          private PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-          private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-          private TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
-         
-          public boolean incrementToken() throws IOException {
-            boolean hasNext = input.incrementToken();
-            log.println(toString(hasNext));
-            return hasNext;
-          }
-          
-          private String toString(boolean hasNext) {
-            if (!hasNext) return "[" + logName + ":EOS:" + fieldName + "]\n";
-            
-            position += posIncrAtt.getPositionIncrement();
-            return "[" + logName + ":" + position + ":" + fieldName + ":"
-                + termAtt.term() + ":" + offsetAtt.startOffset()
-                + "-" + offsetAtt.endOffset() + ":" + typeAtt.type()
-                + "]";
-          }         
-        };
-      }
-    };
-  }
-  
-  
-  /**
-   * Returns an analyzer wrapper that returns at most the first
-   * <code>maxTokens</code> tokens from the underlying child analyzer,
-   * ignoring all remaining tokens.
-   * 
-   * @param child
-   *            the underlying child analyzer
-   * @param maxTokens
-   *            the maximum number of tokens to return from the underlying
-   *            analyzer (a value of Integer.MAX_VALUE indicates unlimited)
-   * @return an analyzer wrapper
-   */
-  public static Analyzer getMaxTokenAnalyzer(
-      final Analyzer child, final int maxTokens) {
-    
-    if (child == null) 
-      throw new IllegalArgumentException("child analyzer must not be null");
-    if (maxTokens < 0) 
-      throw new IllegalArgumentException("maxTokens must not be negative");
-    if (maxTokens == Integer.MAX_VALUE) 
-      return child; // no need to wrap
-  
-    return new Analyzer() {
-      public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new TokenFilter(child.tokenStream(fieldName, reader)) {
-          private int todo = maxTokens;
-          
-          public boolean incrementToken() throws IOException {
-            return --todo >= 0 ? input.incrementToken() : false;
-          }
-        };
-      }
-    };
-  }
-  
-  
-  /**
-   * Returns an English stemming analyzer that stems tokens from the
-   * underlying child analyzer according to the Porter stemming algorithm. The
-   * child analyzer must deliver tokens in lower case for the stemmer to work
-   * properly.
-   * <p>
-   * Background: Stemming reduces token terms to their linguistic root form
-   * e.g. reduces "fishing" and "fishes" to "fish", "family" and "families" to
-   * "famili", as well as "complete" and "completion" to "complet". Note that
-   * the root form is not necessarily a meaningful word in itself, and that
-   * this is not a bug but rather a feature, if you lean back and think about
-   * fuzzy word matching for a bit.
-   * <p>
-   * See the Lucene contrib packages for stemmers (and stop words) for German,
-   * Russian and many more languages.
-   * 
-   * @param child
-   *            the underlying child analyzer
-   * @return an analyzer wrapper
-   */
-  public static Analyzer getPorterStemmerAnalyzer(final Analyzer child) {
-    
-    if (child == null) 
-      throw new IllegalArgumentException("child analyzer must not be null");
-  
-    return new Analyzer() {
-      public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new PorterStemFilter(
-            child.tokenStream(fieldName, reader));
-//        /* PorterStemFilter and SnowballFilter have the same behaviour, 
-//        but PorterStemFilter is much faster. */
-//        return new org.apache.lucene.analysis.snowball.SnowballFilter(
-//            child.tokenStream(fieldName, reader), "English");
-      }
-    };
-  }
-  
-  
-  /**
-   * Returns an analyzer wrapper that wraps the underlying child analyzer's
-   * token stream into a {@link SynonymTokenFilter}.
-   * 
-   * @param child
-   *            the underlying child analyzer
-   * @param synonyms
-   *            the map used to extract synonyms for terms
-   * @param maxSynonyms
-   *            the maximum number of synonym tokens to return per underlying
-   *            token word (a value of Integer.MAX_VALUE indicates unlimited)
-   * @return a new analyzer
-   */
-  public static Analyzer getSynonymAnalyzer(final Analyzer child, 
-      final SynonymMap synonyms, final int maxSynonyms) {
-    
-    if (child == null) 
-      throw new IllegalArgumentException("child analyzer must not be null");
-    if (synonyms == null)
-      throw new IllegalArgumentException("synonyms must not be null");
-    if (maxSynonyms < 0) 
-      throw new IllegalArgumentException("maxSynonyms must not be negative");
-    if (maxSynonyms == 0)
-      return child; // no need to wrap
-  
-    return new Analyzer() {
-      public TokenStream tokenStream(String fieldName, Reader reader) {
-        return new SynonymTokenFilter(
-          child.tokenStream(fieldName, reader), synonyms, maxSynonyms);
-      }
-    };
-  }
-
-  
-  /**
-   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's
-   * token streams, and delivers those cached tokens on subsequent calls to 
-   * <code>tokenStream(String fieldName, Reader reader)</code> 
-   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.
-   * <p>
-   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can 
-   * help improve performance if the same document is added to multiple Lucene indexes, 
-   * because the text analysis phase need not be performed more than once.
-   * <p>
-   * Caveats: 
-   * <ul>
-   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> 
-   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>
-   * <li>The same caching analyzer instance must not be used for more than one document
-   * because the cache is not keyed on the Reader parameter.</li>
-   * </ul>
-   * 
-   * @param child
-   *            the underlying child analyzer
-   * @return a new analyzer
-   */
-  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {
-
-    if (child == null)
-      throw new IllegalArgumentException("child analyzer must not be null");
-
-    return new Analyzer() {
-
-      private final HashMap cache = new HashMap();
-
-      public TokenStream tokenStream(String fieldName, Reader reader) {
-        final ArrayList tokens = (ArrayList) cache.get(fieldName);
-        if (tokens == null) { // not yet cached
-          final ArrayList tokens2 = new ArrayList();
-          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {
-
-            public boolean incrementToken() throws IOException {
-              boolean hasNext = input.incrementToken();
-              if (hasNext) tokens2.add(captureState());
-              return hasNext;
-            }
-          };
-          
-          cache.put(fieldName, tokens2);
-          return tokenStream;
-        } else { // already cached
-          return new TokenStream() {
-
-            private Iterator iter = tokens.iterator();
-
-            public boolean incrementToken() {
-              if (!iter.hasNext()) return false;
-              restoreState((AttributeSource.State) iter.next());
-              return true;
-            }
-          };
-        }
-      }
-    };
-  }
-      
-  
-  /**
-   * Returns (frequency:term) pairs for the top N distinct terms (aka words),
-   * sorted descending by frequency (and ascending by term, if tied).
-   * <p>
-   * Example XQuery:
-   * <pre>
-   * declare namespace util = "java:org.apache.lucene.index.memory.AnalyzerUtil";
-   * declare namespace analyzer = "java:org.apache.lucene.index.memory.PatternAnalyzer";
-   * 
-   * for $pair in util:get-most-frequent-terms(
-   *    analyzer:EXTENDED_ANALYZER(), doc("samples/shakespeare/othello.xml"), 10)
-   * return &lt;word word="{substring-after($pair, ':')}" frequency="{substring-before($pair, ':')}"/>
-   * </pre>
-   * 
-   * @param analyzer
-   *            the analyzer to use for splitting text into terms (aka words)
-   * @param text
-   *            the text to analyze
-   * @param limit
-   *            the maximum number of pairs to return; zero indicates 
-   *            "as many as possible".
-   * @return an array of (frequency:term) pairs in the form of (freq0:term0,
-   *         freq1:term1, ..., freqN:termN). Each pair is a single string
-   *         separated by a ':' delimiter.
-   */
-  public static String[] getMostFrequentTerms(Analyzer analyzer, String text, int limit) {
-    if (analyzer == null) 
-      throw new IllegalArgumentException("analyzer must not be null");
-    if (text == null) 
-      throw new IllegalArgumentException("text must not be null");
-    if (limit <= 0) limit = Integer.MAX_VALUE;
-    
-    // compute frequencies of distinct terms
-    HashMap map = new HashMap();
-    TokenStream stream = analyzer.tokenStream("", new StringReader(text));
-    TermAttribute termAtt = stream.addAttribute(TermAttribute.class);
-    try {
-      while (stream.incrementToken()) {
-        MutableInteger freq = (MutableInteger) map.get(termAtt.term());
-        if (freq == null) {
-          freq = new MutableInteger(1);
-          map.put(termAtt.term(), freq);
-        } else {
-          freq.setValue(freq.intValue() + 1);
-        }
-      }
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    } finally {
-      try {
-        stream.close();
-      } catch (IOException e2) {
-        throw new RuntimeException(e2);
-      }
-    }
-    
-    // sort by frequency, text
-    Map.Entry[] entries = new Map.Entry[map.size()];
-    map.entrySet().toArray(entries);
-    Arrays.sort(entries, new Comparator() {
-      public int compare(Object o1, Object o2) {
-        Map.Entry e1 = (Map.Entry) o1;
-        Map.Entry e2 = (Map.Entry) o2;
-        int f1 = ((MutableInteger) e1.getValue()).intValue();
-        int f2 = ((MutableInteger) e2.getValue()).intValue();
-        if (f2 - f1 != 0) return f2 - f1;
-        String s1 = (String) e1.getKey();
-        String s2 = (String) e2.getKey();
-        return s1.compareTo(s2);
-      }
-    });
-    
-    // return top N entries
-    int size = Math.min(limit, entries.length);
-    String[] pairs = new String[size];
-    for (int i=0; i < size; i++) {
-      pairs[i] = entries[i].getValue() + ":" + entries[i].getKey();
-    }
-    return pairs;
-  }
-  
-  private static final class MutableInteger {
-    private int value;
-    public MutableInteger(int value) { this.value = value; }
-    public int intValue() { return value; }
-    public void setValue(int value) { this.value = value; }
-    public String toString() { return String.valueOf(value); }
-  };
-  
-  
-  
-  // TODO: could use a more general i18n approach ala http://icu.sourceforge.net/docs/papers/text_boundary_analysis_in_java/
-  /** (Line terminator followed by zero or more whitespace) two or more times */
-  private static final Pattern PARAGRAPHS = Pattern.compile("([\\r\\n\\u0085\\u2028\\u2029][ \\t\\x0B\\f]*){2,}");
-  
-  /**
-   * Returns at most the first N paragraphs of the given text. Delimiting
-   * characters are excluded from the results. Each returned paragraph is
-   * whitespace-trimmed via String.trim(), potentially an empty string.
-   * 
-   * @param text
-   *            the text to tokenize into paragraphs
-   * @param limit
-   *            the maximum number of paragraphs to return; zero indicates "as
-   *            many as possible".
-   * @return the first N paragraphs
-   */
-  public static String[] getParagraphs(String text, int limit) {
-    return tokenize(PARAGRAPHS, text, limit);
-  }
-    
-  private static String[] tokenize(Pattern pattern, String text, int limit) {
-    String[] tokens = pattern.split(text, limit);
-    for (int i=tokens.length; --i >= 0; ) tokens[i] = tokens[i].trim();
-    return tokens;
-  }
-  
-  
-  // TODO: don't split on floating point numbers, e.g. 3.1415 (digit before or after '.')
-  /** Divides text into sentences; Includes inverted spanish exclamation and question mark */
-  private static final Pattern SENTENCES  = Pattern.compile("[!\\.\\?\\xA1\\xBF]+");
-
-  /**
-   * Returns at most the first N sentences of the given text. Delimiting
-   * characters are excluded from the results. Each returned sentence is
-   * whitespace-trimmed via String.trim(), potentially an empty string.
-   * 
-   * @param text
-   *            the text to tokenize into sentences
-   * @param limit
-   *            the maximum number of sentences to return; zero indicates "as
-   *            many as possible".
-   * @return the first N sentences
-   */
-  public static String[] getSentences(String text, int limit) {
-//    return tokenize(SENTENCES, text, limit); // equivalent but slower
-    int len = text.length();
-    if (len == 0) return new String[] { text };
-    if (limit <= 0) limit = Integer.MAX_VALUE;
-    
-    // average sentence length heuristic
-    String[] tokens = new String[Math.min(limit, 1 + len/40)];
-    int size = 0;
-    int i = 0;
-    
-    while (i < len && size < limit) {
-      
-      // scan to end of current sentence
-      int start = i;
-      while (i < len && !isSentenceSeparator(text.charAt(i))) i++;
-      
-      if (size == tokens.length) { // grow array
-        String[] tmp = new String[tokens.length << 1];
-        System.arraycopy(tokens, 0, tmp, 0, size);
-        tokens = tmp;
-      }
-      // add sentence (potentially empty)
-      tokens[size++] = text.substring(start, i).trim();
-
-      // scan to beginning of next sentence
-      while (i < len && isSentenceSeparator(text.charAt(i))) i++;
-    }
-    
-    if (size == tokens.length) return tokens;
-    String[] results = new String[size];
-    System.arraycopy(tokens, 0, results, 0, size);
-    return results;
-  }
-
-  private static boolean isSentenceSeparator(char c) {
-    // regex [!\\.\\?\\xA1\\xBF]
-    switch (c) {
-      case '!': return true;
-      case '.': return true;
-      case '?': return true;
-      case 0xA1: return true; // spanish inverted exclamation mark
-      case 0xBF: return true; // spanish inverted question mark
-      default: return false;
-    }   
-  }
-  
-}
diff --git a/contrib/memory/src/java/org/apache/lucene/index/memory/SynonymMap.java b/contrib/memory/src/java/org/apache/lucene/index/memory/SynonymMap.java
deleted file mode 100644
index d5dfdb2..0000000
--- a/contrib/memory/src/java/org/apache/lucene/index/memory/SynonymMap.java
+++ /dev/null
@@ -1,399 +0,0 @@
-package org.apache.lucene.index.memory;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.ByteBuffer;
-import java.nio.charset.Charset;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.TreeMap;
-import java.util.TreeSet;
-
-/**
- * Loads the <a target="_blank" 
- * href="http://www.cogsci.princeton.edu/~wn/">WordNet </a> prolog file <a
- * href="http://www.cogsci.princeton.edu/2.0/WNprolog-2.0.tar.gz">wn_s.pl </a>
- * into a thread-safe main-memory hash map that can be used for fast
- * high-frequency lookups of synonyms for any given (lowercase) word string.
- * <p>
- * There holds: If B is a synonym for A (A -> B) then A is also a synonym for B (B -> A).
- * There does not necessarily hold: A -> B, B -> C then A -> C.
- * <p>
- * Loading typically takes some 1.5 secs, so should be done only once per
- * (server) program execution, using a singleton pattern. Once loaded, a
- * synonym lookup via {@link #getSynonyms(String)}takes constant time O(1).
- * A loaded default synonym map consumes about 10 MB main memory.
- * An instance is immutable, hence thread-safe.
- * <p>
- * This implementation borrows some ideas from the Lucene Syns2Index demo that 
- * Dave Spencer originally contributed to Lucene. Dave's approach
- * involved a persistent Lucene index which is suitable for occasional
- * lookups or very large synonym tables, but considered unsuitable for 
- * high-frequency lookups of medium size synonym tables.
- * <p>
- * Example Usage:
- * <pre>
- * String[] words = new String[] { "hard", "woods", "forest", "wolfish", "xxxx"};
- * SynonymMap map = new SynonymMap(new FileInputStream("samples/fulltext/wn_s.pl"));
- * for (int i = 0; i &lt; words.length; i++) {
- *     String[] synonyms = map.getSynonyms(words[i]);
- *     System.out.println(words[i] + ":" + java.util.Arrays.asList(synonyms).toString());
- * }
- * 
- * Example output:
- * hard:[arduous, backbreaking, difficult, fermented, firmly, grueling, gruelling, heavily, heavy, intemperately, knockout, laborious, punishing, severe, severely, strong, toilsome, tough]
- * woods:[forest, wood]
- * forest:[afforest, timber, timberland, wood, woodland, woods]
- * wolfish:[edacious, esurient, rapacious, ravening, ravenous, voracious, wolflike]
- * xxxx:[]
- * </pre>
- *
- * @see <a target="_blank"
- *      href="http://www.cogsci.princeton.edu/~wn/man/prologdb.5WN.html">prologdb
- *      man page </a>
- * @see <a target="_blank" href="http://www.hostmon.com/rfc/advanced.jsp">Dave's synonym demo site</a>
- */
-public class SynonymMap {
-
-  /** the index data; Map<String word, String[] synonyms> */
-  private final HashMap table;
-  
-  private static final String[] EMPTY = new String[0];
-  
-  private static final boolean DEBUG = false;
-
-  /**
-   * Constructs an instance, loading WordNet synonym data from the given input
-   * stream. Finally closes the stream. The words in the stream must be in
-   * UTF-8 or a compatible subset (for example ASCII, MacRoman, etc.).
-   * 
-   * @param input
-   *            the stream to read from (null indicates an empty synonym map)
-   * @throws IOException
-   *             if an error occured while reading the stream.
-   */
-  public SynonymMap(InputStream input) throws IOException {
-    this.table = input == null ? new HashMap(0) : read(toByteArray(input));
-  }
-  
-  /**
-   * Returns the synonym set for the given word, sorted ascending.
-   * 
-   * @param word
-   *            the word to lookup (must be in lowercase).
-   * @return the synonyms; a set of zero or more words, sorted ascending, each
-   *         word containing lowercase characters that satisfy
-   *         <code>Character.isLetter()</code>.
-   */
-  public String[] getSynonyms(String word) {
-    Object syns = table.get(word);
-    if (syns == null) return EMPTY;
-    if (syns instanceof String) return new String[] {(String) syns};
-    
-    String[] synonyms = (String[]) syns;
-    String[] copy = new String[synonyms.length]; // copy for guaranteed immutability
-    System.arraycopy(synonyms, 0, copy, 0, synonyms.length);
-    return copy;
-  }
-  
-  /**
-   * Returns a String representation of the index data for debugging purposes.
-   * 
-   * @return a String representation
-   */
-  public String toString() {
-    StringBuilder buf = new StringBuilder();
-    Iterator iter = new TreeMap(table).keySet().iterator();
-    int count = 0;
-    int f0 = 0;
-    int f1 = 0;
-    int f2 = 0;
-    int f3 = 0;
-    
-    while (iter.hasNext()) {
-      String word = (String) iter.next();
-      buf.append(word + ":");
-      String[] synonyms = getSynonyms(word);
-      buf.append(Arrays.asList(synonyms));
-      buf.append("\n");
-      count += synonyms.length;
-      if (synonyms.length == 0) f0++;
-      if (synonyms.length == 1) f1++;
-      if (synonyms.length == 2) f2++;
-      if (synonyms.length == 3) f3++;
-    }
-    
-    buf.append("\n\nkeys=" + table.size() + ", synonyms=" + count + ", f0=" + f0 +", f1=" + f1 + ", f2=" + f2 + ", f3=" + f3);
-    return buf.toString();
-  }
-  
-  /**
-   * Analyzes/transforms the given word on input stream loading. This default implementation simply
-   * lowercases the word. Override this method with a custom stemming
-   * algorithm or similar, if desired.
-   * 
-   * @param word
-   *            the word to analyze
-   * @return the same word, or a different word (or null to indicate that the
-   *         word should be ignored)
-   */
-  protected String analyze(String word) {
-    return word.toLowerCase();
-  }
-
-  private static boolean isValid(String str) {
-    for (int i=str.length(); --i >= 0; ) {
-      if (!Character.isLetter(str.charAt(i))) return false;
-    }
-    return true;
-  }
-
-  private HashMap read(byte[] data) {
-    int WORDS  = (int) (76401 / 0.7); // presizing
-    int GROUPS = (int) (88022 / 0.7); // presizing
-    HashMap word2Groups = new HashMap(WORDS);  // Map<String word, int[] groups>
-    HashMap group2Words = new HashMap(GROUPS); // Map<int group, String[] words>
-    HashMap internedWords = new HashMap(WORDS);// Map<String word, String word>
-
-    Charset charset = Charset.forName("UTF-8");
-    int lastNum = -1;
-    Integer lastGroup = null;
-    int len = data.length;
-    int i=0;
-    
-    while (i < len) { // until EOF
-      /* Part A: Parse a line */
-      
-      // scan to beginning of group
-      while (i < len && data[i] != '(') i++;
-      if (i >= len) break; // EOF
-      i++;
-      
-      // parse group
-      int num = 0;
-      while (i < len && data[i] != ',') {
-        num = 10*num + (data[i] - 48);
-        i++;
-      }
-      i++;
-//      if (DEBUG) System.err.println("num="+ num);
-      
-      // scan to beginning of word
-      while (i < len && data[i] != '\'') i++;
-      i++;
-  
-      // scan to end of word
-      int start = i;
-      do {
-        while (i < len && data[i] != '\'') i++;
-        i++;
-      } while (i < len && data[i] != ','); // word must end with "',"
-      
-      if (i >= len) break; // EOF
-      String word = charset.decode(ByteBuffer.wrap(data, start, i-start-1)).toString();
-//      String word = new String(data, 0, start, i-start-1); // ASCII
-      
-      /*
-       * Part B: ignore phrases (with spaces and hyphens) and
-       * non-alphabetic words, and let user customize word (e.g. do some
-       * stemming)
-       */
-      if (!isValid(word)) continue; // ignore
-      word = analyze(word);
-      if (word == null || word.length() == 0) continue; // ignore
-      
-      
-      /* Part C: Add (group,word) to tables */
-      
-      // ensure compact string representation, minimizing memory overhead
-      String w = (String) internedWords.get(word);
-      if (w == null) {
-        word = new String(word); // ensure compact string
-        internedWords.put(word, word);
-      } else {
-        word = w;
-      }
-      
-      Integer group = lastGroup;
-      if (num != lastNum) {
-        group = Integer.valueOf(num);
-        lastGroup = group;
-        lastNum = num;
-      }
-      
-      // add word --> group
-      ArrayList groups = (ArrayList) word2Groups.get(word);
-      if (groups == null) {
-        groups = new ArrayList(1);
-        word2Groups.put(word, groups);
-      }
-      groups.add(group);
-
-      // add group --> word
-      ArrayList words = (ArrayList) group2Words.get(group);
-      if (words == null) {
-        words = new ArrayList(1);
-        group2Words.put(group, words);
-      } 
-      words.add(word);
-    }
-    
-    
-    /* Part D: compute index data structure */
-    HashMap word2Syns = createIndex(word2Groups, group2Words);    
-        
-    /* Part E: minimize memory consumption by a factor 3 (or so) */
-//    if (true) return word2Syns;
-    word2Groups = null; // help gc
-    group2Words = null; // help gc    
-    return optimize(word2Syns, internedWords);
-  }
-  
-  private HashMap createIndex(Map word2Groups, Map group2Words) {
-    HashMap word2Syns = new HashMap();
-    Iterator iter = word2Groups.entrySet().iterator();
-    
-    while (iter.hasNext()) { // for each word
-      Map.Entry entry = (Map.Entry) iter.next();
-      ArrayList group = (ArrayList) entry.getValue();     
-      String word = (String) entry.getKey();
-      
-//      HashSet synonyms = new HashSet();
-      TreeSet synonyms = new TreeSet();
-      for (int i=group.size(); --i >= 0; ) { // for each groupID of word
-        ArrayList words = (ArrayList) group2Words.get(group.get(i));
-        for (int j=words.size(); --j >= 0; ) { // add all words       
-          Object synonym = words.get(j); // note that w and word are interned
-          if (synonym != word) { // a word is implicitly it's own synonym
-            synonyms.add(synonym);
-          }
-        }
-      }
-
-      int size = synonyms.size();
-      if (size > 0) {
-        String[] syns = new String[size];
-        if (size == 1)  
-          syns[0] = (String) synonyms.first();
-        else
-          synonyms.toArray(syns);
-//        if (syns.length > 1) Arrays.sort(syns);
-//        if (DEBUG) System.err.println("word=" + word + ":" + Arrays.asList(syns));
-        word2Syns.put(word, syns);
-      }
-    }
-  
-    return word2Syns;
-  }
-
-  private HashMap optimize(HashMap word2Syns, HashMap internedWords) {
-    if (DEBUG) {
-      System.err.println("before gc");
-      for (int i=0; i < 10; i++) System.gc();
-      System.err.println("after gc");
-    }
-    
-    // collect entries
-    int len = 0;
-    int size = word2Syns.size();
-    String[][] allSynonyms = new String[size][];
-    String[] words = new String[size];
-    Iterator iter = word2Syns.entrySet().iterator();
-    for (int j=0; j < size; j++) {
-      Map.Entry entry = (Map.Entry) iter.next();
-      allSynonyms[j] = (String[]) entry.getValue(); 
-      words[j] = (String) entry.getKey();
-      len += words[j].length();
-    }
-    
-    // assemble large string containing all words
-    StringBuilder buf = new StringBuilder(len);
-    for (int j=0; j < size; j++) buf.append(words[j]);
-    String allWords = new String(buf.toString()); // ensure compact string across JDK versions
-    buf = null;
-    
-    // intern words at app level via memory-overlaid substrings
-    for (int p=0, j=0; j < size; j++) {
-      String word = words[j];
-      internedWords.put(word, allWords.substring(p, p + word.length()));
-      p += word.length();
-    }
-    
-    // replace words with interned words
-    for (int j=0; j < size; j++) {
-      String[] syns = allSynonyms[j];
-      for (int k=syns.length; --k >= 0; ) {
-        syns[k] = (String) internedWords.get(syns[k]);
-      }
-      Object replacement = syns;
-      if (syns.length == 1) replacement = syns[0]; // minimize memory consumption some more
-      word2Syns.remove(words[j]);
-      word2Syns.put(internedWords.get(words[j]), replacement);
-    }
-    
-    if (DEBUG) {
-      words = null;
-      allSynonyms = null;
-      internedWords = null;
-      allWords = null;
-      System.err.println("before gc");
-      for (int i=0; i < 10; i++) System.gc();
-      System.err.println("after gc");
-    }
-    return word2Syns;
-  }
-  
-  // the following utility methods below are copied from Apache style Nux library - see http://dsd.lbl.gov/nux
-  private static byte[] toByteArray(InputStream input) throws IOException {
-    try {
-      // safe and fast even if input.available() behaves weird or buggy
-      int len = Math.max(256, input.available());
-      byte[] buffer = new byte[len];
-      byte[] output = new byte[len];
-      
-      len = 0;
-      int n;
-      while ((n = input.read(buffer)) >= 0) {
-        if (len + n > output.length) { // grow capacity
-          byte tmp[] = new byte[Math.max(output.length << 1, len + n)];
-          System.arraycopy(output, 0, tmp, 0, len);
-          System.arraycopy(buffer, 0, tmp, len, n);
-          buffer = output; // use larger buffer for future larger bulk reads
-          output = tmp;
-        } else {
-          System.arraycopy(buffer, 0, output, len, n);
-        }
-        len += n;
-      }
-
-      if (len == output.length) return output;
-      buffer = null; // help gc
-      buffer = new byte[len];
-      System.arraycopy(output, 0, buffer, 0, len);
-      return buffer;
-    } finally {
-      if (input != null) input.close();
-    }
-  }
-  
-}
\ No newline at end of file
diff --git a/contrib/memory/src/java/org/apache/lucene/index/memory/SynonymTokenFilter.java b/contrib/memory/src/java/org/apache/lucene/index/memory/SynonymTokenFilter.java
deleted file mode 100644
index 1b84101..0000000
--- a/contrib/memory/src/java/org/apache/lucene/index/memory/SynonymTokenFilter.java
+++ /dev/null
@@ -1,150 +0,0 @@
-package org.apache.lucene.index.memory;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.util.AttributeSource;
-
-/**
- * Injects additional tokens for synonyms of token terms fetched from the
- * underlying child stream; the child stream must deliver lowercase tokens
- * for synonyms to be found.
- *
- */
-public class SynonymTokenFilter extends TokenFilter {
-    
-  /** The Token.type used to indicate a synonym to higher level filters. */
-  public static final String SYNONYM_TOKEN_TYPE = "SYNONYM";
-
-  private final SynonymMap synonyms;
-  private final int maxSynonyms;
-  
-  private String[] stack = null;
-  private int index = 0;
-  private AttributeSource.State current = null;
-  private int todo = 0;
-  
-  private TermAttribute termAtt;
-  private TypeAttribute typeAtt;
-  private PositionIncrementAttribute posIncrAtt;
-  
-  /**
-   * Creates an instance for the given underlying stream and synonym table.
-   * 
-   * @param input
-   *            the underlying child token stream
-   * @param synonyms
-   *            the map used to extract synonyms for terms
-   * @param maxSynonyms
-   *            the maximum number of synonym tokens to return per underlying
-   *            token word (a value of Integer.MAX_VALUE indicates unlimited)
-   */
-  public SynonymTokenFilter(TokenStream input, SynonymMap synonyms, int maxSynonyms) {
-    super(input);
-    if (input == null)
-      throw new IllegalArgumentException("input must not be null");
-    if (synonyms == null)
-      throw new IllegalArgumentException("synonyms must not be null");
-    if (maxSynonyms < 0) 
-      throw new IllegalArgumentException("maxSynonyms must not be negative");
-    
-    this.synonyms = synonyms;
-    this.maxSynonyms = maxSynonyms;
-    
-    this.termAtt = addAttribute(TermAttribute.class);
-    this.typeAtt = addAttribute(TypeAttribute.class);
-    this.posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-  }
-  
-  /** Returns the next token in the stream, or null at EOS. */
-  public final boolean incrementToken() throws IOException {
-    while (todo > 0 && index < stack.length) { // pop from stack
-      if (createToken(stack[index++], current)) {
-        todo--;
-        return true;
-      }
-    }
-    
-    if (!input.incrementToken()) return false; // EOS; iterator exhausted 
-    
-    stack = synonyms.getSynonyms(termAtt.term()); // push onto stack
-    if (stack.length > maxSynonyms) randomize(stack);
-    index = 0;
-    current = captureState();
-    todo = maxSynonyms;
-    return true;
-  }
-  
-  /**
-   * Creates and returns a token for the given synonym of the current input
-   * token; Override for custom (stateless or stateful) behavior, if desired.
-   * 
-   * @param synonym 
-   *            a synonym for the current token's term
-   * @param current
-   *            the current token from the underlying child stream
-   * @return a new token, or null to indicate that the given synonym should be
-   *         ignored
-   */
-  protected boolean createToken(String synonym, AttributeSource.State current) {
-    restoreState(current);
-    termAtt.setTermBuffer(synonym);
-    typeAtt.setType(SYNONYM_TOKEN_TYPE);
-    posIncrAtt.setPositionIncrement(0);
-    return true;
-  }
-  
-  /**
-   * Randomize synonyms to later sample a subset. Uses constant random seed
-   * for reproducibility. Uses "DRand", a simple, fast, uniform pseudo-random
-   * number generator with medium statistical quality (multiplicative
-   * congruential method), producing integers in the range [Integer.MIN_VALUE,
-   * Integer.MAX_VALUE].
-   */
-  private static void randomize(Object[] arr) {
-    int seed = 1234567; // constant
-    int randomState = 4*seed + 1;
-//    Random random = new Random(seed); // unnecessary overhead
-    int len = arr.length;
-    for (int i=0; i < len-1; i++) {
-      randomState *= 0x278DDE6D; // z(i+1)=a*z(i) (mod 2**32)
-      int r = randomState % (len-i);
-      if (r < 0) r = -r; // e.g. -9 % 2 == -1
-//      int r = random.nextInt(len-i);
-      
-      // swap arr[i, i+r]
-      Object tmp = arr[i];
-      arr[i] = arr[i + r];
-      arr[i + r] = tmp;
-    }   
-  }
-
-  public void reset() throws IOException {
-    super.reset();
-    stack = null;
-    index = 0;
-    current = null;
-    todo = 0;
-  }
-}
diff --git a/contrib/memory/src/test/org/apache/lucene/index/memory/TestSynonymTokenFilter.java b/contrib/memory/src/test/org/apache/lucene/index/memory/TestSynonymTokenFilter.java
deleted file mode 100644
index de57b62..0000000
--- a/contrib/memory/src/test/org/apache/lucene/index/memory/TestSynonymTokenFilter.java
+++ /dev/null
@@ -1,125 +0,0 @@
-package org.apache.lucene.index.memory;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.Reader;
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceTokenizer;
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-
-public class TestSynonymTokenFilter extends BaseTokenStreamTestCase {
-  File dataDir = new File(System.getProperty("dataDir", "./bin"));
-  File testFile = new File(dataDir, "org/apache/lucene/index/memory/testSynonyms.txt");
-  
-  public void testSynonyms() throws Exception {
-    SynonymMap map = new SynonymMap(new FileInputStream(testFile));
-    /* all expansions */
-    Analyzer analyzer = new SynonymWhitespaceAnalyzer(map, Integer.MAX_VALUE);
-    assertAnalyzesTo(analyzer, "Lost in the woods",
-        new String[] { "lost", "in", "the", "woods", "forest", "wood" },
-        new int[] { 0, 5, 8, 12, 12, 12 },
-        new int[] { 4, 7, 11, 17, 17, 17 },
-        new int[] { 1, 1, 1, 1, 0, 0 });
-  }
-  
-  public void testSynonymsSingleQuote() throws Exception {
-    SynonymMap map = new SynonymMap(new FileInputStream(testFile));
-    /* all expansions */
-    Analyzer analyzer = new SynonymWhitespaceAnalyzer(map, Integer.MAX_VALUE);
-    assertAnalyzesTo(analyzer, "king",
-        new String[] { "king", "baron" });
-  }
-  
-  public void testSynonymsLimitedAmount() throws Exception {
-    SynonymMap map = new SynonymMap(new FileInputStream(testFile));
-    /* limit to one synonym expansion */
-    Analyzer analyzer = new SynonymWhitespaceAnalyzer(map, 1);
-    assertAnalyzesTo(analyzer, "Lost in the woods",
-        /* wood comes before forest due to 
-         * the input file, not lexicographic order
-         */
-        new String[] { "lost", "in", "the", "woods", "wood" },
-        new int[] { 0, 5, 8, 12, 12 },
-        new int[] { 4, 7, 11, 17, 17 },
-        new int[] { 1, 1, 1, 1, 0 });
-  }
-  
-  public void testReusableTokenStream() throws Exception {
-    SynonymMap map = new SynonymMap(new FileInputStream(testFile));
-    /* limit to one synonym expansion */
-    Analyzer analyzer = new SynonymWhitespaceAnalyzer(map, 1);
-    assertAnalyzesToReuse(analyzer, "Lost in the woods",
-        new String[] { "lost", "in", "the", "woods", "wood" },
-        new int[] { 0, 5, 8, 12, 12 },
-        new int[] { 4, 7, 11, 17, 17 },
-        new int[] { 1, 1, 1, 1, 0 });
-    assertAnalyzesToReuse(analyzer, "My wolfish dog went to the forest",
-        new String[] { "my", "wolfish", "ravenous", "dog", "went", "to",
-          "the", "forest", "woods" },
-        new int[] { 0, 3, 3, 11, 15, 20, 23, 27, 27 },
-        new int[] { 2, 10, 10, 14, 19, 22, 26, 33, 33 },
-        new int[] { 1, 1, 0, 1, 1, 1, 1, 1, 0 });
-  }
-  
-  private class SynonymWhitespaceAnalyzer extends Analyzer {
-    private SynonymMap synonyms;
-    private int maxSynonyms;
-    
-    public SynonymWhitespaceAnalyzer(SynonymMap synonyms, int maxSynonyms) {
-      this.synonyms = synonyms;
-      this.maxSynonyms = maxSynonyms;
-    }
-    
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      TokenStream ts = new WhitespaceTokenizer(reader);
-      ts = new LowerCaseFilter(ts);
-      ts = new SynonymTokenFilter(ts, synonyms, maxSynonyms);
-      return ts;
-    }
-    
-    private class SavedStreams {
-      Tokenizer source;
-      TokenStream result;
-    };
-    
-    public TokenStream reusableTokenStream(String fieldName, Reader reader)
-        throws IOException {
-      SavedStreams streams = (SavedStreams) getPreviousTokenStream();
-      if (streams == null) {
-        streams = new SavedStreams();
-        streams.source = new WhitespaceTokenizer(reader);
-        streams.result = new LowerCaseFilter(streams.source);
-        streams.result = new SynonymTokenFilter(streams.result, synonyms, maxSynonyms);
-        setPreviousTokenStream(streams);
-      } else {
-        streams.source.reset(reader);
-        streams.result.reset(); // reset the SynonymTokenFilter
-      }
-      return streams.result;
-    }
-  }
-  
-}
diff --git a/contrib/memory/src/test/org/apache/lucene/index/memory/testSynonyms.txt b/contrib/memory/src/test/org/apache/lucene/index/memory/testSynonyms.txt
deleted file mode 100644
index 822bc96..0000000
--- a/contrib/memory/src/test/org/apache/lucene/index/memory/testSynonyms.txt
+++ /dev/null
@@ -1,9 +0,0 @@
-s(100000001,1,'woods',n,1,0).
-s(100000001,2,'wood',n,1,0).
-s(100000001,3,'forest',n,1,0).
-s(100000002,1,'wolfish',n,1,0).
-s(100000002,2,'ravenous',n,1,0).
-s(100000003,1,'king',n,1,1).
-s(100000003,2,'baron',n,1,1).
-s(100000004,1,'king''sevil',n,1,1).
-s(100000004,2,'meany',n,1,1).
diff --git a/contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil.java b/contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil.java
new file mode 100644
index 0000000..0132bb6
--- /dev/null
+++ b/contrib/wordnet/src/java/org/apache/lucene/wordnet/AnalyzerUtil.java
@@ -0,0 +1,456 @@
+package org.apache.lucene.wordnet;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.io.Reader;
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.regex.Pattern;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.PorterStemFilter;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.AttributeSource;
+
+/**
+ * Various fulltext analysis utilities avoiding redundant code in several
+ * classes.
+ *
+ */
+public class AnalyzerUtil {
+  
+  private AnalyzerUtil() {};
+
+  /**
+   * Returns a simple analyzer wrapper that logs all tokens produced by the
+   * underlying child analyzer to the given log stream (typically System.err);
+   * Otherwise behaves exactly like the child analyzer, delivering the very
+   * same tokens; useful for debugging purposes on custom indexing and/or
+   * querying.
+   * 
+   * @param child
+   *            the underlying child analyzer
+   * @param log
+   *            the print stream to log to (typically System.err)
+   * @param logName
+   *            a name for this logger (typically "log" or similar)
+   * @return a logging analyzer
+   */
+  public static Analyzer getLoggingAnalyzer(final Analyzer child, 
+      final PrintStream log, final String logName) {
+    
+    if (child == null) 
+      throw new IllegalArgumentException("child analyzer must not be null");
+    if (log == null) 
+      throw new IllegalArgumentException("logStream must not be null");
+
+    return new Analyzer() {
+      public TokenStream tokenStream(final String fieldName, Reader reader) {
+        return new TokenFilter(child.tokenStream(fieldName, reader)) {
+          private int position = -1;
+          private TermAttribute termAtt = addAttribute(TermAttribute.class);
+          private PositionIncrementAttribute posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+          private OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
+          private TypeAttribute typeAtt = addAttribute(TypeAttribute.class);
+         
+          public boolean incrementToken() throws IOException {
+            boolean hasNext = input.incrementToken();
+            log.println(toString(hasNext));
+            return hasNext;
+          }
+          
+          private String toString(boolean hasNext) {
+            if (!hasNext) return "[" + logName + ":EOS:" + fieldName + "]\n";
+            
+            position += posIncrAtt.getPositionIncrement();
+            return "[" + logName + ":" + position + ":" + fieldName + ":"
+                + termAtt.term() + ":" + offsetAtt.startOffset()
+                + "-" + offsetAtt.endOffset() + ":" + typeAtt.type()
+                + "]";
+          }         
+        };
+      }
+    };
+  }
+  
+  
+  /**
+   * Returns an analyzer wrapper that returns at most the first
+   * <code>maxTokens</code> tokens from the underlying child analyzer,
+   * ignoring all remaining tokens.
+   * 
+   * @param child
+   *            the underlying child analyzer
+   * @param maxTokens
+   *            the maximum number of tokens to return from the underlying
+   *            analyzer (a value of Integer.MAX_VALUE indicates unlimited)
+   * @return an analyzer wrapper
+   */
+  public static Analyzer getMaxTokenAnalyzer(
+      final Analyzer child, final int maxTokens) {
+    
+    if (child == null) 
+      throw new IllegalArgumentException("child analyzer must not be null");
+    if (maxTokens < 0) 
+      throw new IllegalArgumentException("maxTokens must not be negative");
+    if (maxTokens == Integer.MAX_VALUE) 
+      return child; // no need to wrap
+  
+    return new Analyzer() {
+      public TokenStream tokenStream(String fieldName, Reader reader) {
+        return new TokenFilter(child.tokenStream(fieldName, reader)) {
+          private int todo = maxTokens;
+          
+          public boolean incrementToken() throws IOException {
+            return --todo >= 0 ? input.incrementToken() : false;
+          }
+        };
+      }
+    };
+  }
+  
+  
+  /**
+   * Returns an English stemming analyzer that stems tokens from the
+   * underlying child analyzer according to the Porter stemming algorithm. The
+   * child analyzer must deliver tokens in lower case for the stemmer to work
+   * properly.
+   * <p>
+   * Background: Stemming reduces token terms to their linguistic root form
+   * e.g. reduces "fishing" and "fishes" to "fish", "family" and "families" to
+   * "famili", as well as "complete" and "completion" to "complet". Note that
+   * the root form is not necessarily a meaningful word in itself, and that
+   * this is not a bug but rather a feature, if you lean back and think about
+   * fuzzy word matching for a bit.
+   * <p>
+   * See the Lucene contrib packages for stemmers (and stop words) for German,
+   * Russian and many more languages.
+   * 
+   * @param child
+   *            the underlying child analyzer
+   * @return an analyzer wrapper
+   */
+  public static Analyzer getPorterStemmerAnalyzer(final Analyzer child) {
+    
+    if (child == null) 
+      throw new IllegalArgumentException("child analyzer must not be null");
+  
+    return new Analyzer() {
+      public TokenStream tokenStream(String fieldName, Reader reader) {
+        return new PorterStemFilter(
+            child.tokenStream(fieldName, reader));
+//        /* PorterStemFilter and SnowballFilter have the same behaviour, 
+//        but PorterStemFilter is much faster. */
+//        return new org.apache.lucene.analysis.snowball.SnowballFilter(
+//            child.tokenStream(fieldName, reader), "English");
+      }
+    };
+  }
+  
+  
+  /**
+   * Returns an analyzer wrapper that wraps the underlying child analyzer's
+   * token stream into a {@link SynonymTokenFilter}.
+   * 
+   * @param child
+   *            the underlying child analyzer
+   * @param synonyms
+   *            the map used to extract synonyms for terms
+   * @param maxSynonyms
+   *            the maximum number of synonym tokens to return per underlying
+   *            token word (a value of Integer.MAX_VALUE indicates unlimited)
+   * @return a new analyzer
+   */
+  public static Analyzer getSynonymAnalyzer(final Analyzer child, 
+      final SynonymMap synonyms, final int maxSynonyms) {
+    
+    if (child == null) 
+      throw new IllegalArgumentException("child analyzer must not be null");
+    if (synonyms == null)
+      throw new IllegalArgumentException("synonyms must not be null");
+    if (maxSynonyms < 0) 
+      throw new IllegalArgumentException("maxSynonyms must not be negative");
+    if (maxSynonyms == 0)
+      return child; // no need to wrap
+  
+    return new Analyzer() {
+      public TokenStream tokenStream(String fieldName, Reader reader) {
+        return new SynonymTokenFilter(
+          child.tokenStream(fieldName, reader), synonyms, maxSynonyms);
+      }
+    };
+  }
+
+  
+  /**
+   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's
+   * token streams, and delivers those cached tokens on subsequent calls to 
+   * <code>tokenStream(String fieldName, Reader reader)</code> 
+   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.
+   * <p>
+   * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can 
+   * help improve performance if the same document is added to multiple Lucene indexes, 
+   * because the text analysis phase need not be performed more than once.
+   * <p>
+   * Caveats: 
+   * <ul>
+   * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> 
+   * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>
+   * <li>The same caching analyzer instance must not be used for more than one document
+   * because the cache is not keyed on the Reader parameter.</li>
+   * </ul>
+   * 
+   * @param child
+   *            the underlying child analyzer
+   * @return a new analyzer
+   */
+  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {
+
+    if (child == null)
+      throw new IllegalArgumentException("child analyzer must not be null");
+
+    return new Analyzer() {
+
+      private final HashMap cache = new HashMap();
+
+      public TokenStream tokenStream(String fieldName, Reader reader) {
+        final ArrayList tokens = (ArrayList) cache.get(fieldName);
+        if (tokens == null) { // not yet cached
+          final ArrayList tokens2 = new ArrayList();
+          TokenStream tokenStream = new TokenFilter(child.tokenStream(fieldName, reader)) {
+
+            public boolean incrementToken() throws IOException {
+              boolean hasNext = input.incrementToken();
+              if (hasNext) tokens2.add(captureState());
+              return hasNext;
+            }
+          };
+          
+          cache.put(fieldName, tokens2);
+          return tokenStream;
+        } else { // already cached
+          return new TokenStream() {
+
+            private Iterator iter = tokens.iterator();
+
+            public boolean incrementToken() {
+              if (!iter.hasNext()) return false;
+              restoreState((AttributeSource.State) iter.next());
+              return true;
+            }
+          };
+        }
+      }
+    };
+  }
+      
+  
+  /**
+   * Returns (frequency:term) pairs for the top N distinct terms (aka words),
+   * sorted descending by frequency (and ascending by term, if tied).
+   * <p>
+   * Example XQuery:
+   * <pre>
+   * declare namespace util = "java:org.apache.lucene.index.memory.AnalyzerUtil";
+   * declare namespace analyzer = "java:org.apache.lucene.index.memory.PatternAnalyzer";
+   * 
+   * for $pair in util:get-most-frequent-terms(
+   *    analyzer:EXTENDED_ANALYZER(), doc("samples/shakespeare/othello.xml"), 10)
+   * return &lt;word word="{substring-after($pair, ':')}" frequency="{substring-before($pair, ':')}"/>
+   * </pre>
+   * 
+   * @param analyzer
+   *            the analyzer to use for splitting text into terms (aka words)
+   * @param text
+   *            the text to analyze
+   * @param limit
+   *            the maximum number of pairs to return; zero indicates 
+   *            "as many as possible".
+   * @return an array of (frequency:term) pairs in the form of (freq0:term0,
+   *         freq1:term1, ..., freqN:termN). Each pair is a single string
+   *         separated by a ':' delimiter.
+   */
+  public static String[] getMostFrequentTerms(Analyzer analyzer, String text, int limit) {
+    if (analyzer == null) 
+      throw new IllegalArgumentException("analyzer must not be null");
+    if (text == null) 
+      throw new IllegalArgumentException("text must not be null");
+    if (limit <= 0) limit = Integer.MAX_VALUE;
+    
+    // compute frequencies of distinct terms
+    HashMap map = new HashMap();
+    TokenStream stream = analyzer.tokenStream("", new StringReader(text));
+    TermAttribute termAtt = stream.addAttribute(TermAttribute.class);
+    try {
+      while (stream.incrementToken()) {
+        MutableInteger freq = (MutableInteger) map.get(termAtt.term());
+        if (freq == null) {
+          freq = new MutableInteger(1);
+          map.put(termAtt.term(), freq);
+        } else {
+          freq.setValue(freq.intValue() + 1);
+        }
+      }
+    } catch (IOException e) {
+      throw new RuntimeException(e);
+    } finally {
+      try {
+        stream.close();
+      } catch (IOException e2) {
+        throw new RuntimeException(e2);
+      }
+    }
+    
+    // sort by frequency, text
+    Map.Entry[] entries = new Map.Entry[map.size()];
+    map.entrySet().toArray(entries);
+    Arrays.sort(entries, new Comparator() {
+      public int compare(Object o1, Object o2) {
+        Map.Entry e1 = (Map.Entry) o1;
+        Map.Entry e2 = (Map.Entry) o2;
+        int f1 = ((MutableInteger) e1.getValue()).intValue();
+        int f2 = ((MutableInteger) e2.getValue()).intValue();
+        if (f2 - f1 != 0) return f2 - f1;
+        String s1 = (String) e1.getKey();
+        String s2 = (String) e2.getKey();
+        return s1.compareTo(s2);
+      }
+    });
+    
+    // return top N entries
+    int size = Math.min(limit, entries.length);
+    String[] pairs = new String[size];
+    for (int i=0; i < size; i++) {
+      pairs[i] = entries[i].getValue() + ":" + entries[i].getKey();
+    }
+    return pairs;
+  }
+  
+  private static final class MutableInteger {
+    private int value;
+    public MutableInteger(int value) { this.value = value; }
+    public int intValue() { return value; }
+    public void setValue(int value) { this.value = value; }
+    public String toString() { return String.valueOf(value); }
+  };
+  
+  
+  
+  // TODO: could use a more general i18n approach ala http://icu.sourceforge.net/docs/papers/text_boundary_analysis_in_java/
+  /** (Line terminator followed by zero or more whitespace) two or more times */
+  private static final Pattern PARAGRAPHS = Pattern.compile("([\\r\\n\\u0085\\u2028\\u2029][ \\t\\x0B\\f]*){2,}");
+  
+  /**
+   * Returns at most the first N paragraphs of the given text. Delimiting
+   * characters are excluded from the results. Each returned paragraph is
+   * whitespace-trimmed via String.trim(), potentially an empty string.
+   * 
+   * @param text
+   *            the text to tokenize into paragraphs
+   * @param limit
+   *            the maximum number of paragraphs to return; zero indicates "as
+   *            many as possible".
+   * @return the first N paragraphs
+   */
+  public static String[] getParagraphs(String text, int limit) {
+    return tokenize(PARAGRAPHS, text, limit);
+  }
+    
+  private static String[] tokenize(Pattern pattern, String text, int limit) {
+    String[] tokens = pattern.split(text, limit);
+    for (int i=tokens.length; --i >= 0; ) tokens[i] = tokens[i].trim();
+    return tokens;
+  }
+  
+  
+  // TODO: don't split on floating point numbers, e.g. 3.1415 (digit before or after '.')
+  /** Divides text into sentences; Includes inverted spanish exclamation and question mark */
+  private static final Pattern SENTENCES  = Pattern.compile("[!\\.\\?\\xA1\\xBF]+");
+
+  /**
+   * Returns at most the first N sentences of the given text. Delimiting
+   * characters are excluded from the results. Each returned sentence is
+   * whitespace-trimmed via String.trim(), potentially an empty string.
+   * 
+   * @param text
+   *            the text to tokenize into sentences
+   * @param limit
+   *            the maximum number of sentences to return; zero indicates "as
+   *            many as possible".
+   * @return the first N sentences
+   */
+  public static String[] getSentences(String text, int limit) {
+//    return tokenize(SENTENCES, text, limit); // equivalent but slower
+    int len = text.length();
+    if (len == 0) return new String[] { text };
+    if (limit <= 0) limit = Integer.MAX_VALUE;
+    
+    // average sentence length heuristic
+    String[] tokens = new String[Math.min(limit, 1 + len/40)];
+    int size = 0;
+    int i = 0;
+    
+    while (i < len && size < limit) {
+      
+      // scan to end of current sentence
+      int start = i;
+      while (i < len && !isSentenceSeparator(text.charAt(i))) i++;
+      
+      if (size == tokens.length) { // grow array
+        String[] tmp = new String[tokens.length << 1];
+        System.arraycopy(tokens, 0, tmp, 0, size);
+        tokens = tmp;
+      }
+      // add sentence (potentially empty)
+      tokens[size++] = text.substring(start, i).trim();
+
+      // scan to beginning of next sentence
+      while (i < len && isSentenceSeparator(text.charAt(i))) i++;
+    }
+    
+    if (size == tokens.length) return tokens;
+    String[] results = new String[size];
+    System.arraycopy(tokens, 0, results, 0, size);
+    return results;
+  }
+
+  private static boolean isSentenceSeparator(char c) {
+    // regex [!\\.\\?\\xA1\\xBF]
+    switch (c) {
+      case '!': return true;
+      case '.': return true;
+      case '?': return true;
+      case 0xA1: return true; // spanish inverted exclamation mark
+      case 0xBF: return true; // spanish inverted question mark
+      default: return false;
+    }   
+  }
+  
+}
diff --git a/contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java b/contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java
index b57ff10..e4d4bcb 100755
--- a/contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java
+++ b/contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java
@@ -97,7 +97,7 @@ public final class SynExpand {
 	 *
 	 * @param a optional analyzer used to parse the users query else {@link StandardAnalyzer} is used
 	 *
-	 * @param field optional field name to search in or null if you want the default of "contents"
+	 * @param f optional field name to search in or null if you want the default of "contents"
 	 *
 	 * @param boost optional boost applied to synonyms else no boost is applied
 	 *
diff --git a/contrib/wordnet/src/java/org/apache/lucene/wordnet/SynonymMap.java b/contrib/wordnet/src/java/org/apache/lucene/wordnet/SynonymMap.java
new file mode 100644
index 0000000..31a698c
--- /dev/null
+++ b/contrib/wordnet/src/java/org/apache/lucene/wordnet/SynonymMap.java
@@ -0,0 +1,399 @@
+package org.apache.lucene.wordnet;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.ByteBuffer;
+import java.nio.charset.Charset;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+/**
+ * Loads the <a target="_blank" 
+ * href="http://www.cogsci.princeton.edu/~wn/">WordNet </a> prolog file <a
+ * href="http://www.cogsci.princeton.edu/2.0/WNprolog-2.0.tar.gz">wn_s.pl </a>
+ * into a thread-safe main-memory hash map that can be used for fast
+ * high-frequency lookups of synonyms for any given (lowercase) word string.
+ * <p>
+ * There holds: If B is a synonym for A (A -> B) then A is also a synonym for B (B -> A).
+ * There does not necessarily hold: A -> B, B -> C then A -> C.
+ * <p>
+ * Loading typically takes some 1.5 secs, so should be done only once per
+ * (server) program execution, using a singleton pattern. Once loaded, a
+ * synonym lookup via {@link #getSynonyms(String)}takes constant time O(1).
+ * A loaded default synonym map consumes about 10 MB main memory.
+ * An instance is immutable, hence thread-safe.
+ * <p>
+ * This implementation borrows some ideas from the Lucene Syns2Index demo that 
+ * Dave Spencer originally contributed to Lucene. Dave's approach
+ * involved a persistent Lucene index which is suitable for occasional
+ * lookups or very large synonym tables, but considered unsuitable for 
+ * high-frequency lookups of medium size synonym tables.
+ * <p>
+ * Example Usage:
+ * <pre>
+ * String[] words = new String[] { "hard", "woods", "forest", "wolfish", "xxxx"};
+ * SynonymMap map = new SynonymMap(new FileInputStream("samples/fulltext/wn_s.pl"));
+ * for (int i = 0; i &lt; words.length; i++) {
+ *     String[] synonyms = map.getSynonyms(words[i]);
+ *     System.out.println(words[i] + ":" + java.util.Arrays.asList(synonyms).toString());
+ * }
+ * 
+ * Example output:
+ * hard:[arduous, backbreaking, difficult, fermented, firmly, grueling, gruelling, heavily, heavy, intemperately, knockout, laborious, punishing, severe, severely, strong, toilsome, tough]
+ * woods:[forest, wood]
+ * forest:[afforest, timber, timberland, wood, woodland, woods]
+ * wolfish:[edacious, esurient, rapacious, ravening, ravenous, voracious, wolflike]
+ * xxxx:[]
+ * </pre>
+ *
+ * @see <a target="_blank"
+ *      href="http://www.cogsci.princeton.edu/~wn/man/prologdb.5WN.html">prologdb
+ *      man page </a>
+ * @see <a target="_blank" href="http://www.hostmon.com/rfc/advanced.jsp">Dave's synonym demo site</a>
+ */
+public class SynonymMap {
+
+  /** the index data; Map<String word, String[] synonyms> */
+  private final HashMap table;
+  
+  private static final String[] EMPTY = new String[0];
+  
+  private static final boolean DEBUG = false;
+
+  /**
+   * Constructs an instance, loading WordNet synonym data from the given input
+   * stream. Finally closes the stream. The words in the stream must be in
+   * UTF-8 or a compatible subset (for example ASCII, MacRoman, etc.).
+   * 
+   * @param input
+   *            the stream to read from (null indicates an empty synonym map)
+   * @throws IOException
+   *             if an error occured while reading the stream.
+   */
+  public SynonymMap(InputStream input) throws IOException {
+    this.table = input == null ? new HashMap(0) : read(toByteArray(input));
+  }
+  
+  /**
+   * Returns the synonym set for the given word, sorted ascending.
+   * 
+   * @param word
+   *            the word to lookup (must be in lowercase).
+   * @return the synonyms; a set of zero or more words, sorted ascending, each
+   *         word containing lowercase characters that satisfy
+   *         <code>Character.isLetter()</code>.
+   */
+  public String[] getSynonyms(String word) {
+    Object syns = table.get(word);
+    if (syns == null) return EMPTY;
+    if (syns instanceof String) return new String[] {(String) syns};
+    
+    String[] synonyms = (String[]) syns;
+    String[] copy = new String[synonyms.length]; // copy for guaranteed immutability
+    System.arraycopy(synonyms, 0, copy, 0, synonyms.length);
+    return copy;
+  }
+  
+  /**
+   * Returns a String representation of the index data for debugging purposes.
+   * 
+   * @return a String representation
+   */
+  public String toString() {
+    StringBuilder buf = new StringBuilder();
+    Iterator iter = new TreeMap(table).keySet().iterator();
+    int count = 0;
+    int f0 = 0;
+    int f1 = 0;
+    int f2 = 0;
+    int f3 = 0;
+    
+    while (iter.hasNext()) {
+      String word = (String) iter.next();
+      buf.append(word + ":");
+      String[] synonyms = getSynonyms(word);
+      buf.append(Arrays.asList(synonyms));
+      buf.append("\n");
+      count += synonyms.length;
+      if (synonyms.length == 0) f0++;
+      if (synonyms.length == 1) f1++;
+      if (synonyms.length == 2) f2++;
+      if (synonyms.length == 3) f3++;
+    }
+    
+    buf.append("\n\nkeys=" + table.size() + ", synonyms=" + count + ", f0=" + f0 +", f1=" + f1 + ", f2=" + f2 + ", f3=" + f3);
+    return buf.toString();
+  }
+  
+  /**
+   * Analyzes/transforms the given word on input stream loading. This default implementation simply
+   * lowercases the word. Override this method with a custom stemming
+   * algorithm or similar, if desired.
+   * 
+   * @param word
+   *            the word to analyze
+   * @return the same word, or a different word (or null to indicate that the
+   *         word should be ignored)
+   */
+  protected String analyze(String word) {
+    return word.toLowerCase();
+  }
+
+  private static boolean isValid(String str) {
+    for (int i=str.length(); --i >= 0; ) {
+      if (!Character.isLetter(str.charAt(i))) return false;
+    }
+    return true;
+  }
+
+  private HashMap read(byte[] data) {
+    int WORDS  = (int) (76401 / 0.7); // presizing
+    int GROUPS = (int) (88022 / 0.7); // presizing
+    HashMap word2Groups = new HashMap(WORDS);  // Map<String word, int[] groups>
+    HashMap group2Words = new HashMap(GROUPS); // Map<int group, String[] words>
+    HashMap internedWords = new HashMap(WORDS);// Map<String word, String word>
+
+    Charset charset = Charset.forName("UTF-8");
+    int lastNum = -1;
+    Integer lastGroup = null;
+    int len = data.length;
+    int i=0;
+    
+    while (i < len) { // until EOF
+      /* Part A: Parse a line */
+      
+      // scan to beginning of group
+      while (i < len && data[i] != '(') i++;
+      if (i >= len) break; // EOF
+      i++;
+      
+      // parse group
+      int num = 0;
+      while (i < len && data[i] != ',') {
+        num = 10*num + (data[i] - 48);
+        i++;
+      }
+      i++;
+//      if (DEBUG) System.err.println("num="+ num);
+      
+      // scan to beginning of word
+      while (i < len && data[i] != '\'') i++;
+      i++;
+  
+      // scan to end of word
+      int start = i;
+      do {
+        while (i < len && data[i] != '\'') i++;
+        i++;
+      } while (i < len && data[i] != ','); // word must end with "',"
+      
+      if (i >= len) break; // EOF
+      String word = charset.decode(ByteBuffer.wrap(data, start, i-start-1)).toString();
+//      String word = new String(data, 0, start, i-start-1); // ASCII
+      
+      /*
+       * Part B: ignore phrases (with spaces and hyphens) and
+       * non-alphabetic words, and let user customize word (e.g. do some
+       * stemming)
+       */
+      if (!isValid(word)) continue; // ignore
+      word = analyze(word);
+      if (word == null || word.length() == 0) continue; // ignore
+      
+      
+      /* Part C: Add (group,word) to tables */
+      
+      // ensure compact string representation, minimizing memory overhead
+      String w = (String) internedWords.get(word);
+      if (w == null) {
+        word = new String(word); // ensure compact string
+        internedWords.put(word, word);
+      } else {
+        word = w;
+      }
+      
+      Integer group = lastGroup;
+      if (num != lastNum) {
+        group = Integer.valueOf(num);
+        lastGroup = group;
+        lastNum = num;
+      }
+      
+      // add word --> group
+      ArrayList groups = (ArrayList) word2Groups.get(word);
+      if (groups == null) {
+        groups = new ArrayList(1);
+        word2Groups.put(word, groups);
+      }
+      groups.add(group);
+
+      // add group --> word
+      ArrayList words = (ArrayList) group2Words.get(group);
+      if (words == null) {
+        words = new ArrayList(1);
+        group2Words.put(group, words);
+      } 
+      words.add(word);
+    }
+    
+    
+    /* Part D: compute index data structure */
+    HashMap word2Syns = createIndex(word2Groups, group2Words);    
+        
+    /* Part E: minimize memory consumption by a factor 3 (or so) */
+//    if (true) return word2Syns;
+    word2Groups = null; // help gc
+    group2Words = null; // help gc    
+    return optimize(word2Syns, internedWords);
+  }
+  
+  private HashMap createIndex(Map word2Groups, Map group2Words) {
+    HashMap word2Syns = new HashMap();
+    Iterator iter = word2Groups.entrySet().iterator();
+    
+    while (iter.hasNext()) { // for each word
+      Map.Entry entry = (Map.Entry) iter.next();
+      ArrayList group = (ArrayList) entry.getValue();     
+      String word = (String) entry.getKey();
+      
+//      HashSet synonyms = new HashSet();
+      TreeSet synonyms = new TreeSet();
+      for (int i=group.size(); --i >= 0; ) { // for each groupID of word
+        ArrayList words = (ArrayList) group2Words.get(group.get(i));
+        for (int j=words.size(); --j >= 0; ) { // add all words       
+          Object synonym = words.get(j); // note that w and word are interned
+          if (synonym != word) { // a word is implicitly it's own synonym
+            synonyms.add(synonym);
+          }
+        }
+      }
+
+      int size = synonyms.size();
+      if (size > 0) {
+        String[] syns = new String[size];
+        if (size == 1)  
+          syns[0] = (String) synonyms.first();
+        else
+          synonyms.toArray(syns);
+//        if (syns.length > 1) Arrays.sort(syns);
+//        if (DEBUG) System.err.println("word=" + word + ":" + Arrays.asList(syns));
+        word2Syns.put(word, syns);
+      }
+    }
+  
+    return word2Syns;
+  }
+
+  private HashMap optimize(HashMap word2Syns, HashMap internedWords) {
+    if (DEBUG) {
+      System.err.println("before gc");
+      for (int i=0; i < 10; i++) System.gc();
+      System.err.println("after gc");
+    }
+    
+    // collect entries
+    int len = 0;
+    int size = word2Syns.size();
+    String[][] allSynonyms = new String[size][];
+    String[] words = new String[size];
+    Iterator iter = word2Syns.entrySet().iterator();
+    for (int j=0; j < size; j++) {
+      Map.Entry entry = (Map.Entry) iter.next();
+      allSynonyms[j] = (String[]) entry.getValue(); 
+      words[j] = (String) entry.getKey();
+      len += words[j].length();
+    }
+    
+    // assemble large string containing all words
+    StringBuilder buf = new StringBuilder(len);
+    for (int j=0; j < size; j++) buf.append(words[j]);
+    String allWords = new String(buf.toString()); // ensure compact string across JDK versions
+    buf = null;
+    
+    // intern words at app level via memory-overlaid substrings
+    for (int p=0, j=0; j < size; j++) {
+      String word = words[j];
+      internedWords.put(word, allWords.substring(p, p + word.length()));
+      p += word.length();
+    }
+    
+    // replace words with interned words
+    for (int j=0; j < size; j++) {
+      String[] syns = allSynonyms[j];
+      for (int k=syns.length; --k >= 0; ) {
+        syns[k] = (String) internedWords.get(syns[k]);
+      }
+      Object replacement = syns;
+      if (syns.length == 1) replacement = syns[0]; // minimize memory consumption some more
+      word2Syns.remove(words[j]);
+      word2Syns.put(internedWords.get(words[j]), replacement);
+    }
+    
+    if (DEBUG) {
+      words = null;
+      allSynonyms = null;
+      internedWords = null;
+      allWords = null;
+      System.err.println("before gc");
+      for (int i=0; i < 10; i++) System.gc();
+      System.err.println("after gc");
+    }
+    return word2Syns;
+  }
+  
+  // the following utility methods below are copied from Apache style Nux library - see http://dsd.lbl.gov/nux
+  private static byte[] toByteArray(InputStream input) throws IOException {
+    try {
+      // safe and fast even if input.available() behaves weird or buggy
+      int len = Math.max(256, input.available());
+      byte[] buffer = new byte[len];
+      byte[] output = new byte[len];
+      
+      len = 0;
+      int n;
+      while ((n = input.read(buffer)) >= 0) {
+        if (len + n > output.length) { // grow capacity
+          byte tmp[] = new byte[Math.max(output.length << 1, len + n)];
+          System.arraycopy(output, 0, tmp, 0, len);
+          System.arraycopy(buffer, 0, tmp, len, n);
+          buffer = output; // use larger buffer for future larger bulk reads
+          output = tmp;
+        } else {
+          System.arraycopy(buffer, 0, output, len, n);
+        }
+        len += n;
+      }
+
+      if (len == output.length) return output;
+      buffer = null; // help gc
+      buffer = new byte[len];
+      System.arraycopy(output, 0, buffer, 0, len);
+      return buffer;
+    } finally {
+      if (input != null) input.close();
+    }
+  }
+  
+}
\ No newline at end of file
diff --git a/contrib/wordnet/src/java/org/apache/lucene/wordnet/SynonymTokenFilter.java b/contrib/wordnet/src/java/org/apache/lucene/wordnet/SynonymTokenFilter.java
new file mode 100644
index 0000000..cc26c3e
--- /dev/null
+++ b/contrib/wordnet/src/java/org/apache/lucene/wordnet/SynonymTokenFilter.java
@@ -0,0 +1,150 @@
+package org.apache.lucene.wordnet;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.AttributeSource;
+
+/**
+ * Injects additional tokens for synonyms of token terms fetched from the
+ * underlying child stream; the child stream must deliver lowercase tokens
+ * for synonyms to be found.
+ *
+ */
+public class SynonymTokenFilter extends TokenFilter {
+    
+  /** The Token.type used to indicate a synonym to higher level filters. */
+  public static final String SYNONYM_TOKEN_TYPE = "SYNONYM";
+
+  private final SynonymMap synonyms;
+  private final int maxSynonyms;
+  
+  private String[] stack = null;
+  private int index = 0;
+  private AttributeSource.State current = null;
+  private int todo = 0;
+  
+  private TermAttribute termAtt;
+  private TypeAttribute typeAtt;
+  private PositionIncrementAttribute posIncrAtt;
+  
+  /**
+   * Creates an instance for the given underlying stream and synonym table.
+   * 
+   * @param input
+   *            the underlying child token stream
+   * @param synonyms
+   *            the map used to extract synonyms for terms
+   * @param maxSynonyms
+   *            the maximum number of synonym tokens to return per underlying
+   *            token word (a value of Integer.MAX_VALUE indicates unlimited)
+   */
+  public SynonymTokenFilter(TokenStream input, SynonymMap synonyms, int maxSynonyms) {
+    super(input);
+    if (input == null)
+      throw new IllegalArgumentException("input must not be null");
+    if (synonyms == null)
+      throw new IllegalArgumentException("synonyms must not be null");
+    if (maxSynonyms < 0) 
+      throw new IllegalArgumentException("maxSynonyms must not be negative");
+    
+    this.synonyms = synonyms;
+    this.maxSynonyms = maxSynonyms;
+    
+    this.termAtt = addAttribute(TermAttribute.class);
+    this.typeAtt = addAttribute(TypeAttribute.class);
+    this.posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+  }
+  
+  /** Returns the next token in the stream, or null at EOS. */
+  public final boolean incrementToken() throws IOException {
+    while (todo > 0 && index < stack.length) { // pop from stack
+      if (createToken(stack[index++], current)) {
+        todo--;
+        return true;
+      }
+    }
+    
+    if (!input.incrementToken()) return false; // EOS; iterator exhausted 
+    
+    stack = synonyms.getSynonyms(termAtt.term()); // push onto stack
+    if (stack.length > maxSynonyms) randomize(stack);
+    index = 0;
+    current = captureState();
+    todo = maxSynonyms;
+    return true;
+  }
+  
+  /**
+   * Creates and returns a token for the given synonym of the current input
+   * token; Override for custom (stateless or stateful) behavior, if desired.
+   * 
+   * @param synonym 
+   *            a synonym for the current token's term
+   * @param current
+   *            the current token from the underlying child stream
+   * @return a new token, or null to indicate that the given synonym should be
+   *         ignored
+   */
+  protected boolean createToken(String synonym, AttributeSource.State current) {
+    restoreState(current);
+    termAtt.setTermBuffer(synonym);
+    typeAtt.setType(SYNONYM_TOKEN_TYPE);
+    posIncrAtt.setPositionIncrement(0);
+    return true;
+  }
+  
+  /**
+   * Randomize synonyms to later sample a subset. Uses constant random seed
+   * for reproducibility. Uses "DRand", a simple, fast, uniform pseudo-random
+   * number generator with medium statistical quality (multiplicative
+   * congruential method), producing integers in the range [Integer.MIN_VALUE,
+   * Integer.MAX_VALUE].
+   */
+  private static void randomize(Object[] arr) {
+    int seed = 1234567; // constant
+    int randomState = 4*seed + 1;
+//    Random random = new Random(seed); // unnecessary overhead
+    int len = arr.length;
+    for (int i=0; i < len-1; i++) {
+      randomState *= 0x278DDE6D; // z(i+1)=a*z(i) (mod 2**32)
+      int r = randomState % (len-i);
+      if (r < 0) r = -r; // e.g. -9 % 2 == -1
+//      int r = random.nextInt(len-i);
+      
+      // swap arr[i, i+r]
+      Object tmp = arr[i];
+      arr[i] = arr[i + r];
+      arr[i + r] = tmp;
+    }   
+  }
+
+  public void reset() throws IOException {
+    super.reset();
+    stack = null;
+    index = 0;
+    current = null;
+    todo = 0;
+  }
+}
diff --git a/contrib/wordnet/src/java/org/apache/lucene/wordnet/package.html b/contrib/wordnet/src/java/org/apache/lucene/wordnet/package.html
index 631fb90..19c5b57 100755
--- a/contrib/wordnet/src/java/org/apache/lucene/wordnet/package.html
+++ b/contrib/wordnet/src/java/org/apache/lucene/wordnet/package.html
@@ -21,8 +21,14 @@
 </head>
 <body>
 
-    This package uses synonyms defined by <a href="http://www.cogsci.princeton.edu/~wn/">WordNet</a> to build a
-    Lucene index storing them, which in turn can be used for query expansion.
+    This package uses synonyms defined by <a href="http://www.cogsci.princeton.edu/~wn/">WordNet</a>.
+    There are two methods: query expansion and analysis.
+    
+    Both methods first require you to download the <a href="http://www.cogsci.princeton.edu/2.0/WNprolog-2.0.tar.gz">WordNet prolog database</a>
+    Inside this archive is a file named wn_s.pl, which contains the WordNet synonyms. 
+    
+    <h1>Query Expansion Method</h1>
+    This method creates Lucene index storing the synonyms, which in turn can be used for query expansion.
 
     You normally run {@link org.apache.lucene.wordnet.Syns2Index} once to build the query index/"database", and then call
     {@link org.apache.lucene.wordnet.SynExpand#expand SynExpand.expand(...)} to expand a query.
@@ -31,12 +37,21 @@
 
 	<h3> Instructions </h3>
 	<ol>
-	    <li> Download the <a href="http://www.cogsci.princeton.edu/2.0/WNprolog-2.0.tar.gz">WordNet prolog database</a> , gunzip, untar etc.
 	<li> Invoke Syn2Index as appropriate to build a synonym index.
-	    It'll take 2 arguments, the path to wn_s.pl from that WordNet download, and the index name.
+	    It'll take 2 arguments, the path to wn_s.pl from the WordNet download, and the index name.
    
 	 <li> Update your UI so that as appropriate you call SynExpand.expand(...) to expand user queries with synonyms.
        </ol>
+    
+    <h1>Analysis Method</h1>
+    This method injects additional synonym tokens for tokens from a child {@link org.apache.lucene.analysis.TokenStream}.
+    
+    <h3> Instructions </h3>
+    <ol>
+    	<li>Create a {@link org.apache.lucene.wordnet.SynonymMap}, passing in the path to wn_s.pl
+    	<li>Add a {@link org.apache.lucene.wordnet.SynonymTokenFilter} to your analyzer. Note: SynonymTokenFilter should be after LowerCaseFilter, 
+    	because it expects terms to already be in lowercase.
+    </ol>
 
 </body>
     </html>
\ No newline at end of file
diff --git a/contrib/wordnet/src/test/org/apache/lucene/wordnet/TestSynonymTokenFilter.java b/contrib/wordnet/src/test/org/apache/lucene/wordnet/TestSynonymTokenFilter.java
new file mode 100644
index 0000000..ccbab5c
--- /dev/null
+++ b/contrib/wordnet/src/test/org/apache/lucene/wordnet/TestSynonymTokenFilter.java
@@ -0,0 +1,125 @@
+package org.apache.lucene.wordnet;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.Reader;
+import java.io.StringReader;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.WhitespaceTokenizer;
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+
+public class TestSynonymTokenFilter extends BaseTokenStreamTestCase {
+  File dataDir = new File(System.getProperty("dataDir", "./bin"));
+  File testFile = new File(dataDir, "org/apache/lucene/wordnet/testSynonyms.txt");
+  
+  public void testSynonyms() throws Exception {
+    SynonymMap map = new SynonymMap(new FileInputStream(testFile));
+    /* all expansions */
+    Analyzer analyzer = new SynonymWhitespaceAnalyzer(map, Integer.MAX_VALUE);
+    assertAnalyzesTo(analyzer, "Lost in the woods",
+        new String[] { "lost", "in", "the", "woods", "forest", "wood" },
+        new int[] { 0, 5, 8, 12, 12, 12 },
+        new int[] { 4, 7, 11, 17, 17, 17 },
+        new int[] { 1, 1, 1, 1, 0, 0 });
+  }
+  
+  public void testSynonymsSingleQuote() throws Exception {
+    SynonymMap map = new SynonymMap(new FileInputStream(testFile));
+    /* all expansions */
+    Analyzer analyzer = new SynonymWhitespaceAnalyzer(map, Integer.MAX_VALUE);
+    assertAnalyzesTo(analyzer, "king",
+        new String[] { "king", "baron" });
+  }
+  
+  public void testSynonymsLimitedAmount() throws Exception {
+    SynonymMap map = new SynonymMap(new FileInputStream(testFile));
+    /* limit to one synonym expansion */
+    Analyzer analyzer = new SynonymWhitespaceAnalyzer(map, 1);
+    assertAnalyzesTo(analyzer, "Lost in the woods",
+        /* wood comes before forest due to 
+         * the input file, not lexicographic order
+         */
+        new String[] { "lost", "in", "the", "woods", "wood" },
+        new int[] { 0, 5, 8, 12, 12 },
+        new int[] { 4, 7, 11, 17, 17 },
+        new int[] { 1, 1, 1, 1, 0 });
+  }
+  
+  public void testReusableTokenStream() throws Exception {
+    SynonymMap map = new SynonymMap(new FileInputStream(testFile));
+    /* limit to one synonym expansion */
+    Analyzer analyzer = new SynonymWhitespaceAnalyzer(map, 1);
+    assertAnalyzesToReuse(analyzer, "Lost in the woods",
+        new String[] { "lost", "in", "the", "woods", "wood" },
+        new int[] { 0, 5, 8, 12, 12 },
+        new int[] { 4, 7, 11, 17, 17 },
+        new int[] { 1, 1, 1, 1, 0 });
+    assertAnalyzesToReuse(analyzer, "My wolfish dog went to the forest",
+        new String[] { "my", "wolfish", "ravenous", "dog", "went", "to",
+          "the", "forest", "woods" },
+        new int[] { 0, 3, 3, 11, 15, 20, 23, 27, 27 },
+        new int[] { 2, 10, 10, 14, 19, 22, 26, 33, 33 },
+        new int[] { 1, 1, 0, 1, 1, 1, 1, 1, 0 });
+  }
+  
+  private class SynonymWhitespaceAnalyzer extends Analyzer {
+    private SynonymMap synonyms;
+    private int maxSynonyms;
+    
+    public SynonymWhitespaceAnalyzer(SynonymMap synonyms, int maxSynonyms) {
+      this.synonyms = synonyms;
+      this.maxSynonyms = maxSynonyms;
+    }
+    
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      TokenStream ts = new WhitespaceTokenizer(reader);
+      ts = new LowerCaseFilter(ts);
+      ts = new SynonymTokenFilter(ts, synonyms, maxSynonyms);
+      return ts;
+    }
+    
+    private class SavedStreams {
+      Tokenizer source;
+      TokenStream result;
+    };
+    
+    public TokenStream reusableTokenStream(String fieldName, Reader reader)
+        throws IOException {
+      SavedStreams streams = (SavedStreams) getPreviousTokenStream();
+      if (streams == null) {
+        streams = new SavedStreams();
+        streams.source = new WhitespaceTokenizer(reader);
+        streams.result = new LowerCaseFilter(streams.source);
+        streams.result = new SynonymTokenFilter(streams.result, synonyms, maxSynonyms);
+        setPreviousTokenStream(streams);
+      } else {
+        streams.source.reset(reader);
+        streams.result.reset(); // reset the SynonymTokenFilter
+      }
+      return streams.result;
+    }
+  }
+  
+}

