GitDiffStart: a9c6d278aaa73f9efda7c850cee294dff1d5142a | Sat Jan 24 23:31:33 2015 +0000
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java b/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
index 858af2c..521dd90 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
@@ -28,6 +28,7 @@ import org.apache.lucene.document.SortedNumericDocValuesField;
 import org.apache.lucene.document.SortedSetDocValuesField;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LineFileDocs;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.TestUtil;
@@ -43,7 +44,9 @@ public class TestDuelingCodecs extends LuceneTestCase {
   private Directory rightDir;
   private IndexReader rightReader;
   private Codec rightCodec;
-  
+  private RandomIndexWriter leftWriter;
+  private RandomIndexWriter rightWriter;
+  private long seed;
   private String info;  // for debugging
 
   @Override
@@ -57,10 +60,10 @@ public class TestDuelingCodecs extends LuceneTestCase {
     leftCodec = Codec.forName("SimpleText");
     rightCodec = new RandomCodec(random());
 
-    leftDir = newDirectory();
-    rightDir = newDirectory();
+    leftDir = newFSDirectory(createTempDir("leftDir"));
+    rightDir = newFSDirectory(createTempDir("rightDir"));
 
-    long seed = random().nextLong();
+    seed = random().nextLong();
 
     // must use same seed because of random payloads, etc
     int maxTermLength = TestUtil.nextInt(random(), 1, IndexWriter.MAX_TERM_LENGTH);
@@ -82,44 +85,23 @@ public class TestDuelingCodecs extends LuceneTestCase {
     rightConfig.setMergePolicy(newLogMergePolicy());
 
     // must use same seed because of random docvalues fields, etc
-    RandomIndexWriter leftWriter = new RandomIndexWriter(new Random(seed), leftDir, leftConfig);
-    RandomIndexWriter rightWriter = new RandomIndexWriter(new Random(seed), rightDir, rightConfig);
-    
-    int numdocs = atLeast(100);
-    createRandomIndex(numdocs, leftWriter, seed);
-    createRandomIndex(numdocs, rightWriter, seed);
+    leftWriter = new RandomIndexWriter(new Random(seed), leftDir, leftConfig);
+    rightWriter = new RandomIndexWriter(new Random(seed), rightDir, rightConfig);
 
-    leftReader = maybeWrapReader(leftWriter.getReader());
-    leftWriter.close();
-    rightReader = maybeWrapReader(rightWriter.getReader());
-    rightWriter.close();
-    
-    // check that our readers are valid
-    TestUtil.checkReader(leftReader);
-    TestUtil.checkReader(rightReader);
-    
     info = "left: " + leftCodec.toString() + " / right: " + rightCodec.toString();
   }
   
   @Override
   public void tearDown() throws Exception {
-    if (leftReader != null) {
-      leftReader.close();
-    }
-    if (rightReader != null) {
-      rightReader.close();   
-    }
-
-    if (leftDir != null) {
-      leftDir.close();
-    }
-    if (rightDir != null) {
-      rightDir.close();
-    }
-    
+    IOUtils.close(leftWriter,
+                  rightWriter,
+                  leftReader,
+                  rightReader,
+                  leftDir,
+                  rightDir);
     super.tearDown();
   }
-  
+
   /**
    * populates a writer with random stuff. this must be fully reproducable with the seed!
    */
@@ -162,7 +144,28 @@ public class TestDuelingCodecs extends LuceneTestCase {
    * checks the two indexes are equivalent
    */
   public void testEquals() throws IOException {
+    int numdocs = TEST_NIGHTLY ? atLeast(2000) : atLeast(100);
+    createRandomIndex(numdocs, leftWriter, seed);
+    createRandomIndex(numdocs, rightWriter, seed);
+
+    leftReader = leftWriter.getReader();
+    rightReader = rightWriter.getReader();
+    
     assertReaderEquals(info, leftReader, rightReader);
   }
 
+  public void testCrazyReaderEquals() throws IOException {
+    int numdocs = atLeast(100);
+    createRandomIndex(numdocs, leftWriter, seed);
+    createRandomIndex(numdocs, rightWriter, seed);
+
+    leftReader = wrapReader(leftWriter.getReader());
+    rightReader = wrapReader(rightWriter.getReader());
+    
+    // check that our readers are valid
+    TestUtil.checkReader(leftReader);
+    TestUtil.checkReader(rightReader);
+    
+    assertReaderEquals(info, leftReader, rightReader);
+  }
 }
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
index e4ab151..dd6b303 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
@@ -89,6 +89,7 @@ import org.apache.lucene.index.LogDocMergePolicy;
 import org.apache.lucene.index.LogMergePolicy;
 import org.apache.lucene.index.MergePolicy;
 import org.apache.lucene.index.MergeScheduler;
+import org.apache.lucene.index.MergeTrigger;
 import org.apache.lucene.index.MismatchedDirectoryReader;
 import org.apache.lucene.index.MismatchedLeafReader;
 import org.apache.lucene.index.MockRandomMergePolicy;
@@ -1557,78 +1558,85 @@ public abstract class LuceneTestCase extends Assert {
       throw null; // dummy to prevent compiler failure
     }
   }
+
+  public static IndexReader wrapReader(IndexReader r) throws IOException {
+    Random random = random();
+      
+    // TODO: remove this, and fix those tests to wrap before putting slow around:
+    final boolean wasOriginallyAtomic = r instanceof LeafReader;
+    for (int i = 0, c = random.nextInt(6)+1; i < c; i++) {
+      switch(random.nextInt(6)) {
+      case 0:
+        r = SlowCompositeReaderWrapper.wrap(r);
+        break;
+      case 1:
+        // will create no FC insanity in atomic case, as ParallelLeafReader has own cache key:
+        r = (r instanceof LeafReader) ?
+          new ParallelLeafReader((LeafReader) r) :
+        new ParallelCompositeReader((CompositeReader) r);
+        break;
+      case 2:
+        // Häckidy-Hick-Hack: a standard MultiReader will cause FC insanity, so we use
+        // QueryUtils' reader with a fake cache key, so insanity checker cannot walk
+        // along our reader:
+        r = new FCInvisibleMultiReader(r);
+        break;
+      case 3:
+        final LeafReader ar = SlowCompositeReaderWrapper.wrap(r);
+        final List<String> allFields = new ArrayList<>();
+        for (FieldInfo fi : ar.getFieldInfos()) {
+          allFields.add(fi.name);
+        }
+        Collections.shuffle(allFields, random);
+        final int end = allFields.isEmpty() ? 0 : random.nextInt(allFields.size());
+        final Set<String> fields = new HashSet<>(allFields.subList(0, end));
+        // will create no FC insanity as ParallelLeafReader has own cache key:
+        r = new ParallelLeafReader(
+                                   new FieldFilterLeafReader(ar, fields, false),
+                                   new FieldFilterLeafReader(ar, fields, true)
+                                   );
+        break;
+      case 4:
+        // Häckidy-Hick-Hack: a standard Reader will cause FC insanity, so we use
+        // QueryUtils' reader with a fake cache key, so insanity checker cannot walk
+        // along our reader:
+        if (r instanceof LeafReader) {
+          r = new AssertingLeafReader((LeafReader)r);
+        } else if (r instanceof DirectoryReader) {
+          r = new AssertingDirectoryReader((DirectoryReader)r);
+        }
+        break;
+      case 5:
+        if (r instanceof LeafReader) {
+          r = new MismatchedLeafReader((LeafReader)r, random);
+        } else if (r instanceof DirectoryReader) {
+          r = new MismatchedDirectoryReader((DirectoryReader)r, random);
+        }
+        break;
+      default:
+        fail("should not get here");
+      }
+    }
+    if (wasOriginallyAtomic) {
+      r = SlowCompositeReaderWrapper.wrap(r);
+    } else if ((r instanceof CompositeReader) && !(r instanceof FCInvisibleMultiReader)) {
+      // prevent cache insanity caused by e.g. ParallelCompositeReader, to fix we wrap one more time:
+      r = new FCInvisibleMultiReader(r);
+    }
+    if (VERBOSE) {
+      System.out.println("wrapReader wrapped: " +r);
+    }
+
+    return r;
+  }
   
   /**
    * Sometimes wrap the IndexReader as slow, parallel or filter reader (or
    * combinations of that)
    */
   public static IndexReader maybeWrapReader(IndexReader r) throws IOException {
-    Random random = random();
     if (rarely()) {
-      // TODO: remove this, and fix those tests to wrap before putting slow around:
-      final boolean wasOriginallyAtomic = r instanceof LeafReader;
-      for (int i = 0, c = random.nextInt(6)+1; i < c; i++) {
-        switch(random.nextInt(6)) {
-          case 0:
-            r = SlowCompositeReaderWrapper.wrap(r);
-            break;
-          case 1:
-            // will create no FC insanity in atomic case, as ParallelLeafReader has own cache key:
-            r = (r instanceof LeafReader) ?
-              new ParallelLeafReader((LeafReader) r) :
-              new ParallelCompositeReader((CompositeReader) r);
-            break;
-          case 2:
-            // Häckidy-Hick-Hack: a standard MultiReader will cause FC insanity, so we use
-            // QueryUtils' reader with a fake cache key, so insanity checker cannot walk
-            // along our reader:
-            r = new FCInvisibleMultiReader(r);
-            break;
-          case 3:
-            final LeafReader ar = SlowCompositeReaderWrapper.wrap(r);
-            final List<String> allFields = new ArrayList<>();
-            for (FieldInfo fi : ar.getFieldInfos()) {
-              allFields.add(fi.name);
-            }
-            Collections.shuffle(allFields, random);
-            final int end = allFields.isEmpty() ? 0 : random.nextInt(allFields.size());
-            final Set<String> fields = new HashSet<>(allFields.subList(0, end));
-            // will create no FC insanity as ParallelLeafReader has own cache key:
-            r = new ParallelLeafReader(
-              new FieldFilterLeafReader(ar, fields, false),
-              new FieldFilterLeafReader(ar, fields, true)
-            );
-            break;
-          case 4:
-            // Häckidy-Hick-Hack: a standard Reader will cause FC insanity, so we use
-            // QueryUtils' reader with a fake cache key, so insanity checker cannot walk
-            // along our reader:
-            if (r instanceof LeafReader) {
-              r = new AssertingLeafReader((LeafReader)r);
-            } else if (r instanceof DirectoryReader) {
-              r = new AssertingDirectoryReader((DirectoryReader)r);
-            }
-            break;
-          case 5:
-            if (r instanceof LeafReader) {
-              r = new MismatchedLeafReader((LeafReader)r, random);
-            } else if (r instanceof DirectoryReader) {
-              r = new MismatchedDirectoryReader((DirectoryReader)r, random);
-            }
-            break;
-          default:
-            fail("should not get here");
-        }
-      }
-      if (wasOriginallyAtomic) {
-        r = SlowCompositeReaderWrapper.wrap(r);
-      } else if ((r instanceof CompositeReader) && !(r instanceof FCInvisibleMultiReader)) {
-        // prevent cache insanity caused by e.g. ParallelCompositeReader, to fix we wrap one more time:
-        r = new FCInvisibleMultiReader(r);
-      }
-      if (VERBOSE) {
-        System.out.println("maybeWrapReader wrapped: " +r);
-      }
+      r = wrapReader(r);
     }
     return r;
   }

