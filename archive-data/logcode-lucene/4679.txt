GitDiffStart: 2189b7a761d54426252a2763c4b9e59899f78215 | Wed Dec 24 05:48:58 2014 +0000
diff --git a/dev-tools/scripts/checkJavadocLinks.py b/dev-tools/scripts/checkJavadocLinks.py
index 6a710f6..f57e7eb 100644
--- a/dev-tools/scripts/checkJavadocLinks.py
+++ b/dev-tools/scripts/checkJavadocLinks.py
@@ -40,7 +40,7 @@ class FindHyperlinks(HTMLParser):
 
   def handle_starttag(self, tag, attrs):
     # NOTE: I don't think 'a' should be in here. But try debugging 
-    # NumericRangeQuery.html. (Could be javadocs bug, its a generic type...)
+    # NumericRangeQuery.html. (Could be javadocs bug, it's a generic type...)
     if tag not in ('link', 'meta', 'frame', 'br', 'hr', 'p', 'li', 'img', 'col', 'a'):
       self.stack.append(tag)
     if tag == 'a':
diff --git a/dev-tools/scripts/smokeTestRelease.py b/dev-tools/scripts/smokeTestRelease.py
index 2e1c18f..d8c9c30 100644
--- a/dev-tools/scripts/smokeTestRelease.py
+++ b/dev-tools/scripts/smokeTestRelease.py
@@ -402,7 +402,7 @@ def checkSigs(project, urlString, version, tmpDir, isSigned):
       logFile = '%s/%s.%s.gpg.verify.log' % (tmpDir, project, artifact)
       run('gpg --homedir %s --verify %s %s' % (gpgHomeDir, sigFile, artifactFile),
           logFile)
-      # Forward any GPG warnings, except the expected one (since its a clean world)
+      # Forward any GPG warnings, except the expected one (since it's a clean world)
       f = open(logFile, encoding='UTF-8')
       for line in f.readlines():
         if line.lower().find('warning') != -1 \
@@ -1132,7 +1132,7 @@ def verifyMavenSigs(baseURL, tmpDir, artifacts):
       logFile = '%s/%s.%s.gpg.verify.log' % (tmpDir, project, artifact)
       run('gpg --homedir %s --verify %s %s' % (gpgHomeDir, sigFile, artifactFile),
           logFile)
-      # Forward any GPG warnings, except the expected one (since its a clean world)
+      # Forward any GPG warnings, except the expected one (since it's a clean world)
       f = open(logFile, encoding='UTF-8')
       for line in f.readlines():
         if line.lower().find('warning') != -1 \
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index a227f29..a85a553 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -553,7 +553,7 @@ New Features
 * LUCENE-5825: Benchmark module can use custom postings format, e.g.:
  codec.postingsFormat=Memory (Varun Shenoy, David Smiley)
 
-* LUCENE-5842: When opening large files (where its to expensive to compare
+* LUCENE-5842: When opening large files (where it's too expensive to compare
   checksum against all the bytes), retrieve checksum to validate structure
   of footer, this can detect some forms of corruption such as truncation.
   (Robert Muir)
@@ -1232,7 +1232,7 @@ Bug fixes
 
 * LUCENE-5483: Fix inaccuracies in HunspellStemFilter. Multi-stage affix-stripping,
   prefix-suffix dependencies, and COMPLEXPREFIXES now work correctly according
-  to the hunspell algorithm. Removed recursionCap parameter, as its no longer needed, rules for
+  to the hunspell algorithm. Removed recursionCap parameter, as it's no longer needed, rules for
   recursive affix application are driven correctly by continuation classes in the affix file.
   (Robert Muir)
 
@@ -1262,7 +1262,7 @@ Bug fixes
 
 * LUCENE-5612: NativeFSLockFactory no longer deletes its lock file. This cannot be done
   safely without the risk of deleting someone else's lock file. If you use NativeFSLockFactory,
-  you may see write.lock hanging around from time to time: its harmless.  
+  you may see write.lock hanging around from time to time: it's harmless.  
   (Uwe Schindler, Mike McCandless, Robert Muir)
 
 * LUCENE-5624: Ensure NativeFSLockFactory does not leak file handles if it is unable
@@ -2273,7 +2273,7 @@ Changes in backwards compatibility policy
 * LUCENE-4933: Replace ExactSimScorer/SloppySimScorer with just SimScorer. Previously
   there were 2 implementations as a performance hack to support tableization of
   sqrt(), but this caching is removed, as sqrt is implemented in hardware with modern 
-  jvms and its faster not to cache.  (Robert Muir)
+  jvms and it's faster not to cache.  (Robert Muir)
 
 * LUCENE-5038: MergePolicy now has a default implementation for useCompoundFile based
   on segment size and noCFSRatio. The default implemantion was pulled up from
@@ -2369,7 +2369,7 @@ Bug Fixes
   SortedSetDocValuesReaderState and SortedSetDocValuesAccumulator.
   (Robert Muir, Mike McCandless)
 
-* LUCENE-5120: AnalyzingSuggester modifed it's FST's cached root arc if payloads
+* LUCENE-5120: AnalyzingSuggester modified its FST's cached root arc if payloads
   are used and the entire output resided on the root arc on the first access. This
   caused subsequent suggest calls to fail. (Simon Willnauer)
 
@@ -2943,9 +2943,9 @@ Changes in backwards compatibility policy
     use a different DocValuesFormat per field (like postings).
   - Unified with FieldCache api: DocValues can be accessed via FieldCache API,
     so it works automatically with grouping/join/sort/function queries, etc.
-  - Simplified types: There are only 3 types (NUMERIC, BINARY, SORTED), so its
+  - Simplified types: There are only 3 types (NUMERIC, BINARY, SORTED), so it's
     not necessary to specify for example that all of your binary values have
-    the same length. Instead its easy for the Codec API to optimize encoding
+    the same length. Instead it's easy for the Codec API to optimize encoding
     based on any properties of the content.
    (Simon Willnauer, Adrien Grand, Mike McCandless, Robert Muir)
 
@@ -3565,7 +3565,7 @@ Optimizations
   ForUtil.skipBlock.  (Robert Muir)
 
 * LUCENE-4497: Don't write PosVIntCount to the positions file in 
-  Lucene41PostingsFormat, as its always totalTermFreq % BLOCK_SIZE. (Robert Muir)
+  Lucene41PostingsFormat, as it's always totalTermFreq % BLOCK_SIZE. (Robert Muir)
 
 * LUCENE-4498: In Lucene41PostingsFormat, when a term appears in only one document, 
   Instead of writing a file pointer to a VIntBlock containing the doc id, just 
@@ -3679,7 +3679,7 @@ API Changes
   (Uwe Schindler, Robert Muir)
 
 * LUCENE-4304: removed PayloadProcessorProvider. If you want to change
-  payloads (or other things) when merging indexes, its recommended
+  payloads (or other things) when merging indexes, it's recommended
   to just use a FilterAtomicReader + IndexWriter.addIndexes. See the
   OrdinalMappingAtomicReader and TaxonomyMergeUtils in the facets
   module if you want an example of this.
@@ -3703,7 +3703,7 @@ API Changes
   overdelegates (read(), read(char[], int, int), skip, etc). This made it
   hard to implement CharFilters that were correct. Instead only close() is
   delegated by default: read(char[], int, int) and correct(int) are abstract
-  so that its obvious which methods you should implement.  The protected 
+  so that it's obvious which methods you should implement.  The protected 
   inner Reader is 'input' like CharFilter in the 3.x series, instead of 'in'.  
   (Dawid Weiss, Uwe Schindler, Robert Muir)
 
@@ -3759,7 +3759,7 @@ Bug Fixes
 
 * SOLR-3737: StempelPolishStemFilterFactory loaded its stemmer table incorrectly.
   Also, ensure immutability and use only one instance of this table in RAM (lazy
-  loaded) since its quite large. (sausarkar, Steven Rowe, Robert Muir)
+  loaded) since it's quite large. (sausarkar, Steven Rowe, Robert Muir)
 
 * LUCENE-4310: MappingCharFilter was failing to match input strings
   containing non-BMP Unicode characters.  (Dawid Weiss, Robert Muir,
@@ -3901,7 +3901,7 @@ API Changes
   of embedded morfologik dictionaries to version 1.9. (Dawid Weiss)
 
 * LUCENE-4178: set 'tokenized' to true on FieldType by default, so that if you
-  make a custom FieldType and set indexed = true, its analyzed by the analyzer.
+  make a custom FieldType and set indexed = true, it's analyzed by the analyzer.
   (Robert Muir)
 
 * LUCENE-4220: Removed the buggy JavaCC-based HTML parser in the benchmark
@@ -4222,7 +4222,7 @@ Changes in backwards compatibility policy
   migration notes in MIGRATE.txt for more details.  (Robert Muir, Doron Cohen)
 
 * LUCENE-2315: AttributeSource's methods for accessing attributes are now final,
-  else its easy to corrupt the internal states.  (Uwe Schindler)
+  else it's easy to corrupt the internal states.  (Uwe Schindler)
 
 * LUCENE-2814: The IndexWriter.flush method no longer takes "boolean
   flushDocStores" argument, as we now always flush doc stores (index
@@ -4786,7 +4786,7 @@ New features
     DefaultSimilarity, so you can easily try these out/switch back and 
     forth/run experiments and comparisons without reindexing. Note: most of 
     the models do rely upon index statistics that are new in Lucene 4.0, so 
-    for existing 3.x indexes its a good idea to upgrade your index to the 
+    for existing 3.x indexes it's a good idea to upgrade your index to the 
     new format with IndexUpgrader first.
 
   - Added a new subclass SimilarityBase which provides a simplified API 
@@ -6702,7 +6702,7 @@ New features
 
 * LUCENE-2754, LUCENE-2757: Added a wrapper around MultiTermQueries
   to add span support: SpanMultiTermQueryWrapper<Q extends MultiTermQuery>.
-  Using this wrapper its easy to add fuzzy/wildcard to e.g. a SpanNearQuery.
+  Using this wrapper it's easy to add fuzzy/wildcard to e.g. a SpanNearQuery.
   (Robert Muir, Uwe Schindler)
 
 * LUCENE-2838: ConstantScoreQuery now directly supports wrapping a Query
@@ -8367,7 +8367,7 @@ New features
     diagnostic information about the possibility of inconsistent
     FieldCache usage.  Namely: FieldCache entries for the same field
     with different datatypes or parsers; and FieldCache entries for
-    the same field in both a reader, and one of it's (descendant) sub
+    the same field in both a reader, and one of its (descendant) sub
     readers. 
     (Chris Hostetter, Mark Miller)
 
@@ -8588,7 +8588,7 @@ Changes in backwards compatibility policy
 1. LUCENE-1340: In a minor change to Lucene's backward compatibility
    policy, we are now allowing the Fieldable interface to have
    changes, within reason, and made on a case-by-case basis.  If an
-   application implements it's own Fieldable, please be aware of
+   application implements its own Fieldable, please be aware of
    this.  Otherwise, no need to be concerned.  This is in effect for
    all 2.X releases, starting with 2.4.  Also note, that in all
    likelihood, Fieldable will be changed in 3.0.
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter.java
index 734b7a3..f92023c 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKBigramFilter.java
@@ -56,9 +56,9 @@ public final class CJKBigramFilter extends TokenFilter {
   /** bigram flag for Hangul */
   public static final int HANGUL = 8;
 
-  /** when we emit a bigram, its then marked as this type */
+  /** when we emit a bigram, it's then marked as this type */
   public static final String DOUBLE_TYPE = "<DOUBLE>";
-  /** when we emit a unigram, its then marked as this type */
+  /** when we emit a unigram, it's then marked as this type */
   public static final String SINGLE_TYPE = "<SINGLE>";
 
   // the types from standardtokenizer
@@ -199,7 +199,7 @@ public final class CJKBigramFilter extends TokenFilter {
           if (hasBufferedUnigram()) {
             
             // we have a buffered unigram, and we peeked ahead to see if we could form
-            // a bigram, but we can't, because its not a CJK type. capture the state 
+            // a bigram, but we can't, because it's not a CJK type. capture the state 
             // of this peeked data to be revisited next time thru the loop, and dump our unigram.
             
             loneState = captureState();
@@ -213,7 +213,7 @@ public final class CJKBigramFilter extends TokenFilter {
         // case 3: we have only zero or 1 codepoints buffered, 
         // so not enough to form a bigram. But, we also have no
         // more input. So if we have a buffered codepoint, emit
-        // a unigram, otherwise, its end of stream.
+        // a unigram, otherwise, it's end of stream.
         
         if (hasBufferedUnigram()) {
           flushUnigram(); // flush our remaining unigram
@@ -345,7 +345,7 @@ public final class CJKBigramFilter extends TokenFilter {
       // when outputting unigrams always
       return bufferLen - index == 1;
     } else {
-      // otherwise its only when we have a lone CJK character
+      // otherwise it's only when we have a lone CJK character
       return bufferLen == 1 && index == 0;
     }
   }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/CommonGramsFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/CommonGramsFilter.java
index d04e1b8..2d53b3f 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/CommonGramsFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/commongrams/CommonGramsFilter.java
@@ -112,7 +112,7 @@ public final class CommonGramsFilter extends TokenFilter {
     
     /* We build n-grams before and after stopwords. 
      * When valid, the buffer always contains at least the separator.
-     * If its empty, there is nothing before this stopword.
+     * If it's empty, there is nothing before this stopword.
      */
     if (lastWasCommon || (isCommon() && buffer.length() > 0)) {
       savedState = captureState();
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanStemmer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanStemmer.java
index db9d92d..36f6b95 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanStemmer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanStemmer.java
@@ -137,7 +137,7 @@ public class GermanStemmer
         strip( buffer );
       }
       // Additional step for irregular plural nouns like "Matrizen -> Matrix".
-      // NOTE: this length constraint is probably not a great value, its just to prevent AIOOBE on empty terms
+      // NOTE: this length constraint is probably not a great value, it's just to prevent AIOOBE on empty terms
       if ( buffer.length() > 0 && buffer.charAt( buffer.length() - 1 ) == ( 'z' ) ) {
         buffer.setCharAt( buffer.length() - 1, 'x' );
       }
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekStemmer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekStemmer.java
index 750bd35..fee2f67 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekStemmer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/el/GreekStemmer.java
@@ -34,7 +34,7 @@ public class GreekStemmer {
   
  /**
    * Stems a word contained in a leading portion of a char[] array.
-   * The word is passed through a number of rules that modify it's length.
+   * The word is passed through a number of rules that modify its length.
    * 
    * @param s A char[] array that contains the word to be stemmed.
    * @param len The length of the char[] array.
@@ -327,7 +327,7 @@ public class GreekStemmer {
     }
     
     if (removed && exc8a.contains(s, 0, len)) {
-      // add -αγαν (we removed > 4 chars so its safe)
+      // add -αγαν (we removed > 4 chars so it's safe)
       len += 4;
       s[len - 4] = 'α';
       s[len - 3] = 'γ';
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Dictionary.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Dictionary.java
index 84746c9..fccfd69 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Dictionary.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Dictionary.java
@@ -555,7 +555,7 @@ public class Dictionary {
         // already exists in our hash
         appendFlagsOrd = (-appendFlagsOrd)-1;
       } else if (appendFlagsOrd > Short.MAX_VALUE) {
-        // this limit is probably flexible, but its a good sanity check too
+        // this limit is probably flexible, but it's a good sanity check too
         throw new UnsupportedOperationException("Too many unique append flags, please report this to dev@lucene.apache.org");
       }
       
@@ -1027,7 +1027,7 @@ public class Dictionary {
   }
   
   private String parseStemException(String morphData) {
-    // first see if its an alias
+    // first see if it's an alias
     if (morphAliasCount > 0) {
       try {
         int alias = Integer.parseInt(morphData.trim());
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Stemmer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Stemmer.java
index c530614..5202fb4 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Stemmer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/hunspell/Stemmer.java
@@ -46,7 +46,7 @@ final class Stemmer {
   private final StringBuilder scratchSegment = new StringBuilder();
   private char scratchBuffer[] = new char[32];
   
-  // its '1' if we have no stem exceptions, otherwise every other form
+  // it's '1' if we have no stem exceptions, otherwise every other form
   // is really an ID pointing to the exception table
   private final int formStep;
   
@@ -134,7 +134,7 @@ final class Stemmer {
       return EXACT_CASE;
     }
     
-    // determine if we are title or lowercase (or something funky, in which its exact)
+    // determine if we are title or lowercase (or something funky, in which it's exact)
     boolean seenUpper = false;
     boolean seenLower = false;
     for (int i = 1; i < length; i++) {
@@ -183,7 +183,7 @@ final class Stemmer {
           if (checkKeepCase && Dictionary.hasFlag(wordFlags, (char)dictionary.keepcase)) {
             continue;
           }
-          // we can't add this form, its a pseudostem requiring an affix
+          // we can't add this form, it's a pseudostem requiring an affix
           if (checkNeedAffix && Dictionary.hasFlag(wordFlags, (char)dictionary.needaffix)) {
             continue;
           }
@@ -280,12 +280,12 @@ final class Stemmer {
    * @param word Word to generate the stems for
    * @param previous previous affix that was removed (so we dont remove same one twice)
    * @param prevFlag Flag from a previous stemming step that need to be cross-checked with any affixes in this recursive step
-   * @param prefixFlag flag of the most inner removed prefix, so that when removing a suffix, its also checked against the word
+   * @param prefixFlag flag of the most inner removed prefix, so that when removing a suffix, it's also checked against the word
    * @param recursionDepth current recursiondepth
    * @param doPrefix true if we should remove prefixes
    * @param doSuffix true if we should remove suffixes
    * @param previousWasPrefix true if the previous removal was a prefix:
-   *        if we are removing a suffix, and it has no continuation requirements, its ok.
+   *        if we are removing a suffix, and it has no continuation requirements, it's ok.
    *        but two prefixes (COMPLEXPREFIXES) or two suffixes must have continuation requirements to recurse. 
    * @param circumfix true if the previous prefix removal was signed as a circumfix
    *        this means inner most suffix must also contain circumfix flag.
@@ -501,7 +501,7 @@ final class Stemmer {
    * @param prefixFlag when we already stripped a prefix, we cant simply recurse and check the suffix, unless both are compatible
    *                   so we must check dictionary form against both to add it as a stem!
    * @param recursionDepth current recursion depth
-   * @param prefix true if we are removing a prefix (false if its a suffix)
+   * @param prefix true if we are removing a prefix (false if it's a suffix)
    * @return List of stems for the word, or an empty list if none are found
    */
   List<CharsRef> applyAffix(char strippedWord[], int length, int affix, int prefixFlag, int recursionDepth, boolean prefix, boolean circumfix, boolean caseVariant) throws IOException {    
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/lv/LatvianStemmer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/lv/LatvianStemmer.java
index efee85b..42ad3a1 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/lv/LatvianStemmer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/lv/LatvianStemmer.java
@@ -94,8 +94,8 @@ public class LatvianStemmer {
    * </ul>
    */
   private int unpalatalize(char s[], int len) {
-    // we check the character removed: if its -u then 
-    // its 2,5, or 6 gen pl., and these two can only apply then.
+    // we check the character removed: if it's -u then 
+    // it's 2,5, or 6 gen pl., and these two can only apply then.
     if (s[len] == 'u') {
       // kš -> kst
       if (endsWith(s, len, "kš")) {
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilterFactory.java
index 8eccee2..aa157f0 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilterFactory.java
@@ -33,7 +33,7 @@ import org.apache.lucene.analysis.util.TokenFilterFactory;
  * &lt;/fieldType&gt;</pre>
  * <p>
  * The {@code consumeAllTokens} property is optional and defaults to {@code false}.  
- * See {@link LimitTokenCountFilter} for an explanation of it's use.
+ * See {@link LimitTokenCountFilter} for an explanation of its use.
  */
 public class LimitTokenCountFilterFactory extends TokenFilterFactory {
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.java
index 26ab2d3..ce7746d 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.java
@@ -28,7 +28,7 @@ import java.io.IOException;
  * This filter folds Scandinavian characters å?äæ??-&gt;a and ö?ø?-&gt;o.
  * It also discriminate against use of double vowels aa, ae, ao, oe and oo, leaving just the first one.
  * <p/>
- * It's is a semantically more destructive solution than {@link ScandinavianNormalizationFilter} but
+ * It's a semantically more destructive solution than {@link ScandinavianNormalizationFilter} but
  * can in addition help with matching raksmorgas as räksmörgås.
  * <p/>
  * blåbærsyltetøj == blåbärsyltetöj == blaabaarsyltetoej == blabarsyltetoj
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.java
index a35dae1..aa00935 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/StemmerOverrideFilter.java
@@ -168,7 +168,7 @@ public final class StemmerOverrideFilter extends TokenFilter {
     }
     
     /**
-     * Adds an input string and it's stemmer override output to this builder.
+     * Adds an input string and its stemmer override output to this builder.
      * 
      * @param input the input char sequence 
      * @param output the stemmer override output char sequence
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.java
index f8bb629..03c86b7 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/path/ReversePathHierarchyTokenizer.java
@@ -154,7 +154,7 @@ public class ReversePathHierarchyTokenizer extends Tokenizer {
       resultToken.setLength(0);
       int idx = delimitersCount-1 - skip;
       if (idx >= 0) {
-        // otherwise its ok, because we will skip and return false
+        // otherwise it's ok, because we will skip and return false
         endPosition = delimiterPositions.get(idx);
       }
       finalOffset = correctOffset(length);
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser.java
index f74c1ff..c89c538 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SolrSynonymParser.java
@@ -120,7 +120,7 @@ public class SolrSynonymParser extends SynonymMap.Parser {
       
       // currently we include the term itself in the map,
       // and use includeOrig = false always.
-      // this is how the existing filter does it, but its actually a bug,
+      // this is how the existing filter does it, but it's actually a bug,
       // especially if combined with ignoreCase = true
       for (int i = 0; i < inputs.length; i++) {
         for (int j = 0; j < outputs.length; j++) {
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
index cf49bc5..f659c0e 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
@@ -258,7 +258,7 @@ public final class SynonymFilter extends TokenFilter {
    * @param input input tokenstream
    * @param synonyms synonym map
    * @param ignoreCase case-folds input for matching with {@link Character#toLowerCase(int)}.
-   *                   Note, if you set this to true, its your responsibility to lowercase
+   *                   Note, if you set this to true, it's your responsibility to lowercase
    *                   the input entries when you create the {@link SynonymMap}
    */
   public SynonymFilter(TokenStream input, SynonymMap synonyms, boolean ignoreCase) {
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
index 0a59208..e3b97d7 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/collation/CollationKeyAnalyzer.java
@@ -56,7 +56,7 @@ import java.text.Collator;
  * </ol> 
  * <p>
  *   The <code>ICUCollationKeyAnalyzer</code> in the analysis-icu package 
- *   uses ICU4J's Collator, which makes its
+ *   uses ICU4J's Collator, which makes
  *   its version available, thus allowing collation to be versioned
  *   independently from the JVM.  ICUCollationKeyAnalyzer is also significantly
  *   faster and generates significantly shorter keys than CollationKeyAnalyzer.
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestHyphenationCompoundWordTokenFilterFactory.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestHyphenationCompoundWordTokenFilterFactory.java
index abe9f96..d8dee0e 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestHyphenationCompoundWordTokenFilterFactory.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/compound/TestHyphenationCompoundWordTokenFilterFactory.java
@@ -49,7 +49,7 @@ public class TestHyphenationCompoundWordTokenFilterFactory extends BaseTokenStre
   /**
    * Ensure the factory works with no dictionary: using hyphenation grammar only.
    * Also change the min/max subword sizes from the default. When using no dictionary,
-   * its generally necessary to tweak these, or you get lots of expansions.
+   * it's generally necessary to tweak these, or you get lots of expansions.
    */
   public void testHyphenationOnly() throws Exception {
     Reader reader = new StringReader("basketballkurv");
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java
index 63df6be..97cb8b0 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestDuelingAnalyzers.java
@@ -80,7 +80,7 @@ public class TestDuelingAnalyzers extends BaseTokenStreamTestCase {
     }
   }
   
-  // not so useful since its all one token?!
+  // not so useful since it's all one token?!
   public void testLetterAsciiHuge() throws Exception {
     Random random = random();
     int maxLength = 8192; // CharTokenizer.IO_BUFFER_SIZE*2
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java
index 180faf8..d8a14f3 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestFactories.java
@@ -71,7 +71,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
       if (factory instanceof MultiTermAwareComponent) {
         AbstractAnalysisFactory mtc = ((MultiTermAwareComponent) factory).getMultiTermComponent();
         assertNotNull(mtc);
-        // its not ok to return e.g. a charfilter here: but a tokenizer could wrap a filter around it
+        // it's not ok to return e.g. a charfilter here: but a tokenizer could wrap a filter around it
         assertFalse(mtc instanceof CharFilterFactory);
       }
       
@@ -91,7 +91,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
       if (factory instanceof MultiTermAwareComponent) {
         AbstractAnalysisFactory mtc = ((MultiTermAwareComponent) factory).getMultiTermComponent();
         assertNotNull(mtc);
-        // its not ok to return a charfilter or tokenizer here, this makes no sense
+        // it's not ok to return a charfilter or tokenizer here, this makes no sense
         assertTrue(mtc instanceof TokenFilterFactory);
       }
       
@@ -111,7 +111,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
       if (factory instanceof MultiTermAwareComponent) {
         AbstractAnalysisFactory mtc = ((MultiTermAwareComponent) factory).getMultiTermComponent();
         assertNotNull(mtc);
-        // its not ok to return a tokenizer or tokenfilter here, this makes no sense
+        // it's not ok to return a tokenizer or tokenfilter here, this makes no sense
         assertTrue(mtc instanceof CharFilterFactory);
       }
       
@@ -141,7 +141,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
       throw new RuntimeException(e);
     } catch (InvocationTargetException e) {
       if (e.getCause() instanceof IllegalArgumentException) {
-        // its ok if we dont provide the right parameters to throw this
+        // it's ok if we dont provide the right parameters to throw this
         return null;
       }
     }
@@ -150,7 +150,7 @@ public class TestFactories extends BaseTokenStreamTestCase {
       try {
         ((ResourceLoaderAware) factory).inform(new StringMockResourceLoader(""));
       } catch (IOException ignored) {
-        // its ok if the right files arent available or whatever to throw this
+        // it's ok if the right files arent available or whatever to throw this
       } catch (IllegalArgumentException ignored) {
         // is this ok? I guess so
       }
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java
index c7b4c7e..a0eede0 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/cz/TestCzechStemmer.java
@@ -32,7 +32,7 @@ import org.apache.lucene.analysis.util.CharArraySet;
 /**
  * Test the Czech Stemmer.
  * 
- * Note: its algorithmic, so some stems are nonsense
+ * Note: it's algorithmic, so some stems are nonsense
  *
  */
 public class TestCzechStemmer extends BaseTokenStreamTestCase {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java
index e42ad3f..3d191d5 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenCountAnalyzer.java
@@ -43,7 +43,7 @@ public class TestLimitTokenCountAnalyzer extends BaseTokenStreamTestCase {
       mock.setEnableChecks(consumeAll);
       Analyzer a = new LimitTokenCountAnalyzer(mock, 2, consumeAll);
     
-      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!
+      // dont use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!
       assertTokenStreamContents(a.tokenStream("dummy", "1  2     3  4  5"), new String[] { "1", "2" }, new int[] { 0, 3 }, new int[] { 1, 4 }, consumeAll ? 16 : null);
       assertTokenStreamContents(a.tokenStream("dummy", "1 2 3 4 5"), new String[] { "1", "2" }, new int[] { 0, 2 }, new int[] { 1, 3 }, consumeAll ? 9 : null);
       
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter.java
index 1b541f3..56431fb 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLimitTokenPositionFilter.java
@@ -43,7 +43,7 @@ public class TestLimitTokenPositionFilter extends BaseTokenStreamTestCase {
         }
       };
 
-      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case its correct)!
+      // don't use assertAnalyzesTo here, as the end offset is not the end of the string (unless consumeAll is true, in which case it's correct)!
       assertTokenStreamContents(a.tokenStream("dummy", "1  2     3  4  5"),
           new String[]{"1", "2"}, new int[]{0, 3}, new int[]{1, 4}, consumeAll ? 16 : null);
       assertTokenStreamContents(a.tokenStream("dummy", new StringReader("1 2 3 4 5")),
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayIterator.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayIterator.java
index d593b3d..2c964a9 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayIterator.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharArrayIterator.java
@@ -31,7 +31,7 @@ public class TestCharArrayIterator extends LuceneTestCase {
   }
   
   public void testConsumeWordInstance() {
-    // we use the default locale, as its randomized by LuceneTestCase
+    // we use the default locale, as it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getWordInstance(Locale.getDefault());
     CharArrayIterator ci = CharArrayIterator.newWordInstance();
     for (int i = 0; i < 10000; i++) {
@@ -43,7 +43,7 @@ public class TestCharArrayIterator extends LuceneTestCase {
   
   /* run this to test if your JRE is buggy
   public void testWordInstanceJREBUG() {
-    // we use the default locale, as its randomized by LuceneTestCase
+    // we use the default locale, as it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getWordInstance(Locale.getDefault());
     Segment ci = new Segment();
     for (int i = 0; i < 10000; i++) {
@@ -61,7 +61,7 @@ public class TestCharArrayIterator extends LuceneTestCase {
   }
   
   public void testConsumeSentenceInstance() {
-    // we use the default locale, as its randomized by LuceneTestCase
+    // we use the default locale, as it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getSentenceInstance(Locale.getDefault());
     CharArrayIterator ci = CharArrayIterator.newSentenceInstance();
     for (int i = 0; i < 10000; i++) {
@@ -73,7 +73,7 @@ public class TestCharArrayIterator extends LuceneTestCase {
   
   /* run this to test if your JRE is buggy
   public void testSentenceInstanceJREBUG() {
-    // we use the default locale, as its randomized by LuceneTestCase
+    // we use the default locale, as it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getSentenceInstance(Locale.getDefault());
     Segment ci = new Segment();
     for (int i = 0; i < 10000; i++) {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
index 3e1abb3..13ec32b 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
@@ -28,7 +28,7 @@ import java.util.Locale;
 public class TestCollationKeyAnalyzer extends CollationTestBase {
   // the sort order of ? versus U depends on the version of the rules being used
   // for the inherited root locale: ?'s order isnt specified in Locale.US since 
-  // its not used in english.
+  // it's not used in english.
   private boolean oStrokeFirst = Collator.getInstance(new Locale("")).compare("?", "U") < 0;
   
   // Neither Java 1.4.2 nor 1.5.0 has Farsi Locale collation available in
diff --git a/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/BreakIteratorWrapper.java b/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/BreakIteratorWrapper.java
index 9a8a807..f7cbe50 100644
--- a/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/BreakIteratorWrapper.java
+++ b/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/BreakIteratorWrapper.java
@@ -59,7 +59,7 @@ abstract class BreakIteratorWrapper {
   }
 
   /**
-   * If its a RuleBasedBreakIterator, the rule status can be used for token type. If its
+   * If it's a RuleBasedBreakIterator, the rule status can be used for token type. If it's
    * any other BreakIterator, the rulestatus method is not available, so treat
    * it like a generic BreakIterator.
    */
@@ -71,7 +71,7 @@ abstract class BreakIteratorWrapper {
   }
 
   /**
-   * RuleBasedBreakIterator wrapper: RuleBasedBreakIterator (as long as its not
+   * RuleBasedBreakIterator wrapper: RuleBasedBreakIterator (as long as it's not
    * a DictionaryBasedBreakIterator) behaves correctly.
    */
   static final class RBBIWrapper extends BreakIteratorWrapper {
diff --git a/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ScriptIterator.java b/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ScriptIterator.java
index f573b19..4913f11 100644
--- a/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ScriptIterator.java
+++ b/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ScriptIterator.java
@@ -44,7 +44,7 @@ import com.ibm.icu.text.UTF16;
  * differences:
  * <ul>
  *  <li>Doesn't attempt to match paired punctuation. For tokenization purposes, this
- * is not necessary. Its also quite expensive. 
+ * is not necessary. It's also quite expensive. 
  *  <li>Non-spacing marks inherit the script of their base character, following 
  *  recommendations from UTR #24.
  * </ul>
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseReadingFormFilter.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseReadingFormFilter.java
index c353d37..ec92769 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseReadingFormFilter.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseReadingFormFilter.java
@@ -54,7 +54,7 @@ public final class JapaneseReadingFormFilter extends TokenFilter {
       
       if (useRomaji) {
         if (reading == null) {
-          // if its an OOV term, just try the term text
+          // if it's an OOV term, just try the term text
           buffer.setLength(0);
           ToStringUtil.getRomanization(buffer, termAttr);
           termAttr.setEmpty().append(buffer);
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java
index 1956c99..393faee 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/BinaryDictionary.java
@@ -289,7 +289,7 @@ public abstract class BinaryDictionary implements Dictionary {
     return new String(text);
   }
   
-  /** flag that the entry has baseform data. otherwise its not inflected (same as surface form) */
+  /** flag that the entry has baseform data. otherwise it's not inflected (same as surface form) */
   public static final int HAS_BASEFORM = 1;
   /** flag that the entry has reading data. otherwise reading is surface form converted to katakana */
   public static final int HAS_READING = 2;
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java
index c10e571..41af027 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/dict/UserDictionary.java
@@ -77,7 +77,7 @@ public final class UserDictionary implements Dictionary {
     }
     
     // TODO: should we allow multiple segmentations per input 'phrase'?
-    // the old treemap didn't support this either, and i'm not sure if its needed/useful?
+    // the old treemap didn't support this either, and i'm not sure if it's needed/useful?
 
     Collections.sort(featureEntries, new Comparator<String[]>() {
       @Override
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/TestTokenInfoDictionary.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/TestTokenInfoDictionary.java
index d08809d..3caa14f 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/TestTokenInfoDictionary.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/dict/TestTokenInfoDictionary.java
@@ -67,14 +67,14 @@ public class TestTokenInfoDictionary extends LuceneTestCase {
         String inflectionForm = tid.getInflectionForm(wordId);
         assertTrue(inflectionForm == null || UnicodeUtil.validUTF16String(inflectionForm));
         if (inflectionForm != null) {
-          // check that its actually an ipadic inflection form
+          // check that it's actually an ipadic inflection form
           assertNotNull(ToStringUtil.getInflectedFormTranslation(inflectionForm));          
         }
         
         String inflectionType = tid.getInflectionType(wordId);
         assertTrue(inflectionType == null || UnicodeUtil.validUTF16String(inflectionType));
         if (inflectionType != null) {
-          // check that its actually an ipadic inflection type
+          // check that it's actually an ipadic inflection type
           assertNotNull(ToStringUtil.getInflectionTypeTranslation(inflectionType));
         }
         
@@ -88,7 +88,7 @@ public class TestTokenInfoDictionary extends LuceneTestCase {
         String pos = tid.getPartOfSpeech(wordId);
         assertNotNull(pos);
         assertTrue(UnicodeUtil.validUTF16String(pos));
-        // check that its actually an ipadic pos tag
+        // check that it's actually an ipadic pos tag
         assertNotNull(ToStringUtil.getPOSTranslation(pos));
         
         String pronunciation = tid.getPronunciation(wordId, chars, 0, chars.length);
diff --git a/lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/BeiderMorseFilter.java b/lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/BeiderMorseFilter.java
index ba5c595..f199108 100644
--- a/lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/BeiderMorseFilter.java
+++ b/lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/BeiderMorseFilter.java
@@ -39,7 +39,7 @@ public final class BeiderMorseFilter extends TokenFilter {
   private final LanguageSet languages;
   
   // output is a string such as ab|ac|...
-  // in complex cases like d'angelo its (anZelo|andZelo|...)-(danZelo|...)
+  // in complex cases like d'angelo it's (anZelo|andZelo|...)-(danZelo|...)
   // if there are multiple 's, it starts to nest...
   private static final Pattern pattern = Pattern.compile("([^()|-]+)");
   
diff --git a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java b/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java
index 1be9ec2..bf88bd4 100644
--- a/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java
+++ b/lucene/analysis/smartcn/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java
@@ -47,7 +47,7 @@ class WordSegmenter {
     // tokens from sentence, excluding WordType.SENTENCE_BEGIN and WordType.SENTENCE_END
     List<SegToken> result = Collections.emptyList();
     
-    if (segTokenList.size() > 2) // if its not an empty sentence
+    if (segTokenList.size() > 2) // if it's not an empty sentence
       result = segTokenList.subList(1, segTokenList.size() - 1);
     
     for (SegToken st : result)
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java
index 70167de..dfbbaa2 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravRetLoadFieldSelectorTask.java
@@ -67,7 +67,7 @@ public class SearchTravRetLoadFieldSelectorTask extends SearchTravTask {
 
   @Override
   public void setParams(String params) {
-    this.params = params; // cannot just call super.setParams(), b/c it's params differ.
+    this.params = params; // cannot just call super.setParams(), b/c its params differ.
     fieldsToLoad = new HashSet<>();
     for (StringTokenizer tokenizer = new StringTokenizer(params, ","); tokenizer.hasMoreTokens();) {
       String s = tokenizer.nextToken();
diff --git a/lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java b/lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java
index f43c1f8..e413c9f 100644
--- a/lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java
+++ b/lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java
@@ -50,7 +50,7 @@ import org.apache.lucene.util.BytesRef;
 public class CachingNaiveBayesClassifier extends SimpleNaiveBayesClassifier {
   //for caching classes this will be the classification class list
   private ArrayList<BytesRef> cclasses = new ArrayList<>();
-  // its a term-inmap style map, where the inmap contains class-hit pairs to the
+  // it's a term-inmap style map, where the inmap contains class-hit pairs to the
   // upper term
   private Map<String, Map<BytesRef, Integer>> termCClassHitCache = new HashMap<>();
   // the term frequency in classes
@@ -185,7 +185,7 @@ public class CachingNaiveBayesClassifier extends SimpleNaiveBayesClassifier {
 
     Map<BytesRef, Integer> searched = new ConcurrentHashMap<>();
 
-    // if we dont get the answer, but its relevant we must search it and insert to the cache
+    // if we dont get the answer, but it's relevant we must search it and insert to the cache
     if (insertPoint != null || !justCachedTerms) {
       for (BytesRef cclass : cclasses) {
         BooleanQuery booleanQuery = new BooleanQuery();
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java
index 38b97f7..01bc480 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectDocValuesConsumer.java
@@ -262,7 +262,7 @@ class DirectDocValuesConsumer extends DocValuesConsumer {
     }
   }
 
-  // note: this might not be the most efficient... but its fairly simple
+  // note: this might not be the most efficient... but it's fairly simple
   @Override
   public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
     meta.writeVInt(field.number);
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java
index b6e5b1e..df40a37 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryDocValuesConsumer.java
@@ -196,7 +196,7 @@ class MemoryDocValuesConsumer extends DocValuesConsumer {
       int numBlocks = maxDoc / BLOCK_SIZE;
       float avgBPV = blockSum / (float)numBlocks;
       // just a heuristic, with tiny amounts of blocks our estimate is skewed as we ignore the final "incomplete" block.
-      // with at least 4 blocks its pretty accurate. The difference must also be significant (according to acceptable overhead).
+      // with at least 4 blocks it's pretty accurate. The difference must also be significant (according to acceptable overhead).
       if (numBlocks >= 4 && (avgBPV+avgBPV*acceptableOverheadRatio) < deltaBPV.bitsPerValue) {
         doBlock = true;
       }
@@ -335,7 +335,7 @@ class MemoryDocValuesConsumer extends DocValuesConsumer {
     meta.writeVInt(minLength);
     meta.writeVInt(maxLength);
     
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
+    // if minLength == maxLength, it's a fixed-length byte[], we are done (the addresses are implicit)
     // otherwise, we need to record the length fields...
     if (minLength != maxLength) {
       meta.writeVInt(PackedInts.VERSION_CURRENT);
@@ -432,7 +432,7 @@ class MemoryDocValuesConsumer extends DocValuesConsumer {
     }
   }
 
-  // note: this might not be the most efficient... but its fairly simple
+  // note: this might not be the most efficient... but it's fairly simple
   @Override
   public void addSortedSetField(FieldInfo field, Iterable<BytesRef> values, final Iterable<Number> docToOrdCount, final Iterable<Number> ords) throws IOException {
     meta.writeVInt(field.number);
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
index 59b9913..ae14c68 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
@@ -111,11 +111,11 @@ import org.apache.lucene.index.SegmentWriteState;
  *  </pre>
  *  so the "ord section" begins at startOffset + (9+pattern.length+maxlength)*numValues.
  *  a document's ord list can be retrieved by seeking to "ord section" + (1+ordpattern.length())*docid
- *  this is a comma-separated list, and its padded with spaces to be fixed width. so trim() and split() it.
+ *  this is a comma-separated list, and it's padded with spaces to be fixed width. so trim() and split() it.
  *  and beware the empty string!
  *  an ord's value can be retrieved by seeking to startOffset + (9+pattern.length+maxlength)*ord
  *  
- *  for sorted numerics, its encoded (not very creatively) as a comma-separated list of strings the same as binary.
+ *  for sorted numerics, it's encoded (not very creatively) as a comma-separated list of strings the same as binary.
  *  beware the empty string!
  *   
  *  the reader can just scan this file when it opens, skipping over the data blocks
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java
index 7cf2904..9bd2a99 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesReader.java
@@ -180,7 +180,7 @@ class SimpleTextDocValuesReader extends DocValuesProducer {
           } catch (ParseException pe) {
             throw new CorruptIndexException("failed to parse BigDecimal value", in, pe);
           }
-          SimpleTextUtil.readLine(in, scratch); // read the line telling us if its real or not
+          SimpleTextUtil.readLine(in, scratch); // read the line telling us if it's real or not
           return BigInteger.valueOf(field.minValue).add(bd.toBigIntegerExact()).longValue();
         } catch (IOException ioe) {
           throw new RuntimeException(ioe);
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/Analyzer.java b/lucene/core/src/java/org/apache/lucene/analysis/Analyzer.java
index d064dd2..f6da85c 100644
--- a/lucene/core/src/java/org/apache/lucene/analysis/Analyzer.java
+++ b/lucene/core/src/java/org/apache/lucene/analysis/Analyzer.java
@@ -88,7 +88,7 @@ public abstract class Analyzer implements Closeable {
   /**
    * Expert: create a new Analyzer with a custom {@link ReuseStrategy}.
    * <p>
-   * NOTE: if you just want to reuse on a per-field basis, its easier to
+   * NOTE: if you just want to reuse on a per-field basis, it's easier to
    * use a subclass of {@link AnalyzerWrapper} such as 
    * <a href="{@docRoot}/../analyzers-common/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.html">
    * PerFieldAnalyerWrapper</a> instead.
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/PayloadAttribute.java b/lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/PayloadAttribute.java
index 8793c94..daf6d00 100644
--- a/lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/PayloadAttribute.java
+++ b/lucene/core/src/java/org/apache/lucene/analysis/tokenattributes/PayloadAttribute.java
@@ -29,7 +29,7 @@ import org.apache.lucene.util.BytesRef;
  * in the {@link org.apache.lucene.search.payloads} and
  * {@link org.apache.lucene.search.spans} packages.
  * <p>
- * NOTE: because the payload will be stored at each position, its usually
+ * NOTE: because the payload will be stored at each position, it's usually
  * best to use the minimum number of bytes necessary. Some codec implementations
  * may optimize payload storage when all payloads have the same length.
  * 
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
index 4ae2237..cadc548 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
@@ -116,7 +116,7 @@ public abstract class TermVectorsWriter implements Closeable {
    * @lucene.internal
    */
   // TODO: we should probably nuke this and make a more efficient 4.x format
-  // PreFlex-RW could then be slow and buffer (its only used in tests...)
+  // PreFlex-RW could then be slow and buffer (it's only used in tests...)
   public void addProx(int numProx, DataInput positions, DataInput offsets) throws IOException {
     int position = 0;
     int lastOffset = 0;
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java
index 403ae3f..2c6c018 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsReader.java
@@ -48,7 +48,7 @@ import org.apache.lucene.util.fst.Outputs;
  *  approach is that seekExact is often able to
  *  determine a term cannot exist without doing any IO, and
  *  intersection with Automata is very fast.  Note that this
- *  terms dictionary has it's own fixed terms index (ie, it
+ *  terms dictionary has its own fixed terms index (ie, it
  *  does not support a pluggable terms index
  *  implementation).
  *
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java
index e767066..ebd547d 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java
@@ -199,7 +199,7 @@ public abstract class CompressionMode {
       }
       final int compressedLength = in.readVInt();
       // pad with extra "dummy byte": see javadocs for using Inflater(true)
-      // we do it for compliance, but its unnecessary for years in zlib.
+      // we do it for compliance, but it's unnecessary for years in zlib.
       final int paddedLength = compressedLength + 1;
       if (paddedLength > compressed.length) {
         compressed = new byte[ArrayUtil.oversize(paddedLength, 1)];
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer.java
index 0a8b979..a087281 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesConsumer.java
@@ -324,7 +324,7 @@ class Lucene50DocValuesConsumer extends DocValuesConsumer implements Closeable {
     meta.writeVLong(count);
     meta.writeLong(startFP);
     
-    // if minLength == maxLength, its a fixed-length byte[], we are done (the addresses are implicit)
+    // if minLength == maxLength, it's a fixed-length byte[], we are done (the addresses are implicit)
     // otherwise, we need to record the length fields...
     if (minLength != maxLength) {
       meta.writeLong(data.getFilePointer());
@@ -346,7 +346,7 @@ class Lucene50DocValuesConsumer extends DocValuesConsumer implements Closeable {
   
   /** expert: writes a value dictionary for a sorted/sortedset field */
   private void addTermsDict(FieldInfo field, final Iterable<BytesRef> values) throws IOException {
-    // first check if its a "fixed-length" terms dict
+    // first check if it's a "fixed-length" terms dict
     int minLength = Integer.MAX_VALUE;
     int maxLength = Integer.MIN_VALUE;
     long numValues = 0;
@@ -371,7 +371,7 @@ class Lucene50DocValuesConsumer extends DocValuesConsumer implements Closeable {
       // now write the bytes: sharing prefixes within a block
       final long startFP = data.getFilePointer();
       // currently, we have to store the delta from expected for every 1/nth term
-      // we could avoid this, but its not much and less overall RAM than the previous approach!
+      // we could avoid this, but it's not much and less overall RAM than the previous approach!
       RAMOutputStream addressBuffer = new RAMOutputStream();
       MonotonicBlockPackedWriter termAddresses = new MonotonicBlockPackedWriter(addressBuffer, BLOCK_SIZE);
       // buffers up 16 terms
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.java
index 02aa211..86bbac1 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.java
@@ -149,7 +149,7 @@ import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
  *      Otherwise, the binary values are of variable size, and packed integer metadata (PackedVersion,BlockSize)
  *      is written for the addresses.
  *   <p>MissingOffset points to a byte[] containing a bitset of all documents that had a value for the field.
- *      If its -1, then there are no missing values. If its -2, all values are missing.
+ *      If it's -1, then there are no missing values. If it's -2, all values are missing.
  *   <li><a name="dvd" id="dvd"></a>
  *   <p>The DocValues data or .dvd file.</p>
  *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsConsumer.java
index 535ab5a..4f1d322 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsConsumer.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsConsumer.java
@@ -349,7 +349,7 @@ class Lucene50NormsConsumer extends NormsConsumer {
 
   // specialized deduplication of long->ord for norms: 99.99999% of the time this will be a single-byte range.
   static class NormMap {
-    // we use short: at most we will add 257 values to this map before its rejected as too big above.
+    // we use short: at most we will add 257 values to this map before it's rejected as too big above.
     private final short[] ords = new short[256];
     final int[] freqs = new int[257];
     final byte[] values = new byte[257];
diff --git a/lucene/core/src/java/org/apache/lucene/index/AutomatonTermsEnum.java b/lucene/core/src/java/org/apache/lucene/index/AutomatonTermsEnum.java
index 9ab40b8..7d477c8 100644
--- a/lucene/core/src/java/org/apache/lucene/index/AutomatonTermsEnum.java
+++ b/lucene/core/src/java/org/apache/lucene/index/AutomatonTermsEnum.java
@@ -110,7 +110,7 @@ class AutomatonTermsEnum extends FilteredTermsEnum {
     //System.out.println("ATE.nextSeekTerm term=" + term);
     if (term == null) {
       assert seekBytesRef.length() == 0;
-      // return the empty term, as its valid
+      // return the empty term, as it's valid
       if (runAutomaton.isAccept(runAutomaton.getInitialState())) {   
         return seekBytesRef.get();
       }
@@ -310,7 +310,7 @@ class AutomatonTermsEnum extends FilteredTermsEnum {
   private int backtrack(int position) {
     while (position-- > 0) {
       int nextChar = seekBytesRef.byteAt(position) & 0xff;
-      // if a character is 0xff its a dead-end too,
+      // if a character is 0xff it's a dead-end too,
       // because there is no higher character in binary sort order.
       if (nextChar++ != 0xff) {
         seekBytesRef.setByteAt(position, (byte) nextChar);
diff --git a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
index 299b488..8963535 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
@@ -778,7 +778,7 @@ public class CheckIndex implements Closeable {
       } else {
         Bits liveDocs = reader.getLiveDocs();
         if (liveDocs != null) {
-          // its ok for it to be non-null here, as long as none are set right?
+          // it's ok for it to be non-null here, as long as none are set right?
           for (int j = 0; j < liveDocs.length(); j++) {
             if (!liveDocs.get(j)) {
               throw new RuntimeException("liveDocs mismatch: info says no deletions but doc " + j + " is deleted.");
diff --git a/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java b/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
index 57deefb..dabbcef 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DefaultIndexingChain.java
@@ -92,7 +92,7 @@ final class DefaultIndexingChain extends DocConsumer {
     writeNorms(state);
     writeDocValues(state);
     
-    // its possible all docs hit non-aborting exceptions...
+    // it's possible all docs hit non-aborting exceptions...
     initStoredFieldsWriter();
     fillStoredFields(numDocs);
     storedFieldsWriter.finish(state.fieldInfos, numDocs);
@@ -152,7 +152,7 @@ final class DefaultIndexingChain extends DocConsumer {
       // TODO: catch missing DV fields here?  else we have
       // null/"" depending on how docs landed in segments?
       // but we can't detect all cases, and we should leave
-      // this behavior undefined. dv is not "schemaless": its column-stride.
+      // this behavior undefined. dv is not "schemaless": it's column-stride.
       success = true;
     } finally {
       if (success) {
@@ -662,7 +662,7 @@ final class DefaultIndexingChain extends DocConsumer {
         // trigger streams to perform end-of-stream operations
         stream.end();
 
-        // TODO: maybe add some safety? then again, its already checked 
+        // TODO: maybe add some safety? then again, it's already checked 
         // when we come back around to the field...
         invertState.position += invertState.posIncrAttribute.getPositionIncrement();
         invertState.offset += invertState.offsetAttribute.endOffset();
diff --git a/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java b/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java
index d3c19c8..8c0b93a 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushControl.java
@@ -582,7 +582,7 @@ final class DocumentsWriterFlushControl implements Accountable {
         assert !flushingWriters.containsKey(blockedFlush.dwpt) : "DWPT is already flushing";
         // Record the flushing DWPT to reduce flushBytes in doAfterFlush
         flushingWriters.put(blockedFlush.dwpt, Long.valueOf(blockedFlush.bytes));
-        // don't decr pending here - its already done when DWPT is blocked
+        // don't decr pending here - it's already done when DWPT is blocked
         flushQueue.add(blockedFlush.dwpt);
       }
     }
diff --git a/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java b/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java
index b2ef272..954a4bb 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DocumentsWriterFlushQueue.java
@@ -224,7 +224,7 @@ class DocumentsWriterFlushQueue {
     protected void publish(IndexWriter writer) throws IOException {
       assert !published : "ticket was already publised - can not publish twice";
       published = true;
-      // its a global ticket - no segment to publish
+      // it's a global ticket - no segment to publish
       finishFlush(writer, null, frozenUpdates);
     }
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
index 7365c8d..c001f4d 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexWriter.java
@@ -4412,8 +4412,8 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
         try {
           rollback();
         } catch (Throwable ignored) {
-          // it would be confusing to addSuppressed here, its unrelated to the disaster,
-          // and its possible our internal state is amiss anyway.
+          // it would be confusing to addSuppressed here, it's unrelated to the disaster,
+          // and it's possible our internal state is amiss anyway.
         }
       }
     }
@@ -4598,7 +4598,7 @@ public class IndexWriter implements Closeable, TwoPhaseCommit, Accountable {
   
   /**
    * Interface for internal atomic events. See {@link DocumentsWriter} for details. Events are executed concurrently and no order is guaranteed.
-   * Each event should only rely on the serializeability within it's process method. All actions that must happen before or after a certain action must be
+   * Each event should only rely on the serializeability within its process method. All actions that must happen before or after a certain action must be
    * encoded inside the {@link #process(IndexWriter, boolean, boolean)} method.
    *
    */
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexableField.java b/lucene/core/src/java/org/apache/lucene/index/IndexableField.java
index 26eb9b5..f13ac3c 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexableField.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexableField.java
@@ -42,7 +42,7 @@ public interface IndexableField extends GeneralField {
    *              custom field types (like StringField and NumericField) that do not use
    *              the analyzer to still have good performance. Note: the passed-in type
    *              may be inappropriate, for example if you mix up different types of Fields
-   *              for the same field name. So its the responsibility of the implementation to
+   *              for the same field name. So it's the responsibility of the implementation to
    *              check.
    * @return TokenStream value for indexing the document.  Should always return
    *         a non-null value if the field is to be indexed
diff --git a/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java b/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
index e4f3a8e..cc99e39 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
@@ -374,7 +374,7 @@ public class MultiDocValues {
   /** maps per-segment ordinals to/from global ordinal space */
   // TODO: we could also have a utility method to merge Terms[] and use size() as a weight when we need it
   // TODO: use more efficient packed ints structures?
-  // TODO: pull this out? its pretty generic (maps between N ord()-enabled TermsEnums) 
+  // TODO: pull this out? it's pretty generic (maps between N ord()-enabled TermsEnums) 
   public static class OrdinalMap implements Accountable {
 
     private static class SegmentMap implements Accountable {
diff --git a/lucene/core/src/java/org/apache/lucene/index/OrdTermState.java b/lucene/core/src/java/org/apache/lucene/index/OrdTermState.java
index d071e0f..825c112 100644
--- a/lucene/core/src/java/org/apache/lucene/index/OrdTermState.java
+++ b/lucene/core/src/java/org/apache/lucene/index/OrdTermState.java
@@ -23,7 +23,7 @@ package org.apache.lucene.index;
  * @lucene.experimental
  */
 public class OrdTermState extends TermState {
-  /** Term ordinal, i.e. it's position in the full list of
+  /** Term ordinal, i.e. its position in the full list of
    *  sorted terms. */
   public long ord;
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java b/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
index f8041e7..89be9e6 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentCoreReaders.java
@@ -97,7 +97,7 @@ final class SegmentCoreReaders implements Accountable {
   SegmentCoreReaders(SegmentReader owner, Directory dir, SegmentCommitInfo si, IOContext context) throws IOException {
 
     final Codec codec = si.info.getCodec();
-    final Directory cfsDir; // confusing name: if (cfs) its the cfsdir, otherwise its the segment's directory.
+    final Directory cfsDir; // confusing name: if (cfs) it's the cfsdir, otherwise it's the segment's directory.
 
     boolean success = false;
     
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java b/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
index 3a4f728..6970602 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentInfo.java
@@ -35,7 +35,7 @@ import org.apache.lucene.util.StringHelper;
 import org.apache.lucene.util.Version;
 
 /**
- * Information about a segment such as it's name, directory, and files related
+ * Information about a segment such as its name, directory, and files related
  * to the segment.
  *
  * @lucene.experimental
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java b/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java
index a6926b0..23565fe 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentWriteState.java
@@ -112,7 +112,7 @@ public class SegmentWriteState {
   }
   
   // currently only used by assert? clean up and make real check?
-  // either its a segment suffix (_X_Y) or its a parseable generation
+  // either it's a segment suffix (_X_Y) or it's a parseable generation
   // TODO: this is very confusing how ReadersAndUpdates passes generations via
   // this mechanism, maybe add 'generation' explicitly to ctor create the 'actual suffix' here?
   private boolean assertSegmentSuffix(String segmentSuffix) {
diff --git a/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java b/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java
index fe1579b..24420c2 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SortedSetDocValuesWriter.java
@@ -92,7 +92,7 @@ class SortedSetDocValuesWriter extends DocValuesWriter {
     int count = 0;
     for (int i = 0; i < currentUpto; i++) {
       int termID = currentValues[i];
-      // if its not a duplicate
+      // if it's not a duplicate
       if (termID != lastValue) {
         pending.add(termID); // record the term id
         count++;
diff --git a/lucene/core/src/java/org/apache/lucene/index/package.html b/lucene/core/src/java/org/apache/lucene/index/package.html
index 7458734..130d6c2 100644
--- a/lucene/core/src/java/org/apache/lucene/index/package.html
+++ b/lucene/core/src/java/org/apache/lucene/index/package.html
@@ -58,7 +58,7 @@ Fields fields = reader.fields();
 // access term vector fields for a specified document
 Fields fields = reader.getTermVectors(docid);
 </pre>
-Fields implements Java's Iterable interface, so its easy to enumerate the
+Fields implements Java's Iterable interface, so it's easy to enumerate the
 list of fields:
 <pre class="prettyprint">
 // enumerate list of fields
diff --git a/lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java b/lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
index 466dc90..369681a 100644
--- a/lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java
@@ -213,7 +213,7 @@ public class BooleanQuery extends Query implements Iterable<BooleanClause> {
     public float coord(int overlap, int maxOverlap) {
       // LUCENE-4300: in most cases of maxOverlap=1, BQ rewrites itself away,
       // so coord() is not applied. But when BQ cannot optimize itself away
-      // for a single clause (minNrShouldMatch, prohibited clauses, etc), its
+      // for a single clause (minNrShouldMatch, prohibited clauses, etc), it's
       // important not to apply coord(1,1) for consistency, it might not be 1.0F
       return maxOverlap == 1 ? 1F : similarity.coord(overlap, maxOverlap);
     }
@@ -400,14 +400,14 @@ public class BooleanQuery extends Query implements Iterable<BooleanClause> {
       
       // conjunction-disjunction mix:
       // we create the required and optional pieces with coord disabled, and then
-      // combine the two: if minNrShouldMatch > 0, then its a conjunction: because the
-      // optional side must match. otherwise its required + optional, factoring the
+      // combine the two: if minNrShouldMatch > 0, then it's a conjunction: because the
+      // optional side must match. otherwise it's required + optional, factoring the
       // number of optional terms into the coord calculation
       
       Scorer req = excl(req(required, true), prohibited);
       Scorer opt = opt(optional, minShouldMatch, true);
 
-      // TODO: clean this up: its horrible
+      // TODO: clean this up: it's horrible
       if (disableCoord) {
         if (minShouldMatch > 0) {
           return new ConjunctionScorer(this, new Scorer[] { req, opt }, 1F);
diff --git a/lucene/core/src/java/org/apache/lucene/search/ControlledRealTimeReopenThread.java b/lucene/core/src/java/org/apache/lucene/search/ControlledRealTimeReopenThread.java
index 1c10dbd..c9eb750 100644
--- a/lucene/core/src/java/org/apache/lucene/search/ControlledRealTimeReopenThread.java
+++ b/lucene/core/src/java/org/apache/lucene/search/ControlledRealTimeReopenThread.java
@@ -165,7 +165,7 @@ public class ControlledRealTimeReopenThread<T> extends Thread implements Closeab
       // not sleep for much or any longer before reopening:
       reopenLock.lock();
 
-      // Need to find waitingGen inside lock as its used to determine
+      // Need to find waitingGen inside lock as it's used to determine
       // stale time
       waitingGen = Math.max(waitingGen, targetGen);
 
diff --git a/lucene/core/src/java/org/apache/lucene/search/package.html b/lucene/core/src/java/org/apache/lucene/search/package.html
index 68595b7..d61e91c 100644
--- a/lucene/core/src/java/org/apache/lucene/search/package.html
+++ b/lucene/core/src/java/org/apache/lucene/search/package.html
@@ -521,7 +521,7 @@ on the built-in available scoring models and extending or changing Similarity.
         back
         out of Lucene (similar to Doug adding SpanQuery functionality).</p>
 
-<!-- TODO: integrate this better, its better served as an intro than an appendix -->
+<!-- TODO: integrate this better, it's better served as an intro than an appendix -->
 
 
 <a name="algorithm"></a>
diff --git a/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java b/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
index cc7da6f..ea45f69 100644
--- a/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
+++ b/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
@@ -233,7 +233,7 @@ public abstract class Similarity {
     /** The value for normalization of contained query clauses (e.g. sum of squared weights).
      * <p>
      * NOTE: a Similarity implementation might not use any query normalization at all,
-     * its not required. However, if it wants to participate in query normalization,
+     * it's not required. However, if it wants to participate in query normalization,
      * it can return a value here.
      */
     public abstract float getValueForNormalization();
@@ -241,7 +241,7 @@ public abstract class Similarity {
     /** Assigns the query normalization factor and boost from parent queries to this.
      * <p>
      * NOTE: a Similarity implementation might not use this normalized value at all,
-     * its not required. However, its usually a good idea to at least incorporate 
+     * it's not required. However, it's usually a good idea to at least incorporate 
      * the topLevelBoost (e.g. from an outer BooleanQuery) into its score.
      */
     public abstract void normalize(float queryNorm, float topLevelBoost);
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java b/lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java
index bb36b3f..c6dab4e 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/SpanTermQuery.java
@@ -92,7 +92,7 @@ public class SpanTermQuery extends SpanQuery {
     final TermState state;
     if (termContext == null) {
       // this happens with span-not query, as it doesn't include the NOT side in extractTerms()
-      // so we seek to the term now in this segment..., this sucks because its ugly mostly!
+      // so we seek to the term now in this segment..., this sucks because it's ugly mostly!
       final Terms terms = context.reader().terms(term.field());
       if (terms != null) {
         final TermsEnum termsEnum = terms.iterator(null);
diff --git a/lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java b/lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java
index 471db72..5951ae1 100644
--- a/lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java
+++ b/lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java
@@ -213,7 +213,7 @@ abstract class ByteBufferIndexInput extends IndexInput implements RandomAccessIn
     try {
       return buffers[bi].getShort((int) (pos & chunkSizeMask));
     } catch (IndexOutOfBoundsException ioobe) {
-      // either its a boundary, or read past EOF, fall back:
+      // either it's a boundary, or read past EOF, fall back:
       setPos(pos, bi);
       return readShort();
     } catch (NullPointerException npe) {
@@ -227,7 +227,7 @@ abstract class ByteBufferIndexInput extends IndexInput implements RandomAccessIn
     try {
       return buffers[bi].getInt((int) (pos & chunkSizeMask));
     } catch (IndexOutOfBoundsException ioobe) {
-      // either its a boundary, or read past EOF, fall back:
+      // either it's a boundary, or read past EOF, fall back:
       setPos(pos, bi);
       return readInt();
     } catch (NullPointerException npe) {
@@ -241,7 +241,7 @@ abstract class ByteBufferIndexInput extends IndexInput implements RandomAccessIn
     try {
       return buffers[bi].getLong((int) (pos & chunkSizeMask));
     } catch (IndexOutOfBoundsException ioobe) {
-      // either its a boundary, or read past EOF, fall back:
+      // either it's a boundary, or read past EOF, fall back:
       setPos(pos, bi);
       return readLong();
     } catch (NullPointerException npe) {
diff --git a/lucene/core/src/java/org/apache/lucene/store/MMapDirectory.java b/lucene/core/src/java/org/apache/lucene/store/MMapDirectory.java
index f801ed2..bacb78a 100644
--- a/lucene/core/src/java/org/apache/lucene/store/MMapDirectory.java
+++ b/lucene/core/src/java/org/apache/lucene/store/MMapDirectory.java
@@ -244,7 +244,7 @@ public class MMapDirectory extends FSDirectory {
     final String originalMessage;
     final Throwable originalCause;
     if (ioe.getCause() instanceof OutOfMemoryError) {
-      // nested OOM confuses users, because its "incorrect", just print a plain message:
+      // nested OOM confuses users, because it's "incorrect", just print a plain message:
       originalMessage = "Map failed";
       originalCause = null;
     } else {
diff --git a/lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java b/lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java
index 0aa7a2e..e14ed13 100644
--- a/lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java
+++ b/lucene/core/src/java/org/apache/lucene/store/NativeFSLockFactory.java
@@ -114,7 +114,7 @@ public final class NativeFSLockFactory extends FSLockFactory {
         Files.createFile(path);
       } catch (IOException ignore) {
         // we must create the file to have a truly canonical path.
-        // if its already created, we don't care. if it cant be created, it will fail below.
+        // if it's already created, we don't care. if it cant be created, it will fail below.
       }
       final Path canonicalPath = path.toRealPath();
       // Make sure nobody else in-process has this lock held
diff --git a/lucene/core/src/java/org/apache/lucene/store/RAMDirectory.java b/lucene/core/src/java/org/apache/lucene/store/RAMDirectory.java
index 49e20cf..6f7bed1 100644
--- a/lucene/core/src/java/org/apache/lucene/store/RAMDirectory.java
+++ b/lucene/core/src/java/org/apache/lucene/store/RAMDirectory.java
@@ -105,7 +105,7 @@ public class RAMDirectory extends BaseDirectory implements Accountable {
   public final String[] listAll() {
     ensureOpen();
     // NOTE: this returns a "weakly consistent view". Unless we change Dir API, keep this,
-    // and do not synchronize or anything stronger. its great for testing!
+    // and do not synchronize or anything stronger. it's great for testing!
     // NOTE: fileMap.keySet().toArray(new String[0]) is broken in non Sun JDKs,
     // and the code below is resilient to map changes during the array population.
     Set<String> fileNames = fileMap.keySet();
diff --git a/lucene/core/src/java/org/apache/lucene/util/IOUtils.java b/lucene/core/src/java/org/apache/lucene/util/IOUtils.java
index c5bc39e..d6574ba 100644
--- a/lucene/core/src/java/org/apache/lucene/util/IOUtils.java
+++ b/lucene/core/src/java/org/apache/lucene/util/IOUtils.java
@@ -140,7 +140,7 @@ public final class IOUtils {
    * the read charset doesn't match the expected {@link Charset}. 
    * <p>
    * Decoding readers are useful to load configuration files, stopword lists or synonym files
-   * to detect character set problems. However, its not recommended to use as a common purpose 
+   * to detect character set problems. However, it's not recommended to use as a common purpose 
    * reader.
    * 
    * @param stream the stream to wrap in a reader
@@ -160,7 +160,7 @@ public final class IOUtils {
    * the read charset doesn't match the expected {@link Charset}. 
    * <p>
    * Decoding readers are useful to load configuration files, stopword lists or synonym files
-   * to detect character set problems. However, its not recommended to use as a common purpose 
+   * to detect character set problems. However, it's not recommended to use as a common purpose 
    * reader.
    * @param clazz the class used to locate the resource
    * @param resource the resource name to load
@@ -505,7 +505,7 @@ public final class IOUtils {
       devinfo = sysinfo.resolve(devName);
     }
     
-    // read first byte from rotational, its a 1 if it spins.
+    // read first byte from rotational, it's a 1 if it spins.
     Path info = devinfo.resolve("queue/rotational");
     try (InputStream stream = Files.newInputStream(info)) {
       return stream.read() == '1'; 
@@ -518,7 +518,7 @@ public final class IOUtils {
     FileStore store = Files.getFileStore(path);
     String mount = getMountPoint(store);
 
-    // find the "matching" FileStore from system list, its the one we want.
+    // find the "matching" FileStore from system list, it's the one we want.
     for (FileStore fs : path.getFileSystem().getFileStores()) {
       if (mount.equals(getMountPoint(fs))) {
         return fs;
diff --git a/lucene/core/src/java/org/apache/lucene/util/automaton/createLevAutomata.py b/lucene/core/src/java/org/apache/lucene/util/automaton/createLevAutomata.py
index 16588a5..12b9194 100644
--- a/lucene/core/src/java/org/apache/lucene/util/automaton/createLevAutomata.py
+++ b/lucene/core/src/java/org/apache/lucene/util/automaton/createLevAutomata.py
@@ -325,7 +325,7 @@ def main():
   minErrors = []
   for i in xrange(len(stateMap2)-1):
     w('//   %s -> %s' % (i, stateMap2[i]))
-    # we replace t-notation as its not relevant here
+    # we replace t-notation as it's not relevant here
     st = stateMap2[i].replace('t', '')
     
     v = eval(st)
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestBlockPostingsFormat3.java b/lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestBlockPostingsFormat3.java
index fbb4055..195746c 100644
--- a/lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestBlockPostingsFormat3.java
+++ b/lucene/core/src/test/org/apache/lucene/codecs/lucene50/TestBlockPostingsFormat3.java
@@ -492,7 +492,7 @@ public class TestBlockPostingsFormat3 extends LuceneTestCase {
       assertEquals(freq, rightDocs.freq());
       for (int i = 0; i < freq; i++) {
         assertEquals(leftDocs.nextPosition(), rightDocs.nextPosition());
-        // we don't compare the payloads, its allowed that one is empty etc
+        // we don't compare the payloads, it's allowed that one is empty etc
       }
     }
   }
diff --git a/lucene/core/src/test/org/apache/lucene/document/TestBinaryDocument.java b/lucene/core/src/test/org/apache/lucene/document/TestBinaryDocument.java
index 7048a0a..90a3189 100644
--- a/lucene/core/src/test/org/apache/lucene/document/TestBinaryDocument.java
+++ b/lucene/core/src/test/org/apache/lucene/document/TestBinaryDocument.java
@@ -61,13 +61,13 @@ public class TestBinaryDocument extends LuceneTestCase {
     StoredDocument docFromReader = reader.document(0);
     assertTrue(docFromReader != null);
     
-    /** fetch the binary stored field and compare it's content with the original one */
+    /** fetch the binary stored field and compare its content with the original one */
     BytesRef bytes = docFromReader.getBinaryValue("binaryStored");
     assertNotNull(bytes);
     String binaryFldStoredTest = new String(bytes.bytes, bytes.offset, bytes.length, StandardCharsets.UTF_8);
     assertTrue(binaryFldStoredTest.equals(binaryValStored));
     
-    /** fetch the string field and compare it's content with the original one */
+    /** fetch the string field and compare its content with the original one */
     String stringFldStoredTest = docFromReader.get("stringStored");
     assertTrue(stringFldStoredTest.equals(binaryValStored));
     
@@ -95,7 +95,7 @@ public class TestBinaryDocument extends LuceneTestCase {
     StoredDocument docFromReader = reader.document(0);
     assertTrue(docFromReader != null);
     
-    /** fetch the binary compressed field and compare it's content with the original one */
+    /** fetch the binary compressed field and compare its content with the original one */
     String binaryFldCompressedTest = new String(CompressionTools.decompress(docFromReader.getBinaryValue("binaryCompressed")), StandardCharsets.UTF_8);
     assertTrue(binaryFldCompressedTest.equals(binaryValCompressed));
     assertTrue(CompressionTools.decompressString(docFromReader.getBinaryValue("stringCompressed")).equals(binaryValCompressed));
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java b/lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java
index 5c6424f..e6877d5 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestBagOfPositions.java
@@ -38,7 +38,7 @@ import org.apache.lucene.util.TestUtil;
  * Simple test that adds numeric terms, where each term has the 
  * totalTermFreq of its integer value, and checks that the totalTermFreq is correct. 
  */
-// TODO: somehow factor this with BagOfPostings? its almost the same
+// TODO: somehow factor this with BagOfPostings? it's almost the same
 @SuppressCodecs({"Direct", "Memory"}) // at night this makes like 200k/300k docs and will make Direct's heart beat!
 public class TestBagOfPositions extends LuceneTestCase {
   public void test() throws Exception {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java b/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
index b0552be..31e5b72 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDirectoryReader.java
@@ -939,7 +939,7 @@ public void testFilesOpenClose() throws IOException {
   
     reader.close();
   
-    // Close the top reader, its the only one that should be closed
+    // Close the top reader, it's the only one that should be closed
     assertEquals(1, closeCount[0]);
     writer.close();
   
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java b/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
index d8ce513..972c248 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestDuelingCodecs.java
@@ -50,7 +50,7 @@ public class TestDuelingCodecs extends LuceneTestCase {
   public void setUp() throws Exception {
     super.setUp();
 
-    // for now its SimpleText vs Default(random postings format)
+    // for now it's SimpleText vs Default(random postings format)
     // as this gives the best overall coverage. when we have more
     // codecs we should probably pick 2 from Codec.availableCodecs()
     
@@ -125,7 +125,7 @@ public class TestDuelingCodecs extends LuceneTestCase {
    */
   public static void createRandomIndex(int numdocs, RandomIndexWriter writer, long seed) throws IOException {
     Random random = new Random(seed);
-    // primary source for our data is from linefiledocs, its realistic.
+    // primary source for our data is from linefiledocs, it's realistic.
     LineFileDocs lineFileDocs = new LineFileDocs(random);
 
     // TODO: we should add other fields that use things like docs&freqs but omit positions,
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestFieldReuse.java b/lucene/core/src/test/org/apache/lucene/index/TestFieldReuse.java
index 43e6c71..68cd6a8 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestFieldReuse.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestFieldReuse.java
@@ -57,7 +57,7 @@ public class TestFieldReuse extends BaseTokenStreamTestCase {
         new int[]    { 3 }
     );
     
-    // pass a bogus stream and ensure its still ok
+    // pass a bogus stream and ensure it's still ok
     stringField = new StringField("foo", "beer", Field.Store.NO);
     TokenStream bogus = new NumericTokenStream();
     ts = stringField.tokenStream(null, bogus);
@@ -84,7 +84,7 @@ public class TestFieldReuse extends BaseTokenStreamTestCase {
     assertSame(ts, ts2);
     assertNumericContents(20, ts);
     
-    // pass a bogus stream and ensure its still ok
+    // pass a bogus stream and ensure it's still ok
     intField = new IntField("foo", 2343, Field.Store.NO);
     TokenStream bogus = new CannedTokenStream(new Token("bogus", 0, 5));
     ts = intField.tokenStream(null, bogus);
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
index db087b6..d8aad16 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -165,7 +165,7 @@ public class TestIndexWriter extends LuceneTestCase {
 
 
 
-    // TODO: we have the logic in MDW to do this check, and its better, because it knows about files it tried
+    // TODO: we have the logic in MDW to do this check, and it's better, because it knows about files it tried
     // to delete but couldn't: we should replace this!!!!
     public static void assertNoUnreferencedFiles(Directory dir, String message) throws IOException {
       if (dir instanceof MockDirectoryWrapper) {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
index f961b4c..a575425 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterConfig.java
@@ -235,7 +235,7 @@ public class TestIndexWriterConfig extends LuceneTestCase {
     }
 
     // Test Similarity: 
-    // we shouldnt assert what the default is, just that its not null.
+    // we shouldnt assert what the default is, just that it's not null.
     assertTrue(IndexSearcher.getDefaultSimilarity() == conf.getSimilarity());
     conf.setSimilarity(new MySimilarity());
     assertEquals(MySimilarity.class, conf.getSimilarity().getClass());
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java
index 605212f..6855802 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestIndexWriterOnJRECrash.java
@@ -40,7 +40,7 @@ import org.apache.lucene.util.TestUtil;
 import com.carrotsearch.randomizedtesting.SeedUtils;
 /**
  * Runs TestNRTThreads in a separate process, crashes the JRE in the middle
- * of execution, then runs checkindex to make sure its not corrupt.
+ * of execution, then runs checkindex to make sure it's not corrupt.
  */
 public class TestIndexWriterOnJRECrash extends TestNRTThreads {
   private Path tempDir;
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java b/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
index 667a616..098f599 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestFieldCacheRangeFilter.java
@@ -372,7 +372,7 @@ public class TestFieldCacheRangeFilter extends BaseTestRangeFilter {
     assertEquals("inverse range", 0, result.length);
   }
   
-  // float and double tests are a bit minimalistic, but its complicated, because missing precision
+  // float and double tests are a bit minimalistic, but it's complicated, because missing precision
   
   @Test
   public void testFieldCacheRangeFilterFloats() throws IOException {
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestRegexpQuery.java b/lucene/core/src/test/org/apache/lucene/search/TestRegexpQuery.java
index cc53f37..23fab2c 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestRegexpQuery.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestRegexpQuery.java
@@ -117,7 +117,7 @@ public class TestRegexpQuery extends LuceneTestCase {
   
   /**
    * Test a corner case for backtracking: In this case the term dictionary has
-   * 493432 followed by 49344. When backtracking from 49343... to 4934, its
+   * 493432 followed by 49344. When backtracking from 49343... to 4934, it's
    * necessary to test that 4934 itself is ok before trying to append more
    * characters.
    */
diff --git a/lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java b/lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
index 82b34e4..065ee70 100644
--- a/lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
+++ b/lucene/core/src/test/org/apache/lucene/search/TestSubScorerFreqs.java
@@ -171,7 +171,7 @@ public class TestSubScorerFreqs extends LuceneTestCase {
       boolean includeOptional = occur.contains("SHOULD");
       for (int i = 0; i < maxDocs; i++) {
         Map<Query, Float> doc0 = c.docCounts.get(i);
-        // Y doesnt exist in the index, so its not in the scorer tree
+        // Y doesnt exist in the index, so it's not in the scorer tree
         assertEquals(4, doc0.size());
         assertEquals(1.0F, doc0.get(aQuery), FLOAT_TOLERANCE);
         assertEquals(4.0F, doc0.get(dQuery), FLOAT_TOLERANCE);
@@ -180,7 +180,7 @@ public class TestSubScorerFreqs extends LuceneTestCase {
         }
 
         Map<Query, Float> doc1 = c.docCounts.get(++i);
-        // Y doesnt exist in the index, so its not in the scorer tree
+        // Y doesnt exist in the index, so it's not in the scorer tree
         assertEquals(4, doc1.size());
         assertEquals(1.0F, doc1.get(aQuery), FLOAT_TOLERANCE);
         assertEquals(1.0F, doc1.get(dQuery), FLOAT_TOLERANCE);
diff --git a/lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java b/lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java
index a3b7615..eed1aa0 100644
--- a/lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java
+++ b/lucene/core/src/test/org/apache/lucene/search/similarities/TestSimilarity2.java
@@ -71,7 +71,7 @@ public class TestSimilarity2 extends LuceneTestCase {
     sims.add(new LMJelinekMercerSimilarity(0.7f));
   }
   
-  /** because of stupid things like querynorm, its possible we computeStats on a field that doesnt exist at all
+  /** because of stupid things like querynorm, it's possible we computeStats on a field that doesnt exist at all
    *  test this against a totally empty index, to make sure sims handle it
    */
   public void testEmptyIndex() throws Exception {
diff --git a/lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java b/lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
index 5eaa238..a94cb15 100644
--- a/lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
+++ b/lucene/core/src/test/org/apache/lucene/store/TestNRTCachingDirectory.java
@@ -39,7 +39,7 @@ import org.apache.lucene.util.TestUtil;
 
 public class TestNRTCachingDirectory extends BaseDirectoryTestCase {
 
-  // TODO: RAMDir used here, because its still too slow to use e.g. SimpleFS
+  // TODO: RAMDir used here, because it's still too slow to use e.g. SimpleFS
   // for the threads tests... maybe because of the synchronization in listAll?
   // would be good to investigate further...
   @Override
diff --git a/lucene/core/src/test/org/apache/lucene/util/TestIOUtils.java b/lucene/core/src/test/org/apache/lucene/util/TestIOUtils.java
index 00cad72..7b90b23 100644
--- a/lucene/core/src/test/org/apache/lucene/util/TestIOUtils.java
+++ b/lucene/core/src/test/org/apache/lucene/util/TestIOUtils.java
@@ -214,7 +214,7 @@ public class TestIOUtils extends LuceneTestCase {
     
     @Override
     public void checkAccess(Path path, AccessMode... modes) throws IOException {
-      // TODO: kinda screwed up how we do this, but its easy to get lost. just unravel completely.
+      // TODO: kinda screwed up how we do this, but it's easy to get lost. just unravel completely.
       delegate.checkAccess(FilterPath.unwrap(path), modes);
     }
 
diff --git a/lucene/core/src/test/org/apache/lucene/util/TestNamedSPILoader.java b/lucene/core/src/test/org/apache/lucene/util/TestNamedSPILoader.java
index 08bfa99..4a2af47 100644
--- a/lucene/core/src/test/org/apache/lucene/util/TestNamedSPILoader.java
+++ b/lucene/core/src/test/org/apache/lucene/util/TestNamedSPILoader.java
@@ -21,7 +21,7 @@ import org.apache.lucene.codecs.Codec;
  * limitations under the License.
  */
 
-// TODO: maybe we should test this with mocks, but its easy
+// TODO: maybe we should test this with mocks, but it's easy
 // enough to test the basics via Codec
 public class TestNamedSPILoader extends LuceneTestCase {
   
@@ -31,7 +31,7 @@ public class TestNamedSPILoader extends LuceneTestCase {
     assertEquals(currentName, codec.getName());
   }
   
-  // we want an exception if its not found.
+  // we want an exception if it's not found.
   public void testBogusLookup() {
     try {
       Codec.forName("dskfdskfsdfksdfdsf");
diff --git a/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java b/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
index 8d73a4a..37825e8 100644
--- a/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
+++ b/lucene/core/src/test/org/apache/lucene/util/fst/TestFSTs.java
@@ -1417,10 +1417,10 @@ public class TestFSTs extends LuceneTestCase {
       Util.TopResults<Long> r = Util.shortestPaths(fst, arc, fst.outputs.getNoOutput(), minLongComparator, topN, true);
       assertTrue(r.isComplete);
 
-      // 2. go thru whole treemap (slowCompletor) and check its actually the best suggestion
+      // 2. go thru whole treemap (slowCompletor) and check it's actually the best suggestion
       final List<Result<Long>> matches = new ArrayList<>();
 
-      // TODO: could be faster... but its slowCompletor for a reason
+      // TODO: could be faster... but it's slowCompletor for a reason
       for (Map.Entry<String,Long> e : slowCompletor.entrySet()) {
         if (e.getKey().startsWith(prefix)) {
           //System.out.println("  consider " + e.getKey());
@@ -1538,10 +1538,10 @@ public class TestFSTs extends LuceneTestCase {
 
       Util.TopResults<Pair<Long,Long>> r = Util.shortestPaths(fst, arc, fst.outputs.getNoOutput(), minPairWeightComparator, topN, true);
       assertTrue(r.isComplete);
-      // 2. go thru whole treemap (slowCompletor) and check its actually the best suggestion
+      // 2. go thru whole treemap (slowCompletor) and check it's actually the best suggestion
       final List<Result<Pair<Long,Long>>> matches = new ArrayList<>();
 
-      // TODO: could be faster... but its slowCompletor for a reason
+      // TODO: could be faster... but it's slowCompletor for a reason
       for (Map.Entry<String,TwoLongs> e : slowCompletor.entrySet()) {
         if (e.getKey().startsWith(prefix)) {
           //System.out.println("  consider " + e.getKey());
diff --git a/lucene/demo/build.xml b/lucene/demo/build.xml
index f9345fa..01d7c6a 100644
--- a/lucene/demo/build.xml
+++ b/lucene/demo/build.xml
@@ -42,7 +42,7 @@
 
   <target name="javadocs" depends="javadocs-analyzers-common,javadocs-queryparser,javadocs-facet,javadocs-expressions,compile-core,check-javadocs-uptodate"
           unless="javadocs-uptodate-${name}">
-    <!-- we link the example source in the javadocs, as its ref'ed elsewhere -->
+    <!-- we link the example source in the javadocs, as it's ref'ed elsewhere -->
     <invoke-module-javadoc linksource="yes">
       <links>
         <link href="../analyzers-common"/>
diff --git a/lucene/expressions/src/java/org/apache/lucene/expressions/js/JavascriptCompiler.java b/lucene/expressions/src/java/org/apache/lucene/expressions/js/JavascriptCompiler.java
index f0479b3..d00df91f 100644
--- a/lucene/expressions/src/java/org/apache/lucene/expressions/js/JavascriptCompiler.java
+++ b/lucene/expressions/src/java/org/apache/lucene/expressions/js/JavascriptCompiler.java
@@ -104,7 +104,7 @@ public class JavascriptCompiler {
     return org.objectweb.asm.commons.Method.getMethod(method);
   }
   
-  // This maximum length is theoretically 65535 bytes, but as its CESU-8 encoded we dont know how large it is in bytes, so be safe
+  // This maximum length is theoretically 65535 bytes, but as it's CESU-8 encoded we dont know how large it is in bytes, so be safe
   // rcmuir: "If your ranking function is that large you need to check yourself into a mental institution!"
   private static final int MAX_SOURCE_LENGTH = 16384;
   
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
index 3af66f9..42fe16e 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
@@ -224,7 +224,7 @@ public class TestTaxonomyCombined extends FacetTestCase {
     // Now, open the same taxonomy and add the same categories again.
     // After a few categories, the LuceneTaxonomyWriter implementation
     // will stop looking for each category on disk, and rather read them
-    // all into memory and close it's reader. The bug was that it closed
+    // all into memory and close its reader. The bug was that it closed
     // the reader, but forgot that it did (because it didn't set the reader
     // reference to null).
     tw = new DirectoryTaxonomyWriter(indexDir);
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/DefaultPassageFormatter.java b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/DefaultPassageFormatter.java
index 9d728cd..a1f33df 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/DefaultPassageFormatter.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/DefaultPassageFormatter.java
@@ -62,7 +62,7 @@ public class DefaultPassageFormatter extends PassageFormatter {
     StringBuilder sb = new StringBuilder();
     int pos = 0;
     for (Passage passage : passages) {
-      // don't add ellipsis if its the first one, or if its connected.
+      // don't add ellipsis if it's the first one, or if it's connected.
       if (passage.startOffset > pos && pos > 0) {
         sb.append(ellipsis);
       }
@@ -70,7 +70,7 @@ public class DefaultPassageFormatter extends PassageFormatter {
       for (int i = 0; i < passage.numMatches; i++) {
         int start = passage.matchStarts[i];
         int end = passage.matchEnds[i];
-        // its possible to have overlapping terms
+        // it's possible to have overlapping terms
         if (start > pos) {
           append(sb, content, pos, start);
         }
@@ -81,7 +81,7 @@ public class DefaultPassageFormatter extends PassageFormatter {
           pos = end;
         }
       }
-      // its possible a "term" from the analyzer could span a sentence boundary.
+      // it's possible a "term" from the analyzer could span a sentence boundary.
       append(sb, content, pos, Math.max(pos, passage.endOffset));
       pos = passage.endOffset;
     }
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/MultiTermHighlighting.java b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/MultiTermHighlighting.java
index bf2f1d2..67cdf91 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/MultiTermHighlighting.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/MultiTermHighlighting.java
@@ -157,7 +157,7 @@ class MultiTermHighlighting {
         final CharsRef scratch = new CharsRef();
         final Comparator<CharsRef> comparator = CharsRef.getUTF16SortedAsUTF8Comparator();
         
-        // this is *not* an automaton, but its very simple
+        // this is *not* an automaton, but it's very simple
         list.add(new CharacterRunAutomaton(Automata.makeEmpty()) {
           @Override
           public boolean run(char[] s, int offset, int length) {
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/Passage.java b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/Passage.java
index aea1a17..1c206cd 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/Passage.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/Passage.java
@@ -141,7 +141,7 @@ public final class Passage {
   /**
    * End offsets of the term matches, corresponding with {@link #getMatchStarts}. 
    * <p>
-   * Only {@link #getNumMatches} are valid. Note that its possible that an end offset 
+   * Only {@link #getNumMatches} are valid. Note that it's possible that an end offset 
    * could exceed beyond the bounds of the passage ({@link #getEndOffset()}), if the 
    * Analyzer produced a term which spans a passage boundary.
    */
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleBoundaryScanner.java b/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleBoundaryScanner.java
index 2d6b468..6adfefd 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleBoundaryScanner.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleBoundaryScanner.java
@@ -66,7 +66,7 @@ public class SimpleBoundaryScanner implements BoundaryScanner {
       if( boundaryChars.contains( buffer.charAt( offset - 1 ) ) ) return offset;
       offset--;
     }
-    // if we scanned up to the start of the text, return it, its a "boundary"
+    // if we scanned up to the start of the text, return it, it's a "boundary"
     if (offset == 0) {
       return 0;
     }
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java b/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java
index 0740392..8e1e26e 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestMultiTermHighlighting.java
@@ -837,7 +837,7 @@ public class TestMultiTermHighlighting extends LuceneTestCase {
             StringBuilder sb = new StringBuilder();
             int pos = 0;
             for (Passage passage : passages) {
-              // don't add ellipsis if its the first one, or if its connected.
+              // don't add ellipsis if it's the first one, or if it's connected.
               if (passage.startOffset > pos && pos > 0) {
                 sb.append("... ");
               }
@@ -845,7 +845,7 @@ public class TestMultiTermHighlighting extends LuceneTestCase {
               for (int i = 0; i < passage.numMatches; i++) {
                 int start = passage.matchStarts[i];
                 int end = passage.matchEnds[i];
-                // its possible to have overlapping terms
+                // it's possible to have overlapping terms
                 if (start > pos) {
                   sb.append(content, pos, start);
                 }
@@ -859,7 +859,7 @@ public class TestMultiTermHighlighting extends LuceneTestCase {
                   pos = end;
                 }
               }
-              // its possible a "term" from the analyzer could span a sentence boundary.
+              // it's possible a "term" from the analyzer could span a sentence boundary.
               sb.append(content, pos, Math.max(pos, passage.endOffset));
               pos = passage.endOffset;
             }
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/BreakIteratorBoundaryScannerTest.java b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/BreakIteratorBoundaryScannerTest.java
index c8a9d57..cf4574e 100644
--- a/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/BreakIteratorBoundaryScannerTest.java
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/BreakIteratorBoundaryScannerTest.java
@@ -57,7 +57,7 @@ public class BreakIteratorBoundaryScannerTest extends LuceneTestCase {
 
   public void testSentenceBoundary() throws Exception {
     StringBuilder text = new StringBuilder(TEXT);
-    // we test this with default locale, its randomized by LuceneTestCase
+    // we test this with default locale, it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getSentenceInstance(Locale.getDefault());
     BoundaryScanner scanner = new BreakIteratorBoundaryScanner(bi);
     
@@ -71,7 +71,7 @@ public class BreakIteratorBoundaryScannerTest extends LuceneTestCase {
 
   public void testLineBoundary() throws Exception {
     StringBuilder text = new StringBuilder(TEXT);
-    // we test this with default locale, its randomized by LuceneTestCase
+    // we test this with default locale, it's randomized by LuceneTestCase
     BreakIterator bi = BreakIterator.getLineInstance(Locale.getDefault());
     BoundaryScanner scanner = new BreakIteratorBoundaryScanner(bi);
     
diff --git a/lucene/misc/src/java/org/apache/lucene/index/SortingLeafReader.java b/lucene/misc/src/java/org/apache/lucene/index/SortingLeafReader.java
index 4980651..ee6140b 100644
--- a/lucene/misc/src/java/org/apache/lucene/index/SortingLeafReader.java
+++ b/lucene/misc/src/java/org/apache/lucene/index/SortingLeafReader.java
@@ -510,7 +510,7 @@ public class SortingLeafReader extends FilterLeafReader {
     
     /**
      * A {@link TimSorter} which sorts two parallel arrays of doc IDs and
-     * offsets in one go. Everytime a doc ID is 'swapped', its correponding offset
+     * offsets in one go. Everytime a doc ID is 'swapped', its corresponding offset
      * is swapped too.
      */
     private static final class DocOffsetSorter extends TimSorter {
diff --git a/lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java b/lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java
index f6acd5f..6b6eb8f 100644
--- a/lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java
+++ b/lucene/misc/src/java/org/apache/lucene/uninverting/DocTermOrds.java
@@ -96,7 +96,7 @@ import org.apache.lucene.util.StringHelper;
  *
  *   There are actually 256 byte arrays, to compensate for the fact that the pointers
  *   into the byte arrays are only 3 bytes long.  The correct byte array for a document
- *   is a function of it's id.
+ *   is a function of its id.
  *
  *   To save space and speed up faceting, any term that matches enough documents will
  *   not be un-inverted... it will be skipped while building the un-inverted field structure,
@@ -105,7 +105,7 @@ import org.apache.lucene.util.StringHelper;
  *   To further save memory, the terms (the actual string values) are not all stored in
  *   memory, but a TermIndex is used to convert term numbers to term values only
  *   for the terms needed after faceting has completed.  Only every 128th term value
- *   is stored, along with it's corresponding term number, and this is used as an
+ *   is stored, along with its corresponding term number, and this is used as an
  *   index to find the closest term and iterate until the desired number is hit (very
  *   much like Lucene's own internal term index).
  *
@@ -314,7 +314,7 @@ public class DocTermOrds implements Accountable {
     //
     // During this intermediate form, every document has a (potential) byte[]
     // and the int[maxDoc()] array either contains the termNumber list directly
-    // or the *end* offset of the termNumber list in it's byte array (for faster
+    // or the *end* offset of the termNumber list in its byte array (for faster
     // appending and faster creation of the final form).
     //
     // idea... if things are too large while building, we could do a range of docs
diff --git a/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java b/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java
index a7b5b89..18749ae 100644
--- a/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java
+++ b/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheImpl.java
@@ -911,8 +911,8 @@ class FieldCacheImpl implements FieldCache {
       return DocValues.emptySortedSet();
     } else {
       // if #postings = #docswithfield we know that the field is "single valued enough".
-      // its possible the same term might appear twice in the same document, but SORTED_SET discards frequency.
-      // its still ok with filtering (which we limit to numerics), it just means precisionStep = Inf
+      // it's possible the same term might appear twice in the same document, but SORTED_SET discards frequency.
+      // it's still ok with filtering (which we limit to numerics), it just means precisionStep = Inf
       long numPostings = terms.getSumDocFreq();
       if (numPostings != -1 && numPostings == terms.getDocCount()) {
         return DocValues.singleton(getTermsIndex(reader, field));
diff --git a/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheSanityChecker.java b/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheSanityChecker.java
index 7e9d28f..ae248a6 100644
--- a/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheSanityChecker.java
+++ b/lucene/misc/src/java/org/apache/lucene/uninverting/FieldCacheSanityChecker.java
@@ -348,7 +348,7 @@ final class FieldCacheSanityChecker {
     public CacheEntry[] getCacheEntries() { return entries; }
     /**
      * Multi-Line representation of this Insanity object, starting with 
-     * the Type and Msg, followed by each CacheEntry.toString() on it's 
+     * the Type and Msg, followed by each CacheEntry.toString() on its 
      * own line prefaced by a tab character
      */
     @Override
diff --git a/lucene/module-build.xml b/lucene/module-build.xml
index c68900a..5a8fe9b 100644
--- a/lucene/module-build.xml
+++ b/lucene/module-build.xml
@@ -68,7 +68,7 @@
   <macrodef name="invoke-module-javadoc">
     <!-- additional links for dependencies to other modules -->
       <element name="links" optional="yes"/>
-    <!-- link source (don't do this unless its example code) -->
+    <!-- link source (don't do this unless it's example code) -->
       <attribute name="linksource" default="no"/>
     <sequential>
       <mkdir dir="${javadoc.dir}/${name}"/>
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java
index a38cbb0..2ea7ea3 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MaxFloatFunction.java
@@ -21,7 +21,7 @@ import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
- * <code>MaxFloatFunction</code> returns the max of it's components.
+ * <code>MaxFloatFunction</code> returns the max of its components.
  */
 public class MaxFloatFunction extends MultiFloatFunction {
   public MaxFloatFunction(ValueSource[] sources) {
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java
index f1c426b..a3f20cd 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/MinFloatFunction.java
@@ -21,7 +21,7 @@ import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
- * <code>MinFloatFunction</code> returns the min of it's components.
+ * <code>MinFloatFunction</code> returns the min of its components.
  */
 public class MinFloatFunction extends MultiFloatFunction {
   public MinFloatFunction(ValueSource[] sources) {
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java
index c957462..34b8271 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/ProductFloatFunction.java
@@ -21,7 +21,7 @@ import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
- * <code>ProductFloatFunction</code> returns the product of it's components.
+ * <code>ProductFloatFunction</code> returns the product of its components.
  */
 public class ProductFloatFunction extends MultiFloatFunction {
   public ProductFloatFunction(ValueSource[] sources) {
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java
index 573a484..321edd0 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/valuesource/SumFloatFunction.java
@@ -21,7 +21,7 @@ import org.apache.lucene.queries.function.FunctionValues;
 import org.apache.lucene.queries.function.ValueSource;
 
 /**
- * <code>SumFloatFunction</code> returns the sum of it's components.
+ * <code>SumFloatFunction</code> returns the sum of its components.
  */
 public class SumFloatFunction extends MultiFloatFunction {
   public SumFloatFunction(ValueSource[] sources) {
diff --git a/lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java b/lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
index 0f2fc08..fa3d639 100644
--- a/lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
+++ b/lucene/queries/src/test/org/apache/lucene/queries/CommonTermsQueryTest.java
@@ -531,7 +531,7 @@ public class CommonTermsQueryTest extends LuceneTestCase {
   public static void createRandomIndex(int numdocs, RandomIndexWriter writer,
       long seed) throws IOException {
     Random random = new Random(seed);
-    // primary source for our data is from linefiledocs, its realistic.
+    // primary source for our data is from linefiledocs, it's realistic.
     LineFileDocs lineFileDocs = new LineFileDocs(random);
     
     // TODO: we should add other fields that use things like docs&freqs but omit
diff --git a/lucene/queryparser/docs/xml/LuceneContribQuery.dtd.html b/lucene/queryparser/docs/xml/LuceneContribQuery.dtd.html
index 3b27a11..f373f38 100644
--- a/lucene/queryparser/docs/xml/LuceneContribQuery.dtd.html
+++ b/lucene/queryparser/docs/xml/LuceneContribQuery.dtd.html
@@ -219,7 +219,7 @@ Child of <a href='#BoostQuery'>BoostQuery</a>, <a href='#Clause'>Clause</a>, <a
 <li> as a Clause in a BooleanQuery who's only other clause
 is a "mustNot" match (Lucene requires at least one positive clause) and..</li>
 <li> in a FilteredQuery where a Filter tag is effectively being
-used to select content rather than it's usual role of filtering the results of a query.</li>
+used to select content rather than its usual role of filtering the results of a query.</li>
 </ol></p><p><span class='inTextTitle'>Example:</span> <em>Effectively use a Filter as a query </em>
 </p><pre>	          
                &lt;FilteredQuery&gt;
diff --git a/lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.html b/lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.html
index 7a2f9f1..6da3b41 100644
--- a/lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.html
+++ b/lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.html
@@ -225,7 +225,7 @@ Child of <a href='#Clause'>Clause</a>, <a href='#Query'>Query</a>, <a href='#Cac
 <li> as a Clause in a BooleanQuery who's only other clause
 is a "mustNot" match (Lucene requires at least one positive clause) and..</li>
 <li> in a FilteredQuery where a Filter tag is effectively being
-used to select content rather than it's usual role of filtering the results of a query.</li>
+used to select content rather than its usual role of filtering the results of a query.</li>
 </ol></p><p><span class='inTextTitle'>Example:</span> <em>Effectively use a Filter as a query </em>
 </p><pre>	          
                &lt;FilteredQuery&gt;
diff --git a/lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.org.html b/lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.org.html
index 6f0a004..2fd844c 100644
--- a/lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.org.html
+++ b/lucene/queryparser/docs/xml/LuceneCoreQuery.dtd.org.html
@@ -159,7 +159,7 @@
 <span class="dtd_comment">    &lt;li&gt; as a Clause in a BooleanQuery who's only other clause</span>
 <span class="dtd_comment">    is a &quot;mustNot&quot; match (Lucene requires at least one positive clause) and..&lt;/li&gt;</span>
 <span class="dtd_comment">    &lt;li&gt; in a FilteredQuery where a Filter tag is effectively being </span>
-<span class="dtd_comment">    used to select content rather than it's usual role of filtering the results of a query.&lt;/li&gt;</span>
+<span class="dtd_comment">    used to select content rather than its usual role of filtering the results of a query.&lt;/li&gt;</span>
 <span class="dtd_comment">    &lt;/ol&gt;</span>
 <span class="dtd_comment">    </span>
 <span class="dtd_comment">    </span><span class="dtd_dtddoc_tag">@example</span><span class="dtd_comment"> </span>
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SpanNearClauseFactory.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SpanNearClauseFactory.java
index bbc7ad1..f25006d 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SpanNearClauseFactory.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/surround/query/SpanNearClauseFactory.java
@@ -46,7 +46,7 @@ Operations:
    Are SpanQuery weights handled correctly during search by Lucene?
    Should the resulting SpanOrQuery be sorted?
    Could other SpanQueries be added for use in this factory:
-   - SpanOrQuery: in principle yes, but it only has access to it's terms
+   - SpanOrQuery: in principle yes, but it only has access to its terms
                   via getTerms(); are the corresponding weights available?
    - SpanFirstQuery: treat similar to subquery SpanNearQuery. (ok?)
    - SpanNotQuery: treat similar to subquery SpanNearQuery. (ok?)
diff --git a/lucene/queryparser/src/resources/org/apache/lucene/queryparser/xml/LuceneCoreQuery.dtd b/lucene/queryparser/src/resources/org/apache/lucene/queryparser/xml/LuceneCoreQuery.dtd
index fdeaf6f..8727c5d 100644
--- a/lucene/queryparser/src/resources/org/apache/lucene/queryparser/xml/LuceneCoreQuery.dtd
+++ b/lucene/queryparser/src/resources/org/apache/lucene/queryparser/xml/LuceneCoreQuery.dtd
@@ -153,7 +153,7 @@ Passes content directly through to the standard LuceneQuery parser see "Lucene Q
 	<li> as a Clause in a BooleanQuery who's only other clause
 	is a "mustNot" match (Lucene requires at least one positive clause) and..</li>
 	<li> in a FilteredQuery where a Filter tag is effectively being 
-	used to select content rather than it's usual role of filtering the results of a query.</li>
+	used to select content rather than its usual role of filtering the results of a query.</li>
 	</ol>
 	
 	@example 
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.java b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.java
index 2e2bab0..94680e3 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.java
@@ -51,7 +51,7 @@ import org.apache.lucene.search.Explanation;
  * Originally based on Geoportal's
  * <a href="http://geoportal.svn.sourceforge.net/svnroot/geoportal/Geoportal/trunk/src/com/esri/gpt/catalog/lucene/SpatialRankingValueSource.java">
  *   SpatialRankingValueSource</a> but modified quite a bit. GeoPortal's algorithm will yield a score of 0
- * if either a line or point is compared, and it's doesn't output a 0-1 normalized score (it multiplies the factors),
+ * if either a line or point is compared, and it doesn't output a 0-1 normalized score (it multiplies the factors),
  * and it doesn't support minSideLength, and it had dateline bugs.
  *
  * @lucene.experimental
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/CellTokenStream.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/CellTokenStream.java
index 58926fb..5a3d6c8 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/CellTokenStream.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/CellTokenStream.java
@@ -147,7 +147,7 @@ class CellTokenStream extends TokenStream {
     cellAtt.setOmitLeafByte(false);
   }
 
-  /** Outputs the token of a cell, and if its a leaf, outputs it again with the leaf byte. */
+  /** Outputs the token of a cell, and if it's a leaf, outputs it again with the leaf byte. */
   @Override
   public final boolean incrementToken() {
     if (iter == null)
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java
index 2ea23e9..35cad2b 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java
@@ -63,7 +63,7 @@ public interface Cell {
 
   /**
    * Returns the bytes for this cell, with a leaf byte if this is a leaf cell.
-   * The result param is used to save object allocation, though it's bytes aren't used.
+   * The result param is used to save object allocation, though its bytes aren't used.
    * @param result where the result goes, or null to create new
    */
   BytesRef getTokenBytesWithLeaf(BytesRef result);
@@ -71,7 +71,7 @@ public interface Cell {
   /**
    * Returns the bytes for this cell, without leaf set. The bytes should sort before
    * {@link #getTokenBytesWithLeaf(org.apache.lucene.util.BytesRef)}.
-   * The result param is used to save object allocation, though it's bytes aren't used.
+   * The result param is used to save object allocation, though its bytes aren't used.
    * @param result where the result goes, or null to create new
    */
   BytesRef getTokenBytesNoLeaf(BytesRef result);
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTreeFactory.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTreeFactory.java
index f5a4bc4..bbd753c 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTreeFactory.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTreeFactory.java
@@ -41,7 +41,7 @@ public abstract class SpatialPrefixTreeFactory {
 
   /**
    * The factory  is looked up via "prefixTree" in args, expecting "geohash" or "quad".
-   * If its neither of these, then "geohash" is chosen for a geo context, otherwise "quad" is chosen.
+   * If it's neither of these, then "geohash" is chosen for a geo context, otherwise "quad" is chosen.
    */
   public static SpatialPrefixTree makeSPT(Map<String,String> args, ClassLoader classLoader, SpatialContext ctx) {
     SpatialPrefixTreeFactory instance;
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapePredicateValueSource.java b/lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapePredicateValueSource.java
index a799bbb..fa01371 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapePredicateValueSource.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/util/ShapePredicateValueSource.java
@@ -42,7 +42,7 @@ public class ShapePredicateValueSource extends ValueSource {
 
   /**
    *
-   * @param shapeValuesource Must yield {@link Shape} instances from it's objectVal(doc). If null
+   * @param shapeValuesource Must yield {@link Shape} instances from its objectVal(doc). If null
    *                         then the result is false. This is the left-hand (indexed) side.
    * @param op the predicate
    * @param queryShape The shape on the right-hand (query) side.
diff --git a/lucene/spatial/src/test/org/apache/lucene/spatial/prefix/RandomSpatialOpFuzzyPrefixTreeTest.java b/lucene/spatial/src/test/org/apache/lucene/spatial/prefix/RandomSpatialOpFuzzyPrefixTreeTest.java
index cd5b8dc..78765b8 100644
--- a/lucene/spatial/src/test/org/apache/lucene/spatial/prefix/RandomSpatialOpFuzzyPrefixTreeTest.java
+++ b/lucene/spatial/src/test/org/apache/lucene/spatial/prefix/RandomSpatialOpFuzzyPrefixTreeTest.java
@@ -180,7 +180,7 @@ public class RandomSpatialOpFuzzyPrefixTreeTest extends StrategyTestCase {
         new SpatialArgs(SpatialOperation.IsWithin, ctx.makeRectangle(38, 192, -72, 56))
     ), 1).numFound==0);//no-match
 
-    //this time the rect is a little bigger and is considered a match. It's a
+    //this time the rect is a little bigger and is considered a match. It's
     // an acceptable false-positive because of the grid approximation.
     assertTrue(executeQuery(strategy.makeQuery(
         new SpatialArgs(SpatialOperation.IsWithin, ctx.makeRectangle(38, 192, -72, 80))
@@ -462,7 +462,7 @@ public class RandomSpatialOpFuzzyPrefixTreeTest extends StrategyTestCase {
         return r;
       //test all 4 corners
       // Note: awkwardly, we use a non-geo context for this because in geo, -180 & +180 are the same place, which means
-      //  that "other" might wrap the world horizontally and yet all it's corners could be in shape1 (or shape2) even
+      //  that "other" might wrap the world horizontally and yet all its corners could be in shape1 (or shape2) even
       //  though shape1 is only adjacent to the dateline. I couldn't think of a better way to handle this.
       Rectangle oRect = (Rectangle)other;
       if (cornerContainsNonGeo(oRect.getMinX(), oRect.getMinY())
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
index 6583f2a..0e5b0ec 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/spell/DirectSpellChecker.java
@@ -236,7 +236,7 @@ public class DirectSpellChecker {
    * True if the spellchecker should lowercase terms (default: true)
    * <p>
    * This is a convenience method, if your index field has more complicated
-   * analysis (such as StandardTokenizer removing punctuation), its probably
+   * analysis (such as StandardTokenizer removing punctuation), it's probably
    * better to turn this off, and instead run your query terms through your
    * Analyzer first.
    * <p>
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/LuceneLevenshteinDistance.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/LuceneLevenshteinDistance.java
index c3d70aa..6a6ce44 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/spell/LuceneLevenshteinDistance.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/spell/LuceneLevenshteinDistance.java
@@ -55,8 +55,8 @@ public final class LuceneLevenshteinDistance implements StringDistance {
     // NOTE: if we cared, we could 3*m space instead of m*n space, similar to 
     // what LevenshteinDistance does, except cycling thru a ring of three 
     // horizontal cost arrays... but this comparator is never actually used by 
-    // DirectSpellChecker, its only used for merging results from multiple shards 
-    // in "distributed spellcheck", and its inefficient in other ways too...
+    // DirectSpellChecker, it's only used for merging results from multiple shards 
+    // in "distributed spellcheck", and it's inefficient in other ways too...
 
     // cheaper to do this up front once
     targetPoints = toIntsRef(target);
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
index 126322d..5171653 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
@@ -568,7 +568,7 @@ public class SpellChecker implements java.io.Closeable {
 
   private static Document createDocument(String text, int ng1, int ng2) {
     Document doc = new Document();
-    // the word field is never queried on... its indexed so it can be quickly
+    // the word field is never queried on... it's indexed so it can be quickly
     // checked for rebuild (and stored for retrieval). Doesn't need norms or TF/pos
     Field f = new StringField(F_WORD, text, Field.Store.YES);
     doc.add(f); // orig term
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/suggest/DocumentDictionary.java b/lucene/suggest/src/java/org/apache/lucene/search/suggest/DocumentDictionary.java
index fb77bbf..a3c8212 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/suggest/DocumentDictionary.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/suggest/DocumentDictionary.java
@@ -235,8 +235,8 @@ public class DocumentDictionary implements Dictionary {
     
     /** 
      * Returns the value of the <code>weightField</code> for the current document.
-     * Retrieves the value for the <code>weightField</code> if its stored (using <code>doc</code>)
-     * or if its indexed as {@link NumericDocValues} (using <code>docId</code>) for the document.
+     * Retrieves the value for the <code>weightField</code> if it's stored (using <code>doc</code>)
+     * or if it's indexed as {@link NumericDocValues} (using <code>docId</code>) for the document.
      * If no value is found, then the weight is 0.
      */
     protected long getWeight(StoredDocument doc, int docId) {
diff --git a/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java b/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java
index d7fd558..8393759 100644
--- a/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java
+++ b/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggesterTest.java
@@ -221,13 +221,13 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
     assertEquals(50, results.get(0).value, 0.01F);
 
-    // omit the 'the' since its a stopword, its suggested anyway
+    // omit the 'the' since it's a stopword, it's suggested anyway
     results = suggester.lookup(TestUtil.stringToCharSequence("ghost of chris", random()), false, 1);
     assertEquals(1, results.size());
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
     assertEquals(50, results.get(0).value, 0.01F);
 
-    // omit the 'the' and 'of' since they are stopwords, its suggested anyway
+    // omit the 'the' and 'of' since they are stopwords, it's suggested anyway
     results = suggester.lookup(TestUtil.stringToCharSequence("ghost chris", random()), false, 1);
     assertEquals(1, results.size());
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
@@ -817,7 +817,7 @@ public class AnalyzingSuggesterTest extends LuceneTestCase {
         System.out.println("  analyzed: " + analyzedKey);
       }
 
-      // TODO: could be faster... but its slowCompletor for a reason
+      // TODO: could be faster... but it's slowCompletor for a reason
       for (TermFreq2 e : slowCompletor) {
         if (e.analyzedForm.startsWith(analyzedKey)) {
           matches.add(e);
diff --git a/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java b/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java
index 103c4b9..5847adc 100644
--- a/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java
+++ b/lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/FuzzySuggesterTest.java
@@ -186,13 +186,13 @@ public class FuzzySuggesterTest extends LuceneTestCase {
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
     assertEquals(50, results.get(0).value, 0.01F);
 
-    // omit the 'the' since its a stopword, its suggested anyway
+    // omit the 'the' since it's a stopword, it's suggested anyway
     results = suggester.lookup(TestUtil.stringToCharSequence("ghost of chris", random()), false, 1);
     assertEquals(1, results.size());
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
     assertEquals(50, results.get(0).value, 0.01F);
 
-    // omit the 'the' and 'of' since they are stopwords, its suggested anyway
+    // omit the 'the' and 'of' since they are stopwords, it's suggested anyway
     results = suggester.lookup(TestUtil.stringToCharSequence("ghost chris", random()), false, 1);
     assertEquals(1, results.size());
     assertEquals("the ghost of christmas past", results.get(0).key.toString());
@@ -755,7 +755,7 @@ public class FuzzySuggesterTest extends LuceneTestCase {
       Automaton automaton = suggester.convertAutomaton(suggester.toLevenshteinAutomata(suggester.toLookupAutomaton(analyzedKey)));
       assertTrue(automaton.isDeterministic());
 
-      // TODO: could be faster... but its slowCompletor for a reason
+      // TODO: could be faster... but it's slowCompletor for a reason
       BytesRefBuilder spare = new BytesRefBuilder();
       for (TermFreqPayload2 e : slowCompletor) {
         spare.copyChars(e.analyzedForm);
@@ -1114,8 +1114,8 @@ public class FuzzySuggesterTest extends LuceneTestCase {
     // NOTE: if we cared, we could 3*m space instead of m*n space, similar to 
     // what LevenshteinDistance does, except cycling thru a ring of three 
     // horizontal cost arrays... but this comparator is never actually used by 
-    // DirectSpellChecker, its only used for merging results from multiple shards 
-    // in "distributed spellcheck", and its inefficient in other ways too...
+    // DirectSpellChecker, it's only used for merging results from multiple shards 
+    // in "distributed spellcheck", and it's inefficient in other ways too...
 
     // cheaper to do this up front once
     targetPoints = toIntsRef(target);
diff --git a/lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/WFSTCompletionTest.java b/lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/WFSTCompletionTest.java
index 81babbd..ca0fc75 100644
--- a/lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/WFSTCompletionTest.java
+++ b/lucene/suggest/src/test/org/apache/lucene/search/suggest/fst/WFSTCompletionTest.java
@@ -162,10 +162,10 @@ public class WFSTCompletionTest extends LuceneTestCase {
       final int topN = TestUtil.nextInt(random, 1, 10);
       List<LookupResult> r = suggester.lookup(TestUtil.stringToCharSequence(prefix, random), false, topN);
 
-      // 2. go thru whole treemap (slowCompletor) and check its actually the best suggestion
+      // 2. go thru whole treemap (slowCompletor) and check it's actually the best suggestion
       final List<LookupResult> matches = new ArrayList<>();
 
-      // TODO: could be faster... but its slowCompletor for a reason
+      // TODO: could be faster... but it's slowCompletor for a reason
       for (Map.Entry<String,Long> e : slowCompletor.entrySet()) {
         if (e.getKey().startsWith(prefix)) {
           matches.add(new LookupResult(e.getKey(), e.getValue().longValue()));
diff --git a/lucene/test-framework/build.xml b/lucene/test-framework/build.xml
index 6493e54..df130d7 100644
--- a/lucene/test-framework/build.xml
+++ b/lucene/test-framework/build.xml
@@ -46,7 +46,7 @@
   <!-- redefine the clover setup, because we dont want to run clover for the test-framework -->
   <target name="-clover.setup" if="run.clover"/>
 
-  <!-- redefine the test compilation, so its just a no-op -->
+  <!-- redefine the test compilation, so it's just a no-op -->
   <target name="compile-test"/>
   
   <!-- redefine the forbidden apis for tests, as we check ourselves - no sysout testing -->
diff --git a/lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
index e1b2f87..509761e 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase.java
@@ -49,7 +49,7 @@ import org.apache.lucene.util.TestUtil;
 /** 
  * Base class for all Lucene unit tests that use TokenStreams. 
  * <p>
- * When writing unit tests for analysis components, its highly recommended
+ * When writing unit tests for analysis components, it's highly recommended
  * to use the helper methods here (especially in conjunction with {@link MockAnalyzer} or
  * {@link MockTokenizer}), as they contain many assertions and checks to 
  * catch bugs.
@@ -508,7 +508,7 @@ public abstract class BaseTokenStreamTestCase extends LuceneTestCase {
     try {
       checkRandomData(new Random(seed), a, iterations, maxWordLength, useCharFilter, simple, offsetsAreCorrect, iw);
       // now test with multiple threads: note we do the EXACT same thing we did before in each thread,
-      // so this should only really fail from another thread if its an actual thread problem
+      // so this should only really fail from another thread if it's an actual thread problem
       int numThreads = TestUtil.nextInt(random, 2, 4);
       final CountDownLatch startingGun = new CountDownLatch(1);
       AnalysisThread threads[] = new AnalysisThread[numThreads];
diff --git a/lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java b/lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java
index 1ab9ef0..7b75b1f 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/analysis/MockAnalyzer.java
@@ -30,7 +30,7 @@ import org.apache.lucene.util.automaton.CharacterRunAutomaton;
  * <p>
  * This analyzer is a replacement for Whitespace/Simple/KeywordAnalyzers
  * for unit tests. If you are testing a custom component such as a queryparser
- * or analyzer-wrapper that consumes analysis streams, its a great idea to test
+ * or analyzer-wrapper that consumes analysis streams, it's a great idea to test
  * it with this analyzer instead. MockAnalyzer has the following behavior:
  * <ul>
  *   <li>By default, the assertions in {@link MockTokenizer} are turned on for extra
diff --git a/lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java b/lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java
index 3714c1a..22bf6b8 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/analysis/MockTokenizer.java
@@ -33,7 +33,7 @@ import com.carrotsearch.randomizedtesting.RandomizedContext;
  * Tokenizer for testing.
  * <p>
  * This tokenizer is a replacement for {@link #WHITESPACE}, {@link #SIMPLE}, and {@link #KEYWORD}
- * tokenizers. If you are writing a component such as a TokenFilter, its a great idea to test
+ * tokenizers. If you are writing a component such as a TokenFilter, it's a great idea to test
  * it wrapping this tokenizer instead for extra checks. This tokenizer has the following behavior:
  * <ul>
  *   <li>An internal state-machine is used for checking consumer consistency. These checks can
@@ -66,7 +66,7 @@ public class MockTokenizer extends Tokenizer {
   int off = 0;
   
   // buffered state (previous codepoint and offset). we replay this once we
-  // hit a reject state in case its permissible as the start of a new term.
+  // hit a reject state in case it's permissible as the start of a new term.
   int bufferedCodePoint = -1; // -1 indicates empty buffer
   int bufferedOff = -1;
 
@@ -169,7 +169,7 @@ public class MockTokenizer extends Tokenizer {
           bufferedCodePoint = cp;
           bufferedOff = endOffset;
         } else {
-          // otherwise, its because we hit term limit.
+          // otherwise, it's because we hit term limit.
           bufferedCodePoint = -1;
         }
         int correctedStartOffset = correctOffset(startOffset);
diff --git a/lucene/test-framework/src/java/org/apache/lucene/analysis/package.html b/lucene/test-framework/src/java/org/apache/lucene/analysis/package.html
index 1970811..5177298 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/analysis/package.html
+++ b/lucene/test-framework/src/java/org/apache/lucene/analysis/package.html
@@ -30,11 +30,11 @@ The main classes of interest are:
        as it contains many assertions and checks to catch bugs. </li>
    <li>{@link org.apache.lucene.analysis.MockTokenizer}: Tokenizer for testing.
        Tokenizer that serves as a replacement for WHITESPACE, SIMPLE, and KEYWORD
-       tokenizers. If you are writing a component such as a TokenFilter, its a great idea to test
+       tokenizers. If you are writing a component such as a TokenFilter, it's a great idea to test
        it wrapping this tokenizer instead for extra checks. </li>
    <li>{@link org.apache.lucene.analysis.MockAnalyzer}: Analyzer for testing.
        Analyzer that uses MockTokenizer for additional verification. If you are testing a custom 
-       component such as a queryparser or analyzer-wrapper that consumes analysis streams, its a great 
+       component such as a queryparser or analyzer-wrapper that consumes analysis streams, it's a great 
        idea to test it with this analyzer instead. </li>
 </ul>
 </p>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/HighCompressionCompressingCodec.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/HighCompressionCompressingCodec.java
index a9de107..508810e 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/HighCompressionCompressingCodec.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/compressing/HighCompressionCompressingCodec.java
@@ -29,7 +29,7 @@ public class HighCompressionCompressingCodec extends CompressingCodec {
 
   /** Default constructor. */
   public HighCompressionCompressingCodec() {
-    // we don't worry about zlib block overhead as its
+    // we don't worry about zlib block overhead as it's
     // not bad and try to save space instead:
     this(61440, 512, false);
   }
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
index cc8f628..2a53199 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
@@ -91,7 +91,7 @@ public final class MockRandomPostingsFormat extends PostingsFormat {
       minSkipInterval = 2;
     }
 
-    // we pull this before the seed intentionally: because its not consumed at runtime
+    // we pull this before the seed intentionally: because it's not consumed at runtime
     // (the skipInterval is written into postings header).
     // NOTE: Currently not passed to postings writer.
     //       before, it was being passed in wrongly as acceptableOverhead!
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/AlcoholicMergePolicy.java b/lucene/test-framework/src/java/org/apache/lucene/index/AlcoholicMergePolicy.java
index c4ae077..0f21e1a 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/AlcoholicMergePolicy.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/AlcoholicMergePolicy.java
@@ -58,7 +58,7 @@ public class AlcoholicMergePolicy extends LogMergePolicy {
     int hourOfDay = calendar.get(Calendar.HOUR_OF_DAY);
     if (hourOfDay < 6 || 
         hourOfDay > 20 || 
-        // its 5 o'clock somewhere
+        // it's 5 o'clock somewhere
         random.nextInt(23) == 5) {
       
       Drink[] values = Drink.values();
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java b/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
index 0234022..5a06e12 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/RandomCodec.java
@@ -74,7 +74,7 @@ public class RandomCodec extends AssertingCodec {
   public final Set<String> avoidCodecs;
 
   /** memorized field to postingsformat mappings */
-  // note: we have to sync this map even though its just for debugging/toString, 
+  // note: we have to sync this map even though it's just for debugging/toString, 
   // otherwise DWPT's .toString() calls that iterate over the map can 
   // cause concurrentmodificationexception if indexwriter's infostream is on
   private Map<String,PostingsFormat> previousMappings = Collections.synchronizedMap(new HashMap<String,PostingsFormat>());
diff --git a/lucene/test-framework/src/java/org/apache/lucene/mockfile/LeakFS.java b/lucene/test-framework/src/java/org/apache/lucene/mockfile/LeakFS.java
index 7310034..c3a4e8c 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/mockfile/LeakFS.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/mockfile/LeakFS.java
@@ -55,7 +55,7 @@ public class LeakFS extends HandleTrackingFS {
   @Override
   public synchronized void onClose() {
     if (!openHandles.isEmpty()) {
-      // print the first one as its very verbose otherwise
+      // print the first one as it's very verbose otherwise
       Exception cause = null;
       Iterator<Exception> stacktraces = openHandles.values().iterator();
       if (stacktraces.hasNext()) {
diff --git a/lucene/test-framework/src/java/org/apache/lucene/mockfile/WindowsFS.java b/lucene/test-framework/src/java/org/apache/lucene/mockfile/WindowsFS.java
index d1c7862..1b732eb 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/mockfile/WindowsFS.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/mockfile/WindowsFS.java
@@ -89,7 +89,7 @@ public class WindowsFS extends HandleTrackingFS {
   }
   
   /** 
-   * Checks that its ok to delete {@code Path}. If the file
+   * Checks that it's ok to delete {@code Path}. If the file
    * is still open, it throws IOException("access denied").
    */
   private void checkDeleteAccess(Path path) throws IOException {
diff --git a/lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java b/lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java
index a40c8df..c0e3aed 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/search/CheckHits.java
@@ -342,13 +342,13 @@ public class CheckHits {
     if (!deep) return;
 
     Explanation detail[] = expl.getDetails();
-    // TODO: can we improve this entire method? its really geared to work only with TF/IDF
+    // TODO: can we improve this entire method? it's really geared to work only with TF/IDF
     if (expl.getDescription().endsWith("computed from:")) {
       return; // something more complicated.
     }
     if (detail!=null) {
       if (detail.length==1) {
-        // simple containment, unless its a freq of: (which lets a query explain how the freq is calculated), 
+        // simple containment, unless it's a freq of: (which lets a query explain how the freq is calculated), 
         // just verify contained expl has same score
         if (!expl.getDescription().endsWith("with freq of:"))
           verifyExplanation(q,doc,score,deep,detail[0]);
diff --git a/lucene/test-framework/src/java/org/apache/lucene/store/BaseDirectoryTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/store/BaseDirectoryTestCase.java
index a89bc32..943bf45 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/store/BaseDirectoryTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/store/BaseDirectoryTestCase.java
@@ -735,7 +735,7 @@ public abstract class BaseDirectoryTestCase extends LuceneTestCase {
     
     // this test backdoors the directory via the filesystem. so it must be an FSDir (for now)
     // TODO: figure a way to test this better/clean it up. E.g. we should be testing for FileSwitchDir,
-    // if its using two FSdirs and so on
+    // if it's using two FSdirs and so on
     if (fsdir instanceof FSDirectory == false) {
       fsdir.close();
       assumeTrue("test only works for FSDirectory subclasses", false);
diff --git a/lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java b/lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java
index aae4264..fec20e2 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/store/MockDirectoryWrapper.java
@@ -269,7 +269,7 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
       success = true;
     } finally {
       if (success) {
-        // we don't do this stuff with lucene's commit, but its just for completeness
+        // we don't do this stuff with lucene's commit, but it's just for completeness
         if (unSyncedFiles.contains(source)) {
           unSyncedFiles.remove(source);
           unSyncedFiles.add(dest);
@@ -751,7 +751,7 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
         openFilesDeleted = new HashSet<>();
       }
       if (openFiles.size() > 0) {
-        // print the first one as its very verbose otherwise
+        // print the first one as it's very verbose otherwise
         Exception cause = null;
         Iterator<Exception> stacktraces = openFileHandles.values().iterator();
         if (stacktraces.hasNext()) {
@@ -806,7 +806,7 @@ public class MockDirectoryWrapper extends BaseDirectoryWrapper {
               }
             }
             
-            // its possible we cannot delete the segments_N on windows if someone has it open and
+            // it's possible we cannot delete the segments_N on windows if someone has it open and
             // maybe other files too, depending on timing. normally someone on windows wouldnt have
             // an issue (IFD would nuke this stuff eventually), but we pass NoDeletionPolicy...
             for (String file : pendingDeletions) {
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java b/lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
index 8c39bb9..e0d06dc 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/LineFileDocs.java
@@ -90,7 +90,7 @@ public class LineFileDocs implements Closeable {
     boolean needSkip = true;
     long size = 0L, seekTo = 0L;
     if (is == null) {
-      // if its not in classpath, we load it as absolute filesystem path (e.g. Hudson's home dir)
+      // if it's not in classpath, we load it as absolute filesystem path (e.g. Hudson's home dir)
       Path file = Paths.get(path);
       size = Files.size(file);
       if (path.endsWith(".gz")) {
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java b/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
index 9e1ad64..d6cbda4 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/TestRuleSetupAndRestoreClassEnv.java
@@ -113,7 +113,7 @@ final class TestRuleSetupAndRestoreClassEnv extends AbstractBeforeAfterRule {
 
   @Override
   protected void before() throws Exception {
-    // enable this by default, for IDE consistency with ant tests (as its the default from ant)
+    // enable this by default, for IDE consistency with ant tests (as it's the default from ant)
     // TODO: really should be in solr base classes, but some extend LTC directly.
     // we do this in beforeClass, because some tests currently disable it
     restoreProperties.put("solr.directoryFactory", System.getProperty("solr.directoryFactory"));
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java b/lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
index ad18f00..0a2f976 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/TestUtil.java
@@ -847,7 +847,7 @@ public final class TestUtil {
     }
     MergeScheduler ms = w.getConfig().getMergeScheduler();
     if (ms instanceof ConcurrentMergeScheduler) {
-      // wtf... shouldnt it be even lower since its 1 by default?!?!
+      // wtf... shouldnt it be even lower since it's 1 by default?!?!
       ((ConcurrentMergeScheduler) ms).setMaxMergesAndThreads(3, 2);
     }
   }
diff --git a/lucene/tools/build.xml b/lucene/tools/build.xml
index 4eed862..4f4ed51 100644
--- a/lucene/tools/build.xml
+++ b/lucene/tools/build.xml
@@ -33,7 +33,7 @@
 
   <path id="test.classpath"/>
 
-  <!-- redefine the test compilation, so its just a no-op -->
+  <!-- redefine the test compilation, so it's just a no-op -->
   <target name="compile-test"/>
   
   <!-- redefine the forbidden apis to be no-ops -->
diff --git a/lucene/tools/custom-tasks.xml b/lucene/tools/custom-tasks.xml
index 6b4f823..53c4c93 100644
--- a/lucene/tools/custom-tasks.xml
+++ b/lucene/tools/custom-tasks.xml
@@ -47,7 +47,7 @@
     
     <replaceregex pattern="[-]tests$" replace="-tests" flags="gi" />
 
-    <!-- git hashcode pattern: its always 40 chars right? -->
+    <!-- git hashcode pattern: it's always 40 chars right? -->
     <replaceregex pattern="\-[a-z0-9]{40,40}$" replace="" flags="gi" />
   </filtermapper>
 
diff --git a/solr/CHANGES.txt b/solr/CHANGES.txt
index 3b10c42..0ebec2c 100644
--- a/solr/CHANGES.txt
+++ b/solr/CHANGES.txt
@@ -916,9 +916,9 @@ Bug Fixes
 * SOLR-6393: TransactionLog replay performance on HDFS is very poor. (Mark Miller)  
 
 * SOLR-6268: HdfsUpdateLog has a race condition that can expose a closed HDFS FileSystem instance and should 
-  close it's FileSystem instance if either inherited close method is called. (Mark Miller)
+  close its FileSystem instance if either inherited close method is called. (Mark Miller)
 
-* SOLR-6089: When using the HDFS block cache, when a file is deleted, it's underlying data entries in the 
+* SOLR-6089: When using the HDFS block cache, when a file is deleted, its underlying data entries in the 
   block cache are not removed, which is a problem with the global block cache option. 
   (Mark Miller, Patrick Hunt)
 
@@ -994,7 +994,7 @@ Other Changes
 
 * SOLR-6270: Increased timeouts for MultiThreadedOCPTest. (shalin)
 
-* SOLR-6274: UpdateShardHandler should log the params used to configure it's
+* SOLR-6274: UpdateShardHandler should log the params used to configure its
   HttpClient. (Ramkumar Aiyengar via Mark Miller)
 
 * SOLR-6194: Opened up "public" access to DataSource, DocBuilder, and EntityProcessorWrapper
@@ -1100,7 +1100,7 @@ Bug Fixes
   (Timothy Potter) 
 
 * SOLR-6002: Fix a couple of ugly issues around SolrIndexWriter close and 
-  rollback as well as how SolrIndexWriter manages it's ref counted directory
+  rollback as well as how SolrIndexWriter manages its ref counted directory
   instance. (Mark Miller, Gregory Chanan)
 
 * SOLR-6015: Better way to handle managed synonyms when ignoreCase=true
@@ -1529,7 +1529,7 @@ Bug Fixes
 * SOLR-5647: The lib paths in example-schemaless will now load correctly.
   (Paul Westin via Shawn Heisey)
 
-* SOLR-5770: All attempts to match a SolrCore with it's state in clusterstate.json
+* SOLR-5770: All attempts to match a SolrCore with its state in clusterstate.json
   should be done with the CoreNodeName. (Steve Davids via Mark Miller)
 
 * SOLR-5875: QueryComponent.mergeIds() unmarshals all docs' sort field values once
@@ -1590,7 +1590,7 @@ Bug Fixes
   problem if you hit a bad work item. (Mark Miller)
 
 * SOLR-5796: Increase how long we are willing to wait for a core to see the ZK
-  advertised leader in it's local state. (Timothy Potter, Mark Miller)  
+  advertised leader in its local state. (Timothy Potter, Mark Miller)  
 
 * SOLR-5834: Overseer threads are only being interrupted and not closed.
   (hossman, Mark Miller)
@@ -1618,7 +1618,7 @@ Other Changes
 ---------------------
 
 * SOLR-5796: Make how long we are willing to wait for a core to see the ZK
-  advertised leader in it's local state configurable. 
+  advertised leader in its local state configurable. 
   (Timothy Potter via Mark Miller)
 
 ==================  4.7.0 ==================
@@ -1758,8 +1758,8 @@ Bug Fixes
 
 * SOLR-4612: Admin UI - Analysis Screen contains empty table-columns (steffkes)
 
-* SOLR-5451: SyncStrategy closes it's http connection manager before the
-  executor that uses it in it's close method. (Mark Miller)
+* SOLR-5451: SyncStrategy closes its http connection manager before the
+  executor that uses it in its close method. (Mark Miller)
 
 * SOLR-5460: SolrDispatchFilter#sendError can get a SolrCore that it does not 
   close. (Mark Miller)
@@ -1767,7 +1767,7 @@ Bug Fixes
 * SOLR-5461: Request proxying should only set con.setDoOutput(true) if the
   request is a post. (Mark Miller)
 
-* SOLR-5481: SolrCmdDistributor should not let the http client do it's own 
+* SOLR-5481: SolrCmdDistributor should not let the http client do its own 
   retries. (Mark Miller)
 
 * LUCENE-5347: Fixed Solr's Zookeeper Client to copy files to Zookeeper using
@@ -1848,7 +1848,7 @@ Bug Fixes
   of multiValued string fields. (Andreas Hubold, Vitaliy Zhovtyuk via shalin)
 
 * SOLR-5593: Replicas should accept the last updates from a leader that has just 
-  lost it's connection to ZooKeeper. (Christine Poerschke via Mark Miller)
+  lost its connection to ZooKeeper. (Christine Poerschke via Mark Miller)
 
 * SOLR-5678: SolrZkClient should throw a SolrException when connect times out
   rather than a RuntimeException. (Karl Wright, Anshum Gupta, Mark Miller)
@@ -2460,7 +2460,7 @@ New Features
   where items are preserved across commits.  (Robert Muir)
 
 * SOLR-4249: UniqFieldsUpdateProcessorFactory now extends 
-  FieldMutatingUpdateProcessorFactory and supports all of it's selector options. Use
+  FieldMutatingUpdateProcessorFactory and supports all of its selector options. Use
   of the "fields" init param is now deprecated in favor of "fieldName" (hossman)
   
 * SOLR-2548: Allow multiple threads to be specified for faceting. When threading, one
@@ -3277,7 +3277,7 @@ Bug Fixes
   fullpath not path. (Mark Miller)
 
 * SOLR-4555: When forceNew is used with CachingDirectoryFactory#get, the old
-  CachValue should give up it's path as it will be used by a new Directory
+  CachValue should give up its path as it will be used by a new Directory
   instance. (Mark Miller)
 
 * SOLR-4578: CoreAdminHandler#handleCreateAction gets a SolrCore and does not
@@ -3298,7 +3298,7 @@ Bug Fixes
   working correctly. (Mark Miller)
 
 * SOLR-4570: Even if an explicit shard id is used, ZkController#preRegister 
-  should still wait to see the shard id in it's current ClusterState.
+  should still wait to see the shard id in its current ClusterState.
   (Mark Miller)
 
 * SOLR-4585: The Collections API validates numShards with < 0 but should use 
@@ -3326,7 +3326,7 @@ Bug Fixes
   Directory has a refCnt of 0, but it should call closeDirectory(CacheValue).
   (Mark Miller)
 
-* SOLR-4602: ZkController#unregister should cancel it's election participation 
+* SOLR-4602: ZkController#unregister should cancel its election participation 
   before asking the Overseer to delete the SolrCore information. (Mark Miller)
 
 * SOLR-4601: A Collection that is only partially created and then deleted will 
@@ -3355,7 +3355,7 @@ Bug Fixes
   when used in field:value queries in the lucene QParser.  (hossman, yonik)
 
 * SOLR-4617: SolrCore#reload needs to pass the deletion policy to the next 
-  SolrCore through it's constructor rather than setting a field after.
+  SolrCore through its constructor rather than setting a field after.
   (Mark Miller)
     
 * SOLR-4589: Fixed CPU spikes and poor performance in lazy field loading 
@@ -3810,7 +3810,7 @@ New Features
 * SOLR-4271: Add support for PostingsHighlighter.  (Robert Muir)
 
 * SOLR-4255: The new Solr 4 spatial fields now have a 'filter' boolean local-param
-  that can be set to false to not filter. Its useful when there is already a spatial
+  that can be set to false to not filter. It's useful when there is already a spatial
   filter query but you also need to sort or boost by distance. (David Smiley)
 
 * SOLR-4265, SOLR-4283: Solr now parses request parameters (in URL or sent with POST
@@ -4020,7 +4020,7 @@ Bug Fixes
 * SOLR-3959: Ensure the internal comma separator of poly fields is escaped
   for CSVResponseWriter.  (Areek Zillur via Robert Muir)
   
-* SOLR-4075: A logical shard that has had all of it's SolrCores unloaded should 
+* SOLR-4075: A logical shard that has had all of its SolrCores unloaded should 
   be removed from the cluster state. (Mark Miller, Gilles Comeau)
   
 * SOLR-4034: Check if a collection already exists before trying to create a
@@ -4030,7 +4030,7 @@ Bug Fixes
   (Mark Miller)
   
 * SOLR-4099: Allow the collection api work queue to make forward progress even
-  when it's watcher is not fired for some reason. (Raintung Li via Mark Miller)
+  when its watcher is not fired for some reason. (Raintung Li via Mark Miller)
 
 * SOLR-3960: Fixed a bug where Distributed Grouping ignored PostFilters
   (Nathan Visagan, hossman)
@@ -4046,7 +4046,7 @@ Bug Fixes
   options from being respected in some <fieldType/> declarations (hossman)
 
 * SOLR-4159: When we are starting a shard from rest, a potential leader should 
-  not consider it's last published state when deciding if it can be the new 
+  not consider its last published state when deciding if it can be the new 
   leader. (Mark Miller)
 
 * SOLR-4158: When a core is registering in ZooKeeper it may not wait long 
@@ -4078,7 +4078,7 @@ Bug Fixes
   (steffkes via hossman)
 
 * SOLR-4178: ReplicationHandler should abort any current pulls and wait for 
-  it's executor to stop during core close. (Mark Miller)
+  its executor to stop during core close. (Mark Miller)
 
 * SOLR-3918: Fixed the 'dist-war-excl-slf4j' ant target to exclude all
   slf4j jars, so that the resulting war is usable as is provided the servlet 
@@ -4278,7 +4278,7 @@ Upgrading from Solr 4.0.0-BETA
 In order to better support distributed search mode, the TermVectorComponent's
 response format has been changed so that if the schema defines a 
 uniqueKeyField, then that field value is used as the "key" for each document in
-it's response section, instead of the internal lucene doc id.  Users w/o a 
+its response section, instead of the internal lucene doc id.  Users w/o a 
 uniqueKeyField will continue to see the same response format.  See SOLR-3229
 for more details.
 
@@ -4572,7 +4572,7 @@ Bug Fixes
 
 * SOLR-3783: Fixed Pivot Faceting to work with facet.missing=true (hossman)
 
-* SOLR-3869: A PeerSync attempt to it's replicas by a candidate leader should
+* SOLR-3869: A PeerSync attempt to its replicas by a candidate leader should
   not fail on o.a.http.conn.ConnectTimeoutException. (Mark Miller)
 
 * SOLR-3875: Fixed index boosts on multi-valued fields when docBoost is used 
@@ -6505,7 +6505,7 @@ New Features
   the terms component. Example: fq={!term f=weight}1.5   (hossman, yonik) 
 
 * SOLR-1915: DebugComponent now supports using a NamedList to model
-  Explanation objects in it's responses instead of
+  Explanation objects in its responses instead of
   Explanation.toString  (hossman)
 
 * SOLR-2448: Search results clustering updates: bisecting k-means
@@ -6539,7 +6539,7 @@ Bug Fixes
   commit point on server startup is never removed. (yonik)
 
 * SOLR-2466: SolrJ's CommonsHttpSolrServer would retry requests on failure, regardless
-  of the configured maxRetries, due to HttpClient having it's own retry mechanism
+  of the configured maxRetries, due to HttpClient having its own retry mechanism
   by default.  The retryCount of HttpClient is now set to 0, and SolrJ does
   the retry.  (yonik)
 
@@ -7449,7 +7449,7 @@ If you use custom Tokenizer or TokenFilter components in a chain specified in
 schema.xml, they must support reusability.  If your Tokenizer or TokenFilter
 maintains state, it should implement reset().  If your TokenFilteFactory does
 not return a subclass of TokenFilter, then it should implement reset() and call
-reset() on it's input TokenStream.  TokenizerFactory implementations must
+reset() on its input TokenStream.  TokenizerFactory implementations must
 now return a Tokenizer rather than a TokenStream.
 
 New users of Solr 1.4 will have omitTermFreqAndPositions enabled for non-text
@@ -7687,8 +7687,8 @@ New Features
 47. SOLR-1106: Made CoreAdminHandler Actions pluggable so that additional actions may be plugged in or the existing
     ones can be overridden if needed. (Kay Kay, Noble Paul, shalin)
 
-48. SOLR-1124: Add a top() function query that causes it's argument to
-    have it's values derived from the top level IndexReader, even when
+48. SOLR-1124: Add a top() function query that causes its argument to
+    have its values derived from the top level IndexReader, even when
     invoked from a sub-reader.  top() is implicitly used for the
     ord() and rord() functions.  (yonik)
 
@@ -9440,7 +9440,7 @@ Changes in runtime behavior
  3. A new method "getSolrQueryParser" has been added to the IndexSchema
     class for retrieving a new SolrQueryParser instance with all options
     specified in the schema.xml's <solrQueryParser> block set.  The
-    documentation for the SolrQueryParser constructor and it's use of
+    documentation for the SolrQueryParser constructor and its use of
     IndexSchema have also been clarified.
     (Erik Hatcher and hossman)
 
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationField.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationField.java
index 161f647..1d51fc0 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationField.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationField.java
@@ -60,7 +60,7 @@ public class TestICUCollationField extends SolrTestCaseJ4 {
    * Ugly: but what to do? We want to test custom sort, which reads rules in as a resource.
    * These are largish files, and jvm-specific (as our documentation says, you should always
    * look out for jvm differences with collation).
-   * So its preferable to create this file on-the-fly.
+   * So it's preferable to create this file on-the-fly.
    */
   public static String setupSolrHome() throws Exception {
     String tmpFile = createTempDir().toFile().getAbsolutePath();
diff --git a/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationFieldDocValues.java b/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationFieldDocValues.java
index cba27a0..00baad1 100644
--- a/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationFieldDocValues.java
+++ b/solr/contrib/analysis-extras/src/test/org/apache/solr/schema/TestICUCollationFieldDocValues.java
@@ -58,7 +58,7 @@ public class TestICUCollationFieldDocValues extends SolrTestCaseJ4 {
    * Ugly: but what to do? We want to test custom sort, which reads rules in as a resource.
    * These are largish files, and jvm-specific (as our documentation says, you should always
    * look out for jvm differences with collation).
-   * So its preferable to create this file on-the-fly.
+   * So it's preferable to create this file on-the-fly.
    */
   public static String setupSolrHome() throws Exception {
     File tmpFile = createTempDir().toFile();
diff --git a/solr/contrib/analytics/src/java/org/apache/solr/analytics/expression/MultiDelegateExpression.java b/solr/contrib/analytics/src/java/org/apache/solr/analytics/expression/MultiDelegateExpression.java
index 4ea4825..bb979df 100644
--- a/solr/contrib/analytics/src/java/org/apache/solr/analytics/expression/MultiDelegateExpression.java
+++ b/solr/contrib/analytics/src/java/org/apache/solr/analytics/expression/MultiDelegateExpression.java
@@ -33,7 +33,7 @@ public abstract class MultiDelegateExpression extends Expression {
   }
 }
 /**
- * <code>AddExpression</code> returns the sum of it's components' values.
+ * <code>AddExpression</code> returns the sum of its components' values.
  */
 class AddExpression extends MultiDelegateExpression {
   public AddExpression(Expression[] delegates) {
@@ -56,7 +56,7 @@ class AddExpression extends MultiDelegateExpression {
   }
 }
 /**
- * <code>MultiplyExpression</code> returns the product of it's delegates' values.
+ * <code>MultiplyExpression</code> returns the product of its delegates' values.
  */
 class MultiplyExpression extends MultiDelegateExpression {
   public MultiplyExpression(Expression[] delegates) {
@@ -110,7 +110,7 @@ class DateMathExpression extends MultiDelegateExpression {
   }
 }
 /**
- * <code>ConcatenateExpression</code> returns the concatenation of it's delegates' values in the order given.
+ * <code>ConcatenateExpression</code> returns the concatenation of its delegates' values in the order given.
  */
 class ConcatenateExpression extends MultiDelegateExpression {
   public ConcatenateExpression(Expression[] delegates) {
diff --git a/solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/AddDoubleFunction.java b/solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/AddDoubleFunction.java
index 7784feb..edf98e4 100644
--- a/solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/AddDoubleFunction.java
+++ b/solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/AddDoubleFunction.java
@@ -22,7 +22,7 @@ import org.apache.lucene.queries.function.ValueSource;
 import org.apache.solr.analytics.util.AnalyticsParams;
 
 /**
- * <code>AddDoubleFunction</code> returns the sum of it's components.
+ * <code>AddDoubleFunction</code> returns the sum of its components.
  */
 public class AddDoubleFunction extends MultiDoubleFunction {
   public final static String NAME = AnalyticsParams.ADD;
diff --git a/solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/MultiplyDoubleFunction.java b/solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/MultiplyDoubleFunction.java
index 5f9de24..62c50b5 100644
--- a/solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/MultiplyDoubleFunction.java
+++ b/solr/contrib/analytics/src/java/org/apache/solr/analytics/util/valuesource/MultiplyDoubleFunction.java
@@ -22,7 +22,7 @@ import org.apache.lucene.queries.function.ValueSource;
 import org.apache.solr.analytics.util.AnalyticsParams;
 
 /**
- * <code>MultiplyDoubleFunction</code> returns the product of it's components.
+ * <code>MultiplyDoubleFunction</code> returns the product of its components.
  */
 public class MultiplyDoubleFunction extends MultiDoubleFunction {
   public final static String NAME = AnalyticsParams.MULTIPLY;
diff --git a/solr/contrib/clustering/src/test-files/clustering/solr/collection1/conf/solrconfig.xml b/solr/contrib/clustering/src/test-files/clustering/solr/collection1/conf/solrconfig.xml
index c2a190a..a8ee6e8 100644
--- a/solr/contrib/clustering/src/test-files/clustering/solr/collection1/conf/solrconfig.xml
+++ b/solr/contrib/clustering/src/test-files/clustering/solr/collection1/conf/solrconfig.xml
@@ -305,7 +305,7 @@
   </requestHandler>
 
   <!-- DisMaxRequestHandler allows easy searching across multiple fields
-       for simple user-entered phrases.  It's implementation is now
+       for simple user-entered phrases.  Its implementation is now
        just the standard SearchHandler with a default query parser
        of "dismax". 
        see http://wiki.apache.org/solr/DisMaxRequestHandler
diff --git a/solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java b/solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java
index 9016c26..0b34e9d 100644
--- a/solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java
+++ b/solr/contrib/dataimporthandler-extras/src/java/org/apache/solr/handler/dataimport/MailEntityProcessor.java
@@ -534,7 +534,7 @@ public class MailEntityProcessor extends EntityProcessorBase {
     }
     
     public void remove() {
-      throw new UnsupportedOperationException("Its read only mode...");
+      throw new UnsupportedOperationException("It's read only mode...");
     }
     
     private void getTopLevelFolders(Store mailBox) {
@@ -544,7 +544,7 @@ public class MailEntityProcessor extends EntityProcessorBase {
         try {
           folders.add(mailbox.getFolder(topLevelFolders.get(i)));
         } catch (MessagingException e) {
-          // skip bad ones unless its the last one and still no good folder
+          // skip bad ones unless it's the last one and still no good folder
           if (folders.size() == 0 && i == topLevelFolders.size() - 1) throw new DataImportHandlerException(
               DataImportHandlerException.SEVERE, "Folder retreival failed");
         }
@@ -705,7 +705,7 @@ public class MailEntityProcessor extends EntityProcessorBase {
     }
     
     public void remove() {
-      throw new UnsupportedOperationException("Its read only mode...");
+      throw new UnsupportedOperationException("It's read only mode...");
     }
     
     private SearchTerm getSearchTerm() {
diff --git a/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathRecordReader.java b/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathRecordReader.java
index f3a2f5a..58e57fe 100644
--- a/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathRecordReader.java
+++ b/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestXPathRecordReader.java
@@ -394,7 +394,7 @@ public class TestXPathRecordReader extends AbstractDataImportHandlerTestCase {
              + "  this <boo>top level</boo> is ignored because it is external to the forEach\n"
              + "  <status>as is <boo>this element</boo></status>\n"
              + "  <contenido id=\"10097\" idioma=\"cat\">\n"
-             + "    This one is <boo>not ignored as its</boo> inside a forEach\n"
+             + "    This one is <boo>not ignored as it's</boo> inside a forEach\n"
              + "    <antetitulo><i> big <boo>antler</boo></i></antetitulo>\n"
              + "    <titulo>  My <i>flattened <boo>title</boo></i> </titulo>\n"
              + "    <resumen> My summary <i>skip this!</i>  </resumen>\n"
@@ -407,15 +407,15 @@ public class TestXPathRecordReader extends AbstractDataImportHandlerTestCase {
     assertEquals(1, l.size());
     Map<String, Object> m = l.get(0);
     assertEquals("This one is  inside a forEach", m.get("cont").toString().trim());
-    assertEquals("10097"             ,m.get("id"));
-    assertEquals("My flattened title",m.get("title").toString().trim());
-    assertEquals("My summary"        ,m.get("resume").toString().trim());
-    assertEquals("My text"           ,m.get("text").toString().trim());
-    assertEquals("not ignored as its",(String) ((List) m.get("descdend")).get(0) );
-    assertEquals("antler"            ,(String) ((List) m.get("descdend")).get(1) );
-    assertEquals("Within the body of",(String) ((List) m.get("descdend")).get(2) );
-    assertEquals("inner  as well"    ,(String) ((List) m.get("descdend")).get(3) );
-    assertEquals("sub clauses"       ,m.get("inr_descd").toString().trim());
+    assertEquals("10097"              ,m.get("id"));
+    assertEquals("My flattened title" ,m.get("title").toString().trim());
+    assertEquals("My summary"         ,m.get("resume").toString().trim());
+    assertEquals("My text"            ,m.get("text").toString().trim());
+    assertEquals("not ignored as it's",(String) ((List) m.get("descdend")).get(0) );
+    assertEquals("antler"             ,(String) ((List) m.get("descdend")).get(1) );
+    assertEquals("Within the body of" ,(String) ((List) m.get("descdend")).get(2) );
+    assertEquals("inner  as well"     ,(String) ((List) m.get("descdend")).get(3) );
+    assertEquals("sub clauses"        ,m.get("inr_descd").toString().trim());
   }
 
   @Test
@@ -428,7 +428,7 @@ public class TestXPathRecordReader extends AbstractDataImportHandlerTestCase {
              + "  this <boo>top level</boo> is ignored because it is external to the forEach\n"
              + "  <status>as is <boo>this element</boo></status>\n"
              + "  <contenido id=\"10097\" idioma=\"cat\">\n"
-             + "    This one is <boo>not ignored as its</boo> inside a forEach\n"
+             + "    This one is <boo>not ignored as it's</boo> inside a forEach\n"
              + "    <antetitulo><i> big <boo>antler</boo></i></antetitulo>\n"
              + "    <titulo>  My <i>flattened <boo>title</boo></i> </titulo>\n"
              + "    <resumen> My summary <i>skip this!</i>  </resumen>\n"
@@ -440,13 +440,13 @@ public class TestXPathRecordReader extends AbstractDataImportHandlerTestCase {
     List<Map<String, Object>> l = rr.getAllRecords(new StringReader(xml));
     assertEquals(1, l.size());
     Map<String, Object> m = l.get(0);
-    assertEquals("top level"         ,(String) ((List) m.get("descdend")).get(0) );
-    assertEquals("this element"      ,(String) ((List) m.get("descdend")).get(1) );
-    assertEquals("not ignored as its",(String) ((List) m.get("descdend")).get(2) );
-    assertEquals("antler"            ,(String) ((List) m.get("descdend")).get(3) );
-    assertEquals("title"             ,(String) ((List) m.get("descdend")).get(4) );
-    assertEquals("Within the body of",(String) ((List) m.get("descdend")).get(5) );
-    assertEquals("inner  as well"    ,(String) ((List) m.get("descdend")).get(6) );
+    assertEquals("top level"          ,(String) ((List) m.get("descdend")).get(0) );
+    assertEquals("this element"       ,(String) ((List) m.get("descdend")).get(1) );
+    assertEquals("not ignored as it's",(String) ((List) m.get("descdend")).get(2) );
+    assertEquals("antler"             ,(String) ((List) m.get("descdend")).get(3) );
+    assertEquals("title"              ,(String) ((List) m.get("descdend")).get(4) );
+    assertEquals("Within the body of" ,(String) ((List) m.get("descdend")).get(5) );
+    assertEquals("inner  as well"     ,(String) ((List) m.get("descdend")).get(6) );
   }
 
   @Test
@@ -459,7 +459,7 @@ public class TestXPathRecordReader extends AbstractDataImportHandlerTestCase {
              + "  this <boo>top level</boo> is ignored because it is external to the forEach\n"
              + "  <status>as is <boo>this element</boo></status>\n"
              + "  <contenido id=\"10097\" idioma=\"cat\">\n"
-             + "    This one is <boo>not ignored as its</boo> inside a forEach\n"
+             + "    This one is <boo>not ignored as it's</boo> inside a forEach\n"
              + "    <antetitulo><i> big <boo>antler</boo></i></antetitulo>\n"
              + "    <titulo>  My <i>flattened <boo>title</boo></i> </titulo>\n"
              + "    <resumen> My summary <i>skip this!</i>  </resumen>\n"
@@ -471,11 +471,11 @@ public class TestXPathRecordReader extends AbstractDataImportHandlerTestCase {
     List<Map<String, Object>> l = rr.getAllRecords(new StringReader(xml));
     assertEquals(1, l.size());
     Map<String, Object> m = l.get(0);
-    assertEquals("not ignored as its",((List) m.get("descdend")).get(0) );
-    assertEquals("antler"            ,((List) m.get("descdend")).get(1) );
-    assertEquals("title"             ,((List) m.get("descdend")).get(2) );
-    assertEquals("Within the body of",((List) m.get("descdend")).get(3) );
-    assertEquals("inner  as well"    ,((List) m.get("descdend")).get(4) );
+    assertEquals("not ignored as it's",((List) m.get("descdend")).get(0) );
+    assertEquals("antler"             ,((List) m.get("descdend")).get(1) );
+    assertEquals("title"              ,((List) m.get("descdend")).get(2) );
+    assertEquals("Within the body of" ,((List) m.get("descdend")).get(3) );
+    assertEquals("inner  as well"     ,((List) m.get("descdend")).get(4) );
   }
   
   @Test
diff --git a/solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/solrconfig.xml b/solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/solrconfig.xml
index 94b5750..f8dec56 100644
--- a/solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/solrconfig.xml
+++ b/solr/contrib/morphlines-core/src/test-files/solr/collection1/conf/solrconfig.xml
@@ -122,7 +122,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/solrconfig.xml b/solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/solrconfig.xml
index 550474b..59759ac 100644
--- a/solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/solrconfig.xml
+++ b/solr/contrib/morphlines-core/src/test-files/solr/minimr/conf/solrconfig.xml
@@ -138,7 +138,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/solrconfig.xml b/solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/solrconfig.xml
index 39ffc59..3f41ef3 100644
--- a/solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/solrconfig.xml
+++ b/solr/contrib/morphlines-core/src/test-files/solr/mrunit/conf/solrconfig.xml
@@ -140,7 +140,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/solrconfig.xml b/solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/solrconfig.xml
index 2e864d3..d93ae8c 100644
--- a/solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/solrconfig.xml
+++ b/solr/contrib/morphlines-core/src/test-files/solr/solrcelltest/collection1/conf/solrconfig.xml
@@ -122,7 +122,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/contrib/morphlines-core/src/test-files/solr/solrcloud/conf/solrconfig.xml b/solr/contrib/morphlines-core/src/test-files/solr/solrcloud/conf/solrconfig.xml
index 831ba97..46b8461 100644
--- a/solr/contrib/morphlines-core/src/test-files/solr/solrcloud/conf/solrconfig.xml
+++ b/solr/contrib/morphlines-core/src/test-files/solr/solrcloud/conf/solrconfig.xml
@@ -141,7 +141,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/contrib/morphlines-core/src/test-files/test-morphlines/tutorialReadAvroContainer.conf b/solr/contrib/morphlines-core/src/test-files/test-morphlines/tutorialReadAvroContainer.conf
index 53bbbb7..0c00686 100644
--- a/solr/contrib/morphlines-core/src/test-files/test-morphlines/tutorialReadAvroContainer.conf
+++ b/solr/contrib/morphlines-core/src/test-files/test-morphlines/tutorialReadAvroContainer.conf
@@ -34,7 +34,7 @@ SOLR_LOCATOR : {
 # transformation chain. A morphline consists of one or more (potentially 
 # nested) commands. A morphline is a way to consume records (e.g. Flume events, 
 # HDFS files or blocks), turn them into a stream of records, and pipe the stream 
-# of records through a set of easily configurable transformations on it's way to 
+# of records through a set of easily configurable transformations on its way to 
 # Solr.
 morphlines : [
   {
diff --git a/solr/contrib/uima/src/test-files/uima/solr/collection1/conf/solrconfig.xml b/solr/contrib/uima/src/test-files/uima/solr/collection1/conf/solrconfig.xml
index b0ccebb..974a734 100644
--- a/solr/contrib/uima/src/test-files/uima/solr/collection1/conf/solrconfig.xml
+++ b/solr/contrib/uima/src/test-files/uima/solr/collection1/conf/solrconfig.xml
@@ -398,7 +398,7 @@
 
   <!--
     DisMaxRequestHandler allows easy searching across multiple fields
-    for simple user-entered phrases. It's implementation is now just the
+    for simple user-entered phrases. Its implementation is now just the
     standard SearchHandler with a default query parser of "dismax". see
     http://wiki.apache.org/solr/DisMaxRequestHandler
   -->
diff --git a/solr/contrib/uima/src/test-files/uima/uima-tokenizers-solrconfig.xml b/solr/contrib/uima/src/test-files/uima/uima-tokenizers-solrconfig.xml
index 5ddbe08..35184d4 100644
--- a/solr/contrib/uima/src/test-files/uima/uima-tokenizers-solrconfig.xml
+++ b/solr/contrib/uima/src/test-files/uima/uima-tokenizers-solrconfig.xml
@@ -397,7 +397,7 @@
 
   <!--
     DisMaxRequestHandler allows easy searching across multiple fields
-    for simple user-entered phrases. It's implementation is now just the
+    for simple user-entered phrases. Its implementation is now just the
     standard SearchHandler with a default query parser of "dismax". see
     http://wiki.apache.org/solr/DisMaxRequestHandler
   -->
diff --git a/solr/core/src/java/org/apache/solr/cloud/Assign.java b/solr/core/src/java/org/apache/solr/cloud/Assign.java
index a783417..a544852 100644
--- a/solr/core/src/java/org/apache/solr/cloud/Assign.java
+++ b/solr/core/src/java/org/apache/solr/cloud/Assign.java
@@ -174,7 +174,7 @@ public class Assign {
           + collectionName
           + " is higher than or equal to the number of Solr instances currently live or part of your " + CREATE_NODE_SET + "("
           + nodeList.size()
-          + "). Its unusual to run two replica of the same slice on the same Solr-instance.");
+          + "). It's unusual to run two replica of the same slice on the same Solr-instance.");
     }
 
     int maxCoresAllowedToCreate = maxShardsPerNode * nodeList.size();
diff --git a/solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread.java b/solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread.java
index a3edf9f..a4a4a00 100644
--- a/solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread.java
+++ b/solr/core/src/java/org/apache/solr/cloud/OverseerAutoReplicaFailoverThread.java
@@ -128,7 +128,7 @@ public class OverseerAutoReplicaFailoverThread implements Runnable, Closeable {
         doWork();
       } catch (Exception e) {
         SolrException.log(log, this.getClass().getSimpleName()
-            + " had an error it's thread work loop.", e);
+            + " had an error in its thread work loop.", e);
       }
       
       if (!this.isClosed) {
diff --git a/solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java b/solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java
index fdb7abf..f82fd69 100644
--- a/solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java
+++ b/solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java
@@ -2378,7 +2378,7 @@ public class OverseerCollectionProcessor implements Runnable, Closeable {
             + collectionName
             + " is higher than or equal to the number of Solr instances currently live or live and part of your " + CREATE_NODE_SET + "("
             + nodeList.size()
-            + "). Its unusual to run two replica of the same slice on the same Solr-instance.");
+            + "). It's unusual to run two replica of the same slice on the same Solr-instance.");
       }
       
       int maxShardsAllowedToCreate = maxShardsPerNode * nodeList.size();
diff --git a/solr/core/src/java/org/apache/solr/cloud/ZkController.java b/solr/core/src/java/org/apache/solr/cloud/ZkController.java
index 263ec4f..7510d9e 100644
--- a/solr/core/src/java/org/apache/solr/cloud/ZkController.java
+++ b/solr/core/src/java/org/apache/solr/cloud/ZkController.java
@@ -774,7 +774,7 @@ public final class ZkController {
       }
       zkClient.makePath(nodePath, CreateMode.EPHEMERAL, true);
     } catch (KeeperException e) {
-      // its okay if the node already exists
+      // it's okay if the node already exists
       if (e.code() != KeeperException.Code.NODEEXISTS) {
         throw e;
       }
@@ -1314,7 +1314,7 @@ public final class ZkController {
           zkClient.makePath(collectionPath, ZkStateReader.toJSON(zkProps), CreateMode.PERSISTENT, null, true);
 
         } catch (KeeperException e) {
-          // its okay if the node already exists
+          // it's okay if the node already exists
           if (e.code() != KeeperException.Code.NODEEXISTS) {
             throw e;
           }
@@ -1324,7 +1324,7 @@ public final class ZkController {
       }
       
     } catch (KeeperException e) {
-      // its okay if another beats us creating the node
+      // it's okay if another beats us creating the node
       if (e.code() == KeeperException.Code.NODEEXISTS) {
         return;
       }
@@ -1707,7 +1707,7 @@ public final class ZkController {
         zkClient.makePath(path, ZkStateReader.toJSON(props),
             CreateMode.PERSISTENT, null, true);
       } catch (KeeperException e2) {
-        // its okay if the node already exists
+        // it's okay if the node already exists
         if (e2.code() != KeeperException.Code.NODEEXISTS) {
           throw e;
         }
@@ -1811,7 +1811,7 @@ public final class ZkController {
   
   /**
    * Utility method for trimming and leading and/or trailing slashes from 
-   * it's input.  May return the empty string.  May return null if and only 
+   * its input.  May return the empty string.  May return null if and only 
    * if the input is null.
    */
   public static String trimLeadingAndTrailingSlashes(final String in) {
@@ -2057,7 +2057,7 @@ public final class ZkController {
       stateObj = ZkNodeProps.makeMap();
 
     stateObj.put("state", state);
-    // only update the createdBy value if its not set
+    // only update the createdBy value if it's not set
     if (stateObj.get("createdByNodeName") == null)
       stateObj.put("createdByNodeName", String.valueOf(this.nodeName));
 
diff --git a/solr/core/src/java/org/apache/solr/core/CloseHook.java b/solr/core/src/java/org/apache/solr/core/CloseHook.java
index 5cdaff2..032f44c 100644
--- a/solr/core/src/java/org/apache/solr/core/CloseHook.java
+++ b/solr/core/src/java/org/apache/solr/core/CloseHook.java
@@ -45,7 +45,7 @@ public abstract class CloseHook {
    * <br/>
    * Use this method for post-close clean up operations e.g. deleting the index from disk.
    * <br/>
-   * <b>The core's passed to the method is already closed and therefore, it's update handler or searcher should *NOT* be used</b>
+   * <b>The core's passed to the method is already closed and therefore, its update handler or searcher should *NOT* be used</b>
    *
    * <b>Important:</b> Keep the method implementation as short as possible. If it were to use any heavy i/o , network connections -
    * it might be a better idea to launch in a separate Thread so as to not to block the process of
diff --git a/solr/core/src/java/org/apache/solr/core/DirectoryFactory.java b/solr/core/src/java/org/apache/solr/core/DirectoryFactory.java
index 260b088..a1e39eb 100644
--- a/solr/core/src/java/org/apache/solr/core/DirectoryFactory.java
+++ b/solr/core/src/java/org/apache/solr/core/DirectoryFactory.java
@@ -51,7 +51,7 @@ public abstract class DirectoryFactory implements NamedListInitializedPlugin,
   private static final Logger log = LoggerFactory.getLogger(DirectoryFactory.class.getName());
   
   /**
-   * Indicates a Directory will no longer be used, and when it's ref count
+   * Indicates a Directory will no longer be used, and when its ref count
    * hits 0, it can be closed. On close all directories will be closed
    * whether this has been called or not. This is simply to allow early cleanup.
    * 
diff --git a/solr/core/src/java/org/apache/solr/core/JmxMonitoredMap.java b/solr/core/src/java/org/apache/solr/core/JmxMonitoredMap.java
index 6bba774..cea9768 100644
--- a/solr/core/src/java/org/apache/solr/core/JmxMonitoredMap.java
+++ b/solr/core/src/java/org/apache/solr/core/JmxMonitoredMap.java
@@ -335,13 +335,13 @@ public class JmxMonitoredMap<K, V> extends
       }
 
       if (val != null) {
-        // Its String or one of the simple types, just return it as JMX suggests direct support for such types
+        // It's String or one of the simple types, just return it as JMX suggests direct support for such types
         for (String simpleTypeName : SimpleType.ALLOWED_CLASSNAMES_LIST) {
           if (val.getClass().getName().equals(simpleTypeName)) {
             return val;
           }
         }
-        // Its an arbitrary object which could be something complex and odd, return its toString, assuming that is
+        // It's an arbitrary object which could be something complex and odd, return its toString, assuming that is
         // a workable representation of the object
         return val.toString();
       }
diff --git a/solr/core/src/java/org/apache/solr/core/RequestHandlers.java b/solr/core/src/java/org/apache/solr/core/RequestHandlers.java
index 494ed4a..7d3071e 100644
--- a/solr/core/src/java/org/apache/solr/core/RequestHandlers.java
+++ b/solr/core/src/java/org/apache/solr/core/RequestHandlers.java
@@ -70,7 +70,7 @@ public final class RequestHandlers {
   public static final boolean disableExternalLib = Boolean.parseBoolean(System.getProperty("disable.external.lib", "false"));
 
   /**
-   * Trim the trailing '/' if its there, and convert null to empty string.
+   * Trim the trailing '/' if it's there, and convert null to empty string.
    * 
    * we want:
    *  /update/csv   and
diff --git a/solr/core/src/java/org/apache/solr/core/SolrCore.java b/solr/core/src/java/org/apache/solr/core/SolrCore.java
index eacccca..c955a77 100644
--- a/solr/core/src/java/org/apache/solr/core/SolrCore.java
+++ b/solr/core/src/java/org/apache/solr/core/SolrCore.java
@@ -1473,7 +1473,7 @@ public final class SolrCore implements SolrInfoMBean, Closeable {
   }
 
 
-  /** Opens a new searcher and returns a RefCounted&lt;SolrIndexSearcher&gt; with it's reference incremented.
+  /** Opens a new searcher and returns a RefCounted&lt;SolrIndexSearcher&gt; with its reference incremented.
    *
    * "realtime" means that we need to open quickly for a realtime view of the index, hence don't do any
    * autowarming and add to the _realtimeSearchers queue rather than the _searchers queue (so it won't
diff --git a/solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java b/solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java
index 61f6cc3..93553ba 100644
--- a/solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java
+++ b/solr/core/src/java/org/apache/solr/core/SolrResourceLoader.java
@@ -162,7 +162,7 @@ public class SolrResourceLoader implements ResourceLoader,Closeable
    * Adds every file/dir found in the baseDir which passes the specified Filter
    * to the ClassLoader used by this ResourceLoader.  This method <b>MUST</b>
    * only be called prior to using this ResourceLoader to get any resources, otherwise
-   * it's behavior will be non-deterministic. You also have to {link @reloadLuceneSPI}
+   * its behavior will be non-deterministic. You also have to {link @reloadLuceneSPI}
    * before using this ResourceLoader.
    * 
    * <p>This method will quietly ignore missing or non-directory <code>baseDir</code>
@@ -424,7 +424,7 @@ public class SolrResourceLoader implements ResourceLoader,Closeable
   }
   
   /**
-   * This method loads a class either with it's FQN or a short-name (solr.class-simplename or class-simplename).
+   * This method loads a class either with its FQN or a short-name (solr.class-simplename or class-simplename).
    * It tries to load the class with the name that is given first and if it fails, it tries all the known
    * solr packages. This method caches the FQN of a short-name in a static map in-order to make subsequent lookups
    * for the same class faster. The caching is done only if the class is loaded by the webapp classloader and it
diff --git a/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java b/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java
index 1ca0d4f..b94ec10 100644
--- a/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java
+++ b/solr/core/src/java/org/apache/solr/handler/PingRequestHandler.java
@@ -50,7 +50,7 @@ import org.slf4j.LoggerFactory;
  * </p>
  * 
  * <p> 
- * In it's simplest form, the PingRequestHandler should be
+ * In its simplest form, the PingRequestHandler should be
  * configured with some defaults indicating a request that should be
  * executed.  If the request succeeds, then the PingRequestHandler
  * will respond back with a simple "OK" status.  If the request fails,
diff --git a/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java b/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
index 7f23920..218f460 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java
@@ -651,7 +651,7 @@ public class SpellCheckComponent extends SearchComponent implements SolrCoreAwar
           NamedList spellchecker = (NamedList) initParams.getVal(i);
           String className = (String) spellchecker.get("classname");
           // TODO: this is a little bit sneaky: warn if class isnt supplied
-          // so that its mandatory in a future release?
+          // so that it's mandatory in a future release?
           if (className == null)
             className = IndexBasedSpellChecker.class.getName();
           SolrResourceLoader loader = core.getResourceLoader();
diff --git a/solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java b/solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java
index 9592e6c..c357097 100644
--- a/solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java
+++ b/solr/core/src/java/org/apache/solr/highlight/DefaultSolrHighlighter.java
@@ -759,7 +759,7 @@ class OrderedToken {
   int startOffset;
 }
 
-/** For use with term vectors of multi-valued fields. We want an offset based window into it's TokenStream. */
+/** For use with term vectors of multi-valued fields. We want an offset based window into its TokenStream. */
 final class OffsetWindowTokenFilter extends TokenFilter {
 
   private final OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
diff --git a/solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java b/solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java
index 007ed7b..9f4eb72 100644
--- a/solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java
+++ b/solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java
@@ -161,7 +161,7 @@ public class PostingsSolrHighlighter extends SolrHighlighter implements PluginIn
       for (String field : fieldNames) {
         String snippet = snippets.get(field)[i];
         // box in an array to match the format of existing highlighters, 
-        // even though its always one element.
+        // even though it's always one element.
         if (snippet == null) {
           summary.add(field, new String[0]);
         } else {
diff --git a/solr/core/src/java/org/apache/solr/internal/csv/CSVParser.java b/solr/core/src/java/org/apache/solr/internal/csv/CSVParser.java
index 169c6d9..0582fc1 100644
--- a/solr/core/src/java/org/apache/solr/internal/csv/CSVParser.java
+++ b/solr/core/src/java/org/apache/solr/internal/csv/CSVParser.java
@@ -55,7 +55,7 @@ public class CSVParser {
   private static final int INITIAL_TOKEN_LENGTH = 50;
   
   // the token types
-  /** Token has no valid content, i.e. is in its initilized state. */
+  /** Token has no valid content, i.e. is in its initialized state. */
   protected static final int TT_INVALID = -1;
   /** Token with content, at beginning or in the middle of a line. */
   protected static final int TT_TOKEN = 0;
diff --git a/solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java b/solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java
index 6a45de5..3b421ce 100644
--- a/solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java
+++ b/solr/core/src/java/org/apache/solr/parser/SolrQueryParserBase.java
@@ -791,7 +791,7 @@ public abstract class SolrQueryParserBase extends QueryBuilder {
         automaton = Operations.minus(automaton, falsePositives, Operations.DEFAULT_MAX_DETERMINIZED_STATES);
       }
       return new AutomatonQuery(term, automaton) {
-        // override toString so its completely transparent
+        // override toString so it's completely transparent
         @Override
         public String toString(String field) {
           StringBuilder buffer = new StringBuilder();
diff --git a/solr/core/src/java/org/apache/solr/request/SimpleFacets.java b/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
index 0c47bbe..1273a6d 100644
--- a/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
+++ b/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
@@ -99,7 +99,7 @@ import org.apache.solr.util.DefaultSolrThreadFactory;
  * A class that generates simple Facet information for a request.
  *
  * More advanced facet implementations may compose or subclass this class 
- * to leverage any of it's functionality.
+ * to leverage any of its functionality.
  */
 public class SimpleFacets {
 
@@ -492,8 +492,8 @@ public class SimpleFacets {
     SchemaField sf = searcher.getSchema().getFieldOrNull(groupField);
     
     if (sf != null && sf.hasDocValues() == false && sf.multiValued() == false && sf.getType().getNumericType() != null) {
-      // its a single-valued numeric field: we must currently create insanity :(
-      // there isnt a GroupedFacetCollector that works on numerics right now...
+      // it's a single-valued numeric field: we must currently create insanity :(
+      // there isn't a GroupedFacetCollector that works on numerics right now...
       searcher.search(new MatchAllDocsQuery(), base.getTopFilter(), new FilterCollector(collector) {
         @Override
         public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException {
diff --git a/solr/core/src/java/org/apache/solr/request/UnInvertedField.java b/solr/core/src/java/org/apache/solr/request/UnInvertedField.java
index d96ea5d..0a5aaa9 100644
--- a/solr/core/src/java/org/apache/solr/request/UnInvertedField.java
+++ b/solr/core/src/java/org/apache/solr/request/UnInvertedField.java
@@ -69,7 +69,7 @@ import org.apache.solr.util.PrimUtils;
  *
  *   There are actually 256 byte arrays, to compensate for the fact that the pointers
  *   into the byte arrays are only 3 bytes long.  The correct byte array for a document
- *   is a function of it's id.
+ *   is a function of its id.
  *
  *   To save space and speed up faceting, any term that matches enough documents will
  *   not be un-inverted... it will be skipped while building the un-inverted field structure,
@@ -78,7 +78,7 @@ import org.apache.solr.util.PrimUtils;
  *   To further save memory, the terms (the actual string values) are not all stored in
  *   memory, but a TermIndex is used to convert term numbers to term values only
  *   for the terms needed after faceting has completed.  Only every 128th term value
- *   is stored, along with it's corresponding term number, and this is used as an
+ *   is stored, along with its corresponding term number, and this is used as an
  *   index to find the closest term and iterate until the desired number is hit (very
  *   much like Lucene's own internal term index).
  *
@@ -234,7 +234,7 @@ public class UnInvertedField extends DocTermOrds {
       final int[] counts = new int[numTermsInField + 1];
 
       //
-      // If there is prefix, find it's start and end term numbers
+      // If there is prefix, find its start and end term numbers
       //
       int startTerm = 0;
       int endTerm = numTermsInField;  // one past the end
diff --git a/solr/core/src/java/org/apache/solr/response/CSVResponseWriter.java b/solr/core/src/java/org/apache/solr/response/CSVResponseWriter.java
index 3ef75c9..1cbfebd 100644
--- a/solr/core/src/java/org/apache/solr/response/CSVResponseWriter.java
+++ b/solr/core/src/java/org/apache/solr/response/CSVResponseWriter.java
@@ -88,7 +88,7 @@ class CSVWriter extends TextResponseWriter {
 
   char[] sharedCSVBuf = new char[8192];
 
-  // prevent each instance from creating it's own buffer
+  // prevent each instance from creating its own buffer
   class CSVSharedBufPrinter extends CSVPrinter {
     public CSVSharedBufPrinter(Writer out, CSVStrategy strategy) {
       super(out, strategy);
diff --git a/solr/core/src/java/org/apache/solr/response/PythonResponseWriter.java b/solr/core/src/java/org/apache/solr/response/PythonResponseWriter.java
index 5ff3815..bf35a51 100644
--- a/solr/core/src/java/org/apache/solr/response/PythonResponseWriter.java
+++ b/solr/core/src/java/org/apache/solr/response/PythonResponseWriter.java
@@ -83,7 +83,7 @@ class PythonWriter extends NaNFloatWriter {
     }
 
     // use python unicode strings...
-    // python doesn't tolerate newlines in strings in it's eval(), so we must escape them.
+    // python doesn't tolerate newlines in strings in its eval(), so we must escape them.
 
     StringBuilder sb = new StringBuilder(val.length());
     boolean needUnicode=false;
@@ -120,7 +120,7 @@ class PythonWriter extends NaNFloatWriter {
   old version that always used unicode
   public void writeStr(String name, String val, boolean needsEscaping) throws IOException {
     // use python unicode strings...
-    // python doesn't tolerate newlines in strings in it's eval(), so we must escape them.
+    // python doesn't tolerate newlines in strings in its eval(), so we must escape them.
     writer.write("u'");
     // it might be more efficient to use a stringbuilder or write substrings
     // if writing chars to the stream is slow.
diff --git a/solr/core/src/java/org/apache/solr/response/RawResponseWriter.java b/solr/core/src/java/org/apache/solr/response/RawResponseWriter.java
index 05fec75..a671810 100644
--- a/solr/core/src/java/org/apache/solr/response/RawResponseWriter.java
+++ b/solr/core/src/java/org/apache/solr/response/RawResponseWriter.java
@@ -35,7 +35,7 @@ import org.apache.solr.request.SolrQueryRequest;
  * This writer is a special case that extends and alters the
  * QueryResponseWriter contract.  If SolrQueryResponse contains a
  * ContentStream added with the key {@link #CONTENT}
- * then this writer will output that stream exactly as is (with it's
+ * then this writer will output that stream exactly as is (with its
  * Content-Type).  if no such ContentStream has been added, then a
  * "base" QueryResponseWriter will be used to write the response
  * according to the usual contract.  The name of the "base" writer can
diff --git a/solr/core/src/java/org/apache/solr/schema/CollationField.java b/solr/core/src/java/org/apache/solr/schema/CollationField.java
index 19bf9db..568fc98 100644
--- a/solr/core/src/java/org/apache/solr/schema/CollationField.java
+++ b/solr/core/src/java/org/apache/solr/schema/CollationField.java
@@ -223,7 +223,7 @@ public class CollationField extends FieldType {
   /**
    * analyze the range with the analyzer, instead of the collator.
    * because jdk collators might not be thread safe (when they are
-   * its just that all methods are synced), this keeps things 
+   * it's just that all methods are synced), this keeps things 
    * simple (we already have a threadlocal clone in the reused TS)
    */
   private BytesRef getCollationKey(String field, String text) {     
diff --git a/solr/core/src/java/org/apache/solr/schema/CurrencyField.java b/solr/core/src/java/org/apache/solr/schema/CurrencyField.java
index 7e6cb22..c0bee75 100644
--- a/solr/core/src/java/org/apache/solr/schema/CurrencyField.java
+++ b/solr/core/src/java/org/apache/solr/schema/CurrencyField.java
@@ -252,7 +252,7 @@ public class CurrencyField extends FieldType implements SchemaAware, ResourceLoa
    * <p>
    * Returns a ValueSource over this field in which the numeric value for 
    * each document represents the indexed value as converted to the default 
-   * currency for the field, normalized to it's most granular form based 
+   * currency for the field, normalized to its most granular form based 
    * on the default fractional digits.
    * </p>
    * <p>
diff --git a/solr/core/src/java/org/apache/solr/schema/FieldType.java b/solr/core/src/java/org/apache/solr/schema/FieldType.java
index 0218eaf..09c7c2f 100644
--- a/solr/core/src/java/org/apache/solr/schema/FieldType.java
+++ b/solr/core/src/java/org/apache/solr/schema/FieldType.java
@@ -450,7 +450,7 @@ public abstract class FieldType extends FieldProperties {
   }
   
   /**
-   * DocValues is not enabled for a field, but its indexed, docvalues can be constructed 
+   * DocValues is not enabled for a field, but it's indexed, docvalues can be constructed 
    * on the fly (uninverted, aka fieldcache) on the first request to sort, facet, etc. 
    * This specifies the structure to use.
    * 
@@ -755,7 +755,7 @@ public abstract class FieldType extends FieldProperties {
    *
    * <p>
    * This method is called by the <code>SchemaField</code> constructor to 
-   * check that it's initialization does not violate any fundemental 
+   * check that its initialization does not violate any fundemental 
    * requirements of the <code>FieldType</code>.  The default implementation 
    * does nothing, but subclasses may chose to throw a {@link SolrException}  
    * if invariants are violated by the <code>SchemaField.</code>
diff --git a/solr/core/src/java/org/apache/solr/schema/TextField.java b/solr/core/src/java/org/apache/solr/schema/TextField.java
index cd7b708..28a170f 100644
--- a/solr/core/src/java/org/apache/solr/schema/TextField.java
+++ b/solr/core/src/java/org/apache/solr/schema/TextField.java
@@ -96,7 +96,7 @@ public class TextField extends FieldType {
 
   @Override
   public SortField getSortField(SchemaField field, boolean reverse) {
-    /* :TODO: maybe warn if isTokenized(), but doesn't use LimitTokenCountFilter in it's chain? */
+    /* :TODO: maybe warn if isTokenized(), but doesn't use LimitTokenCountFilter in its chain? */
     field.checkSortability();
     return Sorting.getTextSortField(field.getName(), reverse, field.sortMissingLast(), field.sortMissingFirst());
   }
diff --git a/solr/core/src/java/org/apache/solr/search/BitDocSet.java b/solr/core/src/java/org/apache/solr/search/BitDocSet.java
index f8ae0dd..7b500ed 100644
--- a/solr/core/src/java/org/apache/solr/search/BitDocSet.java
+++ b/solr/core/src/java/org/apache/solr/search/BitDocSet.java
@@ -321,7 +321,7 @@ public class BitDocSet extends DocSetBase {
               @Override
               public long cost() {
                 // we don't want to actually compute cardinality, but
-                // if its already been computed, we use it (pro-rated for the segment)
+                // if it's already been computed, we use it (pro-rated for the segment)
                 if (size != -1) {
                   return (long)(size * ((FixedBitSet.bits2words(maxDoc)<<6) / (float)bs.length()));
                 } else {
diff --git a/solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java
index 9e84445..7ac7fe2 100644
--- a/solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/CollapsingQParserPlugin.java
@@ -617,7 +617,7 @@ public class CollapsingQParserPlugin extends QParserPlugin {
           if(ord > -1) {
             dummy.score = scores[ord];
           } else if (boostDocs != null && boostDocs.containsKey(docId)) {
-            //Its an elevated doc so no score is needed
+            //It's an elevated doc so no score is needed
             dummy.score = 0F;
           } else if (nullPolicy == CollapsingPostFilter.NULL_POLICY_COLLAPSE) {
             dummy.score = nullScore;
diff --git a/solr/core/src/java/org/apache/solr/search/DisMaxQParser.java b/solr/core/src/java/org/apache/solr/search/DisMaxQParser.java
index 670432d..b898cf8 100644
--- a/solr/core/src/java/org/apache/solr/search/DisMaxQParser.java
+++ b/solr/core/src/java/org/apache/solr/search/DisMaxQParser.java
@@ -168,7 +168,7 @@ public class DisMaxQParser extends QParser {
     }
   }
 
-  /** Adds the main query to the query argument. If its blank then false is returned. */
+  /** Adds the main query to the query argument. If it's blank then false is returned. */
   protected boolean addMainQuery(BooleanQuery query, SolrParams solrParams) throws SyntaxError {
     Map<String, Float> phraseFields = SolrPluginUtils.parseFieldBoosts(solrParams.getParams(DisMaxParams.PF));
     float tiebreaker = solrParams.getFloat(DisMaxParams.TIE, 0.0f);
diff --git a/solr/core/src/java/org/apache/solr/search/DocSet.java b/solr/core/src/java/org/apache/solr/search/DocSet.java
index 8d605f0..c7f6494 100644
--- a/solr/core/src/java/org/apache/solr/search/DocSet.java
+++ b/solr/core/src/java/org/apache/solr/search/DocSet.java
@@ -93,7 +93,7 @@ public interface DocSet /* extends Collection<Integer> */ {
 
   /**
    * Returns the number of documents of the intersection of this set with another set.
-   * May be more efficient than actually creating the intersection and then getting it's size.
+   * May be more efficient than actually creating the intersection and then getting its size.
    */
   public int intersectionSize(DocSet other);
 
@@ -109,7 +109,7 @@ public interface DocSet /* extends Collection<Integer> */ {
 
   /**
    * Returns the number of documents of the union of this set with another set.
-   * May be more efficient than actually creating the union and then getting it's size.
+   * May be more efficient than actually creating the union and then getting its size.
    */
   public int unionSize(DocSet other);
 
diff --git a/solr/core/src/java/org/apache/solr/search/DocSetBase.java b/solr/core/src/java/org/apache/solr/search/DocSetBase.java
index 8d7ccd1..8247095 100644
--- a/solr/core/src/java/org/apache/solr/search/DocSetBase.java
+++ b/solr/core/src/java/org/apache/solr/search/DocSetBase.java
@@ -139,7 +139,7 @@ abstract class DocSetBase implements DocSet {
     if (!(other instanceof BitDocSet)) {
       return other.intersectionSize(this);
     }
-    // less efficient way: do the intersection then get it's size
+    // less efficient way: do the intersection then get its size
     return intersection(other).size();
   }
 
diff --git a/solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.java b/solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.java
index dd01466..75d550e 100644
--- a/solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.java
+++ b/solr/core/src/java/org/apache/solr/search/ExtendedDismaxQParser.java
@@ -840,7 +840,7 @@ public class ExtendedDismaxQParser extends QParser {
         // special syntax in a string isn't special
         clause.hasSpecialSyntax = false;        
       } else {
-        // an empty clause... must be just a + or - on it's own
+        // an empty clause... must be just a + or - on its own
         if (clause.val.length() == 0) {
           clause.syntaxError = true;
           if (clause.must != 0) {
@@ -997,7 +997,7 @@ public class ExtendedDismaxQParser extends QParser {
     
     public ExtendedSolrQueryParser(QParser parser, String defaultField) {
       super(parser, defaultField);
-      // don't trust that our parent class won't ever change it's default
+      // don't trust that our parent class won't ever change its default
       setDefaultOperator(QueryParser.Operator.OR);
     }
     
@@ -1227,7 +1227,7 @@ public class ExtendedDismaxQParser extends QParser {
             Query query = super.getFieldQuery(field, val, type == QType.PHRASE);
             // A BooleanQuery is only possible from getFieldQuery if it came from
             // a single whitespace separated term. In this case, check the coordination
-            // factor on the query: if its enabled, that means we aren't a set of synonyms
+            // factor on the query: if it's enabled, that means we aren't a set of synonyms
             // but instead multiple terms from one whitespace-separated term, we must
             // apply minShouldMatch here so that it works correctly with other things
             // like aliasing.
diff --git a/solr/core/src/java/org/apache/solr/search/NestedQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/NestedQParserPlugin.java
index 23910d8..98b7e9c 100644
--- a/solr/core/src/java/org/apache/solr/search/NestedQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/NestedQParserPlugin.java
@@ -23,7 +23,7 @@ import org.apache.solr.common.util.NamedList;
 import org.apache.solr.request.SolrQueryRequest;
 
 /**
- * Create a nested query, with the ability of that query to redefine it's type via
+ * Create a nested query, with the ability of that query to redefine its type via
  * local parameters.  This is useful in specifying defaults in configuration and
  * letting clients indirectly reference them.
  * <br>Example: <code>{!query defType=func v=$q1}</code>
diff --git a/solr/core/src/java/org/apache/solr/search/PostFilter.java b/solr/core/src/java/org/apache/solr/search/PostFilter.java
index 8fc28a6..adc6a49 100644
--- a/solr/core/src/java/org/apache/solr/search/PostFilter.java
+++ b/solr/core/src/java/org/apache/solr/search/PostFilter.java
@@ -42,6 +42,6 @@ import org.apache.lucene.search.IndexSearcher;
  */
 public interface PostFilter extends ExtendedQuery {
 
-  /** Returns a DelegatingCollector to be run after the main query and all of it's filters, but before any sorting or grouping collectors */
+  /** Returns a DelegatingCollector to be run after the main query and all of its filters, but before any sorting or grouping collectors */
   public DelegatingCollector getFilterCollector(IndexSearcher searcher);
 }
diff --git a/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java b/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
index d941065..7007fbe 100644
--- a/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
+++ b/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
@@ -1604,7 +1604,7 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
       final Sort weightedSort = weightSort(cmd.getSort());
       final CursorMark cursor = cmd.getCursorMark();
 
-      // :TODO: make fillFields it's own QueryCommand flag? ...
+      // :TODO: make fillFields its own QueryCommand flag? ...
       // ... see comments in populateNextCursorMarkFromTopDocs for cache issues (SOLR-5595)
       final boolean fillFields = (null != cursor);
       final FieldDoc searchAfter = (null != cursor ? cursor.getSearchAfterFieldDoc() : null);
diff --git a/solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java b/solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java
index 36745c8..194bec5 100644
--- a/solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java
+++ b/solr/core/src/java/org/apache/solr/search/function/OrdFieldSource.java
@@ -79,7 +79,7 @@ public class OrdFieldSource extends ValueSource {
       SolrIndexSearcher is = (SolrIndexSearcher) o;
       SchemaField sf = is.getSchema().getFieldOrNull(field);
       if (sf != null && sf.hasDocValues() == false && sf.multiValued() == false && sf.getType().getNumericType() != null) {
-        // its a single-valued numeric field: we must currently create insanity :(
+        // it's a single-valued numeric field: we must currently create insanity :(
         List<LeafReaderContext> leaves = is.getIndexReader().leaves();
         LeafReader insaneLeaves[] = new LeafReader[leaves.size()];
         int upto = 0;
@@ -95,7 +95,7 @@ public class OrdFieldSource extends ValueSource {
       IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
       r = SlowCompositeReaderWrapper.wrap(topReader);
     }
-    // if its e.g. tokenized/multivalued, emulate old behavior of single-valued fc
+    // if it's e.g. tokenized/multivalued, emulate old behavior of single-valued fc
     final SortedDocValues sindex = SortedSetSelector.wrap(DocValues.getSortedSet(r, field), SortedSetSelector.Type.MIN);
     return new IntDocValues(this) {
       protected String toTerm(String readableValue) {
diff --git a/solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java b/solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java
index 7b6adbf..a37748f 100644
--- a/solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java
+++ b/solr/core/src/java/org/apache/solr/search/function/ReverseOrdFieldSource.java
@@ -79,7 +79,7 @@ public class ReverseOrdFieldSource extends ValueSource {
       SolrIndexSearcher is = (SolrIndexSearcher) o;
       SchemaField sf = is.getSchema().getFieldOrNull(field);
       if (sf != null && sf.hasDocValues() == false && sf.multiValued() == false && sf.getType().getNumericType() != null) {
-        // its a single-valued numeric field: we must currently create insanity :(
+        // it's a single-valued numeric field: we must currently create insanity :(
         List<LeafReaderContext> leaves = is.getIndexReader().leaves();
         LeafReader insaneLeaves[] = new LeafReader[leaves.size()];
         int upto = 0;
@@ -95,7 +95,7 @@ public class ReverseOrdFieldSource extends ValueSource {
       IndexReader topReader = ReaderUtil.getTopLevelContext(readerContext).reader();
       r = SlowCompositeReaderWrapper.wrap(topReader);
     }
-    // if its e.g. tokenized/multivalued, emulate old behavior of single-valued fc
+    // if it's e.g. tokenized/multivalued, emulate old behavior of single-valued fc
     final SortedDocValues sindex = SortedSetSelector.wrap(DocValues.getSortedSet(r, field), SortedSetSelector.Type.MIN);
     final int end = sindex.getValueCount();
 
diff --git a/solr/core/src/java/org/apache/solr/servlet/SolrRequestParsers.java b/solr/core/src/java/org/apache/solr/servlet/SolrRequestParsers.java
index 86f3e05..54ecff7 100644
--- a/solr/core/src/java/org/apache/solr/servlet/SolrRequestParsers.java
+++ b/solr/core/src/java/org/apache/solr/servlet/SolrRequestParsers.java
@@ -548,7 +548,7 @@ public class SolrRequestParsers
       while (iter.hasNext()) {
           FileItem item = (FileItem) iter.next();
 
-          // If its a form field, put it in our parameter map
+          // If it's a form field, put it in our parameter map
           if (item.isFormField()) {
             MultiMapSolrParams.addParam( 
               item.getFieldName(), 
@@ -592,7 +592,7 @@ public class SolrRequestParsers
         parseQueryString(qs, map);
       }
       
-      // may be -1, so we check again later. But if its already greater we can stop processing!
+      // may be -1, so we check again later. But if it's already greater we can stop processing!
       final long totalLength = req.getContentLength();
       final long maxLength = ((long) uploadLimitKB) * 1024L;
       if (totalLength > maxLength) {
diff --git a/solr/core/src/java/org/apache/solr/update/DefaultSolrCoreState.java b/solr/core/src/java/org/apache/solr/update/DefaultSolrCoreState.java
index 93b548b..1ff0e19 100644
--- a/solr/core/src/java/org/apache/solr/update/DefaultSolrCoreState.java
+++ b/solr/core/src/java/org/apache/solr/update/DefaultSolrCoreState.java
@@ -143,7 +143,7 @@ public final class DefaultSolrCoreState extends SolrCoreState implements Recover
       // we need to wait for the Writer to fall out of use
       // first lets stop it from being lent out
       pauseWriter = true;
-      // then lets wait until its out of use
+      // then lets wait until it's out of use
       log.info("Waiting until IndexWriter is unused... core=" + coreName);
       
       while (!writerFree) {
@@ -201,7 +201,7 @@ public final class DefaultSolrCoreState extends SolrCoreState implements Recover
       // we need to wait for the Writer to fall out of use
       // first lets stop it from being lent out
       pauseWriter = true;
-      // then lets wait until its out of use
+      // then lets wait until it's out of use
       log.info("Waiting until IndexWriter is unused... core=" + coreName);
       
       while (!writerFree) {
diff --git a/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java b/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
index 37da540..3943c7f 100644
--- a/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
+++ b/solr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java
@@ -136,7 +136,7 @@ public class DirectUpdateHandler2 extends UpdateHandler implements SolrCoreState
 
     UpdateLog existingLog = updateHandler.getUpdateLog();
     if (this.ulog != null && this.ulog == existingLog) {
-      // If we are reusing the existing update log, inform the log that it's update handler has changed.
+      // If we are reusing the existing update log, inform the log that its update handler has changed.
       // We do this as late as possible.
       this.ulog.init(this, core);
     }
diff --git a/solr/core/src/java/org/apache/solr/update/PeerSync.java b/solr/core/src/java/org/apache/solr/update/PeerSync.java
index 5394f2a..8e045d6 100644
--- a/solr/core/src/java/org/apache/solr/update/PeerSync.java
+++ b/solr/core/src/java/org/apache/solr/update/PeerSync.java
@@ -168,7 +168,7 @@ public class PeerSync  {
   }
 
   /** Returns true if peer sync was successful, meaning that this core may not be considered to have the latest updates
-   *  when considering the last N updates between it and it's peers.
+   *  when considering the last N updates between it and its peers.
    *  A commit is not performed.
    */
   public boolean sync() {
diff --git a/solr/core/src/java/org/apache/solr/update/SolrCmdDistributor.java b/solr/core/src/java/org/apache/solr/update/SolrCmdDistributor.java
index 8cbb863..8efb874 100644
--- a/solr/core/src/java/org/apache/solr/update/SolrCmdDistributor.java
+++ b/solr/core/src/java/org/apache/solr/update/SolrCmdDistributor.java
@@ -118,7 +118,7 @@ public class SolrCmdDistributor {
             doRetry = true;
           }
           
-          // if its a connect exception, lets try again
+          // if it's a connect exception, lets try again
           if (err.e instanceof SolrServerException) {
             if (((SolrServerException) err.e).getRootCause() instanceof ConnectException) {
               doRetry = true;
diff --git a/solr/core/src/java/org/apache/solr/update/UpdateLog.java b/solr/core/src/java/org/apache/solr/update/UpdateLog.java
index c30b8d1..8e066c5 100644
--- a/solr/core/src/java/org/apache/solr/update/UpdateLog.java
+++ b/solr/core/src/java/org/apache/solr/update/UpdateLog.java
@@ -317,7 +317,7 @@ public class UpdateLog implements PluginInfoInitialized {
   }
 
   /* Takes over ownership of the log, keeping it until no longer needed
-     and then decrementing it's reference and dropping it.
+     and then decrementing its reference and dropping it.
    */
   protected synchronized void addOldLog(TransactionLog oldLog, boolean removeOld) {
     if (oldLog == null) return;
diff --git a/solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java b/solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java
index 82526d0..17c5e26 100644
--- a/solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java
+++ b/solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java
@@ -372,7 +372,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
         doDefensiveChecks(phase);
 
         // if request is coming from another collection then we want it to be sent to all replicas
-        // even if it's phase is FROMLEADER
+        // even if its phase is FROMLEADER
         String fromCollection = updateCommand.getReq().getParams().get(DISTRIB_FROM_COLLECTION);
 
         if (DistribPhase.FROMLEADER == phase && !isSubShardLeader && fromCollection == null) {
@@ -774,7 +774,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
     List<Error> errors = cmdDistrib.getErrors();
     // TODO - we may need to tell about more than one error...
     
-    // if its a forward, any fail is a problem - 
+    // if it's a forward, any fail is a problem - 
     // otherwise we assume things are fine if we got it locally
     // until we start allowing min replication param
     if (errors.size() > 0) {
diff --git a/solr/core/src/java/org/apache/solr/util/ConcurrentLRUCache.java b/solr/core/src/java/org/apache/solr/util/ConcurrentLRUCache.java
index 5b5f7df..be7068a 100644
--- a/solr/core/src/java/org/apache/solr/util/ConcurrentLRUCache.java
+++ b/solr/core/src/java/org/apache/solr/util/ConcurrentLRUCache.java
@@ -245,7 +245,7 @@ public class ConcurrentLRUCache<K,V> {
             // this entry is guaranteed not to be in the bottom
             // group, so do nothing but remove it from the eset.
             numKept++;
-            // remove the entry by moving the last element to it's position
+            // remove the entry by moving the last element to its position
             eset[i] = eset[eSize-1];
             eSize--;
 
@@ -258,7 +258,7 @@ public class ConcurrentLRUCache<K,V> {
             evictEntry(ce.key);
             numRemoved++;
 
-            // remove the entry by moving the last element to it's position
+            // remove the entry by moving the last element to its position
             eset[i] = eset[eSize-1];
             eSize--;
           } else {
diff --git a/solr/core/src/java/org/apache/solr/util/SimplePostTool.java b/solr/core/src/java/org/apache/solr/util/SimplePostTool.java
index 6f30a55..fa3be11 100644
--- a/solr/core/src/java/org/apache/solr/util/SimplePostTool.java
+++ b/solr/core/src/java/org/apache/solr/util/SimplePostTool.java
@@ -762,7 +762,7 @@ public class SimplePostTool {
   }
 
   /**
-   * Opens the file and posts it's contents to the solrUrl,
+   * Opens the file and posts its contents to the solrUrl,
    * writes to response to output. 
    */
   public void postFile(File file, OutputStream output, String type) {
diff --git a/solr/core/src/java/org/apache/solr/util/SolrCLI.java b/solr/core/src/java/org/apache/solr/util/SolrCLI.java
index 1c66f1d..14580d8 100644
--- a/solr/core/src/java/org/apache/solr/util/SolrCLI.java
+++ b/solr/core/src/java/org/apache/solr/util/SolrCLI.java
@@ -924,7 +924,7 @@ public class SolrCLI {
           String coreUrl = replicaCoreProps.getCoreUrl();
           boolean isLeader = coreUrl.equals(leaderUrl);
 
-          // if replica's node is not live, it's status is DOWN
+          // if replica's node is not live, its status is DOWN
           String nodeName = replicaCoreProps.getNodeName();
           if (nodeName == null || !liveNodes.contains(nodeName)) {
             replicaStatus = ZkStateReader.DOWN;
diff --git a/solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java b/solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java
index 00ff01c..fc510d1 100644
--- a/solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java
+++ b/solr/core/src/java/org/apache/solr/util/SolrPluginUtils.java
@@ -266,7 +266,7 @@ public class SolrPluginUtils {
    * <li>parsedquery - the main query executed formated by the Solr
    *     QueryParsing utils class (which knows about field types)
    * </li>
-   * <li>parsedquery_toString - the main query executed formated by it's
+   * <li>parsedquery_toString - the main query executed formatted by its
    *     own toString method (in case it has internal state Solr
    *     doesn't know about)
    * </li>
@@ -701,7 +701,7 @@ public class SolrPluginUtils {
   }
 
   /**
-   * Returns it's input if there is an even (ie: balanced) number of
+   * Returns its input if there is an even (ie: balanced) number of
    * '"' characters -- otherwise returns a String in which all '"'
    * characters are striped out.
    */
@@ -758,7 +758,7 @@ public class SolrPluginUtils {
     protected Map<String,Alias> aliases = new HashMap<>(3);
     public DisjunctionMaxQueryParser(QParser qp, String defaultField) {
       super(qp,defaultField);
-      // don't trust that our parent class won't ever change it's default
+      // don't trust that our parent class won't ever change its default
       setDefaultOperator(QueryParser.Operator.OR);
     }
 
diff --git a/solr/core/src/test-files/solr/collection1/conf/solrconfig-externalversionconstraint.xml b/solr/core/src/test-files/solr/collection1/conf/solrconfig-externalversionconstraint.xml
index 131833d..f3488b3 100644
--- a/solr/core/src/test-files/solr/collection1/conf/solrconfig-externalversionconstraint.xml
+++ b/solr/core/src/test-files/solr/collection1/conf/solrconfig-externalversionconstraint.xml
@@ -44,7 +44,7 @@
   <updateRequestProcessorChain name="external-version-constraint" default="true">
     <!-- this chain uses the processor using the "deleteVersionParam" option
          so that deleteById requests are translated into updates to preserve the 
-         (logically) deleted document in the index with a record of it's deleted 
+         (logically) deleted document in the index with a record of its deleted 
          version.
          
          It also demonstrates how to mix in TimestampUpdateProcessorFactory and 
diff --git a/solr/core/src/test-files/solr/collection1/conf/solrconfig.xml b/solr/core/src/test-files/solr/collection1/conf/solrconfig.xml
index 936164d..95bacab 100644
--- a/solr/core/src/test-files/solr/collection1/conf/solrconfig.xml
+++ b/solr/core/src/test-files/solr/collection1/conf/solrconfig.xml
@@ -587,7 +587,7 @@
   <restManager>
     <!-- 
     IMPORTANT: Due to the Lucene SecurityManager, tests can only write to their runtime directory or below.
-    But its easier to just keep everything in memory for testing so no remnants are left behind.
+    But it's easier to just keep everything in memory for testing so no remnants are left behind.
     -->
     <str name="storageIO">org.apache.solr.rest.ManagedResourceStorage$InMemoryStorageIO</str>
   </restManager>
diff --git a/solr/core/src/test/org/apache/solr/BasicFunctionalityTest.java b/solr/core/src/test/org/apache/solr/BasicFunctionalityTest.java
index 86ae23b..2a11d45 100644
--- a/solr/core/src/test/org/apache/solr/BasicFunctionalityTest.java
+++ b/solr/core/src/test/org/apache/solr/BasicFunctionalityTest.java
@@ -625,7 +625,7 @@ public class BasicFunctionalityTest extends SolrTestCaseJ4 {
             ,"*[count(//doc)=2]"
             ,"//arr[@name='multiDefault']"
             );
-    assertQ("1 doc should have it's explicit multiDefault",
+    assertQ("1 doc should have its explicit multiDefault",
             req("multiDefault:a")
             ,"*[count(//doc)=1]"
             );
@@ -634,7 +634,7 @@ public class BasicFunctionalityTest extends SolrTestCaseJ4 {
             req("intDefault:42")
             ,"*[count(//doc)=2]"
             );
-    assertQ("1 doc should have it's explicit intDefault",
+    assertQ("1 doc should have its explicit intDefault",
             req("intDefault:[3 TO 5]")
             ,"*[count(//doc)=1]"
             );
diff --git a/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZk2Test.java b/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZk2Test.java
index c8c298a..67adfc6 100644
--- a/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZk2Test.java
+++ b/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZk2Test.java
@@ -125,7 +125,7 @@ public class BasicDistributedZk2Test extends AbstractFullDistribZkTestBase {
         // expected
       }
       
-      // TODO: bring this to it's own method?
+      // TODO: bring this to its own method?
       // try indexing to a leader that has no replicas up
       ZkStateReader zkStateReader = cloudClient.getZkStateReader();
       ZkNodeProps leaderProps = zkStateReader.getLeaderRetry(
diff --git a/solr/core/src/test/org/apache/solr/cloud/CollectionsAPIDistributedZkTest.java b/solr/core/src/test/org/apache/solr/cloud/CollectionsAPIDistributedZkTest.java
index f9fb6b2..9018ead 100644
--- a/solr/core/src/test/org/apache/solr/cloud/CollectionsAPIDistributedZkTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/CollectionsAPIDistributedZkTest.java
@@ -287,7 +287,7 @@ public class CollectionsAPIDistributedZkTest extends AbstractFullDistribZkTestBa
   
   private void deleteCollectionWithDownNodes() throws Exception {
     String baseUrl = getBaseUrl((HttpSolrServer) clients.get(0));
-    // now try to remove a collection when a couple of it's nodes are down
+    // now try to remove a collection when a couple of its nodes are down
     if (secondConfigSet) {
       createCollection(null, "halfdeletedcollection2", 3, 3, 6,
           createNewSolrServer("", baseUrl), null, "conf2");
diff --git a/solr/core/src/test/org/apache/solr/cloud/LeaderInitiatedRecoveryOnCommitTest.java b/solr/core/src/test/org/apache/solr/cloud/LeaderInitiatedRecoveryOnCommitTest.java
index 4c076dd..bd27357 100644
--- a/solr/core/src/test/org/apache/solr/cloud/LeaderInitiatedRecoveryOnCommitTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/LeaderInitiatedRecoveryOnCommitTest.java
@@ -84,7 +84,7 @@ public class LeaderInitiatedRecoveryOnCommitTest extends BasicDistributedZkTest
             + printClusterStateInfo(),
         notLeaders.size() == 1);
 
-    // let's put the leader in it's own partition, no replicas can contact it now
+    // let's put the leader in its own partition, no replicas can contact it now
     Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, "shard1");
     SocketProxy leaderProxy = getProxyForReplica(leader);
     leaderProxy.close();
@@ -127,7 +127,7 @@ public class LeaderInitiatedRecoveryOnCommitTest extends BasicDistributedZkTest
             + printClusterStateInfo(),
         notLeaders.size() == 2);
 
-    // let's put the leader in it's own partition, no replicas can contact it now
+    // let's put the leader in its own partition, no replicas can contact it now
     Replica leader = cloudClient.getZkStateReader().getLeaderRetry(testCollectionName, "shard1");
     SocketProxy leaderProxy = getProxyForReplica(leader);
     leaderProxy.close();
diff --git a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java
index 21eec1f..47017ec 100644
--- a/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsWriteToMultipleCollectionsTest.java
@@ -146,7 +146,7 @@ public class HdfsWriteToMultipleCollectionsTest extends BasicDistributedZkTest {
             // see SOLR-6424
             assertFalse(blockDirectory.isBlockCacheWriteEnabled());
             Cache cache = blockDirectory.getCache();
-            // we know its a BlockDirectoryCache, but future proof
+            // we know it's a BlockDirectoryCache, but future proof
             assertTrue(cache instanceof BlockDirectoryCache);
             BlockCache blockCache = ((BlockDirectoryCache) cache)
                 .getBlockCache();
diff --git a/solr/core/src/test/org/apache/solr/core/SolrCoreCheckLockOnStartupTest.java b/solr/core/src/test/org/apache/solr/core/SolrCoreCheckLockOnStartupTest.java
index 3a074aa..5c0b2c9 100644
--- a/solr/core/src/test/org/apache/solr/core/SolrCoreCheckLockOnStartupTest.java
+++ b/solr/core/src/test/org/apache/solr/core/SolrCoreCheckLockOnStartupTest.java
@@ -41,7 +41,7 @@ public class SolrCoreCheckLockOnStartupTest extends SolrTestCaseJ4 {
     System.setProperty("solr.directoryFactory", "org.apache.solr.core.SimpleFSDirectoryFactory");
     // test tests native and simple in the same jvm in the same exact directory:
     // the file will remain after the native test (it cannot safely be deleted without the risk of deleting another guys lock)
-    // its ok, these aren't "compatible" anyway: really this test should not re-use the same directory at all.
+    // it's ok, these aren't "compatible" anyway: really this test should not re-use the same directory at all.
     Files.deleteIfExists(new File(new File(initCoreDataDir, "index"), IndexWriter.WRITE_LOCK_NAME).toPath());
   }
 
diff --git a/solr/core/src/test/org/apache/solr/handler/admin/CoreAdminCreateDiscoverTest.java b/solr/core/src/test/org/apache/solr/handler/admin/CoreAdminCreateDiscoverTest.java
index edd5fd6..1d68660 100644
--- a/solr/core/src/test/org/apache/solr/handler/admin/CoreAdminCreateDiscoverTest.java
+++ b/solr/core/src/test/org/apache/solr/handler/admin/CoreAdminCreateDiscoverTest.java
@@ -82,7 +82,7 @@ public class CoreAdminCreateDiscoverTest extends SolrTestCaseJ4 {
     setupCore(coreSysProps, true);
 
     // create a new core (using CoreAdminHandler) w/ properties
-    // Just to be sure its NOT written to the core.properties file
+    // Just to be sure it's NOT written to the core.properties file
     File workDir = new File(solrHomeDirectory, coreSysProps);
     System.setProperty("INSTDIR_TEST", workDir.getAbsolutePath());
     System.setProperty("CONFIG_TEST", "solrconfig_ren.xml");
@@ -227,7 +227,7 @@ public class CoreAdminCreateDiscoverTest extends SolrTestCaseJ4 {
     setupCore(coreNormal, true);
 
     // create a new core (using CoreAdminHandler) w/ properties
-    // Just to be sure its NOT written to the core.properties file
+    // Just to be sure it's NOT written to the core.properties file
     File workDir = new File(solrHomeDirectory, coreNormal);
     File data = new File(workDir, "data");
 
diff --git a/solr/core/src/test/org/apache/solr/handler/component/DebugComponentTest.java b/solr/core/src/test/org/apache/solr/handler/component/DebugComponentTest.java
index 3d1b3f2..ebb8a81 100644
--- a/solr/core/src/test/org/apache/solr/handler/component/DebugComponentTest.java
+++ b/solr/core/src/test/org/apache/solr/handler/component/DebugComponentTest.java
@@ -61,7 +61,7 @@ public class DebugComponentTest extends SolrTestCaseJ4 {
             "//lst[@name='explain']/str[@name='3']",
             "//str[@name='QParser']",// make sure the QParser is specified
             "count(//lst[@name='timing']/*)=3", //should be three pieces to timings
-            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify it's result
+            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify its result
             "count(//lst[@name='prepare']/*)>0",
             "//lst[@name='prepare']/double[@name='time']",
             "count(//lst[@name='process']/*)>0",
@@ -85,7 +85,7 @@ public class DebugComponentTest extends SolrTestCaseJ4 {
             "//lst[@name='explain']/str[@name='2']",
             "//lst[@name='explain']/str[@name='3']",
             "count(//lst[@name='timing']/*)=3", //should be three pieces to timings
-            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify it's result
+            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify its result
             "count(//lst[@name='prepare']/*)>0",
             "//lst[@name='prepare']/double[@name='time']",
             "count(//lst[@name='process']/*)>0",
@@ -100,7 +100,7 @@ public class DebugComponentTest extends SolrTestCaseJ4 {
             "count(//lst[@name='explain']/*)=0",
             "count(//str[@name='QParser'])=0",// make sure the QParser is specified
             "count(//lst[@name='timing']/*)=3", //should be three pieces to timings
-            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify it's result
+            "//lst[@name='timing']/double[@name='time']", //make sure we have a time value, but don't specify its result
             "count(//lst[@name='prepare']/*)>0",
             "//lst[@name='prepare']/double[@name='time']",
             "count(//lst[@name='process']/*)>0",
diff --git a/solr/core/src/test/org/apache/solr/handler/component/DistributedFacetPivotLargeTest.java b/solr/core/src/test/org/apache/solr/handler/component/DistributedFacetPivotLargeTest.java
index 6e48f6d..6584300 100644
--- a/solr/core/src/test/org/apache/solr/handler/component/DistributedFacetPivotLargeTest.java
+++ b/solr/core/src/test/org/apache/solr/handler/component/DistributedFacetPivotLargeTest.java
@@ -178,7 +178,7 @@ public class DistributedFacetPivotLargeTest extends BaseDistributedSearchTestCas
     //
     // This is tricky, here's what i think is happening.... 
     // - "company:honda" only exists on twoShard, and only w/ "place:cardiff"
-    // - twoShard has no other places in it's docs
+    // - twoShard has no other places in its docs
     // - twoShard can't return any other places to w/ honda as a count=0 sub-value
     // - if we refined all other companies places, would twoShard return honda==0 ?
     //   ... but there's no refinement since mincount==0
diff --git a/solr/core/src/test/org/apache/solr/internal/csv/CSVParserTest.java b/solr/core/src/test/org/apache/solr/internal/csv/CSVParserTest.java
index 4adfc21..0c64601 100644
--- a/solr/core/src/test/org/apache/solr/internal/csv/CSVParserTest.java
+++ b/solr/core/src/test/org/apache/solr/internal/csv/CSVParserTest.java
@@ -29,7 +29,7 @@ import junit.framework.TestCase;
  * The test are organized in three different sections:
  * The 'setter/getter' section, the lexer section and finally the parser 
  * section. In case a test fails, you should follow a top-down approach for 
- * fixing a potential bug (its likely that the parser itself fails if the lexer
+ * fixing a potential bug (it's likely that the parser itself fails if the lexer
  * has problems...).
  */
 public class CSVParserTest extends TestCase {
diff --git a/solr/core/src/test/org/apache/solr/internal/csv/CSVStrategyTest.java b/solr/core/src/test/org/apache/solr/internal/csv/CSVStrategyTest.java
index 9b71ed7..383e764 100644
--- a/solr/core/src/test/org/apache/solr/internal/csv/CSVStrategyTest.java
+++ b/solr/core/src/test/org/apache/solr/internal/csv/CSVStrategyTest.java
@@ -26,7 +26,7 @@ import junit.framework.TestCase;
  * The test are organized in three different sections:
  * The 'setter/getter' section, the lexer section and finally the strategy 
  * section. In case a test fails, you should follow a top-down approach for 
- * fixing a potential bug (its likely that the strategy itself fails if the lexer
+ * fixing a potential bug (it's likely that the strategy itself fails if the lexer
  * has problems...).
  */
 public class CSVStrategyTest extends TestCase {
diff --git a/solr/core/src/test/org/apache/solr/response/TestRawResponseWriter.java b/solr/core/src/test/org/apache/solr/response/TestRawResponseWriter.java
index 297904f..4103949 100644
--- a/solr/core/src/test/org/apache/solr/response/TestRawResponseWriter.java
+++ b/solr/core/src/test/org/apache/solr/response/TestRawResponseWriter.java
@@ -50,8 +50,8 @@ public class TestRawResponseWriter extends SolrTestCaseJ4 {
 
   @BeforeClass
   public static void setupCoreAndWriters() throws Exception {
-    // we don't directly use this core or it's config, we use
-    // QueryResponseWriters' constructed programaticly,
+    // we don't directly use this core or its config, we use
+    // QueryResponseWriters' constructed programmatically,
     // but we do use this core for managing the life cycle of the requests
     // we spin up.
     initCore("solrconfig.xml","schema.xml");
diff --git a/solr/core/src/test/org/apache/solr/schema/TestCollationField.java b/solr/core/src/test/org/apache/solr/schema/TestCollationField.java
index db63a8f..f13e2db 100644
--- a/solr/core/src/test/org/apache/solr/schema/TestCollationField.java
+++ b/solr/core/src/test/org/apache/solr/schema/TestCollationField.java
@@ -59,7 +59,7 @@ public class TestCollationField extends SolrTestCaseJ4 {
    * Ugly: but what to do? We want to test custom sort, which reads rules in as a resource.
    * These are largish files, and jvm-specific (as our documentation says, you should always
    * look out for jvm differences with collation).
-   * So its preferable to create this file on-the-fly.
+   * So it's preferable to create this file on-the-fly.
    */
   public static String setupSolrHome() throws Exception {
     // make a solr home underneath the test's TEMP_DIR
diff --git a/solr/core/src/test/org/apache/solr/schema/TestCollationFieldDocValues.java b/solr/core/src/test/org/apache/solr/schema/TestCollationFieldDocValues.java
index 5ecde3b..1440299 100644
--- a/solr/core/src/test/org/apache/solr/schema/TestCollationFieldDocValues.java
+++ b/solr/core/src/test/org/apache/solr/schema/TestCollationFieldDocValues.java
@@ -57,7 +57,7 @@ public class TestCollationFieldDocValues extends SolrTestCaseJ4 {
    * Ugly: but what to do? We want to test custom sort, which reads rules in as a resource.
    * These are largish files, and jvm-specific (as our documentation says, you should always
    * look out for jvm differences with collation).
-   * So its preferable to create this file on-the-fly.
+   * So it's preferable to create this file on-the-fly.
    */
   public static String setupSolrHome() throws Exception {
     // make a solr home underneath the test's TEMP_DIR
diff --git a/solr/core/src/test/org/apache/solr/search/TestStressLucene.java b/solr/core/src/test/org/apache/solr/search/TestStressLucene.java
index 9f95ee3..94951c8 100644
--- a/solr/core/src/test/org/apache/solr/search/TestStressLucene.java
+++ b/solr/core/src/test/org/apache/solr/search/TestStressLucene.java
@@ -317,7 +317,7 @@ public class TestStressLucene extends TestRTGBase {
               int docid = getFirstMatch(r, new Term("id",Integer.toString(id)));
 
               if (docid < 0 && tombstones) {
-                // if we couldn't find the doc, look for it's tombstone
+                // if we couldn't find the doc, look for its tombstone
                 docid = getFirstMatch(r, new Term("id","-"+Integer.toString(id)));
                 if (docid < 0) {
                   if (val == -1L) {
@@ -336,7 +336,7 @@ public class TestStressLucene extends TestRTGBase {
                 if (docid < 0) {
                   verbose("ERROR: Couldn't find a doc for id", id, "using reader",r);
                 }
-                assertTrue(docid >= 0);   // we should have found the document, or it's tombstone
+                assertTrue(docid >= 0);   // we should have found the document, or its tombstone
                 StoredDocument doc = r.document(docid);
                 long foundVal = Long.parseLong(doc.get(field));
                 if (foundVal < Math.abs(val)) {
diff --git a/solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java b/solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java
index 483a33a..058b642 100644
--- a/solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java
+++ b/solr/core/src/test/org/apache/solr/update/DocumentBuilderTest.java
@@ -210,7 +210,7 @@ public class DocumentBuilderTest extends SolrTestCaseJ4 {
   }
   
   /**
-   * Its ok to boost a field if it has norms
+   * It's ok to boost a field if it has norms
    */
   public void testBoost() throws Exception {
     XmlDoc xml = new XmlDoc();
@@ -385,7 +385,7 @@ public class DocumentBuilderTest extends SolrTestCaseJ4 {
   }
   
   /**
-   * Its ok to supply a document boost even if a field omits norms
+   * It's ok to supply a document boost even if a field omits norms
    */
   public void testDocumentBoostOmitNorms() throws Exception {
     XmlDoc xml = new XmlDoc();
diff --git a/solr/core/src/test/org/apache/solr/update/PeerSyncTest.java b/solr/core/src/test/org/apache/solr/update/PeerSyncTest.java
index d8ede04..4d2ebfd 100644
--- a/solr/core/src/test/org/apache/solr/update/PeerSyncTest.java
+++ b/solr/core/src/test/org/apache/solr/update/PeerSyncTest.java
@@ -65,7 +65,7 @@ public class PeerSyncTest extends BaseDistributedSearchTestCase {
     long v = 0;
     add(client0, seenLeader, sdoc("id","1","_version_",++v));
 
-    // this fails because client0 has no context (i.e. no updates of it's own to judge if applying the updates
+    // this fails because client0 has no context (i.e. no updates of its own to judge if applying the updates
     // from client1 will bring it into sync with client1)
     assertSync(client1, numVersions, false, shardsArr[0]);
 
diff --git a/solr/example/example-DIH/solr/db/conf/solrconfig.xml b/solr/example/example-DIH/solr/db/conf/solrconfig.xml
index a0e0191..9fc9399 100755
--- a/solr/example/example-DIH/solr/db/conf/solrconfig.xml
+++ b/solr/example/example-DIH/solr/db/conf/solrconfig.xml
@@ -142,7 +142,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/example/example-DIH/solr/mail/conf/solrconfig.xml b/solr/example/example-DIH/solr/mail/conf/solrconfig.xml
index 656b9d3..93d70aa 100755
--- a/solr/example/example-DIH/solr/mail/conf/solrconfig.xml
+++ b/solr/example/example-DIH/solr/mail/conf/solrconfig.xml
@@ -145,7 +145,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/example/example-DIH/solr/rss/conf/solrconfig.xml b/solr/example/example-DIH/solr/rss/conf/solrconfig.xml
index 9413cb8..2df1272 100755
--- a/solr/example/example-DIH/solr/rss/conf/solrconfig.xml
+++ b/solr/example/example-DIH/solr/rss/conf/solrconfig.xml
@@ -142,7 +142,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/example/example-DIH/solr/solr/conf/solrconfig.xml b/solr/example/example-DIH/solr/solr/conf/solrconfig.xml
index fa9de55..8a390d1 100755
--- a/solr/example/example-DIH/solr/solr/conf/solrconfig.xml
+++ b/solr/example/example-DIH/solr/solr/conf/solrconfig.xml
@@ -142,7 +142,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/example/example-DIH/solr/tika/conf/solrconfig.xml b/solr/example/example-DIH/solr/tika/conf/solrconfig.xml
index 8f7c06d..3be1ebc 100755
--- a/solr/example/example-DIH/solr/tika/conf/solrconfig.xml
+++ b/solr/example/example-DIH/solr/tika/conf/solrconfig.xml
@@ -143,7 +143,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/server/etc/webdefault.xml b/solr/server/etc/webdefault.xml
index 213138b..b987eac 100644
--- a/solr/server/etc/webdefault.xml
+++ b/solr/server/etc/webdefault.xml
@@ -27,7 +27,7 @@
 
   <description>
     Default web.xml file.  
-    This file is applied to a Web application before it's own WEB_INF/web.xml file
+    This file is applied to a Web application before its own WEB_INF/web.xml file
   </description>
 
   <!-- ==================================================================== -->
diff --git a/solr/server/solr/configsets/basic_configs/conf/solrconfig.xml b/solr/server/solr/configsets/basic_configs/conf/solrconfig.xml
index 37a3ddb..d101f62 100755
--- a/solr/server/solr/configsets/basic_configs/conf/solrconfig.xml
+++ b/solr/server/solr/configsets/basic_configs/conf/solrconfig.xml
@@ -70,7 +70,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/server/solr/configsets/data_driven_schema_configs/conf/solrconfig.xml b/solr/server/solr/configsets/data_driven_schema_configs/conf/solrconfig.xml
index 78ecb66..d77f311 100755
--- a/solr/server/solr/configsets/data_driven_schema_configs/conf/solrconfig.xml
+++ b/solr/server/solr/configsets/data_driven_schema_configs/conf/solrconfig.xml
@@ -123,7 +123,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/server/solr/configsets/sample_techproducts_configs/conf/solrconfig.xml b/solr/server/solr/configsets/sample_techproducts_configs/conf/solrconfig.xml
index e08d4f9..9e8a333 100755
--- a/solr/server/solr/configsets/sample_techproducts_configs/conf/solrconfig.xml
+++ b/solr/server/solr/configsets/sample_techproducts_configs/conf/solrconfig.xml
@@ -140,7 +140,7 @@
        index format, but hooks into the schema to provide per-field customization of
        the postings lists and per-document values in the fieldType element
        (postingsFormat/docValuesFormat). Note that most of the alternative implementations
-       are experimental, so if you choose to customize the index format, its a good
+       are experimental, so if you choose to customize the index format, it's a good
        idea to convert back to the official format e.g. via IndexWriter.addIndexes(IndexReader)
        before upgrading to a newer version to avoid unnecessary reindexing.
   -->
diff --git a/solr/solrj/build.xml b/solr/solrj/build.xml
index bebdd24..4e5016a 100644
--- a/solr/solrj/build.xml
+++ b/solr/solrj/build.xml
@@ -66,7 +66,7 @@
 
   <!-- Specialized to use lucene's classpath too, because it refs e.g. qp syntax 
        (even though it doesnt compile with it) 
-       TODO: would be nice to fix this up better, but its hard because of
+       TODO: would be nice to fix this up better, but it's hard because of
        the different ways solr links to lucene javadocs -->
   <target name="-ecj-javadoc-lint-src" depends="-ecj-resolve">
     <ecj-macro srcdir="${src.dir}" configuration="${common.dir}/tools/javadoc/ecj.javadocs.prefs">
diff --git a/solr/solrj/src/java/org/apache/solr/client/solrj/ResponseParser.java b/solr/solrj/src/java/org/apache/solr/client/solrj/ResponseParser.java
index eddc334..d5c3b38 100644
--- a/solr/solrj/src/java/org/apache/solr/client/solrj/ResponseParser.java
+++ b/solr/solrj/src/java/org/apache/solr/client/solrj/ResponseParser.java
@@ -35,7 +35,7 @@ public abstract class ResponseParser
   public abstract NamedList<Object> processResponse(Reader reader);
   
   /**
-   * A well behaved ResponseParser will return it's content-type.
+   * A well behaved ResponseParser will return its content-type.
    * 
    * @return the content-type this parser expects to parse
    */
diff --git a/solr/solrj/src/java/org/apache/solr/client/solrj/SolrQuery.java b/solr/solrj/src/java/org/apache/solr/client/solrj/SolrQuery.java
index aa124d8..50fc1fe 100644
--- a/solr/solrj/src/java/org/apache/solr/client/solrj/SolrQuery.java
+++ b/solr/solrj/src/java/org/apache/solr/client/solrj/SolrQuery.java
@@ -730,7 +730,7 @@ public class SolrQuery extends ModifiableSolrParams
   /**
    * Updates or adds a single sort field specification to the current sort
    * information. If the sort field already exist in the sort information map,
-   * it's position is unchanged and the sort order is set; if it does not exist,
+   * its position is unchanged and the sort order is set; if it does not exist,
    * it is appended at the end with the specified order..
    *
    * @return the modified SolrQuery object, for easy chaining
diff --git a/solr/solrj/src/java/org/apache/solr/common/cloud/ZkCmdExecutor.java b/solr/solrj/src/java/org/apache/solr/common/cloud/ZkCmdExecutor.java
index 36aa045..d77ad06 100644
--- a/solr/solrj/src/java/org/apache/solr/common/cloud/ZkCmdExecutor.java
+++ b/solr/solrj/src/java/org/apache/solr/common/cloud/ZkCmdExecutor.java
@@ -29,7 +29,7 @@ public class ZkCmdExecutor {
   
   /**
    * TODO: At this point, this should probably take a SolrZkClient in
-   * it's constructor.
+   * its constructor.
    * 
    * @param timeoutms
    *          the client timeout for the ZooKeeper clients that will be used
@@ -93,7 +93,7 @@ public class ZkCmdExecutor {
     try {
       zkClient.makePath(path, data, true);
     } catch (NodeExistsException e) {
-      // its okay if another beats us creating the node
+      // it's okay if another beats us creating the node
     }
     
   }
diff --git a/solr/solrj/src/java/org/apache/solr/common/params/RequiredSolrParams.java b/solr/solrj/src/java/org/apache/solr/common/params/RequiredSolrParams.java
index 0861319..0b78d41 100644
--- a/solr/solrj/src/java/org/apache/solr/common/params/RequiredSolrParams.java
+++ b/solr/solrj/src/java/org/apache/solr/common/params/RequiredSolrParams.java
@@ -106,7 +106,7 @@ public class RequiredSolrParams extends SolrParams {
 
   //----------------------------------------------------------
   // Functions with a default value - pass directly to the
-  // wrapped SolrParams (they won't return null - unless its the default)
+  // wrapped SolrParams (they won't return null - unless it's the default)
   //----------------------------------------------------------
 
   @Override
diff --git a/solr/solrj/src/java/org/apache/solr/common/util/Hash.java b/solr/solrj/src/java/org/apache/solr/common/util/Hash.java
index 054e765..81bfa95 100644
--- a/solr/solrj/src/java/org/apache/solr/common/util/Hash.java
+++ b/solr/solrj/src/java/org/apache/solr/common/util/Hash.java
@@ -118,7 +118,7 @@ public class Hash {
 
   /**
    * <p>The hash value of a character sequence is defined to be the hash of
-   * it's unicode code points, according to {@link #lookup3ycs(int[] k, int offset, int length, int initval)}
+   * its unicode code points, according to {@link #lookup3ycs(int[] k, int offset, int length, int initval)}
    * </p>
    * <p>If you know the number of code points in the {@code CharSequence}, you can
    * generate the same hash as the original lookup3
diff --git a/solr/solrj/src/java/org/apache/solr/common/util/NamedList.java b/solr/solrj/src/java/org/apache/solr/common/util/NamedList.java
index 6376694..c539b2b 100644
--- a/solr/solrj/src/java/org/apache/solr/common/util/NamedList.java
+++ b/solr/solrj/src/java/org/apache/solr/common/util/NamedList.java
@@ -455,7 +455,7 @@ public class NamedList<T> implements Cloneable, Serializable, Iterable<Map.Entry
   }
 
   /**
-   * Iterates over the Map and sequentially adds it's key/value pairs
+   * Iterates over the Map and sequentially adds its key/value pairs
    */
   public boolean addAll(Map<String,T> args) {
     for (Map.Entry<String, T> entry : args.entrySet() ) {
diff --git a/solr/solrj/src/java/org/apache/solr/common/util/SimpleOrderedMap.java b/solr/solrj/src/java/org/apache/solr/common/util/SimpleOrderedMap.java
index 4608f90..c9996d1 100644
--- a/solr/solrj/src/java/org/apache/solr/common/util/SimpleOrderedMap.java
+++ b/solr/solrj/src/java/org/apache/solr/common/util/SimpleOrderedMap.java
@@ -32,7 +32,7 @@ import java.util.*;
  * the same way.
  * </p>
  * <p>
- * This class does not provide efficient lookup by key, it's main purpose is
+ * This class does not provide efficient lookup by key, its main purpose is
  * to hold data to be serialized.  It aims to minimize overhead and to be
  * efficient at adding new elements.
  * </p>
diff --git a/solr/test-framework/build.xml b/solr/test-framework/build.xml
index f7c68c3..a8e7911 100644
--- a/solr/test-framework/build.xml
+++ b/solr/test-framework/build.xml
@@ -46,7 +46,7 @@
   <!-- redefine the clover setup, because we dont want to run clover for the test-framework -->
   <target name="-clover.setup" if="run.clover"/>
 
-  <!-- redefine the test compilation, so its just a no-op -->
+  <!-- redefine the test compilation, so it's just a no-op -->
   <target name="compile-test"/>
   
   <!-- redefine the forbidden apis for tests, as we check ourselves -->
diff --git a/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java b/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
index bb313bf..6f8edc7 100644
--- a/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
+++ b/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
@@ -440,7 +440,7 @@ public abstract class SolrTestCaseJ4 extends LuceneTestCase {
      if (endNumOpens-numOpens != endNumCloses-numCloses) {
        String msg = "ERROR: SolrIndexSearcher opens=" + (endNumOpens-numOpens) + " closes=" + (endNumCloses-numCloses);
        log.error(msg);
-       // if its TestReplicationHandler, ignore it. the test is broken and gets no love
+       // if it's TestReplicationHandler, ignore it. the test is broken and gets no love
        if ("TestReplicationHandler".equals(RandomizedContext.current().getTargetClass().getSimpleName())) {
          log.warn("TestReplicationHandler wants to fail!: " + msg);
        } else {
diff --git a/solr/test-framework/src/java/org/apache/solr/util/TestHarness.java b/solr/test-framework/src/java/org/apache/solr/util/TestHarness.java
index 2581f3e..0a09d29 100644
--- a/solr/test-framework/src/java/org/apache/solr/util/TestHarness.java
+++ b/solr/test-framework/src/java/org/apache/solr/util/TestHarness.java
@@ -200,7 +200,7 @@ public class TestHarness extends BaseTestHarness {
     return container;
   }
 
-  /** Gets a core that does not have it's refcount incremented (i.e. there is no need to
+  /** Gets a core that does not have its refcount incremented (i.e. there is no need to
    * close when done).  This is not MT safe in conjunction with reloads!
    */
   public SolrCore getCore() {
@@ -212,7 +212,7 @@ public class TestHarness extends BaseTestHarness {
     return core;
   }
 
-  /** Gets the core with it's reference count incremented.
+  /** Gets the core with its reference count incremented.
    * You must call core.close() when done!
    */
   public SolrCore getCoreInc() {
diff --git a/solr/webapp/web/js/lib/jquery.ajaxfileupload.js b/solr/webapp/web/js/lib/jquery.ajaxfileupload.js
index 0488bf9..272976a 100644
--- a/solr/webapp/web/js/lib/jquery.ajaxfileupload.js
+++ b/solr/webapp/web/js/lib/jquery.ajaxfileupload.js
@@ -128,7 +128,7 @@ THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLI
           wrapElement($element);
 
           // Call user-supplied (or default) onStart(), setting
-          //  it's this context to the file DOM element
+          //  its this context to the file DOM element
           var ret = settings.onStart.apply($element, [settings.params]);
 
           // let onStart have the option to cancel the upload
diff --git a/solr/webapp/web/js/lib/jquery.sammy.js b/solr/webapp/web/js/lib/jquery.sammy.js
index ffa3dcc..21b8a80 100644
--- a/solr/webapp/web/js/lib/jquery.sammy.js
+++ b/solr/webapp/web/js/lib/jquery.sammy.js
@@ -48,7 +48,7 @@ THE SOFTWARE.
 
 
   // `Sammy` (also aliased as $.sammy) is not only the namespace for a
-  // number of prototypes, its also a top level method that allows for easy
+  // number of prototypes, it's also a top level method that allows for easy
   // creation/management of `Sammy.Application` instances. There are a
   // number of different forms for `Sammy()` but each returns an instance
   // of `Sammy.Application`. When a new instance is created using
@@ -1446,7 +1446,7 @@ THE SOFTWARE.
         }
         if (callback) { this.then(callback); }
         if (typeof location === 'string') {
-          // its a path
+          // it's a path
           is_json      = (location.match(/\.json$/) || options.json);
           should_cache = ((is_json && options.cache === true) || options.cache !== false);
           context.next_engine = context.event_context.engineFor(location);
@@ -1474,12 +1474,12 @@ THE SOFTWARE.
           }, options));
           return false;
         } else {
-          // its a dom/jQuery
+          // it's a dom/jQuery
           if (location.nodeType) {
             return location.innerHTML;
           }
           if (location.selector) {
-            // its a jQuery
+            // it's a jQuery
             context.next_engine = location.attr('data-engine');
             if (options.clone === false) {
               return location.remove()[0].innerHTML.toString();
diff --git a/solr/webapp/web/js/require.js b/solr/webapp/web/js/require.js
index 9ddc7c3..5cb347a 100644
--- a/solr/webapp/web/js/require.js
+++ b/solr/webapp/web/js/require.js
@@ -4618,7 +4618,7 @@ jQuery.extend({
 					jQuery.error( "type property can't be changed" );
 				} else if ( !jQuery.support.radioValue && value === "radio" && jQuery.nodeName(elem, "input") ) {
 					// Setting the type on a radio button after the value resets the value in IE6-9
-					// Reset value to it's default in case type is set after value
+					// Reset value to its default in case type is set after value
 					// This is for element creation
 					var val = elem.value;
 					elem.setAttribute( "type", value );
@@ -6277,7 +6277,7 @@ var getText = Sizzle.getText = function( elem ) {
 				// Replace IE's carriage returns
 				return elem.innerText.replace( rReturn, '' );
 			} else {
-				// Traverse it's children
+				// Traverse its children
 				for ( elem = elem.firstChild; elem; elem = elem.nextSibling) {
 					ret += getText( elem );
 				}

