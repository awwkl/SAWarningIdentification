GitDiffStart: 7027c5690edd4766e126e4c18e84edaaa68af610 | Mon Nov 18 22:45:49 2013 +0000
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SumFloatAssociationFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SumFloatAssociationFacets.java
deleted file mode 100644
index d4f1ac4..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SumFloatAssociationFacets.java
+++ /dev/null
@@ -1,188 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.simple.SimpleFacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-
-// nocommit rename to TaxonomySumFloatAssociationFacets
-// nocommit jdoc that this assumes/requires the default encoding
-public class SumFloatAssociationFacets extends TaxonomyFacets {
-  private final float[] values;
-
-  public SumFloatAssociationFacets(TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
-    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
-  }
-
-  public SumFloatAssociationFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
-    super(indexFieldName, taxoReader, config);
-    values = new float[taxoReader.getSize()];
-    sumValues(fc.getMatchingDocs());
-  }
-
-  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
-    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
-    for(MatchingDocs hits : matchingDocs) {
-      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
-      if (dv == null) { // this reader does not have DocValues for the requested category list
-        continue;
-      }
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      BytesRef scratch = new BytesRef();
-      //System.out.println("count seg=" + hits.context.reader());
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        //System.out.println("  doc=" + doc);
-        // nocommit use OrdinalsReader?  but, add a
-        // BytesRef getAssociation()?
-        dv.get(doc, scratch);
-        byte[] bytes = scratch.bytes;
-        int end = scratch.offset + scratch.length;
-        int offset = scratch.offset;
-        while (offset < end) {
-          int ord = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          int value = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          values[ord] += Float.intBitsToFloat(value);
-        }
-        ++doc;
-      }
-    }
-
-    // nocommit we could do this lazily instead:
-
-    // Rollup any necessary dims:
-    // nocommit should we rollup?
-    /*
-    for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
-      String dim = ent.getKey();
-      FacetsConfig.DimConfig ft = ent.getValue();
-      if (ft.hierarchical && ft.multiValued == false) {
-        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
-        // It can be -1 if this field was declared in the
-        // config but never indexed:
-        if (dimRootOrd > 0) {
-          counts[dimRootOrd] += rollup(children[dimRootOrd]);
-        }
-      }
-    }
-    */
-  }
-
-  private float rollup(int ord) {
-    int sum = 0;
-    while (ord != TaxonomyReader.INVALID_ORDINAL) {
-      float childValue = values[ord] + rollup(children[ord]);
-      values[ord] = childValue;
-      sum += childValue;
-      ord = siblings[ord];
-    }
-    return sum;
-  }
-
-  /** Return the count for a specific path.  Returns -1 if
-   *  this path doesn't exist, else the count. */
-  @Override
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    verifyDim(dim);
-    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
-    if (ord < 0) {
-      return -1;
-    }
-    return values[ord];
-  }
-
-  @Override
-  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    FacetsConfig.DimConfig dimConfig = verifyDim(dim);
-    FacetLabel cp = FacetLabel.create(dim, path);
-    int dimOrd = taxoReader.getOrdinal(cp);
-    if (dimOrd == -1) {
-      //System.out.println("no ord for path=" + path);
-      return null;
-    }
-
-    TopOrdAndFloatQueue q = new TopOrdAndFloatQueue(topN);
-    
-    float bottomValue = 0;
-
-    int ord = children[dimOrd];
-    float sumValue = 0;
-
-    TopOrdAndFloatQueue.OrdAndValue reuse = null;
-    while(ord != TaxonomyReader.INVALID_ORDINAL) {
-      if (values[ord] > 0) {
-        sumValue += values[ord];
-        if (values[ord] > bottomValue) {
-          if (reuse == null) {
-            reuse = new TopOrdAndFloatQueue.OrdAndValue();
-          }
-          reuse.ord = ord;
-          reuse.value = values[ord];
-          reuse = q.insertWithOverflow(reuse);
-          if (q.size() == topN) {
-            bottomValue = q.top().value;
-          }
-        }
-      }
-
-      ord = siblings[ord];
-    }
-
-    if (sumValue == 0) {
-      //System.out.println("totCount=0 for path=" + path);
-      return null;
-    }
-
-    /*
-    if (dimConfig.hierarchical && dimConfig.multiValued) {
-      totCount = counts[dimOrd];
-    }
-    */
-
-    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
-    for(int i=labelValues.length-1;i>=0;i--) {
-      TopOrdAndFloatQueue.OrdAndValue ordAndValue = q.pop();
-      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
-      labelValues[i] = new LabelAndValue(child.components[path.length], ordAndValue.value);
-    }
-
-    return new SimpleFacetResult(cp, sumValue, labelValues);
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SumIntAssociationFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SumIntAssociationFacets.java
deleted file mode 100644
index 90b8f7c..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SumIntAssociationFacets.java
+++ /dev/null
@@ -1,189 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.simple.SimpleFacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-
-// nocommit rename to TaxonomySumIntAssociationFacets
-// nocommit jdoc that this assumes/requires the default encoding
-public class SumIntAssociationFacets extends TaxonomyFacets {
-  private final int[] values;
-
-  public SumIntAssociationFacets(TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
-    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
-  }
-
-  public SumIntAssociationFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
-    super(indexFieldName, taxoReader, config);
-    values = new int[taxoReader.getSize()];
-    sumValues(fc.getMatchingDocs());
-  }
-
-  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
-    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
-    for(MatchingDocs hits : matchingDocs) {
-      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
-      if (dv == null) { // this reader does not have DocValues for the requested category list
-        continue;
-      }
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      BytesRef scratch = new BytesRef();
-      //System.out.println("count seg=" + hits.context.reader());
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        //System.out.println("  doc=" + doc);
-        // nocommit use OrdinalsReader?  but, add a
-        // BytesRef getAssociation()?
-        dv.get(doc, scratch);
-        byte[] bytes = scratch.bytes;
-        int end = scratch.offset + scratch.length;
-        int offset = scratch.offset;
-        while (offset < end) {
-          int ord = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          int value = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          values[ord] += value;
-        }
-        ++doc;
-      }
-    }
-
-    // nocommit we could do this lazily instead:
-
-    // Rollup any necessary dims:
-    // nocommit should we rollup?
-    /*
-    for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
-      String dim = ent.getKey();
-      FacetsConfig.DimConfig ft = ent.getValue();
-      if (ft.hierarchical && ft.multiValued == false) {
-        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
-        // It can be -1 if this field was declared in the
-        // config but never indexed:
-        if (dimRootOrd > 0) {
-          counts[dimRootOrd] += rollup(children[dimRootOrd]);
-        }
-      }
-    }
-    */
-  }
-
-  private int rollup(int ord) {
-    int sum = 0;
-    while (ord != TaxonomyReader.INVALID_ORDINAL) {
-      int childValue = values[ord] + rollup(children[ord]);
-      values[ord] = childValue;
-      sum += childValue;
-      ord = siblings[ord];
-    }
-    return sum;
-  }
-
-  /** Return the count for a specific path.  Returns -1 if
-   *  this path doesn't exist, else the count. */
-  @Override
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    verifyDim(dim);
-    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
-    if (ord < 0) {
-      return -1;
-    }
-    return values[ord];
-  }
-
-  @Override
-  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    verifyDim(dim);
-    FacetLabel cp = FacetLabel.create(dim, path);
-    int dimOrd = taxoReader.getOrdinal(cp);
-    if (dimOrd == -1) {
-      //System.out.println("no ord for path=" + path);
-      return null;
-    }
-
-    TopOrdAndIntQueue q = new TopOrdAndIntQueue(topN);
-    
-    int bottomValue = 0;
-
-    int ord = children[dimOrd];
-    long sumValue = 0;
-
-    TopOrdAndIntQueue.OrdAndValue reuse = null;
-    while(ord != TaxonomyReader.INVALID_ORDINAL) {
-      if (values[ord] > 0) {
-        sumValue += values[ord];
-        if (values[ord] > bottomValue) {
-          if (reuse == null) {
-            reuse = new TopOrdAndIntQueue.OrdAndValue();
-          }
-          reuse.ord = ord;
-          reuse.value = values[ord];
-          reuse = q.insertWithOverflow(reuse);
-          if (q.size() == topN) {
-            bottomValue = q.top().value;
-          }
-        }
-      }
-
-      ord = siblings[ord];
-    }
-
-    if (sumValue == 0) {
-      //System.out.println("totCount=0 for path=" + path);
-      return null;
-    }
-
-    /*
-    FacetsConfig.DimConfig ft = config.getDimConfig(path.components[0]);
-    if (ft.hierarchical && ft.multiValued) {
-      totCount = counts[dimOrd];
-    }
-    */
-
-    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
-    for(int i=labelValues.length-1;i>=0;i--) {
-      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
-      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
-      labelValues[i] = new LabelAndValue(child.components[path.length], ordAndValue.value);
-    }
-
-    return new SimpleFacetResult(cp, sumValue, labelValues);
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumFloatAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumFloatAssociations.java
new file mode 100644
index 0000000..ccd7dd3
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumFloatAssociations.java
@@ -0,0 +1,187 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.simple.SimpleFacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+// nocommit jdoc that this assumes/requires the default encoding
+public class TaxonomyFacetSumFloatAssociations extends TaxonomyFacets {
+  private final float[] values;
+
+  public TaxonomyFacetSumFloatAssociations(TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
+  }
+
+  public TaxonomyFacetSumFloatAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    values = new float[taxoReader.getSize()];
+    sumValues(fc.getMatchingDocs());
+  }
+
+  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
+    for(MatchingDocs hits : matchingDocs) {
+      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
+      if (dv == null) { // this reader does not have DocValues for the requested category list
+        continue;
+      }
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      BytesRef scratch = new BytesRef();
+      //System.out.println("count seg=" + hits.context.reader());
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        //System.out.println("  doc=" + doc);
+        // nocommit use OrdinalsReader?  but, add a
+        // BytesRef getAssociation()?
+        dv.get(doc, scratch);
+        byte[] bytes = scratch.bytes;
+        int end = scratch.offset + scratch.length;
+        int offset = scratch.offset;
+        while (offset < end) {
+          int ord = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          int value = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          values[ord] += Float.intBitsToFloat(value);
+        }
+        ++doc;
+      }
+    }
+
+    // nocommit we could do this lazily instead:
+
+    // Rollup any necessary dims:
+    // nocommit should we rollup?
+    /*
+    for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
+      String dim = ent.getKey();
+      FacetsConfig.DimConfig ft = ent.getValue();
+      if (ft.hierarchical && ft.multiValued == false) {
+        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
+        // It can be -1 if this field was declared in the
+        // config but never indexed:
+        if (dimRootOrd > 0) {
+          counts[dimRootOrd] += rollup(children[dimRootOrd]);
+        }
+      }
+    }
+    */
+  }
+
+  private float rollup(int ord) {
+    int sum = 0;
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      float childValue = values[ord] + rollup(children[ord]);
+      values[ord] = childValue;
+      sum += childValue;
+      ord = siblings[ord];
+    }
+    return sum;
+  }
+
+  /** Return the count for a specific path.  Returns -1 if
+   *  this path doesn't exist, else the count. */
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    verifyDim(dim);
+    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
+    if (ord < 0) {
+      return -1;
+    }
+    return values[ord];
+  }
+
+  @Override
+  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    FacetsConfig.DimConfig dimConfig = verifyDim(dim);
+    FacetLabel cp = FacetLabel.create(dim, path);
+    int dimOrd = taxoReader.getOrdinal(cp);
+    if (dimOrd == -1) {
+      //System.out.println("no ord for path=" + path);
+      return null;
+    }
+
+    TopOrdAndFloatQueue q = new TopOrdAndFloatQueue(topN);
+    
+    float bottomValue = 0;
+
+    int ord = children[dimOrd];
+    float sumValue = 0;
+
+    TopOrdAndFloatQueue.OrdAndValue reuse = null;
+    while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      if (values[ord] > 0) {
+        sumValue += values[ord];
+        if (values[ord] > bottomValue) {
+          if (reuse == null) {
+            reuse = new TopOrdAndFloatQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = values[ord];
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomValue = q.top().value;
+          }
+        }
+      }
+
+      ord = siblings[ord];
+    }
+
+    if (sumValue == 0) {
+      //System.out.println("totCount=0 for path=" + path);
+      return null;
+    }
+
+    /*
+    if (dimConfig.hierarchical && dimConfig.multiValued) {
+      totCount = counts[dimOrd];
+    }
+    */
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndFloatQueue.OrdAndValue ordAndValue = q.pop();
+      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
+      labelValues[i] = new LabelAndValue(child.components[path.length], ordAndValue.value);
+    }
+
+    return new SimpleFacetResult(cp, sumValue, labelValues);
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumIntAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumIntAssociations.java
new file mode 100644
index 0000000..2c6365f
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumIntAssociations.java
@@ -0,0 +1,188 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.simple.SimpleFacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+// nocommit jdoc that this assumes/requires the default encoding
+public class TaxonomyFacetSumIntAssociations extends TaxonomyFacets {
+  private final int[] values;
+
+  public TaxonomyFacetSumIntAssociations(TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
+  }
+
+  public TaxonomyFacetSumIntAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    values = new int[taxoReader.getSize()];
+    sumValues(fc.getMatchingDocs());
+  }
+
+  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
+    for(MatchingDocs hits : matchingDocs) {
+      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
+      if (dv == null) { // this reader does not have DocValues for the requested category list
+        continue;
+      }
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      BytesRef scratch = new BytesRef();
+      //System.out.println("count seg=" + hits.context.reader());
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        //System.out.println("  doc=" + doc);
+        // nocommit use OrdinalsReader?  but, add a
+        // BytesRef getAssociation()?
+        dv.get(doc, scratch);
+        byte[] bytes = scratch.bytes;
+        int end = scratch.offset + scratch.length;
+        int offset = scratch.offset;
+        while (offset < end) {
+          int ord = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          int value = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          values[ord] += value;
+        }
+        ++doc;
+      }
+    }
+
+    // nocommit we could do this lazily instead:
+
+    // Rollup any necessary dims:
+    // nocommit should we rollup?
+    /*
+    for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
+      String dim = ent.getKey();
+      FacetsConfig.DimConfig ft = ent.getValue();
+      if (ft.hierarchical && ft.multiValued == false) {
+        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
+        // It can be -1 if this field was declared in the
+        // config but never indexed:
+        if (dimRootOrd > 0) {
+          counts[dimRootOrd] += rollup(children[dimRootOrd]);
+        }
+      }
+    }
+    */
+  }
+
+  private int rollup(int ord) {
+    int sum = 0;
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      int childValue = values[ord] + rollup(children[ord]);
+      values[ord] = childValue;
+      sum += childValue;
+      ord = siblings[ord];
+    }
+    return sum;
+  }
+
+  /** Return the count for a specific path.  Returns -1 if
+   *  this path doesn't exist, else the count. */
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    verifyDim(dim);
+    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
+    if (ord < 0) {
+      return -1;
+    }
+    return values[ord];
+  }
+
+  @Override
+  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    verifyDim(dim);
+    FacetLabel cp = FacetLabel.create(dim, path);
+    int dimOrd = taxoReader.getOrdinal(cp);
+    if (dimOrd == -1) {
+      //System.out.println("no ord for path=" + path);
+      return null;
+    }
+
+    TopOrdAndIntQueue q = new TopOrdAndIntQueue(topN);
+    
+    int bottomValue = 0;
+
+    int ord = children[dimOrd];
+    long sumValue = 0;
+
+    TopOrdAndIntQueue.OrdAndValue reuse = null;
+    while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      if (values[ord] > 0) {
+        sumValue += values[ord];
+        if (values[ord] > bottomValue) {
+          if (reuse == null) {
+            reuse = new TopOrdAndIntQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = values[ord];
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomValue = q.top().value;
+          }
+        }
+      }
+
+      ord = siblings[ord];
+    }
+
+    if (sumValue == 0) {
+      //System.out.println("totCount=0 for path=" + path);
+      return null;
+    }
+
+    /*
+    FacetsConfig.DimConfig ft = config.getDimConfig(path.components[0]);
+    if (ft.hierarchical && ft.multiValued) {
+      totCount = counts[dimOrd];
+    }
+    */
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
+      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
+      labelValues[i] = new LabelAndValue(child.components[path.length], ordAndValue.value);
+    }
+
+    return new SimpleFacetResult(cp, sumValue, labelValues);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestAssociationFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestAssociationFacets.java
deleted file mode 100644
index f42b617..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestAssociationFacets.java
+++ /dev/null
@@ -1,170 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.FacetResult;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.store.Directory;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-
-/** Test for associations */
-public class TestAssociationFacets extends FacetTestCase {
-  
-  private static Directory dir;
-  private static IndexReader reader;
-  private static Directory taxoDir;
-  private static TaxonomyReader taxoReader;
-
-  private static final FacetLabel aint = new FacetLabel("int", "a");
-  private static final FacetLabel bint = new FacetLabel("int", "b");
-  private static final FacetLabel afloat = new FacetLabel("float", "a");
-  private static final FacetLabel bfloat = new FacetLabel("float", "b");
-  private static final FacetsConfig config = new FacetsConfig();
-  
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    dir = newDirectory();
-    taxoDir = newDirectory();
-    // preparations - index, taxonomy, content
-    
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-
-    // Cannot mix ints & floats in the same indexed field:
-    config.setIndexFieldName("int", "$facets.int");
-    config.setIndexFieldName("float", "$facets.float");
-
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter writer = new FacetIndexWriter(dir, iwc, taxoWriter, config);
-
-    // index documents, 50% have only 'b' and all have 'a'
-    for (int i = 0; i < 110; i++) {
-      Document doc = new Document();
-      // every 11th document is added empty, this used to cause the association
-      // aggregators to go into an infinite loop
-      if (i % 11 != 0) {
-        doc.add(new AssociationFacetField(2, "int", "a"));
-        doc.add(new AssociationFacetField(0.5f, "float", "a"));
-        if (i % 2 == 0) { // 50
-          doc.add(new AssociationFacetField(3, "int", "b"));
-          doc.add(new AssociationFacetField(0.2f, "float", "b"));
-        }
-      }
-      writer.addDocument(doc);
-    }
-    
-    taxoWriter.close();
-    reader = DirectoryReader.open(writer, true);
-    writer.close();
-    taxoReader = new DirectoryTaxonomyReader(taxoDir);
-  }
-  
-  @AfterClass
-  public static void afterClass() throws Exception {
-    reader.close();
-    reader = null;
-    dir.close();
-    dir = null;
-    taxoReader.close();
-    taxoReader = null;
-    taxoDir.close();
-    taxoDir = null;
-  }
-  
-  public void testIntSumAssociation() throws Exception {
-    
-    SimpleFacetsCollector fc = new SimpleFacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-
-    Facets facets = new SumIntAssociationFacets("$facets.int", taxoReader, config, fc);
-    
-    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
-    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
-  }
-
-  public void testFloatSumAssociation() throws Exception {
-    SimpleFacetsCollector fc = new SimpleFacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    Facets facets = new SumFloatAssociationFacets("$facets.float", taxoReader, config, fc);
-    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
-    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
-  }  
-
-  /** Make sure we can test both int and float assocs in one
-   *  index, as long as we send each to a different field. */
-  public void testIntAndFloatAssocation() throws Exception {
-    SimpleFacetsCollector fc = new SimpleFacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    Facets facets = new SumFloatAssociationFacets("$facets.float", taxoReader, config, fc);
-    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
-    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
-    
-    facets = new SumIntAssociationFacets("$facets.int", taxoReader, config, fc);
-    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
-    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
-  }
-
-  public void testWrongIndexFieldName() throws Exception {
-    SimpleFacetsCollector fc = new SimpleFacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    Facets facets = new SumFloatAssociationFacets(taxoReader, config, fc);
-    try {
-      facets.getSpecificValue("float");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    try {
-      facets.getTopChildren(10, "float");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetAssociations.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetAssociations.java
new file mode 100644
index 0000000..a0642fa
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetAssociations.java
@@ -0,0 +1,170 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.List;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetsCollector;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.store.Directory;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+
+/** Test for associations */
+public class TestTaxonomyFacetAssociations extends FacetTestCase {
+  
+  private static Directory dir;
+  private static IndexReader reader;
+  private static Directory taxoDir;
+  private static TaxonomyReader taxoReader;
+
+  private static final FacetLabel aint = new FacetLabel("int", "a");
+  private static final FacetLabel bint = new FacetLabel("int", "b");
+  private static final FacetLabel afloat = new FacetLabel("float", "a");
+  private static final FacetLabel bfloat = new FacetLabel("float", "b");
+  private static final FacetsConfig config = new FacetsConfig();
+  
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    dir = newDirectory();
+    taxoDir = newDirectory();
+    // preparations - index, taxonomy, content
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+
+    // Cannot mix ints & floats in the same indexed field:
+    config.setIndexFieldName("int", "$facets.int");
+    config.setIndexFieldName("float", "$facets.float");
+
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new FacetIndexWriter(dir, iwc, taxoWriter, config);
+
+    // index documents, 50% have only 'b' and all have 'a'
+    for (int i = 0; i < 110; i++) {
+      Document doc = new Document();
+      // every 11th document is added empty, this used to cause the association
+      // aggregators to go into an infinite loop
+      if (i % 11 != 0) {
+        doc.add(new AssociationFacetField(2, "int", "a"));
+        doc.add(new AssociationFacetField(0.5f, "float", "a"));
+        if (i % 2 == 0) { // 50
+          doc.add(new AssociationFacetField(3, "int", "b"));
+          doc.add(new AssociationFacetField(0.2f, "float", "b"));
+        }
+      }
+      writer.addDocument(doc);
+    }
+    
+    taxoWriter.close();
+    reader = DirectoryReader.open(writer, true);
+    writer.close();
+    taxoReader = new DirectoryTaxonomyReader(taxoDir);
+  }
+  
+  @AfterClass
+  public static void afterClass() throws Exception {
+    reader.close();
+    reader = null;
+    dir.close();
+    dir = null;
+    taxoReader.close();
+    taxoReader = null;
+    taxoDir.close();
+    taxoDir = null;
+  }
+  
+  public void testIntSumAssociation() throws Exception {
+    
+    SimpleFacetsCollector fc = new SimpleFacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
+    
+    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
+    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
+  }
+
+  public void testFloatSumAssociation() throws Exception {
+    SimpleFacetsCollector fc = new SimpleFacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    
+    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
+    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
+    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
+  }  
+
+  /** Make sure we can test both int and float assocs in one
+   *  index, as long as we send each to a different field. */
+  public void testIntAndFloatAssocation() throws Exception {
+    SimpleFacetsCollector fc = new SimpleFacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    
+    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
+    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
+    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
+    
+    facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
+    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
+    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+    SimpleFacetsCollector fc = new SimpleFacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    Facets facets = new TaxonomyFacetSumFloatAssociations(taxoReader, config, fc);
+    try {
+      facets.getSpecificValue("float");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "float");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts.java
new file mode 100644
index 0000000..8aafa45
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts.java
@@ -0,0 +1,440 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayOutputStream;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.util.PrintTaxonomyStats;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.similarities.DefaultSimilarity;
+import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
+import org.apache.lucene.search.similarities.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestTaxonomyFacetCounts extends FacetTestCase {
+
+  public void testBasic() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("Publish Date");
+
+    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, config);
+
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Susan"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Frank"));
+    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
+    writer.addDocument(doc);
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    // Aggregate the facet counts:
+    SimpleFacetsCollector c = new SimpleFacetsCollector();
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+
+    // Retrieve & verify results:
+    assertEquals("Publish Date (5)\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", facets.getTopChildren(10, "Publish Date").toString());
+    assertEquals("Author (5)\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", facets.getTopChildren(10, "Author").toString());
+
+    // Now user drills down on Publish Date/2010:
+    SimpleDrillDownQuery q2 = new SimpleDrillDownQuery(config);
+    q2.add("Publish Date", "2010");
+    c = new SimpleFacetsCollector();
+    searcher.search(q2, c);
+    facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+    assertEquals("Author (2)\n  Bob (1)\n  Lisa (1)\n", facets.getTopChildren(10, "Author").toString());
+
+    assertEquals(1, facets.getSpecificValue("Author", "Lisa"));
+
+    // Smoke test PrintTaxonomyStats:
+    ByteArrayOutputStream bos = new ByteArrayOutputStream();
+    PrintTaxonomyStats.printStats(taxoReader, new PrintStream(bos, false, "UTF-8"), true);
+    String result = bos.toString("UTF-8");
+    assertTrue(result.indexOf("/Author: 4 immediate children; 5 total categories") != -1);
+    assertTrue(result.indexOf("/Publish Date: 3 immediate children; 12 total categories") != -1);
+    // Make sure at least a few nodes of the tree came out:
+    assertTrue(result.indexOf("  /1999") != -1);
+    assertTrue(result.indexOf("  /2012") != -1);
+    assertTrue(result.indexOf("      /20") != -1);
+
+    taxoReader.close();
+    searcher.getIndexReader().close();
+    dir.close();
+    taxoDir.close();
+  }
+
+  // LUCENE-5333
+  public void testSparseFacets() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, new FacetsConfig());
+
+    Document doc = new Document();
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(doc);
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new FacetField("a", "foo2"));
+    doc.add(new FacetField("b", "bar1"));
+    writer.addDocument(doc);
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new FacetField("a", "foo3"));
+    doc.add(new FacetField("b", "bar2"));
+    doc.add(new FacetField("c", "baz1"));
+    writer.addDocument(doc);
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    SimpleFacetsCollector c = new SimpleFacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    Facets facets;
+    if (random().nextBoolean()) {
+      facets = new FastTaxonomyFacetCounts(taxoReader, new FacetsConfig(), c);
+    } else {
+      OrdinalsReader ordsReader = new DocValuesOrdinalsReader();
+      if (random().nextBoolean()) {
+        ordsReader = new CachedOrdinalsReader(ordsReader);
+      }
+      facets = new TaxonomyFacetCounts(ordsReader, taxoReader, new FacetsConfig(), c);
+    }
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<SimpleFacetResult> results = facets.getAllDims(10);
+
+    assertEquals(3, results.size());
+    assertEquals("a (3)\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
+    assertEquals("b (2)\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
+    assertEquals("c (1)\n  baz1 (1)\n", results.get(2).toString());
+
+    searcher.getIndexReader().close();
+    taxoReader.close();
+    taxoDir.close();
+    dir.close();
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setIndexFieldName("a", "$facets2");
+    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, config);
+
+    Document doc = new Document();
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(doc);
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    SimpleFacetsCollector c = new SimpleFacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    // Uses default $facets field:
+    Facets facets;
+    if (random().nextBoolean()) {
+      facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+    } else {
+      OrdinalsReader ordsReader = new DocValuesOrdinalsReader();
+      if (random().nextBoolean()) {
+        ordsReader = new CachedOrdinalsReader(ordsReader);
+      }
+      facets = new TaxonomyFacetCounts(ordsReader, taxoReader, config, c);
+    }
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<SimpleFacetResult> results = facets.getAllDims(10);
+    assertTrue(results.isEmpty());
+
+    try {
+      facets.getSpecificValue("a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    searcher.getIndexReader().close();
+    taxoReader.close();
+    taxoDir.close();
+    dir.close();
+  }
+
+
+  // nocommit in the sparse case test that we are really
+  // sorting by the correct dim count
+
+  /*
+  public void testReallyNoNormsForDrillDown() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setSimilarity(new PerFieldSimilarityWrapper() {
+        final Similarity sim = new DefaultSimilarity();
+
+        @Override
+        public Similarity get(String name) {
+          assertEquals("field", name);
+          return sim;
+        }
+      });
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    FacetFields facetFields = new FacetFields(taxoWriter);      
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    facetFields.addFields(doc, Collections.singletonList(new CategoryPath("a/path", '/')));
+    writer.addDocument(doc);
+    writer.close();
+    taxoWriter.close();
+    dir.close();
+    taxoDir.close();
+  }
+
+  public void testAllParents() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    CategoryListParams clp = new CategoryListParams("$facets") {
+        @Override
+        public OrdinalPolicy getOrdinalPolicy(String fieldName) {
+          return OrdinalPolicy.ALL_PARENTS;
+        }
+      };
+    FacetIndexingParams fip = new FacetIndexingParams(clp);
+
+    FacetFields facetFields = new FacetFields(taxoWriter, fip);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    facetFields.addFields(doc, Collections.singletonList(new CategoryPath("a/path", '/')));
+    writer.addDocument(doc);
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+    
+    FacetSearchParams fsp = new FacetSearchParams(fip,
+                                                  new CountFacetRequest(new CategoryPath("a", '/'), 10));
+
+    // Aggregate the facet counts:
+    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+    List<FacetResult> results = c.getFacetResults();
+    assertEquals(1, results.size());
+    assertEquals(1, (int) results.get(0).getFacetResultNode().value);
+
+    // LUCENE-4913:
+    for(FacetResultNode childNode : results.get(0).getFacetResultNode().subResults) {
+      assertTrue(childNode.ordinal != 0);
+    }
+
+    searcher.getIndexReader().close();
+    taxoReader.close();
+    dir.close();
+    taxoDir.close();
+  }
+
+  public void testLabelWithDelimiter() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetFields facetFields = new FacetFields(taxoWriter);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    BytesRef br = new BytesRef(new byte[] {(byte) 0xee, (byte) 0x92, (byte) 0xaa, (byte) 0xef, (byte) 0x9d, (byte) 0x89});
+    facetFields.addFields(doc, Collections.singletonList(new CategoryPath("dim/" + br.utf8ToString(), '/')));
+    try {
+      writer.addDocument(doc);
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    writer.close();
+    taxoWriter.close();
+    dir.close();
+    taxoDir.close();
+  }
+  
+  // LUCENE-4583: make sure if we require > 32 KB for one
+  // document, we don't hit exc when using Facet42DocValuesFormat
+  public void testManyFacetsInOneDocument() throws Exception {
+    assumeTrue("default Codec doesn't support huge BinaryDocValues", _TestUtil.fieldSupportsHugeBinaryDocValues(CategoryListParams.DEFAULT_FIELD));
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    
+    FacetFields facetFields = new FacetFields(taxoWriter);
+    
+    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);
+    
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    List<CategoryPath> paths = new ArrayList<CategoryPath>();
+    for (int i = 0; i < numLabels; i++) {
+      paths.add(new CategoryPath("dim", "" + i));
+    }
+    facetFields.addFields(doc, paths);
+    writer.addDocument(doc);
+    
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+    
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+    
+    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath("dim"), Integer.MAX_VALUE));
+    
+    // Aggregate the facet counts:
+    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
+    
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+    List<FacetResult> results = c.getFacetResults();
+    assertEquals(1, results.size());
+    FacetResultNode root = results.get(0).getFacetResultNode();
+    assertEquals(numLabels, root.subResults.size());
+    Set<String> allLabels = new HashSet<String>();
+    for (FacetResultNode childNode : root.subResults) {
+      assertEquals(2, childNode.label.length);
+      allLabels.add(childNode.label.components[1]);
+      assertEquals(1, (int) childNode.value);
+    }
+    assertEquals(numLabels, allLabels.size());
+    
+    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+  */
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetSumValueSource.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetSumValueSource.java
new file mode 100644
index 0000000..9970ddb
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetSumValueSource.java
@@ -0,0 +1,245 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.util.PrintTaxonomyStats;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.queries.function.valuesource.IntFieldSource;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.similarities.DefaultSimilarity;
+import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
+import org.apache.lucene.search.similarities.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
+
+  public void testBasic() throws Exception {
+
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, new FacetsConfig());
+
+    // Reused across documents, to add the necessary facet
+    // fields:
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("Author", "Bob"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new IntField("num", 20, Field.Store.NO));
+    doc.add(new FacetField("Author", "Lisa"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new IntField("num", 30, Field.Store.NO));
+    doc.add(new FacetField("Author", "Lisa"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new IntField("num", 40, Field.Store.NO));
+    doc.add(new FacetField("Author", "Susan"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new IntField("num", 45, Field.Store.NO));
+    doc.add(new FacetField("Author", "Frank"));
+    writer.addDocument(doc);
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    // Aggregate the facet counts:
+    SimpleFacetsCollector c = new SimpleFacetsCollector();
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
+
+    // Retrieve & verify results:
+    assertEquals("Author (145.0)\n  Lisa (50.0)\n  Frank (45.0)\n  Susan (40.0)\n  Bob (10.0)\n", facets.getTopChildren(10, "Author").toString());
+
+    taxoReader.close();
+    searcher.getIndexReader().close();
+    dir.close();
+    taxoDir.close();
+  }
+
+  // LUCENE-5333
+  public void testSparseFacets() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, new FacetsConfig());
+
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(doc);
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new IntField("num", 20, Field.Store.NO));
+    doc.add(new FacetField("a", "foo2"));
+    doc.add(new FacetField("b", "bar1"));
+    writer.addDocument(doc);
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new IntField("num", 30, Field.Store.NO));
+    doc.add(new FacetField("a", "foo3"));
+    doc.add(new FacetField("b", "bar2"));
+    doc.add(new FacetField("c", "baz1"));
+    writer.addDocument(doc);
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    SimpleFacetsCollector c = new SimpleFacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<SimpleFacetResult> results = facets.getAllDims(10);
+
+    assertEquals(3, results.size());
+    assertEquals("a (60.0)\n  foo3 (30.0)\n  foo2 (20.0)\n  foo1 (10.0)\n", results.get(0).toString());
+    assertEquals("b (50.0)\n  bar2 (30.0)\n  bar1 (20.0)\n", results.get(1).toString());
+    assertEquals("c (30.0)\n  baz1 (30.0)\n", results.get(2).toString());
+
+    searcher.getIndexReader().close();
+    taxoReader.close();
+    taxoDir.close();
+    dir.close();
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setIndexFieldName("a", "$facets2");
+
+    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, config);
+
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(doc);
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    SimpleFacetsCollector c = new SimpleFacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, config, c, new IntFieldSource("num"));
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<SimpleFacetResult> results = facets.getAllDims(10);
+    assertTrue(results.isEmpty());
+
+    try {
+      facets.getSpecificValue("a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    searcher.getIndexReader().close();
+    taxoReader.close();
+    taxoDir.close();
+    dir.close();
+  }
+
+  // nocommit in the sparse case test that we are really
+  // sorting by the correct dim count
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacets.java
deleted file mode 100644
index 515709d..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacets.java
+++ /dev/null
@@ -1,440 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.util.PrintTaxonomyStats;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.similarities.DefaultSimilarity;
-import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
-import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestTaxonomyFacets extends FacetTestCase {
-
-  public void testBasic() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig();
-    config.setHierarchical("Publish Date");
-
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, config);
-
-    Document doc = new Document();
-    doc.add(new FacetField("Author", "Bob"));
-    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Lisa"));
-    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Lisa"));
-    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Susan"));
-    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Frank"));
-    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
-    writer.addDocument(doc);
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Aggregate the facet counts:
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
-
-    // Retrieve & verify results:
-    assertEquals("Publish Date (5)\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", facets.getTopChildren(10, "Publish Date").toString());
-    assertEquals("Author (5)\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", facets.getTopChildren(10, "Author").toString());
-
-    // Now user drills down on Publish Date/2010:
-    SimpleDrillDownQuery q2 = new SimpleDrillDownQuery(config);
-    q2.add("Publish Date", "2010");
-    c = new SimpleFacetsCollector();
-    searcher.search(q2, c);
-    facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
-    assertEquals("Author (2)\n  Bob (1)\n  Lisa (1)\n", facets.getTopChildren(10, "Author").toString());
-
-    assertEquals(1, facets.getSpecificValue("Author", "Lisa"));
-
-    // Smoke test PrintTaxonomyStats:
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintTaxonomyStats.printStats(taxoReader, new PrintStream(bos, false, "UTF-8"), true);
-    String result = bos.toString("UTF-8");
-    assertTrue(result.indexOf("/Author: 4 immediate children; 5 total categories") != -1);
-    assertTrue(result.indexOf("/Publish Date: 3 immediate children; 12 total categories") != -1);
-    // Make sure at least a few nodes of the tree came out:
-    assertTrue(result.indexOf("  /1999") != -1);
-    assertTrue(result.indexOf("  /2012") != -1);
-    assertTrue(result.indexOf("      /20") != -1);
-
-    taxoReader.close();
-    searcher.getIndexReader().close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  // LUCENE-5333
-  public void testSparseFacets() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, new FacetsConfig());
-
-    Document doc = new Document();
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(doc);
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new FacetField("a", "foo2"));
-    doc.add(new FacetField("b", "bar1"));
-    writer.addDocument(doc);
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new FacetField("a", "foo3"));
-    doc.add(new FacetField("b", "bar2"));
-    doc.add(new FacetField("c", "baz1"));
-    writer.addDocument(doc);
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    Facets facets;
-    if (random().nextBoolean()) {
-      facets = new FastTaxonomyFacetCounts(taxoReader, new FacetsConfig(), c);
-    } else {
-      OrdinalsReader ordsReader = new DocValuesOrdinalsReader();
-      if (random().nextBoolean()) {
-        ordsReader = new CachedOrdinalsReader(ordsReader);
-      }
-      facets = new TaxonomyFacetCounts(ordsReader, taxoReader, new FacetsConfig(), c);
-    }
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<SimpleFacetResult> results = facets.getAllDims(10);
-
-    assertEquals(3, results.size());
-    assertEquals("a (3)\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
-    assertEquals("b (2)\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
-    assertEquals("c (1)\n  baz1 (1)\n", results.get(2).toString());
-
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    taxoDir.close();
-    dir.close();
-  }
-
-  public void testWrongIndexFieldName() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig();
-    config.setIndexFieldName("a", "$facets2");
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, config);
-
-    Document doc = new Document();
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(doc);
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    // Uses default $facets field:
-    Facets facets;
-    if (random().nextBoolean()) {
-      facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
-    } else {
-      OrdinalsReader ordsReader = new DocValuesOrdinalsReader();
-      if (random().nextBoolean()) {
-        ordsReader = new CachedOrdinalsReader(ordsReader);
-      }
-      facets = new TaxonomyFacetCounts(ordsReader, taxoReader, config, c);
-    }
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<SimpleFacetResult> results = facets.getAllDims(10);
-    assertTrue(results.isEmpty());
-
-    try {
-      facets.getSpecificValue("a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    try {
-      facets.getTopChildren(10, "a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    taxoDir.close();
-    dir.close();
-  }
-
-
-  // nocommit in the sparse case test that we are really
-  // sorting by the correct dim count
-
-  /*
-  public void testReallyNoNormsForDrillDown() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setSimilarity(new PerFieldSimilarityWrapper() {
-        final Similarity sim = new DefaultSimilarity();
-
-        @Override
-        public Similarity get(String name) {
-          assertEquals("field", name);
-          return sim;
-        }
-      });
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    FacetFields facetFields = new FacetFields(taxoWriter);      
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    facetFields.addFields(doc, Collections.singletonList(new CategoryPath("a/path", '/')));
-    writer.addDocument(doc);
-    writer.close();
-    taxoWriter.close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  public void testAllParents() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    CategoryListParams clp = new CategoryListParams("$facets") {
-        @Override
-        public OrdinalPolicy getOrdinalPolicy(String fieldName) {
-          return OrdinalPolicy.ALL_PARENTS;
-        }
-      };
-    FacetIndexingParams fip = new FacetIndexingParams(clp);
-
-    FacetFields facetFields = new FacetFields(taxoWriter, fip);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    facetFields.addFields(doc, Collections.singletonList(new CategoryPath("a/path", '/')));
-    writer.addDocument(doc);
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-    
-    FacetSearchParams fsp = new FacetSearchParams(fip,
-                                                  new CountFacetRequest(new CategoryPath("a", '/'), 10));
-
-    // Aggregate the facet counts:
-    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-    List<FacetResult> results = c.getFacetResults();
-    assertEquals(1, results.size());
-    assertEquals(1, (int) results.get(0).getFacetResultNode().value);
-
-    // LUCENE-4913:
-    for(FacetResultNode childNode : results.get(0).getFacetResultNode().subResults) {
-      assertTrue(childNode.ordinal != 0);
-    }
-
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  public void testLabelWithDelimiter() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetFields facetFields = new FacetFields(taxoWriter);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    BytesRef br = new BytesRef(new byte[] {(byte) 0xee, (byte) 0x92, (byte) 0xaa, (byte) 0xef, (byte) 0x9d, (byte) 0x89});
-    facetFields.addFields(doc, Collections.singletonList(new CategoryPath("dim/" + br.utf8ToString(), '/')));
-    try {
-      writer.addDocument(doc);
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    writer.close();
-    taxoWriter.close();
-    dir.close();
-    taxoDir.close();
-  }
-  
-  // LUCENE-4583: make sure if we require > 32 KB for one
-  // document, we don't hit exc when using Facet42DocValuesFormat
-  public void testManyFacetsInOneDocument() throws Exception {
-    assumeTrue("default Codec doesn't support huge BinaryDocValues", _TestUtil.fieldSupportsHugeBinaryDocValues(CategoryListParams.DEFAULT_FIELD));
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    
-    FacetFields facetFields = new FacetFields(taxoWriter);
-    
-    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);
-    
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    List<CategoryPath> paths = new ArrayList<CategoryPath>();
-    for (int i = 0; i < numLabels; i++) {
-      paths.add(new CategoryPath("dim", "" + i));
-    }
-    facetFields.addFields(doc, paths);
-    writer.addDocument(doc);
-    
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-    
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-    
-    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new CategoryPath("dim"), Integer.MAX_VALUE));
-    
-    // Aggregate the facet counts:
-    FacetsCollector c = FacetsCollector.create(fsp, searcher.getIndexReader(), taxoReader);
-    
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-    List<FacetResult> results = c.getFacetResults();
-    assertEquals(1, results.size());
-    FacetResultNode root = results.get(0).getFacetResultNode();
-    assertEquals(numLabels, root.subResults.size());
-    Set<String> allLabels = new HashSet<String>();
-    for (FacetResultNode childNode : root.subResults) {
-      assertEquals(2, childNode.label.length);
-      allLabels.add(childNode.label.components[1]);
-      assertEquals(1, (int) childNode.value);
-    }
-    assertEquals(numLabels, allLabels.size());
-    
-    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-  */
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetsSumValueSource.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetsSumValueSource.java
deleted file mode 100644
index eef5889..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetsSumValueSource.java
+++ /dev/null
@@ -1,245 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.IntField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.util.PrintTaxonomyStats;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.queries.function.valuesource.IntFieldSource;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.similarities.DefaultSimilarity;
-import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
-import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestTaxonomyFacetsSumValueSource extends FacetTestCase {
-
-  public void testBasic() throws Exception {
-
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, new FacetsConfig());
-
-    // Reused across documents, to add the necessary facet
-    // fields:
-    Document doc = new Document();
-    doc.add(new IntField("num", 10, Field.Store.NO));
-    doc.add(new FacetField("Author", "Bob"));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new IntField("num", 20, Field.Store.NO));
-    doc.add(new FacetField("Author", "Lisa"));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new IntField("num", 30, Field.Store.NO));
-    doc.add(new FacetField("Author", "Lisa"));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new IntField("num", 40, Field.Store.NO));
-    doc.add(new FacetField("Author", "Susan"));
-    writer.addDocument(doc);
-
-    doc = new Document();
-    doc.add(new IntField("num", 45, Field.Store.NO));
-    doc.add(new FacetField("Author", "Frank"));
-    writer.addDocument(doc);
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Aggregate the facet counts:
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
-
-    // Retrieve & verify results:
-    assertEquals("Author (145.0)\n  Lisa (50.0)\n  Frank (45.0)\n  Susan (40.0)\n  Bob (10.0)\n", facets.getTopChildren(10, "Author").toString());
-
-    taxoReader.close();
-    searcher.getIndexReader().close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  // LUCENE-5333
-  public void testSparseFacets() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, new FacetsConfig());
-
-    Document doc = new Document();
-    doc.add(new IntField("num", 10, Field.Store.NO));
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(doc);
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new IntField("num", 20, Field.Store.NO));
-    doc.add(new FacetField("a", "foo2"));
-    doc.add(new FacetField("b", "bar1"));
-    writer.addDocument(doc);
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new IntField("num", 30, Field.Store.NO));
-    doc.add(new FacetField("a", "foo3"));
-    doc.add(new FacetField("b", "bar2"));
-    doc.add(new FacetField("c", "baz1"));
-    writer.addDocument(doc);
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<SimpleFacetResult> results = facets.getAllDims(10);
-
-    assertEquals(3, results.size());
-    assertEquals("a (60.0)\n  foo3 (30.0)\n  foo2 (20.0)\n  foo1 (10.0)\n", results.get(0).toString());
-    assertEquals("b (50.0)\n  bar2 (30.0)\n  bar1 (20.0)\n", results.get(1).toString());
-    assertEquals("c (30.0)\n  baz1 (30.0)\n", results.get(2).toString());
-
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    taxoDir.close();
-    dir.close();
-  }
-
-  public void testWrongIndexFieldName() throws Exception {
-
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig();
-    config.setIndexFieldName("a", "$facets2");
-
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, config);
-
-    Document doc = new Document();
-    doc.add(new IntField("num", 10, Field.Store.NO));
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(doc);
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, config, c, new IntFieldSource("num"));
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<SimpleFacetResult> results = facets.getAllDims(10);
-    assertTrue(results.isEmpty());
-
-    try {
-      facets.getSpecificValue("a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    try {
-      facets.getTopChildren(10, "a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    searcher.getIndexReader().close();
-    taxoReader.close();
-    taxoDir.close();
-    dir.close();
-  }
-
-  // nocommit in the sparse case test that we are really
-  // sorting by the correct dim count
-}

