GitDiffStart: 134a4a400a150a2467fc030f8920e92f3b7dcb7c | Tue Jul 24 04:36:52 2012 +0000
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/TestCommonGramsFilterFactory.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/TestCommonGramsFilterFactory.java
index f2b9204..3739f08 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/TestCommonGramsFilterFactory.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/TestCommonGramsFilterFactory.java
@@ -21,9 +21,10 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.TestStopFilter;
 import org.apache.lucene.analysis.util.CharArraySet;
+import org.apache.lucene.analysis.util.ResourceAsStreamResourceLoader;
 import org.apache.lucene.analysis.util.ResourceLoader;
-import org.apache.solr.core.SolrResourceLoader;
 
 import java.io.StringReader;
 import java.util.Collections;
@@ -38,7 +39,7 @@ import java.util.HashMap;
 public class TestCommonGramsFilterFactory extends BaseTokenStreamTestCase {
 
   public void testInform() throws Exception {
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    ResourceLoader loader = new ResourceAsStreamResourceLoader(TestStopFilter.class);
     assertTrue("loader is null and it shouldn't be", loader != null);
     CommonGramsFilterFactory factory = new CommonGramsFilterFactory();
     Map<String, String> args = new HashMap<String, String>();
@@ -88,7 +89,7 @@ public class TestCommonGramsFilterFactory extends BaseTokenStreamTestCase {
    * If no words are provided, then a set of english default stopwords is used.
    */
   public void testDefaults() throws Exception {
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    ResourceLoader loader = new ResourceAsStreamResourceLoader(TestStopFilter.class);
     assertTrue("loader is null and it shouldn't be", loader != null);
     CommonGramsFilterFactory factory = new CommonGramsFilterFactory();
     factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/TestCommonGramsQueryFilterFactory.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/TestCommonGramsQueryFilterFactory.java
index 80d5534..95de4d4 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/TestCommonGramsQueryFilterFactory.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/commongrams/TestCommonGramsQueryFilterFactory.java
@@ -21,9 +21,10 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.core.TestStopFilter;
 import org.apache.lucene.analysis.util.CharArraySet;
+import org.apache.lucene.analysis.util.ResourceAsStreamResourceLoader;
 import org.apache.lucene.analysis.util.ResourceLoader;
-import org.apache.solr.core.SolrResourceLoader;
 
 import java.io.StringReader;
 import java.util.Collections;
@@ -38,7 +39,7 @@ import java.util.HashMap;
 public class TestCommonGramsQueryFilterFactory extends BaseTokenStreamTestCase {
 
   public void testInform() throws Exception {
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    ResourceLoader loader = new ResourceAsStreamResourceLoader(TestStopFilter.class);
     assertTrue("loader is null and it shouldn't be", loader != null);
     CommonGramsQueryFilterFactory factory = new CommonGramsQueryFilterFactory();
     Map<String, String> args = new HashMap<String, String>();
@@ -88,7 +89,7 @@ public class TestCommonGramsQueryFilterFactory extends BaseTokenStreamTestCase {
    * If no words are provided, then a set of english default stopwords is used.
    */
   public void testDefaults() throws Exception {
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    ResourceLoader loader = new ResourceAsStreamResourceLoader(TestStopFilter.class);
     assertTrue("loader is null and it shouldn't be", loader != null);
     CommonGramsQueryFilterFactory factory = new CommonGramsQueryFilterFactory();
     factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilterFactory.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilterFactory.java
index a066082..dc977c4 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilterFactory.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestStopFilterFactory.java
@@ -19,8 +19,8 @@ package org.apache.lucene.analysis.core;
 
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.util.CharArraySet;
+import org.apache.lucene.analysis.util.ResourceAsStreamResourceLoader;
 import org.apache.lucene.analysis.util.ResourceLoader;
-import org.apache.solr.core.SolrResourceLoader;
 
 import java.util.Map;
 import java.util.HashMap;
@@ -32,7 +32,7 @@ import java.util.HashMap;
 public class TestStopFilterFactory extends BaseTokenStreamTestCase {
 
   public void testInform() throws Exception {
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    ResourceLoader loader = new ResourceAsStreamResourceLoader(getClass());
     assertTrue("loader is null and it shouldn't be", loader != null);
     StopFilterFactory factory = new StopFilterFactory();
     Map<String, String> args = new HashMap<String, String>();
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestTypeTokenFilterFactory.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestTypeTokenFilterFactory.java
index 2ed5bc7..bf4416f 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestTypeTokenFilterFactory.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestTypeTokenFilterFactory.java
@@ -20,8 +20,8 @@ package org.apache.lucene.analysis.core;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.NumericTokenStream;
 import org.apache.lucene.analysis.util.InitializationException;
+import org.apache.lucene.analysis.util.ResourceAsStreamResourceLoader;
 import org.apache.lucene.analysis.util.ResourceLoader;
-import org.apache.solr.core.SolrResourceLoader;
 import org.junit.Test;
 
 import java.util.HashMap;
@@ -35,7 +35,7 @@ public class TestTypeTokenFilterFactory extends BaseTokenStreamTestCase {
 
   @Test
   public void testInform() throws Exception {
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    ResourceLoader loader = new ResourceAsStreamResourceLoader(getClass());
     TypeTokenFilterFactory factory = new TypeTokenFilterFactory();
     Map<String, String> args = new HashMap<String, String>();
     args.put("types", "stoptypes-1.txt");
@@ -95,7 +95,7 @@ public class TestTypeTokenFilterFactory extends BaseTokenStreamTestCase {
       args.put("enablePositionIncrements", "false");
       typeTokenFilterFactory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
       typeTokenFilterFactory.init(args);
-      typeTokenFilterFactory.inform(new SolrResourceLoader(null, null));
+      typeTokenFilterFactory.inform(new ResourceAsStreamResourceLoader(getClass()));
       fail("not supplying 'types' parameter should cause an InitializationException");
     } catch (InitializationException e) {
       // everything ok
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stop-1.txt b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stop-1.txt
new file mode 100644
index 0000000..8dfe809
--- /dev/null
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stop-1.txt
@@ -0,0 +1,17 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+foo
+bar
\ No newline at end of file
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stop-2.txt b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stop-2.txt
new file mode 100644
index 0000000..646b7ff
--- /dev/null
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stop-2.txt
@@ -0,0 +1,17 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+junk
+more
\ No newline at end of file
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stop-snowball.txt b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stop-snowball.txt
new file mode 100644
index 0000000..1c0c6f5
--- /dev/null
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stop-snowball.txt
@@ -0,0 +1,10 @@
+ | This is a file in snowball format, empty lines are ignored, '|' is a comment
+ | Additionally, multiple words can be on the same line, allowing stopwords to be
+ | arranged in tables (useful in some languages where they might inflect)
+
+ | fictitious table below
+
+|third person singular
+|Subject Object Possessive Reflexive
+he       him    his        himself| masculine
+she      her    hers       herself| feminine
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stoptypes-1.txt b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stoptypes-1.txt
new file mode 100644
index 0000000..456348e
--- /dev/null
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stoptypes-1.txt
@@ -0,0 +1,17 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+<NUM>
+<EMAIL>
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stoptypes-2.txt b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stoptypes-2.txt
new file mode 100644
index 0000000..d8a3810
--- /dev/null
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/stoptypes-2.txt
@@ -0,0 +1,17 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+<HOST>
+<APOSTROPHE>
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestElisionFilterFactory.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestElisionFilterFactory.java
index 5770f32..2c48d8e 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestElisionFilterFactory.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/TestElisionFilterFactory.java
@@ -27,8 +27,8 @@ import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.util.ResourceAsStreamResourceLoader;
 import org.apache.lucene.analysis.util.ResourceLoader;
-import org.apache.solr.core.SolrResourceLoader;
 
 /**
  * Simple tests to ensure the French elision filter factory is working.
@@ -42,7 +42,7 @@ public class TestElisionFilterFactory extends BaseTokenStreamTestCase {
     Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
     ElisionFilterFactory factory = new ElisionFilterFactory();
     factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    ResourceLoader loader = new ResourceAsStreamResourceLoader(getClass());
     Map<String,String> args = new HashMap<String,String>();
     args.put("articles", "frenchArticles.txt");
     factory.init(args);
@@ -61,7 +61,7 @@ public class TestElisionFilterFactory extends BaseTokenStreamTestCase {
     factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
     Map<String, String> args = Collections.emptyMap();
     factory.init(args);
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    ResourceLoader loader = new ResourceAsStreamResourceLoader(getClass());
     factory.inform(loader);
     TokenStream stream = factory.create(tokenizer);
     assertTokenStreamContents(stream, new String[] { "avion" });
@@ -75,7 +75,7 @@ public class TestElisionFilterFactory extends BaseTokenStreamTestCase {
     Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, false);
     ElisionFilterFactory factory = new ElisionFilterFactory();
     factory.setLuceneMatchVersion(TEST_VERSION_CURRENT);
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    ResourceLoader loader = new ResourceAsStreamResourceLoader(getClass());
     Map<String,String> args = new HashMap<String,String>();
     args.put("articles", "frenchArticles.txt");
     args.put("ignoreCase", "true");
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/frenchArticles.txt b/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/frenchArticles.txt
new file mode 100644
index 0000000..9141611
--- /dev/null
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/fr/frenchArticles.txt
@@ -0,0 +1,24 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+# A set of articles for testing the French Elision filter.
+# Requiring a text file is a bit weird here...
+l
+m
+t
+qu
+n
+s
+j
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilterFactory.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilterFactory.java
deleted file mode 100644
index b93bc00..0000000
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/miscellaneous/TestWordDelimiterFilterFactory.java
+++ /dev/null
@@ -1,242 +0,0 @@
-package org.apache.lucene.analysis.miscellaneous;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.StringReader;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.solr.SolrTestCaseJ4;
-import org.apache.lucene.analysis.util.ResourceLoader;
-import org.apache.solr.core.SolrResourceLoader;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * New WordDelimiterFilter tests... most of the tests are in ConvertedLegacyTest
- */
-public class TestWordDelimiterFilterFactory extends SolrTestCaseJ4 {
-
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    initCore("solrconfig.xml","schema.xml");
-  }
-
-  public void posTst(String v1, String v2, String s1, String s2) {
-    assertU(adoc("id",  "42",
-                 "subword", v1,
-                 "subword", v2));
-    assertU(commit());
-
-    // there is a positionIncrementGap of 100 between field values, so
-    // we test if that was maintained.
-    assertQ("position increment lost",
-            req("+id:42 +subword:\"" + s1 + ' ' + s2 + "\"~90")
-            ,"//result[@numFound=0]"
-    );
-    assertQ("position increment lost",
-            req("+id:42 +subword:\"" + s1 + ' ' + s2 + "\"~110")
-            ,"//result[@numFound=1]"
-    );
-    clearIndex();
-  }
-
-  @Test
-  public void testRetainPositionIncrement() {
-    posTst("foo","bar","foo","bar");
-    posTst("-foo-","-bar-","foo","bar");
-    posTst("foo","bar","-foo-","-bar-");
-
-    posTst("123","456","123","456");
-    posTst("/123/","/456/","123","456");
-
-    posTst("/123/abc","qwe/456/","abc","qwe");
-
-    posTst("zoo-foo","bar-baz","foo","bar");
-    posTst("zoo-foo-123","456-bar-baz","foo","bar");
-  }
-
-  @Test
-  public void testNoGenerationEdgeCase() {
-    assertU(adoc("id", "222", "numberpartfail", "123.123.123.123"));
-    clearIndex();
-  }
-
-  @Test
-  public void testIgnoreCaseChange() {
-
-    assertU(adoc("id",  "43",
-                 "wdf_nocase", "HellO WilliAM",
-                 "subword", "GoodBye JonEs"));
-    assertU(commit());
-    
-    assertQ("no case change",
-            req("wdf_nocase:(hell o am)")
-            ,"//result[@numFound=0]"
-    );
-    assertQ("case change",
-            req("subword:(good jon)")
-            ,"//result[@numFound=1]"
-    );
-    clearIndex();
-  }
-
-  @Test
-  public void testPreserveOrignalTrue() {
-
-    assertU(adoc("id",  "144",
-                 "wdf_preserve", "404-123"));
-    assertU(commit());
-    
-    assertQ("preserving original word",
-            req("wdf_preserve:404")
-            ,"//result[@numFound=1]"
-    );
-    
-    assertQ("preserving original word",
-        req("wdf_preserve:123")
-        ,"//result[@numFound=1]"
-    );
-
-    assertQ("preserving original word",
-        req("wdf_preserve:404-123*")
-        ,"//result[@numFound=1]"
-    );
-    clearIndex();
-  }
-
-  /***
-  public void testPerformance() throws IOException {
-    String s = "now is the time-for all good men to come to-the aid of their country.";
-    Token tok = new Token();
-    long start = System.currentTimeMillis();
-    int ret=0;
-    for (int i=0; i<1000000; i++) {
-      StringReader r = new StringReader(s);
-      TokenStream ts = new WhitespaceTokenizer(r);
-      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);
-
-      while (ts.next(tok) != null) ret++;
-    }
-
-    System.out.println("ret="+ret+" time="+(System.currentTimeMillis()-start));
-  }
-  ***/
-
-  @Test
-  public void testAlphaNumericWords(){
-     assertU(adoc("id",  "68","numericsubword","Java/J2SE"));
-     assertU(commit());
-
-     assertQ("j2se found",
-            req("numericsubword:(J2SE)")
-            ,"//result[@numFound=1]"
-    );
-      assertQ("no j2 or se",
-            req("numericsubword:(J2 OR SE)")
-            ,"//result[@numFound=0]"
-    );
-    clearIndex();
-  }
-
-  @Test
-  public void testProtectedWords(){
-    assertU(adoc("id", "70","protectedsubword","c# c++ .net Java/J2SE"));
-    assertU(commit());
-
-    assertQ("java found",
-            req("protectedsubword:(java)")
-            ,"//result[@numFound=1]"
-    );
-
-    assertQ(".net found",
-            req("protectedsubword:(.net)")
-            ,"//result[@numFound=1]"
-    );
-
-    assertQ("c# found",
-            req("protectedsubword:(c#)")
-            ,"//result[@numFound=1]"
-    );
-
-    assertQ("c++ found",
-            req("protectedsubword:(c++)")
-            ,"//result[@numFound=1]"
-    );
-
-    assertQ("c found?",
-            req("protectedsubword:c")
-            ,"//result[@numFound=0]"
-    );
-    assertQ("net found?",
-            req("protectedsubword:net")
-            ,"//result[@numFound=0]"
-    );
-    clearIndex();
-  }
-  
-  @Test
-  public void testCustomTypes() throws Exception {
-    String testText = "I borrowed $5,400.00 at 25% interest-rate";
-    WordDelimiterFilterFactory factoryDefault = new WordDelimiterFilterFactory();
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
-    Map<String,String> args = new HashMap<String,String>();
-    args.put("generateWordParts", "1");
-    args.put("generateNumberParts", "1");
-    args.put("catenateWords", "1");
-    args.put("catenateNumbers", "1");
-    args.put("catenateAll", "0");
-    args.put("splitOnCaseChange", "1");
-    
-    /* default behavior */
-    factoryDefault.init(args);
-    factoryDefault.inform(loader);
-    
-    TokenStream ts = factoryDefault.create(
-        new MockTokenizer(new StringReader(testText), MockTokenizer.WHITESPACE, false));
-    BaseTokenStreamTestCase.assertTokenStreamContents(ts, 
-        new String[] { "I", "borrowed", "5", "400", "00", "540000", "at", "25", "interest", "rate", "interestrate" });
-
-    ts = factoryDefault.create(
-        new MockTokenizer(new StringReader("foo\u200Dbar"), MockTokenizer.WHITESPACE, false));
-    BaseTokenStreamTestCase.assertTokenStreamContents(ts, 
-        new String[] { "foo", "bar", "foobar" });
-
-    
-    /* custom behavior */
-    WordDelimiterFilterFactory factoryCustom = new WordDelimiterFilterFactory();
-    // use a custom type mapping
-    args.put("types", "wdftypes.txt");
-    factoryCustom.init(args);
-    factoryCustom.inform(loader);
-    
-    ts = factoryCustom.create(
-        new MockTokenizer(new StringReader(testText), MockTokenizer.WHITESPACE, false));
-    BaseTokenStreamTestCase.assertTokenStreamContents(ts, 
-        new String[] { "I", "borrowed", "$5,400.00", "at", "25%", "interest", "rate", "interestrate" });
-    
-    /* test custom behavior with a char > 0x7F, because we had to make a larger byte[] */
-    ts = factoryCustom.create(
-        new MockTokenizer(new StringReader("foo\u200Dbar"), MockTokenizer.WHITESPACE, false));
-    BaseTokenStreamTestCase.assertTokenStreamContents(ts, 
-        new String[] { "foo\u200Dbar" });
-  }
-}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/payloads/TestDelimitedPayloadTokenFilterFactory.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/payloads/TestDelimitedPayloadTokenFilterFactory.java
index 8f497ab..9e87d03 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/payloads/TestDelimitedPayloadTokenFilterFactory.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/payloads/TestDelimitedPayloadTokenFilterFactory.java
@@ -29,7 +29,7 @@ import org.apache.lucene.analysis.payloads.FloatEncoder;
 import org.apache.lucene.analysis.payloads.PayloadHelper;
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.analysis.util.ResourceLoader;
-import org.apache.solr.core.SolrResourceLoader;
+import org.apache.lucene.analysis.util.StringMockResourceLoader;
 
 public class TestDelimitedPayloadTokenFilterFactory extends BaseTokenStreamTestCase {
 
@@ -38,7 +38,7 @@ public class TestDelimitedPayloadTokenFilterFactory extends BaseTokenStreamTestC
     args.put(DelimitedPayloadTokenFilterFactory.ENCODER_ATTR, "float");
     DelimitedPayloadTokenFilterFactory factory = new DelimitedPayloadTokenFilterFactory();
     factory.init(args);
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    ResourceLoader loader = new StringMockResourceLoader("solr/collection1");
     factory.inform(loader);
 
     TokenStream input = new MockTokenizer(new StringReader("the|0.1 quick|0.1 red|0.1"), MockTokenizer.WHITESPACE, false);
@@ -61,7 +61,7 @@ public class TestDelimitedPayloadTokenFilterFactory extends BaseTokenStreamTestC
     args.put(DelimitedPayloadTokenFilterFactory.DELIMITER_ATTR, "*");
     DelimitedPayloadTokenFilterFactory factory = new DelimitedPayloadTokenFilterFactory();
     factory.init(args);
-    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    ResourceLoader loader = new StringMockResourceLoader("solr/collection1");
     factory.inform(loader);
 
     TokenStream input = new MockTokenizer(new StringReader("the*0.1 quick*0.1 red*0.1"), MockTokenizer.WHITESPACE, false);
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/ResourceAsStreamResourceLoader.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/ResourceAsStreamResourceLoader.java
new file mode 100644
index 0000000..3d06aa4
--- /dev/null
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/ResourceAsStreamResourceLoader.java
@@ -0,0 +1,84 @@
+package org.apache.lucene.analysis.util;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.nio.charset.CharacterCodingException;
+import java.nio.charset.CodingErrorAction;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.util.IOUtils;
+
+public class ResourceAsStreamResourceLoader implements ResourceLoader {
+  Class<?> clazz;
+  
+  public ResourceAsStreamResourceLoader(Class<?> clazz) {
+    this.clazz = clazz;
+  }
+
+  @Override
+  public InputStream openResource(String resource) throws IOException {
+    return clazz.getResourceAsStream(resource);
+  }
+
+  @Override
+  public List<String> getLines(String resource) throws IOException {
+    BufferedReader input = null;
+    ArrayList<String> lines;
+    try {
+      input = new BufferedReader(new InputStreamReader(openResource(resource),
+          IOUtils.CHARSET_UTF_8.newDecoder()
+          .onMalformedInput(CodingErrorAction.REPORT)
+          .onUnmappableCharacter(CodingErrorAction.REPORT)));
+
+      lines = new ArrayList<String>();
+      for (String word=null; (word=input.readLine())!=null;) {
+        // skip initial bom marker
+        if (lines.isEmpty() && word.length() > 0 && word.charAt(0) == '\uFEFF')
+          word = word.substring(1);
+        // skip comments
+        if (word.startsWith("#")) continue;
+        word=word.trim();
+        // skip blank lines
+        if (word.length()==0) continue;
+        lines.add(word);
+      }
+    } catch (CharacterCodingException ex) {
+      throw new RuntimeException("Error loading resource (wrong encoding?): " + resource, ex);
+    } finally {
+      if (input != null)
+        input.close();
+    }
+    return lines;
+  }
+
+  // TODO: do this subpackages thing... wtf is that?
+  @Override
+  public <T> T newInstance(String cname, Class<T> expectedType, String... subpackages) {
+    try {
+      Class<? extends T> clazz = Class.forName(cname).asSubclass(expectedType);
+      return clazz.newInstance();
+    } catch (Exception e) {
+      throw new RuntimeException(e);
+    }
+  }
+}
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/StringMockResourceLoader.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/StringMockResourceLoader.java
new file mode 100644
index 0000000..2a73121
--- /dev/null
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/StringMockResourceLoader.java
@@ -0,0 +1,51 @@
+package org.apache.lucene.analysis.util;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.util.Arrays;
+import java.util.List;
+
+/** Fake resource loader for tests: works if you want to fake reading a single file */
+public class StringMockResourceLoader implements ResourceLoader {
+  String text;
+
+  public StringMockResourceLoader(String text) {
+    this.text = text;
+  }
+
+  public List<String> getLines(String resource) throws IOException {
+    return Arrays.asList(text.split("\n"));
+  }
+
+  // TODO: do this subpackages thing... wtf is that?
+  public <T> T newInstance(String cname, Class<T> expectedType, String... subpackages) {
+    try {
+      Class<? extends T> clazz = Class.forName(cname).asSubclass(expectedType);
+      return clazz.newInstance();
+    } catch (Exception e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  public InputStream openResource(String resource) throws IOException {
+    return new ByteArrayInputStream(text.getBytes("UTF-8"));
+  }
+}
diff --git a/solr/core/src/test/org/apache/solr/analysis/TestWordDelimiterFilterFactory.java b/solr/core/src/test/org/apache/solr/analysis/TestWordDelimiterFilterFactory.java
new file mode 100644
index 0000000..b77104c
--- /dev/null
+++ b/solr/core/src/test/org/apache/solr/analysis/TestWordDelimiterFilterFactory.java
@@ -0,0 +1,244 @@
+package org.apache.solr.analysis;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.StringReader;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.solr.SolrTestCaseJ4;
+import org.apache.lucene.analysis.miscellaneous.WordDelimiterFilterFactory;
+import org.apache.lucene.analysis.util.ResourceLoader;
+import org.apache.solr.core.SolrResourceLoader;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/**
+ * New WordDelimiterFilter tests... most of the tests are in ConvertedLegacyTest
+ */
+// TODO: add a low-level test for this factory
+public class TestWordDelimiterFilterFactory extends SolrTestCaseJ4 {
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    initCore("solrconfig.xml","schema.xml");
+  }
+
+  public void posTst(String v1, String v2, String s1, String s2) {
+    assertU(adoc("id",  "42",
+                 "subword", v1,
+                 "subword", v2));
+    assertU(commit());
+
+    // there is a positionIncrementGap of 100 between field values, so
+    // we test if that was maintained.
+    assertQ("position increment lost",
+            req("+id:42 +subword:\"" + s1 + ' ' + s2 + "\"~90")
+            ,"//result[@numFound=0]"
+    );
+    assertQ("position increment lost",
+            req("+id:42 +subword:\"" + s1 + ' ' + s2 + "\"~110")
+            ,"//result[@numFound=1]"
+    );
+    clearIndex();
+  }
+
+  @Test
+  public void testRetainPositionIncrement() {
+    posTst("foo","bar","foo","bar");
+    posTst("-foo-","-bar-","foo","bar");
+    posTst("foo","bar","-foo-","-bar-");
+
+    posTst("123","456","123","456");
+    posTst("/123/","/456/","123","456");
+
+    posTst("/123/abc","qwe/456/","abc","qwe");
+
+    posTst("zoo-foo","bar-baz","foo","bar");
+    posTst("zoo-foo-123","456-bar-baz","foo","bar");
+  }
+
+  @Test
+  public void testNoGenerationEdgeCase() {
+    assertU(adoc("id", "222", "numberpartfail", "123.123.123.123"));
+    clearIndex();
+  }
+
+  @Test
+  public void testIgnoreCaseChange() {
+
+    assertU(adoc("id",  "43",
+                 "wdf_nocase", "HellO WilliAM",
+                 "subword", "GoodBye JonEs"));
+    assertU(commit());
+    
+    assertQ("no case change",
+            req("wdf_nocase:(hell o am)")
+            ,"//result[@numFound=0]"
+    );
+    assertQ("case change",
+            req("subword:(good jon)")
+            ,"//result[@numFound=1]"
+    );
+    clearIndex();
+  }
+
+  @Test
+  public void testPreserveOrignalTrue() {
+
+    assertU(adoc("id",  "144",
+                 "wdf_preserve", "404-123"));
+    assertU(commit());
+    
+    assertQ("preserving original word",
+            req("wdf_preserve:404")
+            ,"//result[@numFound=1]"
+    );
+    
+    assertQ("preserving original word",
+        req("wdf_preserve:123")
+        ,"//result[@numFound=1]"
+    );
+
+    assertQ("preserving original word",
+        req("wdf_preserve:404-123*")
+        ,"//result[@numFound=1]"
+    );
+    clearIndex();
+  }
+
+  /***
+  public void testPerformance() throws IOException {
+    String s = "now is the time-for all good men to come to-the aid of their country.";
+    Token tok = new Token();
+    long start = System.currentTimeMillis();
+    int ret=0;
+    for (int i=0; i<1000000; i++) {
+      StringReader r = new StringReader(s);
+      TokenStream ts = new WhitespaceTokenizer(r);
+      ts = new WordDelimiterFilter(ts, 1,1,1,1,0);
+
+      while (ts.next(tok) != null) ret++;
+    }
+
+    System.out.println("ret="+ret+" time="+(System.currentTimeMillis()-start));
+  }
+  ***/
+
+  @Test
+  public void testAlphaNumericWords(){
+     assertU(adoc("id",  "68","numericsubword","Java/J2SE"));
+     assertU(commit());
+
+     assertQ("j2se found",
+            req("numericsubword:(J2SE)")
+            ,"//result[@numFound=1]"
+    );
+      assertQ("no j2 or se",
+            req("numericsubword:(J2 OR SE)")
+            ,"//result[@numFound=0]"
+    );
+    clearIndex();
+  }
+
+  @Test
+  public void testProtectedWords(){
+    assertU(adoc("id", "70","protectedsubword","c# c++ .net Java/J2SE"));
+    assertU(commit());
+
+    assertQ("java found",
+            req("protectedsubword:(java)")
+            ,"//result[@numFound=1]"
+    );
+
+    assertQ(".net found",
+            req("protectedsubword:(.net)")
+            ,"//result[@numFound=1]"
+    );
+
+    assertQ("c# found",
+            req("protectedsubword:(c#)")
+            ,"//result[@numFound=1]"
+    );
+
+    assertQ("c++ found",
+            req("protectedsubword:(c++)")
+            ,"//result[@numFound=1]"
+    );
+
+    assertQ("c found?",
+            req("protectedsubword:c")
+            ,"//result[@numFound=0]"
+    );
+    assertQ("net found?",
+            req("protectedsubword:net")
+            ,"//result[@numFound=0]"
+    );
+    clearIndex();
+  }
+  
+  @Test
+  public void testCustomTypes() throws Exception {
+    String testText = "I borrowed $5,400.00 at 25% interest-rate";
+    WordDelimiterFilterFactory factoryDefault = new WordDelimiterFilterFactory();
+    ResourceLoader loader = new SolrResourceLoader("solr/collection1");
+    Map<String,String> args = new HashMap<String,String>();
+    args.put("generateWordParts", "1");
+    args.put("generateNumberParts", "1");
+    args.put("catenateWords", "1");
+    args.put("catenateNumbers", "1");
+    args.put("catenateAll", "0");
+    args.put("splitOnCaseChange", "1");
+    
+    /* default behavior */
+    factoryDefault.init(args);
+    factoryDefault.inform(loader);
+    
+    TokenStream ts = factoryDefault.create(
+        new MockTokenizer(new StringReader(testText), MockTokenizer.WHITESPACE, false));
+    BaseTokenStreamTestCase.assertTokenStreamContents(ts, 
+        new String[] { "I", "borrowed", "5", "400", "00", "540000", "at", "25", "interest", "rate", "interestrate" });
+
+    ts = factoryDefault.create(
+        new MockTokenizer(new StringReader("foo\u200Dbar"), MockTokenizer.WHITESPACE, false));
+    BaseTokenStreamTestCase.assertTokenStreamContents(ts, 
+        new String[] { "foo", "bar", "foobar" });
+
+    
+    /* custom behavior */
+    WordDelimiterFilterFactory factoryCustom = new WordDelimiterFilterFactory();
+    // use a custom type mapping
+    args.put("types", "wdftypes.txt");
+    factoryCustom.init(args);
+    factoryCustom.inform(loader);
+    
+    ts = factoryCustom.create(
+        new MockTokenizer(new StringReader(testText), MockTokenizer.WHITESPACE, false));
+    BaseTokenStreamTestCase.assertTokenStreamContents(ts, 
+        new String[] { "I", "borrowed", "$5,400.00", "at", "25%", "interest", "rate", "interestrate" });
+    
+    /* test custom behavior with a char > 0x7F, because we had to make a larger byte[] */
+    ts = factoryCustom.create(
+        new MockTokenizer(new StringReader("foo\u200Dbar"), MockTokenizer.WHITESPACE, false));
+    BaseTokenStreamTestCase.assertTokenStreamContents(ts, 
+        new String[] { "foo\u200Dbar" });
+  }
+}

