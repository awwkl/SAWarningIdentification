GitDiffStart: 45ea59dd5728afb7a9bb201a09a59ab8eb7c80d0 | Fri Mar 26 05:21:13 2010 +0000
diff --git a/lucene/build.xml b/lucene/build.xml
index c1a4816..268f4a4 100644
--- a/lucene/build.xml
+++ b/lucene/build.xml
@@ -287,14 +287,12 @@ The source distribution does not contain sources of the previous Lucene Java ver
           <packageset dir="contrib/icu/src/java"/>
           <packageset dir="contrib/db/bdb-je/src/java"/>
           <packageset dir="contrib/db/bdb/src/java"/>
-          <packageset dir="contrib/fast-vector-highlighter/src/java"/>
           <packageset dir="contrib/highlighter/src/java"/>
           <packageset dir="contrib/instantiated/src/java"/>
           <packageset dir="contrib/lucli/src/java"/>
           <packageset dir="contrib/memory/src/java"/>
           <packageset dir="contrib/misc/src/java"/>
           <packageset dir="contrib/queries/src/java"/>
-          <packageset dir="contrib/regex/src/java"/>
           <packageset dir="contrib/remote/src/java"/>
           <packageset dir="contrib/spatial/src/java"/>
           <packageset dir="contrib/spellchecker/src/java"/>
@@ -319,15 +317,13 @@ The source distribution does not contain sources of the previous Lucene Java ver
           <group title="contrib: Benchmark" packages="org.apache.lucene.benchmark*"/>
           <group title="contrib: ICU" packages="org.apache.lucene.collation*"/>
           <group title="contrib: DB" packages="org.apache.lucene.store.db*:org.apache.lucene.store.je*:com.sleepycat*"/>
-          <group title="contrib: Fast Vector Highlighter" packages="org.apache.lucene.search.vectorhighlight*"/>
-          <group title="contrib: Highlighter" packages="org.apache.lucene.search.highlight*"/>
+          <group title="contrib: Highlighter" packages="org.apache.lucene.search.highlight:*org.apache.lucene.search.vectorhighlight*"/>
           <group title="contrib: Instantiated" packages="org.apache.lucene.store.instantiated*"/>
           <group title="contrib: Lucli" packages="lucli*"/>
           <group title="contrib: Memory" packages="org.apache.lucene.index.memory*"/>
-          <group title="contrib: Misc " packages="org.apache.lucene.misc*:org.apache.lucene.queryParser.analyzing*:org.apache.lucene.queryParser.precedence*"/>
-          <group title="contrib: Queries" packages="org.apache.lucene.search.similar*"/>
+          <group title="contrib: Misc " packages="org.apache.lucene.misc*"/>
+          <group title="contrib: Queries" packages="org.apache.lucene.search.similar*:org.apache.lucene.search.regex*:org.apache.regexp*"/>
           <group title="contrib: Query Parser" packages="org.apache.lucene.queryParser.*"/>
-          <group title="contrib: RegEx" packages="org.apache.lucene.search.regex*:org.apache.regexp*"/>
           <group title="contrib: Spatial" packages="org.apache.lucene.spatial*"/>
           <group title="contrib: SpellChecker" packages="org.apache.lucene.search.spell*"/>
           <group title="contrib: Surround Parser" packages="org.apache.lucene.queryParser.surround*"/>
diff --git a/lucene/contrib/CHANGES.txt b/lucene/contrib/CHANGES.txt
index 33eda32..391ab36 100644
--- a/lucene/contrib/CHANGES.txt
+++ b/lucene/contrib/CHANGES.txt
@@ -112,6 +112,10 @@ Build
  * LUCENE-2124: Moved the JDK-based collation support from contrib/collation 
    into core, and moved the ICU-based collation support into contrib/icu.  
    (Steven Rowe, Robert Muir)
+
+ * LUCENE-2323: Moved contrib/regex into contrib/queries. Moved the
+   queryparsers under contrib/misc into contrib/queryparser. Moved
+   contrib/fast-vector-highlighter into contrib/highlighter.  (Robert Muir)
    
 Optimizations
 
diff --git a/lucene/contrib/benchmark/build.xml b/lucene/contrib/benchmark/build.xml
index 3dc4c55..60a9286 100644
--- a/lucene/contrib/benchmark/build.xml
+++ b/lucene/contrib/benchmark/build.xml
@@ -130,7 +130,6 @@
         <pathelement path="${common.dir}/build/classes/demo"/>
         <pathelement path="${common.dir}/build/contrib/highlighter/classes/java"/>
         <pathelement path="${common.dir}/build/contrib/memory/classes/java"/>
-        <pathelement path="${common.dir}/build/contrib/fast-vector-highlighter/classes/java"/>
         <pathelement path="${common.dir}/build/contrib/analyzers/common/classes/java"/>
     	<fileset dir="lib">
     		<include name="**/*.jar"/>
@@ -244,12 +243,7 @@
          <fileset dir="${common.dir}/contrib/memory" includes="build.xml"/>
       </subant>
     </target>
-    <target name="compile-vector-highlighter">
-      <subant target="compile">
-         <fileset dir="${common.dir}/contrib/fast-vector-highlighter" includes="build.xml"/>
-      </subant>
-    </target>
 
-    <target name="init" depends="common.init,compile-demo,compile-memory,compile-highlighter,compile-vector-highlighter"/>
+    <target name="init" depends="common.init,compile-demo,compile-memory,compile-highlighter"/>
     
 </project>
diff --git a/lucene/contrib/fast-vector-highlighter/build.xml b/lucene/contrib/fast-vector-highlighter/build.xml
deleted file mode 100644
index 4252018..0000000
--- a/lucene/contrib/fast-vector-highlighter/build.xml
+++ /dev/null
@@ -1,44 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-
-<project name="fast-vector-highlighter" default="default">
-
-  <description>
-    Hits highlighter using TermVectors
-  </description>
-
-  <import file="../contrib-build.xml"/>
-
-  <property name="analyzers.jar" location="${common.dir}/build/contrib/analyzers/lucene-analyzers-${version}.jar"/>
-  <available property="analyzers.jar.present" type="file" file="${analyzers.jar}"/>
-
-  <path id="classpath">
-    <pathelement path="${lucene.jar}"/>
-    <pathelement path="${analyzers.jar}"/>
-    <pathelement path="${project.classpath}"/>
-  </path>
-
-  <target name="compile-core" depends="build-analyzers, common.compile-core" />
-
-  <target name="build-analyzers" unless="analyzers.jar.present">
-    <echo>Fast Vector Highlighter building dependency ${analyzers.jar}</echo>
-    <ant antfile="../analyzers/build.xml" target="default" inheritall="false" dir="../analyzers" />
-  </target>
-
-</project>
diff --git a/lucene/contrib/fast-vector-highlighter/pom.xml.template b/lucene/contrib/fast-vector-highlighter/pom.xml.template
deleted file mode 100644
index 6355811..0000000
--- a/lucene/contrib/fast-vector-highlighter/pom.xml.template
+++ /dev/null
@@ -1,45 +0,0 @@
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
-
-  <!--
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-  -->
-
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.lucene</groupId>
-    <artifactId>lucene-contrib</artifactId>
-    <version>@version@</version>
-  </parent>
-  <groupId>org.apache.lucene</groupId>
-  <artifactId>lucene-fast-vector-highlighter</artifactId>
-  <name>Lucene Fast-Vector-Highlighter</name>
-  <version>@version@</version>
-  <description>
-    This is a Term-Vector based highlighter for Apache Lucene Java
-  </description>
-  <packaging>jar</packaging>
-  <dependencies>
-    <dependency>
-      <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-analyzers</artifactId>
-      <version>@version@</version>
-    </dependency>
-  </dependencies>
-</project>
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java
deleted file mode 100644
index 9b22433..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java
+++ /dev/null
@@ -1,154 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.MapFieldSelector;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.vectorhighlight.FieldFragList.WeightedFragInfo;
-import org.apache.lucene.search.vectorhighlight.FieldFragList.WeightedFragInfo.SubInfo;
-import org.apache.lucene.search.vectorhighlight.FieldPhraseList.WeightedPhraseInfo.Toffs;
-
-public abstract class BaseFragmentsBuilder implements FragmentsBuilder {
-
-  protected String[] preTags, postTags;
-  public static final String[] COLORED_PRE_TAGS = {
-    "<b style=\"background:yellow\">", "<b style=\"background:lawngreen\">", "<b style=\"background:aquamarine\">",
-    "<b style=\"background:magenta\">", "<b style=\"background:palegreen\">", "<b style=\"background:coral\">",
-    "<b style=\"background:wheat\">", "<b style=\"background:khaki\">", "<b style=\"background:lime\">",
-    "<b style=\"background:deepskyblue\">"
-  };
-  public static final String[] COLORED_POST_TAGS = { "</b>" };
-  
-  protected BaseFragmentsBuilder(){
-    this( new String[]{ "<b>" }, new String[]{ "</b>" } );
-  }
-  
-  protected BaseFragmentsBuilder( String[] preTags, String[] postTags ){
-    this.preTags = preTags;
-    this.postTags = postTags;
-  }
-  
-  static Object checkTagsArgument( Object tags ){
-    if( tags instanceof String ) return tags;
-    else if( tags instanceof String[] ) return tags;
-    throw new IllegalArgumentException( "type of preTags/postTags must be a String or String[]" );
-  }
-  
-  public abstract List<WeightedFragInfo> getWeightedFragInfoList( List<WeightedFragInfo> src );
-  
-  public String createFragment( IndexReader reader, int docId,
-      String fieldName, FieldFragList fieldFragList ) throws IOException {
-    String[] fragments = createFragments( reader, docId, fieldName, fieldFragList, 1 );
-    if( fragments == null || fragments.length == 0 ) return null;
-    return fragments[0];
-  }
-
-  public String[] createFragments( IndexReader reader, int docId,
-      String fieldName, FieldFragList fieldFragList, int maxNumFragments )
-      throws IOException {
-    if( maxNumFragments < 0 )
-      throw new IllegalArgumentException( "maxNumFragments(" + maxNumFragments + ") must be positive number." );
-
-    List<WeightedFragInfo> fragInfos = getWeightedFragInfoList( fieldFragList.fragInfos );
-    
-    List<String> fragments = new ArrayList<String>( maxNumFragments );
-    Field[] values = getFields( reader, docId, fieldName );
-    if( values.length == 0 ) return null;
-    StringBuilder buffer = new StringBuilder();
-    int[] nextValueIndex = { 0 };
-    for( int n = 0; n < maxNumFragments && n < fragInfos.size(); n++ ){
-      WeightedFragInfo fragInfo = fragInfos.get( n );
-      fragments.add( makeFragment( buffer, nextValueIndex, values, fragInfo ) );
-    }
-    return fragments.toArray( new String[fragments.size()] );
-  }
-  
-  @Deprecated
-  protected String[] getFieldValues( IndexReader reader, int docId, String fieldName) throws IOException {
-    Document doc = reader.document( docId, new MapFieldSelector( new String[]{ fieldName } ) );
-    return doc.getValues( fieldName ); // according to Document class javadoc, this never returns null
-  }
-  
-  protected Field[] getFields( IndexReader reader, int docId, String fieldName) throws IOException {
-    // according to javadoc, doc.getFields(fieldName) cannot be used with lazy loaded field???
-    Document doc = reader.document( docId, new MapFieldSelector( new String[]{ fieldName } ) );
-    return doc.getFields( fieldName ); // according to Document class javadoc, this never returns null
-  }
-
-  @Deprecated
-  protected String makeFragment( StringBuilder buffer, int[] index, String[] values, WeightedFragInfo fragInfo ){
-    final int s = fragInfo.startOffset;
-    return makeFragment( fragInfo, getFragmentSource( buffer, index, values, s, fragInfo.endOffset ), s );
-  }
-
-  protected String makeFragment( StringBuilder buffer, int[] index, Field[] values, WeightedFragInfo fragInfo ){
-    final int s = fragInfo.startOffset;
-    return makeFragment( fragInfo, getFragmentSource( buffer, index, values, s, fragInfo.endOffset ), s );
-  }
-  
-  private String makeFragment( WeightedFragInfo fragInfo, String src, int s ){
-    StringBuilder fragment = new StringBuilder();
-    int srcIndex = 0;
-    for( SubInfo subInfo : fragInfo.subInfos ){
-      for( Toffs to : subInfo.termsOffsets ){
-        fragment.append( src.substring( srcIndex, to.startOffset - s ) ).append( getPreTag( subInfo.seqnum ) )
-          .append( src.substring( to.startOffset - s, to.endOffset - s ) ).append( getPostTag( subInfo.seqnum ) );
-        srcIndex = to.endOffset - s;
-      }
-    }
-    fragment.append( src.substring( srcIndex ) );
-    return fragment.toString();
-  }
-  
-  @Deprecated
-  protected String getFragmentSource( StringBuilder buffer, int[] index, String[] values,
-      int startOffset, int endOffset ){
-    while( buffer.length() < endOffset && index[0] < values.length ){
-      if( index[0] > 0 && values[index[0]].length() > 0 )
-        buffer.append( ' ' );
-      buffer.append( values[index[0]++] );
-    }
-    int eo = buffer.length() < endOffset ? buffer.length() : endOffset;
-    return buffer.substring( startOffset, eo );
-  }
-
-  protected String getFragmentSource( StringBuilder buffer, int[] index, Field[] values,
-      int startOffset, int endOffset ){
-    while( buffer.length() < endOffset && index[0] < values.length ){
-      if( index[0] > 0 && values[index[0]].isTokenized() && values[index[0]].stringValue().length() > 0 )
-        buffer.append( ' ' );
-      buffer.append( values[index[0]++].stringValue() );
-    }
-    int eo = buffer.length() < endOffset ? buffer.length() : endOffset;
-    return buffer.substring( startOffset, eo );
-  }
-  
-  protected String getPreTag( int num ){
-    return preTags.length > num ? preTags[num] : preTags[0];
-  }
-  
-  protected String getPostTag( int num ){
-    return postTags.length > num ? postTags[num] : postTags[0];
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FastVectorHighlighter.java b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FastVectorHighlighter.java
deleted file mode 100644
index 2df78b3..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FastVectorHighlighter.java
+++ /dev/null
@@ -1,137 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.Query;
-
-/**
- * Another highlighter implementation.
- *
- */
-public class FastVectorHighlighter {
-
-  public static final boolean DEFAULT_PHRASE_HIGHLIGHT = true;
-  public static final boolean DEFAULT_FIELD_MATCH = true;
-  private final boolean phraseHighlight;
-  private final boolean fieldMatch;
-  private final FragListBuilder fragListBuilder;
-  private final FragmentsBuilder fragmentsBuilder;
-
-  /**
-   * the default constructor.
-   */
-  public FastVectorHighlighter(){
-    this( DEFAULT_PHRASE_HIGHLIGHT, DEFAULT_FIELD_MATCH );
-  }
-
-  /**
-   * a constructor. Using SimpleFragListBuilder and ScoreOrderFragmentsBuilder.
-   * 
-   * @param phraseHighlight true or false for phrase highlighting
-   * @param fieldMatch true of false for field matching
-   */
-  public FastVectorHighlighter( boolean phraseHighlight, boolean fieldMatch ){
-    this( phraseHighlight, fieldMatch, new SimpleFragListBuilder(), new ScoreOrderFragmentsBuilder() );
-  }
-
-  /**
-   * a constructor. A FragListBuilder and a FragmentsBuilder can be specified (plugins).
-   * 
-   * @param phraseHighlight true of false for phrase highlighting
-   * @param fieldMatch true of false for field matching
-   * @param fragListBuilder an instance of FragListBuilder
-   * @param fragmentsBuilder an instance of FragmentsBuilder
-   */
-  public FastVectorHighlighter( boolean phraseHighlight, boolean fieldMatch,
-      FragListBuilder fragListBuilder, FragmentsBuilder fragmentsBuilder ){
-    this.phraseHighlight = phraseHighlight;
-    this.fieldMatch = fieldMatch;
-    this.fragListBuilder = fragListBuilder;
-    this.fragmentsBuilder = fragmentsBuilder;
-  }
-
-  /**
-   * create a FieldQuery object.
-   * 
-   * @param query a query
-   * @return the created FieldQuery object
-   */
-  public FieldQuery getFieldQuery( Query query ){
-    return new FieldQuery( query, phraseHighlight, fieldMatch );
-  }
-
-  /**
-   * return the best fragment.
-   * 
-   * @param fieldQuery FieldQuery object
-   * @param reader IndexReader of the index
-   * @param docId document id to be highlighted
-   * @param fieldName field of the document to be highlighted
-   * @param fragCharSize the length (number of chars) of a fragment
-   * @return the best fragment (snippet) string
-   * @throws IOException
-   */
-  public final String getBestFragment( final FieldQuery fieldQuery, IndexReader reader, int docId,
-      String fieldName, int fragCharSize ) throws IOException {
-    FieldFragList fieldFragList = getFieldFragList( fieldQuery, reader, docId, fieldName, fragCharSize );
-    return fragmentsBuilder.createFragment( reader, docId, fieldName, fieldFragList );
-  }
-
-  /**
-   * return the best fragments.
-   * 
-   * @param fieldQuery FieldQuery object
-   * @param reader IndexReader of the index
-   * @param docId document id to be highlighted
-   * @param fieldName field of the document to be highlighted
-   * @param fragCharSize the length (number of chars) of a fragment
-   * @param maxNumFragments maximum number of fragments
-   * @return created fragments or null when no fragments created.
-   *         size of the array can be less than maxNumFragments
-   * @throws IOException
-   */
-  public final String[] getBestFragments( final FieldQuery fieldQuery, IndexReader reader, int docId,
-      String fieldName, int fragCharSize, int maxNumFragments ) throws IOException {
-    FieldFragList fieldFragList = getFieldFragList( fieldQuery, reader, docId, fieldName, fragCharSize );
-    return fragmentsBuilder.createFragments( reader, docId, fieldName, fieldFragList, maxNumFragments );
-  }
-  
-  private FieldFragList getFieldFragList( final FieldQuery fieldQuery, IndexReader reader, int docId,
-      String fieldName, int fragCharSize ) throws IOException {
-    FieldTermStack fieldTermStack = new FieldTermStack( reader, docId, fieldName, fieldQuery );
-    FieldPhraseList fieldPhraseList = new FieldPhraseList( fieldTermStack, fieldQuery );
-    return fragListBuilder.createFieldFragList( fieldPhraseList, fragCharSize );
-  }
-
-  /**
-   * return whether phraseHighlight or not.
-   * 
-   * @return whether phraseHighlight or not
-   */
-  public boolean isPhraseHighlight(){ return phraseHighlight; }
-
-  /**
-   * return whether fieldMatch or not.
-   * 
-   * @return whether fieldMatch or not
-   */
-  public boolean isFieldMatch(){ return fieldMatch; }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldFragList.java b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldFragList.java
deleted file mode 100644
index 6fdf435..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldFragList.java
+++ /dev/null
@@ -1,128 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.search.vectorhighlight.FieldPhraseList.WeightedPhraseInfo;
-import org.apache.lucene.search.vectorhighlight.FieldPhraseList.WeightedPhraseInfo.Toffs;
-
-/**
- * FieldFragList has a list of "frag info" that is used by FragmentsBuilder class
- * to create fragments (snippets).
- */
-public class FieldFragList {
-
-  List<WeightedFragInfo> fragInfos = new ArrayList<WeightedFragInfo>();
-
-  /**
-   * a constructor.
-   * 
-   * @param fragCharSize the length (number of chars) of a fragment
-   */
-  public FieldFragList( int fragCharSize ){
-  }
-
-  /**
-   * convert the list of WeightedPhraseInfo to WeightedFragInfo, then add it to the fragInfos
-   * 
-   * @param startOffset start offset of the fragment
-   * @param endOffset end offset of the fragment
-   * @param phraseInfoList list of WeightedPhraseInfo objects
-   */
-  public void add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ){
-    fragInfos.add( new WeightedFragInfo( startOffset, endOffset, phraseInfoList ) );
-  }
-  
-  public static class WeightedFragInfo {
-
-    List<SubInfo> subInfos;
-    float totalBoost;
-    int startOffset;
-    int endOffset;
-
-    public WeightedFragInfo( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ){
-      this.startOffset = startOffset;
-      this.endOffset = endOffset;
-      subInfos = new ArrayList<SubInfo>();
-      for( WeightedPhraseInfo phraseInfo : phraseInfoList ){
-        SubInfo subInfo = new SubInfo( phraseInfo.text, phraseInfo.termsOffsets, phraseInfo.seqnum );
-        subInfos.add( subInfo );
-        totalBoost += phraseInfo.boost;
-      }
-    }
-    
-    public List<SubInfo> getSubInfos(){
-      return subInfos;
-    }
-    
-    public float getTotalBoost(){
-      return totalBoost;
-    }
-    
-    public int getStartOffset(){
-      return startOffset;
-    }
-    
-    public int getEndOffset(){
-      return endOffset;
-    }
-    
-    @Override
-    public String toString(){
-      StringBuilder sb = new StringBuilder();
-      sb.append( "subInfos=(" );
-      for( SubInfo si : subInfos )
-        sb.append( si.toString() );
-      sb.append( ")/" ).append( totalBoost ).append( '(' ).append( startOffset ).append( ',' ).append( endOffset ).append( ')' );
-      return sb.toString();
-    }
-    
-    public static class SubInfo {
-      final String text;  // unnecessary member, just exists for debugging purpose
-      final List<Toffs> termsOffsets;   // usually termsOffsets.size() == 1,
-                              // but if position-gap > 1 and slop > 0 then size() could be greater than 1
-      int seqnum;
-
-      SubInfo( String text, List<Toffs> termsOffsets, int seqnum ){
-        this.text = text;
-        this.termsOffsets = termsOffsets;
-        this.seqnum = seqnum;
-      }
-      
-      public List<Toffs> getTermsOffsets(){
-        return termsOffsets;
-      }
-      
-      public int getSeqnum(){
-        return seqnum;
-      }
-      
-      @Override
-      public String toString(){
-        StringBuilder sb = new StringBuilder();
-        sb.append( text ).append( '(' );
-        for( Toffs to : termsOffsets )
-          sb.append( to.toString() );
-        sb.append( ')' );
-        return sb.toString();
-      }
-    }
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java
deleted file mode 100644
index 15ae634..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java
+++ /dev/null
@@ -1,191 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.LinkedList;
-import java.util.List;
-
-import org.apache.lucene.search.vectorhighlight.FieldQuery.QueryPhraseMap;
-import org.apache.lucene.search.vectorhighlight.FieldTermStack.TermInfo;
-
-/**
- * FieldPhraseList has a list of WeightedPhraseInfo that is used by FragListBuilder
- * to create a FieldFragList object.
- */
-public class FieldPhraseList {
-
-  LinkedList<WeightedPhraseInfo> phraseList = new LinkedList<WeightedPhraseInfo>();
-
-  /**
-   * a constructor.
-   * 
-   * @param fieldTermStack FieldTermStack object
-   * @param fieldQuery FieldQuery object
-   */
-  public FieldPhraseList( FieldTermStack fieldTermStack, FieldQuery fieldQuery ){
-    final String field = fieldTermStack.getFieldName();
-
-    LinkedList<TermInfo> phraseCandidate = new LinkedList<TermInfo>();
-    QueryPhraseMap currMap = null;
-    QueryPhraseMap nextMap = null;
-    while( !fieldTermStack.isEmpty() ){
-      
-      phraseCandidate.clear();
-
-      TermInfo ti = fieldTermStack.pop();
-      currMap = fieldQuery.getFieldTermMap( field, ti.getText() );
-
-      // if not found, discard top TermInfo from stack, then try next element
-      if( currMap == null ) continue;
-      
-      // if found, search the longest phrase
-      phraseCandidate.add( ti );
-      while( true ){
-        ti = fieldTermStack.pop();
-        nextMap = null;
-        if( ti != null )
-          nextMap = currMap.getTermMap( ti.getText() );
-        if( ti == null || nextMap == null ){
-          if( ti != null )
-            fieldTermStack.push( ti );
-          if( currMap.isValidTermOrPhrase( phraseCandidate ) ){
-            addIfNoOverlap( new WeightedPhraseInfo( phraseCandidate, currMap.getBoost(), currMap.getTermOrPhraseNumber() ) );
-          }
-          else{
-            while( phraseCandidate.size() > 1 ){
-              fieldTermStack.push( phraseCandidate.removeLast() );
-              currMap = fieldQuery.searchPhrase( field, phraseCandidate );
-              if( currMap != null ){
-                addIfNoOverlap( new WeightedPhraseInfo( phraseCandidate, currMap.getBoost(), currMap.getTermOrPhraseNumber() ) );
-                break;
-              }
-            }
-          }
-          break;
-        }
-        else{
-          phraseCandidate.add( ti );
-          currMap = nextMap;
-        }
-      }
-    }
-  }
-  
-  void addIfNoOverlap( WeightedPhraseInfo wpi ){
-    for( WeightedPhraseInfo existWpi : phraseList ){
-      if( existWpi.isOffsetOverlap( wpi ) ) return;
-    }
-    phraseList.add( wpi );
-  }
-  
-  public static class WeightedPhraseInfo {
-
-    String text;  // unnecessary member, just exists for debugging purpose
-    List<Toffs> termsOffsets;   // usually termsOffsets.size() == 1,
-                            // but if position-gap > 1 and slop > 0 then size() could be greater than 1
-    float boost;  // query boost
-    int seqnum;
-    
-    public WeightedPhraseInfo( LinkedList<TermInfo> terms, float boost ){
-      this( terms, boost, 0 );
-    }
-    
-    public WeightedPhraseInfo( LinkedList<TermInfo> terms, float boost, int number ){
-      this.boost = boost;
-      this.seqnum = number;
-      termsOffsets = new ArrayList<Toffs>( terms.size() );
-      TermInfo ti = terms.get( 0 );
-      termsOffsets.add( new Toffs( ti.getStartOffset(), ti.getEndOffset() ) );
-      if( terms.size() == 1 ){
-        text = ti.getText();
-        return;
-      }
-      StringBuilder sb = new StringBuilder();
-      sb.append( ti.getText() );
-      int pos = ti.getPosition();
-      for( int i = 1; i < terms.size(); i++ ){
-        ti = terms.get( i );
-        sb.append( ti.getText() );
-        if( ti.getPosition() - pos == 1 ){
-          Toffs to = termsOffsets.get( termsOffsets.size() - 1 );
-          to.setEndOffset( ti.getEndOffset() );
-        }
-        else{
-          termsOffsets.add( new Toffs( ti.getStartOffset(), ti.getEndOffset() ) );
-        }
-        pos = ti.getPosition();
-      }
-      text = sb.toString();
-    }
-    
-    public int getStartOffset(){
-      return termsOffsets.get( 0 ).startOffset;
-    }
-    
-    public int getEndOffset(){
-      return termsOffsets.get( termsOffsets.size() - 1 ).endOffset;
-    }
-    
-    public boolean isOffsetOverlap( WeightedPhraseInfo other ){
-      int so = getStartOffset();
-      int eo = getEndOffset();
-      int oso = other.getStartOffset();
-      int oeo = other.getEndOffset();
-      if( so <= oso && oso < eo ) return true;
-      if( so < oeo && oeo <= eo ) return true;
-      if( oso <= so && so < oeo ) return true;
-      if( oso < eo && eo <= oeo ) return true;
-      return false;
-    }
-    
-    @Override
-    public String toString(){
-      StringBuilder sb = new StringBuilder();
-      sb.append( text ).append( '(' ).append( boost ).append( ")(" );
-      for( Toffs to : termsOffsets ){
-        sb.append( to );
-      }
-      sb.append( ')' );
-      return sb.toString();
-    }
-    
-    public static class Toffs {
-      int startOffset;
-      int endOffset;
-      public Toffs( int startOffset, int endOffset ){
-        this.startOffset = startOffset;
-        this.endOffset = endOffset;
-      }
-      public void setEndOffset( int endOffset ){
-        this.endOffset = endOffset;
-      }
-      public int getStartOffset(){
-        return startOffset;
-      }
-      public int getEndOffset(){
-        return endOffset;
-      }
-      @Override
-      public String toString(){
-        StringBuilder sb = new StringBuilder();
-        sb.append( '(' ).append( startOffset ).append( ',' ).append( endOffset ).append( ')' );
-        return sb.toString();
-      }
-    }
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java
deleted file mode 100644
index 01ecaf8..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java
+++ /dev/null
@@ -1,399 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.vectorhighlight.FieldTermStack.TermInfo;
-
-/**
- * FieldQuery breaks down query object into terms/phrases and keep
- * them in QueryPhraseMap structure.
- */
-public class FieldQuery {
-
-  final boolean fieldMatch;
-
-  // fieldMatch==true,  Map<fieldName,QueryPhraseMap>
-  // fieldMatch==false, Map<null,QueryPhraseMap>
-  Map<String, QueryPhraseMap> rootMaps = new HashMap<String, QueryPhraseMap>();
-
-  // fieldMatch==true,  Map<fieldName,setOfTermsInQueries>
-  // fieldMatch==false, Map<null,setOfTermsInQueries>
-  Map<String, Set<String>> termSetMap = new HashMap<String, Set<String>>();
-
-  int termOrPhraseNumber; // used for colored tag support
-
-  FieldQuery( Query query, boolean phraseHighlight, boolean fieldMatch ){
-    this.fieldMatch = fieldMatch;
-    Set<Query> flatQueries = new HashSet<Query>();
-    flatten( query, flatQueries );
-    saveTerms( flatQueries );
-    Collection<Query> expandQueries = expand( flatQueries );
-
-    for( Query flatQuery : expandQueries ){
-      QueryPhraseMap rootMap = getRootMap( flatQuery );
-      rootMap.add( flatQuery );
-      if( !phraseHighlight && flatQuery instanceof PhraseQuery ){
-        PhraseQuery pq = (PhraseQuery)flatQuery;
-        if( pq.getTerms().length > 1 ){
-          for( Term term : pq.getTerms() )
-            rootMap.addTerm( term, flatQuery.getBoost() );
-        }
-      }
-    }
-  }
-  
-  void flatten( Query sourceQuery, Collection<Query> flatQueries ){
-    if( sourceQuery instanceof BooleanQuery ){
-      BooleanQuery bq = (BooleanQuery)sourceQuery;
-      for( BooleanClause clause : bq.getClauses() ){
-        if( !clause.isProhibited() )
-          flatten( clause.getQuery(), flatQueries );
-      }
-    }
-    else if( sourceQuery instanceof DisjunctionMaxQuery ){
-      DisjunctionMaxQuery dmq = (DisjunctionMaxQuery)sourceQuery;
-      for( Query query : dmq ){
-        flatten( query, flatQueries );
-      }
-    }
-    else if( sourceQuery instanceof TermQuery ){
-      if( !flatQueries.contains( sourceQuery ) )
-        flatQueries.add( sourceQuery );
-    }
-    else if( sourceQuery instanceof PhraseQuery ){
-      if( !flatQueries.contains( sourceQuery ) ){
-        PhraseQuery pq = (PhraseQuery)sourceQuery;
-        if( pq.getTerms().length > 1 )
-          flatQueries.add( pq );
-        else if( pq.getTerms().length == 1 ){
-          flatQueries.add( new TermQuery( pq.getTerms()[0] ) );
-        }
-      }
-    }
-    // else discard queries
-  }
-  
-  /*
-   * Create expandQueries from flatQueries.
-   * 
-   * expandQueries := flatQueries + overlapped phrase queries
-   * 
-   * ex1) flatQueries={a,b,c}
-   *      => expandQueries={a,b,c}
-   * ex2) flatQueries={a,"b c","c d"}
-   *      => expandQueries={a,"b c","c d","b c d"}
-   */
-  Collection<Query> expand( Collection<Query> flatQueries ){
-    Set<Query> expandQueries = new HashSet<Query>();
-    for( Iterator<Query> i = flatQueries.iterator(); i.hasNext(); ){
-      Query query = i.next();
-      i.remove();
-      expandQueries.add( query );
-      if( !( query instanceof PhraseQuery ) ) continue;
-      for( Iterator<Query> j = flatQueries.iterator(); j.hasNext(); ){
-        Query qj = j.next();
-        if( !( qj instanceof PhraseQuery ) ) continue;
-        checkOverlap( expandQueries, (PhraseQuery)query, (PhraseQuery)qj );
-      }
-    }
-    return expandQueries;
-  }
-
-  /*
-   * Check if PhraseQuery A and B have overlapped part.
-   * 
-   * ex1) A="a b", B="b c" => overlap; expandQueries={"a b c"}
-   * ex2) A="b c", B="a b" => overlap; expandQueries={"a b c"}
-   * ex3) A="a b", B="c d" => no overlap; expandQueries={}
-   */
-  private void checkOverlap( Collection<Query> expandQueries, PhraseQuery a, PhraseQuery b ){
-    if( a.getSlop() != b.getSlop() ) return;
-    Term[] ats = a.getTerms();
-    Term[] bts = b.getTerms();
-    if( fieldMatch && !ats[0].field().equals( bts[0].field() ) ) return;
-    checkOverlap( expandQueries, ats, bts, a.getSlop(), a.getBoost() );
-    checkOverlap( expandQueries, bts, ats, b.getSlop(), b.getBoost() );
-  }
-
-  /*
-   * Check if src and dest have overlapped part and if it is, create PhraseQueries and add expandQueries.
-   * 
-   * ex1) src="a b", dest="c d"       => no overlap
-   * ex2) src="a b", dest="a b c"     => no overlap
-   * ex3) src="a b", dest="b c"       => overlap; expandQueries={"a b c"}
-   * ex4) src="a b c", dest="b c d"   => overlap; expandQueries={"a b c d"}
-   * ex5) src="a b c", dest="b c"     => no overlap
-   * ex6) src="a b c", dest="b"       => no overlap
-   * ex7) src="a a a a", dest="a a a" => overlap;
-   *                                     expandQueries={"a a a a a","a a a a a a"}
-   * ex8) src="a b c d", dest="b c"   => no overlap
-   */
-  private void checkOverlap( Collection<Query> expandQueries, Term[] src, Term[] dest, int slop, float boost ){
-    // beginning from 1 (not 0) is safe because that the PhraseQuery has multiple terms
-    // is guaranteed in flatten() method (if PhraseQuery has only one term, flatten()
-    // converts PhraseQuery to TermQuery)
-    for( int i = 1; i < src.length; i++ ){
-      boolean overlap = true;
-      for( int j = i; j < src.length; j++ ){
-        if( ( j - i ) < dest.length && !src[j].text().equals( dest[j-i].text() ) ){
-          overlap = false;
-          break;
-        }
-      }
-      if( overlap && src.length - i < dest.length ){
-        PhraseQuery pq = new PhraseQuery();
-        for( Term srcTerm : src )
-          pq.add( srcTerm );
-        for( int k = src.length - i; k < dest.length; k++ ){
-          pq.add( new Term( src[0].field(), dest[k].text() ) );
-        }
-        pq.setSlop( slop );
-        pq.setBoost( boost );
-        if(!expandQueries.contains( pq ) )
-          expandQueries.add( pq );
-      }
-    }
-  }
-  
-  QueryPhraseMap getRootMap( Query query ){
-    String key = getKey( query );
-    QueryPhraseMap map = rootMaps.get( key );
-    if( map == null ){
-      map = new QueryPhraseMap( this );
-      rootMaps.put( key, map );
-    }
-    return map;
-  }
-  
-  /*
-   * Return 'key' string. 'key' is the field name of the Query.
-   * If not fieldMatch, 'key' will be null.
-   */
-  private String getKey( Query query ){
-    if( !fieldMatch ) return null;
-    if( query instanceof TermQuery )
-      return ((TermQuery)query).getTerm().field();
-    else if ( query instanceof PhraseQuery ){
-      PhraseQuery pq = (PhraseQuery)query;
-      Term[] terms = pq.getTerms();
-      return terms[0].field();
-    }
-    else
-      throw new RuntimeException( "query \"" + query.toString() + "\" must be flatten first." );
-  }
-
-  /*
-   * Save the set of terms in the queries to termSetMap.
-   * 
-   * ex1) q=name:john
-   *      - fieldMatch==true
-   *          termSetMap=Map<"name",Set<"john">>
-   *      - fieldMatch==false
-   *          termSetMap=Map<null,Set<"john">>
-   *          
-   * ex2) q=name:john title:manager
-   *      - fieldMatch==true
-   *          termSetMap=Map<"name",Set<"john">,
-   *                         "title",Set<"manager">>
-   *      - fieldMatch==false
-   *          termSetMap=Map<null,Set<"john","manager">>
-   *          
-   * ex3) q=name:"john lennon"
-   *      - fieldMatch==true
-   *          termSetMap=Map<"name",Set<"john","lennon">>
-   *      - fieldMatch==false
-   *          termSetMap=Map<null,Set<"john","lennon">>
-   */
-  void saveTerms( Collection<Query> flatQueries ){
-    for( Query query : flatQueries ){
-      Set<String> termSet = getTermSet( query );
-      if( query instanceof TermQuery )
-        termSet.add( ((TermQuery)query).getTerm().text() );
-      else if( query instanceof PhraseQuery ){
-        for( Term term : ((PhraseQuery)query).getTerms() )
-          termSet.add( term.text() );
-      }
-      else
-        throw new RuntimeException( "query \"" + query.toString() + "\" must be flatten first." );
-    }
-  }
-  
-  private Set<String> getTermSet( Query query ){
-    String key = getKey( query );
-    Set<String> set = termSetMap.get( key );
-    if( set == null ){
-      set = new HashSet<String>();
-      termSetMap.put( key, set );
-    }
-    return set;
-  }
-  
-  Set<String> getTermSet( String field ){
-    return termSetMap.get( fieldMatch ? field : null );
-  }
-
-  /**
-   * 
-   * @param fieldName
-   * @param term
-   * @return QueryPhraseMap
-   */
-  public QueryPhraseMap getFieldTermMap( String fieldName, String term ){
-    QueryPhraseMap rootMap = getRootMap( fieldName );
-    return rootMap == null ? null : rootMap.subMap.get( term );
-  }
-
-  /**
-   * 
-   * @param fieldName
-   * @param phraseCandidate
-   * @return QueryPhraseMap
-   */
-  public QueryPhraseMap searchPhrase( String fieldName, final List<TermInfo> phraseCandidate ){
-    QueryPhraseMap root = getRootMap( fieldName );
-    if( root == null ) return null;
-    return root.searchPhrase( phraseCandidate );
-  }
-  
-  private QueryPhraseMap getRootMap( String fieldName ){
-    return rootMaps.get( fieldMatch ? fieldName : null );
-  }
-  
-  int nextTermOrPhraseNumber(){
-    return termOrPhraseNumber++;
-  }
-  
-  public static class QueryPhraseMap {
-
-    boolean terminal;
-    int slop;   // valid if terminal == true and phraseHighlight == true
-    float boost;  // valid if terminal == true
-    int termOrPhraseNumber;   // valid if terminal == true
-    FieldQuery fieldQuery;
-    Map<String, QueryPhraseMap> subMap = new HashMap<String, QueryPhraseMap>();
-    
-    public QueryPhraseMap( FieldQuery fieldQuery ){
-      this.fieldQuery = fieldQuery;
-    }
-
-    void addTerm( Term term, float boost ){
-      QueryPhraseMap map = getOrNewMap( subMap, term.text() );
-      map.markTerminal( boost );
-    }
-    
-    private QueryPhraseMap getOrNewMap( Map<String, QueryPhraseMap> subMap, String term ){
-      QueryPhraseMap map = subMap.get( term );
-      if( map == null ){
-        map = new QueryPhraseMap( fieldQuery );
-        subMap.put( term, map );
-      }
-      return map;
-    }
-
-    void add( Query query ){
-      if( query instanceof TermQuery ){
-        addTerm( ((TermQuery)query).getTerm(), query.getBoost() );
-      }
-      else if( query instanceof PhraseQuery ){
-        PhraseQuery pq = (PhraseQuery)query;
-        Term[] terms = pq.getTerms();
-        Map<String, QueryPhraseMap> map = subMap;
-        QueryPhraseMap qpm = null;
-        for( Term term : terms ){
-          qpm = getOrNewMap( map, term.text() );
-          map = qpm.subMap;
-        }
-        qpm.markTerminal( pq.getSlop(), pq.getBoost() );
-      }
-      else
-        throw new RuntimeException( "query \"" + query.toString() + "\" must be flatten first." );
-    }
-    
-    public QueryPhraseMap getTermMap( String term ){
-      return subMap.get( term );
-    }
-    
-    private void markTerminal( float boost ){
-      markTerminal( 0, boost );
-    }
-    
-    private void markTerminal( int slop, float boost ){
-      this.terminal = true;
-      this.slop = slop;
-      this.boost = boost;
-      this.termOrPhraseNumber = fieldQuery.nextTermOrPhraseNumber();
-    }
-    
-    public boolean isTerminal(){
-      return terminal;
-    }
-    
-    public int getSlop(){
-      return slop;
-    }
-    
-    public float getBoost(){
-      return boost;
-    }
-    
-    public int getTermOrPhraseNumber(){
-      return termOrPhraseNumber;
-    }
-    
-    public QueryPhraseMap searchPhrase( final List<TermInfo> phraseCandidate ){
-      QueryPhraseMap currMap = this;
-      for( TermInfo ti : phraseCandidate ){
-        currMap = currMap.subMap.get( ti.getText() );
-        if( currMap == null ) return null;
-      }
-      return currMap.isValidTermOrPhrase( phraseCandidate ) ? currMap : null;
-    }
-    
-    public boolean isValidTermOrPhrase( final List<TermInfo> phraseCandidate ){
-      // check terminal
-      if( !terminal ) return false;
-
-      // if the candidate is a term, it is valid
-      if( phraseCandidate.size() == 1 ) return true;
-
-      // else check whether the candidate is valid phrase
-      // compare position-gaps between terms to slop
-      int pos = phraseCandidate.get( 0 ).getPosition();
-      for( int i = 1; i < phraseCandidate.size(); i++ ){
-        int nextPos = phraseCandidate.get( i ).getPosition();
-        if( Math.abs( nextPos - pos - 1 ) > slop ) return false;
-        pos = nextPos;
-      }
-      return true;
-    }
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
deleted file mode 100644
index 86ca670..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
+++ /dev/null
@@ -1,173 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.LinkedList;
-import java.util.Set;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Field.Index;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.Field.TermVector;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.TermFreqVector;
-import org.apache.lucene.index.TermPositionVector;
-import org.apache.lucene.index.TermVectorOffsetInfo;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.Version;
-
-/**
- * <code>FieldTermStack</code> is a stack that keeps query terms in the specified field
- * of the document to be highlighted.
- */
-public class FieldTermStack {
-  
-  private final String fieldName;
-  LinkedList<TermInfo> termList = new LinkedList<TermInfo>();
-  
-  public static void main( String[] args ) throws Exception {
-    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
-    QueryParser parser = new QueryParser(Version.LUCENE_CURRENT,  "f", analyzer );
-    Query query = parser.parse( "a x:b" );
-    FieldQuery fieldQuery = new FieldQuery( query, true, false );
-    
-    Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_CURRENT, analyzer));
-    Document doc = new Document();
-    doc.add( new Field( "f", "a a a b b c a b b c d e f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
-    doc.add( new Field( "f", "b a b a f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
-    writer.addDocument( doc );
-    writer.close();
-    
-    IndexReader reader = IndexReader.open( dir, true );
-    new FieldTermStack( reader, 0, "f", fieldQuery );
-    reader.close();
-  }
-
-  /**
-   * a constructor.
-   * 
-   * @param reader IndexReader of the index
-   * @param docId document id to be highlighted
-   * @param fieldName field of the document to be highlighted
-   * @param fieldQuery FieldQuery object
-   * @throws IOException
-   */
-  public FieldTermStack( IndexReader reader, int docId, String fieldName, final FieldQuery fieldQuery ) throws IOException {
-    this.fieldName = fieldName;
-
-    TermFreqVector tfv = reader.getTermFreqVector( docId, fieldName );
-    if( tfv == null ) return; // just return to make null snippets
-    TermPositionVector tpv = null;
-    try{
-      tpv = (TermPositionVector)tfv;
-    }
-    catch( ClassCastException e ){
-      return; // just return to make null snippets
-    }
-    
-    Set<String> termSet = fieldQuery.getTermSet( fieldName );
-    // just return to make null snippet if un-matched fieldName specified when fieldMatch == true
-    if( termSet == null ) return;
-    
-    for( String term : tpv.getTerms() ){
-      if( !termSet.contains( term ) ) continue;
-      int index = tpv.indexOf( term );
-      TermVectorOffsetInfo[] tvois = tpv.getOffsets( index );
-      if( tvois == null ) return; // just return to make null snippets
-      int[] poss = tpv.getTermPositions( index );
-      if( poss == null ) return; // just return to make null snippets
-      for( int i = 0; i < tvois.length; i++ )
-        termList.add( new TermInfo( term, tvois[i].getStartOffset(), tvois[i].getEndOffset(), poss[i] ) );
-    }
-    
-    // sort by position
-    Collections.sort( termList );
-  }
-
-  /**
-   * @return field name
-   */
-  public String getFieldName(){
-    return fieldName;
-  }
-
-  /**
-   * @return the top TermInfo object of the stack
-   */
-  public TermInfo pop(){
-    return termList.poll();
-  }
-
-  /**
-   * @param termInfo the TermInfo object to be put on the top of the stack
-   */
-  public void push( TermInfo termInfo ){
-    // termList.push( termInfo );  // avoid Java 1.6 feature
-    termList.addFirst( termInfo );
-  }
-
-  /**
-   * to know whether the stack is empty
-   * 
-   * @return true if the stack is empty, false if not
-   */
-  public boolean isEmpty(){
-    return termList == null || termList.size() == 0;
-  }
-  
-  public static class TermInfo implements Comparable<TermInfo>{
-
-    final String text;
-    final int startOffset;
-    final int endOffset;
-    final int position;
-
-    TermInfo( String text, int startOffset, int endOffset, int position ){
-      this.text = text;
-      this.startOffset = startOffset;
-      this.endOffset = endOffset;
-      this.position = position;
-    }
-    
-    public String getText(){ return text; }
-    public int getStartOffset(){ return startOffset; }
-    public int getEndOffset(){ return endOffset; }
-    public int getPosition(){ return position; }
-    
-    @Override
-    public String toString(){
-      StringBuilder sb = new StringBuilder();
-      sb.append( text ).append( '(' ).append(startOffset).append( ',' ).append( endOffset ).append( ',' ).append( position ).append( ')' );
-      return sb.toString();
-    }
-
-    public int compareTo( TermInfo o ) {
-      return ( this.position - o.position );
-    }
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragListBuilder.java b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragListBuilder.java
deleted file mode 100644
index 0ed1d3e..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragListBuilder.java
+++ /dev/null
@@ -1,34 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * FragListBuilder is an interface for FieldFragList builder classes.
- * A FragListBuilder class can be plugged in to Highlighter.
- */
-public interface FragListBuilder {
-
-  /**
-   * create a FieldFragList.
-   * 
-   * @param fieldPhraseList FieldPhraseList object
-   * @param fragCharSize the length (number of chars) of a fragment
-   * @return the created FieldFragList object
-   */
-  public FieldFragList createFieldFragList( FieldPhraseList fieldPhraseList, int fragCharSize );
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragmentsBuilder.java b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragmentsBuilder.java
deleted file mode 100644
index c60c3b3..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragmentsBuilder.java
+++ /dev/null
@@ -1,57 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexReader;
-
-/**
- * FragmentsBuilder is an interface for fragments (snippets) builder classes.
- * A FragmentsBuilder class can be plugged in to Highlighter.
- */
-public interface FragmentsBuilder {
-
-  /**
-   * create a fragment.
-   * 
-   * @param reader IndexReader of the index
-   * @param docId document id to be highlighted
-   * @param fieldName field of the document to be highlighted
-   * @param fieldFragList FieldFragList object
-   * @return a created fragment or null when no fragment created
-   * @throws IOException
-   */
-  public String createFragment( IndexReader reader, int docId, String fieldName,
-      FieldFragList fieldFragList ) throws IOException;
-
-  /**
-   * create multiple fragments.
-   * 
-   * @param reader IndexReader of the index
-   * @param docId document id to be highlighter
-   * @param fieldName field of the document to be highlighted
-   * @param fieldFragList FieldFragList object
-   * @param maxNumFragments maximum number of fragments
-   * @return created fragments or null when no fragments created.
-   *         size of the array can be less than maxNumFragments
-   * @throws IOException
-   */
-  public String[] createFragments( IndexReader reader, int docId, String fieldName,
-      FieldFragList fieldFragList, int maxNumFragments ) throws IOException;
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilder.java b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilder.java
deleted file mode 100644
index aac72e3..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilder.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.search.vectorhighlight.FieldFragList.WeightedFragInfo;
-
-/**
- * An implementation of FragmentsBuilder that outputs score-order fragments.
- */
-public class ScoreOrderFragmentsBuilder extends BaseFragmentsBuilder {
-
-  /**
-   * a constructor.
-   */
-  public ScoreOrderFragmentsBuilder(){
-    super();
-  }
-
-  /**
-   * a constructor.
-   * 
-   * @param preTags array of pre-tags for markup terms.
-   * @param postTags array of post-tags for markup terms.
-   */
-  public ScoreOrderFragmentsBuilder( String[] preTags, String[] postTags ){
-    super( preTags, postTags );
-  }
-
-  /**
-   * Sort by score the list of WeightedFragInfo
-   */
-  @Override
-  public List<WeightedFragInfo> getWeightedFragInfoList( List<WeightedFragInfo> src ) {
-    Collections.sort( src, new ScoreComparator() );
-    return src;
-  }
-
-  public static class ScoreComparator implements Comparator<WeightedFragInfo> {
-
-    public int compare( WeightedFragInfo o1, WeightedFragInfo o2 ) {
-      if( o1.totalBoost > o2.totalBoost ) return -1;
-      else if( o1.totalBoost < o2.totalBoost ) return 1;
-      // if same score then check startOffset
-      else{
-        if( o1.startOffset < o2.startOffset ) return -1;
-        else if( o1.startOffset > o2.startOffset ) return 1;
-      }
-      return 0;
-    }
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilder.java b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilder.java
deleted file mode 100644
index 089b42e..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilder.java
+++ /dev/null
@@ -1,84 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.lucene.search.vectorhighlight.FieldPhraseList.WeightedPhraseInfo;
-
-/**
- * A simple implementation of FragListBuilder.
- */
-public class SimpleFragListBuilder implements FragListBuilder {
-  
-  public static final int MARGIN = 6;
-  public static final int MIN_FRAG_CHAR_SIZE = MARGIN * 3;
-
-  public FieldFragList createFieldFragList(FieldPhraseList fieldPhraseList, int fragCharSize) {
-    if( fragCharSize < MIN_FRAG_CHAR_SIZE )
-      throw new IllegalArgumentException( "fragCharSize(" + fragCharSize + ") is too small. It must be " +
-          MIN_FRAG_CHAR_SIZE + " or higher." );
-
-    FieldFragList ffl = new FieldFragList( fragCharSize );
-
-    List<WeightedPhraseInfo> wpil = new ArrayList<WeightedPhraseInfo>();
-    Iterator<WeightedPhraseInfo> ite = fieldPhraseList.phraseList.iterator();
-    WeightedPhraseInfo phraseInfo = null;
-    int startOffset = 0;
-    boolean taken = false;
-    while( true ){
-      if( !taken ){
-        if( !ite.hasNext() ) break;
-        phraseInfo = ite.next();
-      }
-      taken = false;
-      if( phraseInfo == null ) break;
-
-      // if the phrase violates the border of previous fragment, discard it and try next phrase
-      if( phraseInfo.getStartOffset() < startOffset ) continue;
-
-      wpil.clear();
-      wpil.add( phraseInfo );
-      int st = phraseInfo.getStartOffset() - MARGIN < startOffset ?
-          startOffset : phraseInfo.getStartOffset() - MARGIN;
-      int en = st + fragCharSize;
-      if( phraseInfo.getEndOffset() > en )
-        en = phraseInfo.getEndOffset();
-      startOffset = en;
-
-      while( true ){
-        if( ite.hasNext() ){
-          phraseInfo = ite.next();
-          taken = true;
-          if( phraseInfo == null ) break;
-        }
-        else
-          break;
-        if( phraseInfo.getEndOffset() <= en )
-          wpil.add( phraseInfo );
-        else
-          break;
-      }
-      ffl.add( st, en, wpil );
-    }
-    return ffl;
-  }
-
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilder.java b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilder.java
deleted file mode 100644
index 378d692..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilder.java
+++ /dev/null
@@ -1,54 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.List;
-
-import org.apache.lucene.search.vectorhighlight.FieldFragList.WeightedFragInfo;
-
-/**
- * A simple implementation of FragmentsBuilder.
- *
- */
-public class SimpleFragmentsBuilder extends BaseFragmentsBuilder {
-
-  /**
-   * a constructor.
-   */
-  public SimpleFragmentsBuilder() {
-    super();
-  }
-
-  /**
-   * a constructor.
-   * 
-   * @param preTags array of pre-tags for markup terms.
-   * @param postTags array of post-tags for markup terms.
-   */
-  public SimpleFragmentsBuilder( String[] preTags, String[] postTags ) {
-    super( preTags, postTags );
-  }
-
-  /**
-   * do nothing. return the source list.
-   */
-  @Override
-  public List<WeightedFragInfo> getWeightedFragInfoList( List<WeightedFragInfo> src ) {
-    return src;
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/package.html b/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/package.html
deleted file mode 100644
index ee023ab..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/org/apache/lucene/search/vectorhighlight/package.html
+++ /dev/null
@@ -1,143 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-This is an another highlighter implementation.
-
-<h2>Features</h2>
-<ul>
-<li>fast for large docs</li>
-<li>support N-gram fields</li>
-<li>support phrase-unit highlighting with slops</li>
-<li>need Java 1.5</li>
-<li>highlight fields need to be TermVector.WITH_POSITIONS_OFFSETS</li>
-<li>take into account query boost to score fragments</li>
-<li>support colored highlight tags</li>
-<li>pluggable FragListBuilder</li>
-<li>pluggable FragmentsBuilder</li>
-</ul>
-
-<h2>Algorithm</h2>
-<p>To explain the algorithm, let's use the following sample text
- (to be highlighted) and user query:</p>
-
-<table border=1>
-<tr>
-<td><b>Sample Text</b></td>
-<td>Lucene is a search engine library.</td>
-</tr>
-<tr>
-<td><b>User Query</b></td>
-<td>Lucene^2 OR "search library"~1</td>
-</tr>
-</table>
-
-<p>The user query is a BooleanQuery that consists of TermQuery("Lucene") 
-with boost of 2 and PhraseQuery("search library") with slop of 1.</p>
-<p>For your convenience, here is the offsets and positions info of the 
-sample text.</p>
-
-<pre>
-+--------+-----------------------------------+
-|        |          1111111111222222222233333|
-|  offset|01234567890123456789012345678901234|
-+--------+-----------------------------------+
-|document|Lucene is a search engine library. |
-+--------*-----------------------------------+
-|position|0      1  2 3      4      5        |
-+--------*-----------------------------------+
-</pre>
-
-<h3>Step 1.</h3>
-<p>In Step 1, Fast Vector Highlighter generates {@link org.apache.lucene.search.vectorhighlight.FieldQuery.QueryPhraseMap} from the user query.
-<code>QueryPhraseMap</code> consists of the following members:</p>
-<pre>
-public class QueryPhraseMap {
-  boolean terminal;
-  int slop;   // valid if terminal == true and phraseHighlight == true
-  float boost;  // valid if terminal == true
-  Map&lt;String, QueryPhraseMap&gt; subMap;
-} 
-</pre>
-<p><code>QueryPhraseMap</code> has subMap. The key of the subMap is a term 
-text in the user query and the value is a subsequent <code>QueryPhraseMap</code>.
-If the query is a term (not phrase), then the subsequent <code>QueryPhraseMap</code>
-is marked as terminal. If the query is a phrase, then the subsequent <code>QueryPhraseMap</code>
-is not a terminal and it has the next term text in the phrase.</p>
-
-<p>From the sample user query, the following <code>QueryPhraseMap</code> 
-will be generated:</p>
-<pre>
-   QueryPhraseMap
-+--------+-+  +-------+-+
-|"Lucene"|o+->|boost=2|*|  * : terminal
-+--------+-+  +-------+-+
-
-+--------+-+  +---------+-+  +-------+------+-+
-|"search"|o+->|"library"|o+->|boost=1|slop=1|*|
-+--------+-+  +---------+-+  +-------+------+-+
-</pre>
-
-<h3>Step 2.</h3>
-<p>In Step 2, Fast Vector Highlighter generates {@link org.apache.lucene.search.vectorhighlight.FieldTermStack}. Fast Vector Highlighter uses {@link org.apache.lucene.index.TermFreqVector} data
-(must be stored {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS_OFFSETS})
-to generate it. <code>FieldTermStack</code> keeps the terms in the user query.
-Therefore, in this sample case, Fast Vector Highlighter generates the following <code>FieldTermStack</code>:</p>
-<pre>
-   FieldTermStack
-+------------------+
-|"Lucene"(0,6,0)   |
-+------------------+
-|"search"(12,18,3) |
-+------------------+
-|"library"(26,33,5)|
-+------------------+
-where : "termText"(startOffset,endOffset,position)
-</pre>
-<h3>Step 3.</h3>
-<p>In Step 3, Fast Vector Highlighter generates {@link org.apache.lucene.search.vectorhighlight.FieldPhraseList}
-by reference to <code>QueryPhraseMap</code> and <code>FieldTermStack</code>.</p>
-<pre>
-   FieldPhraseList
-+----------------+-----------------+---+
-|"Lucene"        |[(0,6)]          |w=2|
-+----------------+-----------------+---+
-|"search library"|[(12,18),(26,33)]|w=1|
-+----------------+-----------------+---+
-</pre>
-<p>The type of each entry is <code>WeightedPhraseInfo</code> that consists of
-an array of terms offsets and weight. The weight (Fast Vector Highlighter uses query boost to
-calculate the weight) will be taken into account when Fast Vector Highlighter creates
-{@link org.apache.lucene.search.vectorhighlight.FieldFragList} in the next step.</p>
-<h3>Step 4.</h3>
-<p>In Step 4, Fast Vector Highlighter creates <code>FieldFragList</code> by reference to
-<code>FieldPhraseList</code>. In this sample case, the following
-<code>FieldFragList</code> will be generated:</p>
-<pre>
-   FieldFragList
-+---------------------------------+
-|"Lucene"[(0,6)]                  |
-|"search library"[(12,18),(26,33)]|
-|totalBoost=3                     |
-+---------------------------------+
-</pre>
-<h3>Step 5.</h3>
-<p>In Step 5, by using <code>FieldFragList</code> and the field stored data,
-Fast Vector Highlighter creates highlighted snippets!</p>
-</body>
-</html>
diff --git a/lucene/contrib/fast-vector-highlighter/src/java/overview.html b/lucene/contrib/fast-vector-highlighter/src/java/overview.html
deleted file mode 100644
index 80fadd1..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/java/overview.html
+++ /dev/null
@@ -1,26 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>
-     fast-vector-highlighter
-    </title>
-  </head>
-  <body>
-  fast-vector-highlighter
-  </body>
-</html>
\ No newline at end of file
diff --git a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java b/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
deleted file mode 100644
index 883b694..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
+++ /dev/null
@@ -1,429 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-import java.util.Collection;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.KeywordAnalyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Field.Index;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.Field.TermVector;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.DisjunctionMaxQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.LuceneTestCase;
-
-public abstract class AbstractTestCase extends LuceneTestCase {
-
-  protected final String F = "f";
-  protected final String F1 = "f1";
-  protected final String F2 = "f2";
-  protected Directory dir;
-  protected Analyzer analyzerW;
-  protected Analyzer analyzerB;
-  protected Analyzer analyzerK;
-  protected IndexReader reader;  
-  protected QueryParser paW;
-  protected QueryParser paB;
-  
-  protected static final String[] shortMVValues = {
-    "a b c",
-    "",   // empty data in multi valued field
-    "d e"
-  };
-  
-  protected static final String[] longMVValues = {
-    "Followings are the examples of customizable parameters and actual examples of customization:",
-    "The most search engines use only one of these methods. Even the search engines that says they can use the both methods basically"
-  };
-  
-  // test data for LUCENE-1448 bug
-  protected static final String[] biMVValues = {
-    "\nLucene/Solr does not require such additional hardware.",
-    "\nWhen you talk about processing speed, the"
-  };
-  
-  protected static final String[] strMVValues = {
-    "abc",
-    "defg",
-    "hijkl"
-  };
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    analyzerW = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
-    analyzerB = new BigramAnalyzer();
-    analyzerK = new KeywordAnalyzer();
-    paW = new QueryParser(TEST_VERSION_CURRENT,  F, analyzerW );
-    paB = new QueryParser(TEST_VERSION_CURRENT,  F, analyzerB );
-    dir = new RAMDirectory();
-  }
-  
-  @Override
-  protected void tearDown() throws Exception {
-    if( reader != null ){
-      reader.close();
-      reader = null;
-    }
-    super.tearDown();
-  }
-
-  protected Query tq( String text ){
-    return tq( 1F, text );
-  }
-
-  protected Query tq( float boost, String text ){
-    return tq( boost, F, text );
-  }
-  
-  protected Query tq( String field, String text ){
-    return tq( 1F, field, text );
-  }
-  
-  protected Query tq( float boost, String field, String text ){
-    Query query = new TermQuery( new Term( field, text ) );
-    query.setBoost( boost );
-    return query;
-  }
-  
-  protected Query pqF( String... texts ){
-    return pqF( 1F, texts );
-  }
-  
-  protected Query pqF( float boost, String... texts ){
-    return pqF( boost, 0, texts );
-  }
-  
-  protected Query pqF( float boost, int slop, String... texts ){
-    return pq( boost, slop, F, texts );
-  }
-  
-  protected Query pq( String field, String... texts ){
-    return pq( 1F, 0, field, texts );
-  }
-  
-  protected Query pq( float boost, String field, String... texts ){
-    return pq( boost, 0, field, texts );
-  }
-  
-  protected Query pq( float boost, int slop, String field, String... texts ){
-    PhraseQuery query = new PhraseQuery();
-    for( String text : texts ){
-      query.add( new Term( field, text ) );
-    }
-    query.setBoost( boost );
-    query.setSlop( slop );
-    return query;
-  }
-  
-  protected Query dmq( Query... queries ){
-    return dmq( 0.0F, queries );
-  }
-  
-  protected Query dmq( float tieBreakerMultiplier, Query... queries ){
-    DisjunctionMaxQuery query = new DisjunctionMaxQuery( tieBreakerMultiplier );
-    for( Query q : queries ){
-      query.add( q );
-    }
-    return query;
-  }
-  
-  protected void assertCollectionQueries( Collection<Query> actual, Query... expected ){
-    assertEquals( expected.length, actual.size() );
-    for( Query query : expected ){
-      assertTrue( actual.contains( query ) );
-    }
-  }
-
-  static class BigramAnalyzer extends Analyzer {
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      return new BasicNGramTokenizer( reader );
-    }
-  }
-  
-  static class BasicNGramTokenizer extends Tokenizer {
-
-    public static final int DEFAULT_N_SIZE = 2;
-    public static final String DEFAULT_DELIMITERS = " \t\n.,";
-    private final int n;
-    private final String delimiters;
-    private int startTerm;
-    private int lenTerm;
-    private int startOffset;
-    private int nextStartOffset;
-    private int ch;
-    private String snippet;
-    private StringBuilder snippetBuffer;
-    private static final int BUFFER_SIZE = 4096;
-    private char[] charBuffer;
-    private int charBufferIndex;
-    private int charBufferLen;
-    
-    public BasicNGramTokenizer( Reader in ){
-      this( in, DEFAULT_N_SIZE );
-    }
-    
-    public BasicNGramTokenizer( Reader in, int n ){
-      this( in, n, DEFAULT_DELIMITERS );
-    }
-    
-    public BasicNGramTokenizer( Reader in, String delimiters ){
-      this( in, DEFAULT_N_SIZE, delimiters );
-    }
-    
-    public BasicNGramTokenizer( Reader in, int n, String delimiters ){
-      super(in);
-      this.n = n;
-      this.delimiters = delimiters;
-      startTerm = 0;
-      nextStartOffset = 0;
-      snippet = null;
-      snippetBuffer = new StringBuilder();
-      charBuffer = new char[BUFFER_SIZE];
-      charBufferIndex = BUFFER_SIZE;
-      charBufferLen = 0;
-      ch = 0;
-    }
-
-    TermAttribute termAtt = addAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-    @Override
-    public boolean incrementToken() throws IOException {
-      if( !getNextPartialSnippet() )
-        return false;
-      clearAttributes();
-      termAtt.setTermBuffer(snippet, startTerm, lenTerm);
-      offsetAtt.setOffset(correctOffset(startOffset), correctOffset(startOffset + lenTerm));
-      return true;
-    }
-
-    private int getFinalOffset() {
-      return nextStartOffset;
-    }
-    
-    @Override
-    public final void end(){
-      offsetAtt.setOffset(getFinalOffset(),getFinalOffset());
-    }
-    
-    protected boolean getNextPartialSnippet() throws IOException {
-      if( snippet != null && snippet.length() >= startTerm + 1 + n ){
-        startTerm++;
-        startOffset++;
-        lenTerm = n;
-        return true;
-      }
-      return getNextSnippet();
-    }
-    
-    protected boolean getNextSnippet() throws IOException {
-      startTerm = 0;
-      startOffset = nextStartOffset;
-      snippetBuffer.delete( 0, snippetBuffer.length() );
-      while( true ){
-        if( ch != -1 )
-          ch = readCharFromBuffer();
-        if( ch == -1 ) break;
-        else if( !isDelimiter( ch ) )
-          snippetBuffer.append( (char)ch );
-        else if( snippetBuffer.length() > 0 )
-          break;
-        else
-          startOffset++;
-      }
-      if( snippetBuffer.length() == 0 )
-        return false;
-      snippet = snippetBuffer.toString();
-      lenTerm = snippet.length() >= n ? n : snippet.length();
-      return true;
-    }
-    
-    protected int readCharFromBuffer() throws IOException {
-      if( charBufferIndex >= charBufferLen ){
-        charBufferLen = input.read( charBuffer );
-        if( charBufferLen == -1 ){
-          return -1;
-        }
-        charBufferIndex = 0;
-      }
-      int c = charBuffer[charBufferIndex++];
-      nextStartOffset++;
-      return c;
-    }
-    
-    protected boolean isDelimiter( int c ){
-      return delimiters.indexOf( c ) >= 0;
-    }
-    
-    @Override
-    public void reset( Reader input ) throws IOException {
-      super.reset( input );
-      reset();
-    }
-    
-    @Override
-    public void reset() throws IOException {
-      startTerm = 0;
-      nextStartOffset = 0;
-      snippet = null;
-      snippetBuffer.setLength( 0 );
-      charBufferIndex = BUFFER_SIZE;
-      charBufferLen = 0;
-      ch = 0;
-    }
-  }
-
-  protected void make1d1fIndex( String value ) throws Exception {
-    make1dmfIndex( value );
-  }
-  
-  protected void make1d1fIndexB( String value ) throws Exception {
-    make1dmfIndexB( value );
-  }
-  
-  protected void make1dmfIndex( String... values ) throws Exception {
-    make1dmfIndex( analyzerW, values );
-  }
-  
-  protected void make1dmfIndexB( String... values ) throws Exception {
-    make1dmfIndex( analyzerB, values );
-  }
-  
-  // make 1 doc with multi valued field
-  protected void make1dmfIndex( Analyzer analyzer, String... values ) throws Exception {
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, analyzer).setOpenMode(OpenMode.CREATE));
-    Document doc = new Document();
-    for( String value: values )
-      doc.add( new Field( F, value, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
-    writer.addDocument( doc );
-    writer.close();
-
-    reader = IndexReader.open( dir, true );
-  }
-  
-  // make 1 doc with multi valued & not analyzed field
-  protected void make1dmfIndexNA( String... values ) throws Exception {
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, analyzerK).setOpenMode(OpenMode.CREATE));
-    Document doc = new Document();
-    for( String value: values )
-      doc.add( new Field( F, value, Store.YES, Index.NOT_ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
-    writer.addDocument( doc );
-    writer.close();
-
-    reader = IndexReader.open( dir, true );
-  }
-  
-  protected void makeIndexShortMV() throws Exception {
-
-    //  012345
-    // "a b c"
-    //  0 1 2
-    
-    // ""
-
-    //  6789
-    // "d e"
-    //  3 4
-    make1dmfIndex( shortMVValues );
-  }
-  
-  protected void makeIndexLongMV() throws Exception {
-    //           11111111112222222222333333333344444444445555555555666666666677777777778888888888999
-    // 012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012
-    // Followings are the examples of customizable parameters and actual examples of customization:
-    // 0          1   2   3        4  5            6          7   8      9        10 11
-    
-    //        1                                                                                                   2
-    // 999999900000000001111111111222222222233333333334444444444555555555566666666667777777777888888888899999999990000000000111111111122
-    // 345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901
-    // The most search engines use only one of these methods. Even the search engines that says they can use the both methods basically
-    // 12  13  (14)   (15)     16  17   18  19 20    21       22   23 (24)   (25)     26   27   28   29  30  31  32   33      34
-
-    make1dmfIndex( longMVValues );
-  }
-  
-  protected void makeIndexLongMVB() throws Exception {
-    // "*" ... LF
-    
-    //           1111111111222222222233333333334444444444555555
-    // 01234567890123456789012345678901234567890123456789012345
-    // *Lucene/Solr does not require such additional hardware.
-    //  Lu 0        do 10    re 15   su 21       na 31
-    //   uc 1        oe 11    eq 16   uc 22       al 32
-    //    ce 2        es 12    qu 17   ch 23         ha 33
-    //     en 3          no 13  ui 18     ad 24       ar 34
-    //      ne 4          ot 14  ir 19     dd 25       rd 35
-    //       e/ 5                 re 20     di 26       dw 36
-    //        /S 6                           it 27       wa 37
-    //         So 7                           ti 28       ar 38
-    //          ol 8                           io 29       re 39
-    //           lr 9                           on 30
-
-    // 5555666666666677777777778888888888999999999
-    // 6789012345678901234567890123456789012345678
-    // *When you talk about processing speed, the
-    //  Wh 40         ab 48     es 56         th 65
-    //   he 41         bo 49     ss 57         he 66
-    //    en 42         ou 50     si 58
-    //       yo 43       ut 51     in 59
-    //        ou 44         pr 52   ng 60
-    //           ta 45       ro 53     sp 61
-    //            al 46       oc 54     pe 62
-    //             lk 47       ce 55     ee 63
-    //                                    ed 64
-
-    make1dmfIndexB( biMVValues );
-  }
-  
-  protected void makeIndexStrMV() throws Exception {
-
-    //  0123
-    // "abc"
-    
-    //  34567
-    // "defg"
-
-    //     111
-    //  789012
-    // "hijkl"
-    make1dmfIndexNA( strMVValues );
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java b/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java
deleted file mode 100644
index 1eb89d7..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java
+++ /dev/null
@@ -1,191 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.BooleanClause.Occur;
-
-public class FieldPhraseListTest extends AbstractTestCase {
-  
-  public void test1TermIndex() throws Exception {
-    make1d1fIndex( "a" );
-
-    FieldQuery fq = new FieldQuery( tq( "a" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "a(1.0)((0,1))", fpl.phraseList.get( 0 ).toString() );
-
-    fq = new FieldQuery( tq( "b" ), true, true );
-    stack = new FieldTermStack( reader, 0, F, fq );
-    fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 0, fpl.phraseList.size() );
-  }
-  
-  public void test2TermsIndex() throws Exception {
-    make1d1fIndex( "a a" );
-
-    FieldQuery fq = new FieldQuery( tq( "a" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 2, fpl.phraseList.size() );
-    assertEquals( "a(1.0)((0,1))", fpl.phraseList.get( 0 ).toString() );
-    assertEquals( "a(1.0)((2,3))", fpl.phraseList.get( 1 ).toString() );
-  }
-  
-  public void test1PhraseIndex() throws Exception {
-    make1d1fIndex( "a b" );
-
-    FieldQuery fq = new FieldQuery( pqF( "a", "b" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "ab(1.0)((0,3))", fpl.phraseList.get( 0 ).toString() );
-
-    fq = new FieldQuery( tq( "b" ), true, true );
-    stack = new FieldTermStack( reader, 0, F, fq );
-    fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "b(1.0)((2,3))", fpl.phraseList.get( 0 ).toString() );
-  }
-  
-  public void test1PhraseIndexB() throws Exception {
-    // 01 12 23 34 45 56 67 78 (offsets)
-    // bb|bb|ba|ac|cb|ba|ab|bc
-    //  0  1  2  3  4  5  6  7 (positions)
-    make1d1fIndexB( "bbbacbabc" );
-
-    FieldQuery fq = new FieldQuery( pqF( "ba", "ac" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "baac(1.0)((2,5))", fpl.phraseList.get( 0 ).toString() );
-  }
-  
-  public void test2ConcatTermsIndexB() throws Exception {
-    // 01 12 23 (offsets)
-    // ab|ba|ab
-    //  0  1  2 (positions)
-    make1d1fIndexB( "abab" );
-
-    FieldQuery fq = new FieldQuery( tq( "ab" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 2, fpl.phraseList.size() );
-    assertEquals( "ab(1.0)((0,2))", fpl.phraseList.get( 0 ).toString() );
-    assertEquals( "ab(1.0)((2,4))", fpl.phraseList.get( 1 ).toString() );
-  }
-  
-  public void test2Terms1PhraseIndex() throws Exception {
-    make1d1fIndex( "c a a b" );
-
-    // phraseHighlight = true
-    FieldQuery fq = new FieldQuery( pqF( "a", "b" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "ab(1.0)((4,7))", fpl.phraseList.get( 0 ).toString() );
-
-    // phraseHighlight = false
-    fq = new FieldQuery( pqF( "a", "b" ), false, true );
-    stack = new FieldTermStack( reader, 0, F, fq );
-    fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 2, fpl.phraseList.size() );
-    assertEquals( "a(1.0)((2,3))", fpl.phraseList.get( 0 ).toString() );
-    assertEquals( "ab(1.0)((4,7))", fpl.phraseList.get( 1 ).toString() );
-  }
-  
-  public void testPhraseSlop() throws Exception {
-    make1d1fIndex( "c a a b c" );
-
-    FieldQuery fq = new FieldQuery( pqF( 2F, 1, "a", "c" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "ac(2.0)((4,5)(8,9))", fpl.phraseList.get( 0 ).toString() );
-    assertEquals( 4, fpl.phraseList.get( 0 ).getStartOffset() );
-    assertEquals( 9, fpl.phraseList.get( 0 ).getEndOffset() );
-  }
-  
-  public void test2PhrasesOverlap() throws Exception {
-    make1d1fIndex( "d a b c d" );
-
-    BooleanQuery query = new BooleanQuery();
-    query.add( pqF( "a", "b" ), Occur.SHOULD );
-    query.add( pqF( "b", "c" ), Occur.SHOULD );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "abc(1.0)((2,7))", fpl.phraseList.get( 0 ).toString() );
-  }
-  
-  public void test3TermsPhrase() throws Exception {
-    make1d1fIndex( "d a b a b c d" );
-
-    FieldQuery fq = new FieldQuery( pqF( "a", "b", "c" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "abc(1.0)((6,11))", fpl.phraseList.get( 0 ).toString() );
-  }
-  
-  public void testSearchLongestPhrase() throws Exception {
-    make1d1fIndex( "d a b d c a b c" );
-
-    BooleanQuery query = new BooleanQuery();
-    query.add( pqF( "a", "b" ), Occur.SHOULD );
-    query.add( pqF( "a", "b", "c" ), Occur.SHOULD );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 2, fpl.phraseList.size() );
-    assertEquals( "ab(1.0)((2,5))", fpl.phraseList.get( 0 ).toString() );
-    assertEquals( "abc(1.0)((10,15))", fpl.phraseList.get( 1 ).toString() );
-  }
-  
-  public void test1PhraseShortMV() throws Exception {
-    makeIndexShortMV();
-
-    FieldQuery fq = new FieldQuery( tq( "d" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "d(1.0)((6,7))", fpl.phraseList.get( 0 ).toString() );
-  }
-  
-  public void test1PhraseLongMV() throws Exception {
-    makeIndexLongMV();
-
-    FieldQuery fq = new FieldQuery( pqF( "search", "engines" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 2, fpl.phraseList.size() );
-    assertEquals( "searchengines(1.0)((102,116))", fpl.phraseList.get( 0 ).toString() );
-    assertEquals( "searchengines(1.0)((157,171))", fpl.phraseList.get( 1 ).toString() );
-  }
-
-  public void test1PhraseLongMVB() throws Exception {
-    makeIndexLongMVB();
-
-    FieldQuery fq = new FieldQuery( pqF( "sp", "pe", "ee", "ed" ), true, true ); // "speed" -(2gram)-> "sp","pe","ee","ed"
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "sppeeeed(1.0)((88,93))", fpl.phraseList.get( 0 ).toString() );
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java b/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
deleted file mode 100644
index cb73765..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
+++ /dev/null
@@ -1,837 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.vectorhighlight.FieldQuery.QueryPhraseMap;
-import org.apache.lucene.search.vectorhighlight.FieldTermStack.TermInfo;
-
-public class FieldQueryTest extends AbstractTestCase {
-
-  public void testFlattenBoolean() throws Exception {
-    Query query = paW.parse( "A AND B OR C NOT (D AND E)" );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
-    fq.flatten( query, flatQueries );
-    assertCollectionQueries( flatQueries, tq( "A" ), tq( "B" ), tq( "C" ) );
-  }
-
-  public void testFlattenDisjunctionMaxQuery() throws Exception {
-    Query query = dmq( tq( "A" ), tq( "B" ), pqF( "C", "D" ) );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
-    fq.flatten( query, flatQueries );
-    assertCollectionQueries( flatQueries, tq( "A" ), tq( "B" ), pqF( "C", "D" ) );
-  }
-
-  public void testFlattenTermAndPhrase() throws Exception {
-    Query query = paW.parse( "A AND \"B C\"" );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
-    fq.flatten( query, flatQueries );
-    assertCollectionQueries( flatQueries, tq( "A" ), pqF( "B", "C" ) );
-  }
-
-  public void testFlattenTermAndPhrase2gram() throws Exception {
-    Query query = paB.parse( "AA AND BCD OR EFGH" );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
-    fq.flatten( query, flatQueries );
-    assertCollectionQueries( flatQueries, tq( "AA" ), pqF( "BC", "CD" ), pqF( "EF", "FG", "GH" ) );
-  }
-
-  public void testFlatten1TermPhrase() throws Exception {
-    Query query = pqF( "A" );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Set<Query> flatQueries = new HashSet<Query>();
-    fq.flatten( query, flatQueries );
-    assertCollectionQueries( flatQueries, tq( "A" ) );
-  }
-
-  public void testExpand() throws Exception {
-    Query dummy = pqF( "DUMMY" );
-    FieldQuery fq = new FieldQuery( dummy, true, true );
-
-    // "a b","b c" => "a b","b c","a b c"
-    Set<Query> flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b" ) );
-    flatQueries.add( pqF( "b", "c" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b" ), pqF( "b", "c" ), pqF( "a", "b", "c" ) );
-
-    // "a b","b c d" => "a b","b c d","a b c d"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b" ) );
-    flatQueries.add( pqF( "b", "c", "d" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b" ), pqF( "b", "c", "d" ), pqF( "a", "b", "c", "d" ) );
-
-    // "a b c","b c d" => "a b c","b c d","a b c d"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b", "c" ) );
-    flatQueries.add( pqF( "b", "c", "d" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b", "c" ), pqF( "b", "c", "d" ), pqF( "a", "b", "c", "d" ) );
-
-    // "a b c","c d e" => "a b c","c d e","a b c d e"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b", "c" ) );
-    flatQueries.add( pqF( "c", "d", "e" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b", "c" ), pqF( "c", "d", "e" ), pqF( "a", "b", "c", "d", "e" ) );
-
-    // "a b c d","b c" => "a b c d","b c"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b", "c", "d" ) );
-    flatQueries.add( pqF( "b", "c" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b", "c", "d" ), pqF( "b", "c" ) );
-
-    // "a b b","b c" => "a b b","b c","a b b c"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b", "b" ) );
-    flatQueries.add( pqF( "b", "c" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b", "b" ), pqF( "b", "c" ), pqF( "a", "b", "b", "c" ) );
-
-    // "a b","b a" => "a b","b a","a b a", "b a b"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b" ) );
-    flatQueries.add( pqF( "b", "a" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b" ), pqF( "b", "a" ), pqF( "a", "b", "a" ), pqF( "b", "a", "b" ) );
-
-    // "a b","a b c" => "a b","a b c"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b" ) );
-    flatQueries.add( pqF( "a", "b", "c" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b" ), pqF( "a", "b", "c" ) );
-  }
-
-  public void testNoExpand() throws Exception {
-    Query dummy = pqF( "DUMMY" );
-    FieldQuery fq = new FieldQuery( dummy, true, true );
-
-    // "a b","c d" => "a b","c d"
-    Set<Query> flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b" ) );
-    flatQueries.add( pqF( "c", "d" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b" ), pqF( "c", "d" ) );
-
-    // "a","a b" => "a", "a b"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( tq( "a" ) );
-    flatQueries.add( pqF( "a", "b" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        tq( "a" ), pqF( "a", "b" ) );
-
-    // "a b","b" => "a b", "b"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b" ) );
-    flatQueries.add( tq( "b" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b" ), tq( "b" ) );
-
-    // "a b c","b c" => "a b c","b c"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b", "c" ) );
-    flatQueries.add( pqF( "b", "c" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b", "c" ), pqF( "b", "c" ) );
-
-    // "a b","a b c" => "a b","a b c"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b" ) );
-    flatQueries.add( pqF( "a", "b", "c" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b" ), pqF( "a", "b", "c" ) );
-
-    // "a b c","b d e" => "a b c","b d e"
-    flatQueries = new HashSet<Query>();
-    flatQueries.add( pqF( "a", "b", "c" ) );
-    flatQueries.add( pqF( "b", "d", "e" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pqF( "a", "b", "c" ), pqF( "b", "d", "e" ) );
-  }
-
-  public void testExpandNotFieldMatch() throws Exception {
-    Query dummy = pqF( "DUMMY" );
-    FieldQuery fq = new FieldQuery( dummy, true, false );
-
-    // f1:"a b",f2:"b c" => f1:"a b",f2:"b c",f1:"a b c"
-    Set<Query> flatQueries = new HashSet<Query>();
-    flatQueries.add( pq( F1, "a", "b" ) );
-    flatQueries.add( pq( F2, "b", "c" ) );
-    assertCollectionQueries( fq.expand( flatQueries ),
-        pq( F1, "a", "b" ), pq( F2, "b", "c" ), pq( F1, "a", "b", "c" ) );
-  }
-
-  public void testGetFieldTermMap() throws Exception {
-    Query query = tq( "a" );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    
-    QueryPhraseMap pqm = fq.getFieldTermMap( F, "a" );
-    assertNotNull( pqm );
-    assertTrue( pqm.isTerminal() );
-    
-    pqm = fq.getFieldTermMap( F, "b" );
-    assertNull( pqm );
-    
-    pqm = fq.getFieldTermMap( F1, "a" );
-    assertNull( pqm );
-  }
-
-  public void testGetRootMap() throws Exception {
-    Query dummy = pqF( "DUMMY" );
-    FieldQuery fq = new FieldQuery( dummy, true, true );
-
-    QueryPhraseMap rootMap1 = fq.getRootMap( tq( "a" ) );
-    QueryPhraseMap rootMap2 = fq.getRootMap( tq( "a" ) );
-    assertTrue( rootMap1 == rootMap2 );
-    QueryPhraseMap rootMap3 = fq.getRootMap( tq( "b" ) );
-    assertTrue( rootMap1 == rootMap3 );
-    QueryPhraseMap rootMap4 = fq.getRootMap( tq( F1, "b" ) );
-    assertFalse( rootMap4 == rootMap3 );
-  }
-
-  public void testGetRootMapNotFieldMatch() throws Exception {
-    Query dummy = pqF( "DUMMY" );
-    FieldQuery fq = new FieldQuery( dummy, true, false );
-
-    QueryPhraseMap rootMap1 = fq.getRootMap( tq( "a" ) );
-    QueryPhraseMap rootMap2 = fq.getRootMap( tq( "a" ) );
-    assertTrue( rootMap1 == rootMap2 );
-    QueryPhraseMap rootMap3 = fq.getRootMap( tq( "b" ) );
-    assertTrue( rootMap1 == rootMap3 );
-    QueryPhraseMap rootMap4 = fq.getRootMap( tq( F1, "b" ) );
-    assertTrue( rootMap4 == rootMap3 );
-  }
-
-  public void testGetTermSet() throws Exception {
-    Query query = paW.parse( "A AND B OR x:C NOT (D AND E)" );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    assertEquals( 2, fq.termSetMap.size() );
-    Set<String> termSet = fq.getTermSet( F );
-    assertEquals( 2, termSet.size() );
-    assertTrue( termSet.contains( "A" ) );
-    assertTrue( termSet.contains( "B" ) );
-    termSet = fq.getTermSet( "x" );
-    assertEquals( 1, termSet.size() );
-    assertTrue( termSet.contains( "C" ) );
-    termSet = fq.getTermSet( "y" );
-    assertNull( termSet );
-  }
-  
-  public void testQueryPhraseMap1Term() throws Exception {
-    Query query = tq( "a" );
-    
-    // phraseHighlight = true, fieldMatch = true
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Map<String, QueryPhraseMap> map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( null ) );
-    assertNotNull( map.get( F ) );
-    QueryPhraseMap qpm = map.get( F );
-    assertEquals( 1, qpm.subMap.size() );
-    assertTrue( qpm.subMap.get( "a" ) != null );
-    assertTrue( qpm.subMap.get( "a" ).terminal );
-    assertEquals( 1F, qpm.subMap.get( "a" ).boost );
-    
-    // phraseHighlight = true, fieldMatch = false
-    fq = new FieldQuery( query, true, false );
-    map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( F ) );
-    assertNotNull( map.get( null ) );
-    qpm = map.get( null );
-    assertEquals( 1, qpm.subMap.size() );
-    assertTrue( qpm.subMap.get( "a" ) != null );
-    assertTrue( qpm.subMap.get( "a" ).terminal );
-    assertEquals( 1F, qpm.subMap.get( "a" ).boost );
-    
-    // phraseHighlight = false, fieldMatch = true
-    fq = new FieldQuery( query, false, true );
-    map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( null ) );
-    assertNotNull( map.get( F ) );
-    qpm = map.get( F );
-    assertEquals( 1, qpm.subMap.size() );
-    assertTrue( qpm.subMap.get( "a" ) != null );
-    assertTrue( qpm.subMap.get( "a" ).terminal );
-    assertEquals( 1F, qpm.subMap.get( "a" ).boost );
-    
-    // phraseHighlight = false, fieldMatch = false
-    fq = new FieldQuery( query, false, false );
-    map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( F ) );
-    assertNotNull( map.get( null ) );
-    qpm = map.get( null );
-    assertEquals( 1, qpm.subMap.size() );
-    assertTrue( qpm.subMap.get( "a" ) != null );
-    assertTrue( qpm.subMap.get( "a" ).terminal );
-    assertEquals( 1F, qpm.subMap.get( "a" ).boost );
-    
-    // boost != 1
-    query = tq( 2, "a" );
-    fq = new FieldQuery( query, true, true );
-    map = fq.rootMaps;
-    qpm = map.get( F );
-    assertEquals( 2F, qpm.subMap.get( "a" ).boost );
-  }
-  
-  public void testQueryPhraseMap1Phrase() throws Exception {
-    Query query = pqF( "a", "b" );
-    
-    // phraseHighlight = true, fieldMatch = true
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Map<String, QueryPhraseMap> map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( null ) );
-    assertNotNull( map.get( F ) );
-    QueryPhraseMap qpm = map.get( F );
-    assertEquals( 1, qpm.subMap.size() );
-    assertNotNull( qpm.subMap.get( "a" ) );
-    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "b" ) );
-    QueryPhraseMap qpm3 = qpm2.subMap.get( "b" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-    
-    // phraseHighlight = true, fieldMatch = false
-    fq = new FieldQuery( query, true, false );
-    map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( F ) );
-    assertNotNull( map.get( null ) );
-    qpm = map.get( null );
-    assertEquals( 1, qpm.subMap.size() );
-    assertNotNull( qpm.subMap.get( "a" ) );
-    qpm2 = qpm.subMap.get( "a" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "b" ) );
-    qpm3 = qpm2.subMap.get( "b" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-    
-    // phraseHighlight = false, fieldMatch = true
-    fq = new FieldQuery( query, false, true );
-    map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( null ) );
-    assertNotNull( map.get( F ) );
-    qpm = map.get( F );
-    assertEquals( 2, qpm.subMap.size() );
-    assertNotNull( qpm.subMap.get( "a" ) );
-    qpm2 = qpm.subMap.get( "a" );
-    assertTrue( qpm2.terminal );
-    assertEquals( 1F, qpm2.boost );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "b" ) );
-    qpm3 = qpm2.subMap.get( "b" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-
-    assertNotNull( qpm.subMap.get( "b" ) );
-    qpm2 = qpm.subMap.get( "b" );
-    assertTrue( qpm2.terminal );
-    assertEquals( 1F, qpm2.boost );
-    
-    // phraseHighlight = false, fieldMatch = false
-    fq = new FieldQuery( query, false, false );
-    map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( F ) );
-    assertNotNull( map.get( null ) );
-    qpm = map.get( null );
-    assertEquals( 2, qpm.subMap.size() );
-    assertNotNull( qpm.subMap.get( "a" ) );
-    qpm2 = qpm.subMap.get( "a" );
-    assertTrue( qpm2.terminal );
-    assertEquals( 1F, qpm2.boost );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "b" ) );
-    qpm3 = qpm2.subMap.get( "b" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-
-    assertNotNull( qpm.subMap.get( "b" ) );
-    qpm2 = qpm.subMap.get( "b" );
-    assertTrue( qpm2.terminal );
-    assertEquals( 1F, qpm2.boost );
-
-    // boost != 1
-    query = pqF( 2, "a", "b" );
-    // phraseHighlight = false, fieldMatch = false
-    fq = new FieldQuery( query, false, false );
-    map = fq.rootMaps;
-    qpm = map.get( null );
-    qpm2 = qpm.subMap.get( "a" );
-    assertEquals( 2F, qpm2.boost );
-    qpm3 = qpm2.subMap.get( "b" );
-    assertEquals( 2F, qpm3.boost );
-    qpm2 = qpm.subMap.get( "b" );
-    assertEquals( 2F, qpm2.boost );
-  }
-  
-  public void testQueryPhraseMap1PhraseAnother() throws Exception {
-    Query query = pqF( "search", "engines" );
-    
-    // phraseHighlight = true, fieldMatch = true
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Map<String, QueryPhraseMap> map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( null ) );
-    assertNotNull( map.get( F ) );
-    QueryPhraseMap qpm = map.get( F );
-    assertEquals( 1, qpm.subMap.size() );
-    assertNotNull( qpm.subMap.get( "search" ) );
-    QueryPhraseMap qpm2 = qpm.subMap.get( "search" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "engines" ) );
-    QueryPhraseMap qpm3 = qpm2.subMap.get( "engines" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-  }
-  
-  public void testQueryPhraseMap2Phrases() throws Exception {
-    BooleanQuery query = new BooleanQuery();
-    query.add( pqF( "a", "b" ), Occur.SHOULD );
-    query.add( pqF( 2, "c", "d" ), Occur.SHOULD );
-    
-    // phraseHighlight = true, fieldMatch = true
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Map<String, QueryPhraseMap> map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( null ) );
-    assertNotNull( map.get( F ) );
-    QueryPhraseMap qpm = map.get( F );
-    assertEquals( 2, qpm.subMap.size() );
-
-    // "a b"
-    assertNotNull( qpm.subMap.get( "a" ) );
-    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "b" ) );
-    QueryPhraseMap qpm3 = qpm2.subMap.get( "b" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-
-    // "c d"^2
-    assertNotNull( qpm.subMap.get( "c" ) );
-    qpm2 = qpm.subMap.get( "c" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "d" ) );
-    qpm3 = qpm2.subMap.get( "d" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 2F, qpm3.boost );
-  }
-  
-  public void testQueryPhraseMap2PhrasesFields() throws Exception {
-    BooleanQuery query = new BooleanQuery();
-    query.add( pq( F1, "a", "b" ), Occur.SHOULD );
-    query.add( pq( 2F, F2, "c", "d" ), Occur.SHOULD );
-    
-    // phraseHighlight = true, fieldMatch = true
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Map<String, QueryPhraseMap> map = fq.rootMaps;
-    assertEquals( 2, map.size() );
-    assertNull( map.get( null ) );
-
-    // "a b"
-    assertNotNull( map.get( F1 ) );
-    QueryPhraseMap qpm = map.get( F1 );
-    assertEquals( 1, qpm.subMap.size() );
-    assertNotNull( qpm.subMap.get( "a" ) );
-    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "b" ) );
-    QueryPhraseMap qpm3 = qpm2.subMap.get( "b" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-
-    // "c d"^2
-    assertNotNull( map.get( F2 ) );
-    qpm = map.get( F2 );
-    assertEquals( 1, qpm.subMap.size() );
-    assertNotNull( qpm.subMap.get( "c" ) );
-    qpm2 = qpm.subMap.get( "c" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "d" ) );
-    qpm3 = qpm2.subMap.get( "d" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 2F, qpm3.boost );
-    
-    // phraseHighlight = true, fieldMatch = false
-    fq = new FieldQuery( query, true, false );
-    map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( F1 ) );
-    assertNull( map.get( F2 ) );
-    assertNotNull( map.get( null ) );
-    qpm = map.get( null );
-    assertEquals( 2, qpm.subMap.size() );
-
-    // "a b"
-    assertNotNull( qpm.subMap.get( "a" ) );
-    qpm2 = qpm.subMap.get( "a" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "b" ) );
-    qpm3 = qpm2.subMap.get( "b" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-
-    // "c d"^2
-    assertNotNull( qpm.subMap.get( "c" ) );
-    qpm2 = qpm.subMap.get( "c" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "d" ) );
-    qpm3 = qpm2.subMap.get( "d" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 2F, qpm3.boost );
-  }
-  
-  /*
-   * <t>...terminal
-   * 
-   * a-b-c-<t>
-   *     +-d-<t>
-   * b-c-d-<t>
-   * +-d-<t>
-   */
-  public void testQueryPhraseMapOverlapPhrases() throws Exception {
-    BooleanQuery query = new BooleanQuery();
-    query.add( pqF( "a", "b", "c" ), Occur.SHOULD );
-    query.add( pqF( 2, "b", "c", "d" ), Occur.SHOULD );
-    query.add( pqF( 3, "b", "d" ), Occur.SHOULD );
-    
-    // phraseHighlight = true, fieldMatch = true
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Map<String, QueryPhraseMap> map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( null ) );
-    assertNotNull( map.get( F ) );
-    QueryPhraseMap qpm = map.get( F );
-    assertEquals( 2, qpm.subMap.size() );
-
-    // "a b c"
-    assertNotNull( qpm.subMap.get( "a" ) );
-    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "b" ) );
-    QueryPhraseMap qpm3 = qpm2.subMap.get( "b" );
-    assertFalse( qpm3.terminal );
-    assertEquals( 1, qpm3.subMap.size() );
-    assertNotNull( qpm3.subMap.get( "c" ) );
-    QueryPhraseMap qpm4 = qpm3.subMap.get( "c" );
-    assertTrue( qpm4.terminal );
-    assertEquals( 1F, qpm4.boost );
-    assertNotNull( qpm4.subMap.get( "d" ) );
-    QueryPhraseMap qpm5 = qpm4.subMap.get( "d" );
-    assertTrue( qpm5.terminal );
-    assertEquals( 1F, qpm5.boost );
-
-    // "b c d"^2, "b d"^3
-    assertNotNull( qpm.subMap.get( "b" ) );
-    qpm2 = qpm.subMap.get( "b" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 2, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "c" ) );
-    qpm3 = qpm2.subMap.get( "c" );
-    assertFalse( qpm3.terminal );
-    assertEquals( 1, qpm3.subMap.size() );
-    assertNotNull( qpm3.subMap.get( "d" ) );
-    qpm4 = qpm3.subMap.get( "d" );
-    assertTrue( qpm4.terminal );
-    assertEquals( 2F, qpm4.boost );
-    assertNotNull( qpm2.subMap.get( "d" ) );
-    qpm3 = qpm2.subMap.get( "d" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 3F, qpm3.boost );
-  }
-  
-  /*
-   * <t>...terminal
-   * 
-   * a-b-<t>
-   *   +-c-<t>
-   */
-  public void testQueryPhraseMapOverlapPhrases2() throws Exception {
-    BooleanQuery query = new BooleanQuery();
-    query.add( pqF( "a", "b" ), Occur.SHOULD );
-    query.add( pqF( 2, "a", "b", "c" ), Occur.SHOULD );
-    
-    // phraseHighlight = true, fieldMatch = true
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Map<String, QueryPhraseMap> map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( null ) );
-    assertNotNull( map.get( F ) );
-    QueryPhraseMap qpm = map.get( F );
-    assertEquals( 1, qpm.subMap.size() );
-
-    // "a b"
-    assertNotNull( qpm.subMap.get( "a" ) );
-    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "b" ) );
-    QueryPhraseMap qpm3 = qpm2.subMap.get( "b" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-
-    // "a b c"^2
-    assertEquals( 1, qpm3.subMap.size() );
-    assertNotNull( qpm3.subMap.get( "c" ) );
-    QueryPhraseMap qpm4 = qpm3.subMap.get( "c" );
-    assertTrue( qpm4.terminal );
-    assertEquals( 2F, qpm4.boost );
-  }
-  
-  /*
-   * <t>...terminal
-   * 
-   * a-a-a-<t>
-   *     +-a-<t>
-   *       +-a-<t>
-   *         +-a-<t>
-   */
-  public void testQueryPhraseMapOverlapPhrases3() throws Exception {
-    BooleanQuery query = new BooleanQuery();
-    query.add( pqF( "a", "a", "a", "a" ), Occur.SHOULD );
-    query.add( pqF( 2, "a", "a", "a" ), Occur.SHOULD );
-    
-    // phraseHighlight = true, fieldMatch = true
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Map<String, QueryPhraseMap> map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( null ) );
-    assertNotNull( map.get( F ) );
-    QueryPhraseMap qpm = map.get( F );
-    assertEquals( 1, qpm.subMap.size() );
-
-    // "a a a"
-    assertNotNull( qpm.subMap.get( "a" ) );
-    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "a" ) );
-    QueryPhraseMap qpm3 = qpm2.subMap.get( "a" );
-    assertFalse( qpm3.terminal );
-    assertEquals( 1, qpm3.subMap.size() );
-    assertNotNull( qpm3.subMap.get( "a" ) );
-    QueryPhraseMap qpm4 = qpm3.subMap.get( "a" );
-    assertTrue( qpm4.terminal );
-
-    // "a a a a"
-    assertEquals( 1, qpm4.subMap.size() );
-    assertNotNull( qpm4.subMap.get( "a" ) );
-    QueryPhraseMap qpm5 = qpm4.subMap.get( "a" );
-    assertTrue( qpm5.terminal );
-
-    // "a a a a a"
-    assertEquals( 1, qpm5.subMap.size() );
-    assertNotNull( qpm5.subMap.get( "a" ) );
-    QueryPhraseMap qpm6 = qpm5.subMap.get( "a" );
-    assertTrue( qpm6.terminal );
-
-    // "a a a a a a"
-    assertEquals( 1, qpm6.subMap.size() );
-    assertNotNull( qpm6.subMap.get( "a" ) );
-    QueryPhraseMap qpm7 = qpm6.subMap.get( "a" );
-    assertTrue( qpm7.terminal );
-  }
-  
-  public void testQueryPhraseMapOverlap2gram() throws Exception {
-    Query query = paB.parse( "abc AND bcd" );
-    
-    // phraseHighlight = true, fieldMatch = true
-    FieldQuery fq = new FieldQuery( query, true, true );
-    Map<String, QueryPhraseMap> map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( null ) );
-    assertNotNull( map.get( F ) );
-    QueryPhraseMap qpm = map.get( F );
-    assertEquals( 2, qpm.subMap.size() );
-
-    // "ab bc"
-    assertNotNull( qpm.subMap.get( "ab" ) );
-    QueryPhraseMap qpm2 = qpm.subMap.get( "ab" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "bc" ) );
-    QueryPhraseMap qpm3 = qpm2.subMap.get( "bc" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-
-    // "ab bc cd"
-    assertEquals( 1, qpm3.subMap.size() );
-    assertNotNull( qpm3.subMap.get( "cd" ) );
-    QueryPhraseMap qpm4 = qpm3.subMap.get( "cd" );
-    assertTrue( qpm4.terminal );
-    assertEquals( 1F, qpm4.boost );
-
-    // "bc cd"
-    assertNotNull( qpm.subMap.get( "bc" ) );
-    qpm2 = qpm.subMap.get( "bc" );
-    assertFalse( qpm2.terminal );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "cd" ) );
-    qpm3 = qpm2.subMap.get( "cd" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-    
-    // phraseHighlight = false, fieldMatch = true
-    fq = new FieldQuery( query, false, true );
-    map = fq.rootMaps;
-    assertEquals( 1, map.size() );
-    assertNull( map.get( null ) );
-    assertNotNull( map.get( F ) );
-    qpm = map.get( F );
-    assertEquals( 3, qpm.subMap.size() );
-
-    // "ab bc"
-    assertNotNull( qpm.subMap.get( "ab" ) );
-    qpm2 = qpm.subMap.get( "ab" );
-    assertTrue( qpm2.terminal );
-    assertEquals( 1F, qpm2.boost );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "bc" ) );
-    qpm3 = qpm2.subMap.get( "bc" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-
-    // "ab bc cd"
-    assertEquals( 1, qpm3.subMap.size() );
-    assertNotNull( qpm3.subMap.get( "cd" ) );
-    qpm4 = qpm3.subMap.get( "cd" );
-    assertTrue( qpm4.terminal );
-    assertEquals( 1F, qpm4.boost );
-
-    // "bc cd"
-    assertNotNull( qpm.subMap.get( "bc" ) );
-    qpm2 = qpm.subMap.get( "bc" );
-    assertTrue( qpm2.terminal );
-    assertEquals( 1F, qpm2.boost );
-    assertEquals( 1, qpm2.subMap.size() );
-    assertNotNull( qpm2.subMap.get( "cd" ) );
-    qpm3 = qpm2.subMap.get( "cd" );
-    assertTrue( qpm3.terminal );
-    assertEquals( 1F, qpm3.boost );
-
-    // "cd"
-    assertNotNull( qpm.subMap.get( "cd" ) );
-    qpm2 = qpm.subMap.get( "cd" );
-    assertTrue( qpm2.terminal );
-    assertEquals( 1F, qpm2.boost );
-    assertEquals( 0, qpm2.subMap.size() );
-  }
-  
-  public void testSearchPhrase() throws Exception {
-    Query query = pqF( "a", "b", "c" );
-
-    // phraseHighlight = true, fieldMatch = true
-    FieldQuery fq = new FieldQuery( query, true, true );
-    
-    // "a"
-    List<TermInfo> phraseCandidate = new ArrayList<TermInfo>();
-    phraseCandidate.add( new TermInfo( "a", 0, 1, 0 ) );
-    assertNull( fq.searchPhrase( F, phraseCandidate ) );
-    // "a b"
-    phraseCandidate.add( new TermInfo( "b", 2, 3, 1 ) );
-    assertNull( fq.searchPhrase( F, phraseCandidate ) );
-    // "a b c"
-    phraseCandidate.add( new TermInfo( "c", 4, 5, 2 ) );
-    assertNotNull( fq.searchPhrase( F, phraseCandidate ) );
-    assertNull( fq.searchPhrase( "x", phraseCandidate ) );
-
-    // phraseHighlight = true, fieldMatch = false
-    fq = new FieldQuery( query, true, false );
-    
-    // "a b c"
-    assertNotNull( fq.searchPhrase( F, phraseCandidate ) );
-    assertNotNull( fq.searchPhrase( "x", phraseCandidate ) );
-
-    // phraseHighlight = false, fieldMatch = true
-    fq = new FieldQuery( query, false, true );
-    
-    // "a"
-    phraseCandidate.clear();
-    phraseCandidate.add( new TermInfo( "a", 0, 1, 0 ) );
-    assertNotNull( fq.searchPhrase( F, phraseCandidate ) );
-    // "a b"
-    phraseCandidate.add( new TermInfo( "b", 2, 3, 1 ) );
-    assertNull( fq.searchPhrase( F, phraseCandidate ) );
-    // "a b c"
-    phraseCandidate.add( new TermInfo( "c", 4, 5, 2 ) );
-    assertNotNull( fq.searchPhrase( F, phraseCandidate ) );
-    assertNull( fq.searchPhrase( "x", phraseCandidate ) );
-  }
-  
-  public void testSearchPhraseSlop() throws Exception {
-    // "a b c"~0
-    Query query = pqF( "a", "b", "c" );
-
-    // phraseHighlight = true, fieldMatch = true
-    FieldQuery fq = new FieldQuery( query, true, true );
-    
-    // "a b c" w/ position-gap = 2
-    List<TermInfo> phraseCandidate = new ArrayList<TermInfo>();
-    phraseCandidate.add( new TermInfo( "a", 0, 1, 0 ) );
-    phraseCandidate.add( new TermInfo( "b", 2, 3, 2 ) );
-    phraseCandidate.add( new TermInfo( "c", 4, 5, 4 ) );
-    assertNull( fq.searchPhrase( F, phraseCandidate ) );
-
-    // "a b c"~1
-    query = pqF( 1F, 1, "a", "b", "c" );
-
-    // phraseHighlight = true, fieldMatch = true
-    fq = new FieldQuery( query, true, true );
-    
-    // "a b c" w/ position-gap = 2
-    assertNotNull( fq.searchPhrase( F, phraseCandidate ) );
-    
-    // "a b c" w/ position-gap = 3
-    phraseCandidate.clear();
-    phraseCandidate.add( new TermInfo( "a", 0, 1, 0 ) );
-    phraseCandidate.add( new TermInfo( "b", 2, 3, 3 ) );
-    phraseCandidate.add( new TermInfo( "c", 4, 5, 6 ) );
-    assertNull( fq.searchPhrase( F, phraseCandidate ) );
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldTermStackTest.java b/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldTermStackTest.java
deleted file mode 100644
index e434388..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldTermStackTest.java
+++ /dev/null
@@ -1,161 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.BooleanClause.Occur;
-
-public class FieldTermStackTest extends AbstractTestCase {
-  
-  public void test1Term() throws Exception {
-    makeIndex();
-    
-    FieldQuery fq = new FieldQuery( tq( "a" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 6, stack.termList.size() );
-    assertEquals( "a(0,1,0)", stack.pop().toString() );
-    assertEquals( "a(2,3,1)", stack.pop().toString() );
-    assertEquals( "a(4,5,2)", stack.pop().toString() );
-    assertEquals( "a(12,13,6)", stack.pop().toString() );
-    assertEquals( "a(28,29,14)", stack.pop().toString() );
-    assertEquals( "a(32,33,16)", stack.pop().toString() );
-  }
-  
-  public void test2Terms() throws Exception {
-    makeIndex();
-    
-    BooleanQuery query = new BooleanQuery();
-    query.add( tq( "b" ), Occur.SHOULD );
-    query.add( tq( "c" ), Occur.SHOULD );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 8, stack.termList.size() );
-    assertEquals( "b(6,7,3)", stack.pop().toString() );
-    assertEquals( "b(8,9,4)", stack.pop().toString() );
-    assertEquals( "c(10,11,5)", stack.pop().toString() );
-    assertEquals( "b(14,15,7)", stack.pop().toString() );
-    assertEquals( "b(16,17,8)", stack.pop().toString() );
-    assertEquals( "c(18,19,9)", stack.pop().toString() );
-    assertEquals( "b(26,27,13)", stack.pop().toString() );
-    assertEquals( "b(30,31,15)", stack.pop().toString() );
-  }
-  
-  public void test1Phrase() throws Exception {
-    makeIndex();
-    
-    FieldQuery fq = new FieldQuery( pqF( "c", "d" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 3, stack.termList.size() );
-    assertEquals( "c(10,11,5)", stack.pop().toString() );
-    assertEquals( "c(18,19,9)", stack.pop().toString() );
-    assertEquals( "d(20,21,10)", stack.pop().toString() );
-  }
-  
-  private void makeIndex() throws Exception {
-    //           111111111122222
-    // 0123456789012345678901234 (offsets)
-    // a a a b b c a b b c d e f
-    // 0 1 2 3 4 5 6 7 8 9101112 (position)
-    String value1 = "a a a b b c a b b c d e f";
-    // 222233333
-    // 678901234 (offsets)
-    // b a b a f
-    //1314151617 (position)
-    String value2 = "b a b a f";
-    
-    make1dmfIndex( value1, value2 );
-  }
-  
-  public void test1TermB() throws Exception {
-    makeIndexB();
-    
-    FieldQuery fq = new FieldQuery( tq( "ab" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 2, stack.termList.size() );
-    assertEquals( "ab(2,4,2)", stack.pop().toString() );
-    assertEquals( "ab(6,8,6)", stack.pop().toString() );
-  }
-  
-  public void test2TermsB() throws Exception {
-    makeIndexB();
-    
-    BooleanQuery query = new BooleanQuery();
-    query.add( tq( "bc" ), Occur.SHOULD );
-    query.add( tq( "ef" ), Occur.SHOULD );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 3, stack.termList.size() );
-    assertEquals( "bc(4,6,4)", stack.pop().toString() );
-    assertEquals( "bc(8,10,8)", stack.pop().toString() );
-    assertEquals( "ef(11,13,11)", stack.pop().toString() );
-  }
-  
-  public void test1PhraseB() throws Exception {
-    makeIndexB();
-    
-    FieldQuery fq = new FieldQuery( pqF( "ab", "bb" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 4, stack.termList.size() );
-    assertEquals( "ab(2,4,2)", stack.pop().toString() );
-    assertEquals( "bb(3,5,3)", stack.pop().toString() );
-    assertEquals( "ab(6,8,6)", stack.pop().toString() );
-    assertEquals( "bb(7,9,7)", stack.pop().toString() );
-  }
-  
-  private void makeIndexB() throws Exception {
-    //                             1 11 11
-    // 01 12 23 34 45 56 67 78 89 90 01 12 (offsets)
-    // aa|aa|ab|bb|bc|ca|ab|bb|bc|cd|de|ef
-    //  0  1  2  3  4  5  6  7  8  9 10 11 (position)
-    String value = "aaabbcabbcdef";
-    
-    make1dmfIndexB( value );
-  }
-  
-  public void test1PhraseShortMV() throws Exception {
-    makeIndexShortMV();
-    
-    FieldQuery fq = new FieldQuery( tq( "d" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 1, stack.termList.size() );
-    assertEquals( "d(6,7,3)", stack.pop().toString() );
-  }
-  
-  public void test1PhraseLongMV() throws Exception {
-    makeIndexLongMV();
-    
-    FieldQuery fq = new FieldQuery( pqF( "search", "engines" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 4, stack.termList.size() );
-    assertEquals( "search(102,108,14)", stack.pop().toString() );
-    assertEquals( "engines(109,116,15)", stack.pop().toString() );
-    assertEquals( "search(157,163,24)", stack.pop().toString() );
-    assertEquals( "engines(164,171,25)", stack.pop().toString() );
-  }
-
-  public void test1PhraseMVB() throws Exception {
-    makeIndexLongMVB();
-    
-    FieldQuery fq = new FieldQuery( pqF( "sp", "pe", "ee", "ed" ), true, true ); // "speed" -(2gram)-> "sp","pe","ee","ed"
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 4, stack.termList.size() );
-    assertEquals( "sp(88,90,61)", stack.pop().toString() );
-    assertEquals( "pe(89,91,62)", stack.pop().toString() );
-    assertEquals( "ee(90,92,63)", stack.pop().toString() );
-    assertEquals( "ed(91,93,64)", stack.pop().toString() );
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.java b/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.java
deleted file mode 100644
index 1134d38..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.java
+++ /dev/null
@@ -1,318 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.Reader;
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.util.AttributeImpl;
-
-public class IndexTimeSynonymTest extends AbstractTestCase {
-  
-  public void testFieldTermStackIndex1wSearch1term() throws Exception {
-    makeIndex1w();
-    
-    FieldQuery fq = new FieldQuery( tq( "Mac" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 1, stack.termList.size() );
-    assertEquals( "Mac(11,20,3)", stack.pop().toString() );
-  }
-  
-  public void testFieldTermStackIndex1wSearch2terms() throws Exception {
-    makeIndex1w();
-
-    BooleanQuery bq = new BooleanQuery();
-    bq.add( tq( "Mac" ), Occur.SHOULD );
-    bq.add( tq( "MacBook" ), Occur.SHOULD );
-    FieldQuery fq = new FieldQuery( bq, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 2, stack.termList.size() );
-    Set<String> expectedSet = new HashSet<String>();
-    expectedSet.add( "Mac(11,20,3)" );
-    expectedSet.add( "MacBook(11,20,3)" );
-    assertTrue( expectedSet.contains( stack.pop().toString() ) );
-    assertTrue( expectedSet.contains( stack.pop().toString() ) );
-  }
-  
-  public void testFieldTermStackIndex1w2wSearch1term() throws Exception {
-    makeIndex1w2w();
-    
-    FieldQuery fq = new FieldQuery( tq( "pc" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 1, stack.termList.size() );
-    assertEquals( "pc(3,5,1)", stack.pop().toString() );
-  }
-  
-  public void testFieldTermStackIndex1w2wSearch1phrase() throws Exception {
-    makeIndex1w2w();
-    
-    FieldQuery fq = new FieldQuery( pqF( "personal", "computer" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 2, stack.termList.size() );
-    assertEquals( "personal(3,5,1)", stack.pop().toString() );
-    assertEquals( "computer(3,5,2)", stack.pop().toString() );
-  }
-  
-  public void testFieldTermStackIndex1w2wSearch1partial() throws Exception {
-    makeIndex1w2w();
-    
-    FieldQuery fq = new FieldQuery( tq( "computer" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 1, stack.termList.size() );
-    assertEquals( "computer(3,5,2)", stack.pop().toString() );
-  }
-  
-  public void testFieldTermStackIndex1w2wSearch1term1phrase() throws Exception {
-    makeIndex1w2w();
-
-    BooleanQuery bq = new BooleanQuery();
-    bq.add( tq( "pc" ), Occur.SHOULD );
-    bq.add( pqF( "personal", "computer" ), Occur.SHOULD );
-    FieldQuery fq = new FieldQuery( bq, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 3, stack.termList.size() );
-    Set<String> expectedSet = new HashSet<String>();
-    expectedSet.add( "pc(3,5,1)" );
-    expectedSet.add( "personal(3,5,1)" );
-    assertTrue( expectedSet.contains( stack.pop().toString() ) );
-    assertTrue( expectedSet.contains( stack.pop().toString() ) );
-    assertEquals( "computer(3,5,2)", stack.pop().toString() );
-  }
-  
-  public void testFieldTermStackIndex2w1wSearch1term() throws Exception {
-    makeIndex2w1w();
-    
-    FieldQuery fq = new FieldQuery( tq( "pc" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 1, stack.termList.size() );
-    assertEquals( "pc(3,20,1)", stack.pop().toString() );
-  }
-  
-  public void testFieldTermStackIndex2w1wSearch1phrase() throws Exception {
-    makeIndex2w1w();
-    
-    FieldQuery fq = new FieldQuery( pqF( "personal", "computer" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 2, stack.termList.size() );
-    assertEquals( "personal(3,20,1)", stack.pop().toString() );
-    assertEquals( "computer(3,20,2)", stack.pop().toString() );
-  }
-  
-  public void testFieldTermStackIndex2w1wSearch1partial() throws Exception {
-    makeIndex2w1w();
-    
-    FieldQuery fq = new FieldQuery( tq( "computer" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 1, stack.termList.size() );
-    assertEquals( "computer(3,20,2)", stack.pop().toString() );
-  }
-  
-  public void testFieldTermStackIndex2w1wSearch1term1phrase() throws Exception {
-    makeIndex2w1w();
-
-    BooleanQuery bq = new BooleanQuery();
-    bq.add( tq( "pc" ), Occur.SHOULD );
-    bq.add( pqF( "personal", "computer" ), Occur.SHOULD );
-    FieldQuery fq = new FieldQuery( bq, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    assertEquals( 3, stack.termList.size() );
-    Set<String> expectedSet = new HashSet<String>();
-    expectedSet.add( "pc(3,20,1)" );
-    expectedSet.add( "personal(3,20,1)" );
-    assertTrue( expectedSet.contains( stack.pop().toString() ) );
-    assertTrue( expectedSet.contains( stack.pop().toString() ) );
-    assertEquals( "computer(3,20,2)", stack.pop().toString() );
-  }
-  
-  public void testFieldPhraseListIndex1w2wSearch1phrase() throws Exception {
-    makeIndex1w2w();
-    
-    FieldQuery fq = new FieldQuery( pqF( "personal", "computer" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "personalcomputer(1.0)((3,5))", fpl.phraseList.get( 0 ).toString() );
-    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
-    assertEquals( 5, fpl.phraseList.get( 0 ).getEndOffset() );
-  }
-  
-  public void testFieldPhraseListIndex1w2wSearch1partial() throws Exception {
-    makeIndex1w2w();
-    
-    FieldQuery fq = new FieldQuery( tq( "computer" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "computer(1.0)((3,5))", fpl.phraseList.get( 0 ).toString() );
-    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
-    assertEquals( 5, fpl.phraseList.get( 0 ).getEndOffset() );
-  }
-  
-  public void testFieldPhraseListIndex1w2wSearch1term1phrase() throws Exception {
-    makeIndex1w2w();
-
-    BooleanQuery bq = new BooleanQuery();
-    bq.add( tq( "pc" ), Occur.SHOULD );
-    bq.add( pqF( "personal", "computer" ), Occur.SHOULD );
-    FieldQuery fq = new FieldQuery( bq, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertTrue( fpl.phraseList.get( 0 ).toString().indexOf( "(1.0)((3,5))" ) > 0 );
-    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
-    assertEquals( 5, fpl.phraseList.get( 0 ).getEndOffset() );
-  }
-  
-  public void testFieldPhraseListIndex2w1wSearch1term() throws Exception {
-    makeIndex2w1w();
-    
-    FieldQuery fq = new FieldQuery( tq( "pc" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "pc(1.0)((3,20))", fpl.phraseList.get( 0 ).toString() );
-    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
-    assertEquals( 20, fpl.phraseList.get( 0 ).getEndOffset() );
-  }
-  
-  public void testFieldPhraseListIndex2w1wSearch1phrase() throws Exception {
-    makeIndex2w1w();
-    
-    FieldQuery fq = new FieldQuery( pqF( "personal", "computer" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "personalcomputer(1.0)((3,20))", fpl.phraseList.get( 0 ).toString() );
-    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
-    assertEquals( 20, fpl.phraseList.get( 0 ).getEndOffset() );
-  }
-  
-  public void testFieldPhraseListIndex2w1wSearch1partial() throws Exception {
-    makeIndex2w1w();
-    
-    FieldQuery fq = new FieldQuery( tq( "computer" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertEquals( "computer(1.0)((3,20))", fpl.phraseList.get( 0 ).toString() );
-    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
-    assertEquals( 20, fpl.phraseList.get( 0 ).getEndOffset() );
-  }
-  
-  public void testFieldPhraseListIndex2w1wSearch1term1phrase() throws Exception {
-    makeIndex2w1w();
-
-    BooleanQuery bq = new BooleanQuery();
-    bq.add( tq( "pc" ), Occur.SHOULD );
-    bq.add( pqF( "personal", "computer" ), Occur.SHOULD );
-    FieldQuery fq = new FieldQuery( bq, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    assertEquals( 1, fpl.phraseList.size() );
-    assertTrue( fpl.phraseList.get( 0 ).toString().indexOf( "(1.0)((3,20))" ) > 0 );
-    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
-    assertEquals( 20, fpl.phraseList.get( 0 ).getEndOffset() );
-  }
-
-  private void makeIndex1w() throws Exception {
-    //           11111111112
-    // 012345678901234567890
-    // I'll buy a Macintosh
-    //            Mac
-    //            MacBook
-    // 0    1   2 3
-    makeSynonymIndex( "I'll buy a Macintosh",
-        t("I'll",0,4),
-        t("buy",5,8),
-        t("a",9,10),
-        t("Macintosh",11,20),t("Mac",11,20,0),t("MacBook",11,20,0));
-  }
-
-  private void makeIndex1w2w() throws Exception {
-    //           1111111
-    // 01234567890123456
-    // My pc was broken
-    //    personal computer
-    // 0  1  2   3
-    makeSynonymIndex( "My pc was broken",
-        t("My",0,2),
-        t("pc",3,5),t("personal",3,5,0),t("computer",3,5),
-        t("was",6,9),
-        t("broken",10,16));
-  }
-
-  private void makeIndex2w1w() throws Exception {
-    //           1111111111222222222233
-    // 01234567890123456789012345678901
-    // My personal computer was broken
-    //    pc
-    // 0  1        2        3   4
-    makeSynonymIndex( "My personal computer was broken",
-        t("My",0,2),
-        t("personal",3,20),t("pc",3,20,0),t("computer",3,20),
-        t("was",21,24),
-        t("broken",25,31));
-  }
-  
-  void makeSynonymIndex( String value, Token... tokens ) throws Exception {
-    Analyzer analyzer = new TokenArrayAnalyzer( tokens );
-    make1dmfIndex( analyzer, value );
-  }
-
-  public static Token t( String text, int startOffset, int endOffset ){
-    return t( text, startOffset, endOffset, 1 );
-  }
-  
-  public static Token t( String text, int startOffset, int endOffset, int positionIncrement ){
-    Token token = new Token( text, startOffset, endOffset );
-    token.setPositionIncrement( positionIncrement );
-    return token;
-  }
-  
-  public static class TokenArrayAnalyzer extends Analyzer {
-    Token[] tokens;
-    public TokenArrayAnalyzer( Token... tokens ){
-      this.tokens = tokens;
-    }
-    
-    @Override
-    public TokenStream tokenStream(String fieldName, Reader reader) {      
-      TokenStream ts = new TokenStream(Token.TOKEN_ATTRIBUTE_FACTORY) {
-        final AttributeImpl reusableToken = (AttributeImpl) addAttribute(TermAttribute.class);
-        int p = 0;
-        
-        @Override
-        public boolean incrementToken() throws IOException {
-          if( p >= tokens.length ) return false;
-          clearAttributes();
-          tokens[p++].copyTo(reusableToken);
-          return true;
-        }
-      };
-      return ts;
-    }
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilderTest.java b/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilderTest.java
deleted file mode 100644
index 47ca7ed..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilderTest.java
+++ /dev/null
@@ -1,43 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.search.Query;
-
-public class ScoreOrderFragmentsBuilderTest extends AbstractTestCase {
-  
-  public void test3Frags() throws Exception {
-    FieldFragList ffl = ffl( "a c", "a b b b b b b b b b b b a b a b b b b b c a a b b" );
-    ScoreOrderFragmentsBuilder sofb = new ScoreOrderFragmentsBuilder();
-    String[] f = sofb.createFragments( reader, 0, F, ffl, 3 );
-    assertEquals( 3, f.length );
-    // check score order
-    assertEquals( "<b>c</b> <b>a</b> <b>a</b> b b", f[0] );
-    assertEquals( "b b <b>a</b> b <b>a</b> b b b b b ", f[1] );
-    assertEquals( "<b>a</b> b b b b b b b b b ", f[2] );
-  }
-
-  private FieldFragList ffl( String queryValue, String indexValue ) throws Exception {
-    make1d1fIndex( indexValue );
-    Query query = paW.parse( queryValue );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    return new SimpleFragListBuilder().createFieldFragList( fpl, 20 );
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilderTest.java b/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilderTest.java
deleted file mode 100644
index 59f7aba..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilderTest.java
+++ /dev/null
@@ -1,172 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.search.Query;
-
-public class SimpleFragListBuilderTest extends AbstractTestCase {
-  
-  public void testNullFieldFragList() throws Exception {
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl( "a", "b c d" ), 100 );
-    assertEquals( 0, ffl.fragInfos.size() );
-  }
-  
-  public void testTooSmallFragSize() throws Exception {
-    try{
-      SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-      sflb.createFieldFragList( fpl( "a", "b c d" ), SimpleFragListBuilder.MIN_FRAG_CHAR_SIZE - 1 );
-      fail( "IllegalArgumentException must be thrown" );
-    }
-    catch ( IllegalArgumentException expected ) {
-    }
-  }
-  
-  public void testSmallerFragSizeThanTermQuery() throws Exception {
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl( "abcdefghijklmnopqrs", "abcdefghijklmnopqrs" ), SimpleFragListBuilder.MIN_FRAG_CHAR_SIZE );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(abcdefghijklmnopqrs((0,19)))/1.0(0,19)", ffl.fragInfos.get( 0 ).toString() );
-  }
-  
-  public void testSmallerFragSizeThanPhraseQuery() throws Exception {
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl( "\"abcdefgh jklmnopqrs\"", "abcdefgh   jklmnopqrs" ), SimpleFragListBuilder.MIN_FRAG_CHAR_SIZE );
-    assertEquals( 1, ffl.fragInfos.size() );
-    if (VERBOSE) System.out.println( ffl.fragInfos.get( 0 ).toString() );
-    assertEquals( "subInfos=(abcdefghjklmnopqrs((0,21)))/1.0(0,21)", ffl.fragInfos.get( 0 ).toString() );
-  }
-  
-  public void test1TermIndex() throws Exception {
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl( "a", "a" ), 100 );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(a((0,1)))/1.0(0,100)", ffl.fragInfos.get( 0 ).toString() );
-  }
-  
-  public void test2TermsIndex1Frag() throws Exception {
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl( "a", "a a" ), 100 );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(a((0,1))a((2,3)))/2.0(0,100)", ffl.fragInfos.get( 0 ).toString() );
-
-    ffl = sflb.createFieldFragList( fpl( "a", "a b b b b b b b b a" ), 20 );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(a((0,1))a((18,19)))/2.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
-    
-    ffl = sflb.createFieldFragList( fpl( "a", "b b b b a b b b b a" ), 20 );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(a((8,9))a((18,19)))/2.0(2,22)", ffl.fragInfos.get( 0 ).toString() );
-  }
-  
-  public void test2TermsIndex2Frags() throws Exception {
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl( "a", "a b b b b b b b b b b b b b a" ), 20 );
-    assertEquals( 2, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(a((0,1)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
-    assertEquals( "subInfos=(a((28,29)))/1.0(22,42)", ffl.fragInfos.get( 1 ).toString() );
-
-    ffl = sflb.createFieldFragList( fpl( "a", "a b b b b b b b b b b b b a" ), 20 );
-    assertEquals( 2, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(a((0,1)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
-    assertEquals( "subInfos=(a((26,27)))/1.0(20,40)", ffl.fragInfos.get( 1 ).toString() );
-
-    ffl = sflb.createFieldFragList( fpl( "a", "a b b b b b b b b b a" ), 20 );
-    assertEquals( 2, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(a((0,1)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
-    assertEquals( "subInfos=(a((20,21)))/1.0(20,40)", ffl.fragInfos.get( 1 ).toString() );
-  }
-  
-  public void test2TermsQuery() throws Exception {
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl( "a b", "c d e" ), 20 );
-    assertEquals( 0, ffl.fragInfos.size() );
-
-    ffl = sflb.createFieldFragList( fpl( "a b", "d b c" ), 20 );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(b((2,3)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
-
-    ffl = sflb.createFieldFragList( fpl( "a b", "a b c" ), 20 );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(a((0,1))b((2,3)))/2.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
-  }
-  
-  public void testPhraseQuery() throws Exception {
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl( "\"a b\"", "c d e" ), 20 );
-    assertEquals( 0, ffl.fragInfos.size() );
-
-    ffl = sflb.createFieldFragList( fpl( "\"a b\"", "a c b" ), 20 );
-    assertEquals( 0, ffl.fragInfos.size() );
-
-    ffl = sflb.createFieldFragList( fpl( "\"a b\"", "a b c" ), 20 );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(ab((0,3)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
-  }
-  
-  public void testPhraseQuerySlop() throws Exception {
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl( "\"a b\"~1", "a c b" ), 20 );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(ab((0,1)(4,5)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
-  }
-
-  private FieldPhraseList fpl( String queryValue, String indexValue ) throws Exception {
-    make1d1fIndex( indexValue );
-    Query query = paW.parse( queryValue );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    return new FieldPhraseList( stack, fq );
-  }
-  
-  public void test1PhraseShortMV() throws Exception {
-    makeIndexShortMV();
-
-    FieldQuery fq = new FieldQuery( tq( "d" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(d((6,7)))/1.0(0,100)", ffl.fragInfos.get( 0 ).toString() );
-  }
-  
-  public void test1PhraseLongMV() throws Exception {
-    makeIndexLongMV();
-
-    FieldQuery fq = new FieldQuery( pqF( "search", "engines" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(searchengines((102,116))searchengines((157,171)))/2.0(96,196)", ffl.fragInfos.get( 0 ).toString() );
-  }
-
-  public void test1PhraseLongMVB() throws Exception {
-    makeIndexLongMVB();
-
-    FieldQuery fq = new FieldQuery( pqF( "sp", "pe", "ee", "ed" ), true, true ); // "speed" -(2gram)-> "sp","pe","ee","ed"
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
-    assertEquals( 1, ffl.fragInfos.size() );
-    assertEquals( "subInfos=(sppeeeed((88,93)))/1.0(82,182)", ffl.fragInfos.get( 0 ).toString() );
-  }
-}
diff --git a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java b/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java
deleted file mode 100644
index cf5bafe..0000000
--- a/lucene/contrib/fast-vector-highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java
+++ /dev/null
@@ -1,143 +0,0 @@
-package org.apache.lucene.search.vectorhighlight;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Field.Index;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.Field.TermVector;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.search.Query;
-
-public class SimpleFragmentsBuilderTest extends AbstractTestCase {
-  
-  public void test1TermIndex() throws Exception {
-    FieldFragList ffl = ffl( "a", "a" );
-    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
-    assertEquals( "<b>a</b>", sfb.createFragment( reader, 0, F, ffl ) );
-
-    // change tags
-    sfb = new SimpleFragmentsBuilder( new String[]{ "[" }, new String[]{ "]" } );
-    assertEquals( "[a]", sfb.createFragment( reader, 0, F, ffl ) );
-  }
-  
-  public void test2Frags() throws Exception {
-    FieldFragList ffl = ffl( "a", "a b b b b b b b b b b b a b a b" );
-    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
-    String[] f = sfb.createFragments( reader, 0, F, ffl, 3 );
-    // 3 snippets requested, but should be 2
-    assertEquals( 2, f.length );
-    assertEquals( "<b>a</b> b b b b b b b b b ", f[0] );
-    assertEquals( "b b <b>a</b> b <b>a</b> b", f[1] );
-  }
-  
-  public void test3Frags() throws Exception {
-    FieldFragList ffl = ffl( "a c", "a b b b b b b b b b b b a b a b b b b b c a a b b" );
-    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
-    String[] f = sfb.createFragments( reader, 0, F, ffl, 3 );
-    assertEquals( 3, f.length );
-    assertEquals( "<b>a</b> b b b b b b b b b ", f[0] );
-    assertEquals( "b b <b>a</b> b <b>a</b> b b b b b ", f[1] );
-    assertEquals( "<b>c</b> <b>a</b> <b>a</b> b b", f[2] );
-  }
-
-  private FieldFragList ffl( String queryValue, String indexValue ) throws Exception {
-    make1d1fIndex( indexValue );
-    Query query = paW.parse( queryValue );
-    FieldQuery fq = new FieldQuery( query, true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    return new SimpleFragListBuilder().createFieldFragList( fpl, 20 );
-  }
-  
-  public void test1PhraseShortMV() throws Exception {
-    makeIndexShortMV();
-
-    FieldQuery fq = new FieldQuery( tq( "d" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
-    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
-    assertEquals( "a b c <b>d</b> e", sfb.createFragment( reader, 0, F, ffl ) );
-  }
-  
-  public void test1PhraseLongMV() throws Exception {
-    makeIndexLongMV();
-
-    FieldQuery fq = new FieldQuery( pqF( "search", "engines" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
-    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
-    assertEquals( " most <b>search engines</b> use only one of these methods. Even the <b>search engines</b> that says they can use t",
-        sfb.createFragment( reader, 0, F, ffl ) );
-  }
-
-  public void test1PhraseLongMVB() throws Exception {
-    makeIndexLongMVB();
-
-    FieldQuery fq = new FieldQuery( pqF( "sp", "pe", "ee", "ed" ), true, true ); // "speed" -(2gram)-> "sp","pe","ee","ed"
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
-    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
-    assertEquals( "ssing <b>speed</b>, the", sfb.createFragment( reader, 0, F, ffl ) );
-  }
-  
-  public void testUnstoredField() throws Exception {
-    makeUnstoredIndex();
-
-    FieldQuery fq = new FieldQuery( tq( "aaa" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
-    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
-    assertNull( sfb.createFragment( reader, 0, F, ffl ) );
-  }
-  
-  protected void makeUnstoredIndex() throws Exception {
-    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, analyzerW).setOpenMode(OpenMode.CREATE));
-    Document doc = new Document();
-    doc.add( new Field( F, "aaa", Store.NO, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
-    writer.addDocument( doc );
-    writer.close();
-
-    reader = IndexReader.open( dir, true );
-  }
-  
-  public void test1StrMV() throws Exception {
-    makeIndexStrMV();
-
-    FieldQuery fq = new FieldQuery( tq( "defg" ), true, true );
-    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
-    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
-    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
-    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
-    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
-    assertEquals( "abc<b>defg</b>hijkl", sfb.createFragment( reader, 0, F, ffl ) );
-  }
-}
diff --git a/lucene/contrib/highlighter/build.xml b/lucene/contrib/highlighter/build.xml
index bcaee11..cd12329 100644
--- a/lucene/contrib/highlighter/build.xml
+++ b/lucene/contrib/highlighter/build.xml
@@ -28,25 +28,25 @@
   <property name="memory.jar" location="${common.dir}/build/contrib/memory/lucene-memory-${version}.jar"/>
   <available property="memory.jar.present" type="file" file="${memory.jar}"/>
   
-  <property name="regex.jar" location="${common.dir}/build/contrib/regex/lucene-regex-${version}.jar"/>
-  <available property="regex.jar.present" type="file" file="${regex.jar}"/>
+  <property name="queries.jar" location="${common.dir}/build/contrib/queries/lucene-queries-${version}.jar"/>
+  <available property="queries.jar.present" type="file" file="${queries.jar}"/>
 
   <path id="classpath">
     <pathelement path="${lucene.jar}"/>
     <pathelement path="${memory.jar}"/>
-    <pathelement path="${regex.jar}"/>
+    <pathelement path="${queries.jar}"/>
     <pathelement path="${project.classpath}"/>
   </path>
 
-  <target name="compile-core" depends="build-memory, build-regex, common.compile-core" />
+  <target name="compile-core" depends="build-memory, build-queries, common.compile-core" />
 
   <target name="build-memory" unless="memory.jar.present">
     <echo>Highlighter building dependency ${memory.jar}</echo>
     <ant antfile="../memory/build.xml" target="default" inheritall="false" dir="../memory" />
   </target>
   
-  <target name="build-regex" unless="regex.jar.present">
-    <echo>Highlighter building dependency ${regex.jar}</echo>
-    <ant antfile="../regex/build.xml" target="default" inheritall="false" dir="../regex" />
+  <target name="build-queries" unless="queries.jar.present">
+    <echo>Highlighter building dependency ${queries.jar}</echo>
+    <ant antfile="../queries/build.xml" target="default" inheritall="false" dir="../queries" />
   </target>
 </project>
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java
new file mode 100644
index 0000000..9b22433
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/BaseFragmentsBuilder.java
@@ -0,0 +1,154 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.MapFieldSelector;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.vectorhighlight.FieldFragList.WeightedFragInfo;
+import org.apache.lucene.search.vectorhighlight.FieldFragList.WeightedFragInfo.SubInfo;
+import org.apache.lucene.search.vectorhighlight.FieldPhraseList.WeightedPhraseInfo.Toffs;
+
+public abstract class BaseFragmentsBuilder implements FragmentsBuilder {
+
+  protected String[] preTags, postTags;
+  public static final String[] COLORED_PRE_TAGS = {
+    "<b style=\"background:yellow\">", "<b style=\"background:lawngreen\">", "<b style=\"background:aquamarine\">",
+    "<b style=\"background:magenta\">", "<b style=\"background:palegreen\">", "<b style=\"background:coral\">",
+    "<b style=\"background:wheat\">", "<b style=\"background:khaki\">", "<b style=\"background:lime\">",
+    "<b style=\"background:deepskyblue\">"
+  };
+  public static final String[] COLORED_POST_TAGS = { "</b>" };
+  
+  protected BaseFragmentsBuilder(){
+    this( new String[]{ "<b>" }, new String[]{ "</b>" } );
+  }
+  
+  protected BaseFragmentsBuilder( String[] preTags, String[] postTags ){
+    this.preTags = preTags;
+    this.postTags = postTags;
+  }
+  
+  static Object checkTagsArgument( Object tags ){
+    if( tags instanceof String ) return tags;
+    else if( tags instanceof String[] ) return tags;
+    throw new IllegalArgumentException( "type of preTags/postTags must be a String or String[]" );
+  }
+  
+  public abstract List<WeightedFragInfo> getWeightedFragInfoList( List<WeightedFragInfo> src );
+  
+  public String createFragment( IndexReader reader, int docId,
+      String fieldName, FieldFragList fieldFragList ) throws IOException {
+    String[] fragments = createFragments( reader, docId, fieldName, fieldFragList, 1 );
+    if( fragments == null || fragments.length == 0 ) return null;
+    return fragments[0];
+  }
+
+  public String[] createFragments( IndexReader reader, int docId,
+      String fieldName, FieldFragList fieldFragList, int maxNumFragments )
+      throws IOException {
+    if( maxNumFragments < 0 )
+      throw new IllegalArgumentException( "maxNumFragments(" + maxNumFragments + ") must be positive number." );
+
+    List<WeightedFragInfo> fragInfos = getWeightedFragInfoList( fieldFragList.fragInfos );
+    
+    List<String> fragments = new ArrayList<String>( maxNumFragments );
+    Field[] values = getFields( reader, docId, fieldName );
+    if( values.length == 0 ) return null;
+    StringBuilder buffer = new StringBuilder();
+    int[] nextValueIndex = { 0 };
+    for( int n = 0; n < maxNumFragments && n < fragInfos.size(); n++ ){
+      WeightedFragInfo fragInfo = fragInfos.get( n );
+      fragments.add( makeFragment( buffer, nextValueIndex, values, fragInfo ) );
+    }
+    return fragments.toArray( new String[fragments.size()] );
+  }
+  
+  @Deprecated
+  protected String[] getFieldValues( IndexReader reader, int docId, String fieldName) throws IOException {
+    Document doc = reader.document( docId, new MapFieldSelector( new String[]{ fieldName } ) );
+    return doc.getValues( fieldName ); // according to Document class javadoc, this never returns null
+  }
+  
+  protected Field[] getFields( IndexReader reader, int docId, String fieldName) throws IOException {
+    // according to javadoc, doc.getFields(fieldName) cannot be used with lazy loaded field???
+    Document doc = reader.document( docId, new MapFieldSelector( new String[]{ fieldName } ) );
+    return doc.getFields( fieldName ); // according to Document class javadoc, this never returns null
+  }
+
+  @Deprecated
+  protected String makeFragment( StringBuilder buffer, int[] index, String[] values, WeightedFragInfo fragInfo ){
+    final int s = fragInfo.startOffset;
+    return makeFragment( fragInfo, getFragmentSource( buffer, index, values, s, fragInfo.endOffset ), s );
+  }
+
+  protected String makeFragment( StringBuilder buffer, int[] index, Field[] values, WeightedFragInfo fragInfo ){
+    final int s = fragInfo.startOffset;
+    return makeFragment( fragInfo, getFragmentSource( buffer, index, values, s, fragInfo.endOffset ), s );
+  }
+  
+  private String makeFragment( WeightedFragInfo fragInfo, String src, int s ){
+    StringBuilder fragment = new StringBuilder();
+    int srcIndex = 0;
+    for( SubInfo subInfo : fragInfo.subInfos ){
+      for( Toffs to : subInfo.termsOffsets ){
+        fragment.append( src.substring( srcIndex, to.startOffset - s ) ).append( getPreTag( subInfo.seqnum ) )
+          .append( src.substring( to.startOffset - s, to.endOffset - s ) ).append( getPostTag( subInfo.seqnum ) );
+        srcIndex = to.endOffset - s;
+      }
+    }
+    fragment.append( src.substring( srcIndex ) );
+    return fragment.toString();
+  }
+  
+  @Deprecated
+  protected String getFragmentSource( StringBuilder buffer, int[] index, String[] values,
+      int startOffset, int endOffset ){
+    while( buffer.length() < endOffset && index[0] < values.length ){
+      if( index[0] > 0 && values[index[0]].length() > 0 )
+        buffer.append( ' ' );
+      buffer.append( values[index[0]++] );
+    }
+    int eo = buffer.length() < endOffset ? buffer.length() : endOffset;
+    return buffer.substring( startOffset, eo );
+  }
+
+  protected String getFragmentSource( StringBuilder buffer, int[] index, Field[] values,
+      int startOffset, int endOffset ){
+    while( buffer.length() < endOffset && index[0] < values.length ){
+      if( index[0] > 0 && values[index[0]].isTokenized() && values[index[0]].stringValue().length() > 0 )
+        buffer.append( ' ' );
+      buffer.append( values[index[0]++].stringValue() );
+    }
+    int eo = buffer.length() < endOffset ? buffer.length() : endOffset;
+    return buffer.substring( startOffset, eo );
+  }
+  
+  protected String getPreTag( int num ){
+    return preTags.length > num ? preTags[num] : preTags[0];
+  }
+  
+  protected String getPostTag( int num ){
+    return postTags.length > num ? postTags[num] : postTags[0];
+  }
+}
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FastVectorHighlighter.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FastVectorHighlighter.java
new file mode 100644
index 0000000..2df78b3
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FastVectorHighlighter.java
@@ -0,0 +1,137 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.Query;
+
+/**
+ * Another highlighter implementation.
+ *
+ */
+public class FastVectorHighlighter {
+
+  public static final boolean DEFAULT_PHRASE_HIGHLIGHT = true;
+  public static final boolean DEFAULT_FIELD_MATCH = true;
+  private final boolean phraseHighlight;
+  private final boolean fieldMatch;
+  private final FragListBuilder fragListBuilder;
+  private final FragmentsBuilder fragmentsBuilder;
+
+  /**
+   * the default constructor.
+   */
+  public FastVectorHighlighter(){
+    this( DEFAULT_PHRASE_HIGHLIGHT, DEFAULT_FIELD_MATCH );
+  }
+
+  /**
+   * a constructor. Using SimpleFragListBuilder and ScoreOrderFragmentsBuilder.
+   * 
+   * @param phraseHighlight true or false for phrase highlighting
+   * @param fieldMatch true of false for field matching
+   */
+  public FastVectorHighlighter( boolean phraseHighlight, boolean fieldMatch ){
+    this( phraseHighlight, fieldMatch, new SimpleFragListBuilder(), new ScoreOrderFragmentsBuilder() );
+  }
+
+  /**
+   * a constructor. A FragListBuilder and a FragmentsBuilder can be specified (plugins).
+   * 
+   * @param phraseHighlight true of false for phrase highlighting
+   * @param fieldMatch true of false for field matching
+   * @param fragListBuilder an instance of FragListBuilder
+   * @param fragmentsBuilder an instance of FragmentsBuilder
+   */
+  public FastVectorHighlighter( boolean phraseHighlight, boolean fieldMatch,
+      FragListBuilder fragListBuilder, FragmentsBuilder fragmentsBuilder ){
+    this.phraseHighlight = phraseHighlight;
+    this.fieldMatch = fieldMatch;
+    this.fragListBuilder = fragListBuilder;
+    this.fragmentsBuilder = fragmentsBuilder;
+  }
+
+  /**
+   * create a FieldQuery object.
+   * 
+   * @param query a query
+   * @return the created FieldQuery object
+   */
+  public FieldQuery getFieldQuery( Query query ){
+    return new FieldQuery( query, phraseHighlight, fieldMatch );
+  }
+
+  /**
+   * return the best fragment.
+   * 
+   * @param fieldQuery FieldQuery object
+   * @param reader IndexReader of the index
+   * @param docId document id to be highlighted
+   * @param fieldName field of the document to be highlighted
+   * @param fragCharSize the length (number of chars) of a fragment
+   * @return the best fragment (snippet) string
+   * @throws IOException
+   */
+  public final String getBestFragment( final FieldQuery fieldQuery, IndexReader reader, int docId,
+      String fieldName, int fragCharSize ) throws IOException {
+    FieldFragList fieldFragList = getFieldFragList( fieldQuery, reader, docId, fieldName, fragCharSize );
+    return fragmentsBuilder.createFragment( reader, docId, fieldName, fieldFragList );
+  }
+
+  /**
+   * return the best fragments.
+   * 
+   * @param fieldQuery FieldQuery object
+   * @param reader IndexReader of the index
+   * @param docId document id to be highlighted
+   * @param fieldName field of the document to be highlighted
+   * @param fragCharSize the length (number of chars) of a fragment
+   * @param maxNumFragments maximum number of fragments
+   * @return created fragments or null when no fragments created.
+   *         size of the array can be less than maxNumFragments
+   * @throws IOException
+   */
+  public final String[] getBestFragments( final FieldQuery fieldQuery, IndexReader reader, int docId,
+      String fieldName, int fragCharSize, int maxNumFragments ) throws IOException {
+    FieldFragList fieldFragList = getFieldFragList( fieldQuery, reader, docId, fieldName, fragCharSize );
+    return fragmentsBuilder.createFragments( reader, docId, fieldName, fieldFragList, maxNumFragments );
+  }
+  
+  private FieldFragList getFieldFragList( final FieldQuery fieldQuery, IndexReader reader, int docId,
+      String fieldName, int fragCharSize ) throws IOException {
+    FieldTermStack fieldTermStack = new FieldTermStack( reader, docId, fieldName, fieldQuery );
+    FieldPhraseList fieldPhraseList = new FieldPhraseList( fieldTermStack, fieldQuery );
+    return fragListBuilder.createFieldFragList( fieldPhraseList, fragCharSize );
+  }
+
+  /**
+   * return whether phraseHighlight or not.
+   * 
+   * @return whether phraseHighlight or not
+   */
+  public boolean isPhraseHighlight(){ return phraseHighlight; }
+
+  /**
+   * return whether fieldMatch or not.
+   * 
+   * @return whether fieldMatch or not
+   */
+  public boolean isFieldMatch(){ return fieldMatch; }
+}
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldFragList.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldFragList.java
new file mode 100644
index 0000000..6fdf435
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldFragList.java
@@ -0,0 +1,128 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.search.vectorhighlight.FieldPhraseList.WeightedPhraseInfo;
+import org.apache.lucene.search.vectorhighlight.FieldPhraseList.WeightedPhraseInfo.Toffs;
+
+/**
+ * FieldFragList has a list of "frag info" that is used by FragmentsBuilder class
+ * to create fragments (snippets).
+ */
+public class FieldFragList {
+
+  List<WeightedFragInfo> fragInfos = new ArrayList<WeightedFragInfo>();
+
+  /**
+   * a constructor.
+   * 
+   * @param fragCharSize the length (number of chars) of a fragment
+   */
+  public FieldFragList( int fragCharSize ){
+  }
+
+  /**
+   * convert the list of WeightedPhraseInfo to WeightedFragInfo, then add it to the fragInfos
+   * 
+   * @param startOffset start offset of the fragment
+   * @param endOffset end offset of the fragment
+   * @param phraseInfoList list of WeightedPhraseInfo objects
+   */
+  public void add( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ){
+    fragInfos.add( new WeightedFragInfo( startOffset, endOffset, phraseInfoList ) );
+  }
+  
+  public static class WeightedFragInfo {
+
+    List<SubInfo> subInfos;
+    float totalBoost;
+    int startOffset;
+    int endOffset;
+
+    public WeightedFragInfo( int startOffset, int endOffset, List<WeightedPhraseInfo> phraseInfoList ){
+      this.startOffset = startOffset;
+      this.endOffset = endOffset;
+      subInfos = new ArrayList<SubInfo>();
+      for( WeightedPhraseInfo phraseInfo : phraseInfoList ){
+        SubInfo subInfo = new SubInfo( phraseInfo.text, phraseInfo.termsOffsets, phraseInfo.seqnum );
+        subInfos.add( subInfo );
+        totalBoost += phraseInfo.boost;
+      }
+    }
+    
+    public List<SubInfo> getSubInfos(){
+      return subInfos;
+    }
+    
+    public float getTotalBoost(){
+      return totalBoost;
+    }
+    
+    public int getStartOffset(){
+      return startOffset;
+    }
+    
+    public int getEndOffset(){
+      return endOffset;
+    }
+    
+    @Override
+    public String toString(){
+      StringBuilder sb = new StringBuilder();
+      sb.append( "subInfos=(" );
+      for( SubInfo si : subInfos )
+        sb.append( si.toString() );
+      sb.append( ")/" ).append( totalBoost ).append( '(' ).append( startOffset ).append( ',' ).append( endOffset ).append( ')' );
+      return sb.toString();
+    }
+    
+    public static class SubInfo {
+      final String text;  // unnecessary member, just exists for debugging purpose
+      final List<Toffs> termsOffsets;   // usually termsOffsets.size() == 1,
+                              // but if position-gap > 1 and slop > 0 then size() could be greater than 1
+      int seqnum;
+
+      SubInfo( String text, List<Toffs> termsOffsets, int seqnum ){
+        this.text = text;
+        this.termsOffsets = termsOffsets;
+        this.seqnum = seqnum;
+      }
+      
+      public List<Toffs> getTermsOffsets(){
+        return termsOffsets;
+      }
+      
+      public int getSeqnum(){
+        return seqnum;
+      }
+      
+      @Override
+      public String toString(){
+        StringBuilder sb = new StringBuilder();
+        sb.append( text ).append( '(' );
+        for( Toffs to : termsOffsets )
+          sb.append( to.toString() );
+        sb.append( ')' );
+        return sb.toString();
+      }
+    }
+  }
+}
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java
new file mode 100644
index 0000000..15ae634
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldPhraseList.java
@@ -0,0 +1,191 @@
+package org.apache.lucene.search.vectorhighlight;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.LinkedList;
+import java.util.List;
+
+import org.apache.lucene.search.vectorhighlight.FieldQuery.QueryPhraseMap;
+import org.apache.lucene.search.vectorhighlight.FieldTermStack.TermInfo;
+
+/**
+ * FieldPhraseList has a list of WeightedPhraseInfo that is used by FragListBuilder
+ * to create a FieldFragList object.
+ */
+public class FieldPhraseList {
+
+  LinkedList<WeightedPhraseInfo> phraseList = new LinkedList<WeightedPhraseInfo>();
+
+  /**
+   * a constructor.
+   * 
+   * @param fieldTermStack FieldTermStack object
+   * @param fieldQuery FieldQuery object
+   */
+  public FieldPhraseList( FieldTermStack fieldTermStack, FieldQuery fieldQuery ){
+    final String field = fieldTermStack.getFieldName();
+
+    LinkedList<TermInfo> phraseCandidate = new LinkedList<TermInfo>();
+    QueryPhraseMap currMap = null;
+    QueryPhraseMap nextMap = null;
+    while( !fieldTermStack.isEmpty() ){
+      
+      phraseCandidate.clear();
+
+      TermInfo ti = fieldTermStack.pop();
+      currMap = fieldQuery.getFieldTermMap( field, ti.getText() );
+
+      // if not found, discard top TermInfo from stack, then try next element
+      if( currMap == null ) continue;
+      
+      // if found, search the longest phrase
+      phraseCandidate.add( ti );
+      while( true ){
+        ti = fieldTermStack.pop();
+        nextMap = null;
+        if( ti != null )
+          nextMap = currMap.getTermMap( ti.getText() );
+        if( ti == null || nextMap == null ){
+          if( ti != null )
+            fieldTermStack.push( ti );
+          if( currMap.isValidTermOrPhrase( phraseCandidate ) ){
+            addIfNoOverlap( new WeightedPhraseInfo( phraseCandidate, currMap.getBoost(), currMap.getTermOrPhraseNumber() ) );
+          }
+          else{
+            while( phraseCandidate.size() > 1 ){
+              fieldTermStack.push( phraseCandidate.removeLast() );
+              currMap = fieldQuery.searchPhrase( field, phraseCandidate );
+              if( currMap != null ){
+                addIfNoOverlap( new WeightedPhraseInfo( phraseCandidate, currMap.getBoost(), currMap.getTermOrPhraseNumber() ) );
+                break;
+              }
+            }
+          }
+          break;
+        }
+        else{
+          phraseCandidate.add( ti );
+          currMap = nextMap;
+        }
+      }
+    }
+  }
+  
+  void addIfNoOverlap( WeightedPhraseInfo wpi ){
+    for( WeightedPhraseInfo existWpi : phraseList ){
+      if( existWpi.isOffsetOverlap( wpi ) ) return;
+    }
+    phraseList.add( wpi );
+  }
+  
+  public static class WeightedPhraseInfo {
+
+    String text;  // unnecessary member, just exists for debugging purpose
+    List<Toffs> termsOffsets;   // usually termsOffsets.size() == 1,
+                            // but if position-gap > 1 and slop > 0 then size() could be greater than 1
+    float boost;  // query boost
+    int seqnum;
+    
+    public WeightedPhraseInfo( LinkedList<TermInfo> terms, float boost ){
+      this( terms, boost, 0 );
+    }
+    
+    public WeightedPhraseInfo( LinkedList<TermInfo> terms, float boost, int number ){
+      this.boost = boost;
+      this.seqnum = number;
+      termsOffsets = new ArrayList<Toffs>( terms.size() );
+      TermInfo ti = terms.get( 0 );
+      termsOffsets.add( new Toffs( ti.getStartOffset(), ti.getEndOffset() ) );
+      if( terms.size() == 1 ){
+        text = ti.getText();
+        return;
+      }
+      StringBuilder sb = new StringBuilder();
+      sb.append( ti.getText() );
+      int pos = ti.getPosition();
+      for( int i = 1; i < terms.size(); i++ ){
+        ti = terms.get( i );
+        sb.append( ti.getText() );
+        if( ti.getPosition() - pos == 1 ){
+          Toffs to = termsOffsets.get( termsOffsets.size() - 1 );
+          to.setEndOffset( ti.getEndOffset() );
+        }
+        else{
+          termsOffsets.add( new Toffs( ti.getStartOffset(), ti.getEndOffset() ) );
+        }
+        pos = ti.getPosition();
+      }
+      text = sb.toString();
+    }
+    
+    public int getStartOffset(){
+      return termsOffsets.get( 0 ).startOffset;
+    }
+    
+    public int getEndOffset(){
+      return termsOffsets.get( termsOffsets.size() - 1 ).endOffset;
+    }
+    
+    public boolean isOffsetOverlap( WeightedPhraseInfo other ){
+      int so = getStartOffset();
+      int eo = getEndOffset();
+      int oso = other.getStartOffset();
+      int oeo = other.getEndOffset();
+      if( so <= oso && oso < eo ) return true;
+      if( so < oeo && oeo <= eo ) return true;
+      if( oso <= so && so < oeo ) return true;
+      if( oso < eo && eo <= oeo ) return true;
+      return false;
+    }
+    
+    @Override
+    public String toString(){
+      StringBuilder sb = new StringBuilder();
+      sb.append( text ).append( '(' ).append( boost ).append( ")(" );
+      for( Toffs to : termsOffsets ){
+        sb.append( to );
+      }
+      sb.append( ')' );
+      return sb.toString();
+    }
+    
+    public static class Toffs {
+      int startOffset;
+      int endOffset;
+      public Toffs( int startOffset, int endOffset ){
+        this.startOffset = startOffset;
+        this.endOffset = endOffset;
+      }
+      public void setEndOffset( int endOffset ){
+        this.endOffset = endOffset;
+      }
+      public int getStartOffset(){
+        return startOffset;
+      }
+      public int getEndOffset(){
+        return endOffset;
+      }
+      @Override
+      public String toString(){
+        StringBuilder sb = new StringBuilder();
+        sb.append( '(' ).append( startOffset ).append( ',' ).append( endOffset ).append( ')' );
+        return sb.toString();
+      }
+    }
+  }
+}
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java
new file mode 100644
index 0000000..01ecaf8
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldQuery.java
@@ -0,0 +1,399 @@
+package org.apache.lucene.search.vectorhighlight;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.DisjunctionMaxQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.vectorhighlight.FieldTermStack.TermInfo;
+
+/**
+ * FieldQuery breaks down query object into terms/phrases and keep
+ * them in QueryPhraseMap structure.
+ */
+public class FieldQuery {
+
+  final boolean fieldMatch;
+
+  // fieldMatch==true,  Map<fieldName,QueryPhraseMap>
+  // fieldMatch==false, Map<null,QueryPhraseMap>
+  Map<String, QueryPhraseMap> rootMaps = new HashMap<String, QueryPhraseMap>();
+
+  // fieldMatch==true,  Map<fieldName,setOfTermsInQueries>
+  // fieldMatch==false, Map<null,setOfTermsInQueries>
+  Map<String, Set<String>> termSetMap = new HashMap<String, Set<String>>();
+
+  int termOrPhraseNumber; // used for colored tag support
+
+  FieldQuery( Query query, boolean phraseHighlight, boolean fieldMatch ){
+    this.fieldMatch = fieldMatch;
+    Set<Query> flatQueries = new HashSet<Query>();
+    flatten( query, flatQueries );
+    saveTerms( flatQueries );
+    Collection<Query> expandQueries = expand( flatQueries );
+
+    for( Query flatQuery : expandQueries ){
+      QueryPhraseMap rootMap = getRootMap( flatQuery );
+      rootMap.add( flatQuery );
+      if( !phraseHighlight && flatQuery instanceof PhraseQuery ){
+        PhraseQuery pq = (PhraseQuery)flatQuery;
+        if( pq.getTerms().length > 1 ){
+          for( Term term : pq.getTerms() )
+            rootMap.addTerm( term, flatQuery.getBoost() );
+        }
+      }
+    }
+  }
+  
+  void flatten( Query sourceQuery, Collection<Query> flatQueries ){
+    if( sourceQuery instanceof BooleanQuery ){
+      BooleanQuery bq = (BooleanQuery)sourceQuery;
+      for( BooleanClause clause : bq.getClauses() ){
+        if( !clause.isProhibited() )
+          flatten( clause.getQuery(), flatQueries );
+      }
+    }
+    else if( sourceQuery instanceof DisjunctionMaxQuery ){
+      DisjunctionMaxQuery dmq = (DisjunctionMaxQuery)sourceQuery;
+      for( Query query : dmq ){
+        flatten( query, flatQueries );
+      }
+    }
+    else if( sourceQuery instanceof TermQuery ){
+      if( !flatQueries.contains( sourceQuery ) )
+        flatQueries.add( sourceQuery );
+    }
+    else if( sourceQuery instanceof PhraseQuery ){
+      if( !flatQueries.contains( sourceQuery ) ){
+        PhraseQuery pq = (PhraseQuery)sourceQuery;
+        if( pq.getTerms().length > 1 )
+          flatQueries.add( pq );
+        else if( pq.getTerms().length == 1 ){
+          flatQueries.add( new TermQuery( pq.getTerms()[0] ) );
+        }
+      }
+    }
+    // else discard queries
+  }
+  
+  /*
+   * Create expandQueries from flatQueries.
+   * 
+   * expandQueries := flatQueries + overlapped phrase queries
+   * 
+   * ex1) flatQueries={a,b,c}
+   *      => expandQueries={a,b,c}
+   * ex2) flatQueries={a,"b c","c d"}
+   *      => expandQueries={a,"b c","c d","b c d"}
+   */
+  Collection<Query> expand( Collection<Query> flatQueries ){
+    Set<Query> expandQueries = new HashSet<Query>();
+    for( Iterator<Query> i = flatQueries.iterator(); i.hasNext(); ){
+      Query query = i.next();
+      i.remove();
+      expandQueries.add( query );
+      if( !( query instanceof PhraseQuery ) ) continue;
+      for( Iterator<Query> j = flatQueries.iterator(); j.hasNext(); ){
+        Query qj = j.next();
+        if( !( qj instanceof PhraseQuery ) ) continue;
+        checkOverlap( expandQueries, (PhraseQuery)query, (PhraseQuery)qj );
+      }
+    }
+    return expandQueries;
+  }
+
+  /*
+   * Check if PhraseQuery A and B have overlapped part.
+   * 
+   * ex1) A="a b", B="b c" => overlap; expandQueries={"a b c"}
+   * ex2) A="b c", B="a b" => overlap; expandQueries={"a b c"}
+   * ex3) A="a b", B="c d" => no overlap; expandQueries={}
+   */
+  private void checkOverlap( Collection<Query> expandQueries, PhraseQuery a, PhraseQuery b ){
+    if( a.getSlop() != b.getSlop() ) return;
+    Term[] ats = a.getTerms();
+    Term[] bts = b.getTerms();
+    if( fieldMatch && !ats[0].field().equals( bts[0].field() ) ) return;
+    checkOverlap( expandQueries, ats, bts, a.getSlop(), a.getBoost() );
+    checkOverlap( expandQueries, bts, ats, b.getSlop(), b.getBoost() );
+  }
+
+  /*
+   * Check if src and dest have overlapped part and if it is, create PhraseQueries and add expandQueries.
+   * 
+   * ex1) src="a b", dest="c d"       => no overlap
+   * ex2) src="a b", dest="a b c"     => no overlap
+   * ex3) src="a b", dest="b c"       => overlap; expandQueries={"a b c"}
+   * ex4) src="a b c", dest="b c d"   => overlap; expandQueries={"a b c d"}
+   * ex5) src="a b c", dest="b c"     => no overlap
+   * ex6) src="a b c", dest="b"       => no overlap
+   * ex7) src="a a a a", dest="a a a" => overlap;
+   *                                     expandQueries={"a a a a a","a a a a a a"}
+   * ex8) src="a b c d", dest="b c"   => no overlap
+   */
+  private void checkOverlap( Collection<Query> expandQueries, Term[] src, Term[] dest, int slop, float boost ){
+    // beginning from 1 (not 0) is safe because that the PhraseQuery has multiple terms
+    // is guaranteed in flatten() method (if PhraseQuery has only one term, flatten()
+    // converts PhraseQuery to TermQuery)
+    for( int i = 1; i < src.length; i++ ){
+      boolean overlap = true;
+      for( int j = i; j < src.length; j++ ){
+        if( ( j - i ) < dest.length && !src[j].text().equals( dest[j-i].text() ) ){
+          overlap = false;
+          break;
+        }
+      }
+      if( overlap && src.length - i < dest.length ){
+        PhraseQuery pq = new PhraseQuery();
+        for( Term srcTerm : src )
+          pq.add( srcTerm );
+        for( int k = src.length - i; k < dest.length; k++ ){
+          pq.add( new Term( src[0].field(), dest[k].text() ) );
+        }
+        pq.setSlop( slop );
+        pq.setBoost( boost );
+        if(!expandQueries.contains( pq ) )
+          expandQueries.add( pq );
+      }
+    }
+  }
+  
+  QueryPhraseMap getRootMap( Query query ){
+    String key = getKey( query );
+    QueryPhraseMap map = rootMaps.get( key );
+    if( map == null ){
+      map = new QueryPhraseMap( this );
+      rootMaps.put( key, map );
+    }
+    return map;
+  }
+  
+  /*
+   * Return 'key' string. 'key' is the field name of the Query.
+   * If not fieldMatch, 'key' will be null.
+   */
+  private String getKey( Query query ){
+    if( !fieldMatch ) return null;
+    if( query instanceof TermQuery )
+      return ((TermQuery)query).getTerm().field();
+    else if ( query instanceof PhraseQuery ){
+      PhraseQuery pq = (PhraseQuery)query;
+      Term[] terms = pq.getTerms();
+      return terms[0].field();
+    }
+    else
+      throw new RuntimeException( "query \"" + query.toString() + "\" must be flatten first." );
+  }
+
+  /*
+   * Save the set of terms in the queries to termSetMap.
+   * 
+   * ex1) q=name:john
+   *      - fieldMatch==true
+   *          termSetMap=Map<"name",Set<"john">>
+   *      - fieldMatch==false
+   *          termSetMap=Map<null,Set<"john">>
+   *          
+   * ex2) q=name:john title:manager
+   *      - fieldMatch==true
+   *          termSetMap=Map<"name",Set<"john">,
+   *                         "title",Set<"manager">>
+   *      - fieldMatch==false
+   *          termSetMap=Map<null,Set<"john","manager">>
+   *          
+   * ex3) q=name:"john lennon"
+   *      - fieldMatch==true
+   *          termSetMap=Map<"name",Set<"john","lennon">>
+   *      - fieldMatch==false
+   *          termSetMap=Map<null,Set<"john","lennon">>
+   */
+  void saveTerms( Collection<Query> flatQueries ){
+    for( Query query : flatQueries ){
+      Set<String> termSet = getTermSet( query );
+      if( query instanceof TermQuery )
+        termSet.add( ((TermQuery)query).getTerm().text() );
+      else if( query instanceof PhraseQuery ){
+        for( Term term : ((PhraseQuery)query).getTerms() )
+          termSet.add( term.text() );
+      }
+      else
+        throw new RuntimeException( "query \"" + query.toString() + "\" must be flatten first." );
+    }
+  }
+  
+  private Set<String> getTermSet( Query query ){
+    String key = getKey( query );
+    Set<String> set = termSetMap.get( key );
+    if( set == null ){
+      set = new HashSet<String>();
+      termSetMap.put( key, set );
+    }
+    return set;
+  }
+  
+  Set<String> getTermSet( String field ){
+    return termSetMap.get( fieldMatch ? field : null );
+  }
+
+  /**
+   * 
+   * @param fieldName
+   * @param term
+   * @return QueryPhraseMap
+   */
+  public QueryPhraseMap getFieldTermMap( String fieldName, String term ){
+    QueryPhraseMap rootMap = getRootMap( fieldName );
+    return rootMap == null ? null : rootMap.subMap.get( term );
+  }
+
+  /**
+   * 
+   * @param fieldName
+   * @param phraseCandidate
+   * @return QueryPhraseMap
+   */
+  public QueryPhraseMap searchPhrase( String fieldName, final List<TermInfo> phraseCandidate ){
+    QueryPhraseMap root = getRootMap( fieldName );
+    if( root == null ) return null;
+    return root.searchPhrase( phraseCandidate );
+  }
+  
+  private QueryPhraseMap getRootMap( String fieldName ){
+    return rootMaps.get( fieldMatch ? fieldName : null );
+  }
+  
+  int nextTermOrPhraseNumber(){
+    return termOrPhraseNumber++;
+  }
+  
+  public static class QueryPhraseMap {
+
+    boolean terminal;
+    int slop;   // valid if terminal == true and phraseHighlight == true
+    float boost;  // valid if terminal == true
+    int termOrPhraseNumber;   // valid if terminal == true
+    FieldQuery fieldQuery;
+    Map<String, QueryPhraseMap> subMap = new HashMap<String, QueryPhraseMap>();
+    
+    public QueryPhraseMap( FieldQuery fieldQuery ){
+      this.fieldQuery = fieldQuery;
+    }
+
+    void addTerm( Term term, float boost ){
+      QueryPhraseMap map = getOrNewMap( subMap, term.text() );
+      map.markTerminal( boost );
+    }
+    
+    private QueryPhraseMap getOrNewMap( Map<String, QueryPhraseMap> subMap, String term ){
+      QueryPhraseMap map = subMap.get( term );
+      if( map == null ){
+        map = new QueryPhraseMap( fieldQuery );
+        subMap.put( term, map );
+      }
+      return map;
+    }
+
+    void add( Query query ){
+      if( query instanceof TermQuery ){
+        addTerm( ((TermQuery)query).getTerm(), query.getBoost() );
+      }
+      else if( query instanceof PhraseQuery ){
+        PhraseQuery pq = (PhraseQuery)query;
+        Term[] terms = pq.getTerms();
+        Map<String, QueryPhraseMap> map = subMap;
+        QueryPhraseMap qpm = null;
+        for( Term term : terms ){
+          qpm = getOrNewMap( map, term.text() );
+          map = qpm.subMap;
+        }
+        qpm.markTerminal( pq.getSlop(), pq.getBoost() );
+      }
+      else
+        throw new RuntimeException( "query \"" + query.toString() + "\" must be flatten first." );
+    }
+    
+    public QueryPhraseMap getTermMap( String term ){
+      return subMap.get( term );
+    }
+    
+    private void markTerminal( float boost ){
+      markTerminal( 0, boost );
+    }
+    
+    private void markTerminal( int slop, float boost ){
+      this.terminal = true;
+      this.slop = slop;
+      this.boost = boost;
+      this.termOrPhraseNumber = fieldQuery.nextTermOrPhraseNumber();
+    }
+    
+    public boolean isTerminal(){
+      return terminal;
+    }
+    
+    public int getSlop(){
+      return slop;
+    }
+    
+    public float getBoost(){
+      return boost;
+    }
+    
+    public int getTermOrPhraseNumber(){
+      return termOrPhraseNumber;
+    }
+    
+    public QueryPhraseMap searchPhrase( final List<TermInfo> phraseCandidate ){
+      QueryPhraseMap currMap = this;
+      for( TermInfo ti : phraseCandidate ){
+        currMap = currMap.subMap.get( ti.getText() );
+        if( currMap == null ) return null;
+      }
+      return currMap.isValidTermOrPhrase( phraseCandidate ) ? currMap : null;
+    }
+    
+    public boolean isValidTermOrPhrase( final List<TermInfo> phraseCandidate ){
+      // check terminal
+      if( !terminal ) return false;
+
+      // if the candidate is a term, it is valid
+      if( phraseCandidate.size() == 1 ) return true;
+
+      // else check whether the candidate is valid phrase
+      // compare position-gaps between terms to slop
+      int pos = phraseCandidate.get( 0 ).getPosition();
+      for( int i = 1; i < phraseCandidate.size(); i++ ){
+        int nextPos = phraseCandidate.get( i ).getPosition();
+        if( Math.abs( nextPos - pos - 1 ) > slop ) return false;
+        pos = nextPos;
+      }
+      return true;
+    }
+  }
+}
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
new file mode 100644
index 0000000..86ca670
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FieldTermStack.java
@@ -0,0 +1,173 @@
+package org.apache.lucene.search.vectorhighlight;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.LinkedList;
+import java.util.Set;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Index;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.Field.TermVector;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.TermFreqVector;
+import org.apache.lucene.index.TermPositionVector;
+import org.apache.lucene.index.TermVectorOffsetInfo;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.Version;
+
+/**
+ * <code>FieldTermStack</code> is a stack that keeps query terms in the specified field
+ * of the document to be highlighted.
+ */
+public class FieldTermStack {
+  
+  private final String fieldName;
+  LinkedList<TermInfo> termList = new LinkedList<TermInfo>();
+  
+  public static void main( String[] args ) throws Exception {
+    Analyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_CURRENT);
+    QueryParser parser = new QueryParser(Version.LUCENE_CURRENT,  "f", analyzer );
+    Query query = parser.parse( "a x:b" );
+    FieldQuery fieldQuery = new FieldQuery( query, true, false );
+    
+    Directory dir = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(Version.LUCENE_CURRENT, analyzer));
+    Document doc = new Document();
+    doc.add( new Field( "f", "a a a b b c a b b c d e f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
+    doc.add( new Field( "f", "b a b a f", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
+    writer.addDocument( doc );
+    writer.close();
+    
+    IndexReader reader = IndexReader.open( dir, true );
+    new FieldTermStack( reader, 0, "f", fieldQuery );
+    reader.close();
+  }
+
+  /**
+   * a constructor.
+   * 
+   * @param reader IndexReader of the index
+   * @param docId document id to be highlighted
+   * @param fieldName field of the document to be highlighted
+   * @param fieldQuery FieldQuery object
+   * @throws IOException
+   */
+  public FieldTermStack( IndexReader reader, int docId, String fieldName, final FieldQuery fieldQuery ) throws IOException {
+    this.fieldName = fieldName;
+
+    TermFreqVector tfv = reader.getTermFreqVector( docId, fieldName );
+    if( tfv == null ) return; // just return to make null snippets
+    TermPositionVector tpv = null;
+    try{
+      tpv = (TermPositionVector)tfv;
+    }
+    catch( ClassCastException e ){
+      return; // just return to make null snippets
+    }
+    
+    Set<String> termSet = fieldQuery.getTermSet( fieldName );
+    // just return to make null snippet if un-matched fieldName specified when fieldMatch == true
+    if( termSet == null ) return;
+    
+    for( String term : tpv.getTerms() ){
+      if( !termSet.contains( term ) ) continue;
+      int index = tpv.indexOf( term );
+      TermVectorOffsetInfo[] tvois = tpv.getOffsets( index );
+      if( tvois == null ) return; // just return to make null snippets
+      int[] poss = tpv.getTermPositions( index );
+      if( poss == null ) return; // just return to make null snippets
+      for( int i = 0; i < tvois.length; i++ )
+        termList.add( new TermInfo( term, tvois[i].getStartOffset(), tvois[i].getEndOffset(), poss[i] ) );
+    }
+    
+    // sort by position
+    Collections.sort( termList );
+  }
+
+  /**
+   * @return field name
+   */
+  public String getFieldName(){
+    return fieldName;
+  }
+
+  /**
+   * @return the top TermInfo object of the stack
+   */
+  public TermInfo pop(){
+    return termList.poll();
+  }
+
+  /**
+   * @param termInfo the TermInfo object to be put on the top of the stack
+   */
+  public void push( TermInfo termInfo ){
+    // termList.push( termInfo );  // avoid Java 1.6 feature
+    termList.addFirst( termInfo );
+  }
+
+  /**
+   * to know whether the stack is empty
+   * 
+   * @return true if the stack is empty, false if not
+   */
+  public boolean isEmpty(){
+    return termList == null || termList.size() == 0;
+  }
+  
+  public static class TermInfo implements Comparable<TermInfo>{
+
+    final String text;
+    final int startOffset;
+    final int endOffset;
+    final int position;
+
+    TermInfo( String text, int startOffset, int endOffset, int position ){
+      this.text = text;
+      this.startOffset = startOffset;
+      this.endOffset = endOffset;
+      this.position = position;
+    }
+    
+    public String getText(){ return text; }
+    public int getStartOffset(){ return startOffset; }
+    public int getEndOffset(){ return endOffset; }
+    public int getPosition(){ return position; }
+    
+    @Override
+    public String toString(){
+      StringBuilder sb = new StringBuilder();
+      sb.append( text ).append( '(' ).append(startOffset).append( ',' ).append( endOffset ).append( ',' ).append( position ).append( ')' );
+      return sb.toString();
+    }
+
+    public int compareTo( TermInfo o ) {
+      return ( this.position - o.position );
+    }
+  }
+}
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragListBuilder.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragListBuilder.java
new file mode 100644
index 0000000..0ed1d3e
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragListBuilder.java
@@ -0,0 +1,34 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * FragListBuilder is an interface for FieldFragList builder classes.
+ * A FragListBuilder class can be plugged in to Highlighter.
+ */
+public interface FragListBuilder {
+
+  /**
+   * create a FieldFragList.
+   * 
+   * @param fieldPhraseList FieldPhraseList object
+   * @param fragCharSize the length (number of chars) of a fragment
+   * @return the created FieldFragList object
+   */
+  public FieldFragList createFieldFragList( FieldPhraseList fieldPhraseList, int fragCharSize );
+}
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragmentsBuilder.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragmentsBuilder.java
new file mode 100644
index 0000000..c60c3b3
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/FragmentsBuilder.java
@@ -0,0 +1,57 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.IndexReader;
+
+/**
+ * FragmentsBuilder is an interface for fragments (snippets) builder classes.
+ * A FragmentsBuilder class can be plugged in to Highlighter.
+ */
+public interface FragmentsBuilder {
+
+  /**
+   * create a fragment.
+   * 
+   * @param reader IndexReader of the index
+   * @param docId document id to be highlighted
+   * @param fieldName field of the document to be highlighted
+   * @param fieldFragList FieldFragList object
+   * @return a created fragment or null when no fragment created
+   * @throws IOException
+   */
+  public String createFragment( IndexReader reader, int docId, String fieldName,
+      FieldFragList fieldFragList ) throws IOException;
+
+  /**
+   * create multiple fragments.
+   * 
+   * @param reader IndexReader of the index
+   * @param docId document id to be highlighter
+   * @param fieldName field of the document to be highlighted
+   * @param fieldFragList FieldFragList object
+   * @param maxNumFragments maximum number of fragments
+   * @return created fragments or null when no fragments created.
+   *         size of the array can be less than maxNumFragments
+   * @throws IOException
+   */
+  public String[] createFragments( IndexReader reader, int docId, String fieldName,
+      FieldFragList fieldFragList, int maxNumFragments ) throws IOException;
+}
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilder.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilder.java
new file mode 100644
index 0000000..aac72e3
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilder.java
@@ -0,0 +1,70 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.search.vectorhighlight.FieldFragList.WeightedFragInfo;
+
+/**
+ * An implementation of FragmentsBuilder that outputs score-order fragments.
+ */
+public class ScoreOrderFragmentsBuilder extends BaseFragmentsBuilder {
+
+  /**
+   * a constructor.
+   */
+  public ScoreOrderFragmentsBuilder(){
+    super();
+  }
+
+  /**
+   * a constructor.
+   * 
+   * @param preTags array of pre-tags for markup terms.
+   * @param postTags array of post-tags for markup terms.
+   */
+  public ScoreOrderFragmentsBuilder( String[] preTags, String[] postTags ){
+    super( preTags, postTags );
+  }
+
+  /**
+   * Sort by score the list of WeightedFragInfo
+   */
+  @Override
+  public List<WeightedFragInfo> getWeightedFragInfoList( List<WeightedFragInfo> src ) {
+    Collections.sort( src, new ScoreComparator() );
+    return src;
+  }
+
+  public static class ScoreComparator implements Comparator<WeightedFragInfo> {
+
+    public int compare( WeightedFragInfo o1, WeightedFragInfo o2 ) {
+      if( o1.totalBoost > o2.totalBoost ) return -1;
+      else if( o1.totalBoost < o2.totalBoost ) return 1;
+      // if same score then check startOffset
+      else{
+        if( o1.startOffset < o2.startOffset ) return -1;
+        else if( o1.startOffset > o2.startOffset ) return 1;
+      }
+      return 0;
+    }
+  }
+}
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilder.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilder.java
new file mode 100644
index 0000000..089b42e
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilder.java
@@ -0,0 +1,84 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.lucene.search.vectorhighlight.FieldPhraseList.WeightedPhraseInfo;
+
+/**
+ * A simple implementation of FragListBuilder.
+ */
+public class SimpleFragListBuilder implements FragListBuilder {
+  
+  public static final int MARGIN = 6;
+  public static final int MIN_FRAG_CHAR_SIZE = MARGIN * 3;
+
+  public FieldFragList createFieldFragList(FieldPhraseList fieldPhraseList, int fragCharSize) {
+    if( fragCharSize < MIN_FRAG_CHAR_SIZE )
+      throw new IllegalArgumentException( "fragCharSize(" + fragCharSize + ") is too small. It must be " +
+          MIN_FRAG_CHAR_SIZE + " or higher." );
+
+    FieldFragList ffl = new FieldFragList( fragCharSize );
+
+    List<WeightedPhraseInfo> wpil = new ArrayList<WeightedPhraseInfo>();
+    Iterator<WeightedPhraseInfo> ite = fieldPhraseList.phraseList.iterator();
+    WeightedPhraseInfo phraseInfo = null;
+    int startOffset = 0;
+    boolean taken = false;
+    while( true ){
+      if( !taken ){
+        if( !ite.hasNext() ) break;
+        phraseInfo = ite.next();
+      }
+      taken = false;
+      if( phraseInfo == null ) break;
+
+      // if the phrase violates the border of previous fragment, discard it and try next phrase
+      if( phraseInfo.getStartOffset() < startOffset ) continue;
+
+      wpil.clear();
+      wpil.add( phraseInfo );
+      int st = phraseInfo.getStartOffset() - MARGIN < startOffset ?
+          startOffset : phraseInfo.getStartOffset() - MARGIN;
+      int en = st + fragCharSize;
+      if( phraseInfo.getEndOffset() > en )
+        en = phraseInfo.getEndOffset();
+      startOffset = en;
+
+      while( true ){
+        if( ite.hasNext() ){
+          phraseInfo = ite.next();
+          taken = true;
+          if( phraseInfo == null ) break;
+        }
+        else
+          break;
+        if( phraseInfo.getEndOffset() <= en )
+          wpil.add( phraseInfo );
+        else
+          break;
+      }
+      ffl.add( st, en, wpil );
+    }
+    return ffl;
+  }
+
+}
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilder.java b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilder.java
new file mode 100644
index 0000000..378d692
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilder.java
@@ -0,0 +1,54 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.List;
+
+import org.apache.lucene.search.vectorhighlight.FieldFragList.WeightedFragInfo;
+
+/**
+ * A simple implementation of FragmentsBuilder.
+ *
+ */
+public class SimpleFragmentsBuilder extends BaseFragmentsBuilder {
+
+  /**
+   * a constructor.
+   */
+  public SimpleFragmentsBuilder() {
+    super();
+  }
+
+  /**
+   * a constructor.
+   * 
+   * @param preTags array of pre-tags for markup terms.
+   * @param postTags array of post-tags for markup terms.
+   */
+  public SimpleFragmentsBuilder( String[] preTags, String[] postTags ) {
+    super( preTags, postTags );
+  }
+
+  /**
+   * do nothing. return the source list.
+   */
+  @Override
+  public List<WeightedFragInfo> getWeightedFragInfoList( List<WeightedFragInfo> src ) {
+    return src;
+  }
+}
diff --git a/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/package.html b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/package.html
new file mode 100644
index 0000000..ee023ab
--- /dev/null
+++ b/lucene/contrib/highlighter/src/java/org/apache/lucene/search/vectorhighlight/package.html
@@ -0,0 +1,143 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+This is an another highlighter implementation.
+
+<h2>Features</h2>
+<ul>
+<li>fast for large docs</li>
+<li>support N-gram fields</li>
+<li>support phrase-unit highlighting with slops</li>
+<li>need Java 1.5</li>
+<li>highlight fields need to be TermVector.WITH_POSITIONS_OFFSETS</li>
+<li>take into account query boost to score fragments</li>
+<li>support colored highlight tags</li>
+<li>pluggable FragListBuilder</li>
+<li>pluggable FragmentsBuilder</li>
+</ul>
+
+<h2>Algorithm</h2>
+<p>To explain the algorithm, let's use the following sample text
+ (to be highlighted) and user query:</p>
+
+<table border=1>
+<tr>
+<td><b>Sample Text</b></td>
+<td>Lucene is a search engine library.</td>
+</tr>
+<tr>
+<td><b>User Query</b></td>
+<td>Lucene^2 OR "search library"~1</td>
+</tr>
+</table>
+
+<p>The user query is a BooleanQuery that consists of TermQuery("Lucene") 
+with boost of 2 and PhraseQuery("search library") with slop of 1.</p>
+<p>For your convenience, here is the offsets and positions info of the 
+sample text.</p>
+
+<pre>
++--------+-----------------------------------+
+|        |          1111111111222222222233333|
+|  offset|01234567890123456789012345678901234|
++--------+-----------------------------------+
+|document|Lucene is a search engine library. |
++--------*-----------------------------------+
+|position|0      1  2 3      4      5        |
++--------*-----------------------------------+
+</pre>
+
+<h3>Step 1.</h3>
+<p>In Step 1, Fast Vector Highlighter generates {@link org.apache.lucene.search.vectorhighlight.FieldQuery.QueryPhraseMap} from the user query.
+<code>QueryPhraseMap</code> consists of the following members:</p>
+<pre>
+public class QueryPhraseMap {
+  boolean terminal;
+  int slop;   // valid if terminal == true and phraseHighlight == true
+  float boost;  // valid if terminal == true
+  Map&lt;String, QueryPhraseMap&gt; subMap;
+} 
+</pre>
+<p><code>QueryPhraseMap</code> has subMap. The key of the subMap is a term 
+text in the user query and the value is a subsequent <code>QueryPhraseMap</code>.
+If the query is a term (not phrase), then the subsequent <code>QueryPhraseMap</code>
+is marked as terminal. If the query is a phrase, then the subsequent <code>QueryPhraseMap</code>
+is not a terminal and it has the next term text in the phrase.</p>
+
+<p>From the sample user query, the following <code>QueryPhraseMap</code> 
+will be generated:</p>
+<pre>
+   QueryPhraseMap
++--------+-+  +-------+-+
+|"Lucene"|o+->|boost=2|*|  * : terminal
++--------+-+  +-------+-+
+
++--------+-+  +---------+-+  +-------+------+-+
+|"search"|o+->|"library"|o+->|boost=1|slop=1|*|
++--------+-+  +---------+-+  +-------+------+-+
+</pre>
+
+<h3>Step 2.</h3>
+<p>In Step 2, Fast Vector Highlighter generates {@link org.apache.lucene.search.vectorhighlight.FieldTermStack}. Fast Vector Highlighter uses {@link org.apache.lucene.index.TermFreqVector} data
+(must be stored {@link org.apache.lucene.document.Field.TermVector#WITH_POSITIONS_OFFSETS})
+to generate it. <code>FieldTermStack</code> keeps the terms in the user query.
+Therefore, in this sample case, Fast Vector Highlighter generates the following <code>FieldTermStack</code>:</p>
+<pre>
+   FieldTermStack
++------------------+
+|"Lucene"(0,6,0)   |
++------------------+
+|"search"(12,18,3) |
++------------------+
+|"library"(26,33,5)|
++------------------+
+where : "termText"(startOffset,endOffset,position)
+</pre>
+<h3>Step 3.</h3>
+<p>In Step 3, Fast Vector Highlighter generates {@link org.apache.lucene.search.vectorhighlight.FieldPhraseList}
+by reference to <code>QueryPhraseMap</code> and <code>FieldTermStack</code>.</p>
+<pre>
+   FieldPhraseList
++----------------+-----------------+---+
+|"Lucene"        |[(0,6)]          |w=2|
++----------------+-----------------+---+
+|"search library"|[(12,18),(26,33)]|w=1|
++----------------+-----------------+---+
+</pre>
+<p>The type of each entry is <code>WeightedPhraseInfo</code> that consists of
+an array of terms offsets and weight. The weight (Fast Vector Highlighter uses query boost to
+calculate the weight) will be taken into account when Fast Vector Highlighter creates
+{@link org.apache.lucene.search.vectorhighlight.FieldFragList} in the next step.</p>
+<h3>Step 4.</h3>
+<p>In Step 4, Fast Vector Highlighter creates <code>FieldFragList</code> by reference to
+<code>FieldPhraseList</code>. In this sample case, the following
+<code>FieldFragList</code> will be generated:</p>
+<pre>
+   FieldFragList
++---------------------------------+
+|"Lucene"[(0,6)]                  |
+|"search library"[(12,18),(26,33)]|
+|totalBoost=3                     |
++---------------------------------+
+</pre>
+<h3>Step 5.</h3>
+<p>In Step 5, by using <code>FieldFragList</code> and the field stored data,
+Fast Vector Highlighter creates highlighted snippets!</p>
+</body>
+</html>
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
new file mode 100644
index 0000000..883b694
--- /dev/null
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase.java
@@ -0,0 +1,429 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Reader;
+import java.util.Collection;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.KeywordAnalyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Index;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.Field.TermVector;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.DisjunctionMaxQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.LuceneTestCase;
+
+public abstract class AbstractTestCase extends LuceneTestCase {
+
+  protected final String F = "f";
+  protected final String F1 = "f1";
+  protected final String F2 = "f2";
+  protected Directory dir;
+  protected Analyzer analyzerW;
+  protected Analyzer analyzerB;
+  protected Analyzer analyzerK;
+  protected IndexReader reader;  
+  protected QueryParser paW;
+  protected QueryParser paB;
+  
+  protected static final String[] shortMVValues = {
+    "a b c",
+    "",   // empty data in multi valued field
+    "d e"
+  };
+  
+  protected static final String[] longMVValues = {
+    "Followings are the examples of customizable parameters and actual examples of customization:",
+    "The most search engines use only one of these methods. Even the search engines that says they can use the both methods basically"
+  };
+  
+  // test data for LUCENE-1448 bug
+  protected static final String[] biMVValues = {
+    "\nLucene/Solr does not require such additional hardware.",
+    "\nWhen you talk about processing speed, the"
+  };
+  
+  protected static final String[] strMVValues = {
+    "abc",
+    "defg",
+    "hijkl"
+  };
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    analyzerW = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
+    analyzerB = new BigramAnalyzer();
+    analyzerK = new KeywordAnalyzer();
+    paW = new QueryParser(TEST_VERSION_CURRENT,  F, analyzerW );
+    paB = new QueryParser(TEST_VERSION_CURRENT,  F, analyzerB );
+    dir = new RAMDirectory();
+  }
+  
+  @Override
+  protected void tearDown() throws Exception {
+    if( reader != null ){
+      reader.close();
+      reader = null;
+    }
+    super.tearDown();
+  }
+
+  protected Query tq( String text ){
+    return tq( 1F, text );
+  }
+
+  protected Query tq( float boost, String text ){
+    return tq( boost, F, text );
+  }
+  
+  protected Query tq( String field, String text ){
+    return tq( 1F, field, text );
+  }
+  
+  protected Query tq( float boost, String field, String text ){
+    Query query = new TermQuery( new Term( field, text ) );
+    query.setBoost( boost );
+    return query;
+  }
+  
+  protected Query pqF( String... texts ){
+    return pqF( 1F, texts );
+  }
+  
+  protected Query pqF( float boost, String... texts ){
+    return pqF( boost, 0, texts );
+  }
+  
+  protected Query pqF( float boost, int slop, String... texts ){
+    return pq( boost, slop, F, texts );
+  }
+  
+  protected Query pq( String field, String... texts ){
+    return pq( 1F, 0, field, texts );
+  }
+  
+  protected Query pq( float boost, String field, String... texts ){
+    return pq( boost, 0, field, texts );
+  }
+  
+  protected Query pq( float boost, int slop, String field, String... texts ){
+    PhraseQuery query = new PhraseQuery();
+    for( String text : texts ){
+      query.add( new Term( field, text ) );
+    }
+    query.setBoost( boost );
+    query.setSlop( slop );
+    return query;
+  }
+  
+  protected Query dmq( Query... queries ){
+    return dmq( 0.0F, queries );
+  }
+  
+  protected Query dmq( float tieBreakerMultiplier, Query... queries ){
+    DisjunctionMaxQuery query = new DisjunctionMaxQuery( tieBreakerMultiplier );
+    for( Query q : queries ){
+      query.add( q );
+    }
+    return query;
+  }
+  
+  protected void assertCollectionQueries( Collection<Query> actual, Query... expected ){
+    assertEquals( expected.length, actual.size() );
+    for( Query query : expected ){
+      assertTrue( actual.contains( query ) );
+    }
+  }
+
+  static class BigramAnalyzer extends Analyzer {
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {
+      return new BasicNGramTokenizer( reader );
+    }
+  }
+  
+  static class BasicNGramTokenizer extends Tokenizer {
+
+    public static final int DEFAULT_N_SIZE = 2;
+    public static final String DEFAULT_DELIMITERS = " \t\n.,";
+    private final int n;
+    private final String delimiters;
+    private int startTerm;
+    private int lenTerm;
+    private int startOffset;
+    private int nextStartOffset;
+    private int ch;
+    private String snippet;
+    private StringBuilder snippetBuffer;
+    private static final int BUFFER_SIZE = 4096;
+    private char[] charBuffer;
+    private int charBufferIndex;
+    private int charBufferLen;
+    
+    public BasicNGramTokenizer( Reader in ){
+      this( in, DEFAULT_N_SIZE );
+    }
+    
+    public BasicNGramTokenizer( Reader in, int n ){
+      this( in, n, DEFAULT_DELIMITERS );
+    }
+    
+    public BasicNGramTokenizer( Reader in, String delimiters ){
+      this( in, DEFAULT_N_SIZE, delimiters );
+    }
+    
+    public BasicNGramTokenizer( Reader in, int n, String delimiters ){
+      super(in);
+      this.n = n;
+      this.delimiters = delimiters;
+      startTerm = 0;
+      nextStartOffset = 0;
+      snippet = null;
+      snippetBuffer = new StringBuilder();
+      charBuffer = new char[BUFFER_SIZE];
+      charBufferIndex = BUFFER_SIZE;
+      charBufferLen = 0;
+      ch = 0;
+    }
+
+    TermAttribute termAtt = addAttribute(TermAttribute.class);
+    OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
+    @Override
+    public boolean incrementToken() throws IOException {
+      if( !getNextPartialSnippet() )
+        return false;
+      clearAttributes();
+      termAtt.setTermBuffer(snippet, startTerm, lenTerm);
+      offsetAtt.setOffset(correctOffset(startOffset), correctOffset(startOffset + lenTerm));
+      return true;
+    }
+
+    private int getFinalOffset() {
+      return nextStartOffset;
+    }
+    
+    @Override
+    public final void end(){
+      offsetAtt.setOffset(getFinalOffset(),getFinalOffset());
+    }
+    
+    protected boolean getNextPartialSnippet() throws IOException {
+      if( snippet != null && snippet.length() >= startTerm + 1 + n ){
+        startTerm++;
+        startOffset++;
+        lenTerm = n;
+        return true;
+      }
+      return getNextSnippet();
+    }
+    
+    protected boolean getNextSnippet() throws IOException {
+      startTerm = 0;
+      startOffset = nextStartOffset;
+      snippetBuffer.delete( 0, snippetBuffer.length() );
+      while( true ){
+        if( ch != -1 )
+          ch = readCharFromBuffer();
+        if( ch == -1 ) break;
+        else if( !isDelimiter( ch ) )
+          snippetBuffer.append( (char)ch );
+        else if( snippetBuffer.length() > 0 )
+          break;
+        else
+          startOffset++;
+      }
+      if( snippetBuffer.length() == 0 )
+        return false;
+      snippet = snippetBuffer.toString();
+      lenTerm = snippet.length() >= n ? n : snippet.length();
+      return true;
+    }
+    
+    protected int readCharFromBuffer() throws IOException {
+      if( charBufferIndex >= charBufferLen ){
+        charBufferLen = input.read( charBuffer );
+        if( charBufferLen == -1 ){
+          return -1;
+        }
+        charBufferIndex = 0;
+      }
+      int c = charBuffer[charBufferIndex++];
+      nextStartOffset++;
+      return c;
+    }
+    
+    protected boolean isDelimiter( int c ){
+      return delimiters.indexOf( c ) >= 0;
+    }
+    
+    @Override
+    public void reset( Reader input ) throws IOException {
+      super.reset( input );
+      reset();
+    }
+    
+    @Override
+    public void reset() throws IOException {
+      startTerm = 0;
+      nextStartOffset = 0;
+      snippet = null;
+      snippetBuffer.setLength( 0 );
+      charBufferIndex = BUFFER_SIZE;
+      charBufferLen = 0;
+      ch = 0;
+    }
+  }
+
+  protected void make1d1fIndex( String value ) throws Exception {
+    make1dmfIndex( value );
+  }
+  
+  protected void make1d1fIndexB( String value ) throws Exception {
+    make1dmfIndexB( value );
+  }
+  
+  protected void make1dmfIndex( String... values ) throws Exception {
+    make1dmfIndex( analyzerW, values );
+  }
+  
+  protected void make1dmfIndexB( String... values ) throws Exception {
+    make1dmfIndex( analyzerB, values );
+  }
+  
+  // make 1 doc with multi valued field
+  protected void make1dmfIndex( Analyzer analyzer, String... values ) throws Exception {
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, analyzer).setOpenMode(OpenMode.CREATE));
+    Document doc = new Document();
+    for( String value: values )
+      doc.add( new Field( F, value, Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
+    writer.addDocument( doc );
+    writer.close();
+
+    reader = IndexReader.open( dir, true );
+  }
+  
+  // make 1 doc with multi valued & not analyzed field
+  protected void make1dmfIndexNA( String... values ) throws Exception {
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, analyzerK).setOpenMode(OpenMode.CREATE));
+    Document doc = new Document();
+    for( String value: values )
+      doc.add( new Field( F, value, Store.YES, Index.NOT_ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
+    writer.addDocument( doc );
+    writer.close();
+
+    reader = IndexReader.open( dir, true );
+  }
+  
+  protected void makeIndexShortMV() throws Exception {
+
+    //  012345
+    // "a b c"
+    //  0 1 2
+    
+    // ""
+
+    //  6789
+    // "d e"
+    //  3 4
+    make1dmfIndex( shortMVValues );
+  }
+  
+  protected void makeIndexLongMV() throws Exception {
+    //           11111111112222222222333333333344444444445555555555666666666677777777778888888888999
+    // 012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012
+    // Followings are the examples of customizable parameters and actual examples of customization:
+    // 0          1   2   3        4  5            6          7   8      9        10 11
+    
+    //        1                                                                                                   2
+    // 999999900000000001111111111222222222233333333334444444444555555555566666666667777777777888888888899999999990000000000111111111122
+    // 345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901
+    // The most search engines use only one of these methods. Even the search engines that says they can use the both methods basically
+    // 12  13  (14)   (15)     16  17   18  19 20    21       22   23 (24)   (25)     26   27   28   29  30  31  32   33      34
+
+    make1dmfIndex( longMVValues );
+  }
+  
+  protected void makeIndexLongMVB() throws Exception {
+    // "*" ... LF
+    
+    //           1111111111222222222233333333334444444444555555
+    // 01234567890123456789012345678901234567890123456789012345
+    // *Lucene/Solr does not require such additional hardware.
+    //  Lu 0        do 10    re 15   su 21       na 31
+    //   uc 1        oe 11    eq 16   uc 22       al 32
+    //    ce 2        es 12    qu 17   ch 23         ha 33
+    //     en 3          no 13  ui 18     ad 24       ar 34
+    //      ne 4          ot 14  ir 19     dd 25       rd 35
+    //       e/ 5                 re 20     di 26       dw 36
+    //        /S 6                           it 27       wa 37
+    //         So 7                           ti 28       ar 38
+    //          ol 8                           io 29       re 39
+    //           lr 9                           on 30
+
+    // 5555666666666677777777778888888888999999999
+    // 6789012345678901234567890123456789012345678
+    // *When you talk about processing speed, the
+    //  Wh 40         ab 48     es 56         th 65
+    //   he 41         bo 49     ss 57         he 66
+    //    en 42         ou 50     si 58
+    //       yo 43       ut 51     in 59
+    //        ou 44         pr 52   ng 60
+    //           ta 45       ro 53     sp 61
+    //            al 46       oc 54     pe 62
+    //             lk 47       ce 55     ee 63
+    //                                    ed 64
+
+    make1dmfIndexB( biMVValues );
+  }
+  
+  protected void makeIndexStrMV() throws Exception {
+
+    //  0123
+    // "abc"
+    
+    //  34567
+    // "defg"
+
+    //     111
+    //  789012
+    // "hijkl"
+    make1dmfIndexNA( strMVValues );
+  }
+}
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java
new file mode 100644
index 0000000..1eb89d7
--- /dev/null
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldPhraseListTest.java
@@ -0,0 +1,191 @@
+package org.apache.lucene.search.vectorhighlight;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.BooleanClause.Occur;
+
+public class FieldPhraseListTest extends AbstractTestCase {
+  
+  public void test1TermIndex() throws Exception {
+    make1d1fIndex( "a" );
+
+    FieldQuery fq = new FieldQuery( tq( "a" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "a(1.0)((0,1))", fpl.phraseList.get( 0 ).toString() );
+
+    fq = new FieldQuery( tq( "b" ), true, true );
+    stack = new FieldTermStack( reader, 0, F, fq );
+    fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 0, fpl.phraseList.size() );
+  }
+  
+  public void test2TermsIndex() throws Exception {
+    make1d1fIndex( "a a" );
+
+    FieldQuery fq = new FieldQuery( tq( "a" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 2, fpl.phraseList.size() );
+    assertEquals( "a(1.0)((0,1))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( "a(1.0)((2,3))", fpl.phraseList.get( 1 ).toString() );
+  }
+  
+  public void test1PhraseIndex() throws Exception {
+    make1d1fIndex( "a b" );
+
+    FieldQuery fq = new FieldQuery( pqF( "a", "b" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "ab(1.0)((0,3))", fpl.phraseList.get( 0 ).toString() );
+
+    fq = new FieldQuery( tq( "b" ), true, true );
+    stack = new FieldTermStack( reader, 0, F, fq );
+    fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "b(1.0)((2,3))", fpl.phraseList.get( 0 ).toString() );
+  }
+  
+  public void test1PhraseIndexB() throws Exception {
+    // 01 12 23 34 45 56 67 78 (offsets)
+    // bb|bb|ba|ac|cb|ba|ab|bc
+    //  0  1  2  3  4  5  6  7 (positions)
+    make1d1fIndexB( "bbbacbabc" );
+
+    FieldQuery fq = new FieldQuery( pqF( "ba", "ac" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "baac(1.0)((2,5))", fpl.phraseList.get( 0 ).toString() );
+  }
+  
+  public void test2ConcatTermsIndexB() throws Exception {
+    // 01 12 23 (offsets)
+    // ab|ba|ab
+    //  0  1  2 (positions)
+    make1d1fIndexB( "abab" );
+
+    FieldQuery fq = new FieldQuery( tq( "ab" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 2, fpl.phraseList.size() );
+    assertEquals( "ab(1.0)((0,2))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( "ab(1.0)((2,4))", fpl.phraseList.get( 1 ).toString() );
+  }
+  
+  public void test2Terms1PhraseIndex() throws Exception {
+    make1d1fIndex( "c a a b" );
+
+    // phraseHighlight = true
+    FieldQuery fq = new FieldQuery( pqF( "a", "b" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "ab(1.0)((4,7))", fpl.phraseList.get( 0 ).toString() );
+
+    // phraseHighlight = false
+    fq = new FieldQuery( pqF( "a", "b" ), false, true );
+    stack = new FieldTermStack( reader, 0, F, fq );
+    fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 2, fpl.phraseList.size() );
+    assertEquals( "a(1.0)((2,3))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( "ab(1.0)((4,7))", fpl.phraseList.get( 1 ).toString() );
+  }
+  
+  public void testPhraseSlop() throws Exception {
+    make1d1fIndex( "c a a b c" );
+
+    FieldQuery fq = new FieldQuery( pqF( 2F, 1, "a", "c" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "ac(2.0)((4,5)(8,9))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( 4, fpl.phraseList.get( 0 ).getStartOffset() );
+    assertEquals( 9, fpl.phraseList.get( 0 ).getEndOffset() );
+  }
+  
+  public void test2PhrasesOverlap() throws Exception {
+    make1d1fIndex( "d a b c d" );
+
+    BooleanQuery query = new BooleanQuery();
+    query.add( pqF( "a", "b" ), Occur.SHOULD );
+    query.add( pqF( "b", "c" ), Occur.SHOULD );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "abc(1.0)((2,7))", fpl.phraseList.get( 0 ).toString() );
+  }
+  
+  public void test3TermsPhrase() throws Exception {
+    make1d1fIndex( "d a b a b c d" );
+
+    FieldQuery fq = new FieldQuery( pqF( "a", "b", "c" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "abc(1.0)((6,11))", fpl.phraseList.get( 0 ).toString() );
+  }
+  
+  public void testSearchLongestPhrase() throws Exception {
+    make1d1fIndex( "d a b d c a b c" );
+
+    BooleanQuery query = new BooleanQuery();
+    query.add( pqF( "a", "b" ), Occur.SHOULD );
+    query.add( pqF( "a", "b", "c" ), Occur.SHOULD );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 2, fpl.phraseList.size() );
+    assertEquals( "ab(1.0)((2,5))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( "abc(1.0)((10,15))", fpl.phraseList.get( 1 ).toString() );
+  }
+  
+  public void test1PhraseShortMV() throws Exception {
+    makeIndexShortMV();
+
+    FieldQuery fq = new FieldQuery( tq( "d" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "d(1.0)((6,7))", fpl.phraseList.get( 0 ).toString() );
+  }
+  
+  public void test1PhraseLongMV() throws Exception {
+    makeIndexLongMV();
+
+    FieldQuery fq = new FieldQuery( pqF( "search", "engines" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 2, fpl.phraseList.size() );
+    assertEquals( "searchengines(1.0)((102,116))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( "searchengines(1.0)((157,171))", fpl.phraseList.get( 1 ).toString() );
+  }
+
+  public void test1PhraseLongMVB() throws Exception {
+    makeIndexLongMVB();
+
+    FieldQuery fq = new FieldQuery( pqF( "sp", "pe", "ee", "ed" ), true, true ); // "speed" -(2gram)-> "sp","pe","ee","ed"
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "sppeeeed(1.0)((88,93))", fpl.phraseList.get( 0 ).toString() );
+  }
+}
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
new file mode 100644
index 0000000..cb73765
--- /dev/null
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldQueryTest.java
@@ -0,0 +1,837 @@
+package org.apache.lucene.search.vectorhighlight;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.vectorhighlight.FieldQuery.QueryPhraseMap;
+import org.apache.lucene.search.vectorhighlight.FieldTermStack.TermInfo;
+
+public class FieldQueryTest extends AbstractTestCase {
+
+  public void testFlattenBoolean() throws Exception {
+    Query query = paW.parse( "A AND B OR C NOT (D AND E)" );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Set<Query> flatQueries = new HashSet<Query>();
+    fq.flatten( query, flatQueries );
+    assertCollectionQueries( flatQueries, tq( "A" ), tq( "B" ), tq( "C" ) );
+  }
+
+  public void testFlattenDisjunctionMaxQuery() throws Exception {
+    Query query = dmq( tq( "A" ), tq( "B" ), pqF( "C", "D" ) );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Set<Query> flatQueries = new HashSet<Query>();
+    fq.flatten( query, flatQueries );
+    assertCollectionQueries( flatQueries, tq( "A" ), tq( "B" ), pqF( "C", "D" ) );
+  }
+
+  public void testFlattenTermAndPhrase() throws Exception {
+    Query query = paW.parse( "A AND \"B C\"" );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Set<Query> flatQueries = new HashSet<Query>();
+    fq.flatten( query, flatQueries );
+    assertCollectionQueries( flatQueries, tq( "A" ), pqF( "B", "C" ) );
+  }
+
+  public void testFlattenTermAndPhrase2gram() throws Exception {
+    Query query = paB.parse( "AA AND BCD OR EFGH" );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Set<Query> flatQueries = new HashSet<Query>();
+    fq.flatten( query, flatQueries );
+    assertCollectionQueries( flatQueries, tq( "AA" ), pqF( "BC", "CD" ), pqF( "EF", "FG", "GH" ) );
+  }
+
+  public void testFlatten1TermPhrase() throws Exception {
+    Query query = pqF( "A" );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Set<Query> flatQueries = new HashSet<Query>();
+    fq.flatten( query, flatQueries );
+    assertCollectionQueries( flatQueries, tq( "A" ) );
+  }
+
+  public void testExpand() throws Exception {
+    Query dummy = pqF( "DUMMY" );
+    FieldQuery fq = new FieldQuery( dummy, true, true );
+
+    // "a b","b c" => "a b","b c","a b c"
+    Set<Query> flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b" ) );
+    flatQueries.add( pqF( "b", "c" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b" ), pqF( "b", "c" ), pqF( "a", "b", "c" ) );
+
+    // "a b","b c d" => "a b","b c d","a b c d"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b" ) );
+    flatQueries.add( pqF( "b", "c", "d" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b" ), pqF( "b", "c", "d" ), pqF( "a", "b", "c", "d" ) );
+
+    // "a b c","b c d" => "a b c","b c d","a b c d"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b", "c" ) );
+    flatQueries.add( pqF( "b", "c", "d" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b", "c" ), pqF( "b", "c", "d" ), pqF( "a", "b", "c", "d" ) );
+
+    // "a b c","c d e" => "a b c","c d e","a b c d e"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b", "c" ) );
+    flatQueries.add( pqF( "c", "d", "e" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b", "c" ), pqF( "c", "d", "e" ), pqF( "a", "b", "c", "d", "e" ) );
+
+    // "a b c d","b c" => "a b c d","b c"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b", "c", "d" ) );
+    flatQueries.add( pqF( "b", "c" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b", "c", "d" ), pqF( "b", "c" ) );
+
+    // "a b b","b c" => "a b b","b c","a b b c"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b", "b" ) );
+    flatQueries.add( pqF( "b", "c" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b", "b" ), pqF( "b", "c" ), pqF( "a", "b", "b", "c" ) );
+
+    // "a b","b a" => "a b","b a","a b a", "b a b"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b" ) );
+    flatQueries.add( pqF( "b", "a" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b" ), pqF( "b", "a" ), pqF( "a", "b", "a" ), pqF( "b", "a", "b" ) );
+
+    // "a b","a b c" => "a b","a b c"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b" ) );
+    flatQueries.add( pqF( "a", "b", "c" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b" ), pqF( "a", "b", "c" ) );
+  }
+
+  public void testNoExpand() throws Exception {
+    Query dummy = pqF( "DUMMY" );
+    FieldQuery fq = new FieldQuery( dummy, true, true );
+
+    // "a b","c d" => "a b","c d"
+    Set<Query> flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b" ) );
+    flatQueries.add( pqF( "c", "d" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b" ), pqF( "c", "d" ) );
+
+    // "a","a b" => "a", "a b"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( tq( "a" ) );
+    flatQueries.add( pqF( "a", "b" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        tq( "a" ), pqF( "a", "b" ) );
+
+    // "a b","b" => "a b", "b"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b" ) );
+    flatQueries.add( tq( "b" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b" ), tq( "b" ) );
+
+    // "a b c","b c" => "a b c","b c"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b", "c" ) );
+    flatQueries.add( pqF( "b", "c" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b", "c" ), pqF( "b", "c" ) );
+
+    // "a b","a b c" => "a b","a b c"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b" ) );
+    flatQueries.add( pqF( "a", "b", "c" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b" ), pqF( "a", "b", "c" ) );
+
+    // "a b c","b d e" => "a b c","b d e"
+    flatQueries = new HashSet<Query>();
+    flatQueries.add( pqF( "a", "b", "c" ) );
+    flatQueries.add( pqF( "b", "d", "e" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pqF( "a", "b", "c" ), pqF( "b", "d", "e" ) );
+  }
+
+  public void testExpandNotFieldMatch() throws Exception {
+    Query dummy = pqF( "DUMMY" );
+    FieldQuery fq = new FieldQuery( dummy, true, false );
+
+    // f1:"a b",f2:"b c" => f1:"a b",f2:"b c",f1:"a b c"
+    Set<Query> flatQueries = new HashSet<Query>();
+    flatQueries.add( pq( F1, "a", "b" ) );
+    flatQueries.add( pq( F2, "b", "c" ) );
+    assertCollectionQueries( fq.expand( flatQueries ),
+        pq( F1, "a", "b" ), pq( F2, "b", "c" ), pq( F1, "a", "b", "c" ) );
+  }
+
+  public void testGetFieldTermMap() throws Exception {
+    Query query = tq( "a" );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    
+    QueryPhraseMap pqm = fq.getFieldTermMap( F, "a" );
+    assertNotNull( pqm );
+    assertTrue( pqm.isTerminal() );
+    
+    pqm = fq.getFieldTermMap( F, "b" );
+    assertNull( pqm );
+    
+    pqm = fq.getFieldTermMap( F1, "a" );
+    assertNull( pqm );
+  }
+
+  public void testGetRootMap() throws Exception {
+    Query dummy = pqF( "DUMMY" );
+    FieldQuery fq = new FieldQuery( dummy, true, true );
+
+    QueryPhraseMap rootMap1 = fq.getRootMap( tq( "a" ) );
+    QueryPhraseMap rootMap2 = fq.getRootMap( tq( "a" ) );
+    assertTrue( rootMap1 == rootMap2 );
+    QueryPhraseMap rootMap3 = fq.getRootMap( tq( "b" ) );
+    assertTrue( rootMap1 == rootMap3 );
+    QueryPhraseMap rootMap4 = fq.getRootMap( tq( F1, "b" ) );
+    assertFalse( rootMap4 == rootMap3 );
+  }
+
+  public void testGetRootMapNotFieldMatch() throws Exception {
+    Query dummy = pqF( "DUMMY" );
+    FieldQuery fq = new FieldQuery( dummy, true, false );
+
+    QueryPhraseMap rootMap1 = fq.getRootMap( tq( "a" ) );
+    QueryPhraseMap rootMap2 = fq.getRootMap( tq( "a" ) );
+    assertTrue( rootMap1 == rootMap2 );
+    QueryPhraseMap rootMap3 = fq.getRootMap( tq( "b" ) );
+    assertTrue( rootMap1 == rootMap3 );
+    QueryPhraseMap rootMap4 = fq.getRootMap( tq( F1, "b" ) );
+    assertTrue( rootMap4 == rootMap3 );
+  }
+
+  public void testGetTermSet() throws Exception {
+    Query query = paW.parse( "A AND B OR x:C NOT (D AND E)" );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    assertEquals( 2, fq.termSetMap.size() );
+    Set<String> termSet = fq.getTermSet( F );
+    assertEquals( 2, termSet.size() );
+    assertTrue( termSet.contains( "A" ) );
+    assertTrue( termSet.contains( "B" ) );
+    termSet = fq.getTermSet( "x" );
+    assertEquals( 1, termSet.size() );
+    assertTrue( termSet.contains( "C" ) );
+    termSet = fq.getTermSet( "y" );
+    assertNull( termSet );
+  }
+  
+  public void testQueryPhraseMap1Term() throws Exception {
+    Query query = tq( "a" );
+    
+    // phraseHighlight = true, fieldMatch = true
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Map<String, QueryPhraseMap> map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( null ) );
+    assertNotNull( map.get( F ) );
+    QueryPhraseMap qpm = map.get( F );
+    assertEquals( 1, qpm.subMap.size() );
+    assertTrue( qpm.subMap.get( "a" ) != null );
+    assertTrue( qpm.subMap.get( "a" ).terminal );
+    assertEquals( 1F, qpm.subMap.get( "a" ).boost );
+    
+    // phraseHighlight = true, fieldMatch = false
+    fq = new FieldQuery( query, true, false );
+    map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( F ) );
+    assertNotNull( map.get( null ) );
+    qpm = map.get( null );
+    assertEquals( 1, qpm.subMap.size() );
+    assertTrue( qpm.subMap.get( "a" ) != null );
+    assertTrue( qpm.subMap.get( "a" ).terminal );
+    assertEquals( 1F, qpm.subMap.get( "a" ).boost );
+    
+    // phraseHighlight = false, fieldMatch = true
+    fq = new FieldQuery( query, false, true );
+    map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( null ) );
+    assertNotNull( map.get( F ) );
+    qpm = map.get( F );
+    assertEquals( 1, qpm.subMap.size() );
+    assertTrue( qpm.subMap.get( "a" ) != null );
+    assertTrue( qpm.subMap.get( "a" ).terminal );
+    assertEquals( 1F, qpm.subMap.get( "a" ).boost );
+    
+    // phraseHighlight = false, fieldMatch = false
+    fq = new FieldQuery( query, false, false );
+    map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( F ) );
+    assertNotNull( map.get( null ) );
+    qpm = map.get( null );
+    assertEquals( 1, qpm.subMap.size() );
+    assertTrue( qpm.subMap.get( "a" ) != null );
+    assertTrue( qpm.subMap.get( "a" ).terminal );
+    assertEquals( 1F, qpm.subMap.get( "a" ).boost );
+    
+    // boost != 1
+    query = tq( 2, "a" );
+    fq = new FieldQuery( query, true, true );
+    map = fq.rootMaps;
+    qpm = map.get( F );
+    assertEquals( 2F, qpm.subMap.get( "a" ).boost );
+  }
+  
+  public void testQueryPhraseMap1Phrase() throws Exception {
+    Query query = pqF( "a", "b" );
+    
+    // phraseHighlight = true, fieldMatch = true
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Map<String, QueryPhraseMap> map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( null ) );
+    assertNotNull( map.get( F ) );
+    QueryPhraseMap qpm = map.get( F );
+    assertEquals( 1, qpm.subMap.size() );
+    assertNotNull( qpm.subMap.get( "a" ) );
+    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "b" ) );
+    QueryPhraseMap qpm3 = qpm2.subMap.get( "b" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+    
+    // phraseHighlight = true, fieldMatch = false
+    fq = new FieldQuery( query, true, false );
+    map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( F ) );
+    assertNotNull( map.get( null ) );
+    qpm = map.get( null );
+    assertEquals( 1, qpm.subMap.size() );
+    assertNotNull( qpm.subMap.get( "a" ) );
+    qpm2 = qpm.subMap.get( "a" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "b" ) );
+    qpm3 = qpm2.subMap.get( "b" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+    
+    // phraseHighlight = false, fieldMatch = true
+    fq = new FieldQuery( query, false, true );
+    map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( null ) );
+    assertNotNull( map.get( F ) );
+    qpm = map.get( F );
+    assertEquals( 2, qpm.subMap.size() );
+    assertNotNull( qpm.subMap.get( "a" ) );
+    qpm2 = qpm.subMap.get( "a" );
+    assertTrue( qpm2.terminal );
+    assertEquals( 1F, qpm2.boost );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "b" ) );
+    qpm3 = qpm2.subMap.get( "b" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+
+    assertNotNull( qpm.subMap.get( "b" ) );
+    qpm2 = qpm.subMap.get( "b" );
+    assertTrue( qpm2.terminal );
+    assertEquals( 1F, qpm2.boost );
+    
+    // phraseHighlight = false, fieldMatch = false
+    fq = new FieldQuery( query, false, false );
+    map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( F ) );
+    assertNotNull( map.get( null ) );
+    qpm = map.get( null );
+    assertEquals( 2, qpm.subMap.size() );
+    assertNotNull( qpm.subMap.get( "a" ) );
+    qpm2 = qpm.subMap.get( "a" );
+    assertTrue( qpm2.terminal );
+    assertEquals( 1F, qpm2.boost );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "b" ) );
+    qpm3 = qpm2.subMap.get( "b" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+
+    assertNotNull( qpm.subMap.get( "b" ) );
+    qpm2 = qpm.subMap.get( "b" );
+    assertTrue( qpm2.terminal );
+    assertEquals( 1F, qpm2.boost );
+
+    // boost != 1
+    query = pqF( 2, "a", "b" );
+    // phraseHighlight = false, fieldMatch = false
+    fq = new FieldQuery( query, false, false );
+    map = fq.rootMaps;
+    qpm = map.get( null );
+    qpm2 = qpm.subMap.get( "a" );
+    assertEquals( 2F, qpm2.boost );
+    qpm3 = qpm2.subMap.get( "b" );
+    assertEquals( 2F, qpm3.boost );
+    qpm2 = qpm.subMap.get( "b" );
+    assertEquals( 2F, qpm2.boost );
+  }
+  
+  public void testQueryPhraseMap1PhraseAnother() throws Exception {
+    Query query = pqF( "search", "engines" );
+    
+    // phraseHighlight = true, fieldMatch = true
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Map<String, QueryPhraseMap> map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( null ) );
+    assertNotNull( map.get( F ) );
+    QueryPhraseMap qpm = map.get( F );
+    assertEquals( 1, qpm.subMap.size() );
+    assertNotNull( qpm.subMap.get( "search" ) );
+    QueryPhraseMap qpm2 = qpm.subMap.get( "search" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "engines" ) );
+    QueryPhraseMap qpm3 = qpm2.subMap.get( "engines" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+  }
+  
+  public void testQueryPhraseMap2Phrases() throws Exception {
+    BooleanQuery query = new BooleanQuery();
+    query.add( pqF( "a", "b" ), Occur.SHOULD );
+    query.add( pqF( 2, "c", "d" ), Occur.SHOULD );
+    
+    // phraseHighlight = true, fieldMatch = true
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Map<String, QueryPhraseMap> map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( null ) );
+    assertNotNull( map.get( F ) );
+    QueryPhraseMap qpm = map.get( F );
+    assertEquals( 2, qpm.subMap.size() );
+
+    // "a b"
+    assertNotNull( qpm.subMap.get( "a" ) );
+    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "b" ) );
+    QueryPhraseMap qpm3 = qpm2.subMap.get( "b" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+
+    // "c d"^2
+    assertNotNull( qpm.subMap.get( "c" ) );
+    qpm2 = qpm.subMap.get( "c" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "d" ) );
+    qpm3 = qpm2.subMap.get( "d" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 2F, qpm3.boost );
+  }
+  
+  public void testQueryPhraseMap2PhrasesFields() throws Exception {
+    BooleanQuery query = new BooleanQuery();
+    query.add( pq( F1, "a", "b" ), Occur.SHOULD );
+    query.add( pq( 2F, F2, "c", "d" ), Occur.SHOULD );
+    
+    // phraseHighlight = true, fieldMatch = true
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Map<String, QueryPhraseMap> map = fq.rootMaps;
+    assertEquals( 2, map.size() );
+    assertNull( map.get( null ) );
+
+    // "a b"
+    assertNotNull( map.get( F1 ) );
+    QueryPhraseMap qpm = map.get( F1 );
+    assertEquals( 1, qpm.subMap.size() );
+    assertNotNull( qpm.subMap.get( "a" ) );
+    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "b" ) );
+    QueryPhraseMap qpm3 = qpm2.subMap.get( "b" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+
+    // "c d"^2
+    assertNotNull( map.get( F2 ) );
+    qpm = map.get( F2 );
+    assertEquals( 1, qpm.subMap.size() );
+    assertNotNull( qpm.subMap.get( "c" ) );
+    qpm2 = qpm.subMap.get( "c" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "d" ) );
+    qpm3 = qpm2.subMap.get( "d" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 2F, qpm3.boost );
+    
+    // phraseHighlight = true, fieldMatch = false
+    fq = new FieldQuery( query, true, false );
+    map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( F1 ) );
+    assertNull( map.get( F2 ) );
+    assertNotNull( map.get( null ) );
+    qpm = map.get( null );
+    assertEquals( 2, qpm.subMap.size() );
+
+    // "a b"
+    assertNotNull( qpm.subMap.get( "a" ) );
+    qpm2 = qpm.subMap.get( "a" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "b" ) );
+    qpm3 = qpm2.subMap.get( "b" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+
+    // "c d"^2
+    assertNotNull( qpm.subMap.get( "c" ) );
+    qpm2 = qpm.subMap.get( "c" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "d" ) );
+    qpm3 = qpm2.subMap.get( "d" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 2F, qpm3.boost );
+  }
+  
+  /*
+   * <t>...terminal
+   * 
+   * a-b-c-<t>
+   *     +-d-<t>
+   * b-c-d-<t>
+   * +-d-<t>
+   */
+  public void testQueryPhraseMapOverlapPhrases() throws Exception {
+    BooleanQuery query = new BooleanQuery();
+    query.add( pqF( "a", "b", "c" ), Occur.SHOULD );
+    query.add( pqF( 2, "b", "c", "d" ), Occur.SHOULD );
+    query.add( pqF( 3, "b", "d" ), Occur.SHOULD );
+    
+    // phraseHighlight = true, fieldMatch = true
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Map<String, QueryPhraseMap> map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( null ) );
+    assertNotNull( map.get( F ) );
+    QueryPhraseMap qpm = map.get( F );
+    assertEquals( 2, qpm.subMap.size() );
+
+    // "a b c"
+    assertNotNull( qpm.subMap.get( "a" ) );
+    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "b" ) );
+    QueryPhraseMap qpm3 = qpm2.subMap.get( "b" );
+    assertFalse( qpm3.terminal );
+    assertEquals( 1, qpm3.subMap.size() );
+    assertNotNull( qpm3.subMap.get( "c" ) );
+    QueryPhraseMap qpm4 = qpm3.subMap.get( "c" );
+    assertTrue( qpm4.terminal );
+    assertEquals( 1F, qpm4.boost );
+    assertNotNull( qpm4.subMap.get( "d" ) );
+    QueryPhraseMap qpm5 = qpm4.subMap.get( "d" );
+    assertTrue( qpm5.terminal );
+    assertEquals( 1F, qpm5.boost );
+
+    // "b c d"^2, "b d"^3
+    assertNotNull( qpm.subMap.get( "b" ) );
+    qpm2 = qpm.subMap.get( "b" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 2, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "c" ) );
+    qpm3 = qpm2.subMap.get( "c" );
+    assertFalse( qpm3.terminal );
+    assertEquals( 1, qpm3.subMap.size() );
+    assertNotNull( qpm3.subMap.get( "d" ) );
+    qpm4 = qpm3.subMap.get( "d" );
+    assertTrue( qpm4.terminal );
+    assertEquals( 2F, qpm4.boost );
+    assertNotNull( qpm2.subMap.get( "d" ) );
+    qpm3 = qpm2.subMap.get( "d" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 3F, qpm3.boost );
+  }
+  
+  /*
+   * <t>...terminal
+   * 
+   * a-b-<t>
+   *   +-c-<t>
+   */
+  public void testQueryPhraseMapOverlapPhrases2() throws Exception {
+    BooleanQuery query = new BooleanQuery();
+    query.add( pqF( "a", "b" ), Occur.SHOULD );
+    query.add( pqF( 2, "a", "b", "c" ), Occur.SHOULD );
+    
+    // phraseHighlight = true, fieldMatch = true
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Map<String, QueryPhraseMap> map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( null ) );
+    assertNotNull( map.get( F ) );
+    QueryPhraseMap qpm = map.get( F );
+    assertEquals( 1, qpm.subMap.size() );
+
+    // "a b"
+    assertNotNull( qpm.subMap.get( "a" ) );
+    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "b" ) );
+    QueryPhraseMap qpm3 = qpm2.subMap.get( "b" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+
+    // "a b c"^2
+    assertEquals( 1, qpm3.subMap.size() );
+    assertNotNull( qpm3.subMap.get( "c" ) );
+    QueryPhraseMap qpm4 = qpm3.subMap.get( "c" );
+    assertTrue( qpm4.terminal );
+    assertEquals( 2F, qpm4.boost );
+  }
+  
+  /*
+   * <t>...terminal
+   * 
+   * a-a-a-<t>
+   *     +-a-<t>
+   *       +-a-<t>
+   *         +-a-<t>
+   */
+  public void testQueryPhraseMapOverlapPhrases3() throws Exception {
+    BooleanQuery query = new BooleanQuery();
+    query.add( pqF( "a", "a", "a", "a" ), Occur.SHOULD );
+    query.add( pqF( 2, "a", "a", "a" ), Occur.SHOULD );
+    
+    // phraseHighlight = true, fieldMatch = true
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Map<String, QueryPhraseMap> map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( null ) );
+    assertNotNull( map.get( F ) );
+    QueryPhraseMap qpm = map.get( F );
+    assertEquals( 1, qpm.subMap.size() );
+
+    // "a a a"
+    assertNotNull( qpm.subMap.get( "a" ) );
+    QueryPhraseMap qpm2 = qpm.subMap.get( "a" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "a" ) );
+    QueryPhraseMap qpm3 = qpm2.subMap.get( "a" );
+    assertFalse( qpm3.terminal );
+    assertEquals( 1, qpm3.subMap.size() );
+    assertNotNull( qpm3.subMap.get( "a" ) );
+    QueryPhraseMap qpm4 = qpm3.subMap.get( "a" );
+    assertTrue( qpm4.terminal );
+
+    // "a a a a"
+    assertEquals( 1, qpm4.subMap.size() );
+    assertNotNull( qpm4.subMap.get( "a" ) );
+    QueryPhraseMap qpm5 = qpm4.subMap.get( "a" );
+    assertTrue( qpm5.terminal );
+
+    // "a a a a a"
+    assertEquals( 1, qpm5.subMap.size() );
+    assertNotNull( qpm5.subMap.get( "a" ) );
+    QueryPhraseMap qpm6 = qpm5.subMap.get( "a" );
+    assertTrue( qpm6.terminal );
+
+    // "a a a a a a"
+    assertEquals( 1, qpm6.subMap.size() );
+    assertNotNull( qpm6.subMap.get( "a" ) );
+    QueryPhraseMap qpm7 = qpm6.subMap.get( "a" );
+    assertTrue( qpm7.terminal );
+  }
+  
+  public void testQueryPhraseMapOverlap2gram() throws Exception {
+    Query query = paB.parse( "abc AND bcd" );
+    
+    // phraseHighlight = true, fieldMatch = true
+    FieldQuery fq = new FieldQuery( query, true, true );
+    Map<String, QueryPhraseMap> map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( null ) );
+    assertNotNull( map.get( F ) );
+    QueryPhraseMap qpm = map.get( F );
+    assertEquals( 2, qpm.subMap.size() );
+
+    // "ab bc"
+    assertNotNull( qpm.subMap.get( "ab" ) );
+    QueryPhraseMap qpm2 = qpm.subMap.get( "ab" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "bc" ) );
+    QueryPhraseMap qpm3 = qpm2.subMap.get( "bc" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+
+    // "ab bc cd"
+    assertEquals( 1, qpm3.subMap.size() );
+    assertNotNull( qpm3.subMap.get( "cd" ) );
+    QueryPhraseMap qpm4 = qpm3.subMap.get( "cd" );
+    assertTrue( qpm4.terminal );
+    assertEquals( 1F, qpm4.boost );
+
+    // "bc cd"
+    assertNotNull( qpm.subMap.get( "bc" ) );
+    qpm2 = qpm.subMap.get( "bc" );
+    assertFalse( qpm2.terminal );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "cd" ) );
+    qpm3 = qpm2.subMap.get( "cd" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+    
+    // phraseHighlight = false, fieldMatch = true
+    fq = new FieldQuery( query, false, true );
+    map = fq.rootMaps;
+    assertEquals( 1, map.size() );
+    assertNull( map.get( null ) );
+    assertNotNull( map.get( F ) );
+    qpm = map.get( F );
+    assertEquals( 3, qpm.subMap.size() );
+
+    // "ab bc"
+    assertNotNull( qpm.subMap.get( "ab" ) );
+    qpm2 = qpm.subMap.get( "ab" );
+    assertTrue( qpm2.terminal );
+    assertEquals( 1F, qpm2.boost );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "bc" ) );
+    qpm3 = qpm2.subMap.get( "bc" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+
+    // "ab bc cd"
+    assertEquals( 1, qpm3.subMap.size() );
+    assertNotNull( qpm3.subMap.get( "cd" ) );
+    qpm4 = qpm3.subMap.get( "cd" );
+    assertTrue( qpm4.terminal );
+    assertEquals( 1F, qpm4.boost );
+
+    // "bc cd"
+    assertNotNull( qpm.subMap.get( "bc" ) );
+    qpm2 = qpm.subMap.get( "bc" );
+    assertTrue( qpm2.terminal );
+    assertEquals( 1F, qpm2.boost );
+    assertEquals( 1, qpm2.subMap.size() );
+    assertNotNull( qpm2.subMap.get( "cd" ) );
+    qpm3 = qpm2.subMap.get( "cd" );
+    assertTrue( qpm3.terminal );
+    assertEquals( 1F, qpm3.boost );
+
+    // "cd"
+    assertNotNull( qpm.subMap.get( "cd" ) );
+    qpm2 = qpm.subMap.get( "cd" );
+    assertTrue( qpm2.terminal );
+    assertEquals( 1F, qpm2.boost );
+    assertEquals( 0, qpm2.subMap.size() );
+  }
+  
+  public void testSearchPhrase() throws Exception {
+    Query query = pqF( "a", "b", "c" );
+
+    // phraseHighlight = true, fieldMatch = true
+    FieldQuery fq = new FieldQuery( query, true, true );
+    
+    // "a"
+    List<TermInfo> phraseCandidate = new ArrayList<TermInfo>();
+    phraseCandidate.add( new TermInfo( "a", 0, 1, 0 ) );
+    assertNull( fq.searchPhrase( F, phraseCandidate ) );
+    // "a b"
+    phraseCandidate.add( new TermInfo( "b", 2, 3, 1 ) );
+    assertNull( fq.searchPhrase( F, phraseCandidate ) );
+    // "a b c"
+    phraseCandidate.add( new TermInfo( "c", 4, 5, 2 ) );
+    assertNotNull( fq.searchPhrase( F, phraseCandidate ) );
+    assertNull( fq.searchPhrase( "x", phraseCandidate ) );
+
+    // phraseHighlight = true, fieldMatch = false
+    fq = new FieldQuery( query, true, false );
+    
+    // "a b c"
+    assertNotNull( fq.searchPhrase( F, phraseCandidate ) );
+    assertNotNull( fq.searchPhrase( "x", phraseCandidate ) );
+
+    // phraseHighlight = false, fieldMatch = true
+    fq = new FieldQuery( query, false, true );
+    
+    // "a"
+    phraseCandidate.clear();
+    phraseCandidate.add( new TermInfo( "a", 0, 1, 0 ) );
+    assertNotNull( fq.searchPhrase( F, phraseCandidate ) );
+    // "a b"
+    phraseCandidate.add( new TermInfo( "b", 2, 3, 1 ) );
+    assertNull( fq.searchPhrase( F, phraseCandidate ) );
+    // "a b c"
+    phraseCandidate.add( new TermInfo( "c", 4, 5, 2 ) );
+    assertNotNull( fq.searchPhrase( F, phraseCandidate ) );
+    assertNull( fq.searchPhrase( "x", phraseCandidate ) );
+  }
+  
+  public void testSearchPhraseSlop() throws Exception {
+    // "a b c"~0
+    Query query = pqF( "a", "b", "c" );
+
+    // phraseHighlight = true, fieldMatch = true
+    FieldQuery fq = new FieldQuery( query, true, true );
+    
+    // "a b c" w/ position-gap = 2
+    List<TermInfo> phraseCandidate = new ArrayList<TermInfo>();
+    phraseCandidate.add( new TermInfo( "a", 0, 1, 0 ) );
+    phraseCandidate.add( new TermInfo( "b", 2, 3, 2 ) );
+    phraseCandidate.add( new TermInfo( "c", 4, 5, 4 ) );
+    assertNull( fq.searchPhrase( F, phraseCandidate ) );
+
+    // "a b c"~1
+    query = pqF( 1F, 1, "a", "b", "c" );
+
+    // phraseHighlight = true, fieldMatch = true
+    fq = new FieldQuery( query, true, true );
+    
+    // "a b c" w/ position-gap = 2
+    assertNotNull( fq.searchPhrase( F, phraseCandidate ) );
+    
+    // "a b c" w/ position-gap = 3
+    phraseCandidate.clear();
+    phraseCandidate.add( new TermInfo( "a", 0, 1, 0 ) );
+    phraseCandidate.add( new TermInfo( "b", 2, 3, 3 ) );
+    phraseCandidate.add( new TermInfo( "c", 4, 5, 6 ) );
+    assertNull( fq.searchPhrase( F, phraseCandidate ) );
+  }
+}
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldTermStackTest.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldTermStackTest.java
new file mode 100644
index 0000000..e434388
--- /dev/null
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/FieldTermStackTest.java
@@ -0,0 +1,161 @@
+package org.apache.lucene.search.vectorhighlight;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.BooleanClause.Occur;
+
+public class FieldTermStackTest extends AbstractTestCase {
+  
+  public void test1Term() throws Exception {
+    makeIndex();
+    
+    FieldQuery fq = new FieldQuery( tq( "a" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 6, stack.termList.size() );
+    assertEquals( "a(0,1,0)", stack.pop().toString() );
+    assertEquals( "a(2,3,1)", stack.pop().toString() );
+    assertEquals( "a(4,5,2)", stack.pop().toString() );
+    assertEquals( "a(12,13,6)", stack.pop().toString() );
+    assertEquals( "a(28,29,14)", stack.pop().toString() );
+    assertEquals( "a(32,33,16)", stack.pop().toString() );
+  }
+  
+  public void test2Terms() throws Exception {
+    makeIndex();
+    
+    BooleanQuery query = new BooleanQuery();
+    query.add( tq( "b" ), Occur.SHOULD );
+    query.add( tq( "c" ), Occur.SHOULD );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 8, stack.termList.size() );
+    assertEquals( "b(6,7,3)", stack.pop().toString() );
+    assertEquals( "b(8,9,4)", stack.pop().toString() );
+    assertEquals( "c(10,11,5)", stack.pop().toString() );
+    assertEquals( "b(14,15,7)", stack.pop().toString() );
+    assertEquals( "b(16,17,8)", stack.pop().toString() );
+    assertEquals( "c(18,19,9)", stack.pop().toString() );
+    assertEquals( "b(26,27,13)", stack.pop().toString() );
+    assertEquals( "b(30,31,15)", stack.pop().toString() );
+  }
+  
+  public void test1Phrase() throws Exception {
+    makeIndex();
+    
+    FieldQuery fq = new FieldQuery( pqF( "c", "d" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 3, stack.termList.size() );
+    assertEquals( "c(10,11,5)", stack.pop().toString() );
+    assertEquals( "c(18,19,9)", stack.pop().toString() );
+    assertEquals( "d(20,21,10)", stack.pop().toString() );
+  }
+  
+  private void makeIndex() throws Exception {
+    //           111111111122222
+    // 0123456789012345678901234 (offsets)
+    // a a a b b c a b b c d e f
+    // 0 1 2 3 4 5 6 7 8 9101112 (position)
+    String value1 = "a a a b b c a b b c d e f";
+    // 222233333
+    // 678901234 (offsets)
+    // b a b a f
+    //1314151617 (position)
+    String value2 = "b a b a f";
+    
+    make1dmfIndex( value1, value2 );
+  }
+  
+  public void test1TermB() throws Exception {
+    makeIndexB();
+    
+    FieldQuery fq = new FieldQuery( tq( "ab" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 2, stack.termList.size() );
+    assertEquals( "ab(2,4,2)", stack.pop().toString() );
+    assertEquals( "ab(6,8,6)", stack.pop().toString() );
+  }
+  
+  public void test2TermsB() throws Exception {
+    makeIndexB();
+    
+    BooleanQuery query = new BooleanQuery();
+    query.add( tq( "bc" ), Occur.SHOULD );
+    query.add( tq( "ef" ), Occur.SHOULD );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 3, stack.termList.size() );
+    assertEquals( "bc(4,6,4)", stack.pop().toString() );
+    assertEquals( "bc(8,10,8)", stack.pop().toString() );
+    assertEquals( "ef(11,13,11)", stack.pop().toString() );
+  }
+  
+  public void test1PhraseB() throws Exception {
+    makeIndexB();
+    
+    FieldQuery fq = new FieldQuery( pqF( "ab", "bb" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 4, stack.termList.size() );
+    assertEquals( "ab(2,4,2)", stack.pop().toString() );
+    assertEquals( "bb(3,5,3)", stack.pop().toString() );
+    assertEquals( "ab(6,8,6)", stack.pop().toString() );
+    assertEquals( "bb(7,9,7)", stack.pop().toString() );
+  }
+  
+  private void makeIndexB() throws Exception {
+    //                             1 11 11
+    // 01 12 23 34 45 56 67 78 89 90 01 12 (offsets)
+    // aa|aa|ab|bb|bc|ca|ab|bb|bc|cd|de|ef
+    //  0  1  2  3  4  5  6  7  8  9 10 11 (position)
+    String value = "aaabbcabbcdef";
+    
+    make1dmfIndexB( value );
+  }
+  
+  public void test1PhraseShortMV() throws Exception {
+    makeIndexShortMV();
+    
+    FieldQuery fq = new FieldQuery( tq( "d" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 1, stack.termList.size() );
+    assertEquals( "d(6,7,3)", stack.pop().toString() );
+  }
+  
+  public void test1PhraseLongMV() throws Exception {
+    makeIndexLongMV();
+    
+    FieldQuery fq = new FieldQuery( pqF( "search", "engines" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 4, stack.termList.size() );
+    assertEquals( "search(102,108,14)", stack.pop().toString() );
+    assertEquals( "engines(109,116,15)", stack.pop().toString() );
+    assertEquals( "search(157,163,24)", stack.pop().toString() );
+    assertEquals( "engines(164,171,25)", stack.pop().toString() );
+  }
+
+  public void test1PhraseMVB() throws Exception {
+    makeIndexLongMVB();
+    
+    FieldQuery fq = new FieldQuery( pqF( "sp", "pe", "ee", "ed" ), true, true ); // "speed" -(2gram)-> "sp","pe","ee","ed"
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 4, stack.termList.size() );
+    assertEquals( "sp(88,90,61)", stack.pop().toString() );
+    assertEquals( "pe(89,91,62)", stack.pop().toString() );
+    assertEquals( "ee(90,92,63)", stack.pop().toString() );
+    assertEquals( "ed(91,93,64)", stack.pop().toString() );
+  }
+}
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.java
new file mode 100644
index 0000000..1134d38
--- /dev/null
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/IndexTimeSynonymTest.java
@@ -0,0 +1,318 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.Reader;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.util.AttributeImpl;
+
+public class IndexTimeSynonymTest extends AbstractTestCase {
+  
+  public void testFieldTermStackIndex1wSearch1term() throws Exception {
+    makeIndex1w();
+    
+    FieldQuery fq = new FieldQuery( tq( "Mac" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 1, stack.termList.size() );
+    assertEquals( "Mac(11,20,3)", stack.pop().toString() );
+  }
+  
+  public void testFieldTermStackIndex1wSearch2terms() throws Exception {
+    makeIndex1w();
+
+    BooleanQuery bq = new BooleanQuery();
+    bq.add( tq( "Mac" ), Occur.SHOULD );
+    bq.add( tq( "MacBook" ), Occur.SHOULD );
+    FieldQuery fq = new FieldQuery( bq, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 2, stack.termList.size() );
+    Set<String> expectedSet = new HashSet<String>();
+    expectedSet.add( "Mac(11,20,3)" );
+    expectedSet.add( "MacBook(11,20,3)" );
+    assertTrue( expectedSet.contains( stack.pop().toString() ) );
+    assertTrue( expectedSet.contains( stack.pop().toString() ) );
+  }
+  
+  public void testFieldTermStackIndex1w2wSearch1term() throws Exception {
+    makeIndex1w2w();
+    
+    FieldQuery fq = new FieldQuery( tq( "pc" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 1, stack.termList.size() );
+    assertEquals( "pc(3,5,1)", stack.pop().toString() );
+  }
+  
+  public void testFieldTermStackIndex1w2wSearch1phrase() throws Exception {
+    makeIndex1w2w();
+    
+    FieldQuery fq = new FieldQuery( pqF( "personal", "computer" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 2, stack.termList.size() );
+    assertEquals( "personal(3,5,1)", stack.pop().toString() );
+    assertEquals( "computer(3,5,2)", stack.pop().toString() );
+  }
+  
+  public void testFieldTermStackIndex1w2wSearch1partial() throws Exception {
+    makeIndex1w2w();
+    
+    FieldQuery fq = new FieldQuery( tq( "computer" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 1, stack.termList.size() );
+    assertEquals( "computer(3,5,2)", stack.pop().toString() );
+  }
+  
+  public void testFieldTermStackIndex1w2wSearch1term1phrase() throws Exception {
+    makeIndex1w2w();
+
+    BooleanQuery bq = new BooleanQuery();
+    bq.add( tq( "pc" ), Occur.SHOULD );
+    bq.add( pqF( "personal", "computer" ), Occur.SHOULD );
+    FieldQuery fq = new FieldQuery( bq, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 3, stack.termList.size() );
+    Set<String> expectedSet = new HashSet<String>();
+    expectedSet.add( "pc(3,5,1)" );
+    expectedSet.add( "personal(3,5,1)" );
+    assertTrue( expectedSet.contains( stack.pop().toString() ) );
+    assertTrue( expectedSet.contains( stack.pop().toString() ) );
+    assertEquals( "computer(3,5,2)", stack.pop().toString() );
+  }
+  
+  public void testFieldTermStackIndex2w1wSearch1term() throws Exception {
+    makeIndex2w1w();
+    
+    FieldQuery fq = new FieldQuery( tq( "pc" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 1, stack.termList.size() );
+    assertEquals( "pc(3,20,1)", stack.pop().toString() );
+  }
+  
+  public void testFieldTermStackIndex2w1wSearch1phrase() throws Exception {
+    makeIndex2w1w();
+    
+    FieldQuery fq = new FieldQuery( pqF( "personal", "computer" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 2, stack.termList.size() );
+    assertEquals( "personal(3,20,1)", stack.pop().toString() );
+    assertEquals( "computer(3,20,2)", stack.pop().toString() );
+  }
+  
+  public void testFieldTermStackIndex2w1wSearch1partial() throws Exception {
+    makeIndex2w1w();
+    
+    FieldQuery fq = new FieldQuery( tq( "computer" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 1, stack.termList.size() );
+    assertEquals( "computer(3,20,2)", stack.pop().toString() );
+  }
+  
+  public void testFieldTermStackIndex2w1wSearch1term1phrase() throws Exception {
+    makeIndex2w1w();
+
+    BooleanQuery bq = new BooleanQuery();
+    bq.add( tq( "pc" ), Occur.SHOULD );
+    bq.add( pqF( "personal", "computer" ), Occur.SHOULD );
+    FieldQuery fq = new FieldQuery( bq, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    assertEquals( 3, stack.termList.size() );
+    Set<String> expectedSet = new HashSet<String>();
+    expectedSet.add( "pc(3,20,1)" );
+    expectedSet.add( "personal(3,20,1)" );
+    assertTrue( expectedSet.contains( stack.pop().toString() ) );
+    assertTrue( expectedSet.contains( stack.pop().toString() ) );
+    assertEquals( "computer(3,20,2)", stack.pop().toString() );
+  }
+  
+  public void testFieldPhraseListIndex1w2wSearch1phrase() throws Exception {
+    makeIndex1w2w();
+    
+    FieldQuery fq = new FieldQuery( pqF( "personal", "computer" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "personalcomputer(1.0)((3,5))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
+    assertEquals( 5, fpl.phraseList.get( 0 ).getEndOffset() );
+  }
+  
+  public void testFieldPhraseListIndex1w2wSearch1partial() throws Exception {
+    makeIndex1w2w();
+    
+    FieldQuery fq = new FieldQuery( tq( "computer" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "computer(1.0)((3,5))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
+    assertEquals( 5, fpl.phraseList.get( 0 ).getEndOffset() );
+  }
+  
+  public void testFieldPhraseListIndex1w2wSearch1term1phrase() throws Exception {
+    makeIndex1w2w();
+
+    BooleanQuery bq = new BooleanQuery();
+    bq.add( tq( "pc" ), Occur.SHOULD );
+    bq.add( pqF( "personal", "computer" ), Occur.SHOULD );
+    FieldQuery fq = new FieldQuery( bq, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertTrue( fpl.phraseList.get( 0 ).toString().indexOf( "(1.0)((3,5))" ) > 0 );
+    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
+    assertEquals( 5, fpl.phraseList.get( 0 ).getEndOffset() );
+  }
+  
+  public void testFieldPhraseListIndex2w1wSearch1term() throws Exception {
+    makeIndex2w1w();
+    
+    FieldQuery fq = new FieldQuery( tq( "pc" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "pc(1.0)((3,20))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
+    assertEquals( 20, fpl.phraseList.get( 0 ).getEndOffset() );
+  }
+  
+  public void testFieldPhraseListIndex2w1wSearch1phrase() throws Exception {
+    makeIndex2w1w();
+    
+    FieldQuery fq = new FieldQuery( pqF( "personal", "computer" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "personalcomputer(1.0)((3,20))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
+    assertEquals( 20, fpl.phraseList.get( 0 ).getEndOffset() );
+  }
+  
+  public void testFieldPhraseListIndex2w1wSearch1partial() throws Exception {
+    makeIndex2w1w();
+    
+    FieldQuery fq = new FieldQuery( tq( "computer" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertEquals( "computer(1.0)((3,20))", fpl.phraseList.get( 0 ).toString() );
+    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
+    assertEquals( 20, fpl.phraseList.get( 0 ).getEndOffset() );
+  }
+  
+  public void testFieldPhraseListIndex2w1wSearch1term1phrase() throws Exception {
+    makeIndex2w1w();
+
+    BooleanQuery bq = new BooleanQuery();
+    bq.add( tq( "pc" ), Occur.SHOULD );
+    bq.add( pqF( "personal", "computer" ), Occur.SHOULD );
+    FieldQuery fq = new FieldQuery( bq, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    assertEquals( 1, fpl.phraseList.size() );
+    assertTrue( fpl.phraseList.get( 0 ).toString().indexOf( "(1.0)((3,20))" ) > 0 );
+    assertEquals( 3, fpl.phraseList.get( 0 ).getStartOffset() );
+    assertEquals( 20, fpl.phraseList.get( 0 ).getEndOffset() );
+  }
+
+  private void makeIndex1w() throws Exception {
+    //           11111111112
+    // 012345678901234567890
+    // I'll buy a Macintosh
+    //            Mac
+    //            MacBook
+    // 0    1   2 3
+    makeSynonymIndex( "I'll buy a Macintosh",
+        t("I'll",0,4),
+        t("buy",5,8),
+        t("a",9,10),
+        t("Macintosh",11,20),t("Mac",11,20,0),t("MacBook",11,20,0));
+  }
+
+  private void makeIndex1w2w() throws Exception {
+    //           1111111
+    // 01234567890123456
+    // My pc was broken
+    //    personal computer
+    // 0  1  2   3
+    makeSynonymIndex( "My pc was broken",
+        t("My",0,2),
+        t("pc",3,5),t("personal",3,5,0),t("computer",3,5),
+        t("was",6,9),
+        t("broken",10,16));
+  }
+
+  private void makeIndex2w1w() throws Exception {
+    //           1111111111222222222233
+    // 01234567890123456789012345678901
+    // My personal computer was broken
+    //    pc
+    // 0  1        2        3   4
+    makeSynonymIndex( "My personal computer was broken",
+        t("My",0,2),
+        t("personal",3,20),t("pc",3,20,0),t("computer",3,20),
+        t("was",21,24),
+        t("broken",25,31));
+  }
+  
+  void makeSynonymIndex( String value, Token... tokens ) throws Exception {
+    Analyzer analyzer = new TokenArrayAnalyzer( tokens );
+    make1dmfIndex( analyzer, value );
+  }
+
+  public static Token t( String text, int startOffset, int endOffset ){
+    return t( text, startOffset, endOffset, 1 );
+  }
+  
+  public static Token t( String text, int startOffset, int endOffset, int positionIncrement ){
+    Token token = new Token( text, startOffset, endOffset );
+    token.setPositionIncrement( positionIncrement );
+    return token;
+  }
+  
+  public static class TokenArrayAnalyzer extends Analyzer {
+    Token[] tokens;
+    public TokenArrayAnalyzer( Token... tokens ){
+      this.tokens = tokens;
+    }
+    
+    @Override
+    public TokenStream tokenStream(String fieldName, Reader reader) {      
+      TokenStream ts = new TokenStream(Token.TOKEN_ATTRIBUTE_FACTORY) {
+        final AttributeImpl reusableToken = (AttributeImpl) addAttribute(TermAttribute.class);
+        int p = 0;
+        
+        @Override
+        public boolean incrementToken() throws IOException {
+          if( p >= tokens.length ) return false;
+          clearAttributes();
+          tokens[p++].copyTo(reusableToken);
+          return true;
+        }
+      };
+      return ts;
+    }
+  }
+}
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilderTest.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilderTest.java
new file mode 100644
index 0000000..47ca7ed
--- /dev/null
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/ScoreOrderFragmentsBuilderTest.java
@@ -0,0 +1,43 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.Query;
+
+public class ScoreOrderFragmentsBuilderTest extends AbstractTestCase {
+  
+  public void test3Frags() throws Exception {
+    FieldFragList ffl = ffl( "a c", "a b b b b b b b b b b b a b a b b b b b c a a b b" );
+    ScoreOrderFragmentsBuilder sofb = new ScoreOrderFragmentsBuilder();
+    String[] f = sofb.createFragments( reader, 0, F, ffl, 3 );
+    assertEquals( 3, f.length );
+    // check score order
+    assertEquals( "<b>c</b> <b>a</b> <b>a</b> b b", f[0] );
+    assertEquals( "b b <b>a</b> b <b>a</b> b b b b b ", f[1] );
+    assertEquals( "<b>a</b> b b b b b b b b b ", f[2] );
+  }
+
+  private FieldFragList ffl( String queryValue, String indexValue ) throws Exception {
+    make1d1fIndex( indexValue );
+    Query query = paW.parse( queryValue );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    return new SimpleFragListBuilder().createFieldFragList( fpl, 20 );
+  }
+}
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilderTest.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilderTest.java
new file mode 100644
index 0000000..59f7aba
--- /dev/null
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragListBuilderTest.java
@@ -0,0 +1,172 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.Query;
+
+public class SimpleFragListBuilderTest extends AbstractTestCase {
+  
+  public void testNullFieldFragList() throws Exception {
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl( "a", "b c d" ), 100 );
+    assertEquals( 0, ffl.fragInfos.size() );
+  }
+  
+  public void testTooSmallFragSize() throws Exception {
+    try{
+      SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+      sflb.createFieldFragList( fpl( "a", "b c d" ), SimpleFragListBuilder.MIN_FRAG_CHAR_SIZE - 1 );
+      fail( "IllegalArgumentException must be thrown" );
+    }
+    catch ( IllegalArgumentException expected ) {
+    }
+  }
+  
+  public void testSmallerFragSizeThanTermQuery() throws Exception {
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl( "abcdefghijklmnopqrs", "abcdefghijklmnopqrs" ), SimpleFragListBuilder.MIN_FRAG_CHAR_SIZE );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(abcdefghijklmnopqrs((0,19)))/1.0(0,19)", ffl.fragInfos.get( 0 ).toString() );
+  }
+  
+  public void testSmallerFragSizeThanPhraseQuery() throws Exception {
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl( "\"abcdefgh jklmnopqrs\"", "abcdefgh   jklmnopqrs" ), SimpleFragListBuilder.MIN_FRAG_CHAR_SIZE );
+    assertEquals( 1, ffl.fragInfos.size() );
+    if (VERBOSE) System.out.println( ffl.fragInfos.get( 0 ).toString() );
+    assertEquals( "subInfos=(abcdefghjklmnopqrs((0,21)))/1.0(0,21)", ffl.fragInfos.get( 0 ).toString() );
+  }
+  
+  public void test1TermIndex() throws Exception {
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl( "a", "a" ), 100 );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(a((0,1)))/1.0(0,100)", ffl.fragInfos.get( 0 ).toString() );
+  }
+  
+  public void test2TermsIndex1Frag() throws Exception {
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl( "a", "a a" ), 100 );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(a((0,1))a((2,3)))/2.0(0,100)", ffl.fragInfos.get( 0 ).toString() );
+
+    ffl = sflb.createFieldFragList( fpl( "a", "a b b b b b b b b a" ), 20 );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(a((0,1))a((18,19)))/2.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
+    
+    ffl = sflb.createFieldFragList( fpl( "a", "b b b b a b b b b a" ), 20 );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(a((8,9))a((18,19)))/2.0(2,22)", ffl.fragInfos.get( 0 ).toString() );
+  }
+  
+  public void test2TermsIndex2Frags() throws Exception {
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl( "a", "a b b b b b b b b b b b b b a" ), 20 );
+    assertEquals( 2, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(a((0,1)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
+    assertEquals( "subInfos=(a((28,29)))/1.0(22,42)", ffl.fragInfos.get( 1 ).toString() );
+
+    ffl = sflb.createFieldFragList( fpl( "a", "a b b b b b b b b b b b b a" ), 20 );
+    assertEquals( 2, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(a((0,1)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
+    assertEquals( "subInfos=(a((26,27)))/1.0(20,40)", ffl.fragInfos.get( 1 ).toString() );
+
+    ffl = sflb.createFieldFragList( fpl( "a", "a b b b b b b b b b a" ), 20 );
+    assertEquals( 2, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(a((0,1)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
+    assertEquals( "subInfos=(a((20,21)))/1.0(20,40)", ffl.fragInfos.get( 1 ).toString() );
+  }
+  
+  public void test2TermsQuery() throws Exception {
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl( "a b", "c d e" ), 20 );
+    assertEquals( 0, ffl.fragInfos.size() );
+
+    ffl = sflb.createFieldFragList( fpl( "a b", "d b c" ), 20 );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(b((2,3)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
+
+    ffl = sflb.createFieldFragList( fpl( "a b", "a b c" ), 20 );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(a((0,1))b((2,3)))/2.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
+  }
+  
+  public void testPhraseQuery() throws Exception {
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl( "\"a b\"", "c d e" ), 20 );
+    assertEquals( 0, ffl.fragInfos.size() );
+
+    ffl = sflb.createFieldFragList( fpl( "\"a b\"", "a c b" ), 20 );
+    assertEquals( 0, ffl.fragInfos.size() );
+
+    ffl = sflb.createFieldFragList( fpl( "\"a b\"", "a b c" ), 20 );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(ab((0,3)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
+  }
+  
+  public void testPhraseQuerySlop() throws Exception {
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl( "\"a b\"~1", "a c b" ), 20 );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(ab((0,1)(4,5)))/1.0(0,20)", ffl.fragInfos.get( 0 ).toString() );
+  }
+
+  private FieldPhraseList fpl( String queryValue, String indexValue ) throws Exception {
+    make1d1fIndex( indexValue );
+    Query query = paW.parse( queryValue );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    return new FieldPhraseList( stack, fq );
+  }
+  
+  public void test1PhraseShortMV() throws Exception {
+    makeIndexShortMV();
+
+    FieldQuery fq = new FieldQuery( tq( "d" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(d((6,7)))/1.0(0,100)", ffl.fragInfos.get( 0 ).toString() );
+  }
+  
+  public void test1PhraseLongMV() throws Exception {
+    makeIndexLongMV();
+
+    FieldQuery fq = new FieldQuery( pqF( "search", "engines" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(searchengines((102,116))searchengines((157,171)))/2.0(96,196)", ffl.fragInfos.get( 0 ).toString() );
+  }
+
+  public void test1PhraseLongMVB() throws Exception {
+    makeIndexLongMVB();
+
+    FieldQuery fq = new FieldQuery( pqF( "sp", "pe", "ee", "ed" ), true, true ); // "speed" -(2gram)-> "sp","pe","ee","ed"
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
+    assertEquals( 1, ffl.fragInfos.size() );
+    assertEquals( "subInfos=(sppeeeed((88,93)))/1.0(82,182)", ffl.fragInfos.get( 0 ).toString() );
+  }
+}
diff --git a/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java
new file mode 100644
index 0000000..cf5bafe
--- /dev/null
+++ b/lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/SimpleFragmentsBuilderTest.java
@@ -0,0 +1,143 @@
+package org.apache.lucene.search.vectorhighlight;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Index;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.Field.TermVector;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.search.Query;
+
+public class SimpleFragmentsBuilderTest extends AbstractTestCase {
+  
+  public void test1TermIndex() throws Exception {
+    FieldFragList ffl = ffl( "a", "a" );
+    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
+    assertEquals( "<b>a</b>", sfb.createFragment( reader, 0, F, ffl ) );
+
+    // change tags
+    sfb = new SimpleFragmentsBuilder( new String[]{ "[" }, new String[]{ "]" } );
+    assertEquals( "[a]", sfb.createFragment( reader, 0, F, ffl ) );
+  }
+  
+  public void test2Frags() throws Exception {
+    FieldFragList ffl = ffl( "a", "a b b b b b b b b b b b a b a b" );
+    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
+    String[] f = sfb.createFragments( reader, 0, F, ffl, 3 );
+    // 3 snippets requested, but should be 2
+    assertEquals( 2, f.length );
+    assertEquals( "<b>a</b> b b b b b b b b b ", f[0] );
+    assertEquals( "b b <b>a</b> b <b>a</b> b", f[1] );
+  }
+  
+  public void test3Frags() throws Exception {
+    FieldFragList ffl = ffl( "a c", "a b b b b b b b b b b b a b a b b b b b c a a b b" );
+    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
+    String[] f = sfb.createFragments( reader, 0, F, ffl, 3 );
+    assertEquals( 3, f.length );
+    assertEquals( "<b>a</b> b b b b b b b b b ", f[0] );
+    assertEquals( "b b <b>a</b> b <b>a</b> b b b b b ", f[1] );
+    assertEquals( "<b>c</b> <b>a</b> <b>a</b> b b", f[2] );
+  }
+
+  private FieldFragList ffl( String queryValue, String indexValue ) throws Exception {
+    make1d1fIndex( indexValue );
+    Query query = paW.parse( queryValue );
+    FieldQuery fq = new FieldQuery( query, true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    return new SimpleFragListBuilder().createFieldFragList( fpl, 20 );
+  }
+  
+  public void test1PhraseShortMV() throws Exception {
+    makeIndexShortMV();
+
+    FieldQuery fq = new FieldQuery( tq( "d" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
+    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
+    assertEquals( "a b c <b>d</b> e", sfb.createFragment( reader, 0, F, ffl ) );
+  }
+  
+  public void test1PhraseLongMV() throws Exception {
+    makeIndexLongMV();
+
+    FieldQuery fq = new FieldQuery( pqF( "search", "engines" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
+    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
+    assertEquals( " most <b>search engines</b> use only one of these methods. Even the <b>search engines</b> that says they can use t",
+        sfb.createFragment( reader, 0, F, ffl ) );
+  }
+
+  public void test1PhraseLongMVB() throws Exception {
+    makeIndexLongMVB();
+
+    FieldQuery fq = new FieldQuery( pqF( "sp", "pe", "ee", "ed" ), true, true ); // "speed" -(2gram)-> "sp","pe","ee","ed"
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
+    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
+    assertEquals( "ssing <b>speed</b>, the", sfb.createFragment( reader, 0, F, ffl ) );
+  }
+  
+  public void testUnstoredField() throws Exception {
+    makeUnstoredIndex();
+
+    FieldQuery fq = new FieldQuery( tq( "aaa" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
+    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
+    assertNull( sfb.createFragment( reader, 0, F, ffl ) );
+  }
+  
+  protected void makeUnstoredIndex() throws Exception {
+    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, analyzerW).setOpenMode(OpenMode.CREATE));
+    Document doc = new Document();
+    doc.add( new Field( F, "aaa", Store.NO, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS ) );
+    writer.addDocument( doc );
+    writer.close();
+
+    reader = IndexReader.open( dir, true );
+  }
+  
+  public void test1StrMV() throws Exception {
+    makeIndexStrMV();
+
+    FieldQuery fq = new FieldQuery( tq( "defg" ), true, true );
+    FieldTermStack stack = new FieldTermStack( reader, 0, F, fq );
+    FieldPhraseList fpl = new FieldPhraseList( stack, fq );
+    SimpleFragListBuilder sflb = new SimpleFragListBuilder();
+    FieldFragList ffl = sflb.createFieldFragList( fpl, 100 );
+    SimpleFragmentsBuilder sfb = new SimpleFragmentsBuilder();
+    assertEquals( "abc<b>defg</b>hijkl", sfb.createFragment( reader, 0, F, ffl ) );
+  }
+}
diff --git a/lucene/contrib/misc/build.xml b/lucene/contrib/misc/build.xml
index ad311b5..ad8749c 100644
--- a/lucene/contrib/misc/build.xml
+++ b/lucene/contrib/misc/build.xml
@@ -27,17 +27,4 @@
 
   <import file="../contrib-build.xml"/>
 
-  <property name="javacc.path" location="src/java/org/apache/lucene/queryParser/precedence"/>
-
-  <target name="javacc" depends="javacc-check" description="generate precedence query parser from jj (requires javacc 3.2)">
-    <delete>
-      <fileset dir="${javacc.path}" includes="*.java">
-        <containsregexp expression="Generated.*By.*JavaCC"/>
-      </fileset>
-    </delete>
-    <invoke-javacc target="${javacc.path}/PrecedenceQueryParser.jj"
-                   outputDir="${javacc.path}"
-    />
-  </target>
-
 </project>
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
deleted file mode 100644
index 1dac672..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
+++ /dev/null
@@ -1,322 +0,0 @@
-package org.apache.lucene.queryParser.analyzing;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.Version;
-
-/**
- * Overrides Lucene's default QueryParser so that Fuzzy-, Prefix-, Range-, and WildcardQuerys
- * are also passed through the given analyzer, but wild card characters (like <code>*</code>) 
- * don't get removed from the search terms.
- * 
- * <p><b>Warning:</b> This class should only be used with analyzers that do not use stopwords
- * or that add tokens. Also, several stemming analyzers are inappropriate: for example, GermanAnalyzer 
- * will turn <code>H&auml;user</code> into <code>hau</code>, but <code>H?user</code> will 
- * become <code>h?user</code> when using this parser and thus no match would be found (i.e.
- * using this parser will be no improvement over QueryParser in such cases). 
- *
- * @version $Revision$, $Date$
- */
-public class AnalyzingQueryParser extends org.apache.lucene.queryParser.QueryParser {
-
-  /**
-   * Constructs a query parser.
-   * @param field    the default field for query terms.
-   * @param analyzer used to find terms in the query text.
-   */
-  public AnalyzingQueryParser(Version matchVersion, String field, Analyzer analyzer) {
-    super(matchVersion, field, analyzer);
-  }
-
-  /**
-   * Called when parser
-   * parses an input term token that contains one or more wildcard
-   * characters (like <code>*</code>), but is not a prefix term token (one
-   * that has just a single * character at the end).
-   * <p>
-   * Example: will be called for <code>H?user</code> or for <code>H*user</code> 
-   * but not for <code>*user</code>.
-   * <p>
-   * Depending on analyzer and settings, a wildcard term may (most probably will)
-   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
-   * <p>
-   * Overrides super class, by passing terms through analyzer.
-   *
-   * @param  field   Name of the field query will use.
-   * @param  termStr Term token that contains one or more wild card
-   *                 characters (? or *), but is not simple prefix term
-   *
-   * @return Resulting {@link Query} built for the term
-   * @throws ParseException
-   */
-  @Override
-  protected Query getWildcardQuery(String field, String termStr) throws ParseException {
-    List<String> tlist = new ArrayList<String>();
-    List<String> wlist = new ArrayList<String>();
-    /* somewhat a hack: find/store wildcard chars
-     * in order to put them back after analyzing */
-    boolean isWithinToken = (!termStr.startsWith("?") && !termStr.startsWith("*"));
-    StringBuilder tmpBuffer = new StringBuilder();
-    char[] chars = termStr.toCharArray();
-    for (int i = 0; i < termStr.length(); i++) {
-      if (chars[i] == '?' || chars[i] == '*') {
-        if (isWithinToken) {
-          tlist.add(tmpBuffer.toString());
-          tmpBuffer.setLength(0);
-        }
-        isWithinToken = false;
-      } else {
-        if (!isWithinToken) {
-          wlist.add(tmpBuffer.toString());
-          tmpBuffer.setLength(0);
-        }
-        isWithinToken = true;
-      }
-      tmpBuffer.append(chars[i]);
-    }
-    if (isWithinToken) {
-      tlist.add(tmpBuffer.toString());
-    } else {
-      wlist.add(tmpBuffer.toString());
-    }
-
-    // get Analyzer from superclass and tokenize the term
-    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
-    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
-    
-    int countTokens = 0;
-    while (true) {
-      try {
-        if (!source.incrementToken()) break;
-      } catch (IOException e) {
-        break;
-      }
-      String term = termAtt.term();
-      if (!"".equals(term)) {
-        try {
-          tlist.set(countTokens++, term);
-        } catch (IndexOutOfBoundsException ioobe) {
-          countTokens = -1;
-        }
-      }
-    }
-    try {
-      source.close();
-    } catch (IOException e) {
-      // ignore
-    }
-
-    if (countTokens != tlist.size()) {
-      /* this means that the analyzer used either added or consumed 
-       * (common for a stemmer) tokens, and we can't build a WildcardQuery */
-      throw new ParseException("Cannot build WildcardQuery with analyzer "
-          + getAnalyzer().getClass() + " - tokens added or lost");
-    }
-
-    if (tlist.size() == 0) {
-      return null;
-    } else if (tlist.size() == 1) {
-      if (wlist != null && wlist.size() == 1) {
-        /* if wlist contains one wildcard, it must be at the end, because:
-         * 1) wildcards are not allowed in 1st position of a term by QueryParser
-         * 2) if wildcard was *not* in end, there would be *two* or more tokens */
-        return super.getWildcardQuery(field, tlist.get(0)
-            + wlist.get(0).toString());
-      } else {
-        /* we should never get here! if so, this method was called
-         * with a termStr containing no wildcard ... */
-        throw new IllegalArgumentException("getWildcardQuery called without wildcard");
-      }
-    } else {
-      /* the term was tokenized, let's rebuild to one token
-       * with wildcards put back in postion */
-      StringBuilder sb = new StringBuilder();
-      for (int i = 0; i < tlist.size(); i++) {
-        sb.append( tlist.get(i));
-        if (wlist != null && wlist.size() > i) {
-          sb.append(wlist.get(i));
-        }
-      }
-      return super.getWildcardQuery(field, sb.toString());
-    }
-  }
-
-  /**
-   * Called when parser parses an input term
-   * token that uses prefix notation; that is, contains a single '*' wildcard
-   * character as its last character. Since this is a special case
-   * of generic wildcard term, and such a query can be optimized easily,
-   * this usually results in a different query object.
-   * <p>
-   * Depending on analyzer and settings, a prefix term may (most probably will)
-   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
-   * <p>
-   * Overrides super class, by passing terms through analyzer.
-   *
-   * @param  field   Name of the field query will use.
-   * @param  termStr Term token to use for building term for the query
-   *                 (<b>without</b> trailing '*' character!)
-   *
-   * @return Resulting {@link Query} built for the term
-   * @throws ParseException
-   */
-  @Override
-  protected Query getPrefixQuery(String field, String termStr) throws ParseException {
-    // get Analyzer from superclass and tokenize the term
-    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
-    List<String> tlist = new ArrayList<String>();
-    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
-    
-    while (true) {
-      try {
-        if (!source.incrementToken()) break;
-      } catch (IOException e) {
-        break;
-      }
-      tlist.add(termAtt.term());
-    }
-
-    try {
-      source.close();
-    } catch (IOException e) {
-      // ignore
-    }
-
-    if (tlist.size() == 1) {
-      return super.getPrefixQuery(field, tlist.get(0));
-    } else {
-      /* this means that the analyzer used either added or consumed
-       * (common for a stemmer) tokens, and we can't build a PrefixQuery */
-      throw new ParseException("Cannot build PrefixQuery with analyzer "
-          + getAnalyzer().getClass()
-          + (tlist.size() > 1 ? " - token(s) added" : " - token consumed"));
-    }
-  }
-
-  /**
-   * Called when parser parses an input term token that has the fuzzy suffix (~) appended.
-   * <p>
-   * Depending on analyzer and settings, a fuzzy term may (most probably will)
-   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
-   * <p>
-   * Overrides super class, by passing terms through analyzer.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException
-   */
-  @Override
-  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity)
-      throws ParseException {
-    // get Analyzer from superclass and tokenize the term
-    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
-    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
-    String nextToken = null;
-    boolean multipleTokens = false;
-    
-    try {
-      if (source.incrementToken()) {
-        nextToken = termAtt.term();
-      }
-      multipleTokens = source.incrementToken();
-    } catch (IOException e) {
-      nextToken = null;
-    }
-
-    try {
-      source.close();
-    } catch (IOException e) {
-      // ignore
-    }
-
-    if (multipleTokens) {
-      throw new ParseException("Cannot build FuzzyQuery with analyzer " + getAnalyzer().getClass()
-          + " - tokens were added");
-    }
-
-    return (nextToken == null) ? null : super.getFuzzyQuery(field, nextToken, minSimilarity);
-  }
-
-  /**
-   * Overrides super class, by passing terms through analyzer.
-   * @exception ParseException
-   */
-  @Override
-  protected Query getRangeQuery(String field, String part1, String part2, boolean inclusive)
-      throws ParseException {
-    // get Analyzer from superclass and tokenize the terms
-    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(part1));
-    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
-    boolean multipleTokens = false;
-
-    // part1
-    try {
-      if (source.incrementToken()) {
-        part1 = termAtt.term();
-      }
-      multipleTokens = source.incrementToken();
-    } catch (IOException e) {
-      // ignore
-    }
-    try {
-      source.close();
-    } catch (IOException e) {
-      // ignore
-    }
-    if (multipleTokens) {
-      throw new ParseException("Cannot build RangeQuery with analyzer " + getAnalyzer().getClass()
-          + " - tokens were added to part1");
-    }
-
-    // part2
-    source = getAnalyzer().tokenStream(field, new StringReader(part2));
-    termAtt = source.addAttribute(TermAttribute.class);
-    
-    try {
-      if (source.incrementToken()) {
-        part2 = termAtt.term();
-      }
-      multipleTokens = source.incrementToken();
-    } catch (IOException e) {
-      // ignore
-    }
-    try {
-      source.close();
-    } catch (IOException e) {
-      // ignore
-    }
-    if (multipleTokens) {
-      throw new ParseException("Cannot build RangeQuery with analyzer " + getAnalyzer().getClass()
-          + " - tokens were added to part2");
-    }
-    return super.getRangeQuery(field, part1, part2, inclusive);
-  }
-
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/analyzing/package.html b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/analyzing/package.html
deleted file mode 100644
index 2785a6b..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/analyzing/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-QueryParser that passes Fuzzy-, Prefix-, Range-, and WildcardQuerys through the given analyzer.
-</body>
-</html>
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java
deleted file mode 100644
index 6359faf..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java
+++ /dev/null
@@ -1,402 +0,0 @@
-package org.apache.lucene.queryParser.complexPhrase;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanNotQuery;
-import org.apache.lucene.search.spans.SpanOrQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.util.Version;
-
-/**
- * QueryParser which permits complex phrase query syntax eg "(john jon
- * jonathan~) peters*".
- * <p>
- * Performs potentially multiple passes over Query text to parse any nested
- * logic in PhraseQueries. - First pass takes any PhraseQuery content between
- * quotes and stores for subsequent pass. All other query content is parsed as
- * normal - Second pass parses any stored PhraseQuery content, checking all
- * embedded clauses are referring to the same field and therefore can be
- * rewritten as Span queries. All PhraseQuery clauses are expressed as
- * ComplexPhraseQuery objects
- * </p>
- * <p>
- * This could arguably be done in one pass using a new QueryParser but here I am
- * working within the constraints of the existing parser as a base class. This
- * currently simply feeds all phrase content through an analyzer to select
- * phrase terms - any "special" syntax such as * ~ * etc are not given special
- * status
- * </p>
- * 
- */
-public class ComplexPhraseQueryParser extends QueryParser {
-  private ArrayList<ComplexPhraseQuery> complexPhrases = null;
-
-  private boolean isPass2ResolvingPhrases;
-
-  private ComplexPhraseQuery currentPhraseQuery = null;
-
-  public ComplexPhraseQueryParser(Version matchVersion, String f, Analyzer a) {
-    super(matchVersion, f, a);
-  }
-
-  @Override
-  protected Query getFieldQuery(String field, String queryText, int slop) {
-    ComplexPhraseQuery cpq = new ComplexPhraseQuery(field, queryText, slop);
-    complexPhrases.add(cpq); // add to list of phrases to be parsed once
-    // we
-    // are through with this pass
-    return cpq;
-  }
-
-  @Override
-  public Query parse(String query) throws ParseException {
-    if (isPass2ResolvingPhrases) {
-      MultiTermQuery.RewriteMethod oldMethod = getMultiTermRewriteMethod();
-      try {
-        // Temporarily force BooleanQuery rewrite so that Parser will
-        // generate visible
-        // collection of terms which we can convert into SpanQueries.
-        // ConstantScoreRewrite mode produces an
-        // opaque ConstantScoreQuery object which cannot be interrogated for
-        // terms in the same way a BooleanQuery can.
-        // QueryParser is not guaranteed threadsafe anyway so this temporary
-        // state change should not
-        // present an issue
-        setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
-        return super.parse(query);
-      } finally {
-        setMultiTermRewriteMethod(oldMethod);
-      }
-    }
-
-    // First pass - parse the top-level query recording any PhraseQuerys
-    // which will need to be resolved
-    complexPhrases = new ArrayList<ComplexPhraseQuery>();
-    Query q = super.parse(query);
-
-    // Perform second pass, using this QueryParser to parse any nested
-    // PhraseQueries with different
-    // set of syntax restrictions (i.e. all fields must be same)
-    isPass2ResolvingPhrases = true;
-    try {
-      for (Iterator<ComplexPhraseQuery> iterator = complexPhrases.iterator(); iterator.hasNext();) {
-        currentPhraseQuery = iterator.next();
-        // in each phrase, now parse the contents between quotes as a
-        // separate parse operation
-        currentPhraseQuery.parsePhraseElements(this);
-      }
-    } finally {
-      isPass2ResolvingPhrases = false;
-    }
-    return q;
-  }
-
-  // There is No "getTermQuery throws ParseException" method to override so
-  // unfortunately need
-  // to throw a runtime exception here if a term for another field is embedded
-  // in phrase query
-  @Override
-  protected Query newTermQuery(Term term) {
-    if (isPass2ResolvingPhrases) {
-      try {
-        checkPhraseClauseIsForSameField(term.field());
-      } catch (ParseException pe) {
-        throw new RuntimeException("Error parsing complex phrase", pe);
-      }
-    }
-    return super.newTermQuery(term);
-  }
-
-  // Helper method used to report on any clauses that appear in query syntax
-  private void checkPhraseClauseIsForSameField(String field)
-      throws ParseException {
-    if (!field.equals(currentPhraseQuery.field)) {
-      throw new ParseException("Cannot have clause for field \"" + field
-          + "\" nested in phrase " + " for field \"" + currentPhraseQuery.field
-          + "\"");
-    }
-  }
-
-  @Override
-  protected Query getWildcardQuery(String field, String termStr)
-      throws ParseException {
-    if (isPass2ResolvingPhrases) {
-      checkPhraseClauseIsForSameField(field);
-    }
-    return super.getWildcardQuery(field, termStr);
-  }
-
-  @Override
-  protected Query getRangeQuery(String field, String part1, String part2,
-      boolean inclusive) throws ParseException {
-    if (isPass2ResolvingPhrases) {
-      checkPhraseClauseIsForSameField(field);
-    }
-    return super.getRangeQuery(field, part1, part2, inclusive);
-  }
-
-  @Override
-  protected Query newRangeQuery(String field, String part1, String part2,
-      boolean inclusive) {
-    if (isPass2ResolvingPhrases) {
-      // Must use old-style RangeQuery in order to produce a BooleanQuery
-      // that can be turned into SpanOr clause
-      TermRangeQuery rangeQuery = new TermRangeQuery(field, part1, part2, inclusive, inclusive,
-          getRangeCollator());
-      rangeQuery.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
-      return rangeQuery;
-    }
-    return super.newRangeQuery(field, part1, part2, inclusive);
-  }
-
-  @Override
-  protected Query getFuzzyQuery(String field, String termStr,
-      float minSimilarity) throws ParseException {
-    if (isPass2ResolvingPhrases) {
-      checkPhraseClauseIsForSameField(field);
-    }
-    return super.getFuzzyQuery(field, termStr, minSimilarity);
-  }
-
-  /*
-   * Used to handle the query content in between quotes and produced Span-based
-   * interpretations of the clauses.
-   */
-  static class ComplexPhraseQuery extends Query {
-
-    String field;
-
-    String phrasedQueryStringContents;
-
-    int slopFactor;
-
-    private Query contents;
-
-    public ComplexPhraseQuery(String field, String phrasedQueryStringContents,
-        int slopFactor) {
-      super();
-      this.field = field;
-      this.phrasedQueryStringContents = phrasedQueryStringContents;
-      this.slopFactor = slopFactor;
-    }
-
-    // Called by ComplexPhraseQueryParser for each phrase after the main
-    // parse
-    // thread is through
-    protected void parsePhraseElements(QueryParser qp) throws ParseException {
-      // TODO ensure that field-sensitivity is preserved ie the query
-      // string below is parsed as
-      // field+":("+phrasedQueryStringContents+")"
-      // but this will need code in rewrite to unwrap the first layer of
-      // boolean query
-      contents = qp.parse(phrasedQueryStringContents);
-    }
-
-    @Override
-    public Query rewrite(IndexReader reader) throws IOException {
-      // ArrayList spanClauses = new ArrayList();
-      if (contents instanceof TermQuery) {
-        return contents;
-      }
-      // Build a sequence of Span clauses arranged in a SpanNear - child
-      // clauses can be complex
-      // Booleans e.g. nots and ors etc
-      int numNegatives = 0;
-      if (!(contents instanceof BooleanQuery)) {
-        throw new IllegalArgumentException("Unknown query type \""
-            + contents.getClass().getName()
-            + "\" found in phrase query string \"" + phrasedQueryStringContents
-            + "\"");
-      }
-      BooleanQuery bq = (BooleanQuery) contents;
-      BooleanClause[] bclauses = bq.getClauses();
-      SpanQuery[] allSpanClauses = new SpanQuery[bclauses.length];
-      // For all clauses e.g. one* two~
-      for (int i = 0; i < bclauses.length; i++) {
-        // HashSet bclauseterms=new HashSet();
-        Query qc = bclauses[i].getQuery();
-        // Rewrite this clause e.g one* becomes (one OR onerous)
-        qc = qc.rewrite(reader);
-        if (bclauses[i].getOccur().equals(BooleanClause.Occur.MUST_NOT)) {
-          numNegatives++;
-        }
-
-        if (qc instanceof BooleanQuery) {
-          ArrayList<SpanQuery> sc = new ArrayList<SpanQuery>();
-          addComplexPhraseClause(sc, (BooleanQuery) qc);
-          if (sc.size() > 0) {
-            allSpanClauses[i] = sc.get(0);
-          } else {
-            // Insert fake term e.g. phrase query was for "Fred Smithe*" and
-            // there were no "Smithe*" terms - need to
-            // prevent match on just "Fred".
-            allSpanClauses[i] = new SpanTermQuery(new Term(field,
-                "Dummy clause because no terms found - must match nothing"));
-          }
-        } else {
-          if (qc instanceof TermQuery) {
-            TermQuery tq = (TermQuery) qc;
-            allSpanClauses[i] = new SpanTermQuery(tq.getTerm());
-          } else {
-            throw new IllegalArgumentException("Unknown query type \""
-                + qc.getClass().getName()
-                + "\" found in phrase query string \""
-                + phrasedQueryStringContents + "\"");
-          }
-
-        }
-      }
-      if (numNegatives == 0) {
-        // The simple case - no negative elements in phrase
-        return new SpanNearQuery(allSpanClauses, slopFactor, true);
-      }
-      // Complex case - we have mixed positives and negatives in the
-      // sequence.
-      // Need to return a SpanNotQuery
-      ArrayList<SpanQuery> positiveClauses = new ArrayList<SpanQuery>();
-      for (int j = 0; j < allSpanClauses.length; j++) {
-        if (!bclauses[j].getOccur().equals(BooleanClause.Occur.MUST_NOT)) {
-          positiveClauses.add(allSpanClauses[j]);
-        }
-      }
-
-      SpanQuery[] includeClauses = positiveClauses
-          .toArray(new SpanQuery[positiveClauses.size()]);
-
-      SpanQuery include = null;
-      if (includeClauses.length == 1) {
-        include = includeClauses[0]; // only one positive clause
-      } else {
-        // need to increase slop factor based on gaps introduced by
-        // negatives
-        include = new SpanNearQuery(includeClauses, slopFactor + numNegatives,
-            true);
-      }
-      // Use sequence of positive and negative values as the exclude.
-      SpanNearQuery exclude = new SpanNearQuery(allSpanClauses, slopFactor,
-          true);
-      SpanNotQuery snot = new SpanNotQuery(include, exclude);
-      return snot;
-    }
-
-    private void addComplexPhraseClause(List<SpanQuery> spanClauses, BooleanQuery qc) {
-      ArrayList<SpanQuery> ors = new ArrayList<SpanQuery>();
-      ArrayList<SpanQuery> nots = new ArrayList<SpanQuery>();
-      BooleanClause[] bclauses = qc.getClauses();
-
-      // For all clauses e.g. one* two~
-      for (int i = 0; i < bclauses.length; i++) {
-        Query childQuery = bclauses[i].getQuery();
-
-        // select the list to which we will add these options
-        ArrayList<SpanQuery> chosenList = ors;
-        if (bclauses[i].getOccur() == BooleanClause.Occur.MUST_NOT) {
-          chosenList = nots;
-        }
-
-        if (childQuery instanceof TermQuery) {
-          TermQuery tq = (TermQuery) childQuery;
-          SpanTermQuery stq = new SpanTermQuery(tq.getTerm());
-          stq.setBoost(tq.getBoost());
-          chosenList.add(stq);
-        } else if (childQuery instanceof BooleanQuery) {
-          BooleanQuery cbq = (BooleanQuery) childQuery;
-          addComplexPhraseClause(chosenList, cbq);
-        } else {
-          // TODO alternatively could call extract terms here?
-          throw new IllegalArgumentException("Unknown query type:"
-              + childQuery.getClass().getName());
-        }
-      }
-      if (ors.size() == 0) {
-        return;
-      }
-      SpanOrQuery soq = new SpanOrQuery(ors
-          .toArray(new SpanQuery[ors.size()]));
-      if (nots.size() == 0) {
-        spanClauses.add(soq);
-      } else {
-        SpanOrQuery snqs = new SpanOrQuery(nots
-            .toArray(new SpanQuery[nots.size()]));
-        SpanNotQuery snq = new SpanNotQuery(soq, snqs);
-        spanClauses.add(snq);
-      }
-    }
-
-    @Override
-    public String toString(String field) {
-      return "\"" + phrasedQueryStringContents + "\"";
-    }
-
-    @Override
-    public int hashCode() {
-      final int prime = 31;
-      int result = 1;
-      result = prime * result + ((field == null) ? 0 : field.hashCode());
-      result = prime
-          * result
-          + ((phrasedQueryStringContents == null) ? 0
-              : phrasedQueryStringContents.hashCode());
-      result = prime * result + slopFactor;
-      return result;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-      if (this == obj)
-        return true;
-      if (obj == null)
-        return false;
-      if (getClass() != obj.getClass())
-        return false;
-      ComplexPhraseQuery other = (ComplexPhraseQuery) obj;
-      if (field == null) {
-        if (other.field != null)
-          return false;
-      } else if (!field.equals(other.field))
-        return false;
-      if (phrasedQueryStringContents == null) {
-        if (other.phrasedQueryStringContents != null)
-          return false;
-      } else if (!phrasedQueryStringContents
-          .equals(other.phrasedQueryStringContents))
-        return false;
-      if (slopFactor != other.slopFactor)
-        return false;
-      return true;
-    }
-  }
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/complexPhrase/package.html b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/complexPhrase/package.html
deleted file mode 100644
index ade19fc..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/complexPhrase/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-QueryParser which permits complex phrase query syntax eg "(john jon jonathan~) peters*"
-</body>
-</html>
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/ExtendableQueryParser.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/ExtendableQueryParser.java
deleted file mode 100644
index 1533d11..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/ExtendableQueryParser.java
+++ /dev/null
@@ -1,142 +0,0 @@
-package org.apache.lucene.queryParser.ext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.queryParser.ext.Extensions.Pair;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.util.Version;
-
-/**
- * The {@link ExtendableQueryParser} enables arbitrary query parser extension
- * based on a customizable field naming scheme. The lucene query syntax allows
- * implicit and explicit field definitions as query prefix followed by a colon
- * (':') character. The {@link ExtendableQueryParser} allows to encode extension
- * keys into the field symbol associated with a registered instance of
- * {@link ParserExtension}. A customizable separation character separates the
- * extension key from the actual field symbol. The {@link ExtendableQueryParser}
- * splits (@see {@link Extensions#splitExtensionField(String, String)}) the
- * extension key from the field symbol and tries to resolve the associated
- * {@link ParserExtension}. If the parser can't resolve the key or the field
- * token does not contain a separation character, {@link ExtendableQueryParser}
- * yields the same behavior as its super class {@link QueryParser}. Otherwise,
- * if the key is associated with a {@link ParserExtension} instance, the parser
- * builds an instance of {@link ExtensionQuery} to be processed by
- * {@link ParserExtension#parse(ExtensionQuery)}.If a extension field does not
- * contain a field part the default field for the query will be used.
- * <p>
- * To guarantee that an extension field is processed with its associated
- * extension, the extension query part must escape any special characters like
- * '*' or '['. If the extension query contains any whitespace characters, the
- * extension query part must be enclosed in quotes.
- * Example ('_' used as separation character):
- * <pre>
- *   title_customExt:"Apache Lucene\?" OR content_customExt:prefix\*
- * </pre>
- * 
- * Search on the default field:
- * <pre>
- *   _customExt:"Apache Lucene\?" OR _customExt:prefix\*
- * </pre>
- * </p>
- * <p>
- * The {@link ExtendableQueryParser} itself does not implement the logic how
- * field and extension key are separated or ordered. All logic regarding the
- * extension key and field symbol parsing is located in {@link Extensions}.
- * Customized extension schemes should be implemented by sub-classing
- * {@link Extensions}.
- * </p>
- * <p>
- * For details about the default encoding scheme see {@link Extensions}.
- * </p>
- * 
- * @see Extensions
- * @see ParserExtension
- * @see ExtensionQuery
- */
-public class ExtendableQueryParser extends QueryParser {
-
-  private final String defaultField;
-  private final Extensions extensions;
-
-  /**
-   * Default empty extensions instance
-   */
-  private static final Extensions DEFAULT_EXTENSION = new Extensions();
-
-  /**
-   * Creates a new {@link ExtendableQueryParser} instance
-   * 
-   * @param matchVersion
-   *          the lucene version to use.
-   * @param f
-   *          the default query field
-   * @param a
-   *          the analyzer used to find terms in a query string
-   */
-  public ExtendableQueryParser(final Version matchVersion, final String f,
-      final Analyzer a) {
-    this(matchVersion, f, a, DEFAULT_EXTENSION);
-
-  }
-
-  /**
-   * Creates a new {@link ExtendableQueryParser} instance
-   * 
-   * @param matchVersion
-   *          the lucene version to use.
-   * @param f
-   *          the default query field
-   * @param a
-   *          the analyzer used to find terms in a query string
-   * @param ext
-   *          the query parser extensions
-   */
-  public ExtendableQueryParser(final Version matchVersion, final String f,
-      final Analyzer a, final Extensions ext) {
-    super(matchVersion, f, a);
-    this.defaultField = f;
-    this.extensions = ext;
-  }
-
-  /**
-   * Returns the extension field delimiter character.
-   * 
-   * @return the extension field delimiter character.
-   */
-  public char getExtensionFieldDelimiter() {
-    return extensions.getExtensionFieldDelimiter();
-  }
-
-  @Override
-  protected Query getFieldQuery(final String field, final String queryText)
-      throws ParseException {
-    final Pair<String,String> splitExtensionField = this.extensions
-        .splitExtensionField(defaultField, field);
-    final ParserExtension extension = this.extensions
-        .getExtension(splitExtensionField.cud);
-    if (extension != null) {
-      return extension.parse(new ExtensionQuery(this, splitExtensionField.cur,
-          queryText));
-    }
-    return super.getFieldQuery(field, queryText);
-  }
-
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/ExtensionQuery.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/ExtensionQuery.java
deleted file mode 100644
index f84faf8..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/ExtensionQuery.java
+++ /dev/null
@@ -1,75 +0,0 @@
-package org.apache.lucene.queryParser.ext;
-
-import org.apache.lucene.queryParser.QueryParser;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * {@link ExtensionQuery} holds all query components extracted from the original
- * query string like the query field and the extension query string.
- * 
- * @see Extensions
- * @see ExtendableQueryParser
- * @see ParserExtension
- */
-public class ExtensionQuery {
-
-  private final String field;
-  private final String rawQueryString;
-  private final QueryParser topLevelParser;
-
-  /**
-   * Creates a new {@link ExtensionQuery}
-   * 
-   * @param field
-   *          the query field
-   * @param rawQueryString
-   *          the raw extension query string
-   */
-  public ExtensionQuery(QueryParser topLevelParser, String field, String rawQueryString) {
-    this.field = field;
-    this.rawQueryString = rawQueryString;
-    this.topLevelParser = topLevelParser;
-  }
-
-  /**
-   * Returns the query field
-   * 
-   * @return the query field
-   */
-  public String getField() {
-    return field;
-  }
-
-  /**
-   * Returns the raw extension query string
-   * 
-   * @return the raw extension query string
-   */
-  public String getRawQueryString() {
-    return rawQueryString;
-  }
-  
-  /**
-   * Returns the top level parser which created this {@link ExtensionQuery} 
-   * @return the top level parser which created this {@link ExtensionQuery}
-   */
-  public QueryParser getTopLevelParser() {
-    return topLevelParser;
-  }
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/Extensions.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/Extensions.java
deleted file mode 100644
index edf763d..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/Extensions.java
+++ /dev/null
@@ -1,217 +0,0 @@
-package org.apache.lucene.queryParser.ext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.queryParser.QueryParser;
-
-/**
- * The {@link Extensions} class represents an extension mapping to associate
- * {@link ParserExtension} instances with extension keys. An extension key is a
- * string encoded into a Lucene standard query parser field symbol recognized by
- * {@link ExtendableQueryParser}. The query parser passes each extension field
- * token to {@link #splitExtensionField(String, String)} to separate the
- * extension key from the field identifier.
- * <p>
- * In addition to the key to extension mapping this class also defines the field
- * name overloading scheme. {@link ExtendableQueryParser} uses the given
- * extension to split the actual field name and extension key by calling
- * {@link #splitExtensionField(String, String)}. To change the order or the key
- * / field name encoding scheme users can subclass {@link Extensions} to
- * implement their own.
- * 
- * @see ExtendableQueryParser
- * @see ParserExtension
- */
-public class Extensions {
-  private final Map<String,ParserExtension> extensions = new HashMap<String,ParserExtension>();
-  private final char extensionFieldDelimiter;
-  /**
-   * The default extension field delimiter character. This constant is set to
-   * ':'
-   */
-  public static final char DEFAULT_EXTENSION_FIELD_DELIMITER = ':';
-
-  /**
-   * Creates a new {@link Extensions} instance with the
-   * {@link #DEFAULT_EXTENSION_FIELD_DELIMITER} as a delimiter character.
-   */
-  public Extensions() {
-    this(DEFAULT_EXTENSION_FIELD_DELIMITER);
-  }
-
-  /**
-   * Creates a new {@link Extensions} instance
-   * 
-   * @param extensionFieldDelimiter
-   *          the extensions field delimiter character
-   */
-  public Extensions(char extensionFieldDelimiter) {
-    this.extensionFieldDelimiter = extensionFieldDelimiter;
-  }
-
-  /**
-   * Adds a new {@link ParserExtension} instance associated with the given key.
-   * 
-   * @param key
-   *          the parser extension key
-   * @param extension
-   *          the parser extension
-   */
-  public void add(String key, ParserExtension extension) {
-    this.extensions.put(key, extension);
-  }
-
-  /**
-   * Returns the {@link ParserExtension} instance for the given key or
-   * <code>null</code> if no extension can be found for the key.
-   * 
-   * @param key
-   *          the extension key
-   * @return the {@link ParserExtension} instance for the given key or
-   *         <code>null</code> if no extension can be found for the key.
-   */
-  public final ParserExtension getExtension(String key) {
-    return this.extensions.get(key);
-  }
-
-  /**
-   * Returns the extension field delimiter
-   * 
-   * @return the extension field delimiter
-   */
-  public char getExtensionFieldDelimiter() {
-    return extensionFieldDelimiter;
-  }
-
-  /**
-   * Splits a extension field and returns the field / extension part as a
-   * {@link Pair}. This method tries to split on the first occurrence of the
-   * extension field delimiter, if the delimiter is not present in the string
-   * the result will contain a <code>null</code> value for the extension key and
-   * the given field string as the field value. If the given extension field
-   * string contains no field identifier the result pair will carry the given
-   * default field as the field value.
-   * 
-   * @param defaultField
-   *          the default query field
-   * @param field
-   *          the extension field string
-   * @return a {@link Pair} with the field name as the {@link Pair#cur} and the
-   *         extension key as the {@link Pair#cud}
-   */
-  public Pair<String,String> splitExtensionField(String defaultField,
-      String field) {
-    int indexOf = field.indexOf(this.extensionFieldDelimiter);
-    if (indexOf < 0)
-      return new Pair<String,String>(field, null);
-    final String indexField = indexOf == 0 ? defaultField : field.substring(0,
-        indexOf);
-    final String extensionKey = field.substring(indexOf + 1);
-    return new Pair<String,String>(indexField, extensionKey);
-
-  }
-
-  /**
-   * Escapes an extension field. The default implementation is equivalent to
-   * {@link QueryParser#escape(String)}.
-   * 
-   * @param extfield
-   *          the extension field identifier
-   * @return the extension field identifier with all special chars escaped with
-   *         a backslash character.
-   */
-  public String escapeExtensionField(String extfield) {
-    return QueryParser.escape(extfield);
-  }
-
-  /**
-   * Builds an extension field string from a given extension key and the default
-   * query field. The default field and the key are delimited with the extension
-   * field delimiter character. This method makes no assumption about the order
-   * of the extension key and the field. By default the extension key is
-   * appended to the end of the returned string while the field is added to the
-   * beginning. Special Query characters are escaped in the result.
-   * <p>
-   * Note: {@link Extensions} subclasses must maintain the contract between
-   * {@link #buildExtensionField(String)} and
-   * {@link #splitExtensionField(String, String)} where the latter inverts the
-   * former.
-   * </p>
-   */
-  public String buildExtensionField(String extensionKey) {
-    return buildExtensionField(extensionKey, "");
-  }
-
-  /**
-   * Builds an extension field string from a given extension key and the
-   * extensions field. The field and the key are delimited with the extension
-   * field delimiter character. This method makes no assumption about the order
-   * of the extension key and the field. By default the extension key is
-   * appended to the end of the returned string while the field is added to the
-   * beginning. Special Query characters are escaped in the result.
-   * <p>
-   * Note: {@link Extensions} subclasses must maintain the contract between
-   * {@link #buildExtensionField(String, String)} and
-   * {@link #splitExtensionField(String, String)} where the latter inverts the
-   * former.
-   * </p>
-   * 
-   * @param extensionKey
-   *          the extension key
-   * @param field
-   *          the field to apply the extension on.
-   * @return escaped extension field identifier
-   * @see #buildExtensionField(String) to use the default query field
-   */
-  public String buildExtensionField(String extensionKey, String field) {
-    StringBuilder builder = new StringBuilder(field);
-    builder.append(this.extensionFieldDelimiter);
-    builder.append(extensionKey);
-    return escapeExtensionField(builder.toString());
-  }
-
-  /**
-   * This class represents a generic pair.
-   * 
-   * @param <Cur>
-   *          the pairs first element
-   * @param <Cud>
-   *          the pairs last element of the pair.
-   */
-  public static class Pair<Cur,Cud> {
-
-    public final Cur cur;
-    public final Cud cud;
-
-    /**
-     * Creates a new Pair
-     * 
-     * @param cur
-     *          the pairs first element
-     * @param cud
-     *          the pairs last element
-     */
-    public Pair(Cur cur, Cud cud) {
-      this.cur = cur;
-      this.cud = cud;
-    }
-  }
-
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/ParserExtension.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/ParserExtension.java
deleted file mode 100644
index b173858..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/ParserExtension.java
+++ /dev/null
@@ -1,53 +0,0 @@
-package org.apache.lucene.queryParser.ext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.Query;
-
-/**
- * This class represents an extension base class to the Lucene standard
- * {@link QueryParser}. The {@link QueryParser} is generated by the JavaCC
- * parser generator. Changing or adding functionality or syntax in the standard
- * query parser requires changes to the JavaCC source file. To enable extending
- * the standard query parser without changing the JavaCC sources and re-generate
- * the parser the {@link ParserExtension} can be customized and plugged into an
- * instance of {@link ExtendableQueryParser}, a direct subclass of
- * {@link QueryParser}.
- * 
- * @see Extensions
- * @see ExtendableQueryParser
- */
-public abstract class ParserExtension {
-
-  /**
-   * Processes the given {@link ExtensionQuery} and returns a corresponding
-   * {@link Query} instance. Subclasses must either return a {@link Query}
-   * instance or raise a {@link ParseException}. This method must not return
-   * <code>null</code>.
-   * 
-   * @param query
-   *          the extension query
-   * @return a new query instance
-   * @throws ParseException
-   *           if the query can not be parsed.
-   */
-  public abstract Query parse(final ExtensionQuery query) throws ParseException;
-
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/package.html b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/package.html
deleted file mode 100644
index 13549a8..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/ext/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Extendable QueryParser provides a simple and flexible extension mechanism by overloading query field names.
-</body>
-</html>
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/CharStream.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/CharStream.java
deleted file mode 100644
index 9a84eb5..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/CharStream.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. CharStream.java Version 4.1 */
-/* JavaCCOptions:STATIC=false */
-package org.apache.lucene.queryParser.precedence;
-
-/**
- * This interface describes a character stream that maintains line and
- * column number positions of the characters.  It also has the capability
- * to backup the stream to some extent.  An implementation of this
- * interface is used in the TokenManager implementation generated by
- * JavaCCParser.
- *
- * All the methods except backup can be implemented in any fashion. backup
- * needs to be implemented correctly for the correct operation of the lexer.
- * Rest of the methods are all used to get information like line number,
- * column number and the String that constitutes a token and are not used
- * by the lexer. Hence their implementation won't affect the generated lexer's
- * operation.
- */
-
-public interface CharStream {
-
-  /**
-   * Returns the next character from the selected input.  The method
-   * of selecting the input is the responsibility of the class
-   * implementing this interface.  Can throw any java.io.IOException.
-   */
-  char readChar() throws java.io.IOException;
-
-  /**
-   * Returns the column position of the character last read.
-   * @deprecated
-   * @see #getEndColumn
-   */
-  int getColumn();
-
-  /**
-   * Returns the line number of the character last read.
-   * @deprecated
-   * @see #getEndLine
-   */
-  int getLine();
-
-  /**
-   * Returns the column number of the last character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getEndColumn();
-
-  /**
-   * Returns the line number of the last character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getEndLine();
-
-  /**
-   * Returns the column number of the first character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getBeginColumn();
-
-  /**
-   * Returns the line number of the first character for current token (being
-   * matched after the last call to BeginTOken).
-   */
-  int getBeginLine();
-
-  /**
-   * Backs up the input stream by amount steps. Lexer calls this method if it
-   * had already read some characters, but could not use them to match a
-   * (longer) token. So, they will be used again as the prefix of the next
-   * token and it is the implemetation's responsibility to do this right.
-   */
-  void backup(int amount);
-
-  /**
-   * Returns the next character that marks the beginning of the next token.
-   * All characters must remain in the buffer between two successive calls
-   * to this method to implement backup correctly.
-   */
-  char BeginToken() throws java.io.IOException;
-
-  /**
-   * Returns a string made up of characters from the marked token beginning
-   * to the current buffer position. Implementations have the choice of returning
-   * anything that they want to. For example, for efficiency, one might decide
-   * to just return null, which is a valid implementation.
-   */
-  String GetImage();
-
-  /**
-   * Returns an array of characters that make up the suffix of length 'len' for
-   * the currently matched token. This is used to build up the matched string
-   * for use in actions in the case of MORE. A simple and inefficient
-   * implementation of this is as follows :
-   *
-   *   {
-   *      String t = GetImage();
-   *      return t.substring(t.length() - len, t.length()).toCharArray();
-   *   }
-   */
-  char[] GetSuffix(int len);
-
-  /**
-   * The lexer calls this function to indicate that it is done with the stream
-   * and hence implementations can free any resources held by this class.
-   * Again, the body of this function can be just empty and it will not
-   * affect the lexer's operation.
-   */
-  void Done();
-
-}
-/* JavaCC - OriginalChecksum=8cc617b193267dc876ef9699367c8186 (do not edit this line) */
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java
deleted file mode 100644
index f7b2879..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java
+++ /dev/null
@@ -1,123 +0,0 @@
-// FastCharStream.java
-package org.apache.lucene.queryParser.precedence;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.io.*;
-
-/** An efficient implementation of JavaCC's CharStream interface.  <p>Note that
- * this does not do line-number counting, but instead keeps track of the
- * character position of the token in the input, as required by Lucene's {@link
- * org.apache.lucene.analysis.Token} API. */
-public final class FastCharStream implements CharStream {
-  char[] buffer = null;
-
-  int bufferLength = 0;				  // end of valid chars
-  int bufferPosition = 0;			  // next char to read
-
-  int tokenStart = 0;				  // offset in buffer
-  int bufferStart = 0;				  // position in file of buffer
-
-  Reader input;					  // source of chars
-
-  /** Constructs from a Reader. */
-  public FastCharStream(Reader r) {
-    input = r;
-  }
-
-  public final char readChar() throws IOException {
-    if (bufferPosition >= bufferLength)
-      refill();
-    return buffer[bufferPosition++];
-  }
-
-  private final void refill() throws IOException {
-    int newPosition = bufferLength - tokenStart;
-
-    if (tokenStart == 0) {			  // token won't fit in buffer
-      if (buffer == null) {			  // first time: alloc buffer
-	buffer = new char[2048];
-      } else if (bufferLength == buffer.length) { // grow buffer
-	char[] newBuffer = new char[buffer.length*2];
-	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
-	buffer = newBuffer;
-      }
-    } else {					  // shift token to front
-      System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
-    }
-
-    bufferLength = newPosition;			  // update state
-    bufferPosition = newPosition;
-    bufferStart += tokenStart;
-    tokenStart = 0;
-
-    int charsRead =				  // fill space in buffer
-      input.read(buffer, newPosition, buffer.length-newPosition);
-    if (charsRead == -1)
-      throw new IOException("read past eof");
-    else
-      bufferLength += charsRead;
-  }
-
-  public final char BeginToken() throws IOException {
-    tokenStart = bufferPosition;
-    return readChar();
-  }
-
-  public final void backup(int amount) {
-    bufferPosition -= amount;
-  }
-
-  public final String GetImage() {
-    return new String(buffer, tokenStart, bufferPosition - tokenStart);
-  }
-
-  public final char[] GetSuffix(int len) {
-    char[] value = new char[len];
-    System.arraycopy(buffer, bufferPosition - len, value, 0, len);
-    return value;
-  }
-
-  public final void Done() {
-    try {
-      input.close();
-    } catch (IOException e) {
-      System.err.println("Caught: " + e + "; ignoring.");
-    }
-  }
-
-  public final int getColumn() {
-    return bufferStart + bufferPosition;
-  }
-  public final int getLine() {
-    return 1;
-  }
-  public final int getEndColumn() {
-    return bufferStart + bufferPosition;
-  }
-  public final int getEndLine() {
-    return 1;
-  }
-  public final int getBeginColumn() {
-    return bufferStart + tokenStart;
-  }
-  public final int getBeginLine() {
-    return 1;
-  }
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/ParseException.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/ParseException.java
deleted file mode 100644
index 6e9ec48..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/ParseException.java
+++ /dev/null
@@ -1,198 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. ParseException.java Version 4.1 */
-/* JavaCCOptions:KEEP_LINE_COL=null */
-package org.apache.lucene.queryParser.precedence;
-
-/**
- * This exception is thrown when parse errors are encountered.
- * You can explicitly create objects of this exception type by
- * calling the method generateParseException in the generated
- * parser.
- *
- * You can modify this class to customize your error reporting
- * mechanisms so long as you retain the public fields.
- */
-public class ParseException extends Exception {
-
-  /**
-   * This constructor is used by the method "generateParseException"
-   * in the generated parser.  Calling this constructor generates
-   * a new object of this type with the fields "currentToken",
-   * "expectedTokenSequences", and "tokenImage" set.  The boolean
-   * flag "specialConstructor" is also set to true to indicate that
-   * this constructor was used to create this object.
-   * This constructor calls its super class with the empty string
-   * to force the "toString" method of parent class "Throwable" to
-   * print the error message in the form:
-   *     ParseException: <result of getMessage>
-   */
-  public ParseException(Token currentTokenVal,
-                        int[][] expectedTokenSequencesVal,
-                        String[] tokenImageVal
-                       )
-  {
-    super("");
-    specialConstructor = true;
-    currentToken = currentTokenVal;
-    expectedTokenSequences = expectedTokenSequencesVal;
-    tokenImage = tokenImageVal;
-  }
-
-  /**
-   * The following constructors are for use by you for whatever
-   * purpose you can think of.  Constructing the exception in this
-   * manner makes the exception behave in the normal way - i.e., as
-   * documented in the class "Throwable".  The fields "errorToken",
-   * "expectedTokenSequences", and "tokenImage" do not contain
-   * relevant information.  The JavaCC generated code does not use
-   * these constructors.
-   */
-
-  public ParseException() {
-    super();
-    specialConstructor = false;
-  }
-
-  /** Constructor with message. */
-  public ParseException(String message) {
-    super(message);
-    specialConstructor = false;
-  }
-
-  /**
-   * This variable determines which constructor was used to create
-   * this object and thereby affects the semantics of the
-   * "getMessage" method (see below).
-   */
-  protected boolean specialConstructor;
-
-  /**
-   * This is the last token that has been consumed successfully.  If
-   * this object has been created due to a parse error, the token
-   * followng this token will (therefore) be the first error token.
-   */
-  public Token currentToken;
-
-  /**
-   * Each entry in this array is an array of integers.  Each array
-   * of integers represents a sequence of tokens (by their ordinal
-   * values) that is expected at this point of the parse.
-   */
-  public int[][] expectedTokenSequences;
-
-  /**
-   * This is a reference to the "tokenImage" array of the generated
-   * parser within which the parse error occurred.  This array is
-   * defined in the generated ...Constants interface.
-   */
-  public String[] tokenImage;
-
-  /**
-   * This method has the standard behavior when this object has been
-   * created using the standard constructors.  Otherwise, it uses
-   * "currentToken" and "expectedTokenSequences" to generate a parse
-   * error message and returns it.  If this object has been created
-   * due to a parse error, and you do not catch it (it gets thrown
-   * from the parser), then this method is called during the printing
-   * of the final stack trace, and hence the correct error message
-   * gets displayed.
-   */
-  public String getMessage() {
-    if (!specialConstructor) {
-      return super.getMessage();
-    }
-    StringBuffer expected = new StringBuffer();
-    int maxSize = 0;
-    for (int i = 0; i < expectedTokenSequences.length; i++) {
-      if (maxSize < expectedTokenSequences[i].length) {
-        maxSize = expectedTokenSequences[i].length;
-      }
-      for (int j = 0; j < expectedTokenSequences[i].length; j++) {
-        expected.append(tokenImage[expectedTokenSequences[i][j]]).append(' ');
-      }
-      if (expectedTokenSequences[i][expectedTokenSequences[i].length - 1] != 0) {
-        expected.append("...");
-      }
-      expected.append(eol).append("    ");
-    }
-    String retval = "Encountered \"";
-    Token tok = currentToken.next;
-    for (int i = 0; i < maxSize; i++) {
-      if (i != 0) retval += " ";
-      if (tok.kind == 0) {
-        retval += tokenImage[0];
-        break;
-      }
-      retval += " " + tokenImage[tok.kind];
-      retval += " \"";
-      retval += add_escapes(tok.image);
-      retval += " \"";
-      tok = tok.next;
-    }
-    retval += "\" at line " + currentToken.next.beginLine + ", column " + currentToken.next.beginColumn;
-    retval += "." + eol;
-    if (expectedTokenSequences.length == 1) {
-      retval += "Was expecting:" + eol + "    ";
-    } else {
-      retval += "Was expecting one of:" + eol + "    ";
-    }
-    retval += expected.toString();
-    return retval;
-  }
-
-  /**
-   * The end of line string for this machine.
-   */
-  protected String eol = System.getProperty("line.separator", "\n");
-
-  /**
-   * Used to convert raw characters to their escaped version
-   * when these raw version cannot be used as part of an ASCII
-   * string literal.
-   */
-  protected String add_escapes(String str) {
-      StringBuffer retval = new StringBuffer();
-      char ch;
-      for (int i = 0; i < str.length(); i++) {
-        switch (str.charAt(i))
-        {
-           case 0 :
-              continue;
-           case '\b':
-              retval.append("\\b");
-              continue;
-           case '\t':
-              retval.append("\\t");
-              continue;
-           case '\n':
-              retval.append("\\n");
-              continue;
-           case '\f':
-              retval.append("\\f");
-              continue;
-           case '\r':
-              retval.append("\\r");
-              continue;
-           case '\"':
-              retval.append("\\\"");
-              continue;
-           case '\'':
-              retval.append("\\\'");
-              continue;
-           case '\\':
-              retval.append("\\\\");
-              continue;
-           default:
-              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
-                 String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
-              } else {
-                 retval.append(ch);
-              }
-              continue;
-        }
-      }
-      return retval.toString();
-   }
-
-}
-/* JavaCC - OriginalChecksum=15fbbe38a36c8ac9e2740d030624c321 (do not edit this line) */
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
deleted file mode 100644
index f8043bc..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
+++ /dev/null
@@ -1,1293 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParser.java */
-package org.apache.lucene.queryParser.precedence;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.text.DateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.List;
-import java.util.Locale;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.*;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.MultiPhraseQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.AttributeSource;
-
-/**
- * Experimental query parser variant designed to handle operator precedence
- * in a more sensible fashion than QueryParser.  There are still some
- * open issues with this parser. The following tests are currently failing
- * in TestPrecedenceQueryParser and are disabled to make this test pass:
- * <ul>
- * <li> testSimple
- * <li> testWildcard
- * <li> testPrecedence
- * </ul>
- *
- * This class is generated by JavaCC.  The only method that clients should need
- * to call is {@link #parse(String)}.
- *
- * The syntax for query strings is as follows:
- * A Query is a series of clauses.
- * A clause may be prefixed by:
- * <ul>
- * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
- * that the clause is required or prohibited respectively; or
- * <li> a term followed by a colon, indicating the field to be searched.
- * This enables one to construct queries which search multiple fields.
- * </ul>
- *
- * A clause may be either:
- * <ul>
- * <li> a term, indicating all the documents that contain this term; or
- * <li> a nested query, enclosed in parentheses.  Note that this may be used
- * with a <code>+</code>/<code>-</code> prefix to require any of a set of
- * terms.
- * </ul>
- *
- * Thus, in BNF, the query grammar is:
- * <pre>
- *   Query  ::= ( Clause )*
- *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
- * </pre>
- *
- * <p>
- * Examples of appropriately formatted queries can be found in the <a
- * href="../../../../../../../queryparsersyntax.html">query syntax
- * documentation</a>.
- * </p>
- *
- * @author Brian Goetz
- * @author Peter Halacsy
- * @author Tatu Saloranta
- */
-
-public class PrecedenceQueryParser implements PrecedenceQueryParserConstants {
-
-  private static final int CONJ_NONE   = 0;
-  private static final int CONJ_AND    = 1;
-  private static final int CONJ_OR     = 2;
-
-  private static final int MOD_NONE    = 0;
-  private static final int MOD_NOT     = 10;
-  private static final int MOD_REQ     = 11;
-
-  // make it possible to call setDefaultOperator() without accessing
-  // the nested class:
-  public static final Operator AND_OPERATOR = Operator.AND;
-  public static final Operator OR_OPERATOR = Operator.OR;
-
-  /** The actual operator that parser uses to combine query terms */
-  private Operator operator = OR_OPERATOR;
-
-  boolean lowercaseExpandedTerms = true;
-
-  Analyzer analyzer;
-  String field;
-  int phraseSlop = 0;
-  float fuzzyMinSim = FuzzyQuery.defaultMinSimilarity;
-  int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
-  Locale locale = Locale.getDefault();
-
-  static enum Operator { OR, AND }
-
-  /** Constructs a query parser.
-   *  @param f  the default field for query terms.
-   *  @param a   used to find terms in the query text.
-   */
-  public PrecedenceQueryParser(String f, Analyzer a) {
-    this(new FastCharStream(new StringReader("")));
-    analyzer = a;
-    field = f;
-  }
-
-  /** Parses a query string, returning a {@link org.apache.lucene.search.Query}.
-   *  @param expression  the query string to be parsed.
-   *  @throws ParseException if the parsing fails
-   */
-  public Query parse(String expression) throws ParseException {
-    // optimize empty query to be empty BooleanQuery
-    if (expression == null || expression.trim().length() == 0) {
-      return new BooleanQuery();
-    }
-
-    ReInit(new FastCharStream(new StringReader(expression)));
-    try {
-      Query query = Query(field);
-      return (query != null) ? query : new BooleanQuery();
-    }
-    catch (TokenMgrError tme) {
-      throw new ParseException(tme.getMessage());
-    }
-    catch (BooleanQuery.TooManyClauses tmc) {
-      throw new ParseException("Too many boolean clauses");
-    }
-  }
-
-   /**
-   * @return Returns the analyzer.
-   */
-  public Analyzer getAnalyzer() {
-    return analyzer;
-  }
-
-  /**
-   * @return Returns the field.
-   */
-  public String getField() {
-    return field;
-  }
-
-   /**
-   * Get the minimal similarity for fuzzy queries.
-   */
-  public float getFuzzyMinSim() {
-      return fuzzyMinSim;
-  }
-
-  /**
-   * Set the minimum similarity for fuzzy queries.
-   * Default is 0.5f.
-   */
-  public void setFuzzyMinSim(float fuzzyMinSim) {
-      this.fuzzyMinSim = fuzzyMinSim;
-  }
-
-   /**
-   * Get the prefix length for fuzzy queries. 
-   * @return Returns the fuzzyPrefixLength.
-   */
-  public int getFuzzyPrefixLength() {
-    return fuzzyPrefixLength;
-  }
-
-  /**
-   * Set the prefix length for fuzzy queries. Default is 0.
-   * @param fuzzyPrefixLength The fuzzyPrefixLength to set.
-   */
-  public void setFuzzyPrefixLength(int fuzzyPrefixLength) {
-    this.fuzzyPrefixLength = fuzzyPrefixLength;
-  }
-
-  /**
-   * Sets the default slop for phrases.  If zero, then exact phrase matches
-   * are required.  Default value is zero.
-   */
-  public void setPhraseSlop(int phraseSlop) {
-    this.phraseSlop = phraseSlop;
-  }
-
-  /**
-   * Gets the default slop for phrases.
-   */
-  public int getPhraseSlop() {
-    return phraseSlop;
-  }
-
-  /**
-   * Sets the boolean operator of the QueryParser.
-   * In default mode (<code>OR_OPERATOR</code>) terms without any modifiers
-   * are considered optional: for example <code>capital of Hungary</code> is equal to
-   * <code>capital OR of OR Hungary</code>.<br/>
-   * In <code>AND_OPERATOR</code> mode terms are considered to be in conjunction: the
-   * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
-   */
-  public void setDefaultOperator(Operator op) {
-    this.operator = op;
-  }
-
-  /**
-   * Gets implicit operator setting, which will be either AND_OPERATOR
-   * or OR_OPERATOR.
-   */
-  public Operator getDefaultOperator() {
-    return operator;
-  }
-
-  /**
-   * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically
-   * lower-cased or not.  Default is <code>true</code>.
-   */
-  public void setLowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
-    this.lowercaseExpandedTerms = lowercaseExpandedTerms;
-  }
-
-  /**
-   * @see #setLowercaseExpandedTerms(boolean)
-   */
-  public boolean getLowercaseExpandedTerms() {
-    return lowercaseExpandedTerms;
-  }
-
-  /**
-   * Set locale used by date range parsing.
-   */
-  public void setLocale(Locale locale) {
-    this.locale = locale;
-  }
-
-  /**
-   * Returns current locale, allowing access by subclasses.
-   */
-  public Locale getLocale() {
-    return locale;
-  }
-
-  protected void addClause(List<BooleanClause> clauses, int conj, int modifier, Query q) {
-    boolean required, prohibited;
-
-    // If this term is introduced by AND, make the preceding term required,
-    // unless it's already prohibited
-    if (clauses.size() > 0 && conj == CONJ_AND) {
-      BooleanClause c = clauses.get(clauses.size()-1);
-      if (!c.isProhibited())
-        c.setOccur(BooleanClause.Occur.MUST);
-    }
-
-    if (clauses.size() > 0 && operator == AND_OPERATOR && conj == CONJ_OR) {
-      // If this term is introduced by OR, make the preceding term optional,
-      // unless it's prohibited (that means we leave -a OR b but +a OR b-->a OR b)
-      // notice if the input is a OR b, first term is parsed as required; without
-      // this modification a OR b would parsed as +a OR b
-      BooleanClause c = clauses.get(clauses.size()-1);
-      if (!c.isProhibited())
-        c.setOccur(BooleanClause.Occur.SHOULD);
-    }
-
-    // We might have been passed a null query; the term might have been
-    // filtered away by the analyzer.
-    if (q == null)
-      return;
-
-    if (operator == OR_OPERATOR) {
-      // We set REQUIRED if we're introduced by AND or +; PROHIBITED if
-      // introduced by NOT or -; make sure not to set both.
-      prohibited = (modifier == MOD_NOT);
-      required = (modifier == MOD_REQ);
-      if (conj == CONJ_AND && !prohibited) {
-        required = true;
-      }
-    } else {
-      // We set PROHIBITED if we're introduced by NOT or -; We set REQUIRED
-      // if not PROHIBITED and not introduced by OR
-      prohibited = (modifier == MOD_NOT);
-      required   = (!prohibited && conj != CONJ_OR);
-    }
-    if (required && !prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST));
-    else if (!required && !prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
-    else if (!required && prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST_NOT));
-    else
-      throw new RuntimeException("Clause cannot be both required and prohibited");
-  }
-
-  /**
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFieldQuery(String field, String queryText)  throws ParseException {
-    // Use the analyzer to get all the tokens, and then build a TermQuery,
-    // PhraseQuery, or nothing based on the term count
-
-    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
-    List<AttributeSource.State> list = new ArrayList<AttributeSource.State>();
-    int positionCount = 0;
-    boolean severalTokensAtSamePosition = false;
-    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
-    PositionIncrementAttribute posincrAtt = source.addAttribute(PositionIncrementAttribute.class);
-
-    try {
-      while (source.incrementToken()) {
-        list.add(source.captureState());
-        if (posincrAtt.getPositionIncrement() == 1)
-          positionCount++;
-        else
-          severalTokensAtSamePosition = true;
-      }
-      source.end();
-      source.close();
-    } catch (IOException e) {
-      // ignore, should never happen for StringReaders
-    }
-
-    if (list.size() == 0)
-      return null;
-    else if (list.size() == 1) {
-      source.restoreState(list.get(0));
-      return new TermQuery(new Term(field, termAtt.term()));
-    } else {
-      if (severalTokensAtSamePosition) {
-        if (positionCount == 1) {
-          // no phrase query:
-          BooleanQuery q = new BooleanQuery();
-          for (int i = 0; i < list.size(); i++) {
-            source.restoreState(list.get(i));
-            TermQuery currentQuery = new TermQuery(
-                new Term(field, termAtt.term()));
-            q.add(currentQuery, BooleanClause.Occur.SHOULD);
-          }
-          return q;
-        }
-        else {
-          // phrase query:
-          MultiPhraseQuery mpq = new MultiPhraseQuery();
-          List<Term> multiTerms = new ArrayList<Term>();
-          for (int i = 0; i < list.size(); i++) {
-            source.restoreState(list.get(i));
-            if (posincrAtt.getPositionIncrement() == 1 && multiTerms.size() > 0) {
-              mpq.add(multiTerms.toArray(new Term[0]));
-              multiTerms.clear();
-            }
-            multiTerms.add(new Term(field, termAtt.term()));
-          }
-          mpq.add(multiTerms.toArray(new Term[0]));
-          return mpq;
-        }
-      }
-      else {
-        PhraseQuery q = new PhraseQuery();
-        q.setSlop(phraseSlop);
-        for (int i = 0; i < list.size(); i++) {
-          source.restoreState(list.get(i));
-          q.add(new Term(field, termAtt.term()));
-        }
-        return q;
-      }
-    }
-  }
-
-  /**
-   * Base implementation delegates to {@link #getFieldQuery(String,String)}.
-   * This method may be overridden, for example, to return
-   * a SpanNearQuery instead of a PhraseQuery.
-   *
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFieldQuery(String field, String queryText, int slop)
-        throws ParseException {
-    Query query = getFieldQuery(field, queryText);
-
-    if (query instanceof PhraseQuery) {
-      ((PhraseQuery) query).setSlop(slop);
-    }
-    if (query instanceof MultiPhraseQuery) {
-      ((MultiPhraseQuery) query).setSlop(slop);
-    }
-
-    return query;
-  }
-
-  /**
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getRangeQuery(String field,
-                                String part1,
-                                String part2,
-                                boolean inclusive) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      part1 = part1.toLowerCase();
-      part2 = part2.toLowerCase();
-    }
-    try {
-      DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT, locale);
-      df.setLenient(true);
-      Date d1 = df.parse(part1);
-      Date d2 = df.parse(part2);
-      part1 = DateTools.dateToString(d1, DateTools.Resolution.DAY);
-      part2 = DateTools.dateToString(d2, DateTools.Resolution.DAY);
-    }
-    catch (Exception e) { }
-
-    return new TermRangeQuery(field, part1, part2, inclusive, inclusive);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getBooleanQuery(List<BooleanClause> clauses) throws ParseException
-  {
-    return getBooleanQuery(clauses, false);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   * @param disableCoord true if coord scoring should be disabled.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getBooleanQuery(List<BooleanClause> clauses, boolean disableCoord)
-      throws ParseException {
-    if (clauses == null || clauses.size() == 0)
-      return null;
-
-    BooleanQuery query = new BooleanQuery(disableCoord);
-    for (int i = 0; i < clauses.size(); i++) {
-      query.add(clauses.get(i));
-    }
-    return query;
-  }
-
-  /**
-   * Factory method for generating a query. Called when parser
-   * parses an input term token that contains one or more wildcard
-   * characters (? and *), but is not a prefix term token (one
-   * that has just a single * character at the end)
-   *<p>
-   * Depending on settings, prefix term may be lower-cased
-   * automatically. It will not go through the default Analyzer,
-   * however, since normal Analyzers are unlikely to work properly
-   * with wildcard templates.
-   *<p>
-   * Can be overridden by extending classes, to provide custom handling for
-   * wildcard queries, which may be necessary due to missing analyzer calls.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token that contains one or more wild card
-   *   characters (? or *), but is not simple prefix term
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getWildcardQuery(String field, String termStr) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new WildcardQuery(t);
-  }
-
-  /**
-   * Factory method for generating a query (similar to
-   * {@link #getWildcardQuery}). Called when parser parses an input term
-   * token that uses prefix notation; that is, contains a single '*' wildcard
-   * character as its last character. Since this is a special case
-   * of generic wildcard term, and such a query can be optimized easily,
-   * this usually results in a different query object.
-   *<p>
-   * Depending on settings, a prefix term may be lower-cased
-   * automatically. It will not go through the default Analyzer,
-   * however, since normal Analyzers are unlikely to work properly
-   * with wildcard templates.
-   *<p>
-   * Can be overridden by extending classes, to provide custom handling for
-   * wild card queries, which may be necessary due to missing analyzer calls.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *    (<b>without</b> trailing '*' character!)
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getPrefixQuery(String field, String termStr) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new PrefixQuery(t);
-  }
-
-   /**
-   * Factory method for generating a query (similar to
-   * {@link #getWildcardQuery}). Called when parser parses
-   * an input term token that has the fuzzy suffix (~) appended.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new FuzzyQuery(t, minSimilarity, fuzzyPrefixLength);
-  }
-
-  /**
-   * Returns a String where the escape char has been
-   * removed, or kept only once if there was a double escape.
-   */
-  private String discardEscapeChar(String input) {
-    char[] caSource = input.toCharArray();
-    char[] caDest = new char[caSource.length];
-    int j = 0;
-    for (int i = 0; i < caSource.length; i++) {
-      if ((caSource[i] != '\\') || (i > 0 && caSource[i-1] == '\\')) {
-        caDest[j++]=caSource[i];
-      }
-    }
-    return new String(caDest, 0, j);
-  }
-
-  /**
-   * Returns a String where those characters that QueryParser
-   * expects to be escaped are escaped by a preceding <code>\</code>.
-   */
-  public static String escape(String s) {
-    StringBuffer sb = new StringBuffer();
-    for (int i = 0; i < s.length(); i++) {
-      char c = s.charAt(i);
-      // NOTE: keep this in sync with _ESCAPED_CHAR below!
-      if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
-        || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
-        || c == '*' || c == '?') {
-        sb.append('\\');
-      }
-      sb.append(c);
-    }
-    return sb.toString();
-  }
-
-  /**
-   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
-   * Usage:<br>
-   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
-   */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
-
-// *   Query  ::= ( Clause )*
-// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
-  final public int Conjunction() throws ParseException {
-  int ret = CONJ_NONE;
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case AND:
-    case OR:
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case AND:
-        jj_consume_token(AND);
-            ret = CONJ_AND;
-        break;
-      case OR:
-        jj_consume_token(OR);
-              ret = CONJ_OR;
-        break;
-      default:
-        jj_la1[0] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      break;
-    default:
-      jj_la1[1] = jj_gen;
-      ;
-    }
-    {if (true) return ret;}
-    throw new Error("Missing return statement in function");
-  }
-
-  final public int Modifier() throws ParseException {
-  int ret = MOD_NONE;
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case NOT:
-    case PLUS:
-    case MINUS:
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case PLUS:
-        jj_consume_token(PLUS);
-              ret = MOD_REQ;
-        break;
-      case MINUS:
-        jj_consume_token(MINUS);
-                 ret = MOD_NOT;
-        break;
-      case NOT:
-        jj_consume_token(NOT);
-               ret = MOD_NOT;
-        break;
-      default:
-        jj_la1[2] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      break;
-    default:
-      jj_la1[3] = jj_gen;
-      ;
-    }
-    {if (true) return ret;}
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Query Query(String field) throws ParseException {
-  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-  Query q, firstQuery=null;
-  boolean orPresent = false;
-  int modifier;
-    modifier = Modifier();
-    q = andExpression(field);
-    addClause(clauses, CONJ_NONE, modifier, q);
-    if (modifier == MOD_NONE)
-      firstQuery = q;
-    label_1:
-    while (true) {
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case OR:
-      case NOT:
-      case PLUS:
-      case MINUS:
-      case LPAREN:
-      case QUOTED:
-      case TERM:
-      case PREFIXTERM:
-      case WILDTERM:
-      case RANGEIN_START:
-      case RANGEEX_START:
-      case NUMBER:
-        ;
-        break;
-      default:
-        jj_la1[4] = jj_gen;
-        break label_1;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case OR:
-        jj_consume_token(OR);
-            orPresent=true;
-        break;
-      default:
-        jj_la1[5] = jj_gen;
-        ;
-      }
-      modifier = Modifier();
-      q = andExpression(field);
-      addClause(clauses, orPresent ? CONJ_OR : CONJ_NONE, modifier, q);
-    }
-      if (clauses.size() == 1 && firstQuery != null)
-        {if (true) return firstQuery;}
-      else {
-        {if (true) return getBooleanQuery(clauses);}
-      }
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Query andExpression(String field) throws ParseException {
-  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-  Query q, firstQuery=null;
-  int modifier;
-    q = Clause(field);
-    addClause(clauses, CONJ_NONE, MOD_NONE, q);
-    firstQuery = q;
-    label_2:
-    while (true) {
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case AND:
-        ;
-        break;
-      default:
-        jj_la1[6] = jj_gen;
-        break label_2;
-      }
-      jj_consume_token(AND);
-      modifier = Modifier();
-      q = Clause(field);
-      addClause(clauses, CONJ_AND, modifier, q);
-    }
-      if (clauses.size() == 1 && firstQuery != null)
-        {if (true) return firstQuery;}
-      else {
-        {if (true) return getBooleanQuery(clauses);}
-      }
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Query Clause(String field) throws ParseException {
-  Query q;
-  Token fieldToken=null, boost=null;
-    if (jj_2_1(2)) {
-      fieldToken = jj_consume_token(TERM);
-      jj_consume_token(COLON);
-      field=discardEscapeChar(fieldToken.image);
-    } else {
-      ;
-    }
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case QUOTED:
-    case TERM:
-    case PREFIXTERM:
-    case WILDTERM:
-    case RANGEIN_START:
-    case RANGEEX_START:
-    case NUMBER:
-      q = Term(field);
-      break;
-    case LPAREN:
-      jj_consume_token(LPAREN);
-      q = Query(field);
-      jj_consume_token(RPAREN);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[7] = jj_gen;
-        ;
-      }
-      break;
-    default:
-      jj_la1[8] = jj_gen;
-      jj_consume_token(-1);
-      throw new ParseException();
-    }
-      if (boost != null) {
-        float f = (float)1.0;
-  try {
-    f = Float.valueOf(boost.image).floatValue();
-          q.setBoost(f);
-  } catch (Exception ignored) { }
-      }
-      {if (true) return q;}
-    throw new Error("Missing return statement in function");
-  }
-
-  final public Query Term(String field) throws ParseException {
-  Token term, boost=null, fuzzySlop=null, goop1, goop2;
-  boolean prefix = false;
-  boolean wildcard = false;
-  boolean fuzzy = false;
-  Query q;
-    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-    case TERM:
-    case PREFIXTERM:
-    case WILDTERM:
-    case NUMBER:
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case TERM:
-        term = jj_consume_token(TERM);
-        break;
-      case PREFIXTERM:
-        term = jj_consume_token(PREFIXTERM);
-                             prefix=true;
-        break;
-      case WILDTERM:
-        term = jj_consume_token(WILDTERM);
-                           wildcard=true;
-        break;
-      case NUMBER:
-        term = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[9] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case FUZZY_SLOP:
-        fuzzySlop = jj_consume_token(FUZZY_SLOP);
-                                fuzzy=true;
-        break;
-      default:
-        jj_la1[10] = jj_gen;
-        ;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-        case FUZZY_SLOP:
-          fuzzySlop = jj_consume_token(FUZZY_SLOP);
-                                                         fuzzy=true;
-          break;
-        default:
-          jj_la1[11] = jj_gen;
-          ;
-        }
-        break;
-      default:
-        jj_la1[12] = jj_gen;
-        ;
-      }
-       String termImage=discardEscapeChar(term.image);
-       if (wildcard) {
-       q = getWildcardQuery(field, termImage);
-       } else if (prefix) {
-         q = getPrefixQuery(field,
-           discardEscapeChar(term.image.substring
-          (0, term.image.length()-1)));
-       } else if (fuzzy) {
-          float fms = fuzzyMinSim;
-          try {
-            fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
-          } catch (Exception ignored) { }
-         if(fms < 0.0f || fms > 1.0f){
-           {if (true) throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");}
-         }
-         q = getFuzzyQuery(field, termImage, fms);
-       } else {
-         q = getFieldQuery(field, termImage);
-       }
-      break;
-    case RANGEIN_START:
-      jj_consume_token(RANGEIN_START);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEIN_GOOP:
-        goop1 = jj_consume_token(RANGEIN_GOOP);
-        break;
-      case RANGEIN_QUOTED:
-        goop1 = jj_consume_token(RANGEIN_QUOTED);
-        break;
-      default:
-        jj_la1[13] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEIN_TO:
-        jj_consume_token(RANGEIN_TO);
-        break;
-      default:
-        jj_la1[14] = jj_gen;
-        ;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEIN_GOOP:
-        goop2 = jj_consume_token(RANGEIN_GOOP);
-        break;
-      case RANGEIN_QUOTED:
-        goop2 = jj_consume_token(RANGEIN_QUOTED);
-        break;
-      default:
-        jj_la1[15] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      jj_consume_token(RANGEIN_END);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[16] = jj_gen;
-        ;
-      }
-          if (goop1.kind == RANGEIN_QUOTED) {
-            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
-          } else {
-            goop1.image = discardEscapeChar(goop1.image);
-          }
-          if (goop2.kind == RANGEIN_QUOTED) {
-            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
-      } else {
-        goop2.image = discardEscapeChar(goop2.image);
-      }
-          q = getRangeQuery(field, goop1.image, goop2.image, true);
-      break;
-    case RANGEEX_START:
-      jj_consume_token(RANGEEX_START);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEEX_GOOP:
-        goop1 = jj_consume_token(RANGEEX_GOOP);
-        break;
-      case RANGEEX_QUOTED:
-        goop1 = jj_consume_token(RANGEEX_QUOTED);
-        break;
-      default:
-        jj_la1[17] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEEX_TO:
-        jj_consume_token(RANGEEX_TO);
-        break;
-      default:
-        jj_la1[18] = jj_gen;
-        ;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case RANGEEX_GOOP:
-        goop2 = jj_consume_token(RANGEEX_GOOP);
-        break;
-      case RANGEEX_QUOTED:
-        goop2 = jj_consume_token(RANGEEX_QUOTED);
-        break;
-      default:
-        jj_la1[19] = jj_gen;
-        jj_consume_token(-1);
-        throw new ParseException();
-      }
-      jj_consume_token(RANGEEX_END);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[20] = jj_gen;
-        ;
-      }
-          if (goop1.kind == RANGEEX_QUOTED) {
-            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
-          } else {
-            goop1.image = discardEscapeChar(goop1.image);
-          }
-          if (goop2.kind == RANGEEX_QUOTED) {
-            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
-      } else {
-        goop2.image = discardEscapeChar(goop2.image);
-      }
-
-          q = getRangeQuery(field, goop1.image, goop2.image, false);
-      break;
-    case QUOTED:
-      term = jj_consume_token(QUOTED);
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case FUZZY_SLOP:
-        fuzzySlop = jj_consume_token(FUZZY_SLOP);
-        break;
-      default:
-        jj_la1[21] = jj_gen;
-        ;
-      }
-      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
-      case CARAT:
-        jj_consume_token(CARAT);
-        boost = jj_consume_token(NUMBER);
-        break;
-      default:
-        jj_la1[22] = jj_gen;
-        ;
-      }
-         int s = phraseSlop;
-
-         if (fuzzySlop != null) {
-           try {
-             s = Float.valueOf(fuzzySlop.image.substring(1)).intValue();
-           }
-           catch (Exception ignored) { }
-         }
-         q = getFieldQuery(field, term.image.substring(1, term.image.length()-1), s);
-      break;
-    default:
-      jj_la1[23] = jj_gen;
-      jj_consume_token(-1);
-      throw new ParseException();
-    }
-    if (boost != null) {
-      float f = (float) 1.0;
-      try {
-        f = Float.valueOf(boost.image).floatValue();
-      }
-      catch (Exception ignored) {
-    /* Should this be handled somehow? (defaults to "no boost", if
-     * boost number is invalid)
-     */
-      }
-
-      // avoid boosting null queries, such as those caused by stop words
-      if (q != null) {
-        q.setBoost(f);
-      }
-    }
-    {if (true) return q;}
-    throw new Error("Missing return statement in function");
-  }
-
-  private boolean jj_2_1(int xla) {
-    jj_la = xla; jj_lastpos = jj_scanpos = token;
-    try { return !jj_3_1(); }
-    catch(LookaheadSuccess ls) { return true; }
-    finally { jj_save(0, xla); }
-  }
-
-  private boolean jj_3_1() {
-    if (jj_scan_token(TERM)) return true;
-    if (jj_scan_token(COLON)) return true;
-    return false;
-  }
-
-  /** Generated Token Manager. */
-  public PrecedenceQueryParserTokenManager token_source;
-  /** Current token. */
-  public Token token;
-  /** Next token. */
-  public Token jj_nt;
-  private int jj_ntk;
-  private Token jj_scanpos, jj_lastpos;
-  private int jj_la;
-  private int jj_gen;
-  final private int[] jj_la1 = new int[24];
-  static private int[] jj_la1_0;
-  static {
-      jj_la1_init_0();
-   }
-   private static void jj_la1_init_0() {
-      jj_la1_0 = new int[] {0x180,0x180,0xe00,0xe00,0xfb1f00,0x100,0x80,0x8000,0xfb1000,0x9a0000,0x40000,0x40000,0x8000,0xc000000,0x1000000,0xc000000,0x8000,0xc0000000,0x10000000,0xc0000000,0x8000,0x40000,0x8000,0xfb0000,};
-   }
-  final private JJCalls[] jj_2_rtns = new JJCalls[1];
-  private boolean jj_rescan = false;
-  private int jj_gc = 0;
-
-  /** Constructor with user supplied CharStream. */
-  public PrecedenceQueryParser(CharStream stream) {
-    token_source = new PrecedenceQueryParserTokenManager(stream);
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  /** Reinitialise. */
-  public void ReInit(CharStream stream) {
-    token_source.ReInit(stream);
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  /** Constructor with generated Token Manager. */
-  public PrecedenceQueryParser(PrecedenceQueryParserTokenManager tm) {
-    token_source = tm;
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  /** Reinitialise. */
-  public void ReInit(PrecedenceQueryParserTokenManager tm) {
-    token_source = tm;
-    token = new Token();
-    jj_ntk = -1;
-    jj_gen = 0;
-    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
-    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
-  }
-
-  private Token jj_consume_token(int kind) throws ParseException {
-    Token oldToken;
-    if ((oldToken = token).next != null) token = token.next;
-    else token = token.next = token_source.getNextToken();
-    jj_ntk = -1;
-    if (token.kind == kind) {
-      jj_gen++;
-      if (++jj_gc > 100) {
-        jj_gc = 0;
-        for (int i = 0; i < jj_2_rtns.length; i++) {
-          JJCalls c = jj_2_rtns[i];
-          while (c != null) {
-            if (c.gen < jj_gen) c.first = null;
-            c = c.next;
-          }
-        }
-      }
-      return token;
-    }
-    token = oldToken;
-    jj_kind = kind;
-    throw generateParseException();
-  }
-
-  static private final class LookaheadSuccess extends java.lang.Error { }
-  final private LookaheadSuccess jj_ls = new LookaheadSuccess();
-  private boolean jj_scan_token(int kind) {
-    if (jj_scanpos == jj_lastpos) {
-      jj_la--;
-      if (jj_scanpos.next == null) {
-        jj_lastpos = jj_scanpos = jj_scanpos.next = token_source.getNextToken();
-      } else {
-        jj_lastpos = jj_scanpos = jj_scanpos.next;
-      }
-    } else {
-      jj_scanpos = jj_scanpos.next;
-    }
-    if (jj_rescan) {
-      int i = 0; Token tok = token;
-      while (tok != null && tok != jj_scanpos) { i++; tok = tok.next; }
-      if (tok != null) jj_add_error_token(kind, i);
-    }
-    if (jj_scanpos.kind != kind) return true;
-    if (jj_la == 0 && jj_scanpos == jj_lastpos) throw jj_ls;
-    return false;
-  }
-
-
-/** Get the next Token. */
-  final public Token getNextToken() {
-    if (token.next != null) token = token.next;
-    else token = token.next = token_source.getNextToken();
-    jj_ntk = -1;
-    jj_gen++;
-    return token;
-  }
-
-/** Get the specific Token. */
-  final public Token getToken(int index) {
-    Token t = token;
-    for (int i = 0; i < index; i++) {
-      if (t.next != null) t = t.next;
-      else t = t.next = token_source.getNextToken();
-    }
-    return t;
-  }
-
-  private int jj_ntk() {
-    if ((jj_nt=token.next) == null)
-      return (jj_ntk = (token.next=token_source.getNextToken()).kind);
-    else
-      return (jj_ntk = jj_nt.kind);
-  }
-
-  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<int[]>();
-  private int[] jj_expentry;
-  private int jj_kind = -1;
-  private int[] jj_lasttokens = new int[100];
-  private int jj_endpos;
-
-  private void jj_add_error_token(int kind, int pos) {
-    if (pos >= 100) return;
-    if (pos == jj_endpos + 1) {
-      jj_lasttokens[jj_endpos++] = kind;
-    } else if (jj_endpos != 0) {
-      jj_expentry = new int[jj_endpos];
-      for (int i = 0; i < jj_endpos; i++) {
-        jj_expentry[i] = jj_lasttokens[i];
-      }
-      jj_entries_loop: for (java.util.Iterator it = jj_expentries.iterator(); it.hasNext();) {
-        int[] oldentry = (int[])(it.next());
-        if (oldentry.length == jj_expentry.length) {
-          for (int i = 0; i < jj_expentry.length; i++) {
-            if (oldentry[i] != jj_expentry[i]) {
-              continue jj_entries_loop;
-            }
-          }
-          jj_expentries.add(jj_expentry);
-          break jj_entries_loop;
-        }
-      }
-      if (pos != 0) jj_lasttokens[(jj_endpos = pos) - 1] = kind;
-    }
-  }
-
-  /** Generate ParseException. */
-  public ParseException generateParseException() {
-    jj_expentries.clear();
-    boolean[] la1tokens = new boolean[32];
-    if (jj_kind >= 0) {
-      la1tokens[jj_kind] = true;
-      jj_kind = -1;
-    }
-    for (int i = 0; i < 24; i++) {
-      if (jj_la1[i] == jj_gen) {
-        for (int j = 0; j < 32; j++) {
-          if ((jj_la1_0[i] & (1<<j)) != 0) {
-            la1tokens[j] = true;
-          }
-        }
-      }
-    }
-    for (int i = 0; i < 32; i++) {
-      if (la1tokens[i]) {
-        jj_expentry = new int[1];
-        jj_expentry[0] = i;
-        jj_expentries.add(jj_expentry);
-      }
-    }
-    jj_endpos = 0;
-    jj_rescan_token();
-    jj_add_error_token(0, 0);
-    int[][] exptokseq = new int[jj_expentries.size()][];
-    for (int i = 0; i < jj_expentries.size(); i++) {
-      exptokseq[i] = jj_expentries.get(i);
-    }
-    return new ParseException(token, exptokseq, tokenImage);
-  }
-
-  /** Enable tracing. */
-  final public void enable_tracing() {
-  }
-
-  /** Disable tracing. */
-  final public void disable_tracing() {
-  }
-
-  private void jj_rescan_token() {
-    jj_rescan = true;
-    for (int i = 0; i < 1; i++) {
-    try {
-      JJCalls p = jj_2_rtns[i];
-      do {
-        if (p.gen > jj_gen) {
-          jj_la = p.arg; jj_lastpos = jj_scanpos = p.first;
-          switch (i) {
-            case 0: jj_3_1(); break;
-          }
-        }
-        p = p.next;
-      } while (p != null);
-      } catch(LookaheadSuccess ls) { }
-    }
-    jj_rescan = false;
-  }
-
-  private void jj_save(int index, int xla) {
-    JJCalls p = jj_2_rtns[index];
-    while (p.gen > jj_gen) {
-      if (p.next == null) { p = p.next = new JJCalls(); break; }
-      p = p.next;
-    }
-    p.gen = jj_gen + xla - jj_la; p.first = token; p.arg = xla;
-  }
-
-  static final class JJCalls {
-    int gen;
-    Token first;
-    int arg;
-    JJCalls next;
-  }
-
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
deleted file mode 100644
index 9794e13..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
+++ /dev/null
@@ -1,905 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-options {
-  STATIC=false;
-  JAVA_UNICODE_ESCAPE=true;
-  USER_CHAR_STREAM=true;
-}
-
-PARSER_BEGIN(PrecedenceQueryParser)
-
-package org.apache.lucene.queryParser.precedence;
-
-import java.io.IOException;
-import java.io.StringReader;
-import java.text.DateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.List;
-import java.util.Locale;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.*;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.MultiPhraseQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.AttributeSource;
-
-/**
- * Experimental query parser variant designed to handle operator precedence
- * in a more sensible fashion than QueryParser.  There are still some
- * open issues with this parser. The following tests are currently failing
- * in TestPrecedenceQueryParser and are disabled to make this test pass:
- * <ul>
- * <li> testSimple
- * <li> testWildcard
- * <li> testPrecedence
- * </ul>
- *
- * This class is generated by JavaCC.  The only method that clients should need
- * to call is {@link #parse(String)}.
- *
- * The syntax for query strings is as follows:
- * A Query is a series of clauses.
- * A clause may be prefixed by:
- * <ul>
- * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
- * that the clause is required or prohibited respectively; or
- * <li> a term followed by a colon, indicating the field to be searched.
- * This enables one to construct queries which search multiple fields.
- * </ul>
- *
- * A clause may be either:
- * <ul>
- * <li> a term, indicating all the documents that contain this term; or
- * <li> a nested query, enclosed in parentheses.  Note that this may be used
- * with a <code>+</code>/<code>-</code> prefix to require any of a set of
- * terms.
- * </ul>
- *
- * Thus, in BNF, the query grammar is:
- * <pre>
- *   Query  ::= ( Clause )*
- *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
- * </pre>
- *
- * <p>
- * Examples of appropriately formatted queries can be found in the <a
- * href="../../../../../../../queryparsersyntax.html">query syntax
- * documentation</a>.
- * </p>
- *
- * @author Brian Goetz
- * @author Peter Halacsy
- * @author Tatu Saloranta
- */
-
-public class PrecedenceQueryParser {
-
-  private static final int CONJ_NONE   = 0;
-  private static final int CONJ_AND    = 1;
-  private static final int CONJ_OR     = 2;
-
-  private static final int MOD_NONE    = 0;
-  private static final int MOD_NOT     = 10;
-  private static final int MOD_REQ     = 11;
-
-  // make it possible to call setDefaultOperator() without accessing
-  // the nested class:
-  public static final Operator AND_OPERATOR = Operator.AND;
-  public static final Operator OR_OPERATOR = Operator.OR;
-
-  /** The actual operator that parser uses to combine query terms */
-  private Operator operator = OR_OPERATOR;
-
-  boolean lowercaseExpandedTerms = true;
-
-  Analyzer analyzer;
-  String field;
-  int phraseSlop = 0;
-  float fuzzyMinSim = FuzzyQuery.defaultMinSimilarity;
-  int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
-  Locale locale = Locale.getDefault();
-
-  static enum Operator { OR, AND }
-
-  /** Constructs a query parser.
-   *  @param f  the default field for query terms.
-   *  @param a   used to find terms in the query text.
-   */
-  public PrecedenceQueryParser(String f, Analyzer a) {
-    this(new FastCharStream(new StringReader("")));
-    analyzer = a;
-    field = f;
-  }
-
-  /** Parses a query string, returning a {@link org.apache.lucene.search.Query}.
-   *  @param expression  the query string to be parsed.
-   *  @throws ParseException if the parsing fails
-   */
-  public Query parse(String expression) throws ParseException {
-    // optimize empty query to be empty BooleanQuery
-    if (expression == null || expression.trim().length() == 0) {
-      return new BooleanQuery();
-    }
-
-    ReInit(new FastCharStream(new StringReader(expression)));
-    try {
-      Query query = Query(field);
-      return (query != null) ? query : new BooleanQuery();
-    }
-    catch (TokenMgrError tme) {
-      throw new ParseException(tme.getMessage());
-    }
-    catch (BooleanQuery.TooManyClauses tmc) {
-      throw new ParseException("Too many boolean clauses");
-    }
-  }
-
-   /**
-   * @return Returns the analyzer.
-   */
-  public Analyzer getAnalyzer() {
-    return analyzer;
-  }
-
-  /**
-   * @return Returns the field.
-   */
-  public String getField() {
-    return field;
-  }
-
-   /**
-   * Get the minimal similarity for fuzzy queries.
-   */
-  public float getFuzzyMinSim() {
-      return fuzzyMinSim;
-  }
-
-  /**
-   * Set the minimum similarity for fuzzy queries.
-   * Default is 0.5f.
-   */
-  public void setFuzzyMinSim(float fuzzyMinSim) {
-      this.fuzzyMinSim = fuzzyMinSim;
-  }
-
-   /**
-   * Get the prefix length for fuzzy queries. 
-   * @return Returns the fuzzyPrefixLength.
-   */
-  public int getFuzzyPrefixLength() {
-    return fuzzyPrefixLength;
-  }
-
-  /**
-   * Set the prefix length for fuzzy queries. Default is 0.
-   * @param fuzzyPrefixLength The fuzzyPrefixLength to set.
-   */
-  public void setFuzzyPrefixLength(int fuzzyPrefixLength) {
-    this.fuzzyPrefixLength = fuzzyPrefixLength;
-  }
-
-  /**
-   * Sets the default slop for phrases.  If zero, then exact phrase matches
-   * are required.  Default value is zero.
-   */
-  public void setPhraseSlop(int phraseSlop) {
-    this.phraseSlop = phraseSlop;
-  }
-
-  /**
-   * Gets the default slop for phrases.
-   */
-  public int getPhraseSlop() {
-    return phraseSlop;
-  }
-
-  /**
-   * Sets the boolean operator of the QueryParser.
-   * In default mode (<code>OR_OPERATOR</code>) terms without any modifiers
-   * are considered optional: for example <code>capital of Hungary</code> is equal to
-   * <code>capital OR of OR Hungary</code>.<br/>
-   * In <code>AND_OPERATOR</code> mode terms are considered to be in conjunction: the
-   * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
-   */
-  public void setDefaultOperator(Operator op) {
-    this.operator = op;
-  }
-
-  /**
-   * Gets implicit operator setting, which will be either AND_OPERATOR
-   * or OR_OPERATOR.
-   */
-  public Operator getDefaultOperator() {
-    return operator;
-  }
-
-  /**
-   * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically
-   * lower-cased or not.  Default is <code>true</code>.
-   */
-  public void setLowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
-    this.lowercaseExpandedTerms = lowercaseExpandedTerms;
-  }
-
-  /**
-   * @see #setLowercaseExpandedTerms(boolean)
-   */
-  public boolean getLowercaseExpandedTerms() {
-    return lowercaseExpandedTerms;
-  }
-
-  /**
-   * Set locale used by date range parsing.
-   */
-  public void setLocale(Locale locale) {
-    this.locale = locale;
-  }
-
-  /**
-   * Returns current locale, allowing access by subclasses.
-   */
-  public Locale getLocale() {
-    return locale;
-  }
-
-  protected void addClause(List<BooleanClause> clauses, int conj, int modifier, Query q) {
-    boolean required, prohibited;
-
-    // If this term is introduced by AND, make the preceding term required,
-    // unless it's already prohibited
-    if (clauses.size() > 0 && conj == CONJ_AND) {
-      BooleanClause c = clauses.get(clauses.size()-1);
-      if (!c.isProhibited())
-        c.setOccur(BooleanClause.Occur.MUST);
-    }
-
-    if (clauses.size() > 0 && operator == AND_OPERATOR && conj == CONJ_OR) {
-      // If this term is introduced by OR, make the preceding term optional,
-      // unless it's prohibited (that means we leave -a OR b but +a OR b-->a OR b)
-      // notice if the input is a OR b, first term is parsed as required; without
-      // this modification a OR b would parsed as +a OR b
-      BooleanClause c = clauses.get(clauses.size()-1);
-      if (!c.isProhibited())
-        c.setOccur(BooleanClause.Occur.SHOULD);
-    }
-
-    // We might have been passed a null query; the term might have been
-    // filtered away by the analyzer.
-    if (q == null)
-      return;
-
-    if (operator == OR_OPERATOR) {
-      // We set REQUIRED if we're introduced by AND or +; PROHIBITED if
-      // introduced by NOT or -; make sure not to set both.
-      prohibited = (modifier == MOD_NOT);
-      required = (modifier == MOD_REQ);
-      if (conj == CONJ_AND && !prohibited) {
-        required = true;
-      }
-    } else {
-      // We set PROHIBITED if we're introduced by NOT or -; We set REQUIRED
-      // if not PROHIBITED and not introduced by OR
-      prohibited = (modifier == MOD_NOT);
-      required   = (!prohibited && conj != CONJ_OR);
-    }
-    if (required && !prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST));
-    else if (!required && !prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
-    else if (!required && prohibited)
-      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST_NOT));
-    else
-      throw new RuntimeException("Clause cannot be both required and prohibited");
-  }
-
-  /**
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFieldQuery(String field, String queryText)  throws ParseException {
-    // Use the analyzer to get all the tokens, and then build a TermQuery,
-    // PhraseQuery, or nothing based on the term count
-
-    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
-    List<AttributeSource.State> list = new ArrayList<AttributeSource.State>();
-    int positionCount = 0;
-    boolean severalTokensAtSamePosition = false;
-    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
-    PositionIncrementAttribute posincrAtt = source.addAttribute(PositionIncrementAttribute.class);
-
-    try {
-      while (source.incrementToken()) {
-        list.add(source.captureState());
-        if (posincrAtt.getPositionIncrement() == 1)
-          positionCount++;
-        else
-          severalTokensAtSamePosition = true;
-      }
-      source.end();
-      source.close();
-    } catch (IOException e) {
-      // ignore, should never happen for StringReaders
-    }
-
-    if (list.size() == 0)
-      return null;
-    else if (list.size() == 1) {
-      source.restoreState(list.get(0));
-      return new TermQuery(new Term(field, termAtt.term()));
-    } else {
-      if (severalTokensAtSamePosition) {
-        if (positionCount == 1) {
-          // no phrase query:
-          BooleanQuery q = new BooleanQuery();
-          for (int i = 0; i < list.size(); i++) {
-            source.restoreState(list.get(i));
-            TermQuery currentQuery = new TermQuery(
-                new Term(field, termAtt.term()));
-            q.add(currentQuery, BooleanClause.Occur.SHOULD);
-          }
-          return q;
-        }
-        else {
-          // phrase query:
-          MultiPhraseQuery mpq = new MultiPhraseQuery();
-          List<Term> multiTerms = new ArrayList<Term>();
-          for (int i = 0; i < list.size(); i++) {
-            source.restoreState(list.get(i));
-            if (posincrAtt.getPositionIncrement() == 1 && multiTerms.size() > 0) {
-              mpq.add(multiTerms.toArray(new Term[0]));
-              multiTerms.clear();
-            }
-            multiTerms.add(new Term(field, termAtt.term()));
-          }
-          mpq.add(multiTerms.toArray(new Term[0]));
-          return mpq;
-        }
-      }
-      else {
-        PhraseQuery q = new PhraseQuery();
-        q.setSlop(phraseSlop);
-        for (int i = 0; i < list.size(); i++) {
-          source.restoreState(list.get(i));
-          q.add(new Term(field, termAtt.term()));
-        }
-        return q;
-      }
-    }
-  }
-
-  /**
-   * Base implementation delegates to {@link #getFieldQuery(String,String)}.
-   * This method may be overridden, for example, to return
-   * a SpanNearQuery instead of a PhraseQuery.
-   *
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFieldQuery(String field, String queryText, int slop)
-        throws ParseException {
-    Query query = getFieldQuery(field, queryText);
-
-    if (query instanceof PhraseQuery) {
-      ((PhraseQuery) query).setSlop(slop);
-    }
-    if (query instanceof MultiPhraseQuery) {
-      ((MultiPhraseQuery) query).setSlop(slop);
-    }
-
-    return query;
-  }
-
-  /**
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getRangeQuery(String field,
-                                String part1,
-                                String part2,
-                                boolean inclusive) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      part1 = part1.toLowerCase();
-      part2 = part2.toLowerCase();
-    }
-    try {
-      DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT, locale);
-      df.setLenient(true);
-      Date d1 = df.parse(part1);
-      Date d2 = df.parse(part2);
-      part1 = DateTools.dateToString(d1, DateTools.Resolution.DAY);
-      part2 = DateTools.dateToString(d2, DateTools.Resolution.DAY);
-    }
-    catch (Exception e) { }
-
-    return new TermRangeQuery(field, part1, part2, inclusive, inclusive);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getBooleanQuery(List<BooleanClause> clauses) throws ParseException
-  {
-    return getBooleanQuery(clauses, false);
-  }
-
-  /**
-   * Factory method for generating query, given a set of clauses.
-   * By default creates a boolean query composed of clauses passed in.
-   *
-   * Can be overridden by extending classes, to modify query being
-   * returned.
-   *
-   * @param clauses List that contains {@link BooleanClause} instances
-   *    to join.
-   * @param disableCoord true if coord scoring should be disabled.
-   *
-   * @return Resulting {@link Query} object.
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getBooleanQuery(List<BooleanClause> clauses, boolean disableCoord)
-      throws ParseException {
-    if (clauses == null || clauses.size() == 0)
-      return null;
-
-    BooleanQuery query = new BooleanQuery(disableCoord);
-    for (int i = 0; i < clauses.size(); i++) {
-      query.add(clauses.get(i));
-    }
-    return query;
-  }
-
-  /**
-   * Factory method for generating a query. Called when parser
-   * parses an input term token that contains one or more wildcard
-   * characters (? and *), but is not a prefix term token (one
-   * that has just a single * character at the end)
-   *<p>
-   * Depending on settings, prefix term may be lower-cased
-   * automatically. It will not go through the default Analyzer,
-   * however, since normal Analyzers are unlikely to work properly
-   * with wildcard templates.
-   *<p>
-   * Can be overridden by extending classes, to provide custom handling for
-   * wildcard queries, which may be necessary due to missing analyzer calls.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token that contains one or more wild card
-   *   characters (? or *), but is not simple prefix term
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getWildcardQuery(String field, String termStr) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new WildcardQuery(t);
-  }
-
-  /**
-   * Factory method for generating a query (similar to
-   * {@link #getWildcardQuery}). Called when parser parses an input term
-   * token that uses prefix notation; that is, contains a single '*' wildcard
-   * character as its last character. Since this is a special case
-   * of generic wildcard term, and such a query can be optimized easily,
-   * this usually results in a different query object.
-   *<p>
-   * Depending on settings, a prefix term may be lower-cased
-   * automatically. It will not go through the default Analyzer,
-   * however, since normal Analyzers are unlikely to work properly
-   * with wildcard templates.
-   *<p>
-   * Can be overridden by extending classes, to provide custom handling for
-   * wild card queries, which may be necessary due to missing analyzer calls.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *    (<b>without</b> trailing '*' character!)
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getPrefixQuery(String field, String termStr) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new PrefixQuery(t);
-  }
-
-   /**
-   * Factory method for generating a query (similar to
-   * {@link #getWildcardQuery}). Called when parser parses
-   * an input term token that has the fuzzy suffix (~) appended.
-   *
-   * @param field Name of the field query will use.
-   * @param termStr Term token to use for building term for the query
-   *
-   * @return Resulting {@link Query} built for the term
-   * @exception ParseException throw in overridden method to disallow
-   */
-  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
-  {
-    if (lowercaseExpandedTerms) {
-      termStr = termStr.toLowerCase();
-    }
-    Term t = new Term(field, termStr);
-    return new FuzzyQuery(t, minSimilarity, fuzzyPrefixLength);
-  }
-
-  /**
-   * Returns a String where the escape char has been
-   * removed, or kept only once if there was a double escape.
-   */
-  private String discardEscapeChar(String input) {
-    char[] caSource = input.toCharArray();
-    char[] caDest = new char[caSource.length];
-    int j = 0;
-    for (int i = 0; i < caSource.length; i++) {
-      if ((caSource[i] != '\\') || (i > 0 && caSource[i-1] == '\\')) {
-        caDest[j++]=caSource[i];
-      }
-    }
-    return new String(caDest, 0, j);
-  }
-
-  /**
-   * Returns a String where those characters that QueryParser
-   * expects to be escaped are escaped by a preceding <code>\</code>.
-   */
-  public static String escape(String s) {
-    StringBuffer sb = new StringBuffer();
-    for (int i = 0; i < s.length(); i++) {
-      char c = s.charAt(i);
-      // NOTE: keep this in sync with _ESCAPED_CHAR below!
-      if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
-        || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
-        || c == '*' || c == '?') {
-        sb.append('\\');
-      }
-      sb.append(c);
-    }
-    return sb.toString();
-  }
-
-  /**
-   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
-   * Usage:<br>
-   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
-   */
-  public static void main(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
-      System.exit(0);
-    }
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
-                           new org.apache.lucene.analysis.SimpleAnalyzer());
-    Query q = qp.parse(args[0]);
-    System.out.println(q.toString("field"));
-  }
-}
-
-PARSER_END(PrecedenceQueryParser)
-
-/* ***************** */
-/* Token Definitions */
-/* ***************** */
-
-<*> TOKEN : {
-  <#_NUM_CHAR:   ["0"-"9"] >
-// NOTE: keep this in sync with escape(String) above!
-| <#_ESCAPED_CHAR: "\\" [ "\\", "+", "-", "!", "(", ")", ":", "^",
-                          "[", "]", "\"", "{", "}", "~", "*", "?" ] >
-| <#_TERM_START_CHAR: ( ~[ " ", "\t", "\n", "\r", "+", "-", "!", "(", ")", ":", "^",
-                           "[", "]", "\"", "{", "}", "~", "*", "?" ]
-                       | <_ESCAPED_CHAR> ) >
-| <#_TERM_CHAR: ( <_TERM_START_CHAR> | <_ESCAPED_CHAR> | "-" | "+" ) >
-| <#_WHITESPACE: ( " " | "\t" | "\n" | "\r") >
-}
-
-<DEFAULT, RangeIn, RangeEx> SKIP : {
-  < <_WHITESPACE>>
-}
-
-// OG: to support prefix queries:
-// http://nagoya.apache.org/bugzilla/show_bug.cgi?id=12137
-// Change from:
-// | <WILDTERM:  <_TERM_START_CHAR>
-//              (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
-// To:
-//
-// | <WILDTERM:  (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
-
-<DEFAULT> TOKEN : {
-  <AND:       ("AND" | "&&") >
-| <OR:        ("OR" | "||") >
-| <NOT:       ("NOT" | "!") >
-| <PLUS:      "+" >
-| <MINUS:     "-" >
-| <LPAREN:    "(" >
-| <RPAREN:    ")" >
-| <COLON:     ":" >
-| <CARAT:     "^" > : Boost
-| <QUOTED:     "\"" (~["\""])+ "\"">
-| <TERM:      <_TERM_START_CHAR> (<_TERM_CHAR>)*  >
-| <FUZZY_SLOP:     "~" ( (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? )? >
-| <PREFIXTERM:  <_TERM_START_CHAR> (<_TERM_CHAR>)* "*" >
-| <WILDTERM:  <_TERM_START_CHAR>
-              (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
-| <RANGEIN_START: "[" > : RangeIn
-| <RANGEEX_START: "{" > : RangeEx
-}
-
-<Boost> TOKEN : {
-<NUMBER:    (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? > : DEFAULT
-}
-
-<RangeIn> TOKEN : {
-<RANGEIN_TO: "TO">
-| <RANGEIN_END: "]"> : DEFAULT
-| <RANGEIN_QUOTED: "\"" (~["\""])+ "\"">
-| <RANGEIN_GOOP: (~[ " ", "]" ])+ >
-}
-
-<RangeEx> TOKEN : {
-<RANGEEX_TO: "TO">
-| <RANGEEX_END: "}"> : DEFAULT
-| <RANGEEX_QUOTED: "\"" (~["\""])+ "\"">
-| <RANGEEX_GOOP: (~[ " ", "}" ])+ >
-}
-
-// *   Query  ::= ( Clause )*
-// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
-
-int Conjunction() : {
-  int ret = CONJ_NONE;
-}
-{
-  [
-    <AND> { ret = CONJ_AND; }
-    | <OR>  { ret = CONJ_OR; }
-  ]
-  { return ret; }
-}
-
-int Modifier() : {
-  int ret = MOD_NONE;
-}
-{
-  [
-     <PLUS> { ret = MOD_REQ; }
-     | <MINUS> { ret = MOD_NOT; }
-     | <NOT> { ret = MOD_NOT; }
-  ]
-  { return ret; }
-}
-
-Query Query(String field) :
-{
-  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-  Query q, firstQuery=null;
-  boolean orPresent = false;
-  int modifier;
-}
-{
-  modifier=Modifier() q=andExpression(field)
-  {
-    addClause(clauses, CONJ_NONE, modifier, q);
-    if (modifier == MOD_NONE)
-      firstQuery = q;
-  }
-  (
-    [<OR> { orPresent=true; }] modifier=Modifier() q=andExpression(field)
-    { addClause(clauses, orPresent ? CONJ_OR : CONJ_NONE, modifier, q); }
-  )*
-    {
-      if (clauses.size() == 1 && firstQuery != null)
-        return firstQuery;
-      else {
-        return getBooleanQuery(clauses);
-      }
-    }
-}
-
-Query andExpression(String field) :
-{
-  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
-  Query q, firstQuery=null;
-  int modifier;
-}
-{
-  q=Clause(field)
-  {
-    addClause(clauses, CONJ_NONE, MOD_NONE, q);
-    firstQuery = q;
-  }
-  (
-    <AND> modifier=Modifier() q=Clause(field)
-    { addClause(clauses, CONJ_AND, modifier, q); }
-  )*
-    {
-      if (clauses.size() == 1 && firstQuery != null)
-        return firstQuery;
-      else {
-        return getBooleanQuery(clauses);
-      }
-    }
-}
-
-Query Clause(String field) : {
-  Query q;
-  Token fieldToken=null, boost=null;
-}
-{
-  [
-    LOOKAHEAD(2)
-    fieldToken=<TERM> <COLON> {
-      field=discardEscapeChar(fieldToken.image);
-    }
-  ]
-
-  (
-   q=Term(field)
-   | <LPAREN> q=Query(field) <RPAREN> (<CARAT> boost=<NUMBER>)?
-
-  )
-    {
-      if (boost != null) {
-        float f = (float)1.0;
-  try {
-    f = Float.valueOf(boost.image).floatValue();
-          q.setBoost(f);
-  } catch (Exception ignored) { }
-      }
-      return q;
-    }
-}
-
-
-Query Term(String field) : {
-  Token term, boost=null, fuzzySlop=null, goop1, goop2;
-  boolean prefix = false;
-  boolean wildcard = false;
-  boolean fuzzy = false;
-  Query q;
-}
-{
-  (
-     (
-       term=<TERM>
-       | term=<PREFIXTERM> { prefix=true; }
-       | term=<WILDTERM> { wildcard=true; }
-       | term=<NUMBER>
-     )
-     [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ]
-     [ <CARAT> boost=<NUMBER> [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ] ]
-     {
-       String termImage=discardEscapeChar(term.image);
-       if (wildcard) {
-       q = getWildcardQuery(field, termImage);
-       } else if (prefix) {
-         q = getPrefixQuery(field,
-           discardEscapeChar(term.image.substring
-          (0, term.image.length()-1)));
-       } else if (fuzzy) {
-       	  float fms = fuzzyMinSim;
-       	  try {
-            fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
-       	  } catch (Exception ignored) { }
-       	 if(fms < 0.0f || fms > 1.0f){
-       	   throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");
-       	 }
-         q = getFuzzyQuery(field, termImage, fms);
-       } else {
-         q = getFieldQuery(field, termImage);
-       }
-     }
-     | ( <RANGEIN_START> ( goop1=<RANGEIN_GOOP>|goop1=<RANGEIN_QUOTED> )
-         [ <RANGEIN_TO> ] ( goop2=<RANGEIN_GOOP>|goop2=<RANGEIN_QUOTED> )
-         <RANGEIN_END> )
-       [ <CARAT> boost=<NUMBER> ]
-        {
-          if (goop1.kind == RANGEIN_QUOTED) {
-            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
-          } else {
-            goop1.image = discardEscapeChar(goop1.image);
-          }
-          if (goop2.kind == RANGEIN_QUOTED) {
-            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
-      } else {
-        goop2.image = discardEscapeChar(goop2.image);
-      }
-          q = getRangeQuery(field, goop1.image, goop2.image, true);
-        }
-     | ( <RANGEEX_START> ( goop1=<RANGEEX_GOOP>|goop1=<RANGEEX_QUOTED> )
-         [ <RANGEEX_TO> ] ( goop2=<RANGEEX_GOOP>|goop2=<RANGEEX_QUOTED> )
-         <RANGEEX_END> )
-       [ <CARAT> boost=<NUMBER> ]
-        {
-          if (goop1.kind == RANGEEX_QUOTED) {
-            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
-          } else {
-            goop1.image = discardEscapeChar(goop1.image);
-          }
-          if (goop2.kind == RANGEEX_QUOTED) {
-            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
-      } else {
-        goop2.image = discardEscapeChar(goop2.image);
-      }
-
-          q = getRangeQuery(field, goop1.image, goop2.image, false);
-        }
-     | term=<QUOTED>
-       [ fuzzySlop=<FUZZY_SLOP> ]
-       [ <CARAT> boost=<NUMBER> ]
-       {
-         int s = phraseSlop;
-
-         if (fuzzySlop != null) {
-           try {
-             s = Float.valueOf(fuzzySlop.image.substring(1)).intValue();
-           }
-           catch (Exception ignored) { }
-         }
-         q = getFieldQuery(field, term.image.substring(1, term.image.length()-1), s);
-       }
-  )
-  {
-    if (boost != null) {
-      float f = (float) 1.0;
-      try {
-        f = Float.valueOf(boost.image).floatValue();
-      }
-      catch (Exception ignored) {
-    /* Should this be handled somehow? (defaults to "no boost", if
-     * boost number is invalid)
-     */
-      }
-
-      // avoid boosting null queries, such as those caused by stop words
-      if (q != null) {
-        q.setBoost(f);
-      }
-    }
-    return q;
-  }
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java
deleted file mode 100644
index 50c55bd..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java
+++ /dev/null
@@ -1,119 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParserConstants.java */
-package org.apache.lucene.queryParser.precedence;
-
-
-/**
- * Token literal values and constants.
- * Generated by org.javacc.parser.OtherFilesGen#start()
- */
-public interface PrecedenceQueryParserConstants {
-
-  /** End of File. */
-  int EOF = 0;
-  /** RegularExpression Id. */
-  int _NUM_CHAR = 1;
-  /** RegularExpression Id. */
-  int _ESCAPED_CHAR = 2;
-  /** RegularExpression Id. */
-  int _TERM_START_CHAR = 3;
-  /** RegularExpression Id. */
-  int _TERM_CHAR = 4;
-  /** RegularExpression Id. */
-  int _WHITESPACE = 5;
-  /** RegularExpression Id. */
-  int AND = 7;
-  /** RegularExpression Id. */
-  int OR = 8;
-  /** RegularExpression Id. */
-  int NOT = 9;
-  /** RegularExpression Id. */
-  int PLUS = 10;
-  /** RegularExpression Id. */
-  int MINUS = 11;
-  /** RegularExpression Id. */
-  int LPAREN = 12;
-  /** RegularExpression Id. */
-  int RPAREN = 13;
-  /** RegularExpression Id. */
-  int COLON = 14;
-  /** RegularExpression Id. */
-  int CARAT = 15;
-  /** RegularExpression Id. */
-  int QUOTED = 16;
-  /** RegularExpression Id. */
-  int TERM = 17;
-  /** RegularExpression Id. */
-  int FUZZY_SLOP = 18;
-  /** RegularExpression Id. */
-  int PREFIXTERM = 19;
-  /** RegularExpression Id. */
-  int WILDTERM = 20;
-  /** RegularExpression Id. */
-  int RANGEIN_START = 21;
-  /** RegularExpression Id. */
-  int RANGEEX_START = 22;
-  /** RegularExpression Id. */
-  int NUMBER = 23;
-  /** RegularExpression Id. */
-  int RANGEIN_TO = 24;
-  /** RegularExpression Id. */
-  int RANGEIN_END = 25;
-  /** RegularExpression Id. */
-  int RANGEIN_QUOTED = 26;
-  /** RegularExpression Id. */
-  int RANGEIN_GOOP = 27;
-  /** RegularExpression Id. */
-  int RANGEEX_TO = 28;
-  /** RegularExpression Id. */
-  int RANGEEX_END = 29;
-  /** RegularExpression Id. */
-  int RANGEEX_QUOTED = 30;
-  /** RegularExpression Id. */
-  int RANGEEX_GOOP = 31;
-
-  /** Lexical state. */
-  int Boost = 0;
-  /** Lexical state. */
-  int RangeEx = 1;
-  /** Lexical state. */
-  int RangeIn = 2;
-  /** Lexical state. */
-  int DEFAULT = 3;
-
-  /** Literal token values. */
-  String[] tokenImage = {
-    "<EOF>",
-    "<_NUM_CHAR>",
-    "<_ESCAPED_CHAR>",
-    "<_TERM_START_CHAR>",
-    "<_TERM_CHAR>",
-    "<_WHITESPACE>",
-    "<token of kind 6>",
-    "<AND>",
-    "<OR>",
-    "<NOT>",
-    "\"+\"",
-    "\"-\"",
-    "\"(\"",
-    "\")\"",
-    "\":\"",
-    "\"^\"",
-    "<QUOTED>",
-    "<TERM>",
-    "<FUZZY_SLOP>",
-    "<PREFIXTERM>",
-    "<WILDTERM>",
-    "\"[\"",
-    "\"{\"",
-    "<NUMBER>",
-    "\"TO\"",
-    "\"]\"",
-    "<RANGEIN_QUOTED>",
-    "<RANGEIN_GOOP>",
-    "\"TO\"",
-    "\"}\"",
-    "<RANGEEX_QUOTED>",
-    "<RANGEEX_GOOP>",
-  };
-
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java
deleted file mode 100644
index 6f4878b..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java
+++ /dev/null
@@ -1,1081 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParserTokenManager.java */
-package org.apache.lucene.queryParser.precedence;
-import java.io.IOException;
-import java.io.StringReader;
-import java.text.DateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.List;
-import java.util.Locale;
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.*;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.MultiPhraseQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.AttributeSource;
-
-/** Token Manager. */
-public class PrecedenceQueryParserTokenManager implements PrecedenceQueryParserConstants
-{
-
-  /** Debug output. */
-  public  java.io.PrintStream debugStream = System.out;
-  /** Set debug output. */
-  public  void setDebugStream(java.io.PrintStream ds) { debugStream = ds; }
-private final int jjStopStringLiteralDfa_3(int pos, long active0)
-{
-   switch (pos)
-   {
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_3(int pos, long active0)
-{
-   return jjMoveNfa_3(jjStopStringLiteralDfa_3(pos, active0), pos + 1);
-}
-private int jjStopAtPos(int pos, int kind)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   return pos + 1;
-}
-private int jjMoveStringLiteralDfa0_3()
-{
-   switch(curChar)
-   {
-      case 40:
-         return jjStopAtPos(0, 12);
-      case 41:
-         return jjStopAtPos(0, 13);
-      case 43:
-         return jjStopAtPos(0, 10);
-      case 45:
-         return jjStopAtPos(0, 11);
-      case 58:
-         return jjStopAtPos(0, 14);
-      case 91:
-         return jjStopAtPos(0, 21);
-      case 94:
-         return jjStopAtPos(0, 15);
-      case 123:
-         return jjStopAtPos(0, 22);
-      default :
-         return jjMoveNfa_3(0, 0);
-   }
-}
-static final long[] jjbitVec0 = {
-   0xfffffffffffffffeL, 0xffffffffffffffffL, 0xffffffffffffffffL, 0xffffffffffffffffL
-};
-static final long[] jjbitVec2 = {
-   0x0L, 0x0L, 0xffffffffffffffffL, 0xffffffffffffffffL
-};
-private int jjMoveNfa_3(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 33;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0x7bffd0f8ffffd9ffL & l) != 0L)
-                  {
-                     if (kind > 17)
-                        kind = 17;
-                     jjCheckNAddStates(0, 6);
-                  }
-                  else if ((0x100002600L & l) != 0L)
-                  {
-                     if (kind > 6)
-                        kind = 6;
-                  }
-                  else if (curChar == 34)
-                     jjCheckNAdd(15);
-                  else if (curChar == 33)
-                  {
-                     if (kind > 9)
-                        kind = 9;
-                  }
-                  if (curChar == 38)
-                     jjstateSet[jjnewStateCnt++] = 4;
-                  break;
-               case 4:
-                  if (curChar == 38 && kind > 7)
-                     kind = 7;
-                  break;
-               case 5:
-                  if (curChar == 38)
-                     jjstateSet[jjnewStateCnt++] = 4;
-                  break;
-               case 13:
-                  if (curChar == 33 && kind > 9)
-                     kind = 9;
-                  break;
-               case 14:
-                  if (curChar == 34)
-                     jjCheckNAdd(15);
-                  break;
-               case 15:
-                  if ((0xfffffffbffffffffL & l) != 0L)
-                     jjCheckNAddTwoStates(15, 16);
-                  break;
-               case 16:
-                  if (curChar == 34 && kind > 16)
-                     kind = 16;
-                  break;
-               case 18:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 18)
-                     kind = 18;
-                  jjAddStates(7, 8);
-                  break;
-               case 19:
-                  if (curChar == 46)
-                     jjCheckNAdd(20);
-                  break;
-               case 20:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 18)
-                     kind = 18;
-                  jjCheckNAdd(20);
-                  break;
-               case 21:
-                  if ((0x7bffd0f8ffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddStates(0, 6);
-                  break;
-               case 22:
-                  if ((0x7bfff8f8ffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddTwoStates(22, 23);
-                  break;
-               case 24:
-                  if ((0x84002f0600000000L & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddTwoStates(22, 23);
-                  break;
-               case 25:
-                  if ((0x7bfff8f8ffffd9ffL & l) != 0L)
-                     jjCheckNAddStates(9, 11);
-                  break;
-               case 26:
-                  if (curChar == 42 && kind > 19)
-                     kind = 19;
-                  break;
-               case 28:
-                  if ((0x84002f0600000000L & l) != 0L)
-                     jjCheckNAddStates(9, 11);
-                  break;
-               case 29:
-                  if ((0xfbfffcf8ffffd9ffL & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(29, 30);
-                  break;
-               case 31:
-                  if ((0x84002f0600000000L & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(29, 30);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0x97ffffff97ffffffL & l) != 0L)
-                  {
-                     if (kind > 17)
-                        kind = 17;
-                     jjCheckNAddStates(0, 6);
-                  }
-                  else if (curChar == 126)
-                  {
-                     if (kind > 18)
-                        kind = 18;
-                     jjstateSet[jjnewStateCnt++] = 18;
-                  }
-                  if (curChar == 92)
-                     jjCheckNAddStates(12, 14);
-                  else if (curChar == 78)
-                     jjstateSet[jjnewStateCnt++] = 11;
-                  else if (curChar == 124)
-                     jjstateSet[jjnewStateCnt++] = 8;
-                  else if (curChar == 79)
-                     jjstateSet[jjnewStateCnt++] = 6;
-                  else if (curChar == 65)
-                     jjstateSet[jjnewStateCnt++] = 2;
-                  break;
-               case 1:
-                  if (curChar == 68 && kind > 7)
-                     kind = 7;
-                  break;
-               case 2:
-                  if (curChar == 78)
-                     jjstateSet[jjnewStateCnt++] = 1;
-                  break;
-               case 3:
-                  if (curChar == 65)
-                     jjstateSet[jjnewStateCnt++] = 2;
-                  break;
-               case 6:
-                  if (curChar == 82 && kind > 8)
-                     kind = 8;
-                  break;
-               case 7:
-                  if (curChar == 79)
-                     jjstateSet[jjnewStateCnt++] = 6;
-                  break;
-               case 8:
-                  if (curChar == 124 && kind > 8)
-                     kind = 8;
-                  break;
-               case 9:
-                  if (curChar == 124)
-                     jjstateSet[jjnewStateCnt++] = 8;
-                  break;
-               case 10:
-                  if (curChar == 84 && kind > 9)
-                     kind = 9;
-                  break;
-               case 11:
-                  if (curChar == 79)
-                     jjstateSet[jjnewStateCnt++] = 10;
-                  break;
-               case 12:
-                  if (curChar == 78)
-                     jjstateSet[jjnewStateCnt++] = 11;
-                  break;
-               case 15:
-                  jjAddStates(15, 16);
-                  break;
-               case 17:
-                  if (curChar != 126)
-                     break;
-                  if (kind > 18)
-                     kind = 18;
-                  jjstateSet[jjnewStateCnt++] = 18;
-                  break;
-               case 21:
-                  if ((0x97ffffff97ffffffL & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddStates(0, 6);
-                  break;
-               case 22:
-                  if ((0x97ffffff97ffffffL & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddTwoStates(22, 23);
-                  break;
-               case 23:
-                  if (curChar == 92)
-                     jjCheckNAddTwoStates(24, 24);
-                  break;
-               case 24:
-                  if ((0x6800000078000000L & l) == 0L)
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddTwoStates(22, 23);
-                  break;
-               case 25:
-                  if ((0x97ffffff97ffffffL & l) != 0L)
-                     jjCheckNAddStates(9, 11);
-                  break;
-               case 27:
-                  if (curChar == 92)
-                     jjCheckNAddTwoStates(28, 28);
-                  break;
-               case 28:
-                  if ((0x6800000078000000L & l) != 0L)
-                     jjCheckNAddStates(9, 11);
-                  break;
-               case 29:
-                  if ((0x97ffffff97ffffffL & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(29, 30);
-                  break;
-               case 30:
-                  if (curChar == 92)
-                     jjCheckNAddTwoStates(31, 31);
-                  break;
-               case 31:
-                  if ((0x6800000078000000L & l) == 0L)
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(29, 30);
-                  break;
-               case 32:
-                  if (curChar == 92)
-                     jjCheckNAddStates(12, 14);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int hiByte = (int)(curChar >> 8);
-         int i1 = hiByte >> 6;
-         long l1 = 1L << (hiByte & 077);
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddStates(0, 6);
-                  break;
-               case 15:
-                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     jjAddStates(15, 16);
-                  break;
-               case 22:
-                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 17)
-                     kind = 17;
-                  jjCheckNAddTwoStates(22, 23);
-                  break;
-               case 25:
-                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     jjCheckNAddStates(9, 11);
-                  break;
-               case 29:
-                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 20)
-                     kind = 20;
-                  jjCheckNAddTwoStates(29, 30);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 33 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private final int jjStopStringLiteralDfa_1(int pos, long active0)
-{
-   switch (pos)
-   {
-      case 0:
-         if ((active0 & 0x10000000L) != 0L)
-         {
-            jjmatchedKind = 31;
-            return 4;
-         }
-         return -1;
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_1(int pos, long active0)
-{
-   return jjMoveNfa_1(jjStopStringLiteralDfa_1(pos, active0), pos + 1);
-}
-private int jjMoveStringLiteralDfa0_1()
-{
-   switch(curChar)
-   {
-      case 84:
-         return jjMoveStringLiteralDfa1_1(0x10000000L);
-      case 125:
-         return jjStopAtPos(0, 29);
-      default :
-         return jjMoveNfa_1(0, 0);
-   }
-}
-private int jjMoveStringLiteralDfa1_1(long active0)
-{
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_1(0, active0);
-      return 1;
-   }
-   switch(curChar)
-   {
-      case 79:
-         if ((active0 & 0x10000000L) != 0L)
-            return jjStartNfaWithStates_1(1, 28, 4);
-         break;
-      default :
-         break;
-   }
-   return jjStartNfa_1(0, active0);
-}
-private int jjStartNfaWithStates_1(int pos, int kind, int state)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) { return pos + 1; }
-   return jjMoveNfa_1(state, pos + 1);
-}
-private int jjMoveNfa_1(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 5;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0xfffffffeffffffffL & l) != 0L)
-                  {
-                     if (kind > 31)
-                        kind = 31;
-                     jjCheckNAdd(4);
-                  }
-                  if ((0x100002600L & l) != 0L)
-                  {
-                     if (kind > 6)
-                        kind = 6;
-                  }
-                  else if (curChar == 34)
-                     jjCheckNAdd(2);
-                  break;
-               case 1:
-                  if (curChar == 34)
-                     jjCheckNAdd(2);
-                  break;
-               case 2:
-                  if ((0xfffffffbffffffffL & l) != 0L)
-                     jjCheckNAddTwoStates(2, 3);
-                  break;
-               case 3:
-                  if (curChar == 34 && kind > 30)
-                     kind = 30;
-                  break;
-               case 4:
-                  if ((0xfffffffeffffffffL & l) == 0L)
-                     break;
-                  if (kind > 31)
-                     kind = 31;
-                  jjCheckNAdd(4);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 4:
-                  if ((0xdfffffffffffffffL & l) == 0L)
-                     break;
-                  if (kind > 31)
-                     kind = 31;
-                  jjCheckNAdd(4);
-                  break;
-               case 2:
-                  jjAddStates(17, 18);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int hiByte = (int)(curChar >> 8);
-         int i1 = hiByte >> 6;
-         long l1 = 1L << (hiByte & 077);
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 4:
-                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 31)
-                     kind = 31;
-                  jjCheckNAdd(4);
-                  break;
-               case 2:
-                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     jjAddStates(17, 18);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 5 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private int jjMoveStringLiteralDfa0_0()
-{
-   return jjMoveNfa_0(0, 0);
-}
-private int jjMoveNfa_0(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 3;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjAddStates(19, 20);
-                  break;
-               case 1:
-                  if (curChar == 46)
-                     jjCheckNAdd(2);
-                  break;
-               case 2:
-                  if ((0x3ff000000000000L & l) == 0L)
-                     break;
-                  if (kind > 23)
-                     kind = 23;
-                  jjCheckNAdd(2);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int hiByte = (int)(curChar >> 8);
-         int i1 = hiByte >> 6;
-         long l1 = 1L << (hiByte & 077);
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 3 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-private final int jjStopStringLiteralDfa_2(int pos, long active0)
-{
-   switch (pos)
-   {
-      case 0:
-         if ((active0 & 0x1000000L) != 0L)
-         {
-            jjmatchedKind = 27;
-            return 4;
-         }
-         return -1;
-      default :
-         return -1;
-   }
-}
-private final int jjStartNfa_2(int pos, long active0)
-{
-   return jjMoveNfa_2(jjStopStringLiteralDfa_2(pos, active0), pos + 1);
-}
-private int jjMoveStringLiteralDfa0_2()
-{
-   switch(curChar)
-   {
-      case 84:
-         return jjMoveStringLiteralDfa1_2(0x1000000L);
-      case 93:
-         return jjStopAtPos(0, 25);
-      default :
-         return jjMoveNfa_2(0, 0);
-   }
-}
-private int jjMoveStringLiteralDfa1_2(long active0)
-{
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) {
-      jjStopStringLiteralDfa_2(0, active0);
-      return 1;
-   }
-   switch(curChar)
-   {
-      case 79:
-         if ((active0 & 0x1000000L) != 0L)
-            return jjStartNfaWithStates_2(1, 24, 4);
-         break;
-      default :
-         break;
-   }
-   return jjStartNfa_2(0, active0);
-}
-private int jjStartNfaWithStates_2(int pos, int kind, int state)
-{
-   jjmatchedKind = kind;
-   jjmatchedPos = pos;
-   try { curChar = input_stream.readChar(); }
-   catch(java.io.IOException e) { return pos + 1; }
-   return jjMoveNfa_2(state, pos + 1);
-}
-private int jjMoveNfa_2(int startState, int curPos)
-{
-   int startsAt = 0;
-   jjnewStateCnt = 5;
-   int i = 1;
-   jjstateSet[0] = startState;
-   int kind = 0x7fffffff;
-   for (;;)
-   {
-      if (++jjround == 0x7fffffff)
-         ReInitRounds();
-      if (curChar < 64)
-      {
-         long l = 1L << curChar;
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-                  if ((0xfffffffeffffffffL & l) != 0L)
-                  {
-                     if (kind > 27)
-                        kind = 27;
-                     jjCheckNAdd(4);
-                  }
-                  if ((0x100002600L & l) != 0L)
-                  {
-                     if (kind > 6)
-                        kind = 6;
-                  }
-                  else if (curChar == 34)
-                     jjCheckNAdd(2);
-                  break;
-               case 1:
-                  if (curChar == 34)
-                     jjCheckNAdd(2);
-                  break;
-               case 2:
-                  if ((0xfffffffbffffffffL & l) != 0L)
-                     jjCheckNAddTwoStates(2, 3);
-                  break;
-               case 3:
-                  if (curChar == 34 && kind > 26)
-                     kind = 26;
-                  break;
-               case 4:
-                  if ((0xfffffffeffffffffL & l) == 0L)
-                     break;
-                  if (kind > 27)
-                     kind = 27;
-                  jjCheckNAdd(4);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else if (curChar < 128)
-      {
-         long l = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 4:
-                  if ((0xffffffffdfffffffL & l) == 0L)
-                     break;
-                  if (kind > 27)
-                     kind = 27;
-                  jjCheckNAdd(4);
-                  break;
-               case 2:
-                  jjAddStates(17, 18);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      else
-      {
-         int hiByte = (int)(curChar >> 8);
-         int i1 = hiByte >> 6;
-         long l1 = 1L << (hiByte & 077);
-         int i2 = (curChar & 0xff) >> 6;
-         long l2 = 1L << (curChar & 077);
-         do
-         {
-            switch(jjstateSet[--i])
-            {
-               case 0:
-               case 4:
-                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     break;
-                  if (kind > 27)
-                     kind = 27;
-                  jjCheckNAdd(4);
-                  break;
-               case 2:
-                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
-                     jjAddStates(17, 18);
-                  break;
-               default : break;
-            }
-         } while(i != startsAt);
-      }
-      if (kind != 0x7fffffff)
-      {
-         jjmatchedKind = kind;
-         jjmatchedPos = curPos;
-         kind = 0x7fffffff;
-      }
-      ++curPos;
-      if ((i = jjnewStateCnt) == (startsAt = 5 - (jjnewStateCnt = startsAt)))
-         return curPos;
-      try { curChar = input_stream.readChar(); }
-      catch(java.io.IOException e) { return curPos; }
-   }
-}
-static final int[] jjnextStates = {
-   22, 25, 26, 29, 30, 27, 23, 18, 19, 25, 26, 27, 24, 28, 31, 15, 
-   16, 2, 3, 0, 1, 
-};
-private static final boolean jjCanMove_0(int hiByte, int i1, int i2, long l1, long l2)
-{
-   switch(hiByte)
-   {
-      case 0:
-         return ((jjbitVec2[i2] & l2) != 0L);
-      default :
-         if ((jjbitVec0[i1] & l1) != 0L)
-            return true;
-         return false;
-   }
-}
-
-/** Token literal values. */
-public static final String[] jjstrLiteralImages = {
-"", null, null, null, null, null, null, null, null, null, "\53", "\55", "\50", 
-"\51", "\72", "\136", null, null, null, null, null, "\133", "\173", null, "\124\117", 
-"\135", null, null, "\124\117", "\175", null, null, };
-
-/** Lexer state names. */
-public static final String[] lexStateNames = {
-   "Boost",
-   "RangeEx",
-   "RangeIn",
-   "DEFAULT",
-};
-
-/** Lex State array. */
-public static final int[] jjnewLexState = {
-   -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 2, 1, 3, -1, 
-   3, -1, -1, -1, 3, -1, -1, 
-};
-static final long[] jjtoToken = {
-   0xffffff81L, 
-};
-static final long[] jjtoSkip = {
-   0x40L, 
-};
-protected CharStream input_stream;
-private final int[] jjrounds = new int[33];
-private final int[] jjstateSet = new int[66];
-protected char curChar;
-/** Constructor. */
-public PrecedenceQueryParserTokenManager(CharStream stream){
-   input_stream = stream;
-}
-
-/** Constructor. */
-public PrecedenceQueryParserTokenManager(CharStream stream, int lexState){
-   this(stream);
-   SwitchTo(lexState);
-}
-
-/** Reinitialise parser. */
-public void ReInit(CharStream stream)
-{
-   jjmatchedPos = jjnewStateCnt = 0;
-   curLexState = defaultLexState;
-   input_stream = stream;
-   ReInitRounds();
-}
-private void ReInitRounds()
-{
-   int i;
-   jjround = 0x80000001;
-   for (i = 33; i-- > 0;)
-      jjrounds[i] = 0x80000000;
-}
-
-/** Reinitialise parser. */
-public void ReInit(CharStream stream, int lexState)
-{
-   ReInit(stream);
-   SwitchTo(lexState);
-}
-
-/** Switch to specified lex state. */
-public void SwitchTo(int lexState)
-{
-   if (lexState >= 4 || lexState < 0)
-      throw new TokenMgrError("Error: Ignoring invalid lexical state : " + lexState + ". State unchanged.", TokenMgrError.INVALID_LEXICAL_STATE);
-   else
-      curLexState = lexState;
-}
-
-protected Token jjFillToken()
-{
-   final Token t;
-   final String curTokenImage;
-   final int beginLine;
-   final int endLine;
-   final int beginColumn;
-   final int endColumn;
-   String im = jjstrLiteralImages[jjmatchedKind];
-   curTokenImage = (im == null) ? input_stream.GetImage() : im;
-   beginLine = input_stream.getBeginLine();
-   beginColumn = input_stream.getBeginColumn();
-   endLine = input_stream.getEndLine();
-   endColumn = input_stream.getEndColumn();
-   t = Token.newToken(jjmatchedKind, curTokenImage);
-
-   t.beginLine = beginLine;
-   t.endLine = endLine;
-   t.beginColumn = beginColumn;
-   t.endColumn = endColumn;
-
-   return t;
-}
-
-int curLexState = 3;
-int defaultLexState = 3;
-int jjnewStateCnt;
-int jjround;
-int jjmatchedPos;
-int jjmatchedKind;
-
-/** Get the next Token. */
-public Token getNextToken() 
-{
-  Token matchedToken;
-  int curPos = 0;
-
-  EOFLoop :
-  for (;;)
-  {
-   try
-   {
-      curChar = input_stream.BeginToken();
-   }
-   catch(java.io.IOException e)
-   {
-      jjmatchedKind = 0;
-      matchedToken = jjFillToken();
-      return matchedToken;
-   }
-
-   switch(curLexState)
-   {
-     case 0:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_0();
-       break;
-     case 1:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_1();
-       break;
-     case 2:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_2();
-       break;
-     case 3:
-       jjmatchedKind = 0x7fffffff;
-       jjmatchedPos = 0;
-       curPos = jjMoveStringLiteralDfa0_3();
-       break;
-   }
-     if (jjmatchedKind != 0x7fffffff)
-     {
-        if (jjmatchedPos + 1 < curPos)
-           input_stream.backup(curPos - jjmatchedPos - 1);
-        if ((jjtoToken[jjmatchedKind >> 6] & (1L << (jjmatchedKind & 077))) != 0L)
-        {
-           matchedToken = jjFillToken();
-       if (jjnewLexState[jjmatchedKind] != -1)
-         curLexState = jjnewLexState[jjmatchedKind];
-           return matchedToken;
-        }
-        else
-        {
-         if (jjnewLexState[jjmatchedKind] != -1)
-           curLexState = jjnewLexState[jjmatchedKind];
-           continue EOFLoop;
-        }
-     }
-     int error_line = input_stream.getEndLine();
-     int error_column = input_stream.getEndColumn();
-     String error_after = null;
-     boolean EOFSeen = false;
-     try { input_stream.readChar(); input_stream.backup(1); }
-     catch (java.io.IOException e1) {
-        EOFSeen = true;
-        error_after = curPos <= 1 ? "" : input_stream.GetImage();
-        if (curChar == '\n' || curChar == '\r') {
-           error_line++;
-           error_column = 0;
-        }
-        else
-           error_column++;
-     }
-     if (!EOFSeen) {
-        input_stream.backup(1);
-        error_after = curPos <= 1 ? "" : input_stream.GetImage();
-     }
-     throw new TokenMgrError(EOFSeen, curLexState, error_line, error_column, error_after, curChar, TokenMgrError.LEXICAL_ERROR);
-  }
-}
-
-private void jjCheckNAdd(int state)
-{
-   if (jjrounds[state] != jjround)
-   {
-      jjstateSet[jjnewStateCnt++] = state;
-      jjrounds[state] = jjround;
-   }
-}
-private void jjAddStates(int start, int end)
-{
-   do {
-      jjstateSet[jjnewStateCnt++] = jjnextStates[start];
-   } while (start++ != end);
-}
-private void jjCheckNAddTwoStates(int state1, int state2)
-{
-   jjCheckNAdd(state1);
-   jjCheckNAdd(state2);
-}
-
-private void jjCheckNAddStates(int start, int end)
-{
-   do {
-      jjCheckNAdd(jjnextStates[start]);
-   } while (start++ != end);
-}
-
-}
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/Token.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/Token.java
deleted file mode 100644
index 8402b3d..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/Token.java
+++ /dev/null
@@ -1,124 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. Token.java Version 4.1 */
-/* JavaCCOptions:TOKEN_EXTENDS=,KEEP_LINE_COL=null */
-package org.apache.lucene.queryParser.precedence;
-
-/**
- * Describes the input token stream.
- */
-
-public class Token {
-
-  /**
-   * An integer that describes the kind of this token.  This numbering
-   * system is determined by JavaCCParser, and a table of these numbers is
-   * stored in the file ...Constants.java.
-   */
-  public int kind;
-
-  /** The line number of the first character of this Token. */
-  public int beginLine;
-  /** The column number of the first character of this Token. */
-  public int beginColumn;
-  /** The line number of the last character of this Token. */
-  public int endLine;
-  /** The column number of the last character of this Token. */
-  public int endColumn;
-
-  /**
-   * The string image of the token.
-   */
-  public String image;
-
-  /**
-   * A reference to the next regular (non-special) token from the input
-   * stream.  If this is the last token from the input stream, or if the
-   * token manager has not read tokens beyond this one, this field is
-   * set to null.  This is true only if this token is also a regular
-   * token.  Otherwise, see below for a description of the contents of
-   * this field.
-   */
-  public Token next;
-
-  /**
-   * This field is used to access special tokens that occur prior to this
-   * token, but after the immediately preceding regular (non-special) token.
-   * If there are no such special tokens, this field is set to null.
-   * When there are more than one such special token, this field refers
-   * to the last of these special tokens, which in turn refers to the next
-   * previous special token through its specialToken field, and so on
-   * until the first special token (whose specialToken field is null).
-   * The next fields of special tokens refer to other special tokens that
-   * immediately follow it (without an intervening regular token).  If there
-   * is no such token, this field is null.
-   */
-  public Token specialToken;
-
-  /**
-   * An optional attribute value of the Token.
-   * Tokens which are not used as syntactic sugar will often contain
-   * meaningful values that will be used later on by the compiler or
-   * interpreter. This attribute value is often different from the image.
-   * Any subclass of Token that actually wants to return a non-null value can
-   * override this method as appropriate.
-   */
-  public Object getValue() {
-    return null;
-  }
-
-  /**
-   * No-argument constructor
-   */
-  public Token() {}
-
-  /**
-   * Constructs a new token for the specified Image.
-   */
-  public Token(int kind)
-  {
-     this(kind, null);
-  }
-
-  /**
-   * Constructs a new token for the specified Image and Kind.
-   */
-  public Token(int kind, String image)
-  {
-     this.kind = kind;
-     this.image = image;
-  }
-
-  /**
-   * Returns the image.
-   */
-  public String toString()
-  {
-     return image;
-  }
-
-  /**
-   * Returns a new Token object, by default. However, if you want, you
-   * can create and return subclass objects based on the value of ofKind.
-   * Simply add the cases to the switch for all those special cases.
-   * For example, if you have a subclass of Token called IDToken that
-   * you want to create if ofKind is ID, simply add something like :
-   *
-   *    case MyParserConstants.ID : return new IDToken(ofKind, image);
-   *
-   * to the following switch statement. Then you can cast matchedToken
-   * variable to the appropriate type and use sit in your lexical actions.
-   */
-  public static Token newToken(int ofKind, String image)
-  {
-     switch(ofKind)
-     {
-       default : return new Token(ofKind, image);
-     }
-  }
-
-  public static Token newToken(int ofKind)
-  {
-     return newToken(ofKind, null);
-  }
-
-}
-/* JavaCC - OriginalChecksum=0dc5808f2ab8aac8775ea9175fa2cb51 (do not edit this line) */
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java
deleted file mode 100644
index 01e8751..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/* Generated By:JavaCC: Do not edit this line. TokenMgrError.java Version 4.1 */
-/* JavaCCOptions: */
-package org.apache.lucene.queryParser.precedence;
-
-/** Token Manager Error. */
-@SuppressWarnings("serial")
-public class TokenMgrError extends Error
-{
-
-   /*
-    * Ordinals for various reasons why an Error of this type can be thrown.
-    */
-
-   /**
-    * Lexical error occurred.
-    */
-   static final int LEXICAL_ERROR = 0;
-
-   /**
-    * An attempt was made to create a second instance of a static token manager.
-    */
-   static final int STATIC_LEXER_ERROR = 1;
-
-   /**
-    * Tried to change to an invalid lexical state.
-    */
-   static final int INVALID_LEXICAL_STATE = 2;
-
-   /**
-    * Detected (and bailed out of) an infinite loop in the token manager.
-    */
-   static final int LOOP_DETECTED = 3;
-
-   /**
-    * Indicates the reason why the exception is thrown. It will have
-    * one of the above 4 values.
-    */
-   int errorCode;
-
-   /**
-    * Replaces unprintable characters by their escaped (or unicode escaped)
-    * equivalents in the given string
-    */
-   protected static final String addEscapes(String str) {
-      StringBuffer retval = new StringBuffer();
-      char ch;
-      for (int i = 0; i < str.length(); i++) {
-        switch (str.charAt(i))
-        {
-           case 0 :
-              continue;
-           case '\b':
-              retval.append("\\b");
-              continue;
-           case '\t':
-              retval.append("\\t");
-              continue;
-           case '\n':
-              retval.append("\\n");
-              continue;
-           case '\f':
-              retval.append("\\f");
-              continue;
-           case '\r':
-              retval.append("\\r");
-              continue;
-           case '\"':
-              retval.append("\\\"");
-              continue;
-           case '\'':
-              retval.append("\\\'");
-              continue;
-           case '\\':
-              retval.append("\\\\");
-              continue;
-           default:
-              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
-                 String s = "0000" + Integer.toString(ch, 16);
-                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
-              } else {
-                 retval.append(ch);
-              }
-              continue;
-        }
-      }
-      return retval.toString();
-   }
-
-   /**
-    * Returns a detailed message for the Error when it is thrown by the
-    * token manager to indicate a lexical error.
-    * Parameters :
-    *    EOFSeen     : indicates if EOF caused the lexical error
-    *    curLexState : lexical state in which this error occurred
-    *    errorLine   : line number when the error occurred
-    *    errorColumn : column number when the error occurred
-    *    errorAfter  : prefix that was seen before this error occurred
-    *    curchar     : the offending character
-    * Note: You can customize the lexical error message by modifying this method.
-    */
-   protected static String LexicalError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar) {
-      return("Lexical error at line " +
-           errorLine + ", column " +
-           errorColumn + ".  Encountered: " +
-           (EOFSeen ? "<EOF> " : ("\"" + addEscapes(String.valueOf(curChar)) + "\"") + " (" + (int)curChar + "), ") +
-           "after : \"" + addEscapes(errorAfter) + "\"");
-   }
-
-   /**
-    * You can also modify the body of this method to customize your error messages.
-    * For example, cases like LOOP_DETECTED and INVALID_LEXICAL_STATE are not
-    * of end-users concern, so you can return something like :
-    *
-    *     "Internal Error : Please file a bug report .... "
-    *
-    * from this method for such cases in the release version of your parser.
-    */
-   public String getMessage() {
-      return super.getMessage();
-   }
-
-   /*
-    * Constructors of various flavors follow.
-    */
-
-   /** No arg constructor. */
-   public TokenMgrError() {
-   }
-
-   /** Constructor with message and reason. */
-   public TokenMgrError(String message, int reason) {
-      super(message);
-      errorCode = reason;
-   }
-
-   /** Full Constructor. */
-   public TokenMgrError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar, int reason) {
-      this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
-   }
-}
-/* JavaCC - OriginalChecksum=257b82f2650841e86289a309cb3dae76 (do not edit this line) */
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/package.html b/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/package.html
deleted file mode 100644
index fdc3a30..0000000
--- a/lucene/contrib/misc/src/java/org/apache/lucene/queryParser/precedence/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-QueryParser designed to handle operator precedence in a more sensible fashion than the default QueryParser.
-</body>
-</html>
diff --git a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java b/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java
deleted file mode 100644
index 8973e15..0000000
--- a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java
+++ /dev/null
@@ -1,120 +0,0 @@
-package org.apache.lucene.queryParser.analyzing;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Reader;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.ASCIIFoldingFilter;
-import org.apache.lucene.analysis.LowerCaseFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.standard.StandardFilter;
-import org.apache.lucene.analysis.standard.StandardTokenizer;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * @version $Revision$, $Date$
- */
-public class TestAnalyzingQueryParser extends LuceneTestCase {
-
-  private Analyzer a;
-
-  private String[] wildcardInput;
-  private String[] wildcardExpected;
-  private String[] prefixInput;
-  private String[] prefixExpected;
-  private String[] rangeInput;
-  private String[] rangeExpected;
-  private String[] fuzzyInput;
-  private String[] fuzzyExpected;
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    wildcardInput = new String[] { "übersetzung über*ung",
-        "Mötley Cr\u00fce Mötl?* Crü?", "Renée Zellweger Ren?? Zellw?ger" };
-    wildcardExpected = new String[] { "ubersetzung uber*ung", "motley crue motl?* cru?",
-        "renee zellweger ren?? zellw?ger" };
-
-    prefixInput = new String[] { "übersetzung übersetz*",
-        "Mötley Crüe Mötl* crü*", "René? Zellw*" };
-    prefixExpected = new String[] { "ubersetzung ubersetz*", "motley crue motl* cru*",
-        "rene? zellw*" };
-
-    rangeInput = new String[] { "[aa TO bb]", "{Anaïs TO Zoé}" };
-    rangeExpected = new String[] { "[aa TO bb]", "{anais TO zoe}" };
-
-    fuzzyInput = new String[] { "?bersetzung ?bersetzung~0.9",
-        "Mötley Crüe Mötley~0.75 Crüe~0.5",
-        "Renée Zellweger Renée~0.9 Zellweger~" };
-    fuzzyExpected = new String[] { "ubersetzung ubersetzung~0.9",
-        "motley crue motley~0.75 crue~0.5", "renee zellweger renee~0.9 zellweger~0.5" };
-
-    a = new ASCIIAnalyzer();
-  }
-
-  public void testWildCardQuery() throws ParseException {
-    for (int i = 0; i < wildcardInput.length; i++) {
-      assertEquals("Testing wildcards with analyzer " + a.getClass() + ", input string: "
-          + wildcardInput[i], wildcardExpected[i], parseWithAnalyzingQueryParser(wildcardInput[i], a));
-    }
-  }
-
-  public void testPrefixQuery() throws ParseException {
-    for (int i = 0; i < prefixInput.length; i++) {
-      assertEquals("Testing prefixes with analyzer " + a.getClass() + ", input string: "
-          + prefixInput[i], prefixExpected[i], parseWithAnalyzingQueryParser(prefixInput[i], a));
-    }
-  }
-
-  public void testRangeQuery() throws ParseException {
-    for (int i = 0; i < rangeInput.length; i++) {
-      assertEquals("Testing ranges with analyzer " + a.getClass() + ", input string: "
-          + rangeInput[i], rangeExpected[i], parseWithAnalyzingQueryParser(rangeInput[i], a));
-    }
-  }
-
-  public void testFuzzyQuery() throws ParseException {
-    for (int i = 0; i < fuzzyInput.length; i++) {
-      assertEquals("Testing fuzzys with analyzer " + a.getClass() + ", input string: "
-          + fuzzyInput[i], fuzzyExpected[i], parseWithAnalyzingQueryParser(fuzzyInput[i], a));
-    }
-  }
-
-  private String parseWithAnalyzingQueryParser(String s, Analyzer a) throws ParseException {
-    AnalyzingQueryParser qp = new AnalyzingQueryParser(TEST_VERSION_CURRENT, "field", a);
-    org.apache.lucene.search.Query q = qp.parse(s);
-    return q.toString("field");
-  }
-
-}
-
-class ASCIIAnalyzer extends org.apache.lucene.analysis.Analyzer {
-  public ASCIIAnalyzer() {
-  }
-
-  @Override
-  public TokenStream tokenStream(String fieldName, Reader reader) {
-    TokenStream result = new StandardTokenizer(LuceneTestCase.TEST_VERSION_CURRENT, reader);
-    result = new StandardFilter(result);
-    result = new ASCIIFoldingFilter(result);
-    result = new LowerCaseFilter(LuceneTestCase.TEST_VERSION_CURRENT, result);
-    return result;
-  }
-}
diff --git a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java b/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
deleted file mode 100644
index b2dc6ac..0000000
--- a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
+++ /dev/null
@@ -1,148 +0,0 @@
-package org.apache.lucene.queryParser.complexPhrase;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.HashSet;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.LuceneTestCase;
-
-public class TestComplexPhraseQuery extends LuceneTestCase {
-
-  Analyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
-
-  DocData docsContent[] = { new DocData("john smith", "1"),
-      new DocData("johathon smith", "2"),
-      new DocData("john percival smith", "3"),
-      new DocData("jackson waits tom", "4") };
-
-  private IndexSearcher searcher;
-
-  String defaultFieldName = "name";
-
-  public void testComplexPhrases() throws Exception {
-    checkMatches("\"john smith\"", "1"); // Simple multi-term still works
-    checkMatches("\"j*   smyth~\"", "1,2"); // wildcards and fuzzies are OK in
-    // phrases
-    checkMatches("\"(jo* -john)  smith\"", "2"); // boolean logic works
-    checkMatches("\"jo*  smith\"~2", "1,2,3"); // position logic works.
-    checkMatches("\"jo* [sma TO smZ]\" ", "1,2"); // range queries supported
-    checkMatches("\"john\"", "1,3"); // Simple single-term still works
-    checkMatches("\"(john OR johathon)  smith\"", "1,2"); // boolean logic with
-    // brackets works.
-    checkMatches("\"(jo* -john) smyth~\"", "2"); // boolean logic with
-    // brackets works.
-
-    // checkMatches("\"john -percival\"", "1"); // not logic doesn't work
-    // currently :(.
-
-    checkMatches("\"john  nosuchword*\"", ""); // phrases with clauses producing
-    // empty sets
-
-    checkBadQuery("\"jo*  id:1 smith\""); // mixing fields in a phrase is bad
-    checkBadQuery("\"jo* \"smith\" \""); // phrases inside phrases is bad
-  }
-
-  private void checkBadQuery(String qString) {
-    QueryParser qp = new ComplexPhraseQueryParser(TEST_VERSION_CURRENT, defaultFieldName, analyzer);
-    Throwable expected = null;
-    try {
-      qp.parse(qString);
-    } catch (Throwable e) {
-      expected = e;
-    }
-    assertNotNull("Expected parse error in " + qString, expected);
-
-  }
-
-  private void checkMatches(String qString, String expectedVals)
-      throws Exception {
-    QueryParser qp = new ComplexPhraseQueryParser(TEST_VERSION_CURRENT, defaultFieldName, analyzer);
-    qp.setFuzzyPrefixLength(1); // usually a good idea
-
-    Query q = qp.parse(qString);
-
-    HashSet<String> expecteds = new HashSet<String>();
-    String[] vals = expectedVals.split(",");
-    for (int i = 0; i < vals.length; i++) {
-      if (vals[i].length() > 0)
-        expecteds.add(vals[i]);
-    }
-
-    TopDocs td = searcher.search(q, 10);
-    ScoreDoc[] sd = td.scoreDocs;
-    for (int i = 0; i < sd.length; i++) {
-      Document doc = searcher.doc(sd[i].doc);
-      String id = doc.get("id");
-      assertTrue(qString + "matched doc#" + id + " not expected", expecteds
-          .contains(id));
-      expecteds.remove(id);
-    }
-
-    assertEquals(qString + " missing some matches ", 0, expecteds.size());
-
-  }
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    RAMDirectory rd = new RAMDirectory();
-    IndexWriter w = new IndexWriter(rd, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
-    for (int i = 0; i < docsContent.length; i++) {
-      Document doc = new Document();
-      doc.add(new Field("name", docsContent[i].name, Field.Store.YES,
-          Field.Index.ANALYZED));
-      doc.add(new Field("id", docsContent[i].id, Field.Store.YES,
-          Field.Index.ANALYZED));
-      w.addDocument(doc);
-    }
-    w.close();
-    searcher = new IndexSearcher(rd, true);
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    searcher.close();
-    super.tearDown();
-  }
-
-  static class DocData {
-    String name;
-
-    String id;
-
-    public DocData(String name, String id) {
-      super();
-      this.name = name;
-      this.id = id;
-    }
-  }
-
-}
diff --git a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/ext/ExtensionStub.java b/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/ext/ExtensionStub.java
deleted file mode 100644
index 63ce2b3..0000000
--- a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/ext/ExtensionStub.java
+++ /dev/null
@@ -1,33 +0,0 @@
-package org.apache.lucene.queryParser.ext;
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-class ExtensionStub extends ParserExtension {
-
-  @Override
-  public Query parse(ExtensionQuery components) throws ParseException {
-    return new TermQuery(new Term(components.getField(), components
-        .getRawQueryString()));
-  }
-
-}
\ No newline at end of file
diff --git a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java b/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java
deleted file mode 100644
index e465e64..0000000
--- a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java
+++ /dev/null
@@ -1,136 +0,0 @@
-package org.apache.lucene.queryParser.ext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.queryParser.ParseException;
-import org.apache.lucene.queryParser.QueryParser;
-import org.apache.lucene.queryParser.TestQueryParser;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-
-/**
- * Testcase for the class {@link ExtendableQueryParser}
- */
-public class TestExtendableQueryParser extends TestQueryParser {
-  private static char[] DELIMITERS = new char[] {
-      Extensions.DEFAULT_EXTENSION_FIELD_DELIMITER, '-', '|' };
-
-  public TestExtendableQueryParser(String name) {
-    super(name);
-  }
-
-  @Override
-  public QueryParser getParser(Analyzer a) throws Exception {
-    return getParser(a, null);
-  }
-
-  public QueryParser getParser(Analyzer a, Extensions extensions)
-      throws Exception {
-    if (a == null)
-      a = new SimpleAnalyzer(TEST_VERSION_CURRENT);
-    QueryParser qp = extensions == null ? new ExtendableQueryParser(
-        TEST_VERSION_CURRENT, "field", a) : new ExtendableQueryParser(
-        TEST_VERSION_CURRENT, "field", a, extensions);
-    qp.setDefaultOperator(QueryParser.OR_OPERATOR);
-    return qp;
-  }
-
-  public void testUnescapedExtDelimiter() throws Exception {
-    Extensions ext = newExtensions(':');
-    ext.add("testExt", new ExtensionStub());
-    ExtendableQueryParser parser = (ExtendableQueryParser) getParser(null, ext);
-    try {
-      parser.parse("aField:testExt:\"foo \\& bar\"");
-      fail("extension field delimiter is not escaped");
-    } catch (ParseException e) {
-    }
-  }
-
-  public void testExtFieldUnqoted() throws Exception {
-    for (int i = 0; i < DELIMITERS.length; i++) {
-      Extensions ext = newExtensions(DELIMITERS[i]);
-      ext.add("testExt", new ExtensionStub());
-      ExtendableQueryParser parser = (ExtendableQueryParser) getParser(null,
-          ext);
-      String field = ext.buildExtensionField("testExt", "aField");
-      Query query = parser.parse(String.format("%s:foo bar", field));
-      assertTrue("expected instance of BooleanQuery but was "
-          + query.getClass(), query instanceof BooleanQuery);
-      BooleanQuery bquery = (BooleanQuery) query;
-      BooleanClause[] clauses = bquery.getClauses();
-      assertEquals(2, clauses.length);
-      BooleanClause booleanClause = clauses[0];
-      query = booleanClause.getQuery();
-      assertTrue("expected instance of TermQuery but was " + query.getClass(),
-          query instanceof TermQuery);
-      TermQuery tquery = (TermQuery) query;
-      assertEquals("aField", tquery.getTerm()
-          .field());
-      assertEquals("foo", tquery.getTerm().text());
-
-      booleanClause = clauses[1];
-      query = booleanClause.getQuery();
-      assertTrue("expected instance of TermQuery but was " + query.getClass(),
-          query instanceof TermQuery);
-      tquery = (TermQuery) query;
-      assertEquals("field", tquery.getTerm().field());
-      assertEquals("bar", tquery.getTerm().text());
-    }
-  }
-
-  public void testExtDefaultField() throws Exception {
-    for (int i = 0; i < DELIMITERS.length; i++) {
-      Extensions ext = newExtensions(DELIMITERS[i]);
-      ext.add("testExt", new ExtensionStub());
-      ExtendableQueryParser parser = (ExtendableQueryParser) getParser(null,
-          ext);
-      String field = ext.buildExtensionField("testExt");
-      Query parse = parser.parse(String.format("%s:\"foo \\& bar\"", field));
-      assertTrue("expected instance of TermQuery but was " + parse.getClass(),
-          parse instanceof TermQuery);
-      TermQuery tquery = (TermQuery) parse;
-      assertEquals("field", tquery.getTerm().field());
-      assertEquals("foo & bar", tquery.getTerm().text());
-    }
-  }
-
-  public Extensions newExtensions(char delimiter) {
-    return new Extensions(delimiter);
-  }
-
-  public void testExtField() throws Exception {
-    for (int i = 0; i < DELIMITERS.length; i++) {
-      Extensions ext = newExtensions(DELIMITERS[i]);
-      ext.add("testExt", new ExtensionStub());
-      ExtendableQueryParser parser = (ExtendableQueryParser) getParser(null,
-          ext);
-      String field = ext.buildExtensionField("testExt", "afield");
-      Query parse = parser.parse(String.format("%s:\"foo \\& bar\"", field));
-      assertTrue("expected instance of TermQuery but was " + parse.getClass(),
-          parse instanceof TermQuery);
-      TermQuery tquery = (TermQuery) parse;
-      assertEquals("afield", tquery.getTerm().field());
-      assertEquals("foo & bar", tquery.getTerm().text());
-    }
-  }
-
-}
diff --git a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/ext/TestExtensions.java b/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/ext/TestExtensions.java
deleted file mode 100644
index 8c061b6..0000000
--- a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/ext/TestExtensions.java
+++ /dev/null
@@ -1,79 +0,0 @@
-package org.apache.lucene.queryParser.ext;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.LuceneTestCase;
-
-/**
- * Testcase for the {@link Extensions} class
- */
-public class TestExtensions extends LuceneTestCase {
-
-  private Extensions ext;
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    this.ext = new Extensions();
-  }
-
-  public void testBuildExtensionField() {
-    assertEquals("field\\:key", ext.buildExtensionField("key", "field"));
-    assertEquals("\\:key", ext.buildExtensionField("key"));
-
-    ext = new Extensions('.');
-    assertEquals("field.key", ext.buildExtensionField("key", "field"));
-    assertEquals(".key", ext.buildExtensionField("key"));
-  }
-
-  public void testSplitExtensionField() {
-    assertEquals("field\\:key", ext.buildExtensionField("key", "field"));
-    assertEquals("\\:key", ext.buildExtensionField("key"));
-
-    ext = new Extensions('.');
-    assertEquals("field.key", ext.buildExtensionField("key", "field"));
-    assertEquals(".key", ext.buildExtensionField("key"));
-  }
-
-  public void testAddGetExtension() {
-    ParserExtension extension = new ExtensionStub();
-    assertNull(ext.getExtension("foo"));
-    ext.add("foo", extension);
-    assertSame(extension, ext.getExtension("foo"));
-    ext.add("foo", null);
-    assertNull(ext.getExtension("foo"));
-  }
-
-  public void testGetExtDelimiter() {
-    assertEquals(Extensions.DEFAULT_EXTENSION_FIELD_DELIMITER, this.ext
-        .getExtensionFieldDelimiter());
-    ext = new Extensions('?');
-    assertEquals('?', this.ext.getExtensionFieldDelimiter());
-  }
-
-  public void testEscapeExtension() {
-    assertEquals("abc\\:\\?\\{\\}\\[\\]\\\\\\(\\)\\+\\-\\!\\~", ext
-        .escapeExtensionField("abc:?{}[]\\()+-!~"));
-    try {
-      ext.escapeExtensionField(null);
-      fail("should throw NPE - escape string is null");
-    } catch (NullPointerException e) {
-      // 
-    }
-  }
-}
diff --git a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java b/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
deleted file mode 100644
index 961fe51..0000000
--- a/lucene/contrib/misc/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
+++ /dev/null
@@ -1,613 +0,0 @@
-package org.apache.lucene.queryParser.precedence;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.LowerCaseTokenizer;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.WhitespaceAnalyzer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.document.DateTools;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.FuzzyQuery;
-import org.apache.lucene.search.PhraseQuery;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.WildcardQuery;
-import org.apache.lucene.util.LocalizedTestCase;
-
-import java.io.IOException;
-import java.io.Reader;
-import java.text.DateFormat;
-import java.util.Arrays;
-import java.util.Calendar;
-import java.util.GregorianCalendar;
-import java.util.HashSet;
-import java.util.Collections;
-
-public class TestPrecedenceQueryParser extends LocalizedTestCase {
-  
-  public TestPrecedenceQueryParser(String name) {
-    super(name, new HashSet<String>(Arrays.asList(new String[]{
-      "testDateRange", "testNumber"
-    })));
-  }
-
-  public static Analyzer qpAnalyzer = new QPTestAnalyzer();
-
-  public static class QPTestFilter extends TokenFilter {
-    /**
-     * Filter which discards the token 'stop' and which expands the
-     * token 'phrase' into 'phrase1 phrase2'
-     */
-    public QPTestFilter(TokenStream in) {
-      super(in);
-    }
-
-    boolean inPhrase = false;
-    int savedStart = 0, savedEnd = 0;
-
-    TermAttribute termAtt = addAttribute(TermAttribute.class);
-    OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-    
-    @Override
-    public boolean incrementToken() throws IOException {
-      clearAttributes();
-      if (inPhrase) {
-        inPhrase = false;
-        termAtt.setTermBuffer("phrase2");
-        offsetAtt.setOffset(savedStart, savedEnd);
-        return true;
-      } else
-        while(input.incrementToken())
-          if (termAtt.term().equals("phrase")) {
-            inPhrase = true;
-            savedStart = offsetAtt.startOffset();
-            savedEnd = offsetAtt.endOffset();
-            termAtt.setTermBuffer("phrase1");
-            offsetAtt.setOffset(savedStart, savedEnd);
-            return true;
-          } else if (!termAtt.term().equals("stop"))
-            return true;
-      return false;
-    }
-  }
-
-  public static class QPTestAnalyzer extends Analyzer {
-
-    /** Filters LowerCaseTokenizer with StopFilter. */
-    @Override
-    public final TokenStream tokenStream(String fieldName, Reader reader) {
-      return new QPTestFilter(new LowerCaseTokenizer(TEST_VERSION_CURRENT, reader));
-    }
-  }
-
-  public static class QPTestParser extends PrecedenceQueryParser {
-    public QPTestParser(String f, Analyzer a) {
-      super(f, a);
-    }
-
-    @Override
-    protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException {
-      throw new ParseException("Fuzzy queries not allowed");
-    }
-
-    @Override
-    protected Query getWildcardQuery(String field, String termStr) throws ParseException {
-      throw new ParseException("Wildcard queries not allowed");
-    }
-  }
-
-  private int originalMaxClauses;
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    originalMaxClauses = BooleanQuery.getMaxClauseCount();
-  }
-
-  public PrecedenceQueryParser getParser(Analyzer a) throws Exception {
-    if (a == null)
-      a = new SimpleAnalyzer(TEST_VERSION_CURRENT);
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", a);
-    qp.setDefaultOperator(PrecedenceQueryParser.OR_OPERATOR);
-    return qp;
-  }
-
-  public Query getQuery(String query, Analyzer a) throws Exception {
-    return getParser(a).parse(query);
-  }
-
-  public void assertQueryEquals(String query, Analyzer a, String result)
-    throws Exception {
-    Query q = getQuery(query, a);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("Query /" + query + "/ yielded /" + s
-           + "/, expecting /" + result + "/");
-    }
-  }
-
-  public void assertWildcardQueryEquals(String query, boolean lowercase, String result)
-    throws Exception {
-    PrecedenceQueryParser qp = getParser(null);
-    qp.setLowercaseExpandedTerms(lowercase);
-    Query q = qp.parse(query);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("WildcardQuery /" + query + "/ yielded /" + s
-           + "/, expecting /" + result + "/");
-    }
-  }
-
-  public void assertWildcardQueryEquals(String query, String result) throws Exception {
-    PrecedenceQueryParser qp = getParser(null);
-    Query q = qp.parse(query);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("WildcardQuery /" + query + "/ yielded /" + s + "/, expecting /"
-          + result + "/");
-    }
-  }
-
-  public Query getQueryDOA(String query, Analyzer a)
-    throws Exception {
-    if (a == null)
-      a = new SimpleAnalyzer(TEST_VERSION_CURRENT);
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", a);
-    qp.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR);
-    return qp.parse(query);
-  }
-
-  public void assertQueryEqualsDOA(String query, Analyzer a, String result)
-    throws Exception {
-    Query q = getQueryDOA(query, a);
-    String s = q.toString("field");
-    if (!s.equals(result)) {
-      fail("Query /" + query + "/ yielded /" + s
-           + "/, expecting /" + result + "/");
-    }
-  }
-
-  // failing tests disabled since PrecedenceQueryParser
-  // is currently unmaintained
-  public void _testSimple() throws Exception {
-    assertQueryEquals("", null, "");
-
-    assertQueryEquals("term term term", null, "term term term");
-    assertQueryEquals("türm term term", null, "türm term term");
-    assertQueryEquals("ümlaut", null, "ümlaut");
-
-    assertQueryEquals("+a", null, "+a");
-    assertQueryEquals("-a", null, "-a");
-    assertQueryEquals("a AND b", null, "+a +b");
-    assertQueryEquals("(a AND b)", null, "+a +b");
-    assertQueryEquals("c OR (a AND b)", null, "c (+a +b)");
-    assertQueryEquals("a AND NOT b", null, "+a -b");
-    assertQueryEquals("a AND -b", null, "+a -b");
-    assertQueryEquals("a AND !b", null, "+a -b");
-    assertQueryEquals("a && b", null, "+a +b");
-    assertQueryEquals("a && ! b", null, "+a -b");
-
-    assertQueryEquals("a OR b", null, "a b");
-    assertQueryEquals("a || b", null, "a b");
-
-    assertQueryEquals("+term -term term", null, "+term -term term");
-    assertQueryEquals("foo:term AND field:anotherTerm", null,
-                      "+foo:term +anotherterm");
-    assertQueryEquals("term AND \"phrase phrase\"", null,
-                      "+term +\"phrase phrase\"");
-    assertQueryEquals("\"hello there\"", null, "\"hello there\"");
-    assertTrue(getQuery("a AND b", null) instanceof BooleanQuery);
-    assertTrue(getQuery("hello", null) instanceof TermQuery);
-    assertTrue(getQuery("\"hello there\"", null) instanceof PhraseQuery);
-
-    assertQueryEquals("germ term^2.0", null, "germ term^2.0");
-    assertQueryEquals("(term)^2.0", null, "term^2.0");
-    assertQueryEquals("(germ term)^2.0", null, "(germ term)^2.0");
-    assertQueryEquals("term^2.0", null, "term^2.0");
-    assertQueryEquals("term^2", null, "term^2.0");
-    assertQueryEquals("\"germ term\"^2.0", null, "\"germ term\"^2.0");
-    assertQueryEquals("\"term germ\"^2", null, "\"term germ\"^2.0");
-
-    assertQueryEquals("(foo OR bar) AND (baz OR boo)", null,
-                      "+(foo bar) +(baz boo)");
-    assertQueryEquals("((a OR b) AND NOT c) OR d", null,
-                      "(+(a b) -c) d");
-    assertQueryEquals("+(apple \"steve jobs\") -(foo bar baz)", null,
-                      "+(apple \"steve jobs\") -(foo bar baz)");
-    assertQueryEquals("+title:(dog OR cat) -author:\"bob dole\"", null,
-                      "+(title:dog title:cat) -author:\"bob dole\"");
-    
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", new StandardAnalyzer(TEST_VERSION_CURRENT));
-    // make sure OR is the default:
-    assertEquals(PrecedenceQueryParser.OR_OPERATOR, qp.getDefaultOperator());
-    qp.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR);
-    assertEquals(PrecedenceQueryParser.AND_OPERATOR, qp.getDefaultOperator());
-    qp.setDefaultOperator(PrecedenceQueryParser.OR_OPERATOR);
-    assertEquals(PrecedenceQueryParser.OR_OPERATOR, qp.getDefaultOperator());
-
-    assertQueryEquals("a OR !b", null, "a (-b)");
-    assertQueryEquals("a OR ! b", null, "a (-b)");
-    assertQueryEquals("a OR -b", null, "a (-b)");
-  }
-
-  public void testPunct() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
-    assertQueryEquals("a&b", a, "a&b");
-    assertQueryEquals("a&&b", a, "a&&b");
-    assertQueryEquals(".NET", a, ".NET");
-  }
-
-  public void testSlop() throws Exception {
-    assertQueryEquals("\"term germ\"~2", null, "\"term germ\"~2");
-    assertQueryEquals("\"term germ\"~2 flork", null, "\"term germ\"~2 flork");
-    assertQueryEquals("\"term\"~2", null, "term");
-    assertQueryEquals("\" \"~2 germ", null, "germ");
-    assertQueryEquals("\"term germ\"~2^2", null, "\"term germ\"~2^2.0");
-  }
-
-  public void testNumber() throws Exception {
-// The numbers go away because SimpleAnalzyer ignores them
-    assertQueryEquals("3", null, "");
-    assertQueryEquals("term 1.0 1 2", null, "term");
-    assertQueryEquals("term term1 term2", null, "term term term");
-
-    Analyzer a = new StandardAnalyzer(TEST_VERSION_CURRENT);
-    assertQueryEquals("3", a, "3");
-    assertQueryEquals("term 1.0 1 2", a, "term 1.0 1 2");
-    assertQueryEquals("term term1 term2", a, "term term1 term2");
-  }
-
-  // failing tests disabled since PrecedenceQueryParser
-  // is currently unmaintained
-  public void _testWildcard() throws Exception {
-    assertQueryEquals("term*", null, "term*");
-    assertQueryEquals("term*^2", null, "term*^2.0");
-    assertQueryEquals("term~", null, "term~0.5");
-    assertQueryEquals("term~0.7", null, "term~0.7");
-    assertQueryEquals("term~^2", null, "term^2.0~0.5");
-    assertQueryEquals("term^2~", null, "term^2.0~0.5");
-    assertQueryEquals("term*germ", null, "term*germ");
-    assertQueryEquals("term*germ^3", null, "term*germ^3.0");
-
-    assertTrue(getQuery("term*", null) instanceof PrefixQuery);
-    assertTrue(getQuery("term*^2", null) instanceof PrefixQuery);
-    assertTrue(getQuery("term~", null) instanceof FuzzyQuery);
-    assertTrue(getQuery("term~0.7", null) instanceof FuzzyQuery);
-    FuzzyQuery fq = (FuzzyQuery)getQuery("term~0.7", null);
-    assertEquals(0.7f, fq.getMinSimilarity(), 0.1f);
-    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
-    fq = (FuzzyQuery)getQuery("term~", null);
-    assertEquals(0.5f, fq.getMinSimilarity(), 0.1f);
-    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
-    try {
-      getQuery("term~1.1", null);   // value > 1, throws exception
-      fail();
-    } catch(ParseException pe) {
-      // expected exception
-    }
-    assertTrue(getQuery("term*germ", null) instanceof WildcardQuery);
-
-/* Tests to see that wild card terms are (or are not) properly
-	 * lower-cased with propery parser configuration
-	 */
-// First prefix queries:
-    // by default, convert to lowercase:
-    assertWildcardQueryEquals("Term*", true, "term*");
-    // explicitly set lowercase:
-    assertWildcardQueryEquals("term*", true, "term*");
-    assertWildcardQueryEquals("Term*", true, "term*");
-    assertWildcardQueryEquals("TERM*", true, "term*");
-    // explicitly disable lowercase conversion:
-    assertWildcardQueryEquals("term*", false, "term*");
-    assertWildcardQueryEquals("Term*", false, "Term*");
-    assertWildcardQueryEquals("TERM*", false, "TERM*");
-// Then 'full' wildcard queries:
-    // by default, convert to lowercase:
-    assertWildcardQueryEquals("Te?m", "te?m");
-    // explicitly set lowercase:
-    assertWildcardQueryEquals("te?m", true, "te?m");
-    assertWildcardQueryEquals("Te?m", true, "te?m");
-    assertWildcardQueryEquals("TE?M", true, "te?m");
-    assertWildcardQueryEquals("Te?m*gerM", true, "te?m*germ");
-    // explicitly disable lowercase conversion:
-    assertWildcardQueryEquals("te?m", false, "te?m");
-    assertWildcardQueryEquals("Te?m", false, "Te?m");
-    assertWildcardQueryEquals("TE?M", false, "TE?M");
-    assertWildcardQueryEquals("Te?m*gerM", false, "Te?m*gerM");
-//  Fuzzy queries:
-    assertWildcardQueryEquals("Term~", "term~0.5");
-    assertWildcardQueryEquals("Term~", true, "term~0.5");
-    assertWildcardQueryEquals("Term~", false, "Term~0.5");
-//  Range queries:
-    assertWildcardQueryEquals("[A TO C]", "[a TO c]");
-    assertWildcardQueryEquals("[A TO C]", true, "[a TO c]");
-    assertWildcardQueryEquals("[A TO C]", false, "[A TO C]");
-  }
-
-  public void testQPA() throws Exception {
-    assertQueryEquals("term term term", qpAnalyzer, "term term term");
-    assertQueryEquals("term +stop term", qpAnalyzer, "term term");
-    assertQueryEquals("term -stop term", qpAnalyzer, "term term");
-    assertQueryEquals("drop AND stop AND roll", qpAnalyzer, "+drop +roll");
-    assertQueryEquals("term phrase term", qpAnalyzer,
-                      "term \"phrase1 phrase2\" term");
-    // note the parens in this next assertion differ from the original
-    // QueryParser behavior
-    assertQueryEquals("term AND NOT phrase term", qpAnalyzer,
-                      "(+term -\"phrase1 phrase2\") term");
-    assertQueryEquals("stop", qpAnalyzer, "");
-    assertQueryEquals("stop OR stop AND stop", qpAnalyzer, "");
-    assertTrue(getQuery("term term term", qpAnalyzer) instanceof BooleanQuery);
-    assertTrue(getQuery("term +stop", qpAnalyzer) instanceof TermQuery);
-  }
-
-  public void testRange() throws Exception {
-    assertQueryEquals("[ a TO z]", null, "[a TO z]");
-    assertTrue(getQuery("[ a TO z]", null) instanceof TermRangeQuery);
-    assertQueryEquals("[ a TO z ]", null, "[a TO z]");
-    assertQueryEquals("{ a TO z}", null, "{a TO z}");
-    assertQueryEquals("{ a TO z }", null, "{a TO z}");
-    assertQueryEquals("{ a TO z }^2.0", null, "{a TO z}^2.0");
-    assertQueryEquals("[ a TO z] OR bar", null, "[a TO z] bar");
-    assertQueryEquals("[ a TO z] AND bar", null, "+[a TO z] +bar");
-    assertQueryEquals("( bar blar { a TO z}) ", null, "bar blar {a TO z}");
-    assertQueryEquals("gack ( bar blar { a TO z}) ", null, "gack (bar blar {a TO z})");
-  }
-  
-  private String escapeDateString(String s) {
-    if (s.contains(" ")) {
-      return "\"" + s + "\"";
-    } else {
-      return s;
-    }
-  }
-
-  public String getDate(String s) throws Exception {
-    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
-    return DateTools.dateToString(df.parse(s), DateTools.Resolution.DAY);
-  }
-
-  public String getLocalizedDate(int year, int month, int day) {
-    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
-    Calendar calendar = new GregorianCalendar();
-    calendar.clear();
-    calendar.set(year, month, day);
-    calendar.set(Calendar.HOUR_OF_DAY, 23);
-    calendar.set(Calendar.MINUTE, 59);
-    calendar.set(Calendar.SECOND, 59);
-    calendar.set(Calendar.MILLISECOND, 999);
-    return df.format(calendar.getTime());
-  }
-
-  public void testDateRange() throws Exception {
-    String startDate = getLocalizedDate(2002, 1, 1);
-    String endDate = getLocalizedDate(2002, 1, 4);
-    assertQueryEquals("[ " + escapeDateString(startDate) + " TO " + escapeDateString(endDate) + "]", null,
-                      "[" + getDate(startDate) + " TO " + getDate(endDate) + "]");
-    assertQueryEquals("{  " + escapeDateString(startDate) + "    " + escapeDateString(endDate) + "   }", null,
-                      "{" + getDate(startDate) + " TO " + getDate(endDate) + "}");
-  }
-
-  public void testEscaped() throws Exception {
-    Analyzer a = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
-    
-    /*assertQueryEquals("\\[brackets", a, "\\[brackets");
-    assertQueryEquals("\\[brackets", null, "brackets");
-    assertQueryEquals("\\\\", a, "\\\\");
-    assertQueryEquals("\\+blah", a, "\\+blah");
-    assertQueryEquals("\\(blah", a, "\\(blah");
-
-    assertQueryEquals("\\-blah", a, "\\-blah");
-    assertQueryEquals("\\!blah", a, "\\!blah");
-    assertQueryEquals("\\{blah", a, "\\{blah");
-    assertQueryEquals("\\}blah", a, "\\}blah");
-    assertQueryEquals("\\:blah", a, "\\:blah");
-    assertQueryEquals("\\^blah", a, "\\^blah");
-    assertQueryEquals("\\[blah", a, "\\[blah");
-    assertQueryEquals("\\]blah", a, "\\]blah");
-    assertQueryEquals("\\\"blah", a, "\\\"blah");
-    assertQueryEquals("\\(blah", a, "\\(blah");
-    assertQueryEquals("\\)blah", a, "\\)blah");
-    assertQueryEquals("\\~blah", a, "\\~blah");
-    assertQueryEquals("\\*blah", a, "\\*blah");
-    assertQueryEquals("\\?blah", a, "\\?blah");
-    //assertQueryEquals("foo \\&\\& bar", a, "foo \\&\\& bar");
-    //assertQueryEquals("foo \\|| bar", a, "foo \\|| bar");
-    //assertQueryEquals("foo \\AND bar", a, "foo \\AND bar");*/
-
-    assertQueryEquals("a\\-b:c", a, "a-b:c");
-    assertQueryEquals("a\\+b:c", a, "a+b:c");
-    assertQueryEquals("a\\:b:c", a, "a:b:c");
-    assertQueryEquals("a\\\\b:c", a, "a\\b:c");
-
-    assertQueryEquals("a:b\\-c", a, "a:b-c");
-    assertQueryEquals("a:b\\+c", a, "a:b+c");
-    assertQueryEquals("a:b\\:c", a, "a:b:c");
-    assertQueryEquals("a:b\\\\c", a, "a:b\\c");
-
-    assertQueryEquals("a:b\\-c*", a, "a:b-c*");
-    assertQueryEquals("a:b\\+c*", a, "a:b+c*");
-    assertQueryEquals("a:b\\:c*", a, "a:b:c*");
-
-    assertQueryEquals("a:b\\\\c*", a, "a:b\\c*");
-
-    assertQueryEquals("a:b\\-?c", a, "a:b-?c");
-    assertQueryEquals("a:b\\+?c", a, "a:b+?c");
-    assertQueryEquals("a:b\\:?c", a, "a:b:?c");
-
-    assertQueryEquals("a:b\\\\?c", a, "a:b\\?c");
-
-    assertQueryEquals("a:b\\-c~", a, "a:b-c~0.5");
-    assertQueryEquals("a:b\\+c~", a, "a:b+c~0.5");
-    assertQueryEquals("a:b\\:c~", a, "a:b:c~0.5");
-    assertQueryEquals("a:b\\\\c~", a, "a:b\\c~0.5");
-
-    assertQueryEquals("[ a\\- TO a\\+ ]", null, "[a- TO a+]");
-    assertQueryEquals("[ a\\: TO a\\~ ]", null, "[a: TO a~]");
-    assertQueryEquals("[ a\\\\ TO a\\* ]", null, "[a\\ TO a*]");
-  }
-
-  public void testTabNewlineCarriageReturn()
-    throws Exception {
-    assertQueryEqualsDOA("+weltbank +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \n +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\r+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\r\n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r\n+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r\n +worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \r \n +worlbank", null,
-      "+weltbank +worlbank");
-
-    assertQueryEqualsDOA("+weltbank\t+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \t+worlbank", null,
-      "+weltbank +worlbank");
-    assertQueryEqualsDOA("weltbank \t +worlbank", null,
-      "+weltbank +worlbank");
-  }
-
-  public void testSimpleDAO()
-    throws Exception {
-    assertQueryEqualsDOA("term term term", null, "+term +term +term");
-    assertQueryEqualsDOA("term +term term", null, "+term +term +term");
-    assertQueryEqualsDOA("term term +term", null, "+term +term +term");
-    assertQueryEqualsDOA("term +term +term", null, "+term +term +term");
-    assertQueryEqualsDOA("-term term term", null, "-term +term +term");
-  }
-
-  public void testBoost()
-    throws Exception {
-    StandardAnalyzer oneStopAnalyzer = new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.singleton("on"));
-    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", oneStopAnalyzer);
-    Query q = qp.parse("on^1.0");
-    assertNotNull(q);
-    q = qp.parse("\"hello\"^2.0");
-    assertNotNull(q);
-    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
-    q = qp.parse("hello^2.0");
-    assertNotNull(q);
-    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
-    q = qp.parse("\"on\"^1.0");
-    assertNotNull(q);
-
-    q = getParser(new StandardAnalyzer(TEST_VERSION_CURRENT)).parse("the^3");
-    assertNotNull(q);
-  }
-
-  public void testException() throws Exception {
-    try {
-      assertQueryEquals("\"some phrase", null, "abc");
-      fail("ParseException expected, not thrown");
-    } catch (ParseException expected) {
-    }
-  }
-
-  public void testCustomQueryParserWildcard() {
-    try {
-      new QPTestParser("contents", new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).parse("a?t");
-    } catch (ParseException expected) {
-      return;
-    }
-    fail("Wildcard queries should not be allowed");
-  }
-
-  public void testCustomQueryParserFuzzy() throws Exception {
-    try {
-      new QPTestParser("contents", new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).parse("xunit~");
-    } catch (ParseException expected) {
-      return;
-    }
-    fail("Fuzzy queries should not be allowed");
-  }
-
-  public void testBooleanQuery() throws Exception {
-    BooleanQuery.setMaxClauseCount(2);
-    try {
-      getParser(new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).parse("one two three");
-      fail("ParseException expected due to too many boolean clauses");
-    } catch (ParseException expected) {
-      // too many boolean clauses, so ParseException is expected
-    }
-  }
-
-  /**
-   * This test differs from the original QueryParser, showing how the
-   * precedence issue has been corrected.
-   */
-  // failing tests disabled since PrecedenceQueryParser
-  // is currently unmaintained
-  public void _testPrecedence() throws Exception {
-    PrecedenceQueryParser parser = getParser(new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
-    Query query1 = parser.parse("A AND B OR C AND D");
-    Query query2 = parser.parse("(A AND B) OR (C AND D)");
-    assertEquals(query1, query2);
-
-    query1 = parser.parse("A OR B C");
-    query2 = parser.parse("A B C");
-    assertEquals(query1, query2);
-
-    query1 = parser.parse("A AND B C");
-    query2 = parser.parse("(+A +B) C");
-    assertEquals(query1, query2);
-
-    query1 = parser.parse("A AND NOT B");
-    query2 = parser.parse("+A -B");
-    assertEquals(query1, query2);
-
-    query1 = parser.parse("A OR NOT B");
-    query2 = parser.parse("A -B");
-    assertEquals(query1, query2);
-
-    query1 = parser.parse("A OR NOT B AND C");
-    query2 = parser.parse("A (-B +C)");
-    assertEquals(query1, query2);
-  }
-
-
-  @Override
-  protected void tearDown() throws Exception {
-    BooleanQuery.setMaxClauseCount(originalMaxClauses);
-    super.tearDown();
-  }
-
-}
diff --git a/lucene/contrib/queries/README.txt b/lucene/contrib/queries/README.txt
index 0db8236..817969f 100644
--- a/lucene/contrib/queries/README.txt
+++ b/lucene/contrib/queries/README.txt
@@ -17,6 +17,7 @@ but reduces the overall score of docs containing these terms.
 a sequence. An example might be a collection of primary keys from a database query result or perhaps 
 a choice of "category" labels picked by the end user.
 
+==== RegexQuery - Implements the regular expression term search query. 
 
 Mark Harwood
 25/02/2006
diff --git a/lucene/contrib/queries/build.xml b/lucene/contrib/queries/build.xml
index b07ae00..9115240 100644
--- a/lucene/contrib/queries/build.xml
+++ b/lucene/contrib/queries/build.xml
@@ -23,5 +23,14 @@
     Queries - various query object exotica not in core
   </description>
 
+  <path id="additional.dependencies">
+    <fileset dir="lib" includes="*-oro-*.jar,*-regexp-*.jar"/>
+  </path>
+
+  <pathconvert property="project.classpath"
+               targetos="unix"
+               refid="additional.dependencies"
+  />
+
   <import file="../contrib-build.xml"/>
 </project>
diff --git a/lucene/contrib/queries/lib/jakarta-regexp-1.4.jar b/lucene/contrib/queries/lib/jakarta-regexp-1.4.jar
new file mode 100644
index 0000000..0366f23
--- /dev/null
+++ b/lucene/contrib/queries/lib/jakarta-regexp-1.4.jar
@@ -0,0 +1,2 @@
+AnyObjectId[5d70c357a1e6c4c702af313c94aaf3168d300dcf] was removed in git history.
+Apache SVN contains full history.
\ No newline at end of file
diff --git a/lucene/contrib/queries/lib/regexp.LICENSE b/lucene/contrib/queries/lib/regexp.LICENSE
new file mode 100644
index 0000000..261eeb9
--- /dev/null
+++ b/lucene/contrib/queries/lib/regexp.LICENSE
@@ -0,0 +1,201 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/lucene/contrib/queries/pom.xml.template b/lucene/contrib/queries/pom.xml.template
index 3fe4ae4..0cd1086 100644
--- a/lucene/contrib/queries/pom.xml.template
+++ b/lucene/contrib/queries/pom.xml.template
@@ -35,4 +35,11 @@
     Queries - various query object exotica not in core
   </description>
   <packaging>jar</packaging>
+  <dependencies>
+    <dependency>
+      <groupId>jakarta-regexp</groupId>
+      <artifactId>jakarta-regexp</artifactId>
+      <version>${jakarta-regexp-version}</version>
+    </dependency>
+  </dependencies>
 </project>
diff --git a/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/JakartaRegexpCapabilities.java b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/JakartaRegexpCapabilities.java
new file mode 100644
index 0000000..3e7c429
--- /dev/null
+++ b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/JakartaRegexpCapabilities.java
@@ -0,0 +1,93 @@
+package org.apache.lucene.search.regex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.regexp.RE;
+import org.apache.regexp.RegexpTunnel;
+
+/**
+ * Implementation tying <a href="http://jakarta.apache.org/regexp">Jakarta
+ * Regexp</a> to RegexQuery. Jakarta Regepx internally supports a
+ * {@link #prefix} implementation which can offer performance gains under
+ * certain circumstances. Yet, the implementation appears to be rather shaky as
+ * it doesn't always provide a prefix even if one would exist.
+ */
+public class JakartaRegexpCapabilities implements RegexCapabilities {
+  private RE regexp;
+  
+  // Define the flags that are possible. Redefine them here
+  // to avoid exposing the RE class to the caller.
+  
+  private int flags = RE.MATCH_NORMAL;
+
+  /**
+   * Flag to specify normal, case-sensitive matching behaviour. This is the default.
+   */
+  public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;
+  
+  /**
+   * Flag to specify that matching should be case-independent (folded)
+   */
+  public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;
+ 
+  /**
+   * Constructs a RegexCapabilities with the default MATCH_NORMAL match style.
+   */
+  public JakartaRegexpCapabilities() {}
+  
+  /**
+   * Constructs a RegexCapabilities with the provided match flags.
+   * Multiple flags should be ORed together.
+   * 
+   * @param flags The matching style
+   */
+  public JakartaRegexpCapabilities(int flags)
+  {
+    this.flags = flags;
+  }
+  
+  public void compile(String pattern) {
+    regexp = new RE(pattern, this.flags);
+  }
+
+  public boolean match(String string) {
+    return regexp.match(string);
+  }
+
+  public String prefix() {
+    char[] prefix = RegexpTunnel.getPrefix(regexp);
+    return prefix == null ? null : new String(prefix);
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    final JakartaRegexpCapabilities that = (JakartaRegexpCapabilities) o;
+
+    if (regexp != null ? !regexp.equals(that.regexp) : that.regexp != null) return false;
+
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    return (regexp != null ? regexp.hashCode() : 0);
+  }
+}
diff --git a/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/JavaUtilRegexCapabilities.java b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/JavaUtilRegexCapabilities.java
new file mode 100644
index 0000000..9bb32b7
--- /dev/null
+++ b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/JavaUtilRegexCapabilities.java
@@ -0,0 +1,97 @@
+package org.apache.lucene.search.regex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.regex.Pattern;
+
+/**
+ * An implementation tying Java's built-in java.util.regex to RegexQuery.
+ *
+ * Note that because this implementation currently only returns null from
+ * {@link #prefix} that queries using this implementation will enumerate and
+ * attempt to {@link #match} each term for the specified field in the index.
+ */
+public class JavaUtilRegexCapabilities implements RegexCapabilities {
+  private Pattern pattern;
+  private int flags = 0;
+  
+  // Define the optional flags from Pattern that can be used.
+  // Do this here to keep Pattern contained within this class.
+  
+  public static final int FLAG_CANON_EQ = Pattern.CANON_EQ;
+  public static final int FLAG_CASE_INSENSITIVE = Pattern.CASE_INSENSITIVE;
+  public static final int FLAG_COMMENTS = Pattern.COMMENTS;
+  public static final int FLAG_DOTALL = Pattern.DOTALL;
+  public static final int FLAG_LITERAL = Pattern.LITERAL;
+  public static final int FLAG_MULTILINE = Pattern.MULTILINE;
+  public static final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;
+  public static final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;
+  
+  /**
+   * Default constructor that uses java.util.regex.Pattern 
+   * with its default flags.
+   */
+  public JavaUtilRegexCapabilities()  {
+    this.flags = 0;
+  }
+  
+  /**
+   * Constructor that allows for the modification of the flags that
+   * the java.util.regex.Pattern will use to compile the regular expression.
+   * This gives the user the ability to fine-tune how the regular expression 
+   * to match the functionality that they need. 
+   * The {@link java.util.regex.Pattern Pattern} class supports specifying 
+   * these fields via the regular expression text itself, but this gives the caller
+   * another option to modify the behavior. Useful in cases where the regular expression text
+   * cannot be modified, or if doing so is undesired.
+   * 
+   * @param flags The flags that are ORed together.
+   */
+  public JavaUtilRegexCapabilities(int flags) {
+    this.flags = flags;
+  }
+  
+  public void compile(String pattern) {
+    this.pattern = Pattern.compile(pattern, this.flags);
+  }
+
+  public boolean match(String string) {
+    return pattern.matcher(string).matches();
+  }
+
+  public String prefix() {
+    return null;
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    final JavaUtilRegexCapabilities that = (JavaUtilRegexCapabilities) o;
+
+    if (pattern != null ? !pattern.equals(that.pattern) : that.pattern != null) return false;
+
+    return true;
+  }
+
+  @Override
+  public int hashCode() {
+    return (pattern != null ? pattern.hashCode() : 0);
+  }
+}
diff --git a/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexCapabilities.java b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexCapabilities.java
new file mode 100644
index 0000000..6270efd
--- /dev/null
+++ b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexCapabilities.java
@@ -0,0 +1,48 @@
+package org.apache.lucene.search.regex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Defines basic operations needed by {@link RegexQuery} for a regular
+ * expression implementation.
+ */
+public interface RegexCapabilities {
+  /**
+   * Called by the constructor of {@link RegexTermEnum} allowing
+   * implementations to cache a compiled version of the regular
+   * expression pattern.
+   *
+   * @param pattern regular expression pattern
+   */
+  void compile(String pattern);
+
+  /**
+   *
+   * @param string
+   * @return true if string matches the pattern last passed to {@link #compile}.
+   */
+  boolean match(String string);
+
+  /**
+   * A wise prefix implementation can reduce the term enumeration (and thus increase performance)
+   * of RegexQuery dramatically!
+   *
+   * @return static non-regex prefix of the pattern last passed to {@link #compile}.  May return null.
+   */
+  String prefix();
+}
diff --git a/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexQuery.java b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexQuery.java
new file mode 100644
index 0000000..9562d5a
--- /dev/null
+++ b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexQuery.java
@@ -0,0 +1,97 @@
+package org.apache.lucene.search.regex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.FilteredTermEnum;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.util.ToStringUtils;
+
+import java.io.IOException;
+
+/** Implements the regular expression term search query.
+ * The expressions supported depend on the regular expression implementation
+ * used by way of the {@link RegexCapabilities} interface.
+ *
+ * @see RegexTermEnum
+ */
+public class RegexQuery extends MultiTermQuery implements RegexQueryCapable {
+  private RegexCapabilities regexImpl = new JavaUtilRegexCapabilities();
+  private Term term;
+
+  /** Constructs a query for terms matching <code>term</code>. */
+  public RegexQuery(Term term) {
+    this.term = term;
+  }
+  
+  public Term getTerm() { return term; }
+
+  /**
+   * Defines which {@link RegexCapabilities} implementation is used by this instance.
+   *
+   * @param impl
+   */
+  public void setRegexImplementation(RegexCapabilities impl) {
+    this.regexImpl = impl;
+  }
+
+  /**
+   * @return The implementation used by this instance.
+   */
+  public RegexCapabilities getRegexImplementation() {
+    return regexImpl;
+  }
+
+  @Override
+  protected FilteredTermEnum getEnum(IndexReader reader) throws IOException {
+    return new RegexTermEnum(reader, term, regexImpl);
+  }
+
+  @Override
+  public String toString(String field) {
+    StringBuilder buffer = new StringBuilder();
+    if (!term.field().equals(field)) {
+      buffer.append(term.field());
+      buffer.append(":");
+    }
+    buffer.append(term.text());
+    buffer.append(ToStringUtils.boost(getBoost()));
+    return buffer.toString();
+  }
+
+  /* generated by IntelliJ IDEA */
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+    if (!super.equals(o)) return false;
+
+    final RegexQuery that = (RegexQuery) o;
+
+    return regexImpl.equals(that.regexImpl);
+  }
+
+  /* generated by IntelliJ IDEA */
+  @Override
+  public int hashCode() {
+    int result = super.hashCode();
+    result = 29 * result + regexImpl.hashCode();
+    return result;
+  }
+}
diff --git a/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexQueryCapable.java b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexQueryCapable.java
new file mode 100644
index 0000000..bb8a2c3
--- /dev/null
+++ b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexQueryCapable.java
@@ -0,0 +1,27 @@
+package org.apache.lucene.search.regex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/**
+ * Defines methods for regular expression supporting Querys to use.
+ */
+public interface RegexQueryCapable {
+  void setRegexImplementation(RegexCapabilities impl);
+  RegexCapabilities getRegexImplementation();
+}
diff --git a/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexTermEnum.java b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexTermEnum.java
new file mode 100644
index 0000000..ae814aa
--- /dev/null
+++ b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/RegexTermEnum.java
@@ -0,0 +1,83 @@
+package org.apache.lucene.search.regex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.search.FilteredTermEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+
+import java.io.IOException;
+
+/**
+ * Subclass of FilteredTermEnum for enumerating all terms that match the
+ * specified regular expression term using the specified regular expression
+ * implementation.
+ * <p>
+ * Term enumerations are always ordered by Term.compareTo().  Each term in
+ * the enumeration is greater than all that precede it.
+ */
+
+public class RegexTermEnum extends FilteredTermEnum {
+  private String field = "";
+  private String pre = "";
+  private boolean endEnum = false;
+  private RegexCapabilities regexImpl;
+
+  public RegexTermEnum(IndexReader reader, Term term, RegexCapabilities regexImpl) throws IOException {
+    super();
+    field = term.field();
+    String text = term.text();
+    this.regexImpl = regexImpl;
+
+    regexImpl.compile(text);
+
+    pre = regexImpl.prefix();
+    if (pre == null) pre = "";
+
+    setEnum(reader.terms(new Term(term.field(), pre)));
+  }
+
+  @Override
+  protected final boolean termCompare(Term term) {
+    if (field == term.field()) {
+      String searchText = term.text();
+      if (searchText.startsWith(pre)) {
+        return regexImpl.match(searchText);
+      }
+    }
+    endEnum = true;
+    return false;
+  }
+
+  @Override
+  public final float difference() {
+// TODO: adjust difference based on distance of searchTerm.text() and term().text()
+    return 1.0f;
+  }
+
+  @Override
+  public final boolean endEnum() {
+    return endEnum;
+  }
+
+  @Override
+  public void close() throws IOException {
+    super.close();
+    field = null;
+  }
+}
diff --git a/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/SpanRegexQuery.java b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/SpanRegexQuery.java
new file mode 100644
index 0000000..aed0521
--- /dev/null
+++ b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/SpanRegexQuery.java
@@ -0,0 +1,132 @@
+package org.apache.lucene.search.regex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.spans.SpanOrQuery;
+import org.apache.lucene.search.spans.SpanQuery;
+import org.apache.lucene.search.spans.SpanTermQuery;
+import org.apache.lucene.search.spans.Spans;
+import org.apache.lucene.util.ToStringUtils;
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.ArrayList;
+
+/**
+ * A SpanQuery version of {@link RegexQuery} allowing regular expression
+ * queries to be nested within other SpanQuery subclasses.
+ */
+public class SpanRegexQuery extends SpanQuery implements RegexQueryCapable {
+  private RegexCapabilities regexImpl = new JavaUtilRegexCapabilities();
+  private Term term;
+
+  public SpanRegexQuery(Term term) {
+    this.term = term;
+  }
+
+  public Term getTerm() { return term; }
+
+  @Override
+  public Query rewrite(IndexReader reader) throws IOException {
+    RegexQuery orig = new RegexQuery(term);
+    orig.setRegexImplementation(regexImpl);
+    orig.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
+    BooleanQuery bq = (BooleanQuery) orig.rewrite(reader);
+
+    BooleanClause[] clauses = bq.getClauses();
+    SpanQuery[] sqs = new SpanQuery[clauses.length];
+    for (int i = 0; i < clauses.length; i++) {
+      BooleanClause clause = clauses[i];
+
+      // Clauses from RegexQuery.rewrite are always TermQuery's
+      TermQuery tq = (TermQuery) clause.getQuery();
+
+      sqs[i] = new SpanTermQuery(tq.getTerm());
+      sqs[i].setBoost(tq.getBoost());
+    }
+
+    SpanOrQuery query = new SpanOrQuery(sqs);
+    query.setBoost(orig.getBoost());
+
+    return query;
+  }
+
+  @Override
+  public Spans getSpans(IndexReader reader) throws IOException {
+    throw new UnsupportedOperationException("Query should have been rewritten");
+  }
+
+  @Override
+  public String getField() {
+    return term.field();
+  }
+
+  public Collection<Term> getTerms() {
+    Collection<Term> terms = new ArrayList<Term>();
+    terms.add(term);
+    return terms;
+  }
+
+  /* generated by IntelliJ IDEA */
+  @Override
+  public boolean equals(Object o) {
+    if (this == o) return true;
+    if (o == null || getClass() != o.getClass()) return false;
+
+    final SpanRegexQuery that = (SpanRegexQuery) o;
+
+    if (!regexImpl.equals(that.regexImpl)) return false;
+    if (!term.equals(that.term)) return false;
+
+    return true;
+  }
+
+  /* generated by IntelliJ IDEA */
+  @Override
+  public int hashCode() {
+    int result;
+    result = regexImpl.hashCode();
+    result = 29 * result + term.hashCode();
+    return result;
+  }
+
+  @Override
+  public String toString(String field) {
+    StringBuilder buffer = new StringBuilder();
+    buffer.append("spanRegexQuery(");
+    buffer.append(term);
+    buffer.append(")");
+    buffer.append(ToStringUtils.boost(getBoost()));
+    return buffer.toString();
+  }
+
+  public void setRegexImplementation(RegexCapabilities impl) {
+    this.regexImpl = impl;
+  }
+
+  public RegexCapabilities getRegexImplementation() {
+    return regexImpl;
+  }
+}
diff --git a/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/package.html b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/package.html
new file mode 100644
index 0000000..c963307
--- /dev/null
+++ b/lucene/contrib/queries/src/java/org/apache/lucene/search/regex/package.html
@@ -0,0 +1,5 @@
+<html><head></head>
+<body>
+Regular expression Query.
+</body>
+</html>
diff --git a/lucene/contrib/queries/src/java/org/apache/regexp/RegexpTunnel.java b/lucene/contrib/queries/src/java/org/apache/regexp/RegexpTunnel.java
new file mode 100644
index 0000000..9e43b93
--- /dev/null
+++ b/lucene/contrib/queries/src/java/org/apache/regexp/RegexpTunnel.java
@@ -0,0 +1,29 @@
+package org.apache.regexp;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/**
+ * This class exists as a gateway to access useful Jakarta Regexp package protected data.
+ */
+public class RegexpTunnel {
+  public static char[] getPrefix(RE regexp) {
+    REProgram program = regexp.getProgram();
+    return program.prefix;
+  }
+}
diff --git a/lucene/contrib/queries/src/java/org/apache/regexp/package.html b/lucene/contrib/queries/src/java/org/apache/regexp/package.html
new file mode 100644
index 0000000..15b3b7a2
--- /dev/null
+++ b/lucene/contrib/queries/src/java/org/apache/regexp/package.html
@@ -0,0 +1,24 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html><head></head>
+<body>
+This package exists to allow access to useful package protected data within
+Jakarta Regexp.  This data has now been opened up with an accessor, but
+an official release with that change has not been made to date.
+</body>
+</html>
diff --git a/lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestJakartaRegexpCapabilities.java b/lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestJakartaRegexpCapabilities.java
new file mode 100644
index 0000000..e9ca88d
--- /dev/null
+++ b/lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestJakartaRegexpCapabilities.java
@@ -0,0 +1,46 @@
+package org.apache.lucene.search.regex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import junit.framework.TestCase;
+
+/**
+ * Testcase for {@link JakartaRegexpCapabilities}
+ */
+public class TestJakartaRegexpCapabilities extends TestCase {
+
+  public void testGetPrefix(){
+    JakartaRegexpCapabilities cap = new JakartaRegexpCapabilities();
+    cap.compile("luc[e]?");
+    assertTrue(cap.match("luce"));
+    assertEquals("luc", cap.prefix());
+    
+    cap.compile("lucene");
+    assertTrue(cap.match("lucene"));
+    assertEquals("lucene", cap.prefix());
+  }
+  
+  public void testShakyPrefix(){
+    JakartaRegexpCapabilities cap = new JakartaRegexpCapabilities();
+    cap.compile("(ab|ac)");
+    assertTrue(cap.match("ab"));
+    assertTrue(cap.match("ac"));
+    // why is it not a???
+    assertNull(cap.prefix());
+  }
+}
diff --git a/lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestRegexQuery.java b/lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestRegexQuery.java
new file mode 100644
index 0000000..4785933
--- /dev/null
+++ b/lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestRegexQuery.java
@@ -0,0 +1,134 @@
+package org.apache.lucene.search.regex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.index.TermEnum;
+
+import org.apache.lucene.search.spans.SpanNearQuery;
+import org.apache.lucene.search.spans.SpanQuery;
+import org.apache.lucene.util.LuceneTestCase;
+
+public class TestRegexQuery extends LuceneTestCase {
+  private IndexSearcher searcher;
+  private final String FN = "field";
+
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    RAMDirectory directory = new RAMDirectory();
+    try {
+      IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(
+          TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));
+      Document doc = new Document();
+      doc.add(new Field(FN, "the quick brown fox jumps over the lazy dog", Field.Store.NO, Field.Index.ANALYZED));
+      writer.addDocument(doc);
+      writer.optimize();
+      writer.close();
+      searcher = new IndexSearcher(directory, true);
+    } catch (Exception e) {
+      fail(e.toString());
+    }
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    searcher.close();
+    super.tearDown();
+  }
+
+  private Term newTerm(String value) { return new Term(FN, value); }
+
+  private int  regexQueryNrHits(String regex, RegexCapabilities capability) throws Exception {
+    RegexQuery query = new RegexQuery( newTerm(regex));
+    
+    if ( capability != null )
+      query.setRegexImplementation(capability);
+    
+    return searcher.search(query, null, 1000).totalHits;
+  }
+
+  private int  spanRegexQueryNrHits(String regex1, String regex2, int slop, boolean ordered) throws Exception {
+    SpanRegexQuery srq1 = new SpanRegexQuery( newTerm(regex1));
+    SpanRegexQuery srq2 = new SpanRegexQuery( newTerm(regex2));
+    SpanNearQuery query = new SpanNearQuery( new SpanQuery[]{srq1, srq2}, slop, ordered);
+    
+    return searcher.search(query, null, 1000).totalHits;
+  }
+
+  public void testMatchAll() throws Exception {
+    TermEnum terms = new RegexQuery(new Term(FN, "jum.")).getEnum(searcher.getIndexReader());
+    // no term should match
+    assertNull(terms.term());
+    assertFalse(terms.next());
+  }
+
+  public void testRegex1() throws Exception {
+    assertEquals(1, regexQueryNrHits("^q.[aeiou]c.*$", null));
+  }
+
+  public void testRegex2() throws Exception {
+    assertEquals(0, regexQueryNrHits("^.[aeiou]c.*$", null));
+  }
+
+  public void testRegex3() throws Exception {
+    assertEquals(0, regexQueryNrHits("^q.[aeiou]c$", null));
+  }
+
+  public void testSpanRegex1() throws Exception {
+    assertEquals(1, spanRegexQueryNrHits("^q.[aeiou]c.*$", "dog", 6, true));
+  }
+
+  public void testSpanRegex2() throws Exception {
+    assertEquals(0, spanRegexQueryNrHits("^q.[aeiou]c.*$", "dog", 5, true));
+  }
+
+  public void testEquals() throws Exception {
+    RegexQuery query1 = new RegexQuery( newTerm("foo.*"));
+    query1.setRegexImplementation(new JakartaRegexpCapabilities());
+
+    RegexQuery query2 = new RegexQuery( newTerm("foo.*"));
+    assertFalse(query1.equals(query2));
+  }
+  
+  public void testJakartaCaseSensativeFail() throws Exception {
+    assertEquals(0, regexQueryNrHits("^.*DOG.*$", null));
+  }
+
+  public void testJavaUtilCaseSensativeFail() throws Exception {
+    assertEquals(0, regexQueryNrHits("^.*DOG.*$", null));
+  }
+  
+  public void testJakartaCaseInsensative() throws Exception {
+    assertEquals(1, regexQueryNrHits("^.*DOG.*$", new JakartaRegexpCapabilities(JakartaRegexpCapabilities.FLAG_MATCH_CASEINDEPENDENT)));
+  }
+  
+  public void testJavaUtilCaseInsensative() throws Exception {
+    assertEquals(1, regexQueryNrHits("^.*DOG.*$", new JavaUtilRegexCapabilities(JavaUtilRegexCapabilities.FLAG_CASE_INSENSITIVE)));
+  }
+
+}
+
diff --git a/lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java b/lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
new file mode 100644
index 0000000..7fd16dc
--- /dev/null
+++ b/lucene/contrib/queries/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
@@ -0,0 +1,128 @@
+package org.apache.lucene.search.regex;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MultiSearcher;
+import org.apache.lucene.search.spans.SpanFirstQuery;
+import org.apache.lucene.search.spans.SpanNearQuery;
+import org.apache.lucene.search.spans.SpanQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.LockObtainFailedException;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.LuceneTestCase;
+
+public class TestSpanRegexQuery extends LuceneTestCase {
+  
+  Directory indexStoreA = new RAMDirectory();
+
+  Directory indexStoreB = new RAMDirectory();
+
+  public void testSpanRegex() throws Exception {
+    RAMDirectory directory = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));
+    Document doc = new Document();
+    // doc.add(new Field("field", "the quick brown fox jumps over the lazy dog",
+    // Field.Store.NO, Field.Index.ANALYZED));
+    // writer.addDocument(doc);
+    // doc = new Document();
+    doc.add(new Field("field", "auto update", Field.Store.NO,
+        Field.Index.ANALYZED));
+    writer.addDocument(doc);
+    doc = new Document();
+    doc.add(new Field("field", "first auto update", Field.Store.NO,
+        Field.Index.ANALYZED));
+    writer.addDocument(doc);
+    writer.optimize();
+    writer.close();
+
+    IndexSearcher searcher = new IndexSearcher(directory, true);
+    SpanRegexQuery srq = new SpanRegexQuery(new Term("field", "aut.*"));
+    SpanFirstQuery sfq = new SpanFirstQuery(srq, 1);
+    // SpanNearQuery query = new SpanNearQuery(new SpanQuery[] {srq, stq}, 6,
+    // true);
+    int numHits = searcher.search(sfq, null, 1000).totalHits;
+    assertEquals(1, numHits);
+  }
+
+  public void testSpanRegexBug() throws CorruptIndexException, IOException {
+    createRAMDirectories();
+
+    SpanRegexQuery srq = new SpanRegexQuery(new Term("field", "a.*"));
+    SpanRegexQuery stq = new SpanRegexQuery(new Term("field", "b.*"));
+    SpanNearQuery query = new SpanNearQuery(new SpanQuery[] { srq, stq }, 6,
+        true);
+
+    // 1. Search the same store which works
+    IndexSearcher[] arrSearcher = new IndexSearcher[2];
+    arrSearcher[0] = new IndexSearcher(indexStoreA, true);
+    arrSearcher[1] = new IndexSearcher(indexStoreB, true);
+    MultiSearcher searcher = new MultiSearcher(arrSearcher);
+    int numHits = searcher.search(query, null, 1000).totalHits;
+    arrSearcher[0].close();
+    arrSearcher[1].close();
+
+    // Will fail here
+    // We expect 2 but only one matched
+    // The rewriter function only write it once on the first IndexSearcher
+    // So it's using term: a1 b1 to search on the second IndexSearcher
+    // As a result, it won't match the document in the second IndexSearcher
+    assertEquals(2, numHits);
+    indexStoreA.close();
+    indexStoreB.close();
+  }
+
+  private void createRAMDirectories() throws CorruptIndexException,
+      LockObtainFailedException, IOException {
+    // creating a document to store
+    Document lDoc = new Document();
+    lDoc.add(new Field("field", "a1 b1", Field.Store.NO,
+        Field.Index.ANALYZED_NO_NORMS));
+
+    // creating a document to store
+    Document lDoc2 = new Document();
+    lDoc2.add(new Field("field", "a2 b2", Field.Store.NO,
+        Field.Index.ANALYZED_NO_NORMS));
+
+    // creating first index writer
+    IndexWriter writerA = new IndexWriter(indexStoreA, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.CREATE));
+    writerA.addDocument(lDoc);
+    writerA.optimize();
+    writerA.close();
+
+    // creating second index writer
+    IndexWriter writerB = new IndexWriter(indexStoreB, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.CREATE));
+    writerB.addDocument(lDoc2);
+    writerB.optimize();
+    writerB.close();
+  }
+}
diff --git a/lucene/contrib/queryparser/build.xml b/lucene/contrib/queryparser/build.xml
index 59d0ec1..4f32cdd 100644
--- a/lucene/contrib/queryparser/build.xml
+++ b/lucene/contrib/queryparser/build.xml
@@ -28,7 +28,9 @@
   <!--
     NOTE: see the README.javacc for details on how to fully regenerate the parser
   -->
-  <target name="javacc" depends="init,javacc-check" if="javacc.present">
+  <target name="javacc" depends="javacc-flexible,javacc-precedence"/>
+
+  <target name="javacc-flexible" depends="init,javacc-check" if="javacc.present">
     <invoke-javacc target="src/java/org/apache/lucene/queryParser/standard/parser/StandardSyntaxParser.jj"
                    outputDir="src/java/org/apache/lucene/queryParser/standard/parser"
     />
@@ -86,5 +88,17 @@
   	                         flags="g"
   	                         byline="false"/>
   </target>
+  
+  <property name="javacc.precedence.path" location="src/java/org/apache/lucene/queryParser/precedence"/>
 
+  <target name="javacc-precedence" depends="javacc-check" description="generate precedence query parser from jj (requires javacc 3.2)">
+    <delete>
+      <fileset dir="${javacc.precedence.path}" includes="*.java">
+        <containsregexp expression="Generated.*By.*JavaCC"/>
+      </fileset>
+    </delete>
+    <invoke-javacc target="${javacc.precedence.path}/PrecedenceQueryParser.jj"
+                   outputDir="${javacc.precedence.path}"
+    />
+  </target>
 </project>
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
new file mode 100644
index 0000000..1dac672
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/analyzing/AnalyzingQueryParser.java
@@ -0,0 +1,322 @@
+package org.apache.lucene.queryParser.analyzing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.Version;
+
+/**
+ * Overrides Lucene's default QueryParser so that Fuzzy-, Prefix-, Range-, and WildcardQuerys
+ * are also passed through the given analyzer, but wild card characters (like <code>*</code>) 
+ * don't get removed from the search terms.
+ * 
+ * <p><b>Warning:</b> This class should only be used with analyzers that do not use stopwords
+ * or that add tokens. Also, several stemming analyzers are inappropriate: for example, GermanAnalyzer 
+ * will turn <code>H&auml;user</code> into <code>hau</code>, but <code>H?user</code> will 
+ * become <code>h?user</code> when using this parser and thus no match would be found (i.e.
+ * using this parser will be no improvement over QueryParser in such cases). 
+ *
+ * @version $Revision$, $Date$
+ */
+public class AnalyzingQueryParser extends org.apache.lucene.queryParser.QueryParser {
+
+  /**
+   * Constructs a query parser.
+   * @param field    the default field for query terms.
+   * @param analyzer used to find terms in the query text.
+   */
+  public AnalyzingQueryParser(Version matchVersion, String field, Analyzer analyzer) {
+    super(matchVersion, field, analyzer);
+  }
+
+  /**
+   * Called when parser
+   * parses an input term token that contains one or more wildcard
+   * characters (like <code>*</code>), but is not a prefix term token (one
+   * that has just a single * character at the end).
+   * <p>
+   * Example: will be called for <code>H?user</code> or for <code>H*user</code> 
+   * but not for <code>*user</code>.
+   * <p>
+   * Depending on analyzer and settings, a wildcard term may (most probably will)
+   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
+   * <p>
+   * Overrides super class, by passing terms through analyzer.
+   *
+   * @param  field   Name of the field query will use.
+   * @param  termStr Term token that contains one or more wild card
+   *                 characters (? or *), but is not simple prefix term
+   *
+   * @return Resulting {@link Query} built for the term
+   * @throws ParseException
+   */
+  @Override
+  protected Query getWildcardQuery(String field, String termStr) throws ParseException {
+    List<String> tlist = new ArrayList<String>();
+    List<String> wlist = new ArrayList<String>();
+    /* somewhat a hack: find/store wildcard chars
+     * in order to put them back after analyzing */
+    boolean isWithinToken = (!termStr.startsWith("?") && !termStr.startsWith("*"));
+    StringBuilder tmpBuffer = new StringBuilder();
+    char[] chars = termStr.toCharArray();
+    for (int i = 0; i < termStr.length(); i++) {
+      if (chars[i] == '?' || chars[i] == '*') {
+        if (isWithinToken) {
+          tlist.add(tmpBuffer.toString());
+          tmpBuffer.setLength(0);
+        }
+        isWithinToken = false;
+      } else {
+        if (!isWithinToken) {
+          wlist.add(tmpBuffer.toString());
+          tmpBuffer.setLength(0);
+        }
+        isWithinToken = true;
+      }
+      tmpBuffer.append(chars[i]);
+    }
+    if (isWithinToken) {
+      tlist.add(tmpBuffer.toString());
+    } else {
+      wlist.add(tmpBuffer.toString());
+    }
+
+    // get Analyzer from superclass and tokenize the term
+    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
+    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
+    
+    int countTokens = 0;
+    while (true) {
+      try {
+        if (!source.incrementToken()) break;
+      } catch (IOException e) {
+        break;
+      }
+      String term = termAtt.term();
+      if (!"".equals(term)) {
+        try {
+          tlist.set(countTokens++, term);
+        } catch (IndexOutOfBoundsException ioobe) {
+          countTokens = -1;
+        }
+      }
+    }
+    try {
+      source.close();
+    } catch (IOException e) {
+      // ignore
+    }
+
+    if (countTokens != tlist.size()) {
+      /* this means that the analyzer used either added or consumed 
+       * (common for a stemmer) tokens, and we can't build a WildcardQuery */
+      throw new ParseException("Cannot build WildcardQuery with analyzer "
+          + getAnalyzer().getClass() + " - tokens added or lost");
+    }
+
+    if (tlist.size() == 0) {
+      return null;
+    } else if (tlist.size() == 1) {
+      if (wlist != null && wlist.size() == 1) {
+        /* if wlist contains one wildcard, it must be at the end, because:
+         * 1) wildcards are not allowed in 1st position of a term by QueryParser
+         * 2) if wildcard was *not* in end, there would be *two* or more tokens */
+        return super.getWildcardQuery(field, tlist.get(0)
+            + wlist.get(0).toString());
+      } else {
+        /* we should never get here! if so, this method was called
+         * with a termStr containing no wildcard ... */
+        throw new IllegalArgumentException("getWildcardQuery called without wildcard");
+      }
+    } else {
+      /* the term was tokenized, let's rebuild to one token
+       * with wildcards put back in postion */
+      StringBuilder sb = new StringBuilder();
+      for (int i = 0; i < tlist.size(); i++) {
+        sb.append( tlist.get(i));
+        if (wlist != null && wlist.size() > i) {
+          sb.append(wlist.get(i));
+        }
+      }
+      return super.getWildcardQuery(field, sb.toString());
+    }
+  }
+
+  /**
+   * Called when parser parses an input term
+   * token that uses prefix notation; that is, contains a single '*' wildcard
+   * character as its last character. Since this is a special case
+   * of generic wildcard term, and such a query can be optimized easily,
+   * this usually results in a different query object.
+   * <p>
+   * Depending on analyzer and settings, a prefix term may (most probably will)
+   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
+   * <p>
+   * Overrides super class, by passing terms through analyzer.
+   *
+   * @param  field   Name of the field query will use.
+   * @param  termStr Term token to use for building term for the query
+   *                 (<b>without</b> trailing '*' character!)
+   *
+   * @return Resulting {@link Query} built for the term
+   * @throws ParseException
+   */
+  @Override
+  protected Query getPrefixQuery(String field, String termStr) throws ParseException {
+    // get Analyzer from superclass and tokenize the term
+    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
+    List<String> tlist = new ArrayList<String>();
+    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
+    
+    while (true) {
+      try {
+        if (!source.incrementToken()) break;
+      } catch (IOException e) {
+        break;
+      }
+      tlist.add(termAtt.term());
+    }
+
+    try {
+      source.close();
+    } catch (IOException e) {
+      // ignore
+    }
+
+    if (tlist.size() == 1) {
+      return super.getPrefixQuery(field, tlist.get(0));
+    } else {
+      /* this means that the analyzer used either added or consumed
+       * (common for a stemmer) tokens, and we can't build a PrefixQuery */
+      throw new ParseException("Cannot build PrefixQuery with analyzer "
+          + getAnalyzer().getClass()
+          + (tlist.size() > 1 ? " - token(s) added" : " - token consumed"));
+    }
+  }
+
+  /**
+   * Called when parser parses an input term token that has the fuzzy suffix (~) appended.
+   * <p>
+   * Depending on analyzer and settings, a fuzzy term may (most probably will)
+   * be lower-cased automatically. It <b>will</b> go through the default Analyzer.
+   * <p>
+   * Overrides super class, by passing terms through analyzer.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException
+   */
+  @Override
+  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity)
+      throws ParseException {
+    // get Analyzer from superclass and tokenize the term
+    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(termStr));
+    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
+    String nextToken = null;
+    boolean multipleTokens = false;
+    
+    try {
+      if (source.incrementToken()) {
+        nextToken = termAtt.term();
+      }
+      multipleTokens = source.incrementToken();
+    } catch (IOException e) {
+      nextToken = null;
+    }
+
+    try {
+      source.close();
+    } catch (IOException e) {
+      // ignore
+    }
+
+    if (multipleTokens) {
+      throw new ParseException("Cannot build FuzzyQuery with analyzer " + getAnalyzer().getClass()
+          + " - tokens were added");
+    }
+
+    return (nextToken == null) ? null : super.getFuzzyQuery(field, nextToken, minSimilarity);
+  }
+
+  /**
+   * Overrides super class, by passing terms through analyzer.
+   * @exception ParseException
+   */
+  @Override
+  protected Query getRangeQuery(String field, String part1, String part2, boolean inclusive)
+      throws ParseException {
+    // get Analyzer from superclass and tokenize the terms
+    TokenStream source = getAnalyzer().tokenStream(field, new StringReader(part1));
+    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
+    boolean multipleTokens = false;
+
+    // part1
+    try {
+      if (source.incrementToken()) {
+        part1 = termAtt.term();
+      }
+      multipleTokens = source.incrementToken();
+    } catch (IOException e) {
+      // ignore
+    }
+    try {
+      source.close();
+    } catch (IOException e) {
+      // ignore
+    }
+    if (multipleTokens) {
+      throw new ParseException("Cannot build RangeQuery with analyzer " + getAnalyzer().getClass()
+          + " - tokens were added to part1");
+    }
+
+    // part2
+    source = getAnalyzer().tokenStream(field, new StringReader(part2));
+    termAtt = source.addAttribute(TermAttribute.class);
+    
+    try {
+      if (source.incrementToken()) {
+        part2 = termAtt.term();
+      }
+      multipleTokens = source.incrementToken();
+    } catch (IOException e) {
+      // ignore
+    }
+    try {
+      source.close();
+    } catch (IOException e) {
+      // ignore
+    }
+    if (multipleTokens) {
+      throw new ParseException("Cannot build RangeQuery with analyzer " + getAnalyzer().getClass()
+          + " - tokens were added to part2");
+    }
+    return super.getRangeQuery(field, part1, part2, inclusive);
+  }
+
+}
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/analyzing/package.html b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/analyzing/package.html
new file mode 100644
index 0000000..2785a6b
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/analyzing/package.html
@@ -0,0 +1,22 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+QueryParser that passes Fuzzy-, Prefix-, Range-, and WildcardQuerys through the given analyzer.
+</body>
+</html>
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java
new file mode 100644
index 0000000..6359faf
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/complexPhrase/ComplexPhraseQueryParser.java
@@ -0,0 +1,402 @@
+package org.apache.lucene.queryParser.complexPhrase;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.MultiTermQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.spans.SpanNearQuery;
+import org.apache.lucene.search.spans.SpanNotQuery;
+import org.apache.lucene.search.spans.SpanOrQuery;
+import org.apache.lucene.search.spans.SpanQuery;
+import org.apache.lucene.search.spans.SpanTermQuery;
+import org.apache.lucene.util.Version;
+
+/**
+ * QueryParser which permits complex phrase query syntax eg "(john jon
+ * jonathan~) peters*".
+ * <p>
+ * Performs potentially multiple passes over Query text to parse any nested
+ * logic in PhraseQueries. - First pass takes any PhraseQuery content between
+ * quotes and stores for subsequent pass. All other query content is parsed as
+ * normal - Second pass parses any stored PhraseQuery content, checking all
+ * embedded clauses are referring to the same field and therefore can be
+ * rewritten as Span queries. All PhraseQuery clauses are expressed as
+ * ComplexPhraseQuery objects
+ * </p>
+ * <p>
+ * This could arguably be done in one pass using a new QueryParser but here I am
+ * working within the constraints of the existing parser as a base class. This
+ * currently simply feeds all phrase content through an analyzer to select
+ * phrase terms - any "special" syntax such as * ~ * etc are not given special
+ * status
+ * </p>
+ * 
+ */
+public class ComplexPhraseQueryParser extends QueryParser {
+  private ArrayList<ComplexPhraseQuery> complexPhrases = null;
+
+  private boolean isPass2ResolvingPhrases;
+
+  private ComplexPhraseQuery currentPhraseQuery = null;
+
+  public ComplexPhraseQueryParser(Version matchVersion, String f, Analyzer a) {
+    super(matchVersion, f, a);
+  }
+
+  @Override
+  protected Query getFieldQuery(String field, String queryText, int slop) {
+    ComplexPhraseQuery cpq = new ComplexPhraseQuery(field, queryText, slop);
+    complexPhrases.add(cpq); // add to list of phrases to be parsed once
+    // we
+    // are through with this pass
+    return cpq;
+  }
+
+  @Override
+  public Query parse(String query) throws ParseException {
+    if (isPass2ResolvingPhrases) {
+      MultiTermQuery.RewriteMethod oldMethod = getMultiTermRewriteMethod();
+      try {
+        // Temporarily force BooleanQuery rewrite so that Parser will
+        // generate visible
+        // collection of terms which we can convert into SpanQueries.
+        // ConstantScoreRewrite mode produces an
+        // opaque ConstantScoreQuery object which cannot be interrogated for
+        // terms in the same way a BooleanQuery can.
+        // QueryParser is not guaranteed threadsafe anyway so this temporary
+        // state change should not
+        // present an issue
+        setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
+        return super.parse(query);
+      } finally {
+        setMultiTermRewriteMethod(oldMethod);
+      }
+    }
+
+    // First pass - parse the top-level query recording any PhraseQuerys
+    // which will need to be resolved
+    complexPhrases = new ArrayList<ComplexPhraseQuery>();
+    Query q = super.parse(query);
+
+    // Perform second pass, using this QueryParser to parse any nested
+    // PhraseQueries with different
+    // set of syntax restrictions (i.e. all fields must be same)
+    isPass2ResolvingPhrases = true;
+    try {
+      for (Iterator<ComplexPhraseQuery> iterator = complexPhrases.iterator(); iterator.hasNext();) {
+        currentPhraseQuery = iterator.next();
+        // in each phrase, now parse the contents between quotes as a
+        // separate parse operation
+        currentPhraseQuery.parsePhraseElements(this);
+      }
+    } finally {
+      isPass2ResolvingPhrases = false;
+    }
+    return q;
+  }
+
+  // There is No "getTermQuery throws ParseException" method to override so
+  // unfortunately need
+  // to throw a runtime exception here if a term for another field is embedded
+  // in phrase query
+  @Override
+  protected Query newTermQuery(Term term) {
+    if (isPass2ResolvingPhrases) {
+      try {
+        checkPhraseClauseIsForSameField(term.field());
+      } catch (ParseException pe) {
+        throw new RuntimeException("Error parsing complex phrase", pe);
+      }
+    }
+    return super.newTermQuery(term);
+  }
+
+  // Helper method used to report on any clauses that appear in query syntax
+  private void checkPhraseClauseIsForSameField(String field)
+      throws ParseException {
+    if (!field.equals(currentPhraseQuery.field)) {
+      throw new ParseException("Cannot have clause for field \"" + field
+          + "\" nested in phrase " + " for field \"" + currentPhraseQuery.field
+          + "\"");
+    }
+  }
+
+  @Override
+  protected Query getWildcardQuery(String field, String termStr)
+      throws ParseException {
+    if (isPass2ResolvingPhrases) {
+      checkPhraseClauseIsForSameField(field);
+    }
+    return super.getWildcardQuery(field, termStr);
+  }
+
+  @Override
+  protected Query getRangeQuery(String field, String part1, String part2,
+      boolean inclusive) throws ParseException {
+    if (isPass2ResolvingPhrases) {
+      checkPhraseClauseIsForSameField(field);
+    }
+    return super.getRangeQuery(field, part1, part2, inclusive);
+  }
+
+  @Override
+  protected Query newRangeQuery(String field, String part1, String part2,
+      boolean inclusive) {
+    if (isPass2ResolvingPhrases) {
+      // Must use old-style RangeQuery in order to produce a BooleanQuery
+      // that can be turned into SpanOr clause
+      TermRangeQuery rangeQuery = new TermRangeQuery(field, part1, part2, inclusive, inclusive,
+          getRangeCollator());
+      rangeQuery.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
+      return rangeQuery;
+    }
+    return super.newRangeQuery(field, part1, part2, inclusive);
+  }
+
+  @Override
+  protected Query getFuzzyQuery(String field, String termStr,
+      float minSimilarity) throws ParseException {
+    if (isPass2ResolvingPhrases) {
+      checkPhraseClauseIsForSameField(field);
+    }
+    return super.getFuzzyQuery(field, termStr, minSimilarity);
+  }
+
+  /*
+   * Used to handle the query content in between quotes and produced Span-based
+   * interpretations of the clauses.
+   */
+  static class ComplexPhraseQuery extends Query {
+
+    String field;
+
+    String phrasedQueryStringContents;
+
+    int slopFactor;
+
+    private Query contents;
+
+    public ComplexPhraseQuery(String field, String phrasedQueryStringContents,
+        int slopFactor) {
+      super();
+      this.field = field;
+      this.phrasedQueryStringContents = phrasedQueryStringContents;
+      this.slopFactor = slopFactor;
+    }
+
+    // Called by ComplexPhraseQueryParser for each phrase after the main
+    // parse
+    // thread is through
+    protected void parsePhraseElements(QueryParser qp) throws ParseException {
+      // TODO ensure that field-sensitivity is preserved ie the query
+      // string below is parsed as
+      // field+":("+phrasedQueryStringContents+")"
+      // but this will need code in rewrite to unwrap the first layer of
+      // boolean query
+      contents = qp.parse(phrasedQueryStringContents);
+    }
+
+    @Override
+    public Query rewrite(IndexReader reader) throws IOException {
+      // ArrayList spanClauses = new ArrayList();
+      if (contents instanceof TermQuery) {
+        return contents;
+      }
+      // Build a sequence of Span clauses arranged in a SpanNear - child
+      // clauses can be complex
+      // Booleans e.g. nots and ors etc
+      int numNegatives = 0;
+      if (!(contents instanceof BooleanQuery)) {
+        throw new IllegalArgumentException("Unknown query type \""
+            + contents.getClass().getName()
+            + "\" found in phrase query string \"" + phrasedQueryStringContents
+            + "\"");
+      }
+      BooleanQuery bq = (BooleanQuery) contents;
+      BooleanClause[] bclauses = bq.getClauses();
+      SpanQuery[] allSpanClauses = new SpanQuery[bclauses.length];
+      // For all clauses e.g. one* two~
+      for (int i = 0; i < bclauses.length; i++) {
+        // HashSet bclauseterms=new HashSet();
+        Query qc = bclauses[i].getQuery();
+        // Rewrite this clause e.g one* becomes (one OR onerous)
+        qc = qc.rewrite(reader);
+        if (bclauses[i].getOccur().equals(BooleanClause.Occur.MUST_NOT)) {
+          numNegatives++;
+        }
+
+        if (qc instanceof BooleanQuery) {
+          ArrayList<SpanQuery> sc = new ArrayList<SpanQuery>();
+          addComplexPhraseClause(sc, (BooleanQuery) qc);
+          if (sc.size() > 0) {
+            allSpanClauses[i] = sc.get(0);
+          } else {
+            // Insert fake term e.g. phrase query was for "Fred Smithe*" and
+            // there were no "Smithe*" terms - need to
+            // prevent match on just "Fred".
+            allSpanClauses[i] = new SpanTermQuery(new Term(field,
+                "Dummy clause because no terms found - must match nothing"));
+          }
+        } else {
+          if (qc instanceof TermQuery) {
+            TermQuery tq = (TermQuery) qc;
+            allSpanClauses[i] = new SpanTermQuery(tq.getTerm());
+          } else {
+            throw new IllegalArgumentException("Unknown query type \""
+                + qc.getClass().getName()
+                + "\" found in phrase query string \""
+                + phrasedQueryStringContents + "\"");
+          }
+
+        }
+      }
+      if (numNegatives == 0) {
+        // The simple case - no negative elements in phrase
+        return new SpanNearQuery(allSpanClauses, slopFactor, true);
+      }
+      // Complex case - we have mixed positives and negatives in the
+      // sequence.
+      // Need to return a SpanNotQuery
+      ArrayList<SpanQuery> positiveClauses = new ArrayList<SpanQuery>();
+      for (int j = 0; j < allSpanClauses.length; j++) {
+        if (!bclauses[j].getOccur().equals(BooleanClause.Occur.MUST_NOT)) {
+          positiveClauses.add(allSpanClauses[j]);
+        }
+      }
+
+      SpanQuery[] includeClauses = positiveClauses
+          .toArray(new SpanQuery[positiveClauses.size()]);
+
+      SpanQuery include = null;
+      if (includeClauses.length == 1) {
+        include = includeClauses[0]; // only one positive clause
+      } else {
+        // need to increase slop factor based on gaps introduced by
+        // negatives
+        include = new SpanNearQuery(includeClauses, slopFactor + numNegatives,
+            true);
+      }
+      // Use sequence of positive and negative values as the exclude.
+      SpanNearQuery exclude = new SpanNearQuery(allSpanClauses, slopFactor,
+          true);
+      SpanNotQuery snot = new SpanNotQuery(include, exclude);
+      return snot;
+    }
+
+    private void addComplexPhraseClause(List<SpanQuery> spanClauses, BooleanQuery qc) {
+      ArrayList<SpanQuery> ors = new ArrayList<SpanQuery>();
+      ArrayList<SpanQuery> nots = new ArrayList<SpanQuery>();
+      BooleanClause[] bclauses = qc.getClauses();
+
+      // For all clauses e.g. one* two~
+      for (int i = 0; i < bclauses.length; i++) {
+        Query childQuery = bclauses[i].getQuery();
+
+        // select the list to which we will add these options
+        ArrayList<SpanQuery> chosenList = ors;
+        if (bclauses[i].getOccur() == BooleanClause.Occur.MUST_NOT) {
+          chosenList = nots;
+        }
+
+        if (childQuery instanceof TermQuery) {
+          TermQuery tq = (TermQuery) childQuery;
+          SpanTermQuery stq = new SpanTermQuery(tq.getTerm());
+          stq.setBoost(tq.getBoost());
+          chosenList.add(stq);
+        } else if (childQuery instanceof BooleanQuery) {
+          BooleanQuery cbq = (BooleanQuery) childQuery;
+          addComplexPhraseClause(chosenList, cbq);
+        } else {
+          // TODO alternatively could call extract terms here?
+          throw new IllegalArgumentException("Unknown query type:"
+              + childQuery.getClass().getName());
+        }
+      }
+      if (ors.size() == 0) {
+        return;
+      }
+      SpanOrQuery soq = new SpanOrQuery(ors
+          .toArray(new SpanQuery[ors.size()]));
+      if (nots.size() == 0) {
+        spanClauses.add(soq);
+      } else {
+        SpanOrQuery snqs = new SpanOrQuery(nots
+            .toArray(new SpanQuery[nots.size()]));
+        SpanNotQuery snq = new SpanNotQuery(soq, snqs);
+        spanClauses.add(snq);
+      }
+    }
+
+    @Override
+    public String toString(String field) {
+      return "\"" + phrasedQueryStringContents + "\"";
+    }
+
+    @Override
+    public int hashCode() {
+      final int prime = 31;
+      int result = 1;
+      result = prime * result + ((field == null) ? 0 : field.hashCode());
+      result = prime
+          * result
+          + ((phrasedQueryStringContents == null) ? 0
+              : phrasedQueryStringContents.hashCode());
+      result = prime * result + slopFactor;
+      return result;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+      if (this == obj)
+        return true;
+      if (obj == null)
+        return false;
+      if (getClass() != obj.getClass())
+        return false;
+      ComplexPhraseQuery other = (ComplexPhraseQuery) obj;
+      if (field == null) {
+        if (other.field != null)
+          return false;
+      } else if (!field.equals(other.field))
+        return false;
+      if (phrasedQueryStringContents == null) {
+        if (other.phrasedQueryStringContents != null)
+          return false;
+      } else if (!phrasedQueryStringContents
+          .equals(other.phrasedQueryStringContents))
+        return false;
+      if (slopFactor != other.slopFactor)
+        return false;
+      return true;
+    }
+  }
+}
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/complexPhrase/package.html b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/complexPhrase/package.html
new file mode 100644
index 0000000..ade19fc
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/complexPhrase/package.html
@@ -0,0 +1,22 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+QueryParser which permits complex phrase query syntax eg "(john jon jonathan~) peters*"
+</body>
+</html>
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtendableQueryParser.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtendableQueryParser.java
new file mode 100644
index 0000000..1533d11
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtendableQueryParser.java
@@ -0,0 +1,142 @@
+package org.apache.lucene.queryParser.ext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryParser.ext.Extensions.Pair;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.util.Version;
+
+/**
+ * The {@link ExtendableQueryParser} enables arbitrary query parser extension
+ * based on a customizable field naming scheme. The lucene query syntax allows
+ * implicit and explicit field definitions as query prefix followed by a colon
+ * (':') character. The {@link ExtendableQueryParser} allows to encode extension
+ * keys into the field symbol associated with a registered instance of
+ * {@link ParserExtension}. A customizable separation character separates the
+ * extension key from the actual field symbol. The {@link ExtendableQueryParser}
+ * splits (@see {@link Extensions#splitExtensionField(String, String)}) the
+ * extension key from the field symbol and tries to resolve the associated
+ * {@link ParserExtension}. If the parser can't resolve the key or the field
+ * token does not contain a separation character, {@link ExtendableQueryParser}
+ * yields the same behavior as its super class {@link QueryParser}. Otherwise,
+ * if the key is associated with a {@link ParserExtension} instance, the parser
+ * builds an instance of {@link ExtensionQuery} to be processed by
+ * {@link ParserExtension#parse(ExtensionQuery)}.If a extension field does not
+ * contain a field part the default field for the query will be used.
+ * <p>
+ * To guarantee that an extension field is processed with its associated
+ * extension, the extension query part must escape any special characters like
+ * '*' or '['. If the extension query contains any whitespace characters, the
+ * extension query part must be enclosed in quotes.
+ * Example ('_' used as separation character):
+ * <pre>
+ *   title_customExt:"Apache Lucene\?" OR content_customExt:prefix\*
+ * </pre>
+ * 
+ * Search on the default field:
+ * <pre>
+ *   _customExt:"Apache Lucene\?" OR _customExt:prefix\*
+ * </pre>
+ * </p>
+ * <p>
+ * The {@link ExtendableQueryParser} itself does not implement the logic how
+ * field and extension key are separated or ordered. All logic regarding the
+ * extension key and field symbol parsing is located in {@link Extensions}.
+ * Customized extension schemes should be implemented by sub-classing
+ * {@link Extensions}.
+ * </p>
+ * <p>
+ * For details about the default encoding scheme see {@link Extensions}.
+ * </p>
+ * 
+ * @see Extensions
+ * @see ParserExtension
+ * @see ExtensionQuery
+ */
+public class ExtendableQueryParser extends QueryParser {
+
+  private final String defaultField;
+  private final Extensions extensions;
+
+  /**
+   * Default empty extensions instance
+   */
+  private static final Extensions DEFAULT_EXTENSION = new Extensions();
+
+  /**
+   * Creates a new {@link ExtendableQueryParser} instance
+   * 
+   * @param matchVersion
+   *          the lucene version to use.
+   * @param f
+   *          the default query field
+   * @param a
+   *          the analyzer used to find terms in a query string
+   */
+  public ExtendableQueryParser(final Version matchVersion, final String f,
+      final Analyzer a) {
+    this(matchVersion, f, a, DEFAULT_EXTENSION);
+
+  }
+
+  /**
+   * Creates a new {@link ExtendableQueryParser} instance
+   * 
+   * @param matchVersion
+   *          the lucene version to use.
+   * @param f
+   *          the default query field
+   * @param a
+   *          the analyzer used to find terms in a query string
+   * @param ext
+   *          the query parser extensions
+   */
+  public ExtendableQueryParser(final Version matchVersion, final String f,
+      final Analyzer a, final Extensions ext) {
+    super(matchVersion, f, a);
+    this.defaultField = f;
+    this.extensions = ext;
+  }
+
+  /**
+   * Returns the extension field delimiter character.
+   * 
+   * @return the extension field delimiter character.
+   */
+  public char getExtensionFieldDelimiter() {
+    return extensions.getExtensionFieldDelimiter();
+  }
+
+  @Override
+  protected Query getFieldQuery(final String field, final String queryText)
+      throws ParseException {
+    final Pair<String,String> splitExtensionField = this.extensions
+        .splitExtensionField(defaultField, field);
+    final ParserExtension extension = this.extensions
+        .getExtension(splitExtensionField.cud);
+    if (extension != null) {
+      return extension.parse(new ExtensionQuery(this, splitExtensionField.cur,
+          queryText));
+    }
+    return super.getFieldQuery(field, queryText);
+  }
+
+}
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtensionQuery.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtensionQuery.java
new file mode 100644
index 0000000..f84faf8
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ExtensionQuery.java
@@ -0,0 +1,75 @@
+package org.apache.lucene.queryParser.ext;
+
+import org.apache.lucene.queryParser.QueryParser;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * {@link ExtensionQuery} holds all query components extracted from the original
+ * query string like the query field and the extension query string.
+ * 
+ * @see Extensions
+ * @see ExtendableQueryParser
+ * @see ParserExtension
+ */
+public class ExtensionQuery {
+
+  private final String field;
+  private final String rawQueryString;
+  private final QueryParser topLevelParser;
+
+  /**
+   * Creates a new {@link ExtensionQuery}
+   * 
+   * @param field
+   *          the query field
+   * @param rawQueryString
+   *          the raw extension query string
+   */
+  public ExtensionQuery(QueryParser topLevelParser, String field, String rawQueryString) {
+    this.field = field;
+    this.rawQueryString = rawQueryString;
+    this.topLevelParser = topLevelParser;
+  }
+
+  /**
+   * Returns the query field
+   * 
+   * @return the query field
+   */
+  public String getField() {
+    return field;
+  }
+
+  /**
+   * Returns the raw extension query string
+   * 
+   * @return the raw extension query string
+   */
+  public String getRawQueryString() {
+    return rawQueryString;
+  }
+  
+  /**
+   * Returns the top level parser which created this {@link ExtensionQuery} 
+   * @return the top level parser which created this {@link ExtensionQuery}
+   */
+  public QueryParser getTopLevelParser() {
+    return topLevelParser;
+  }
+}
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/Extensions.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/Extensions.java
new file mode 100644
index 0000000..edf763d
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/Extensions.java
@@ -0,0 +1,217 @@
+package org.apache.lucene.queryParser.ext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.queryParser.QueryParser;
+
+/**
+ * The {@link Extensions} class represents an extension mapping to associate
+ * {@link ParserExtension} instances with extension keys. An extension key is a
+ * string encoded into a Lucene standard query parser field symbol recognized by
+ * {@link ExtendableQueryParser}. The query parser passes each extension field
+ * token to {@link #splitExtensionField(String, String)} to separate the
+ * extension key from the field identifier.
+ * <p>
+ * In addition to the key to extension mapping this class also defines the field
+ * name overloading scheme. {@link ExtendableQueryParser} uses the given
+ * extension to split the actual field name and extension key by calling
+ * {@link #splitExtensionField(String, String)}. To change the order or the key
+ * / field name encoding scheme users can subclass {@link Extensions} to
+ * implement their own.
+ * 
+ * @see ExtendableQueryParser
+ * @see ParserExtension
+ */
+public class Extensions {
+  private final Map<String,ParserExtension> extensions = new HashMap<String,ParserExtension>();
+  private final char extensionFieldDelimiter;
+  /**
+   * The default extension field delimiter character. This constant is set to
+   * ':'
+   */
+  public static final char DEFAULT_EXTENSION_FIELD_DELIMITER = ':';
+
+  /**
+   * Creates a new {@link Extensions} instance with the
+   * {@link #DEFAULT_EXTENSION_FIELD_DELIMITER} as a delimiter character.
+   */
+  public Extensions() {
+    this(DEFAULT_EXTENSION_FIELD_DELIMITER);
+  }
+
+  /**
+   * Creates a new {@link Extensions} instance
+   * 
+   * @param extensionFieldDelimiter
+   *          the extensions field delimiter character
+   */
+  public Extensions(char extensionFieldDelimiter) {
+    this.extensionFieldDelimiter = extensionFieldDelimiter;
+  }
+
+  /**
+   * Adds a new {@link ParserExtension} instance associated with the given key.
+   * 
+   * @param key
+   *          the parser extension key
+   * @param extension
+   *          the parser extension
+   */
+  public void add(String key, ParserExtension extension) {
+    this.extensions.put(key, extension);
+  }
+
+  /**
+   * Returns the {@link ParserExtension} instance for the given key or
+   * <code>null</code> if no extension can be found for the key.
+   * 
+   * @param key
+   *          the extension key
+   * @return the {@link ParserExtension} instance for the given key or
+   *         <code>null</code> if no extension can be found for the key.
+   */
+  public final ParserExtension getExtension(String key) {
+    return this.extensions.get(key);
+  }
+
+  /**
+   * Returns the extension field delimiter
+   * 
+   * @return the extension field delimiter
+   */
+  public char getExtensionFieldDelimiter() {
+    return extensionFieldDelimiter;
+  }
+
+  /**
+   * Splits a extension field and returns the field / extension part as a
+   * {@link Pair}. This method tries to split on the first occurrence of the
+   * extension field delimiter, if the delimiter is not present in the string
+   * the result will contain a <code>null</code> value for the extension key and
+   * the given field string as the field value. If the given extension field
+   * string contains no field identifier the result pair will carry the given
+   * default field as the field value.
+   * 
+   * @param defaultField
+   *          the default query field
+   * @param field
+   *          the extension field string
+   * @return a {@link Pair} with the field name as the {@link Pair#cur} and the
+   *         extension key as the {@link Pair#cud}
+   */
+  public Pair<String,String> splitExtensionField(String defaultField,
+      String field) {
+    int indexOf = field.indexOf(this.extensionFieldDelimiter);
+    if (indexOf < 0)
+      return new Pair<String,String>(field, null);
+    final String indexField = indexOf == 0 ? defaultField : field.substring(0,
+        indexOf);
+    final String extensionKey = field.substring(indexOf + 1);
+    return new Pair<String,String>(indexField, extensionKey);
+
+  }
+
+  /**
+   * Escapes an extension field. The default implementation is equivalent to
+   * {@link QueryParser#escape(String)}.
+   * 
+   * @param extfield
+   *          the extension field identifier
+   * @return the extension field identifier with all special chars escaped with
+   *         a backslash character.
+   */
+  public String escapeExtensionField(String extfield) {
+    return QueryParser.escape(extfield);
+  }
+
+  /**
+   * Builds an extension field string from a given extension key and the default
+   * query field. The default field and the key are delimited with the extension
+   * field delimiter character. This method makes no assumption about the order
+   * of the extension key and the field. By default the extension key is
+   * appended to the end of the returned string while the field is added to the
+   * beginning. Special Query characters are escaped in the result.
+   * <p>
+   * Note: {@link Extensions} subclasses must maintain the contract between
+   * {@link #buildExtensionField(String)} and
+   * {@link #splitExtensionField(String, String)} where the latter inverts the
+   * former.
+   * </p>
+   */
+  public String buildExtensionField(String extensionKey) {
+    return buildExtensionField(extensionKey, "");
+  }
+
+  /**
+   * Builds an extension field string from a given extension key and the
+   * extensions field. The field and the key are delimited with the extension
+   * field delimiter character. This method makes no assumption about the order
+   * of the extension key and the field. By default the extension key is
+   * appended to the end of the returned string while the field is added to the
+   * beginning. Special Query characters are escaped in the result.
+   * <p>
+   * Note: {@link Extensions} subclasses must maintain the contract between
+   * {@link #buildExtensionField(String, String)} and
+   * {@link #splitExtensionField(String, String)} where the latter inverts the
+   * former.
+   * </p>
+   * 
+   * @param extensionKey
+   *          the extension key
+   * @param field
+   *          the field to apply the extension on.
+   * @return escaped extension field identifier
+   * @see #buildExtensionField(String) to use the default query field
+   */
+  public String buildExtensionField(String extensionKey, String field) {
+    StringBuilder builder = new StringBuilder(field);
+    builder.append(this.extensionFieldDelimiter);
+    builder.append(extensionKey);
+    return escapeExtensionField(builder.toString());
+  }
+
+  /**
+   * This class represents a generic pair.
+   * 
+   * @param <Cur>
+   *          the pairs first element
+   * @param <Cud>
+   *          the pairs last element of the pair.
+   */
+  public static class Pair<Cur,Cud> {
+
+    public final Cur cur;
+    public final Cud cud;
+
+    /**
+     * Creates a new Pair
+     * 
+     * @param cur
+     *          the pairs first element
+     * @param cud
+     *          the pairs last element
+     */
+    public Pair(Cur cur, Cud cud) {
+      this.cur = cur;
+      this.cud = cud;
+    }
+  }
+
+}
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ParserExtension.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ParserExtension.java
new file mode 100644
index 0000000..b173858
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/ParserExtension.java
@@ -0,0 +1,53 @@
+package org.apache.lucene.queryParser.ext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.Query;
+
+/**
+ * This class represents an extension base class to the Lucene standard
+ * {@link QueryParser}. The {@link QueryParser} is generated by the JavaCC
+ * parser generator. Changing or adding functionality or syntax in the standard
+ * query parser requires changes to the JavaCC source file. To enable extending
+ * the standard query parser without changing the JavaCC sources and re-generate
+ * the parser the {@link ParserExtension} can be customized and plugged into an
+ * instance of {@link ExtendableQueryParser}, a direct subclass of
+ * {@link QueryParser}.
+ * 
+ * @see Extensions
+ * @see ExtendableQueryParser
+ */
+public abstract class ParserExtension {
+
+  /**
+   * Processes the given {@link ExtensionQuery} and returns a corresponding
+   * {@link Query} instance. Subclasses must either return a {@link Query}
+   * instance or raise a {@link ParseException}. This method must not return
+   * <code>null</code>.
+   * 
+   * @param query
+   *          the extension query
+   * @return a new query instance
+   * @throws ParseException
+   *           if the query can not be parsed.
+   */
+  public abstract Query parse(final ExtensionQuery query) throws ParseException;
+
+}
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/package.html b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/package.html
new file mode 100644
index 0000000..13549a8
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/ext/package.html
@@ -0,0 +1,22 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html><head></head>
+<body>
+Extendable QueryParser provides a simple and flexible extension mechanism by overloading query field names.
+</body>
+</html>
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/CharStream.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/CharStream.java
new file mode 100644
index 0000000..9a84eb5
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/CharStream.java
@@ -0,0 +1,112 @@
+/* Generated By:JavaCC: Do not edit this line. CharStream.java Version 4.1 */
+/* JavaCCOptions:STATIC=false */
+package org.apache.lucene.queryParser.precedence;
+
+/**
+ * This interface describes a character stream that maintains line and
+ * column number positions of the characters.  It also has the capability
+ * to backup the stream to some extent.  An implementation of this
+ * interface is used in the TokenManager implementation generated by
+ * JavaCCParser.
+ *
+ * All the methods except backup can be implemented in any fashion. backup
+ * needs to be implemented correctly for the correct operation of the lexer.
+ * Rest of the methods are all used to get information like line number,
+ * column number and the String that constitutes a token and are not used
+ * by the lexer. Hence their implementation won't affect the generated lexer's
+ * operation.
+ */
+
+public interface CharStream {
+
+  /**
+   * Returns the next character from the selected input.  The method
+   * of selecting the input is the responsibility of the class
+   * implementing this interface.  Can throw any java.io.IOException.
+   */
+  char readChar() throws java.io.IOException;
+
+  /**
+   * Returns the column position of the character last read.
+   * @deprecated
+   * @see #getEndColumn
+   */
+  int getColumn();
+
+  /**
+   * Returns the line number of the character last read.
+   * @deprecated
+   * @see #getEndLine
+   */
+  int getLine();
+
+  /**
+   * Returns the column number of the last character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getEndColumn();
+
+  /**
+   * Returns the line number of the last character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getEndLine();
+
+  /**
+   * Returns the column number of the first character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getBeginColumn();
+
+  /**
+   * Returns the line number of the first character for current token (being
+   * matched after the last call to BeginTOken).
+   */
+  int getBeginLine();
+
+  /**
+   * Backs up the input stream by amount steps. Lexer calls this method if it
+   * had already read some characters, but could not use them to match a
+   * (longer) token. So, they will be used again as the prefix of the next
+   * token and it is the implemetation's responsibility to do this right.
+   */
+  void backup(int amount);
+
+  /**
+   * Returns the next character that marks the beginning of the next token.
+   * All characters must remain in the buffer between two successive calls
+   * to this method to implement backup correctly.
+   */
+  char BeginToken() throws java.io.IOException;
+
+  /**
+   * Returns a string made up of characters from the marked token beginning
+   * to the current buffer position. Implementations have the choice of returning
+   * anything that they want to. For example, for efficiency, one might decide
+   * to just return null, which is a valid implementation.
+   */
+  String GetImage();
+
+  /**
+   * Returns an array of characters that make up the suffix of length 'len' for
+   * the currently matched token. This is used to build up the matched string
+   * for use in actions in the case of MORE. A simple and inefficient
+   * implementation of this is as follows :
+   *
+   *   {
+   *      String t = GetImage();
+   *      return t.substring(t.length() - len, t.length()).toCharArray();
+   *   }
+   */
+  char[] GetSuffix(int len);
+
+  /**
+   * The lexer calls this function to indicate that it is done with the stream
+   * and hence implementations can free any resources held by this class.
+   * Again, the body of this function can be just empty and it will not
+   * affect the lexer's operation.
+   */
+  void Done();
+
+}
+/* JavaCC - OriginalChecksum=8cc617b193267dc876ef9699367c8186 (do not edit this line) */
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java
new file mode 100644
index 0000000..f7b2879
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/FastCharStream.java
@@ -0,0 +1,123 @@
+// FastCharStream.java
+package org.apache.lucene.queryParser.precedence;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import java.io.*;
+
+/** An efficient implementation of JavaCC's CharStream interface.  <p>Note that
+ * this does not do line-number counting, but instead keeps track of the
+ * character position of the token in the input, as required by Lucene's {@link
+ * org.apache.lucene.analysis.Token} API. */
+public final class FastCharStream implements CharStream {
+  char[] buffer = null;
+
+  int bufferLength = 0;				  // end of valid chars
+  int bufferPosition = 0;			  // next char to read
+
+  int tokenStart = 0;				  // offset in buffer
+  int bufferStart = 0;				  // position in file of buffer
+
+  Reader input;					  // source of chars
+
+  /** Constructs from a Reader. */
+  public FastCharStream(Reader r) {
+    input = r;
+  }
+
+  public final char readChar() throws IOException {
+    if (bufferPosition >= bufferLength)
+      refill();
+    return buffer[bufferPosition++];
+  }
+
+  private final void refill() throws IOException {
+    int newPosition = bufferLength - tokenStart;
+
+    if (tokenStart == 0) {			  // token won't fit in buffer
+      if (buffer == null) {			  // first time: alloc buffer
+	buffer = new char[2048];
+      } else if (bufferLength == buffer.length) { // grow buffer
+	char[] newBuffer = new char[buffer.length*2];
+	System.arraycopy(buffer, 0, newBuffer, 0, bufferLength);
+	buffer = newBuffer;
+      }
+    } else {					  // shift token to front
+      System.arraycopy(buffer, tokenStart, buffer, 0, newPosition);
+    }
+
+    bufferLength = newPosition;			  // update state
+    bufferPosition = newPosition;
+    bufferStart += tokenStart;
+    tokenStart = 0;
+
+    int charsRead =				  // fill space in buffer
+      input.read(buffer, newPosition, buffer.length-newPosition);
+    if (charsRead == -1)
+      throw new IOException("read past eof");
+    else
+      bufferLength += charsRead;
+  }
+
+  public final char BeginToken() throws IOException {
+    tokenStart = bufferPosition;
+    return readChar();
+  }
+
+  public final void backup(int amount) {
+    bufferPosition -= amount;
+  }
+
+  public final String GetImage() {
+    return new String(buffer, tokenStart, bufferPosition - tokenStart);
+  }
+
+  public final char[] GetSuffix(int len) {
+    char[] value = new char[len];
+    System.arraycopy(buffer, bufferPosition - len, value, 0, len);
+    return value;
+  }
+
+  public final void Done() {
+    try {
+      input.close();
+    } catch (IOException e) {
+      System.err.println("Caught: " + e + "; ignoring.");
+    }
+  }
+
+  public final int getColumn() {
+    return bufferStart + bufferPosition;
+  }
+  public final int getLine() {
+    return 1;
+  }
+  public final int getEndColumn() {
+    return bufferStart + bufferPosition;
+  }
+  public final int getEndLine() {
+    return 1;
+  }
+  public final int getBeginColumn() {
+    return bufferStart + tokenStart;
+  }
+  public final int getBeginLine() {
+    return 1;
+  }
+}
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/ParseException.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/ParseException.java
new file mode 100644
index 0000000..6e9ec48
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/ParseException.java
@@ -0,0 +1,198 @@
+/* Generated By:JavaCC: Do not edit this line. ParseException.java Version 4.1 */
+/* JavaCCOptions:KEEP_LINE_COL=null */
+package org.apache.lucene.queryParser.precedence;
+
+/**
+ * This exception is thrown when parse errors are encountered.
+ * You can explicitly create objects of this exception type by
+ * calling the method generateParseException in the generated
+ * parser.
+ *
+ * You can modify this class to customize your error reporting
+ * mechanisms so long as you retain the public fields.
+ */
+public class ParseException extends Exception {
+
+  /**
+   * This constructor is used by the method "generateParseException"
+   * in the generated parser.  Calling this constructor generates
+   * a new object of this type with the fields "currentToken",
+   * "expectedTokenSequences", and "tokenImage" set.  The boolean
+   * flag "specialConstructor" is also set to true to indicate that
+   * this constructor was used to create this object.
+   * This constructor calls its super class with the empty string
+   * to force the "toString" method of parent class "Throwable" to
+   * print the error message in the form:
+   *     ParseException: <result of getMessage>
+   */
+  public ParseException(Token currentTokenVal,
+                        int[][] expectedTokenSequencesVal,
+                        String[] tokenImageVal
+                       )
+  {
+    super("");
+    specialConstructor = true;
+    currentToken = currentTokenVal;
+    expectedTokenSequences = expectedTokenSequencesVal;
+    tokenImage = tokenImageVal;
+  }
+
+  /**
+   * The following constructors are for use by you for whatever
+   * purpose you can think of.  Constructing the exception in this
+   * manner makes the exception behave in the normal way - i.e., as
+   * documented in the class "Throwable".  The fields "errorToken",
+   * "expectedTokenSequences", and "tokenImage" do not contain
+   * relevant information.  The JavaCC generated code does not use
+   * these constructors.
+   */
+
+  public ParseException() {
+    super();
+    specialConstructor = false;
+  }
+
+  /** Constructor with message. */
+  public ParseException(String message) {
+    super(message);
+    specialConstructor = false;
+  }
+
+  /**
+   * This variable determines which constructor was used to create
+   * this object and thereby affects the semantics of the
+   * "getMessage" method (see below).
+   */
+  protected boolean specialConstructor;
+
+  /**
+   * This is the last token that has been consumed successfully.  If
+   * this object has been created due to a parse error, the token
+   * followng this token will (therefore) be the first error token.
+   */
+  public Token currentToken;
+
+  /**
+   * Each entry in this array is an array of integers.  Each array
+   * of integers represents a sequence of tokens (by their ordinal
+   * values) that is expected at this point of the parse.
+   */
+  public int[][] expectedTokenSequences;
+
+  /**
+   * This is a reference to the "tokenImage" array of the generated
+   * parser within which the parse error occurred.  This array is
+   * defined in the generated ...Constants interface.
+   */
+  public String[] tokenImage;
+
+  /**
+   * This method has the standard behavior when this object has been
+   * created using the standard constructors.  Otherwise, it uses
+   * "currentToken" and "expectedTokenSequences" to generate a parse
+   * error message and returns it.  If this object has been created
+   * due to a parse error, and you do not catch it (it gets thrown
+   * from the parser), then this method is called during the printing
+   * of the final stack trace, and hence the correct error message
+   * gets displayed.
+   */
+  public String getMessage() {
+    if (!specialConstructor) {
+      return super.getMessage();
+    }
+    StringBuffer expected = new StringBuffer();
+    int maxSize = 0;
+    for (int i = 0; i < expectedTokenSequences.length; i++) {
+      if (maxSize < expectedTokenSequences[i].length) {
+        maxSize = expectedTokenSequences[i].length;
+      }
+      for (int j = 0; j < expectedTokenSequences[i].length; j++) {
+        expected.append(tokenImage[expectedTokenSequences[i][j]]).append(' ');
+      }
+      if (expectedTokenSequences[i][expectedTokenSequences[i].length - 1] != 0) {
+        expected.append("...");
+      }
+      expected.append(eol).append("    ");
+    }
+    String retval = "Encountered \"";
+    Token tok = currentToken.next;
+    for (int i = 0; i < maxSize; i++) {
+      if (i != 0) retval += " ";
+      if (tok.kind == 0) {
+        retval += tokenImage[0];
+        break;
+      }
+      retval += " " + tokenImage[tok.kind];
+      retval += " \"";
+      retval += add_escapes(tok.image);
+      retval += " \"";
+      tok = tok.next;
+    }
+    retval += "\" at line " + currentToken.next.beginLine + ", column " + currentToken.next.beginColumn;
+    retval += "." + eol;
+    if (expectedTokenSequences.length == 1) {
+      retval += "Was expecting:" + eol + "    ";
+    } else {
+      retval += "Was expecting one of:" + eol + "    ";
+    }
+    retval += expected.toString();
+    return retval;
+  }
+
+  /**
+   * The end of line string for this machine.
+   */
+  protected String eol = System.getProperty("line.separator", "\n");
+
+  /**
+   * Used to convert raw characters to their escaped version
+   * when these raw version cannot be used as part of an ASCII
+   * string literal.
+   */
+  protected String add_escapes(String str) {
+      StringBuffer retval = new StringBuffer();
+      char ch;
+      for (int i = 0; i < str.length(); i++) {
+        switch (str.charAt(i))
+        {
+           case 0 :
+              continue;
+           case '\b':
+              retval.append("\\b");
+              continue;
+           case '\t':
+              retval.append("\\t");
+              continue;
+           case '\n':
+              retval.append("\\n");
+              continue;
+           case '\f':
+              retval.append("\\f");
+              continue;
+           case '\r':
+              retval.append("\\r");
+              continue;
+           case '\"':
+              retval.append("\\\"");
+              continue;
+           case '\'':
+              retval.append("\\\'");
+              continue;
+           case '\\':
+              retval.append("\\\\");
+              continue;
+           default:
+              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
+                 String s = "0000" + Integer.toString(ch, 16);
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
+              } else {
+                 retval.append(ch);
+              }
+              continue;
+        }
+      }
+      return retval.toString();
+   }
+
+}
+/* JavaCC - OriginalChecksum=15fbbe38a36c8ac9e2740d030624c321 (do not edit this line) */
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
new file mode 100644
index 0000000..f8043bc
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.java
@@ -0,0 +1,1293 @@
+/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParser.java */
+package org.apache.lucene.queryParser.precedence;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Locale;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.*;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.AttributeSource;
+
+/**
+ * Experimental query parser variant designed to handle operator precedence
+ * in a more sensible fashion than QueryParser.  There are still some
+ * open issues with this parser. The following tests are currently failing
+ * in TestPrecedenceQueryParser and are disabled to make this test pass:
+ * <ul>
+ * <li> testSimple
+ * <li> testWildcard
+ * <li> testPrecedence
+ * </ul>
+ *
+ * This class is generated by JavaCC.  The only method that clients should need
+ * to call is {@link #parse(String)}.
+ *
+ * The syntax for query strings is as follows:
+ * A Query is a series of clauses.
+ * A clause may be prefixed by:
+ * <ul>
+ * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
+ * that the clause is required or prohibited respectively; or
+ * <li> a term followed by a colon, indicating the field to be searched.
+ * This enables one to construct queries which search multiple fields.
+ * </ul>
+ *
+ * A clause may be either:
+ * <ul>
+ * <li> a term, indicating all the documents that contain this term; or
+ * <li> a nested query, enclosed in parentheses.  Note that this may be used
+ * with a <code>+</code>/<code>-</code> prefix to require any of a set of
+ * terms.
+ * </ul>
+ *
+ * Thus, in BNF, the query grammar is:
+ * <pre>
+ *   Query  ::= ( Clause )*
+ *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
+ * </pre>
+ *
+ * <p>
+ * Examples of appropriately formatted queries can be found in the <a
+ * href="../../../../../../../queryparsersyntax.html">query syntax
+ * documentation</a>.
+ * </p>
+ *
+ * @author Brian Goetz
+ * @author Peter Halacsy
+ * @author Tatu Saloranta
+ */
+
+public class PrecedenceQueryParser implements PrecedenceQueryParserConstants {
+
+  private static final int CONJ_NONE   = 0;
+  private static final int CONJ_AND    = 1;
+  private static final int CONJ_OR     = 2;
+
+  private static final int MOD_NONE    = 0;
+  private static final int MOD_NOT     = 10;
+  private static final int MOD_REQ     = 11;
+
+  // make it possible to call setDefaultOperator() without accessing
+  // the nested class:
+  public static final Operator AND_OPERATOR = Operator.AND;
+  public static final Operator OR_OPERATOR = Operator.OR;
+
+  /** The actual operator that parser uses to combine query terms */
+  private Operator operator = OR_OPERATOR;
+
+  boolean lowercaseExpandedTerms = true;
+
+  Analyzer analyzer;
+  String field;
+  int phraseSlop = 0;
+  float fuzzyMinSim = FuzzyQuery.defaultMinSimilarity;
+  int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
+  Locale locale = Locale.getDefault();
+
+  static enum Operator { OR, AND }
+
+  /** Constructs a query parser.
+   *  @param f  the default field for query terms.
+   *  @param a   used to find terms in the query text.
+   */
+  public PrecedenceQueryParser(String f, Analyzer a) {
+    this(new FastCharStream(new StringReader("")));
+    analyzer = a;
+    field = f;
+  }
+
+  /** Parses a query string, returning a {@link org.apache.lucene.search.Query}.
+   *  @param expression  the query string to be parsed.
+   *  @throws ParseException if the parsing fails
+   */
+  public Query parse(String expression) throws ParseException {
+    // optimize empty query to be empty BooleanQuery
+    if (expression == null || expression.trim().length() == 0) {
+      return new BooleanQuery();
+    }
+
+    ReInit(new FastCharStream(new StringReader(expression)));
+    try {
+      Query query = Query(field);
+      return (query != null) ? query : new BooleanQuery();
+    }
+    catch (TokenMgrError tme) {
+      throw new ParseException(tme.getMessage());
+    }
+    catch (BooleanQuery.TooManyClauses tmc) {
+      throw new ParseException("Too many boolean clauses");
+    }
+  }
+
+   /**
+   * @return Returns the analyzer.
+   */
+  public Analyzer getAnalyzer() {
+    return analyzer;
+  }
+
+  /**
+   * @return Returns the field.
+   */
+  public String getField() {
+    return field;
+  }
+
+   /**
+   * Get the minimal similarity for fuzzy queries.
+   */
+  public float getFuzzyMinSim() {
+      return fuzzyMinSim;
+  }
+
+  /**
+   * Set the minimum similarity for fuzzy queries.
+   * Default is 0.5f.
+   */
+  public void setFuzzyMinSim(float fuzzyMinSim) {
+      this.fuzzyMinSim = fuzzyMinSim;
+  }
+
+   /**
+   * Get the prefix length for fuzzy queries. 
+   * @return Returns the fuzzyPrefixLength.
+   */
+  public int getFuzzyPrefixLength() {
+    return fuzzyPrefixLength;
+  }
+
+  /**
+   * Set the prefix length for fuzzy queries. Default is 0.
+   * @param fuzzyPrefixLength The fuzzyPrefixLength to set.
+   */
+  public void setFuzzyPrefixLength(int fuzzyPrefixLength) {
+    this.fuzzyPrefixLength = fuzzyPrefixLength;
+  }
+
+  /**
+   * Sets the default slop for phrases.  If zero, then exact phrase matches
+   * are required.  Default value is zero.
+   */
+  public void setPhraseSlop(int phraseSlop) {
+    this.phraseSlop = phraseSlop;
+  }
+
+  /**
+   * Gets the default slop for phrases.
+   */
+  public int getPhraseSlop() {
+    return phraseSlop;
+  }
+
+  /**
+   * Sets the boolean operator of the QueryParser.
+   * In default mode (<code>OR_OPERATOR</code>) terms without any modifiers
+   * are considered optional: for example <code>capital of Hungary</code> is equal to
+   * <code>capital OR of OR Hungary</code>.<br/>
+   * In <code>AND_OPERATOR</code> mode terms are considered to be in conjunction: the
+   * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
+   */
+  public void setDefaultOperator(Operator op) {
+    this.operator = op;
+  }
+
+  /**
+   * Gets implicit operator setting, which will be either AND_OPERATOR
+   * or OR_OPERATOR.
+   */
+  public Operator getDefaultOperator() {
+    return operator;
+  }
+
+  /**
+   * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically
+   * lower-cased or not.  Default is <code>true</code>.
+   */
+  public void setLowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
+    this.lowercaseExpandedTerms = lowercaseExpandedTerms;
+  }
+
+  /**
+   * @see #setLowercaseExpandedTerms(boolean)
+   */
+  public boolean getLowercaseExpandedTerms() {
+    return lowercaseExpandedTerms;
+  }
+
+  /**
+   * Set locale used by date range parsing.
+   */
+  public void setLocale(Locale locale) {
+    this.locale = locale;
+  }
+
+  /**
+   * Returns current locale, allowing access by subclasses.
+   */
+  public Locale getLocale() {
+    return locale;
+  }
+
+  protected void addClause(List<BooleanClause> clauses, int conj, int modifier, Query q) {
+    boolean required, prohibited;
+
+    // If this term is introduced by AND, make the preceding term required,
+    // unless it's already prohibited
+    if (clauses.size() > 0 && conj == CONJ_AND) {
+      BooleanClause c = clauses.get(clauses.size()-1);
+      if (!c.isProhibited())
+        c.setOccur(BooleanClause.Occur.MUST);
+    }
+
+    if (clauses.size() > 0 && operator == AND_OPERATOR && conj == CONJ_OR) {
+      // If this term is introduced by OR, make the preceding term optional,
+      // unless it's prohibited (that means we leave -a OR b but +a OR b-->a OR b)
+      // notice if the input is a OR b, first term is parsed as required; without
+      // this modification a OR b would parsed as +a OR b
+      BooleanClause c = clauses.get(clauses.size()-1);
+      if (!c.isProhibited())
+        c.setOccur(BooleanClause.Occur.SHOULD);
+    }
+
+    // We might have been passed a null query; the term might have been
+    // filtered away by the analyzer.
+    if (q == null)
+      return;
+
+    if (operator == OR_OPERATOR) {
+      // We set REQUIRED if we're introduced by AND or +; PROHIBITED if
+      // introduced by NOT or -; make sure not to set both.
+      prohibited = (modifier == MOD_NOT);
+      required = (modifier == MOD_REQ);
+      if (conj == CONJ_AND && !prohibited) {
+        required = true;
+      }
+    } else {
+      // We set PROHIBITED if we're introduced by NOT or -; We set REQUIRED
+      // if not PROHIBITED and not introduced by OR
+      prohibited = (modifier == MOD_NOT);
+      required   = (!prohibited && conj != CONJ_OR);
+    }
+    if (required && !prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST));
+    else if (!required && !prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
+    else if (!required && prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST_NOT));
+    else
+      throw new RuntimeException("Clause cannot be both required and prohibited");
+  }
+
+  /**
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFieldQuery(String field, String queryText)  throws ParseException {
+    // Use the analyzer to get all the tokens, and then build a TermQuery,
+    // PhraseQuery, or nothing based on the term count
+
+    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
+    List<AttributeSource.State> list = new ArrayList<AttributeSource.State>();
+    int positionCount = 0;
+    boolean severalTokensAtSamePosition = false;
+    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
+    PositionIncrementAttribute posincrAtt = source.addAttribute(PositionIncrementAttribute.class);
+
+    try {
+      while (source.incrementToken()) {
+        list.add(source.captureState());
+        if (posincrAtt.getPositionIncrement() == 1)
+          positionCount++;
+        else
+          severalTokensAtSamePosition = true;
+      }
+      source.end();
+      source.close();
+    } catch (IOException e) {
+      // ignore, should never happen for StringReaders
+    }
+
+    if (list.size() == 0)
+      return null;
+    else if (list.size() == 1) {
+      source.restoreState(list.get(0));
+      return new TermQuery(new Term(field, termAtt.term()));
+    } else {
+      if (severalTokensAtSamePosition) {
+        if (positionCount == 1) {
+          // no phrase query:
+          BooleanQuery q = new BooleanQuery();
+          for (int i = 0; i < list.size(); i++) {
+            source.restoreState(list.get(i));
+            TermQuery currentQuery = new TermQuery(
+                new Term(field, termAtt.term()));
+            q.add(currentQuery, BooleanClause.Occur.SHOULD);
+          }
+          return q;
+        }
+        else {
+          // phrase query:
+          MultiPhraseQuery mpq = new MultiPhraseQuery();
+          List<Term> multiTerms = new ArrayList<Term>();
+          for (int i = 0; i < list.size(); i++) {
+            source.restoreState(list.get(i));
+            if (posincrAtt.getPositionIncrement() == 1 && multiTerms.size() > 0) {
+              mpq.add(multiTerms.toArray(new Term[0]));
+              multiTerms.clear();
+            }
+            multiTerms.add(new Term(field, termAtt.term()));
+          }
+          mpq.add(multiTerms.toArray(new Term[0]));
+          return mpq;
+        }
+      }
+      else {
+        PhraseQuery q = new PhraseQuery();
+        q.setSlop(phraseSlop);
+        for (int i = 0; i < list.size(); i++) {
+          source.restoreState(list.get(i));
+          q.add(new Term(field, termAtt.term()));
+        }
+        return q;
+      }
+    }
+  }
+
+  /**
+   * Base implementation delegates to {@link #getFieldQuery(String,String)}.
+   * This method may be overridden, for example, to return
+   * a SpanNearQuery instead of a PhraseQuery.
+   *
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFieldQuery(String field, String queryText, int slop)
+        throws ParseException {
+    Query query = getFieldQuery(field, queryText);
+
+    if (query instanceof PhraseQuery) {
+      ((PhraseQuery) query).setSlop(slop);
+    }
+    if (query instanceof MultiPhraseQuery) {
+      ((MultiPhraseQuery) query).setSlop(slop);
+    }
+
+    return query;
+  }
+
+  /**
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getRangeQuery(String field,
+                                String part1,
+                                String part2,
+                                boolean inclusive) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      part1 = part1.toLowerCase();
+      part2 = part2.toLowerCase();
+    }
+    try {
+      DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT, locale);
+      df.setLenient(true);
+      Date d1 = df.parse(part1);
+      Date d2 = df.parse(part2);
+      part1 = DateTools.dateToString(d1, DateTools.Resolution.DAY);
+      part2 = DateTools.dateToString(d2, DateTools.Resolution.DAY);
+    }
+    catch (Exception e) { }
+
+    return new TermRangeQuery(field, part1, part2, inclusive, inclusive);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getBooleanQuery(List<BooleanClause> clauses) throws ParseException
+  {
+    return getBooleanQuery(clauses, false);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   * @param disableCoord true if coord scoring should be disabled.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getBooleanQuery(List<BooleanClause> clauses, boolean disableCoord)
+      throws ParseException {
+    if (clauses == null || clauses.size() == 0)
+      return null;
+
+    BooleanQuery query = new BooleanQuery(disableCoord);
+    for (int i = 0; i < clauses.size(); i++) {
+      query.add(clauses.get(i));
+    }
+    return query;
+  }
+
+  /**
+   * Factory method for generating a query. Called when parser
+   * parses an input term token that contains one or more wildcard
+   * characters (? and *), but is not a prefix term token (one
+   * that has just a single * character at the end)
+   *<p>
+   * Depending on settings, prefix term may be lower-cased
+   * automatically. It will not go through the default Analyzer,
+   * however, since normal Analyzers are unlikely to work properly
+   * with wildcard templates.
+   *<p>
+   * Can be overridden by extending classes, to provide custom handling for
+   * wildcard queries, which may be necessary due to missing analyzer calls.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token that contains one or more wild card
+   *   characters (? or *), but is not simple prefix term
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getWildcardQuery(String field, String termStr) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new WildcardQuery(t);
+  }
+
+  /**
+   * Factory method for generating a query (similar to
+   * {@link #getWildcardQuery}). Called when parser parses an input term
+   * token that uses prefix notation; that is, contains a single '*' wildcard
+   * character as its last character. Since this is a special case
+   * of generic wildcard term, and such a query can be optimized easily,
+   * this usually results in a different query object.
+   *<p>
+   * Depending on settings, a prefix term may be lower-cased
+   * automatically. It will not go through the default Analyzer,
+   * however, since normal Analyzers are unlikely to work properly
+   * with wildcard templates.
+   *<p>
+   * Can be overridden by extending classes, to provide custom handling for
+   * wild card queries, which may be necessary due to missing analyzer calls.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *    (<b>without</b> trailing '*' character!)
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getPrefixQuery(String field, String termStr) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new PrefixQuery(t);
+  }
+
+   /**
+   * Factory method for generating a query (similar to
+   * {@link #getWildcardQuery}). Called when parser parses
+   * an input term token that has the fuzzy suffix (~) appended.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new FuzzyQuery(t, minSimilarity, fuzzyPrefixLength);
+  }
+
+  /**
+   * Returns a String where the escape char has been
+   * removed, or kept only once if there was a double escape.
+   */
+  private String discardEscapeChar(String input) {
+    char[] caSource = input.toCharArray();
+    char[] caDest = new char[caSource.length];
+    int j = 0;
+    for (int i = 0; i < caSource.length; i++) {
+      if ((caSource[i] != '\\') || (i > 0 && caSource[i-1] == '\\')) {
+        caDest[j++]=caSource[i];
+      }
+    }
+    return new String(caDest, 0, j);
+  }
+
+  /**
+   * Returns a String where those characters that QueryParser
+   * expects to be escaped are escaped by a preceding <code>\</code>.
+   */
+  public static String escape(String s) {
+    StringBuffer sb = new StringBuffer();
+    for (int i = 0; i < s.length(); i++) {
+      char c = s.charAt(i);
+      // NOTE: keep this in sync with _ESCAPED_CHAR below!
+      if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
+        || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
+        || c == '*' || c == '?') {
+        sb.append('\\');
+      }
+      sb.append(c);
+    }
+    return sb.toString();
+  }
+
+  /**
+   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
+   * Usage:<br>
+   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
+   */
+  public static void main(String[] args) throws Exception {
+    if (args.length == 0) {
+      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+      System.exit(0);
+    }
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
+                           new org.apache.lucene.analysis.SimpleAnalyzer());
+    Query q = qp.parse(args[0]);
+    System.out.println(q.toString("field"));
+  }
+
+// *   Query  ::= ( Clause )*
+// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
+  final public int Conjunction() throws ParseException {
+  int ret = CONJ_NONE;
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case AND:
+    case OR:
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case AND:
+        jj_consume_token(AND);
+            ret = CONJ_AND;
+        break;
+      case OR:
+        jj_consume_token(OR);
+              ret = CONJ_OR;
+        break;
+      default:
+        jj_la1[0] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      break;
+    default:
+      jj_la1[1] = jj_gen;
+      ;
+    }
+    {if (true) return ret;}
+    throw new Error("Missing return statement in function");
+  }
+
+  final public int Modifier() throws ParseException {
+  int ret = MOD_NONE;
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case NOT:
+    case PLUS:
+    case MINUS:
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case PLUS:
+        jj_consume_token(PLUS);
+              ret = MOD_REQ;
+        break;
+      case MINUS:
+        jj_consume_token(MINUS);
+                 ret = MOD_NOT;
+        break;
+      case NOT:
+        jj_consume_token(NOT);
+               ret = MOD_NOT;
+        break;
+      default:
+        jj_la1[2] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      break;
+    default:
+      jj_la1[3] = jj_gen;
+      ;
+    }
+    {if (true) return ret;}
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Query Query(String field) throws ParseException {
+  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+  Query q, firstQuery=null;
+  boolean orPresent = false;
+  int modifier;
+    modifier = Modifier();
+    q = andExpression(field);
+    addClause(clauses, CONJ_NONE, modifier, q);
+    if (modifier == MOD_NONE)
+      firstQuery = q;
+    label_1:
+    while (true) {
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case OR:
+      case NOT:
+      case PLUS:
+      case MINUS:
+      case LPAREN:
+      case QUOTED:
+      case TERM:
+      case PREFIXTERM:
+      case WILDTERM:
+      case RANGEIN_START:
+      case RANGEEX_START:
+      case NUMBER:
+        ;
+        break;
+      default:
+        jj_la1[4] = jj_gen;
+        break label_1;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case OR:
+        jj_consume_token(OR);
+            orPresent=true;
+        break;
+      default:
+        jj_la1[5] = jj_gen;
+        ;
+      }
+      modifier = Modifier();
+      q = andExpression(field);
+      addClause(clauses, orPresent ? CONJ_OR : CONJ_NONE, modifier, q);
+    }
+      if (clauses.size() == 1 && firstQuery != null)
+        {if (true) return firstQuery;}
+      else {
+        {if (true) return getBooleanQuery(clauses);}
+      }
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Query andExpression(String field) throws ParseException {
+  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+  Query q, firstQuery=null;
+  int modifier;
+    q = Clause(field);
+    addClause(clauses, CONJ_NONE, MOD_NONE, q);
+    firstQuery = q;
+    label_2:
+    while (true) {
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case AND:
+        ;
+        break;
+      default:
+        jj_la1[6] = jj_gen;
+        break label_2;
+      }
+      jj_consume_token(AND);
+      modifier = Modifier();
+      q = Clause(field);
+      addClause(clauses, CONJ_AND, modifier, q);
+    }
+      if (clauses.size() == 1 && firstQuery != null)
+        {if (true) return firstQuery;}
+      else {
+        {if (true) return getBooleanQuery(clauses);}
+      }
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Query Clause(String field) throws ParseException {
+  Query q;
+  Token fieldToken=null, boost=null;
+    if (jj_2_1(2)) {
+      fieldToken = jj_consume_token(TERM);
+      jj_consume_token(COLON);
+      field=discardEscapeChar(fieldToken.image);
+    } else {
+      ;
+    }
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case QUOTED:
+    case TERM:
+    case PREFIXTERM:
+    case WILDTERM:
+    case RANGEIN_START:
+    case RANGEEX_START:
+    case NUMBER:
+      q = Term(field);
+      break;
+    case LPAREN:
+      jj_consume_token(LPAREN);
+      q = Query(field);
+      jj_consume_token(RPAREN);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[7] = jj_gen;
+        ;
+      }
+      break;
+    default:
+      jj_la1[8] = jj_gen;
+      jj_consume_token(-1);
+      throw new ParseException();
+    }
+      if (boost != null) {
+        float f = (float)1.0;
+  try {
+    f = Float.valueOf(boost.image).floatValue();
+          q.setBoost(f);
+  } catch (Exception ignored) { }
+      }
+      {if (true) return q;}
+    throw new Error("Missing return statement in function");
+  }
+
+  final public Query Term(String field) throws ParseException {
+  Token term, boost=null, fuzzySlop=null, goop1, goop2;
+  boolean prefix = false;
+  boolean wildcard = false;
+  boolean fuzzy = false;
+  Query q;
+    switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+    case TERM:
+    case PREFIXTERM:
+    case WILDTERM:
+    case NUMBER:
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case TERM:
+        term = jj_consume_token(TERM);
+        break;
+      case PREFIXTERM:
+        term = jj_consume_token(PREFIXTERM);
+                             prefix=true;
+        break;
+      case WILDTERM:
+        term = jj_consume_token(WILDTERM);
+                           wildcard=true;
+        break;
+      case NUMBER:
+        term = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[9] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case FUZZY_SLOP:
+        fuzzySlop = jj_consume_token(FUZZY_SLOP);
+                                fuzzy=true;
+        break;
+      default:
+        jj_la1[10] = jj_gen;
+        ;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+        case FUZZY_SLOP:
+          fuzzySlop = jj_consume_token(FUZZY_SLOP);
+                                                         fuzzy=true;
+          break;
+        default:
+          jj_la1[11] = jj_gen;
+          ;
+        }
+        break;
+      default:
+        jj_la1[12] = jj_gen;
+        ;
+      }
+       String termImage=discardEscapeChar(term.image);
+       if (wildcard) {
+       q = getWildcardQuery(field, termImage);
+       } else if (prefix) {
+         q = getPrefixQuery(field,
+           discardEscapeChar(term.image.substring
+          (0, term.image.length()-1)));
+       } else if (fuzzy) {
+          float fms = fuzzyMinSim;
+          try {
+            fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
+          } catch (Exception ignored) { }
+         if(fms < 0.0f || fms > 1.0f){
+           {if (true) throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");}
+         }
+         q = getFuzzyQuery(field, termImage, fms);
+       } else {
+         q = getFieldQuery(field, termImage);
+       }
+      break;
+    case RANGEIN_START:
+      jj_consume_token(RANGEIN_START);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEIN_GOOP:
+        goop1 = jj_consume_token(RANGEIN_GOOP);
+        break;
+      case RANGEIN_QUOTED:
+        goop1 = jj_consume_token(RANGEIN_QUOTED);
+        break;
+      default:
+        jj_la1[13] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEIN_TO:
+        jj_consume_token(RANGEIN_TO);
+        break;
+      default:
+        jj_la1[14] = jj_gen;
+        ;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEIN_GOOP:
+        goop2 = jj_consume_token(RANGEIN_GOOP);
+        break;
+      case RANGEIN_QUOTED:
+        goop2 = jj_consume_token(RANGEIN_QUOTED);
+        break;
+      default:
+        jj_la1[15] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      jj_consume_token(RANGEIN_END);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[16] = jj_gen;
+        ;
+      }
+          if (goop1.kind == RANGEIN_QUOTED) {
+            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
+          } else {
+            goop1.image = discardEscapeChar(goop1.image);
+          }
+          if (goop2.kind == RANGEIN_QUOTED) {
+            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
+      } else {
+        goop2.image = discardEscapeChar(goop2.image);
+      }
+          q = getRangeQuery(field, goop1.image, goop2.image, true);
+      break;
+    case RANGEEX_START:
+      jj_consume_token(RANGEEX_START);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEEX_GOOP:
+        goop1 = jj_consume_token(RANGEEX_GOOP);
+        break;
+      case RANGEEX_QUOTED:
+        goop1 = jj_consume_token(RANGEEX_QUOTED);
+        break;
+      default:
+        jj_la1[17] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEEX_TO:
+        jj_consume_token(RANGEEX_TO);
+        break;
+      default:
+        jj_la1[18] = jj_gen;
+        ;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case RANGEEX_GOOP:
+        goop2 = jj_consume_token(RANGEEX_GOOP);
+        break;
+      case RANGEEX_QUOTED:
+        goop2 = jj_consume_token(RANGEEX_QUOTED);
+        break;
+      default:
+        jj_la1[19] = jj_gen;
+        jj_consume_token(-1);
+        throw new ParseException();
+      }
+      jj_consume_token(RANGEEX_END);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[20] = jj_gen;
+        ;
+      }
+          if (goop1.kind == RANGEEX_QUOTED) {
+            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
+          } else {
+            goop1.image = discardEscapeChar(goop1.image);
+          }
+          if (goop2.kind == RANGEEX_QUOTED) {
+            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
+      } else {
+        goop2.image = discardEscapeChar(goop2.image);
+      }
+
+          q = getRangeQuery(field, goop1.image, goop2.image, false);
+      break;
+    case QUOTED:
+      term = jj_consume_token(QUOTED);
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case FUZZY_SLOP:
+        fuzzySlop = jj_consume_token(FUZZY_SLOP);
+        break;
+      default:
+        jj_la1[21] = jj_gen;
+        ;
+      }
+      switch ((jj_ntk==-1)?jj_ntk():jj_ntk) {
+      case CARAT:
+        jj_consume_token(CARAT);
+        boost = jj_consume_token(NUMBER);
+        break;
+      default:
+        jj_la1[22] = jj_gen;
+        ;
+      }
+         int s = phraseSlop;
+
+         if (fuzzySlop != null) {
+           try {
+             s = Float.valueOf(fuzzySlop.image.substring(1)).intValue();
+           }
+           catch (Exception ignored) { }
+         }
+         q = getFieldQuery(field, term.image.substring(1, term.image.length()-1), s);
+      break;
+    default:
+      jj_la1[23] = jj_gen;
+      jj_consume_token(-1);
+      throw new ParseException();
+    }
+    if (boost != null) {
+      float f = (float) 1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+      }
+      catch (Exception ignored) {
+    /* Should this be handled somehow? (defaults to "no boost", if
+     * boost number is invalid)
+     */
+      }
+
+      // avoid boosting null queries, such as those caused by stop words
+      if (q != null) {
+        q.setBoost(f);
+      }
+    }
+    {if (true) return q;}
+    throw new Error("Missing return statement in function");
+  }
+
+  private boolean jj_2_1(int xla) {
+    jj_la = xla; jj_lastpos = jj_scanpos = token;
+    try { return !jj_3_1(); }
+    catch(LookaheadSuccess ls) { return true; }
+    finally { jj_save(0, xla); }
+  }
+
+  private boolean jj_3_1() {
+    if (jj_scan_token(TERM)) return true;
+    if (jj_scan_token(COLON)) return true;
+    return false;
+  }
+
+  /** Generated Token Manager. */
+  public PrecedenceQueryParserTokenManager token_source;
+  /** Current token. */
+  public Token token;
+  /** Next token. */
+  public Token jj_nt;
+  private int jj_ntk;
+  private Token jj_scanpos, jj_lastpos;
+  private int jj_la;
+  private int jj_gen;
+  final private int[] jj_la1 = new int[24];
+  static private int[] jj_la1_0;
+  static {
+      jj_la1_init_0();
+   }
+   private static void jj_la1_init_0() {
+      jj_la1_0 = new int[] {0x180,0x180,0xe00,0xe00,0xfb1f00,0x100,0x80,0x8000,0xfb1000,0x9a0000,0x40000,0x40000,0x8000,0xc000000,0x1000000,0xc000000,0x8000,0xc0000000,0x10000000,0xc0000000,0x8000,0x40000,0x8000,0xfb0000,};
+   }
+  final private JJCalls[] jj_2_rtns = new JJCalls[1];
+  private boolean jj_rescan = false;
+  private int jj_gc = 0;
+
+  /** Constructor with user supplied CharStream. */
+  public PrecedenceQueryParser(CharStream stream) {
+    token_source = new PrecedenceQueryParserTokenManager(stream);
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  /** Reinitialise. */
+  public void ReInit(CharStream stream) {
+    token_source.ReInit(stream);
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  /** Constructor with generated Token Manager. */
+  public PrecedenceQueryParser(PrecedenceQueryParserTokenManager tm) {
+    token_source = tm;
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  /** Reinitialise. */
+  public void ReInit(PrecedenceQueryParserTokenManager tm) {
+    token_source = tm;
+    token = new Token();
+    jj_ntk = -1;
+    jj_gen = 0;
+    for (int i = 0; i < 24; i++) jj_la1[i] = -1;
+    for (int i = 0; i < jj_2_rtns.length; i++) jj_2_rtns[i] = new JJCalls();
+  }
+
+  private Token jj_consume_token(int kind) throws ParseException {
+    Token oldToken;
+    if ((oldToken = token).next != null) token = token.next;
+    else token = token.next = token_source.getNextToken();
+    jj_ntk = -1;
+    if (token.kind == kind) {
+      jj_gen++;
+      if (++jj_gc > 100) {
+        jj_gc = 0;
+        for (int i = 0; i < jj_2_rtns.length; i++) {
+          JJCalls c = jj_2_rtns[i];
+          while (c != null) {
+            if (c.gen < jj_gen) c.first = null;
+            c = c.next;
+          }
+        }
+      }
+      return token;
+    }
+    token = oldToken;
+    jj_kind = kind;
+    throw generateParseException();
+  }
+
+  static private final class LookaheadSuccess extends java.lang.Error { }
+  final private LookaheadSuccess jj_ls = new LookaheadSuccess();
+  private boolean jj_scan_token(int kind) {
+    if (jj_scanpos == jj_lastpos) {
+      jj_la--;
+      if (jj_scanpos.next == null) {
+        jj_lastpos = jj_scanpos = jj_scanpos.next = token_source.getNextToken();
+      } else {
+        jj_lastpos = jj_scanpos = jj_scanpos.next;
+      }
+    } else {
+      jj_scanpos = jj_scanpos.next;
+    }
+    if (jj_rescan) {
+      int i = 0; Token tok = token;
+      while (tok != null && tok != jj_scanpos) { i++; tok = tok.next; }
+      if (tok != null) jj_add_error_token(kind, i);
+    }
+    if (jj_scanpos.kind != kind) return true;
+    if (jj_la == 0 && jj_scanpos == jj_lastpos) throw jj_ls;
+    return false;
+  }
+
+
+/** Get the next Token. */
+  final public Token getNextToken() {
+    if (token.next != null) token = token.next;
+    else token = token.next = token_source.getNextToken();
+    jj_ntk = -1;
+    jj_gen++;
+    return token;
+  }
+
+/** Get the specific Token. */
+  final public Token getToken(int index) {
+    Token t = token;
+    for (int i = 0; i < index; i++) {
+      if (t.next != null) t = t.next;
+      else t = t.next = token_source.getNextToken();
+    }
+    return t;
+  }
+
+  private int jj_ntk() {
+    if ((jj_nt=token.next) == null)
+      return (jj_ntk = (token.next=token_source.getNextToken()).kind);
+    else
+      return (jj_ntk = jj_nt.kind);
+  }
+
+  private java.util.List<int[]> jj_expentries = new java.util.ArrayList<int[]>();
+  private int[] jj_expentry;
+  private int jj_kind = -1;
+  private int[] jj_lasttokens = new int[100];
+  private int jj_endpos;
+
+  private void jj_add_error_token(int kind, int pos) {
+    if (pos >= 100) return;
+    if (pos == jj_endpos + 1) {
+      jj_lasttokens[jj_endpos++] = kind;
+    } else if (jj_endpos != 0) {
+      jj_expentry = new int[jj_endpos];
+      for (int i = 0; i < jj_endpos; i++) {
+        jj_expentry[i] = jj_lasttokens[i];
+      }
+      jj_entries_loop: for (java.util.Iterator it = jj_expentries.iterator(); it.hasNext();) {
+        int[] oldentry = (int[])(it.next());
+        if (oldentry.length == jj_expentry.length) {
+          for (int i = 0; i < jj_expentry.length; i++) {
+            if (oldentry[i] != jj_expentry[i]) {
+              continue jj_entries_loop;
+            }
+          }
+          jj_expentries.add(jj_expentry);
+          break jj_entries_loop;
+        }
+      }
+      if (pos != 0) jj_lasttokens[(jj_endpos = pos) - 1] = kind;
+    }
+  }
+
+  /** Generate ParseException. */
+  public ParseException generateParseException() {
+    jj_expentries.clear();
+    boolean[] la1tokens = new boolean[32];
+    if (jj_kind >= 0) {
+      la1tokens[jj_kind] = true;
+      jj_kind = -1;
+    }
+    for (int i = 0; i < 24; i++) {
+      if (jj_la1[i] == jj_gen) {
+        for (int j = 0; j < 32; j++) {
+          if ((jj_la1_0[i] & (1<<j)) != 0) {
+            la1tokens[j] = true;
+          }
+        }
+      }
+    }
+    for (int i = 0; i < 32; i++) {
+      if (la1tokens[i]) {
+        jj_expentry = new int[1];
+        jj_expentry[0] = i;
+        jj_expentries.add(jj_expentry);
+      }
+    }
+    jj_endpos = 0;
+    jj_rescan_token();
+    jj_add_error_token(0, 0);
+    int[][] exptokseq = new int[jj_expentries.size()][];
+    for (int i = 0; i < jj_expentries.size(); i++) {
+      exptokseq[i] = jj_expentries.get(i);
+    }
+    return new ParseException(token, exptokseq, tokenImage);
+  }
+
+  /** Enable tracing. */
+  final public void enable_tracing() {
+  }
+
+  /** Disable tracing. */
+  final public void disable_tracing() {
+  }
+
+  private void jj_rescan_token() {
+    jj_rescan = true;
+    for (int i = 0; i < 1; i++) {
+    try {
+      JJCalls p = jj_2_rtns[i];
+      do {
+        if (p.gen > jj_gen) {
+          jj_la = p.arg; jj_lastpos = jj_scanpos = p.first;
+          switch (i) {
+            case 0: jj_3_1(); break;
+          }
+        }
+        p = p.next;
+      } while (p != null);
+      } catch(LookaheadSuccess ls) { }
+    }
+    jj_rescan = false;
+  }
+
+  private void jj_save(int index, int xla) {
+    JJCalls p = jj_2_rtns[index];
+    while (p.gen > jj_gen) {
+      if (p.next == null) { p = p.next = new JJCalls(); break; }
+      p = p.next;
+    }
+    p.gen = jj_gen + xla - jj_la; p.first = token; p.arg = xla;
+  }
+
+  static final class JJCalls {
+    int gen;
+    Token first;
+    int arg;
+    JJCalls next;
+  }
+
+}
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
new file mode 100644
index 0000000..9794e13
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj
@@ -0,0 +1,905 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+options {
+  STATIC=false;
+  JAVA_UNICODE_ESCAPE=true;
+  USER_CHAR_STREAM=true;
+}
+
+PARSER_BEGIN(PrecedenceQueryParser)
+
+package org.apache.lucene.queryParser.precedence;
+
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Locale;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.*;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.AttributeSource;
+
+/**
+ * Experimental query parser variant designed to handle operator precedence
+ * in a more sensible fashion than QueryParser.  There are still some
+ * open issues with this parser. The following tests are currently failing
+ * in TestPrecedenceQueryParser and are disabled to make this test pass:
+ * <ul>
+ * <li> testSimple
+ * <li> testWildcard
+ * <li> testPrecedence
+ * </ul>
+ *
+ * This class is generated by JavaCC.  The only method that clients should need
+ * to call is {@link #parse(String)}.
+ *
+ * The syntax for query strings is as follows:
+ * A Query is a series of clauses.
+ * A clause may be prefixed by:
+ * <ul>
+ * <li> a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating
+ * that the clause is required or prohibited respectively; or
+ * <li> a term followed by a colon, indicating the field to be searched.
+ * This enables one to construct queries which search multiple fields.
+ * </ul>
+ *
+ * A clause may be either:
+ * <ul>
+ * <li> a term, indicating all the documents that contain this term; or
+ * <li> a nested query, enclosed in parentheses.  Note that this may be used
+ * with a <code>+</code>/<code>-</code> prefix to require any of a set of
+ * terms.
+ * </ul>
+ *
+ * Thus, in BNF, the query grammar is:
+ * <pre>
+ *   Query  ::= ( Clause )*
+ *   Clause ::= ["+", "-"] [&lt;TERM&gt; ":"] ( &lt;TERM&gt; | "(" Query ")" )
+ * </pre>
+ *
+ * <p>
+ * Examples of appropriately formatted queries can be found in the <a
+ * href="../../../../../../../queryparsersyntax.html">query syntax
+ * documentation</a>.
+ * </p>
+ *
+ * @author Brian Goetz
+ * @author Peter Halacsy
+ * @author Tatu Saloranta
+ */
+
+public class PrecedenceQueryParser {
+
+  private static final int CONJ_NONE   = 0;
+  private static final int CONJ_AND    = 1;
+  private static final int CONJ_OR     = 2;
+
+  private static final int MOD_NONE    = 0;
+  private static final int MOD_NOT     = 10;
+  private static final int MOD_REQ     = 11;
+
+  // make it possible to call setDefaultOperator() without accessing
+  // the nested class:
+  public static final Operator AND_OPERATOR = Operator.AND;
+  public static final Operator OR_OPERATOR = Operator.OR;
+
+  /** The actual operator that parser uses to combine query terms */
+  private Operator operator = OR_OPERATOR;
+
+  boolean lowercaseExpandedTerms = true;
+
+  Analyzer analyzer;
+  String field;
+  int phraseSlop = 0;
+  float fuzzyMinSim = FuzzyQuery.defaultMinSimilarity;
+  int fuzzyPrefixLength = FuzzyQuery.defaultPrefixLength;
+  Locale locale = Locale.getDefault();
+
+  static enum Operator { OR, AND }
+
+  /** Constructs a query parser.
+   *  @param f  the default field for query terms.
+   *  @param a   used to find terms in the query text.
+   */
+  public PrecedenceQueryParser(String f, Analyzer a) {
+    this(new FastCharStream(new StringReader("")));
+    analyzer = a;
+    field = f;
+  }
+
+  /** Parses a query string, returning a {@link org.apache.lucene.search.Query}.
+   *  @param expression  the query string to be parsed.
+   *  @throws ParseException if the parsing fails
+   */
+  public Query parse(String expression) throws ParseException {
+    // optimize empty query to be empty BooleanQuery
+    if (expression == null || expression.trim().length() == 0) {
+      return new BooleanQuery();
+    }
+
+    ReInit(new FastCharStream(new StringReader(expression)));
+    try {
+      Query query = Query(field);
+      return (query != null) ? query : new BooleanQuery();
+    }
+    catch (TokenMgrError tme) {
+      throw new ParseException(tme.getMessage());
+    }
+    catch (BooleanQuery.TooManyClauses tmc) {
+      throw new ParseException("Too many boolean clauses");
+    }
+  }
+
+   /**
+   * @return Returns the analyzer.
+   */
+  public Analyzer getAnalyzer() {
+    return analyzer;
+  }
+
+  /**
+   * @return Returns the field.
+   */
+  public String getField() {
+    return field;
+  }
+
+   /**
+   * Get the minimal similarity for fuzzy queries.
+   */
+  public float getFuzzyMinSim() {
+      return fuzzyMinSim;
+  }
+
+  /**
+   * Set the minimum similarity for fuzzy queries.
+   * Default is 0.5f.
+   */
+  public void setFuzzyMinSim(float fuzzyMinSim) {
+      this.fuzzyMinSim = fuzzyMinSim;
+  }
+
+   /**
+   * Get the prefix length for fuzzy queries. 
+   * @return Returns the fuzzyPrefixLength.
+   */
+  public int getFuzzyPrefixLength() {
+    return fuzzyPrefixLength;
+  }
+
+  /**
+   * Set the prefix length for fuzzy queries. Default is 0.
+   * @param fuzzyPrefixLength The fuzzyPrefixLength to set.
+   */
+  public void setFuzzyPrefixLength(int fuzzyPrefixLength) {
+    this.fuzzyPrefixLength = fuzzyPrefixLength;
+  }
+
+  /**
+   * Sets the default slop for phrases.  If zero, then exact phrase matches
+   * are required.  Default value is zero.
+   */
+  public void setPhraseSlop(int phraseSlop) {
+    this.phraseSlop = phraseSlop;
+  }
+
+  /**
+   * Gets the default slop for phrases.
+   */
+  public int getPhraseSlop() {
+    return phraseSlop;
+  }
+
+  /**
+   * Sets the boolean operator of the QueryParser.
+   * In default mode (<code>OR_OPERATOR</code>) terms without any modifiers
+   * are considered optional: for example <code>capital of Hungary</code> is equal to
+   * <code>capital OR of OR Hungary</code>.<br/>
+   * In <code>AND_OPERATOR</code> mode terms are considered to be in conjunction: the
+   * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
+   */
+  public void setDefaultOperator(Operator op) {
+    this.operator = op;
+  }
+
+  /**
+   * Gets implicit operator setting, which will be either AND_OPERATOR
+   * or OR_OPERATOR.
+   */
+  public Operator getDefaultOperator() {
+    return operator;
+  }
+
+  /**
+   * Whether terms of wildcard, prefix, fuzzy and range queries are to be automatically
+   * lower-cased or not.  Default is <code>true</code>.
+   */
+  public void setLowercaseExpandedTerms(boolean lowercaseExpandedTerms) {
+    this.lowercaseExpandedTerms = lowercaseExpandedTerms;
+  }
+
+  /**
+   * @see #setLowercaseExpandedTerms(boolean)
+   */
+  public boolean getLowercaseExpandedTerms() {
+    return lowercaseExpandedTerms;
+  }
+
+  /**
+   * Set locale used by date range parsing.
+   */
+  public void setLocale(Locale locale) {
+    this.locale = locale;
+  }
+
+  /**
+   * Returns current locale, allowing access by subclasses.
+   */
+  public Locale getLocale() {
+    return locale;
+  }
+
+  protected void addClause(List<BooleanClause> clauses, int conj, int modifier, Query q) {
+    boolean required, prohibited;
+
+    // If this term is introduced by AND, make the preceding term required,
+    // unless it's already prohibited
+    if (clauses.size() > 0 && conj == CONJ_AND) {
+      BooleanClause c = clauses.get(clauses.size()-1);
+      if (!c.isProhibited())
+        c.setOccur(BooleanClause.Occur.MUST);
+    }
+
+    if (clauses.size() > 0 && operator == AND_OPERATOR && conj == CONJ_OR) {
+      // If this term is introduced by OR, make the preceding term optional,
+      // unless it's prohibited (that means we leave -a OR b but +a OR b-->a OR b)
+      // notice if the input is a OR b, first term is parsed as required; without
+      // this modification a OR b would parsed as +a OR b
+      BooleanClause c = clauses.get(clauses.size()-1);
+      if (!c.isProhibited())
+        c.setOccur(BooleanClause.Occur.SHOULD);
+    }
+
+    // We might have been passed a null query; the term might have been
+    // filtered away by the analyzer.
+    if (q == null)
+      return;
+
+    if (operator == OR_OPERATOR) {
+      // We set REQUIRED if we're introduced by AND or +; PROHIBITED if
+      // introduced by NOT or -; make sure not to set both.
+      prohibited = (modifier == MOD_NOT);
+      required = (modifier == MOD_REQ);
+      if (conj == CONJ_AND && !prohibited) {
+        required = true;
+      }
+    } else {
+      // We set PROHIBITED if we're introduced by NOT or -; We set REQUIRED
+      // if not PROHIBITED and not introduced by OR
+      prohibited = (modifier == MOD_NOT);
+      required   = (!prohibited && conj != CONJ_OR);
+    }
+    if (required && !prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST));
+    else if (!required && !prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.SHOULD));
+    else if (!required && prohibited)
+      clauses.add(new BooleanClause(q, BooleanClause.Occur.MUST_NOT));
+    else
+      throw new RuntimeException("Clause cannot be both required and prohibited");
+  }
+
+  /**
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFieldQuery(String field, String queryText)  throws ParseException {
+    // Use the analyzer to get all the tokens, and then build a TermQuery,
+    // PhraseQuery, or nothing based on the term count
+
+    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
+    List<AttributeSource.State> list = new ArrayList<AttributeSource.State>();
+    int positionCount = 0;
+    boolean severalTokensAtSamePosition = false;
+    TermAttribute termAtt = source.addAttribute(TermAttribute.class);
+    PositionIncrementAttribute posincrAtt = source.addAttribute(PositionIncrementAttribute.class);
+
+    try {
+      while (source.incrementToken()) {
+        list.add(source.captureState());
+        if (posincrAtt.getPositionIncrement() == 1)
+          positionCount++;
+        else
+          severalTokensAtSamePosition = true;
+      }
+      source.end();
+      source.close();
+    } catch (IOException e) {
+      // ignore, should never happen for StringReaders
+    }
+
+    if (list.size() == 0)
+      return null;
+    else if (list.size() == 1) {
+      source.restoreState(list.get(0));
+      return new TermQuery(new Term(field, termAtt.term()));
+    } else {
+      if (severalTokensAtSamePosition) {
+        if (positionCount == 1) {
+          // no phrase query:
+          BooleanQuery q = new BooleanQuery();
+          for (int i = 0; i < list.size(); i++) {
+            source.restoreState(list.get(i));
+            TermQuery currentQuery = new TermQuery(
+                new Term(field, termAtt.term()));
+            q.add(currentQuery, BooleanClause.Occur.SHOULD);
+          }
+          return q;
+        }
+        else {
+          // phrase query:
+          MultiPhraseQuery mpq = new MultiPhraseQuery();
+          List<Term> multiTerms = new ArrayList<Term>();
+          for (int i = 0; i < list.size(); i++) {
+            source.restoreState(list.get(i));
+            if (posincrAtt.getPositionIncrement() == 1 && multiTerms.size() > 0) {
+              mpq.add(multiTerms.toArray(new Term[0]));
+              multiTerms.clear();
+            }
+            multiTerms.add(new Term(field, termAtt.term()));
+          }
+          mpq.add(multiTerms.toArray(new Term[0]));
+          return mpq;
+        }
+      }
+      else {
+        PhraseQuery q = new PhraseQuery();
+        q.setSlop(phraseSlop);
+        for (int i = 0; i < list.size(); i++) {
+          source.restoreState(list.get(i));
+          q.add(new Term(field, termAtt.term()));
+        }
+        return q;
+      }
+    }
+  }
+
+  /**
+   * Base implementation delegates to {@link #getFieldQuery(String,String)}.
+   * This method may be overridden, for example, to return
+   * a SpanNearQuery instead of a PhraseQuery.
+   *
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFieldQuery(String field, String queryText, int slop)
+        throws ParseException {
+    Query query = getFieldQuery(field, queryText);
+
+    if (query instanceof PhraseQuery) {
+      ((PhraseQuery) query).setSlop(slop);
+    }
+    if (query instanceof MultiPhraseQuery) {
+      ((MultiPhraseQuery) query).setSlop(slop);
+    }
+
+    return query;
+  }
+
+  /**
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getRangeQuery(String field,
+                                String part1,
+                                String part2,
+                                boolean inclusive) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      part1 = part1.toLowerCase();
+      part2 = part2.toLowerCase();
+    }
+    try {
+      DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT, locale);
+      df.setLenient(true);
+      Date d1 = df.parse(part1);
+      Date d2 = df.parse(part2);
+      part1 = DateTools.dateToString(d1, DateTools.Resolution.DAY);
+      part2 = DateTools.dateToString(d2, DateTools.Resolution.DAY);
+    }
+    catch (Exception e) { }
+
+    return new TermRangeQuery(field, part1, part2, inclusive, inclusive);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getBooleanQuery(List<BooleanClause> clauses) throws ParseException
+  {
+    return getBooleanQuery(clauses, false);
+  }
+
+  /**
+   * Factory method for generating query, given a set of clauses.
+   * By default creates a boolean query composed of clauses passed in.
+   *
+   * Can be overridden by extending classes, to modify query being
+   * returned.
+   *
+   * @param clauses List that contains {@link BooleanClause} instances
+   *    to join.
+   * @param disableCoord true if coord scoring should be disabled.
+   *
+   * @return Resulting {@link Query} object.
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getBooleanQuery(List<BooleanClause> clauses, boolean disableCoord)
+      throws ParseException {
+    if (clauses == null || clauses.size() == 0)
+      return null;
+
+    BooleanQuery query = new BooleanQuery(disableCoord);
+    for (int i = 0; i < clauses.size(); i++) {
+      query.add(clauses.get(i));
+    }
+    return query;
+  }
+
+  /**
+   * Factory method for generating a query. Called when parser
+   * parses an input term token that contains one or more wildcard
+   * characters (? and *), but is not a prefix term token (one
+   * that has just a single * character at the end)
+   *<p>
+   * Depending on settings, prefix term may be lower-cased
+   * automatically. It will not go through the default Analyzer,
+   * however, since normal Analyzers are unlikely to work properly
+   * with wildcard templates.
+   *<p>
+   * Can be overridden by extending classes, to provide custom handling for
+   * wildcard queries, which may be necessary due to missing analyzer calls.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token that contains one or more wild card
+   *   characters (? or *), but is not simple prefix term
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getWildcardQuery(String field, String termStr) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new WildcardQuery(t);
+  }
+
+  /**
+   * Factory method for generating a query (similar to
+   * {@link #getWildcardQuery}). Called when parser parses an input term
+   * token that uses prefix notation; that is, contains a single '*' wildcard
+   * character as its last character. Since this is a special case
+   * of generic wildcard term, and such a query can be optimized easily,
+   * this usually results in a different query object.
+   *<p>
+   * Depending on settings, a prefix term may be lower-cased
+   * automatically. It will not go through the default Analyzer,
+   * however, since normal Analyzers are unlikely to work properly
+   * with wildcard templates.
+   *<p>
+   * Can be overridden by extending classes, to provide custom handling for
+   * wild card queries, which may be necessary due to missing analyzer calls.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *    (<b>without</b> trailing '*' character!)
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getPrefixQuery(String field, String termStr) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new PrefixQuery(t);
+  }
+
+   /**
+   * Factory method for generating a query (similar to
+   * {@link #getWildcardQuery}). Called when parser parses
+   * an input term token that has the fuzzy suffix (~) appended.
+   *
+   * @param field Name of the field query will use.
+   * @param termStr Term token to use for building term for the query
+   *
+   * @return Resulting {@link Query} built for the term
+   * @exception ParseException throw in overridden method to disallow
+   */
+  protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException
+  {
+    if (lowercaseExpandedTerms) {
+      termStr = termStr.toLowerCase();
+    }
+    Term t = new Term(field, termStr);
+    return new FuzzyQuery(t, minSimilarity, fuzzyPrefixLength);
+  }
+
+  /**
+   * Returns a String where the escape char has been
+   * removed, or kept only once if there was a double escape.
+   */
+  private String discardEscapeChar(String input) {
+    char[] caSource = input.toCharArray();
+    char[] caDest = new char[caSource.length];
+    int j = 0;
+    for (int i = 0; i < caSource.length; i++) {
+      if ((caSource[i] != '\\') || (i > 0 && caSource[i-1] == '\\')) {
+        caDest[j++]=caSource[i];
+      }
+    }
+    return new String(caDest, 0, j);
+  }
+
+  /**
+   * Returns a String where those characters that QueryParser
+   * expects to be escaped are escaped by a preceding <code>\</code>.
+   */
+  public static String escape(String s) {
+    StringBuffer sb = new StringBuffer();
+    for (int i = 0; i < s.length(); i++) {
+      char c = s.charAt(i);
+      // NOTE: keep this in sync with _ESCAPED_CHAR below!
+      if (c == '\\' || c == '+' || c == '-' || c == '!' || c == '(' || c == ')' || c == ':'
+        || c == '^' || c == '[' || c == ']' || c == '\"' || c == '{' || c == '}' || c == '~'
+        || c == '*' || c == '?') {
+        sb.append('\\');
+      }
+      sb.append(c);
+    }
+    return sb.toString();
+  }
+
+  /**
+   * Command line tool to test QueryParser, using {@link org.apache.lucene.analysis.SimpleAnalyzer}.
+   * Usage:<br>
+   * <code>java org.apache.lucene.queryParser.QueryParser &lt;input&gt;</code>
+   */
+  public static void main(String[] args) throws Exception {
+    if (args.length == 0) {
+      System.out.println("Usage: java org.apache.lucene.queryParser.QueryParser <input>");
+      System.exit(0);
+    }
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field",
+                           new org.apache.lucene.analysis.SimpleAnalyzer());
+    Query q = qp.parse(args[0]);
+    System.out.println(q.toString("field"));
+  }
+}
+
+PARSER_END(PrecedenceQueryParser)
+
+/* ***************** */
+/* Token Definitions */
+/* ***************** */
+
+<*> TOKEN : {
+  <#_NUM_CHAR:   ["0"-"9"] >
+// NOTE: keep this in sync with escape(String) above!
+| <#_ESCAPED_CHAR: "\\" [ "\\", "+", "-", "!", "(", ")", ":", "^",
+                          "[", "]", "\"", "{", "}", "~", "*", "?" ] >
+| <#_TERM_START_CHAR: ( ~[ " ", "\t", "\n", "\r", "+", "-", "!", "(", ")", ":", "^",
+                           "[", "]", "\"", "{", "}", "~", "*", "?" ]
+                       | <_ESCAPED_CHAR> ) >
+| <#_TERM_CHAR: ( <_TERM_START_CHAR> | <_ESCAPED_CHAR> | "-" | "+" ) >
+| <#_WHITESPACE: ( " " | "\t" | "\n" | "\r") >
+}
+
+<DEFAULT, RangeIn, RangeEx> SKIP : {
+  < <_WHITESPACE>>
+}
+
+// OG: to support prefix queries:
+// http://nagoya.apache.org/bugzilla/show_bug.cgi?id=12137
+// Change from:
+// | <WILDTERM:  <_TERM_START_CHAR>
+//              (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
+// To:
+//
+// | <WILDTERM:  (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
+
+<DEFAULT> TOKEN : {
+  <AND:       ("AND" | "&&") >
+| <OR:        ("OR" | "||") >
+| <NOT:       ("NOT" | "!") >
+| <PLUS:      "+" >
+| <MINUS:     "-" >
+| <LPAREN:    "(" >
+| <RPAREN:    ")" >
+| <COLON:     ":" >
+| <CARAT:     "^" > : Boost
+| <QUOTED:     "\"" (~["\""])+ "\"">
+| <TERM:      <_TERM_START_CHAR> (<_TERM_CHAR>)*  >
+| <FUZZY_SLOP:     "~" ( (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? )? >
+| <PREFIXTERM:  <_TERM_START_CHAR> (<_TERM_CHAR>)* "*" >
+| <WILDTERM:  <_TERM_START_CHAR>
+              (<_TERM_CHAR> | ( [ "*", "?" ] ))* >
+| <RANGEIN_START: "[" > : RangeIn
+| <RANGEEX_START: "{" > : RangeEx
+}
+
+<Boost> TOKEN : {
+<NUMBER:    (<_NUM_CHAR>)+ ( "." (<_NUM_CHAR>)+ )? > : DEFAULT
+}
+
+<RangeIn> TOKEN : {
+<RANGEIN_TO: "TO">
+| <RANGEIN_END: "]"> : DEFAULT
+| <RANGEIN_QUOTED: "\"" (~["\""])+ "\"">
+| <RANGEIN_GOOP: (~[ " ", "]" ])+ >
+}
+
+<RangeEx> TOKEN : {
+<RANGEEX_TO: "TO">
+| <RANGEEX_END: "}"> : DEFAULT
+| <RANGEEX_QUOTED: "\"" (~["\""])+ "\"">
+| <RANGEEX_GOOP: (~[ " ", "}" ])+ >
+}
+
+// *   Query  ::= ( Clause )*
+// *   Clause ::= ["+", "-"] [<TERM> ":"] ( <TERM> | "(" Query ")" )
+
+int Conjunction() : {
+  int ret = CONJ_NONE;
+}
+{
+  [
+    <AND> { ret = CONJ_AND; }
+    | <OR>  { ret = CONJ_OR; }
+  ]
+  { return ret; }
+}
+
+int Modifier() : {
+  int ret = MOD_NONE;
+}
+{
+  [
+     <PLUS> { ret = MOD_REQ; }
+     | <MINUS> { ret = MOD_NOT; }
+     | <NOT> { ret = MOD_NOT; }
+  ]
+  { return ret; }
+}
+
+Query Query(String field) :
+{
+  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+  Query q, firstQuery=null;
+  boolean orPresent = false;
+  int modifier;
+}
+{
+  modifier=Modifier() q=andExpression(field)
+  {
+    addClause(clauses, CONJ_NONE, modifier, q);
+    if (modifier == MOD_NONE)
+      firstQuery = q;
+  }
+  (
+    [<OR> { orPresent=true; }] modifier=Modifier() q=andExpression(field)
+    { addClause(clauses, orPresent ? CONJ_OR : CONJ_NONE, modifier, q); }
+  )*
+    {
+      if (clauses.size() == 1 && firstQuery != null)
+        return firstQuery;
+      else {
+        return getBooleanQuery(clauses);
+      }
+    }
+}
+
+Query andExpression(String field) :
+{
+  List<BooleanClause> clauses = new ArrayList<BooleanClause>();
+  Query q, firstQuery=null;
+  int modifier;
+}
+{
+  q=Clause(field)
+  {
+    addClause(clauses, CONJ_NONE, MOD_NONE, q);
+    firstQuery = q;
+  }
+  (
+    <AND> modifier=Modifier() q=Clause(field)
+    { addClause(clauses, CONJ_AND, modifier, q); }
+  )*
+    {
+      if (clauses.size() == 1 && firstQuery != null)
+        return firstQuery;
+      else {
+        return getBooleanQuery(clauses);
+      }
+    }
+}
+
+Query Clause(String field) : {
+  Query q;
+  Token fieldToken=null, boost=null;
+}
+{
+  [
+    LOOKAHEAD(2)
+    fieldToken=<TERM> <COLON> {
+      field=discardEscapeChar(fieldToken.image);
+    }
+  ]
+
+  (
+   q=Term(field)
+   | <LPAREN> q=Query(field) <RPAREN> (<CARAT> boost=<NUMBER>)?
+
+  )
+    {
+      if (boost != null) {
+        float f = (float)1.0;
+  try {
+    f = Float.valueOf(boost.image).floatValue();
+          q.setBoost(f);
+  } catch (Exception ignored) { }
+      }
+      return q;
+    }
+}
+
+
+Query Term(String field) : {
+  Token term, boost=null, fuzzySlop=null, goop1, goop2;
+  boolean prefix = false;
+  boolean wildcard = false;
+  boolean fuzzy = false;
+  Query q;
+}
+{
+  (
+     (
+       term=<TERM>
+       | term=<PREFIXTERM> { prefix=true; }
+       | term=<WILDTERM> { wildcard=true; }
+       | term=<NUMBER>
+     )
+     [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ]
+     [ <CARAT> boost=<NUMBER> [ fuzzySlop=<FUZZY_SLOP> { fuzzy=true; } ] ]
+     {
+       String termImage=discardEscapeChar(term.image);
+       if (wildcard) {
+       q = getWildcardQuery(field, termImage);
+       } else if (prefix) {
+         q = getPrefixQuery(field,
+           discardEscapeChar(term.image.substring
+          (0, term.image.length()-1)));
+       } else if (fuzzy) {
+       	  float fms = fuzzyMinSim;
+       	  try {
+            fms = Float.valueOf(fuzzySlop.image.substring(1)).floatValue();
+       	  } catch (Exception ignored) { }
+       	 if(fms < 0.0f || fms > 1.0f){
+       	   throw new ParseException("Minimum similarity for a FuzzyQuery has to be between 0.0f and 1.0f !");
+       	 }
+         q = getFuzzyQuery(field, termImage, fms);
+       } else {
+         q = getFieldQuery(field, termImage);
+       }
+     }
+     | ( <RANGEIN_START> ( goop1=<RANGEIN_GOOP>|goop1=<RANGEIN_QUOTED> )
+         [ <RANGEIN_TO> ] ( goop2=<RANGEIN_GOOP>|goop2=<RANGEIN_QUOTED> )
+         <RANGEIN_END> )
+       [ <CARAT> boost=<NUMBER> ]
+        {
+          if (goop1.kind == RANGEIN_QUOTED) {
+            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
+          } else {
+            goop1.image = discardEscapeChar(goop1.image);
+          }
+          if (goop2.kind == RANGEIN_QUOTED) {
+            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
+      } else {
+        goop2.image = discardEscapeChar(goop2.image);
+      }
+          q = getRangeQuery(field, goop1.image, goop2.image, true);
+        }
+     | ( <RANGEEX_START> ( goop1=<RANGEEX_GOOP>|goop1=<RANGEEX_QUOTED> )
+         [ <RANGEEX_TO> ] ( goop2=<RANGEEX_GOOP>|goop2=<RANGEEX_QUOTED> )
+         <RANGEEX_END> )
+       [ <CARAT> boost=<NUMBER> ]
+        {
+          if (goop1.kind == RANGEEX_QUOTED) {
+            goop1.image = goop1.image.substring(1, goop1.image.length()-1);
+          } else {
+            goop1.image = discardEscapeChar(goop1.image);
+          }
+          if (goop2.kind == RANGEEX_QUOTED) {
+            goop2.image = goop2.image.substring(1, goop2.image.length()-1);
+      } else {
+        goop2.image = discardEscapeChar(goop2.image);
+      }
+
+          q = getRangeQuery(field, goop1.image, goop2.image, false);
+        }
+     | term=<QUOTED>
+       [ fuzzySlop=<FUZZY_SLOP> ]
+       [ <CARAT> boost=<NUMBER> ]
+       {
+         int s = phraseSlop;
+
+         if (fuzzySlop != null) {
+           try {
+             s = Float.valueOf(fuzzySlop.image.substring(1)).intValue();
+           }
+           catch (Exception ignored) { }
+         }
+         q = getFieldQuery(field, term.image.substring(1, term.image.length()-1), s);
+       }
+  )
+  {
+    if (boost != null) {
+      float f = (float) 1.0;
+      try {
+        f = Float.valueOf(boost.image).floatValue();
+      }
+      catch (Exception ignored) {
+    /* Should this be handled somehow? (defaults to "no boost", if
+     * boost number is invalid)
+     */
+      }
+
+      // avoid boosting null queries, such as those caused by stop words
+      if (q != null) {
+        q.setBoost(f);
+      }
+    }
+    return q;
+  }
+}
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java
new file mode 100644
index 0000000..50c55bd
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserConstants.java
@@ -0,0 +1,119 @@
+/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParserConstants.java */
+package org.apache.lucene.queryParser.precedence;
+
+
+/**
+ * Token literal values and constants.
+ * Generated by org.javacc.parser.OtherFilesGen#start()
+ */
+public interface PrecedenceQueryParserConstants {
+
+  /** End of File. */
+  int EOF = 0;
+  /** RegularExpression Id. */
+  int _NUM_CHAR = 1;
+  /** RegularExpression Id. */
+  int _ESCAPED_CHAR = 2;
+  /** RegularExpression Id. */
+  int _TERM_START_CHAR = 3;
+  /** RegularExpression Id. */
+  int _TERM_CHAR = 4;
+  /** RegularExpression Id. */
+  int _WHITESPACE = 5;
+  /** RegularExpression Id. */
+  int AND = 7;
+  /** RegularExpression Id. */
+  int OR = 8;
+  /** RegularExpression Id. */
+  int NOT = 9;
+  /** RegularExpression Id. */
+  int PLUS = 10;
+  /** RegularExpression Id. */
+  int MINUS = 11;
+  /** RegularExpression Id. */
+  int LPAREN = 12;
+  /** RegularExpression Id. */
+  int RPAREN = 13;
+  /** RegularExpression Id. */
+  int COLON = 14;
+  /** RegularExpression Id. */
+  int CARAT = 15;
+  /** RegularExpression Id. */
+  int QUOTED = 16;
+  /** RegularExpression Id. */
+  int TERM = 17;
+  /** RegularExpression Id. */
+  int FUZZY_SLOP = 18;
+  /** RegularExpression Id. */
+  int PREFIXTERM = 19;
+  /** RegularExpression Id. */
+  int WILDTERM = 20;
+  /** RegularExpression Id. */
+  int RANGEIN_START = 21;
+  /** RegularExpression Id. */
+  int RANGEEX_START = 22;
+  /** RegularExpression Id. */
+  int NUMBER = 23;
+  /** RegularExpression Id. */
+  int RANGEIN_TO = 24;
+  /** RegularExpression Id. */
+  int RANGEIN_END = 25;
+  /** RegularExpression Id. */
+  int RANGEIN_QUOTED = 26;
+  /** RegularExpression Id. */
+  int RANGEIN_GOOP = 27;
+  /** RegularExpression Id. */
+  int RANGEEX_TO = 28;
+  /** RegularExpression Id. */
+  int RANGEEX_END = 29;
+  /** RegularExpression Id. */
+  int RANGEEX_QUOTED = 30;
+  /** RegularExpression Id. */
+  int RANGEEX_GOOP = 31;
+
+  /** Lexical state. */
+  int Boost = 0;
+  /** Lexical state. */
+  int RangeEx = 1;
+  /** Lexical state. */
+  int RangeIn = 2;
+  /** Lexical state. */
+  int DEFAULT = 3;
+
+  /** Literal token values. */
+  String[] tokenImage = {
+    "<EOF>",
+    "<_NUM_CHAR>",
+    "<_ESCAPED_CHAR>",
+    "<_TERM_START_CHAR>",
+    "<_TERM_CHAR>",
+    "<_WHITESPACE>",
+    "<token of kind 6>",
+    "<AND>",
+    "<OR>",
+    "<NOT>",
+    "\"+\"",
+    "\"-\"",
+    "\"(\"",
+    "\")\"",
+    "\":\"",
+    "\"^\"",
+    "<QUOTED>",
+    "<TERM>",
+    "<FUZZY_SLOP>",
+    "<PREFIXTERM>",
+    "<WILDTERM>",
+    "\"[\"",
+    "\"{\"",
+    "<NUMBER>",
+    "\"TO\"",
+    "\"]\"",
+    "<RANGEIN_QUOTED>",
+    "<RANGEIN_GOOP>",
+    "\"TO\"",
+    "\"}\"",
+    "<RANGEEX_QUOTED>",
+    "<RANGEEX_GOOP>",
+  };
+
+}
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java
new file mode 100644
index 0000000..6f4878b
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParserTokenManager.java
@@ -0,0 +1,1081 @@
+/* Generated By:JavaCC: Do not edit this line. PrecedenceQueryParserTokenManager.java */
+package org.apache.lucene.queryParser.precedence;
+import java.io.IOException;
+import java.io.StringReader;
+import java.text.DateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.Locale;
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.*;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.MultiPhraseQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.AttributeSource;
+
+/** Token Manager. */
+public class PrecedenceQueryParserTokenManager implements PrecedenceQueryParserConstants
+{
+
+  /** Debug output. */
+  public  java.io.PrintStream debugStream = System.out;
+  /** Set debug output. */
+  public  void setDebugStream(java.io.PrintStream ds) { debugStream = ds; }
+private final int jjStopStringLiteralDfa_3(int pos, long active0)
+{
+   switch (pos)
+   {
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_3(int pos, long active0)
+{
+   return jjMoveNfa_3(jjStopStringLiteralDfa_3(pos, active0), pos + 1);
+}
+private int jjStopAtPos(int pos, int kind)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   return pos + 1;
+}
+private int jjMoveStringLiteralDfa0_3()
+{
+   switch(curChar)
+   {
+      case 40:
+         return jjStopAtPos(0, 12);
+      case 41:
+         return jjStopAtPos(0, 13);
+      case 43:
+         return jjStopAtPos(0, 10);
+      case 45:
+         return jjStopAtPos(0, 11);
+      case 58:
+         return jjStopAtPos(0, 14);
+      case 91:
+         return jjStopAtPos(0, 21);
+      case 94:
+         return jjStopAtPos(0, 15);
+      case 123:
+         return jjStopAtPos(0, 22);
+      default :
+         return jjMoveNfa_3(0, 0);
+   }
+}
+static final long[] jjbitVec0 = {
+   0xfffffffffffffffeL, 0xffffffffffffffffL, 0xffffffffffffffffL, 0xffffffffffffffffL
+};
+static final long[] jjbitVec2 = {
+   0x0L, 0x0L, 0xffffffffffffffffL, 0xffffffffffffffffL
+};
+private int jjMoveNfa_3(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 33;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0x7bffd0f8ffffd9ffL & l) != 0L)
+                  {
+                     if (kind > 17)
+                        kind = 17;
+                     jjCheckNAddStates(0, 6);
+                  }
+                  else if ((0x100002600L & l) != 0L)
+                  {
+                     if (kind > 6)
+                        kind = 6;
+                  }
+                  else if (curChar == 34)
+                     jjCheckNAdd(15);
+                  else if (curChar == 33)
+                  {
+                     if (kind > 9)
+                        kind = 9;
+                  }
+                  if (curChar == 38)
+                     jjstateSet[jjnewStateCnt++] = 4;
+                  break;
+               case 4:
+                  if (curChar == 38 && kind > 7)
+                     kind = 7;
+                  break;
+               case 5:
+                  if (curChar == 38)
+                     jjstateSet[jjnewStateCnt++] = 4;
+                  break;
+               case 13:
+                  if (curChar == 33 && kind > 9)
+                     kind = 9;
+                  break;
+               case 14:
+                  if (curChar == 34)
+                     jjCheckNAdd(15);
+                  break;
+               case 15:
+                  if ((0xfffffffbffffffffL & l) != 0L)
+                     jjCheckNAddTwoStates(15, 16);
+                  break;
+               case 16:
+                  if (curChar == 34 && kind > 16)
+                     kind = 16;
+                  break;
+               case 18:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 18)
+                     kind = 18;
+                  jjAddStates(7, 8);
+                  break;
+               case 19:
+                  if (curChar == 46)
+                     jjCheckNAdd(20);
+                  break;
+               case 20:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 18)
+                     kind = 18;
+                  jjCheckNAdd(20);
+                  break;
+               case 21:
+                  if ((0x7bffd0f8ffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddStates(0, 6);
+                  break;
+               case 22:
+                  if ((0x7bfff8f8ffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddTwoStates(22, 23);
+                  break;
+               case 24:
+                  if ((0x84002f0600000000L & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddTwoStates(22, 23);
+                  break;
+               case 25:
+                  if ((0x7bfff8f8ffffd9ffL & l) != 0L)
+                     jjCheckNAddStates(9, 11);
+                  break;
+               case 26:
+                  if (curChar == 42 && kind > 19)
+                     kind = 19;
+                  break;
+               case 28:
+                  if ((0x84002f0600000000L & l) != 0L)
+                     jjCheckNAddStates(9, 11);
+                  break;
+               case 29:
+                  if ((0xfbfffcf8ffffd9ffL & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(29, 30);
+                  break;
+               case 31:
+                  if ((0x84002f0600000000L & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(29, 30);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0x97ffffff97ffffffL & l) != 0L)
+                  {
+                     if (kind > 17)
+                        kind = 17;
+                     jjCheckNAddStates(0, 6);
+                  }
+                  else if (curChar == 126)
+                  {
+                     if (kind > 18)
+                        kind = 18;
+                     jjstateSet[jjnewStateCnt++] = 18;
+                  }
+                  if (curChar == 92)
+                     jjCheckNAddStates(12, 14);
+                  else if (curChar == 78)
+                     jjstateSet[jjnewStateCnt++] = 11;
+                  else if (curChar == 124)
+                     jjstateSet[jjnewStateCnt++] = 8;
+                  else if (curChar == 79)
+                     jjstateSet[jjnewStateCnt++] = 6;
+                  else if (curChar == 65)
+                     jjstateSet[jjnewStateCnt++] = 2;
+                  break;
+               case 1:
+                  if (curChar == 68 && kind > 7)
+                     kind = 7;
+                  break;
+               case 2:
+                  if (curChar == 78)
+                     jjstateSet[jjnewStateCnt++] = 1;
+                  break;
+               case 3:
+                  if (curChar == 65)
+                     jjstateSet[jjnewStateCnt++] = 2;
+                  break;
+               case 6:
+                  if (curChar == 82 && kind > 8)
+                     kind = 8;
+                  break;
+               case 7:
+                  if (curChar == 79)
+                     jjstateSet[jjnewStateCnt++] = 6;
+                  break;
+               case 8:
+                  if (curChar == 124 && kind > 8)
+                     kind = 8;
+                  break;
+               case 9:
+                  if (curChar == 124)
+                     jjstateSet[jjnewStateCnt++] = 8;
+                  break;
+               case 10:
+                  if (curChar == 84 && kind > 9)
+                     kind = 9;
+                  break;
+               case 11:
+                  if (curChar == 79)
+                     jjstateSet[jjnewStateCnt++] = 10;
+                  break;
+               case 12:
+                  if (curChar == 78)
+                     jjstateSet[jjnewStateCnt++] = 11;
+                  break;
+               case 15:
+                  jjAddStates(15, 16);
+                  break;
+               case 17:
+                  if (curChar != 126)
+                     break;
+                  if (kind > 18)
+                     kind = 18;
+                  jjstateSet[jjnewStateCnt++] = 18;
+                  break;
+               case 21:
+                  if ((0x97ffffff97ffffffL & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddStates(0, 6);
+                  break;
+               case 22:
+                  if ((0x97ffffff97ffffffL & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddTwoStates(22, 23);
+                  break;
+               case 23:
+                  if (curChar == 92)
+                     jjCheckNAddTwoStates(24, 24);
+                  break;
+               case 24:
+                  if ((0x6800000078000000L & l) == 0L)
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddTwoStates(22, 23);
+                  break;
+               case 25:
+                  if ((0x97ffffff97ffffffL & l) != 0L)
+                     jjCheckNAddStates(9, 11);
+                  break;
+               case 27:
+                  if (curChar == 92)
+                     jjCheckNAddTwoStates(28, 28);
+                  break;
+               case 28:
+                  if ((0x6800000078000000L & l) != 0L)
+                     jjCheckNAddStates(9, 11);
+                  break;
+               case 29:
+                  if ((0x97ffffff97ffffffL & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(29, 30);
+                  break;
+               case 30:
+                  if (curChar == 92)
+                     jjCheckNAddTwoStates(31, 31);
+                  break;
+               case 31:
+                  if ((0x6800000078000000L & l) == 0L)
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(29, 30);
+                  break;
+               case 32:
+                  if (curChar == 92)
+                     jjCheckNAddStates(12, 14);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int hiByte = (int)(curChar >> 8);
+         int i1 = hiByte >> 6;
+         long l1 = 1L << (hiByte & 077);
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddStates(0, 6);
+                  break;
+               case 15:
+                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     jjAddStates(15, 16);
+                  break;
+               case 22:
+                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 17)
+                     kind = 17;
+                  jjCheckNAddTwoStates(22, 23);
+                  break;
+               case 25:
+                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     jjCheckNAddStates(9, 11);
+                  break;
+               case 29:
+                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 20)
+                     kind = 20;
+                  jjCheckNAddTwoStates(29, 30);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 33 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private final int jjStopStringLiteralDfa_1(int pos, long active0)
+{
+   switch (pos)
+   {
+      case 0:
+         if ((active0 & 0x10000000L) != 0L)
+         {
+            jjmatchedKind = 31;
+            return 4;
+         }
+         return -1;
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_1(int pos, long active0)
+{
+   return jjMoveNfa_1(jjStopStringLiteralDfa_1(pos, active0), pos + 1);
+}
+private int jjMoveStringLiteralDfa0_1()
+{
+   switch(curChar)
+   {
+      case 84:
+         return jjMoveStringLiteralDfa1_1(0x10000000L);
+      case 125:
+         return jjStopAtPos(0, 29);
+      default :
+         return jjMoveNfa_1(0, 0);
+   }
+}
+private int jjMoveStringLiteralDfa1_1(long active0)
+{
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_1(0, active0);
+      return 1;
+   }
+   switch(curChar)
+   {
+      case 79:
+         if ((active0 & 0x10000000L) != 0L)
+            return jjStartNfaWithStates_1(1, 28, 4);
+         break;
+      default :
+         break;
+   }
+   return jjStartNfa_1(0, active0);
+}
+private int jjStartNfaWithStates_1(int pos, int kind, int state)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) { return pos + 1; }
+   return jjMoveNfa_1(state, pos + 1);
+}
+private int jjMoveNfa_1(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 5;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0xfffffffeffffffffL & l) != 0L)
+                  {
+                     if (kind > 31)
+                        kind = 31;
+                     jjCheckNAdd(4);
+                  }
+                  if ((0x100002600L & l) != 0L)
+                  {
+                     if (kind > 6)
+                        kind = 6;
+                  }
+                  else if (curChar == 34)
+                     jjCheckNAdd(2);
+                  break;
+               case 1:
+                  if (curChar == 34)
+                     jjCheckNAdd(2);
+                  break;
+               case 2:
+                  if ((0xfffffffbffffffffL & l) != 0L)
+                     jjCheckNAddTwoStates(2, 3);
+                  break;
+               case 3:
+                  if (curChar == 34 && kind > 30)
+                     kind = 30;
+                  break;
+               case 4:
+                  if ((0xfffffffeffffffffL & l) == 0L)
+                     break;
+                  if (kind > 31)
+                     kind = 31;
+                  jjCheckNAdd(4);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 4:
+                  if ((0xdfffffffffffffffL & l) == 0L)
+                     break;
+                  if (kind > 31)
+                     kind = 31;
+                  jjCheckNAdd(4);
+                  break;
+               case 2:
+                  jjAddStates(17, 18);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int hiByte = (int)(curChar >> 8);
+         int i1 = hiByte >> 6;
+         long l1 = 1L << (hiByte & 077);
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 4:
+                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 31)
+                     kind = 31;
+                  jjCheckNAdd(4);
+                  break;
+               case 2:
+                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     jjAddStates(17, 18);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 5 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private int jjMoveStringLiteralDfa0_0()
+{
+   return jjMoveNfa_0(0, 0);
+}
+private int jjMoveNfa_0(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 3;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjAddStates(19, 20);
+                  break;
+               case 1:
+                  if (curChar == 46)
+                     jjCheckNAdd(2);
+                  break;
+               case 2:
+                  if ((0x3ff000000000000L & l) == 0L)
+                     break;
+                  if (kind > 23)
+                     kind = 23;
+                  jjCheckNAdd(2);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int hiByte = (int)(curChar >> 8);
+         int i1 = hiByte >> 6;
+         long l1 = 1L << (hiByte & 077);
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 3 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+private final int jjStopStringLiteralDfa_2(int pos, long active0)
+{
+   switch (pos)
+   {
+      case 0:
+         if ((active0 & 0x1000000L) != 0L)
+         {
+            jjmatchedKind = 27;
+            return 4;
+         }
+         return -1;
+      default :
+         return -1;
+   }
+}
+private final int jjStartNfa_2(int pos, long active0)
+{
+   return jjMoveNfa_2(jjStopStringLiteralDfa_2(pos, active0), pos + 1);
+}
+private int jjMoveStringLiteralDfa0_2()
+{
+   switch(curChar)
+   {
+      case 84:
+         return jjMoveStringLiteralDfa1_2(0x1000000L);
+      case 93:
+         return jjStopAtPos(0, 25);
+      default :
+         return jjMoveNfa_2(0, 0);
+   }
+}
+private int jjMoveStringLiteralDfa1_2(long active0)
+{
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) {
+      jjStopStringLiteralDfa_2(0, active0);
+      return 1;
+   }
+   switch(curChar)
+   {
+      case 79:
+         if ((active0 & 0x1000000L) != 0L)
+            return jjStartNfaWithStates_2(1, 24, 4);
+         break;
+      default :
+         break;
+   }
+   return jjStartNfa_2(0, active0);
+}
+private int jjStartNfaWithStates_2(int pos, int kind, int state)
+{
+   jjmatchedKind = kind;
+   jjmatchedPos = pos;
+   try { curChar = input_stream.readChar(); }
+   catch(java.io.IOException e) { return pos + 1; }
+   return jjMoveNfa_2(state, pos + 1);
+}
+private int jjMoveNfa_2(int startState, int curPos)
+{
+   int startsAt = 0;
+   jjnewStateCnt = 5;
+   int i = 1;
+   jjstateSet[0] = startState;
+   int kind = 0x7fffffff;
+   for (;;)
+   {
+      if (++jjround == 0x7fffffff)
+         ReInitRounds();
+      if (curChar < 64)
+      {
+         long l = 1L << curChar;
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+                  if ((0xfffffffeffffffffL & l) != 0L)
+                  {
+                     if (kind > 27)
+                        kind = 27;
+                     jjCheckNAdd(4);
+                  }
+                  if ((0x100002600L & l) != 0L)
+                  {
+                     if (kind > 6)
+                        kind = 6;
+                  }
+                  else if (curChar == 34)
+                     jjCheckNAdd(2);
+                  break;
+               case 1:
+                  if (curChar == 34)
+                     jjCheckNAdd(2);
+                  break;
+               case 2:
+                  if ((0xfffffffbffffffffL & l) != 0L)
+                     jjCheckNAddTwoStates(2, 3);
+                  break;
+               case 3:
+                  if (curChar == 34 && kind > 26)
+                     kind = 26;
+                  break;
+               case 4:
+                  if ((0xfffffffeffffffffL & l) == 0L)
+                     break;
+                  if (kind > 27)
+                     kind = 27;
+                  jjCheckNAdd(4);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else if (curChar < 128)
+      {
+         long l = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 4:
+                  if ((0xffffffffdfffffffL & l) == 0L)
+                     break;
+                  if (kind > 27)
+                     kind = 27;
+                  jjCheckNAdd(4);
+                  break;
+               case 2:
+                  jjAddStates(17, 18);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      else
+      {
+         int hiByte = (int)(curChar >> 8);
+         int i1 = hiByte >> 6;
+         long l1 = 1L << (hiByte & 077);
+         int i2 = (curChar & 0xff) >> 6;
+         long l2 = 1L << (curChar & 077);
+         do
+         {
+            switch(jjstateSet[--i])
+            {
+               case 0:
+               case 4:
+                  if (!jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     break;
+                  if (kind > 27)
+                     kind = 27;
+                  jjCheckNAdd(4);
+                  break;
+               case 2:
+                  if (jjCanMove_0(hiByte, i1, i2, l1, l2))
+                     jjAddStates(17, 18);
+                  break;
+               default : break;
+            }
+         } while(i != startsAt);
+      }
+      if (kind != 0x7fffffff)
+      {
+         jjmatchedKind = kind;
+         jjmatchedPos = curPos;
+         kind = 0x7fffffff;
+      }
+      ++curPos;
+      if ((i = jjnewStateCnt) == (startsAt = 5 - (jjnewStateCnt = startsAt)))
+         return curPos;
+      try { curChar = input_stream.readChar(); }
+      catch(java.io.IOException e) { return curPos; }
+   }
+}
+static final int[] jjnextStates = {
+   22, 25, 26, 29, 30, 27, 23, 18, 19, 25, 26, 27, 24, 28, 31, 15, 
+   16, 2, 3, 0, 1, 
+};
+private static final boolean jjCanMove_0(int hiByte, int i1, int i2, long l1, long l2)
+{
+   switch(hiByte)
+   {
+      case 0:
+         return ((jjbitVec2[i2] & l2) != 0L);
+      default :
+         if ((jjbitVec0[i1] & l1) != 0L)
+            return true;
+         return false;
+   }
+}
+
+/** Token literal values. */
+public static final String[] jjstrLiteralImages = {
+"", null, null, null, null, null, null, null, null, null, "\53", "\55", "\50", 
+"\51", "\72", "\136", null, null, null, null, null, "\133", "\173", null, "\124\117", 
+"\135", null, null, "\124\117", "\175", null, null, };
+
+/** Lexer state names. */
+public static final String[] lexStateNames = {
+   "Boost",
+   "RangeEx",
+   "RangeIn",
+   "DEFAULT",
+};
+
+/** Lex State array. */
+public static final int[] jjnewLexState = {
+   -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 2, 1, 3, -1, 
+   3, -1, -1, -1, 3, -1, -1, 
+};
+static final long[] jjtoToken = {
+   0xffffff81L, 
+};
+static final long[] jjtoSkip = {
+   0x40L, 
+};
+protected CharStream input_stream;
+private final int[] jjrounds = new int[33];
+private final int[] jjstateSet = new int[66];
+protected char curChar;
+/** Constructor. */
+public PrecedenceQueryParserTokenManager(CharStream stream){
+   input_stream = stream;
+}
+
+/** Constructor. */
+public PrecedenceQueryParserTokenManager(CharStream stream, int lexState){
+   this(stream);
+   SwitchTo(lexState);
+}
+
+/** Reinitialise parser. */
+public void ReInit(CharStream stream)
+{
+   jjmatchedPos = jjnewStateCnt = 0;
+   curLexState = defaultLexState;
+   input_stream = stream;
+   ReInitRounds();
+}
+private void ReInitRounds()
+{
+   int i;
+   jjround = 0x80000001;
+   for (i = 33; i-- > 0;)
+      jjrounds[i] = 0x80000000;
+}
+
+/** Reinitialise parser. */
+public void ReInit(CharStream stream, int lexState)
+{
+   ReInit(stream);
+   SwitchTo(lexState);
+}
+
+/** Switch to specified lex state. */
+public void SwitchTo(int lexState)
+{
+   if (lexState >= 4 || lexState < 0)
+      throw new TokenMgrError("Error: Ignoring invalid lexical state : " + lexState + ". State unchanged.", TokenMgrError.INVALID_LEXICAL_STATE);
+   else
+      curLexState = lexState;
+}
+
+protected Token jjFillToken()
+{
+   final Token t;
+   final String curTokenImage;
+   final int beginLine;
+   final int endLine;
+   final int beginColumn;
+   final int endColumn;
+   String im = jjstrLiteralImages[jjmatchedKind];
+   curTokenImage = (im == null) ? input_stream.GetImage() : im;
+   beginLine = input_stream.getBeginLine();
+   beginColumn = input_stream.getBeginColumn();
+   endLine = input_stream.getEndLine();
+   endColumn = input_stream.getEndColumn();
+   t = Token.newToken(jjmatchedKind, curTokenImage);
+
+   t.beginLine = beginLine;
+   t.endLine = endLine;
+   t.beginColumn = beginColumn;
+   t.endColumn = endColumn;
+
+   return t;
+}
+
+int curLexState = 3;
+int defaultLexState = 3;
+int jjnewStateCnt;
+int jjround;
+int jjmatchedPos;
+int jjmatchedKind;
+
+/** Get the next Token. */
+public Token getNextToken() 
+{
+  Token matchedToken;
+  int curPos = 0;
+
+  EOFLoop :
+  for (;;)
+  {
+   try
+   {
+      curChar = input_stream.BeginToken();
+   }
+   catch(java.io.IOException e)
+   {
+      jjmatchedKind = 0;
+      matchedToken = jjFillToken();
+      return matchedToken;
+   }
+
+   switch(curLexState)
+   {
+     case 0:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_0();
+       break;
+     case 1:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_1();
+       break;
+     case 2:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_2();
+       break;
+     case 3:
+       jjmatchedKind = 0x7fffffff;
+       jjmatchedPos = 0;
+       curPos = jjMoveStringLiteralDfa0_3();
+       break;
+   }
+     if (jjmatchedKind != 0x7fffffff)
+     {
+        if (jjmatchedPos + 1 < curPos)
+           input_stream.backup(curPos - jjmatchedPos - 1);
+        if ((jjtoToken[jjmatchedKind >> 6] & (1L << (jjmatchedKind & 077))) != 0L)
+        {
+           matchedToken = jjFillToken();
+       if (jjnewLexState[jjmatchedKind] != -1)
+         curLexState = jjnewLexState[jjmatchedKind];
+           return matchedToken;
+        }
+        else
+        {
+         if (jjnewLexState[jjmatchedKind] != -1)
+           curLexState = jjnewLexState[jjmatchedKind];
+           continue EOFLoop;
+        }
+     }
+     int error_line = input_stream.getEndLine();
+     int error_column = input_stream.getEndColumn();
+     String error_after = null;
+     boolean EOFSeen = false;
+     try { input_stream.readChar(); input_stream.backup(1); }
+     catch (java.io.IOException e1) {
+        EOFSeen = true;
+        error_after = curPos <= 1 ? "" : input_stream.GetImage();
+        if (curChar == '\n' || curChar == '\r') {
+           error_line++;
+           error_column = 0;
+        }
+        else
+           error_column++;
+     }
+     if (!EOFSeen) {
+        input_stream.backup(1);
+        error_after = curPos <= 1 ? "" : input_stream.GetImage();
+     }
+     throw new TokenMgrError(EOFSeen, curLexState, error_line, error_column, error_after, curChar, TokenMgrError.LEXICAL_ERROR);
+  }
+}
+
+private void jjCheckNAdd(int state)
+{
+   if (jjrounds[state] != jjround)
+   {
+      jjstateSet[jjnewStateCnt++] = state;
+      jjrounds[state] = jjround;
+   }
+}
+private void jjAddStates(int start, int end)
+{
+   do {
+      jjstateSet[jjnewStateCnt++] = jjnextStates[start];
+   } while (start++ != end);
+}
+private void jjCheckNAddTwoStates(int state1, int state2)
+{
+   jjCheckNAdd(state1);
+   jjCheckNAdd(state2);
+}
+
+private void jjCheckNAddStates(int start, int end)
+{
+   do {
+      jjCheckNAdd(jjnextStates[start]);
+   } while (start++ != end);
+}
+
+}
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/Token.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/Token.java
new file mode 100644
index 0000000..8402b3d
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/Token.java
@@ -0,0 +1,124 @@
+/* Generated By:JavaCC: Do not edit this line. Token.java Version 4.1 */
+/* JavaCCOptions:TOKEN_EXTENDS=,KEEP_LINE_COL=null */
+package org.apache.lucene.queryParser.precedence;
+
+/**
+ * Describes the input token stream.
+ */
+
+public class Token {
+
+  /**
+   * An integer that describes the kind of this token.  This numbering
+   * system is determined by JavaCCParser, and a table of these numbers is
+   * stored in the file ...Constants.java.
+   */
+  public int kind;
+
+  /** The line number of the first character of this Token. */
+  public int beginLine;
+  /** The column number of the first character of this Token. */
+  public int beginColumn;
+  /** The line number of the last character of this Token. */
+  public int endLine;
+  /** The column number of the last character of this Token. */
+  public int endColumn;
+
+  /**
+   * The string image of the token.
+   */
+  public String image;
+
+  /**
+   * A reference to the next regular (non-special) token from the input
+   * stream.  If this is the last token from the input stream, or if the
+   * token manager has not read tokens beyond this one, this field is
+   * set to null.  This is true only if this token is also a regular
+   * token.  Otherwise, see below for a description of the contents of
+   * this field.
+   */
+  public Token next;
+
+  /**
+   * This field is used to access special tokens that occur prior to this
+   * token, but after the immediately preceding regular (non-special) token.
+   * If there are no such special tokens, this field is set to null.
+   * When there are more than one such special token, this field refers
+   * to the last of these special tokens, which in turn refers to the next
+   * previous special token through its specialToken field, and so on
+   * until the first special token (whose specialToken field is null).
+   * The next fields of special tokens refer to other special tokens that
+   * immediately follow it (without an intervening regular token).  If there
+   * is no such token, this field is null.
+   */
+  public Token specialToken;
+
+  /**
+   * An optional attribute value of the Token.
+   * Tokens which are not used as syntactic sugar will often contain
+   * meaningful values that will be used later on by the compiler or
+   * interpreter. This attribute value is often different from the image.
+   * Any subclass of Token that actually wants to return a non-null value can
+   * override this method as appropriate.
+   */
+  public Object getValue() {
+    return null;
+  }
+
+  /**
+   * No-argument constructor
+   */
+  public Token() {}
+
+  /**
+   * Constructs a new token for the specified Image.
+   */
+  public Token(int kind)
+  {
+     this(kind, null);
+  }
+
+  /**
+   * Constructs a new token for the specified Image and Kind.
+   */
+  public Token(int kind, String image)
+  {
+     this.kind = kind;
+     this.image = image;
+  }
+
+  /**
+   * Returns the image.
+   */
+  public String toString()
+  {
+     return image;
+  }
+
+  /**
+   * Returns a new Token object, by default. However, if you want, you
+   * can create and return subclass objects based on the value of ofKind.
+   * Simply add the cases to the switch for all those special cases.
+   * For example, if you have a subclass of Token called IDToken that
+   * you want to create if ofKind is ID, simply add something like :
+   *
+   *    case MyParserConstants.ID : return new IDToken(ofKind, image);
+   *
+   * to the following switch statement. Then you can cast matchedToken
+   * variable to the appropriate type and use sit in your lexical actions.
+   */
+  public static Token newToken(int ofKind, String image)
+  {
+     switch(ofKind)
+     {
+       default : return new Token(ofKind, image);
+     }
+  }
+
+  public static Token newToken(int ofKind)
+  {
+     return newToken(ofKind, null);
+  }
+
+}
+/* JavaCC - OriginalChecksum=0dc5808f2ab8aac8775ea9175fa2cb51 (do not edit this line) */
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java
new file mode 100644
index 0000000..01e8751
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/TokenMgrError.java
@@ -0,0 +1,141 @@
+/* Generated By:JavaCC: Do not edit this line. TokenMgrError.java Version 4.1 */
+/* JavaCCOptions: */
+package org.apache.lucene.queryParser.precedence;
+
+/** Token Manager Error. */
+@SuppressWarnings("serial")
+public class TokenMgrError extends Error
+{
+
+   /*
+    * Ordinals for various reasons why an Error of this type can be thrown.
+    */
+
+   /**
+    * Lexical error occurred.
+    */
+   static final int LEXICAL_ERROR = 0;
+
+   /**
+    * An attempt was made to create a second instance of a static token manager.
+    */
+   static final int STATIC_LEXER_ERROR = 1;
+
+   /**
+    * Tried to change to an invalid lexical state.
+    */
+   static final int INVALID_LEXICAL_STATE = 2;
+
+   /**
+    * Detected (and bailed out of) an infinite loop in the token manager.
+    */
+   static final int LOOP_DETECTED = 3;
+
+   /**
+    * Indicates the reason why the exception is thrown. It will have
+    * one of the above 4 values.
+    */
+   int errorCode;
+
+   /**
+    * Replaces unprintable characters by their escaped (or unicode escaped)
+    * equivalents in the given string
+    */
+   protected static final String addEscapes(String str) {
+      StringBuffer retval = new StringBuffer();
+      char ch;
+      for (int i = 0; i < str.length(); i++) {
+        switch (str.charAt(i))
+        {
+           case 0 :
+              continue;
+           case '\b':
+              retval.append("\\b");
+              continue;
+           case '\t':
+              retval.append("\\t");
+              continue;
+           case '\n':
+              retval.append("\\n");
+              continue;
+           case '\f':
+              retval.append("\\f");
+              continue;
+           case '\r':
+              retval.append("\\r");
+              continue;
+           case '\"':
+              retval.append("\\\"");
+              continue;
+           case '\'':
+              retval.append("\\\'");
+              continue;
+           case '\\':
+              retval.append("\\\\");
+              continue;
+           default:
+              if ((ch = str.charAt(i)) < 0x20 || ch > 0x7e) {
+                 String s = "0000" + Integer.toString(ch, 16);
+                 retval.append("\\u" + s.substring(s.length() - 4, s.length()));
+              } else {
+                 retval.append(ch);
+              }
+              continue;
+        }
+      }
+      return retval.toString();
+   }
+
+   /**
+    * Returns a detailed message for the Error when it is thrown by the
+    * token manager to indicate a lexical error.
+    * Parameters :
+    *    EOFSeen     : indicates if EOF caused the lexical error
+    *    curLexState : lexical state in which this error occurred
+    *    errorLine   : line number when the error occurred
+    *    errorColumn : column number when the error occurred
+    *    errorAfter  : prefix that was seen before this error occurred
+    *    curchar     : the offending character
+    * Note: You can customize the lexical error message by modifying this method.
+    */
+   protected static String LexicalError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar) {
+      return("Lexical error at line " +
+           errorLine + ", column " +
+           errorColumn + ".  Encountered: " +
+           (EOFSeen ? "<EOF> " : ("\"" + addEscapes(String.valueOf(curChar)) + "\"") + " (" + (int)curChar + "), ") +
+           "after : \"" + addEscapes(errorAfter) + "\"");
+   }
+
+   /**
+    * You can also modify the body of this method to customize your error messages.
+    * For example, cases like LOOP_DETECTED and INVALID_LEXICAL_STATE are not
+    * of end-users concern, so you can return something like :
+    *
+    *     "Internal Error : Please file a bug report .... "
+    *
+    * from this method for such cases in the release version of your parser.
+    */
+   public String getMessage() {
+      return super.getMessage();
+   }
+
+   /*
+    * Constructors of various flavors follow.
+    */
+
+   /** No arg constructor. */
+   public TokenMgrError() {
+   }
+
+   /** Constructor with message and reason. */
+   public TokenMgrError(String message, int reason) {
+      super(message);
+      errorCode = reason;
+   }
+
+   /** Full Constructor. */
+   public TokenMgrError(boolean EOFSeen, int lexState, int errorLine, int errorColumn, String errorAfter, char curChar, int reason) {
+      this(LexicalError(EOFSeen, lexState, errorLine, errorColumn, errorAfter, curChar), reason);
+   }
+}
+/* JavaCC - OriginalChecksum=257b82f2650841e86289a309cb3dae76 (do not edit this line) */
diff --git a/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/package.html b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/package.html
new file mode 100644
index 0000000..fdc3a30
--- /dev/null
+++ b/lucene/contrib/queryparser/src/java/org/apache/lucene/queryParser/precedence/package.html
@@ -0,0 +1,22 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+QueryParser designed to handle operator precedence in a more sensible fashion than the default QueryParser.
+</body>
+</html>
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java
new file mode 100644
index 0000000..8973e15
--- /dev/null
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/analyzing/TestAnalyzingQueryParser.java
@@ -0,0 +1,120 @@
+package org.apache.lucene.queryParser.analyzing;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.ASCIIFoldingFilter;
+import org.apache.lucene.analysis.LowerCaseFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.standard.StandardFilter;
+import org.apache.lucene.analysis.standard.StandardTokenizer;
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * @version $Revision$, $Date$
+ */
+public class TestAnalyzingQueryParser extends LuceneTestCase {
+
+  private Analyzer a;
+
+  private String[] wildcardInput;
+  private String[] wildcardExpected;
+  private String[] prefixInput;
+  private String[] prefixExpected;
+  private String[] rangeInput;
+  private String[] rangeExpected;
+  private String[] fuzzyInput;
+  private String[] fuzzyExpected;
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    wildcardInput = new String[] { "übersetzung über*ung",
+        "Mötley Cr\u00fce Mötl?* Crü?", "Renée Zellweger Ren?? Zellw?ger" };
+    wildcardExpected = new String[] { "ubersetzung uber*ung", "motley crue motl?* cru?",
+        "renee zellweger ren?? zellw?ger" };
+
+    prefixInput = new String[] { "übersetzung übersetz*",
+        "Mötley Crüe Mötl* crü*", "René? Zellw*" };
+    prefixExpected = new String[] { "ubersetzung ubersetz*", "motley crue motl* cru*",
+        "rene? zellw*" };
+
+    rangeInput = new String[] { "[aa TO bb]", "{Anaïs TO Zoé}" };
+    rangeExpected = new String[] { "[aa TO bb]", "{anais TO zoe}" };
+
+    fuzzyInput = new String[] { "?bersetzung ?bersetzung~0.9",
+        "Mötley Crüe Mötley~0.75 Crüe~0.5",
+        "Renée Zellweger Renée~0.9 Zellweger~" };
+    fuzzyExpected = new String[] { "ubersetzung ubersetzung~0.9",
+        "motley crue motley~0.75 crue~0.5", "renee zellweger renee~0.9 zellweger~0.5" };
+
+    a = new ASCIIAnalyzer();
+  }
+
+  public void testWildCardQuery() throws ParseException {
+    for (int i = 0; i < wildcardInput.length; i++) {
+      assertEquals("Testing wildcards with analyzer " + a.getClass() + ", input string: "
+          + wildcardInput[i], wildcardExpected[i], parseWithAnalyzingQueryParser(wildcardInput[i], a));
+    }
+  }
+
+  public void testPrefixQuery() throws ParseException {
+    for (int i = 0; i < prefixInput.length; i++) {
+      assertEquals("Testing prefixes with analyzer " + a.getClass() + ", input string: "
+          + prefixInput[i], prefixExpected[i], parseWithAnalyzingQueryParser(prefixInput[i], a));
+    }
+  }
+
+  public void testRangeQuery() throws ParseException {
+    for (int i = 0; i < rangeInput.length; i++) {
+      assertEquals("Testing ranges with analyzer " + a.getClass() + ", input string: "
+          + rangeInput[i], rangeExpected[i], parseWithAnalyzingQueryParser(rangeInput[i], a));
+    }
+  }
+
+  public void testFuzzyQuery() throws ParseException {
+    for (int i = 0; i < fuzzyInput.length; i++) {
+      assertEquals("Testing fuzzys with analyzer " + a.getClass() + ", input string: "
+          + fuzzyInput[i], fuzzyExpected[i], parseWithAnalyzingQueryParser(fuzzyInput[i], a));
+    }
+  }
+
+  private String parseWithAnalyzingQueryParser(String s, Analyzer a) throws ParseException {
+    AnalyzingQueryParser qp = new AnalyzingQueryParser(TEST_VERSION_CURRENT, "field", a);
+    org.apache.lucene.search.Query q = qp.parse(s);
+    return q.toString("field");
+  }
+
+}
+
+class ASCIIAnalyzer extends org.apache.lucene.analysis.Analyzer {
+  public ASCIIAnalyzer() {
+  }
+
+  @Override
+  public TokenStream tokenStream(String fieldName, Reader reader) {
+    TokenStream result = new StandardTokenizer(LuceneTestCase.TEST_VERSION_CURRENT, reader);
+    result = new StandardFilter(result);
+    result = new ASCIIFoldingFilter(result);
+    result = new LowerCaseFilter(LuceneTestCase.TEST_VERSION_CURRENT, result);
+    return result;
+  }
+}
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
new file mode 100644
index 0000000..b2dc6ac
--- /dev/null
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/complexPhrase/TestComplexPhraseQuery.java
@@ -0,0 +1,148 @@
+package org.apache.lucene.queryParser.complexPhrase;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.HashSet;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.LuceneTestCase;
+
+public class TestComplexPhraseQuery extends LuceneTestCase {
+
+  Analyzer analyzer = new StandardAnalyzer(TEST_VERSION_CURRENT);
+
+  DocData docsContent[] = { new DocData("john smith", "1"),
+      new DocData("johathon smith", "2"),
+      new DocData("john percival smith", "3"),
+      new DocData("jackson waits tom", "4") };
+
+  private IndexSearcher searcher;
+
+  String defaultFieldName = "name";
+
+  public void testComplexPhrases() throws Exception {
+    checkMatches("\"john smith\"", "1"); // Simple multi-term still works
+    checkMatches("\"j*   smyth~\"", "1,2"); // wildcards and fuzzies are OK in
+    // phrases
+    checkMatches("\"(jo* -john)  smith\"", "2"); // boolean logic works
+    checkMatches("\"jo*  smith\"~2", "1,2,3"); // position logic works.
+    checkMatches("\"jo* [sma TO smZ]\" ", "1,2"); // range queries supported
+    checkMatches("\"john\"", "1,3"); // Simple single-term still works
+    checkMatches("\"(john OR johathon)  smith\"", "1,2"); // boolean logic with
+    // brackets works.
+    checkMatches("\"(jo* -john) smyth~\"", "2"); // boolean logic with
+    // brackets works.
+
+    // checkMatches("\"john -percival\"", "1"); // not logic doesn't work
+    // currently :(.
+
+    checkMatches("\"john  nosuchword*\"", ""); // phrases with clauses producing
+    // empty sets
+
+    checkBadQuery("\"jo*  id:1 smith\""); // mixing fields in a phrase is bad
+    checkBadQuery("\"jo* \"smith\" \""); // phrases inside phrases is bad
+  }
+
+  private void checkBadQuery(String qString) {
+    QueryParser qp = new ComplexPhraseQueryParser(TEST_VERSION_CURRENT, defaultFieldName, analyzer);
+    Throwable expected = null;
+    try {
+      qp.parse(qString);
+    } catch (Throwable e) {
+      expected = e;
+    }
+    assertNotNull("Expected parse error in " + qString, expected);
+
+  }
+
+  private void checkMatches(String qString, String expectedVals)
+      throws Exception {
+    QueryParser qp = new ComplexPhraseQueryParser(TEST_VERSION_CURRENT, defaultFieldName, analyzer);
+    qp.setFuzzyPrefixLength(1); // usually a good idea
+
+    Query q = qp.parse(qString);
+
+    HashSet<String> expecteds = new HashSet<String>();
+    String[] vals = expectedVals.split(",");
+    for (int i = 0; i < vals.length; i++) {
+      if (vals[i].length() > 0)
+        expecteds.add(vals[i]);
+    }
+
+    TopDocs td = searcher.search(q, 10);
+    ScoreDoc[] sd = td.scoreDocs;
+    for (int i = 0; i < sd.length; i++) {
+      Document doc = searcher.doc(sd[i].doc);
+      String id = doc.get("id");
+      assertTrue(qString + "matched doc#" + id + " not expected", expecteds
+          .contains(id));
+      expecteds.remove(id);
+    }
+
+    assertEquals(qString + " missing some matches ", 0, expecteds.size());
+
+  }
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    RAMDirectory rd = new RAMDirectory();
+    IndexWriter w = new IndexWriter(rd, new IndexWriterConfig(TEST_VERSION_CURRENT, analyzer));
+    for (int i = 0; i < docsContent.length; i++) {
+      Document doc = new Document();
+      doc.add(new Field("name", docsContent[i].name, Field.Store.YES,
+          Field.Index.ANALYZED));
+      doc.add(new Field("id", docsContent[i].id, Field.Store.YES,
+          Field.Index.ANALYZED));
+      w.addDocument(doc);
+    }
+    w.close();
+    searcher = new IndexSearcher(rd, true);
+  }
+
+  @Override
+  protected void tearDown() throws Exception {
+    searcher.close();
+    super.tearDown();
+  }
+
+  static class DocData {
+    String name;
+
+    String id;
+
+    public DocData(String name, String id) {
+      super();
+      this.name = name;
+      this.id = id;
+    }
+  }
+
+}
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/ExtensionStub.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/ExtensionStub.java
new file mode 100644
index 0000000..63ce2b3
--- /dev/null
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/ExtensionStub.java
@@ -0,0 +1,33 @@
+package org.apache.lucene.queryParser.ext;
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+class ExtensionStub extends ParserExtension {
+
+  @Override
+  public Query parse(ExtensionQuery components) throws ParseException {
+    return new TermQuery(new Term(components.getField(), components
+        .getRawQueryString()));
+  }
+
+}
\ No newline at end of file
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java
new file mode 100644
index 0000000..e465e64
--- /dev/null
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/TestExtendableQueryParser.java
@@ -0,0 +1,136 @@
+package org.apache.lucene.queryParser.ext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.queryParser.ParseException;
+import org.apache.lucene.queryParser.QueryParser;
+import org.apache.lucene.queryParser.TestQueryParser;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+
+/**
+ * Testcase for the class {@link ExtendableQueryParser}
+ */
+public class TestExtendableQueryParser extends TestQueryParser {
+  private static char[] DELIMITERS = new char[] {
+      Extensions.DEFAULT_EXTENSION_FIELD_DELIMITER, '-', '|' };
+
+  public TestExtendableQueryParser(String name) {
+    super(name);
+  }
+
+  @Override
+  public QueryParser getParser(Analyzer a) throws Exception {
+    return getParser(a, null);
+  }
+
+  public QueryParser getParser(Analyzer a, Extensions extensions)
+      throws Exception {
+    if (a == null)
+      a = new SimpleAnalyzer(TEST_VERSION_CURRENT);
+    QueryParser qp = extensions == null ? new ExtendableQueryParser(
+        TEST_VERSION_CURRENT, "field", a) : new ExtendableQueryParser(
+        TEST_VERSION_CURRENT, "field", a, extensions);
+    qp.setDefaultOperator(QueryParser.OR_OPERATOR);
+    return qp;
+  }
+
+  public void testUnescapedExtDelimiter() throws Exception {
+    Extensions ext = newExtensions(':');
+    ext.add("testExt", new ExtensionStub());
+    ExtendableQueryParser parser = (ExtendableQueryParser) getParser(null, ext);
+    try {
+      parser.parse("aField:testExt:\"foo \\& bar\"");
+      fail("extension field delimiter is not escaped");
+    } catch (ParseException e) {
+    }
+  }
+
+  public void testExtFieldUnqoted() throws Exception {
+    for (int i = 0; i < DELIMITERS.length; i++) {
+      Extensions ext = newExtensions(DELIMITERS[i]);
+      ext.add("testExt", new ExtensionStub());
+      ExtendableQueryParser parser = (ExtendableQueryParser) getParser(null,
+          ext);
+      String field = ext.buildExtensionField("testExt", "aField");
+      Query query = parser.parse(String.format("%s:foo bar", field));
+      assertTrue("expected instance of BooleanQuery but was "
+          + query.getClass(), query instanceof BooleanQuery);
+      BooleanQuery bquery = (BooleanQuery) query;
+      BooleanClause[] clauses = bquery.getClauses();
+      assertEquals(2, clauses.length);
+      BooleanClause booleanClause = clauses[0];
+      query = booleanClause.getQuery();
+      assertTrue("expected instance of TermQuery but was " + query.getClass(),
+          query instanceof TermQuery);
+      TermQuery tquery = (TermQuery) query;
+      assertEquals("aField", tquery.getTerm()
+          .field());
+      assertEquals("foo", tquery.getTerm().text());
+
+      booleanClause = clauses[1];
+      query = booleanClause.getQuery();
+      assertTrue("expected instance of TermQuery but was " + query.getClass(),
+          query instanceof TermQuery);
+      tquery = (TermQuery) query;
+      assertEquals("field", tquery.getTerm().field());
+      assertEquals("bar", tquery.getTerm().text());
+    }
+  }
+
+  public void testExtDefaultField() throws Exception {
+    for (int i = 0; i < DELIMITERS.length; i++) {
+      Extensions ext = newExtensions(DELIMITERS[i]);
+      ext.add("testExt", new ExtensionStub());
+      ExtendableQueryParser parser = (ExtendableQueryParser) getParser(null,
+          ext);
+      String field = ext.buildExtensionField("testExt");
+      Query parse = parser.parse(String.format("%s:\"foo \\& bar\"", field));
+      assertTrue("expected instance of TermQuery but was " + parse.getClass(),
+          parse instanceof TermQuery);
+      TermQuery tquery = (TermQuery) parse;
+      assertEquals("field", tquery.getTerm().field());
+      assertEquals("foo & bar", tquery.getTerm().text());
+    }
+  }
+
+  public Extensions newExtensions(char delimiter) {
+    return new Extensions(delimiter);
+  }
+
+  public void testExtField() throws Exception {
+    for (int i = 0; i < DELIMITERS.length; i++) {
+      Extensions ext = newExtensions(DELIMITERS[i]);
+      ext.add("testExt", new ExtensionStub());
+      ExtendableQueryParser parser = (ExtendableQueryParser) getParser(null,
+          ext);
+      String field = ext.buildExtensionField("testExt", "afield");
+      Query parse = parser.parse(String.format("%s:\"foo \\& bar\"", field));
+      assertTrue("expected instance of TermQuery but was " + parse.getClass(),
+          parse instanceof TermQuery);
+      TermQuery tquery = (TermQuery) parse;
+      assertEquals("afield", tquery.getTerm().field());
+      assertEquals("foo & bar", tquery.getTerm().text());
+    }
+  }
+
+}
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/TestExtensions.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/TestExtensions.java
new file mode 100644
index 0000000..8c061b6
--- /dev/null
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/ext/TestExtensions.java
@@ -0,0 +1,79 @@
+package org.apache.lucene.queryParser.ext;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.LuceneTestCase;
+
+/**
+ * Testcase for the {@link Extensions} class
+ */
+public class TestExtensions extends LuceneTestCase {
+
+  private Extensions ext;
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    this.ext = new Extensions();
+  }
+
+  public void testBuildExtensionField() {
+    assertEquals("field\\:key", ext.buildExtensionField("key", "field"));
+    assertEquals("\\:key", ext.buildExtensionField("key"));
+
+    ext = new Extensions('.');
+    assertEquals("field.key", ext.buildExtensionField("key", "field"));
+    assertEquals(".key", ext.buildExtensionField("key"));
+  }
+
+  public void testSplitExtensionField() {
+    assertEquals("field\\:key", ext.buildExtensionField("key", "field"));
+    assertEquals("\\:key", ext.buildExtensionField("key"));
+
+    ext = new Extensions('.');
+    assertEquals("field.key", ext.buildExtensionField("key", "field"));
+    assertEquals(".key", ext.buildExtensionField("key"));
+  }
+
+  public void testAddGetExtension() {
+    ParserExtension extension = new ExtensionStub();
+    assertNull(ext.getExtension("foo"));
+    ext.add("foo", extension);
+    assertSame(extension, ext.getExtension("foo"));
+    ext.add("foo", null);
+    assertNull(ext.getExtension("foo"));
+  }
+
+  public void testGetExtDelimiter() {
+    assertEquals(Extensions.DEFAULT_EXTENSION_FIELD_DELIMITER, this.ext
+        .getExtensionFieldDelimiter());
+    ext = new Extensions('?');
+    assertEquals('?', this.ext.getExtensionFieldDelimiter());
+  }
+
+  public void testEscapeExtension() {
+    assertEquals("abc\\:\\?\\{\\}\\[\\]\\\\\\(\\)\\+\\-\\!\\~", ext
+        .escapeExtensionField("abc:?{}[]\\()+-!~"));
+    try {
+      ext.escapeExtensionField(null);
+      fail("should throw NPE - escape string is null");
+    } catch (NullPointerException e) {
+      // 
+    }
+  }
+}
diff --git a/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
new file mode 100644
index 0000000..961fe51
--- /dev/null
+++ b/lucene/contrib/queryparser/src/test/org/apache/lucene/queryParser/precedence/TestPrecedenceQueryParser.java
@@ -0,0 +1,613 @@
+package org.apache.lucene.queryParser.precedence;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.LowerCaseTokenizer;
+import org.apache.lucene.analysis.SimpleAnalyzer;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.WhitespaceAnalyzer;
+import org.apache.lucene.analysis.standard.StandardAnalyzer;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.document.DateTools;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.FuzzyQuery;
+import org.apache.lucene.search.PhraseQuery;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.WildcardQuery;
+import org.apache.lucene.util.LocalizedTestCase;
+
+import java.io.IOException;
+import java.io.Reader;
+import java.text.DateFormat;
+import java.util.Arrays;
+import java.util.Calendar;
+import java.util.GregorianCalendar;
+import java.util.HashSet;
+import java.util.Collections;
+
+public class TestPrecedenceQueryParser extends LocalizedTestCase {
+  
+  public TestPrecedenceQueryParser(String name) {
+    super(name, new HashSet<String>(Arrays.asList(new String[]{
+      "testDateRange", "testNumber"
+    })));
+  }
+
+  public static Analyzer qpAnalyzer = new QPTestAnalyzer();
+
+  public static class QPTestFilter extends TokenFilter {
+    /**
+     * Filter which discards the token 'stop' and which expands the
+     * token 'phrase' into 'phrase1 phrase2'
+     */
+    public QPTestFilter(TokenStream in) {
+      super(in);
+    }
+
+    boolean inPhrase = false;
+    int savedStart = 0, savedEnd = 0;
+
+    TermAttribute termAtt = addAttribute(TermAttribute.class);
+    OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
+    
+    @Override
+    public boolean incrementToken() throws IOException {
+      clearAttributes();
+      if (inPhrase) {
+        inPhrase = false;
+        termAtt.setTermBuffer("phrase2");
+        offsetAtt.setOffset(savedStart, savedEnd);
+        return true;
+      } else
+        while(input.incrementToken())
+          if (termAtt.term().equals("phrase")) {
+            inPhrase = true;
+            savedStart = offsetAtt.startOffset();
+            savedEnd = offsetAtt.endOffset();
+            termAtt.setTermBuffer("phrase1");
+            offsetAtt.setOffset(savedStart, savedEnd);
+            return true;
+          } else if (!termAtt.term().equals("stop"))
+            return true;
+      return false;
+    }
+  }
+
+  public static class QPTestAnalyzer extends Analyzer {
+
+    /** Filters LowerCaseTokenizer with StopFilter. */
+    @Override
+    public final TokenStream tokenStream(String fieldName, Reader reader) {
+      return new QPTestFilter(new LowerCaseTokenizer(TEST_VERSION_CURRENT, reader));
+    }
+  }
+
+  public static class QPTestParser extends PrecedenceQueryParser {
+    public QPTestParser(String f, Analyzer a) {
+      super(f, a);
+    }
+
+    @Override
+    protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException {
+      throw new ParseException("Fuzzy queries not allowed");
+    }
+
+    @Override
+    protected Query getWildcardQuery(String field, String termStr) throws ParseException {
+      throw new ParseException("Wildcard queries not allowed");
+    }
+  }
+
+  private int originalMaxClauses;
+
+  @Override
+  protected void setUp() throws Exception {
+    super.setUp();
+    originalMaxClauses = BooleanQuery.getMaxClauseCount();
+  }
+
+  public PrecedenceQueryParser getParser(Analyzer a) throws Exception {
+    if (a == null)
+      a = new SimpleAnalyzer(TEST_VERSION_CURRENT);
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", a);
+    qp.setDefaultOperator(PrecedenceQueryParser.OR_OPERATOR);
+    return qp;
+  }
+
+  public Query getQuery(String query, Analyzer a) throws Exception {
+    return getParser(a).parse(query);
+  }
+
+  public void assertQueryEquals(String query, Analyzer a, String result)
+    throws Exception {
+    Query q = getQuery(query, a);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("Query /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  public void assertWildcardQueryEquals(String query, boolean lowercase, String result)
+    throws Exception {
+    PrecedenceQueryParser qp = getParser(null);
+    qp.setLowercaseExpandedTerms(lowercase);
+    Query q = qp.parse(query);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("WildcardQuery /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  public void assertWildcardQueryEquals(String query, String result) throws Exception {
+    PrecedenceQueryParser qp = getParser(null);
+    Query q = qp.parse(query);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("WildcardQuery /" + query + "/ yielded /" + s + "/, expecting /"
+          + result + "/");
+    }
+  }
+
+  public Query getQueryDOA(String query, Analyzer a)
+    throws Exception {
+    if (a == null)
+      a = new SimpleAnalyzer(TEST_VERSION_CURRENT);
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", a);
+    qp.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR);
+    return qp.parse(query);
+  }
+
+  public void assertQueryEqualsDOA(String query, Analyzer a, String result)
+    throws Exception {
+    Query q = getQueryDOA(query, a);
+    String s = q.toString("field");
+    if (!s.equals(result)) {
+      fail("Query /" + query + "/ yielded /" + s
+           + "/, expecting /" + result + "/");
+    }
+  }
+
+  // failing tests disabled since PrecedenceQueryParser
+  // is currently unmaintained
+  public void _testSimple() throws Exception {
+    assertQueryEquals("", null, "");
+
+    assertQueryEquals("term term term", null, "term term term");
+    assertQueryEquals("türm term term", null, "türm term term");
+    assertQueryEquals("ümlaut", null, "ümlaut");
+
+    assertQueryEquals("+a", null, "+a");
+    assertQueryEquals("-a", null, "-a");
+    assertQueryEquals("a AND b", null, "+a +b");
+    assertQueryEquals("(a AND b)", null, "+a +b");
+    assertQueryEquals("c OR (a AND b)", null, "c (+a +b)");
+    assertQueryEquals("a AND NOT b", null, "+a -b");
+    assertQueryEquals("a AND -b", null, "+a -b");
+    assertQueryEquals("a AND !b", null, "+a -b");
+    assertQueryEquals("a && b", null, "+a +b");
+    assertQueryEquals("a && ! b", null, "+a -b");
+
+    assertQueryEquals("a OR b", null, "a b");
+    assertQueryEquals("a || b", null, "a b");
+
+    assertQueryEquals("+term -term term", null, "+term -term term");
+    assertQueryEquals("foo:term AND field:anotherTerm", null,
+                      "+foo:term +anotherterm");
+    assertQueryEquals("term AND \"phrase phrase\"", null,
+                      "+term +\"phrase phrase\"");
+    assertQueryEquals("\"hello there\"", null, "\"hello there\"");
+    assertTrue(getQuery("a AND b", null) instanceof BooleanQuery);
+    assertTrue(getQuery("hello", null) instanceof TermQuery);
+    assertTrue(getQuery("\"hello there\"", null) instanceof PhraseQuery);
+
+    assertQueryEquals("germ term^2.0", null, "germ term^2.0");
+    assertQueryEquals("(term)^2.0", null, "term^2.0");
+    assertQueryEquals("(germ term)^2.0", null, "(germ term)^2.0");
+    assertQueryEquals("term^2.0", null, "term^2.0");
+    assertQueryEquals("term^2", null, "term^2.0");
+    assertQueryEquals("\"germ term\"^2.0", null, "\"germ term\"^2.0");
+    assertQueryEquals("\"term germ\"^2", null, "\"term germ\"^2.0");
+
+    assertQueryEquals("(foo OR bar) AND (baz OR boo)", null,
+                      "+(foo bar) +(baz boo)");
+    assertQueryEquals("((a OR b) AND NOT c) OR d", null,
+                      "(+(a b) -c) d");
+    assertQueryEquals("+(apple \"steve jobs\") -(foo bar baz)", null,
+                      "+(apple \"steve jobs\") -(foo bar baz)");
+    assertQueryEquals("+title:(dog OR cat) -author:\"bob dole\"", null,
+                      "+(title:dog title:cat) -author:\"bob dole\"");
+    
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", new StandardAnalyzer(TEST_VERSION_CURRENT));
+    // make sure OR is the default:
+    assertEquals(PrecedenceQueryParser.OR_OPERATOR, qp.getDefaultOperator());
+    qp.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR);
+    assertEquals(PrecedenceQueryParser.AND_OPERATOR, qp.getDefaultOperator());
+    qp.setDefaultOperator(PrecedenceQueryParser.OR_OPERATOR);
+    assertEquals(PrecedenceQueryParser.OR_OPERATOR, qp.getDefaultOperator());
+
+    assertQueryEquals("a OR !b", null, "a (-b)");
+    assertQueryEquals("a OR ! b", null, "a (-b)");
+    assertQueryEquals("a OR -b", null, "a (-b)");
+  }
+
+  public void testPunct() throws Exception {
+    Analyzer a = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
+    assertQueryEquals("a&b", a, "a&b");
+    assertQueryEquals("a&&b", a, "a&&b");
+    assertQueryEquals(".NET", a, ".NET");
+  }
+
+  public void testSlop() throws Exception {
+    assertQueryEquals("\"term germ\"~2", null, "\"term germ\"~2");
+    assertQueryEquals("\"term germ\"~2 flork", null, "\"term germ\"~2 flork");
+    assertQueryEquals("\"term\"~2", null, "term");
+    assertQueryEquals("\" \"~2 germ", null, "germ");
+    assertQueryEquals("\"term germ\"~2^2", null, "\"term germ\"~2^2.0");
+  }
+
+  public void testNumber() throws Exception {
+// The numbers go away because SimpleAnalzyer ignores them
+    assertQueryEquals("3", null, "");
+    assertQueryEquals("term 1.0 1 2", null, "term");
+    assertQueryEquals("term term1 term2", null, "term term term");
+
+    Analyzer a = new StandardAnalyzer(TEST_VERSION_CURRENT);
+    assertQueryEquals("3", a, "3");
+    assertQueryEquals("term 1.0 1 2", a, "term 1.0 1 2");
+    assertQueryEquals("term term1 term2", a, "term term1 term2");
+  }
+
+  // failing tests disabled since PrecedenceQueryParser
+  // is currently unmaintained
+  public void _testWildcard() throws Exception {
+    assertQueryEquals("term*", null, "term*");
+    assertQueryEquals("term*^2", null, "term*^2.0");
+    assertQueryEquals("term~", null, "term~0.5");
+    assertQueryEquals("term~0.7", null, "term~0.7");
+    assertQueryEquals("term~^2", null, "term^2.0~0.5");
+    assertQueryEquals("term^2~", null, "term^2.0~0.5");
+    assertQueryEquals("term*germ", null, "term*germ");
+    assertQueryEquals("term*germ^3", null, "term*germ^3.0");
+
+    assertTrue(getQuery("term*", null) instanceof PrefixQuery);
+    assertTrue(getQuery("term*^2", null) instanceof PrefixQuery);
+    assertTrue(getQuery("term~", null) instanceof FuzzyQuery);
+    assertTrue(getQuery("term~0.7", null) instanceof FuzzyQuery);
+    FuzzyQuery fq = (FuzzyQuery)getQuery("term~0.7", null);
+    assertEquals(0.7f, fq.getMinSimilarity(), 0.1f);
+    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
+    fq = (FuzzyQuery)getQuery("term~", null);
+    assertEquals(0.5f, fq.getMinSimilarity(), 0.1f);
+    assertEquals(FuzzyQuery.defaultPrefixLength, fq.getPrefixLength());
+    try {
+      getQuery("term~1.1", null);   // value > 1, throws exception
+      fail();
+    } catch(ParseException pe) {
+      // expected exception
+    }
+    assertTrue(getQuery("term*germ", null) instanceof WildcardQuery);
+
+/* Tests to see that wild card terms are (or are not) properly
+	 * lower-cased with propery parser configuration
+	 */
+// First prefix queries:
+    // by default, convert to lowercase:
+    assertWildcardQueryEquals("Term*", true, "term*");
+    // explicitly set lowercase:
+    assertWildcardQueryEquals("term*", true, "term*");
+    assertWildcardQueryEquals("Term*", true, "term*");
+    assertWildcardQueryEquals("TERM*", true, "term*");
+    // explicitly disable lowercase conversion:
+    assertWildcardQueryEquals("term*", false, "term*");
+    assertWildcardQueryEquals("Term*", false, "Term*");
+    assertWildcardQueryEquals("TERM*", false, "TERM*");
+// Then 'full' wildcard queries:
+    // by default, convert to lowercase:
+    assertWildcardQueryEquals("Te?m", "te?m");
+    // explicitly set lowercase:
+    assertWildcardQueryEquals("te?m", true, "te?m");
+    assertWildcardQueryEquals("Te?m", true, "te?m");
+    assertWildcardQueryEquals("TE?M", true, "te?m");
+    assertWildcardQueryEquals("Te?m*gerM", true, "te?m*germ");
+    // explicitly disable lowercase conversion:
+    assertWildcardQueryEquals("te?m", false, "te?m");
+    assertWildcardQueryEquals("Te?m", false, "Te?m");
+    assertWildcardQueryEquals("TE?M", false, "TE?M");
+    assertWildcardQueryEquals("Te?m*gerM", false, "Te?m*gerM");
+//  Fuzzy queries:
+    assertWildcardQueryEquals("Term~", "term~0.5");
+    assertWildcardQueryEquals("Term~", true, "term~0.5");
+    assertWildcardQueryEquals("Term~", false, "Term~0.5");
+//  Range queries:
+    assertWildcardQueryEquals("[A TO C]", "[a TO c]");
+    assertWildcardQueryEquals("[A TO C]", true, "[a TO c]");
+    assertWildcardQueryEquals("[A TO C]", false, "[A TO C]");
+  }
+
+  public void testQPA() throws Exception {
+    assertQueryEquals("term term term", qpAnalyzer, "term term term");
+    assertQueryEquals("term +stop term", qpAnalyzer, "term term");
+    assertQueryEquals("term -stop term", qpAnalyzer, "term term");
+    assertQueryEquals("drop AND stop AND roll", qpAnalyzer, "+drop +roll");
+    assertQueryEquals("term phrase term", qpAnalyzer,
+                      "term \"phrase1 phrase2\" term");
+    // note the parens in this next assertion differ from the original
+    // QueryParser behavior
+    assertQueryEquals("term AND NOT phrase term", qpAnalyzer,
+                      "(+term -\"phrase1 phrase2\") term");
+    assertQueryEquals("stop", qpAnalyzer, "");
+    assertQueryEquals("stop OR stop AND stop", qpAnalyzer, "");
+    assertTrue(getQuery("term term term", qpAnalyzer) instanceof BooleanQuery);
+    assertTrue(getQuery("term +stop", qpAnalyzer) instanceof TermQuery);
+  }
+
+  public void testRange() throws Exception {
+    assertQueryEquals("[ a TO z]", null, "[a TO z]");
+    assertTrue(getQuery("[ a TO z]", null) instanceof TermRangeQuery);
+    assertQueryEquals("[ a TO z ]", null, "[a TO z]");
+    assertQueryEquals("{ a TO z}", null, "{a TO z}");
+    assertQueryEquals("{ a TO z }", null, "{a TO z}");
+    assertQueryEquals("{ a TO z }^2.0", null, "{a TO z}^2.0");
+    assertQueryEquals("[ a TO z] OR bar", null, "[a TO z] bar");
+    assertQueryEquals("[ a TO z] AND bar", null, "+[a TO z] +bar");
+    assertQueryEquals("( bar blar { a TO z}) ", null, "bar blar {a TO z}");
+    assertQueryEquals("gack ( bar blar { a TO z}) ", null, "gack (bar blar {a TO z})");
+  }
+  
+  private String escapeDateString(String s) {
+    if (s.contains(" ")) {
+      return "\"" + s + "\"";
+    } else {
+      return s;
+    }
+  }
+
+  public String getDate(String s) throws Exception {
+    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
+    return DateTools.dateToString(df.parse(s), DateTools.Resolution.DAY);
+  }
+
+  public String getLocalizedDate(int year, int month, int day) {
+    DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT);
+    Calendar calendar = new GregorianCalendar();
+    calendar.clear();
+    calendar.set(year, month, day);
+    calendar.set(Calendar.HOUR_OF_DAY, 23);
+    calendar.set(Calendar.MINUTE, 59);
+    calendar.set(Calendar.SECOND, 59);
+    calendar.set(Calendar.MILLISECOND, 999);
+    return df.format(calendar.getTime());
+  }
+
+  public void testDateRange() throws Exception {
+    String startDate = getLocalizedDate(2002, 1, 1);
+    String endDate = getLocalizedDate(2002, 1, 4);
+    assertQueryEquals("[ " + escapeDateString(startDate) + " TO " + escapeDateString(endDate) + "]", null,
+                      "[" + getDate(startDate) + " TO " + getDate(endDate) + "]");
+    assertQueryEquals("{  " + escapeDateString(startDate) + "    " + escapeDateString(endDate) + "   }", null,
+                      "{" + getDate(startDate) + " TO " + getDate(endDate) + "}");
+  }
+
+  public void testEscaped() throws Exception {
+    Analyzer a = new WhitespaceAnalyzer(TEST_VERSION_CURRENT);
+    
+    /*assertQueryEquals("\\[brackets", a, "\\[brackets");
+    assertQueryEquals("\\[brackets", null, "brackets");
+    assertQueryEquals("\\\\", a, "\\\\");
+    assertQueryEquals("\\+blah", a, "\\+blah");
+    assertQueryEquals("\\(blah", a, "\\(blah");
+
+    assertQueryEquals("\\-blah", a, "\\-blah");
+    assertQueryEquals("\\!blah", a, "\\!blah");
+    assertQueryEquals("\\{blah", a, "\\{blah");
+    assertQueryEquals("\\}blah", a, "\\}blah");
+    assertQueryEquals("\\:blah", a, "\\:blah");
+    assertQueryEquals("\\^blah", a, "\\^blah");
+    assertQueryEquals("\\[blah", a, "\\[blah");
+    assertQueryEquals("\\]blah", a, "\\]blah");
+    assertQueryEquals("\\\"blah", a, "\\\"blah");
+    assertQueryEquals("\\(blah", a, "\\(blah");
+    assertQueryEquals("\\)blah", a, "\\)blah");
+    assertQueryEquals("\\~blah", a, "\\~blah");
+    assertQueryEquals("\\*blah", a, "\\*blah");
+    assertQueryEquals("\\?blah", a, "\\?blah");
+    //assertQueryEquals("foo \\&\\& bar", a, "foo \\&\\& bar");
+    //assertQueryEquals("foo \\|| bar", a, "foo \\|| bar");
+    //assertQueryEquals("foo \\AND bar", a, "foo \\AND bar");*/
+
+    assertQueryEquals("a\\-b:c", a, "a-b:c");
+    assertQueryEquals("a\\+b:c", a, "a+b:c");
+    assertQueryEquals("a\\:b:c", a, "a:b:c");
+    assertQueryEquals("a\\\\b:c", a, "a\\b:c");
+
+    assertQueryEquals("a:b\\-c", a, "a:b-c");
+    assertQueryEquals("a:b\\+c", a, "a:b+c");
+    assertQueryEquals("a:b\\:c", a, "a:b:c");
+    assertQueryEquals("a:b\\\\c", a, "a:b\\c");
+
+    assertQueryEquals("a:b\\-c*", a, "a:b-c*");
+    assertQueryEquals("a:b\\+c*", a, "a:b+c*");
+    assertQueryEquals("a:b\\:c*", a, "a:b:c*");
+
+    assertQueryEquals("a:b\\\\c*", a, "a:b\\c*");
+
+    assertQueryEquals("a:b\\-?c", a, "a:b-?c");
+    assertQueryEquals("a:b\\+?c", a, "a:b+?c");
+    assertQueryEquals("a:b\\:?c", a, "a:b:?c");
+
+    assertQueryEquals("a:b\\\\?c", a, "a:b\\?c");
+
+    assertQueryEquals("a:b\\-c~", a, "a:b-c~0.5");
+    assertQueryEquals("a:b\\+c~", a, "a:b+c~0.5");
+    assertQueryEquals("a:b\\:c~", a, "a:b:c~0.5");
+    assertQueryEquals("a:b\\\\c~", a, "a:b\\c~0.5");
+
+    assertQueryEquals("[ a\\- TO a\\+ ]", null, "[a- TO a+]");
+    assertQueryEquals("[ a\\: TO a\\~ ]", null, "[a: TO a~]");
+    assertQueryEquals("[ a\\\\ TO a\\* ]", null, "[a\\ TO a*]");
+  }
+
+  public void testTabNewlineCarriageReturn()
+    throws Exception {
+    assertQueryEqualsDOA("+weltbank +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \n +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\r+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\r\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r\n+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r\n +worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \r \n +worlbank", null,
+      "+weltbank +worlbank");
+
+    assertQueryEqualsDOA("+weltbank\t+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \t+worlbank", null,
+      "+weltbank +worlbank");
+    assertQueryEqualsDOA("weltbank \t +worlbank", null,
+      "+weltbank +worlbank");
+  }
+
+  public void testSimpleDAO()
+    throws Exception {
+    assertQueryEqualsDOA("term term term", null, "+term +term +term");
+    assertQueryEqualsDOA("term +term term", null, "+term +term +term");
+    assertQueryEqualsDOA("term term +term", null, "+term +term +term");
+    assertQueryEqualsDOA("term +term +term", null, "+term +term +term");
+    assertQueryEqualsDOA("-term term term", null, "-term +term +term");
+  }
+
+  public void testBoost()
+    throws Exception {
+    StandardAnalyzer oneStopAnalyzer = new StandardAnalyzer(TEST_VERSION_CURRENT, Collections.singleton("on"));
+    PrecedenceQueryParser qp = new PrecedenceQueryParser("field", oneStopAnalyzer);
+    Query q = qp.parse("on^1.0");
+    assertNotNull(q);
+    q = qp.parse("\"hello\"^2.0");
+    assertNotNull(q);
+    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
+    q = qp.parse("hello^2.0");
+    assertNotNull(q);
+    assertEquals(q.getBoost(), (float) 2.0, (float) 0.5);
+    q = qp.parse("\"on\"^1.0");
+    assertNotNull(q);
+
+    q = getParser(new StandardAnalyzer(TEST_VERSION_CURRENT)).parse("the^3");
+    assertNotNull(q);
+  }
+
+  public void testException() throws Exception {
+    try {
+      assertQueryEquals("\"some phrase", null, "abc");
+      fail("ParseException expected, not thrown");
+    } catch (ParseException expected) {
+    }
+  }
+
+  public void testCustomQueryParserWildcard() {
+    try {
+      new QPTestParser("contents", new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).parse("a?t");
+    } catch (ParseException expected) {
+      return;
+    }
+    fail("Wildcard queries should not be allowed");
+  }
+
+  public void testCustomQueryParserFuzzy() throws Exception {
+    try {
+      new QPTestParser("contents", new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).parse("xunit~");
+    } catch (ParseException expected) {
+      return;
+    }
+    fail("Fuzzy queries should not be allowed");
+  }
+
+  public void testBooleanQuery() throws Exception {
+    BooleanQuery.setMaxClauseCount(2);
+    try {
+      getParser(new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).parse("one two three");
+      fail("ParseException expected due to too many boolean clauses");
+    } catch (ParseException expected) {
+      // too many boolean clauses, so ParseException is expected
+    }
+  }
+
+  /**
+   * This test differs from the original QueryParser, showing how the
+   * precedence issue has been corrected.
+   */
+  // failing tests disabled since PrecedenceQueryParser
+  // is currently unmaintained
+  public void _testPrecedence() throws Exception {
+    PrecedenceQueryParser parser = getParser(new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
+    Query query1 = parser.parse("A AND B OR C AND D");
+    Query query2 = parser.parse("(A AND B) OR (C AND D)");
+    assertEquals(query1, query2);
+
+    query1 = parser.parse("A OR B C");
+    query2 = parser.parse("A B C");
+    assertEquals(query1, query2);
+
+    query1 = parser.parse("A AND B C");
+    query2 = parser.parse("(+A +B) C");
+    assertEquals(query1, query2);
+
+    query1 = parser.parse("A AND NOT B");
+    query2 = parser.parse("+A -B");
+    assertEquals(query1, query2);
+
+    query1 = parser.parse("A OR NOT B");
+    query2 = parser.parse("A -B");
+    assertEquals(query1, query2);
+
+    query1 = parser.parse("A OR NOT B AND C");
+    query2 = parser.parse("A (-B +C)");
+    assertEquals(query1, query2);
+  }
+
+
+  @Override
+  protected void tearDown() throws Exception {
+    BooleanQuery.setMaxClauseCount(originalMaxClauses);
+    super.tearDown();
+  }
+
+}
diff --git a/lucene/contrib/regex/build.xml b/lucene/contrib/regex/build.xml
deleted file mode 100644
index 4c4831e..0000000
--- a/lucene/contrib/regex/build.xml
+++ /dev/null
@@ -1,36 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-
-<project name="regex" default="default">
-
-  <description>
-    Regular expression query
-  </description>
-
-  <path id="additional.dependencies">
-    <fileset dir="lib" includes="*-oro-*.jar,*-regexp-*.jar"/>
-  </path>
-
-  <pathconvert property="project.classpath"
-               targetos="unix"
-               refid="additional.dependencies"
-  />
-
-  <import file="../contrib-build.xml"/>
-</project>
diff --git a/lucene/contrib/regex/lib/jakarta-regexp-1.4.jar b/lucene/contrib/regex/lib/jakarta-regexp-1.4.jar
deleted file mode 100644
index 0366f23..0000000
--- a/lucene/contrib/regex/lib/jakarta-regexp-1.4.jar
+++ /dev/null
@@ -1,2 +0,0 @@
-AnyObjectId[5d70c357a1e6c4c702af313c94aaf3168d300dcf] was removed in git history.
-Apache SVN contains full history.
\ No newline at end of file
diff --git a/lucene/contrib/regex/lib/regexp.LICENSE b/lucene/contrib/regex/lib/regexp.LICENSE
deleted file mode 100644
index 261eeb9..0000000
--- a/lucene/contrib/regex/lib/regexp.LICENSE
+++ /dev/null
@@ -1,201 +0,0 @@
-                                 Apache License
-                           Version 2.0, January 2004
-                        http://www.apache.org/licenses/
-
-   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-   1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
diff --git a/lucene/contrib/regex/pom.xml.template b/lucene/contrib/regex/pom.xml.template
deleted file mode 100644
index 975520b..0000000
--- a/lucene/contrib/regex/pom.xml.template
+++ /dev/null
@@ -1,43 +0,0 @@
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
-
-  <!--
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-    
-    http://www.apache.org/licenses/LICENSE-2.0
-    
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-  -->
-
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.lucene</groupId>
-    <artifactId>lucene-contrib</artifactId>
-    <version>@version@</version>
-  </parent>
-  <groupId>org.apache.lucene</groupId>
-  <artifactId>lucene-regex</artifactId>
-  <name>Lucene Regex</name>
-  <version>@version@</version>
-  <description>Regular expression query</description>
-  <packaging>jar</packaging>
-  <dependencies>
-    <dependency>
-      <groupId>jakarta-regexp</groupId>
-      <artifactId>jakarta-regexp</artifactId>
-      <version>${jakarta-regexp-version}</version>
-    </dependency>
-  </dependencies>
-</project>
diff --git a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/JakartaRegexpCapabilities.java b/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/JakartaRegexpCapabilities.java
deleted file mode 100644
index 3e7c429..0000000
--- a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/JakartaRegexpCapabilities.java
+++ /dev/null
@@ -1,93 +0,0 @@
-package org.apache.lucene.search.regex;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.regexp.RE;
-import org.apache.regexp.RegexpTunnel;
-
-/**
- * Implementation tying <a href="http://jakarta.apache.org/regexp">Jakarta
- * Regexp</a> to RegexQuery. Jakarta Regepx internally supports a
- * {@link #prefix} implementation which can offer performance gains under
- * certain circumstances. Yet, the implementation appears to be rather shaky as
- * it doesn't always provide a prefix even if one would exist.
- */
-public class JakartaRegexpCapabilities implements RegexCapabilities {
-  private RE regexp;
-  
-  // Define the flags that are possible. Redefine them here
-  // to avoid exposing the RE class to the caller.
-  
-  private int flags = RE.MATCH_NORMAL;
-
-  /**
-   * Flag to specify normal, case-sensitive matching behaviour. This is the default.
-   */
-  public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;
-  
-  /**
-   * Flag to specify that matching should be case-independent (folded)
-   */
-  public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;
- 
-  /**
-   * Constructs a RegexCapabilities with the default MATCH_NORMAL match style.
-   */
-  public JakartaRegexpCapabilities() {}
-  
-  /**
-   * Constructs a RegexCapabilities with the provided match flags.
-   * Multiple flags should be ORed together.
-   * 
-   * @param flags The matching style
-   */
-  public JakartaRegexpCapabilities(int flags)
-  {
-    this.flags = flags;
-  }
-  
-  public void compile(String pattern) {
-    regexp = new RE(pattern, this.flags);
-  }
-
-  public boolean match(String string) {
-    return regexp.match(string);
-  }
-
-  public String prefix() {
-    char[] prefix = RegexpTunnel.getPrefix(regexp);
-    return prefix == null ? null : new String(prefix);
-  }
-
-  @Override
-  public boolean equals(Object o) {
-    if (this == o) return true;
-    if (o == null || getClass() != o.getClass()) return false;
-
-    final JakartaRegexpCapabilities that = (JakartaRegexpCapabilities) o;
-
-    if (regexp != null ? !regexp.equals(that.regexp) : that.regexp != null) return false;
-
-    return true;
-  }
-
-  @Override
-  public int hashCode() {
-    return (regexp != null ? regexp.hashCode() : 0);
-  }
-}
diff --git a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/JavaUtilRegexCapabilities.java b/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/JavaUtilRegexCapabilities.java
deleted file mode 100644
index 9bb32b7..0000000
--- a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/JavaUtilRegexCapabilities.java
+++ /dev/null
@@ -1,97 +0,0 @@
-package org.apache.lucene.search.regex;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.regex.Pattern;
-
-/**
- * An implementation tying Java's built-in java.util.regex to RegexQuery.
- *
- * Note that because this implementation currently only returns null from
- * {@link #prefix} that queries using this implementation will enumerate and
- * attempt to {@link #match} each term for the specified field in the index.
- */
-public class JavaUtilRegexCapabilities implements RegexCapabilities {
-  private Pattern pattern;
-  private int flags = 0;
-  
-  // Define the optional flags from Pattern that can be used.
-  // Do this here to keep Pattern contained within this class.
-  
-  public static final int FLAG_CANON_EQ = Pattern.CANON_EQ;
-  public static final int FLAG_CASE_INSENSITIVE = Pattern.CASE_INSENSITIVE;
-  public static final int FLAG_COMMENTS = Pattern.COMMENTS;
-  public static final int FLAG_DOTALL = Pattern.DOTALL;
-  public static final int FLAG_LITERAL = Pattern.LITERAL;
-  public static final int FLAG_MULTILINE = Pattern.MULTILINE;
-  public static final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;
-  public static final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;
-  
-  /**
-   * Default constructor that uses java.util.regex.Pattern 
-   * with its default flags.
-   */
-  public JavaUtilRegexCapabilities()  {
-    this.flags = 0;
-  }
-  
-  /**
-   * Constructor that allows for the modification of the flags that
-   * the java.util.regex.Pattern will use to compile the regular expression.
-   * This gives the user the ability to fine-tune how the regular expression 
-   * to match the functionality that they need. 
-   * The {@link java.util.regex.Pattern Pattern} class supports specifying 
-   * these fields via the regular expression text itself, but this gives the caller
-   * another option to modify the behavior. Useful in cases where the regular expression text
-   * cannot be modified, or if doing so is undesired.
-   * 
-   * @param flags The flags that are ORed together.
-   */
-  public JavaUtilRegexCapabilities(int flags) {
-    this.flags = flags;
-  }
-  
-  public void compile(String pattern) {
-    this.pattern = Pattern.compile(pattern, this.flags);
-  }
-
-  public boolean match(String string) {
-    return pattern.matcher(string).matches();
-  }
-
-  public String prefix() {
-    return null;
-  }
-
-  @Override
-  public boolean equals(Object o) {
-    if (this == o) return true;
-    if (o == null || getClass() != o.getClass()) return false;
-
-    final JavaUtilRegexCapabilities that = (JavaUtilRegexCapabilities) o;
-
-    if (pattern != null ? !pattern.equals(that.pattern) : that.pattern != null) return false;
-
-    return true;
-  }
-
-  @Override
-  public int hashCode() {
-    return (pattern != null ? pattern.hashCode() : 0);
-  }
-}
diff --git a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexCapabilities.java b/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexCapabilities.java
deleted file mode 100644
index 6270efd..0000000
--- a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexCapabilities.java
+++ /dev/null
@@ -1,48 +0,0 @@
-package org.apache.lucene.search.regex;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Defines basic operations needed by {@link RegexQuery} for a regular
- * expression implementation.
- */
-public interface RegexCapabilities {
-  /**
-   * Called by the constructor of {@link RegexTermEnum} allowing
-   * implementations to cache a compiled version of the regular
-   * expression pattern.
-   *
-   * @param pattern regular expression pattern
-   */
-  void compile(String pattern);
-
-  /**
-   *
-   * @param string
-   * @return true if string matches the pattern last passed to {@link #compile}.
-   */
-  boolean match(String string);
-
-  /**
-   * A wise prefix implementation can reduce the term enumeration (and thus increase performance)
-   * of RegexQuery dramatically!
-   *
-   * @return static non-regex prefix of the pattern last passed to {@link #compile}.  May return null.
-   */
-  String prefix();
-}
diff --git a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexQuery.java b/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexQuery.java
deleted file mode 100644
index 9562d5a..0000000
--- a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexQuery.java
+++ /dev/null
@@ -1,97 +0,0 @@
-package org.apache.lucene.search.regex;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.FilteredTermEnum;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.ToStringUtils;
-
-import java.io.IOException;
-
-/** Implements the regular expression term search query.
- * The expressions supported depend on the regular expression implementation
- * used by way of the {@link RegexCapabilities} interface.
- *
- * @see RegexTermEnum
- */
-public class RegexQuery extends MultiTermQuery implements RegexQueryCapable {
-  private RegexCapabilities regexImpl = new JavaUtilRegexCapabilities();
-  private Term term;
-
-  /** Constructs a query for terms matching <code>term</code>. */
-  public RegexQuery(Term term) {
-    this.term = term;
-  }
-  
-  public Term getTerm() { return term; }
-
-  /**
-   * Defines which {@link RegexCapabilities} implementation is used by this instance.
-   *
-   * @param impl
-   */
-  public void setRegexImplementation(RegexCapabilities impl) {
-    this.regexImpl = impl;
-  }
-
-  /**
-   * @return The implementation used by this instance.
-   */
-  public RegexCapabilities getRegexImplementation() {
-    return regexImpl;
-  }
-
-  @Override
-  protected FilteredTermEnum getEnum(IndexReader reader) throws IOException {
-    return new RegexTermEnum(reader, term, regexImpl);
-  }
-
-  @Override
-  public String toString(String field) {
-    StringBuilder buffer = new StringBuilder();
-    if (!term.field().equals(field)) {
-      buffer.append(term.field());
-      buffer.append(":");
-    }
-    buffer.append(term.text());
-    buffer.append(ToStringUtils.boost(getBoost()));
-    return buffer.toString();
-  }
-
-  /* generated by IntelliJ IDEA */
-  @Override
-  public boolean equals(Object o) {
-    if (this == o) return true;
-    if (o == null || getClass() != o.getClass()) return false;
-    if (!super.equals(o)) return false;
-
-    final RegexQuery that = (RegexQuery) o;
-
-    return regexImpl.equals(that.regexImpl);
-  }
-
-  /* generated by IntelliJ IDEA */
-  @Override
-  public int hashCode() {
-    int result = super.hashCode();
-    result = 29 * result + regexImpl.hashCode();
-    return result;
-  }
-}
diff --git a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexQueryCapable.java b/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexQueryCapable.java
deleted file mode 100644
index bb8a2c3..0000000
--- a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexQueryCapable.java
+++ /dev/null
@@ -1,27 +0,0 @@
-package org.apache.lucene.search.regex;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/**
- * Defines methods for regular expression supporting Querys to use.
- */
-public interface RegexQueryCapable {
-  void setRegexImplementation(RegexCapabilities impl);
-  RegexCapabilities getRegexImplementation();
-}
diff --git a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexTermEnum.java b/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexTermEnum.java
deleted file mode 100644
index ae814aa..0000000
--- a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/RegexTermEnum.java
+++ /dev/null
@@ -1,83 +0,0 @@
-package org.apache.lucene.search.regex;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.search.FilteredTermEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-
-import java.io.IOException;
-
-/**
- * Subclass of FilteredTermEnum for enumerating all terms that match the
- * specified regular expression term using the specified regular expression
- * implementation.
- * <p>
- * Term enumerations are always ordered by Term.compareTo().  Each term in
- * the enumeration is greater than all that precede it.
- */
-
-public class RegexTermEnum extends FilteredTermEnum {
-  private String field = "";
-  private String pre = "";
-  private boolean endEnum = false;
-  private RegexCapabilities regexImpl;
-
-  public RegexTermEnum(IndexReader reader, Term term, RegexCapabilities regexImpl) throws IOException {
-    super();
-    field = term.field();
-    String text = term.text();
-    this.regexImpl = regexImpl;
-
-    regexImpl.compile(text);
-
-    pre = regexImpl.prefix();
-    if (pre == null) pre = "";
-
-    setEnum(reader.terms(new Term(term.field(), pre)));
-  }
-
-  @Override
-  protected final boolean termCompare(Term term) {
-    if (field == term.field()) {
-      String searchText = term.text();
-      if (searchText.startsWith(pre)) {
-        return regexImpl.match(searchText);
-      }
-    }
-    endEnum = true;
-    return false;
-  }
-
-  @Override
-  public final float difference() {
-// TODO: adjust difference based on distance of searchTerm.text() and term().text()
-    return 1.0f;
-  }
-
-  @Override
-  public final boolean endEnum() {
-    return endEnum;
-  }
-
-  @Override
-  public void close() throws IOException {
-    super.close();
-    field = null;
-  }
-}
diff --git a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/SpanRegexQuery.java b/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/SpanRegexQuery.java
deleted file mode 100644
index aed0521..0000000
--- a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/SpanRegexQuery.java
+++ /dev/null
@@ -1,132 +0,0 @@
-package org.apache.lucene.search.regex;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.MultiTermQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.spans.SpanOrQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.search.spans.SpanTermQuery;
-import org.apache.lucene.search.spans.Spans;
-import org.apache.lucene.util.ToStringUtils;
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.ArrayList;
-
-/**
- * A SpanQuery version of {@link RegexQuery} allowing regular expression
- * queries to be nested within other SpanQuery subclasses.
- */
-public class SpanRegexQuery extends SpanQuery implements RegexQueryCapable {
-  private RegexCapabilities regexImpl = new JavaUtilRegexCapabilities();
-  private Term term;
-
-  public SpanRegexQuery(Term term) {
-    this.term = term;
-  }
-
-  public Term getTerm() { return term; }
-
-  @Override
-  public Query rewrite(IndexReader reader) throws IOException {
-    RegexQuery orig = new RegexQuery(term);
-    orig.setRegexImplementation(regexImpl);
-    orig.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);
-    BooleanQuery bq = (BooleanQuery) orig.rewrite(reader);
-
-    BooleanClause[] clauses = bq.getClauses();
-    SpanQuery[] sqs = new SpanQuery[clauses.length];
-    for (int i = 0; i < clauses.length; i++) {
-      BooleanClause clause = clauses[i];
-
-      // Clauses from RegexQuery.rewrite are always TermQuery's
-      TermQuery tq = (TermQuery) clause.getQuery();
-
-      sqs[i] = new SpanTermQuery(tq.getTerm());
-      sqs[i].setBoost(tq.getBoost());
-    }
-
-    SpanOrQuery query = new SpanOrQuery(sqs);
-    query.setBoost(orig.getBoost());
-
-    return query;
-  }
-
-  @Override
-  public Spans getSpans(IndexReader reader) throws IOException {
-    throw new UnsupportedOperationException("Query should have been rewritten");
-  }
-
-  @Override
-  public String getField() {
-    return term.field();
-  }
-
-  public Collection<Term> getTerms() {
-    Collection<Term> terms = new ArrayList<Term>();
-    terms.add(term);
-    return terms;
-  }
-
-  /* generated by IntelliJ IDEA */
-  @Override
-  public boolean equals(Object o) {
-    if (this == o) return true;
-    if (o == null || getClass() != o.getClass()) return false;
-
-    final SpanRegexQuery that = (SpanRegexQuery) o;
-
-    if (!regexImpl.equals(that.regexImpl)) return false;
-    if (!term.equals(that.term)) return false;
-
-    return true;
-  }
-
-  /* generated by IntelliJ IDEA */
-  @Override
-  public int hashCode() {
-    int result;
-    result = regexImpl.hashCode();
-    result = 29 * result + term.hashCode();
-    return result;
-  }
-
-  @Override
-  public String toString(String field) {
-    StringBuilder buffer = new StringBuilder();
-    buffer.append("spanRegexQuery(");
-    buffer.append(term);
-    buffer.append(")");
-    buffer.append(ToStringUtils.boost(getBoost()));
-    return buffer.toString();
-  }
-
-  public void setRegexImplementation(RegexCapabilities impl) {
-    this.regexImpl = impl;
-  }
-
-  public RegexCapabilities getRegexImplementation() {
-    return regexImpl;
-  }
-}
diff --git a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/package.html b/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/package.html
deleted file mode 100644
index c963307..0000000
--- a/lucene/contrib/regex/src/java/org/apache/lucene/search/regex/package.html
+++ /dev/null
@@ -1,5 +0,0 @@
-<html><head></head>
-<body>
-Regular expression Query.
-</body>
-</html>
diff --git a/lucene/contrib/regex/src/java/org/apache/regexp/RegexpTunnel.java b/lucene/contrib/regex/src/java/org/apache/regexp/RegexpTunnel.java
deleted file mode 100644
index 9e43b93..0000000
--- a/lucene/contrib/regex/src/java/org/apache/regexp/RegexpTunnel.java
+++ /dev/null
@@ -1,29 +0,0 @@
-package org.apache.regexp;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/**
- * This class exists as a gateway to access useful Jakarta Regexp package protected data.
- */
-public class RegexpTunnel {
-  public static char[] getPrefix(RE regexp) {
-    REProgram program = regexp.getProgram();
-    return program.prefix;
-  }
-}
diff --git a/lucene/contrib/regex/src/java/org/apache/regexp/package.html b/lucene/contrib/regex/src/java/org/apache/regexp/package.html
deleted file mode 100644
index 15b3b7a2..0000000
--- a/lucene/contrib/regex/src/java/org/apache/regexp/package.html
+++ /dev/null
@@ -1,24 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-This package exists to allow access to useful package protected data within
-Jakarta Regexp.  This data has now been opened up with an accessor, but
-an official release with that change has not been made to date.
-</body>
-</html>
diff --git a/lucene/contrib/regex/src/java/overview.html b/lucene/contrib/regex/src/java/overview.html
deleted file mode 100644
index 87f3adf..0000000
--- a/lucene/contrib/regex/src/java/overview.html
+++ /dev/null
@@ -1,26 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>
-      regex
-    </title>
-  </head>
-  <body>
-  regex
-  </body>
-</html>
\ No newline at end of file
diff --git a/lucene/contrib/regex/src/test/org/apache/lucene/search/regex/TestJakartaRegexpCapabilities.java b/lucene/contrib/regex/src/test/org/apache/lucene/search/regex/TestJakartaRegexpCapabilities.java
deleted file mode 100644
index e9ca88d..0000000
--- a/lucene/contrib/regex/src/test/org/apache/lucene/search/regex/TestJakartaRegexpCapabilities.java
+++ /dev/null
@@ -1,46 +0,0 @@
-package org.apache.lucene.search.regex;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import junit.framework.TestCase;
-
-/**
- * Testcase for {@link JakartaRegexpCapabilities}
- */
-public class TestJakartaRegexpCapabilities extends TestCase {
-
-  public void testGetPrefix(){
-    JakartaRegexpCapabilities cap = new JakartaRegexpCapabilities();
-    cap.compile("luc[e]?");
-    assertTrue(cap.match("luce"));
-    assertEquals("luc", cap.prefix());
-    
-    cap.compile("lucene");
-    assertTrue(cap.match("lucene"));
-    assertEquals("lucene", cap.prefix());
-  }
-  
-  public void testShakyPrefix(){
-    JakartaRegexpCapabilities cap = new JakartaRegexpCapabilities();
-    cap.compile("(ab|ac)");
-    assertTrue(cap.match("ab"));
-    assertTrue(cap.match("ac"));
-    // why is it not a???
-    assertNull(cap.prefix());
-  }
-}
diff --git a/lucene/contrib/regex/src/test/org/apache/lucene/search/regex/TestRegexQuery.java b/lucene/contrib/regex/src/test/org/apache/lucene/search/regex/TestRegexQuery.java
deleted file mode 100644
index 4785933..0000000
--- a/lucene/contrib/regex/src/test/org/apache/lucene/search/regex/TestRegexQuery.java
+++ /dev/null
@@ -1,134 +0,0 @@
-package org.apache.lucene.search.regex;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.index.TermEnum;
-
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.util.LuceneTestCase;
-
-public class TestRegexQuery extends LuceneTestCase {
-  private IndexSearcher searcher;
-  private final String FN = "field";
-
-
-  @Override
-  protected void setUp() throws Exception {
-    super.setUp();
-    RAMDirectory directory = new RAMDirectory();
-    try {
-      IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(
-          TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));
-      Document doc = new Document();
-      doc.add(new Field(FN, "the quick brown fox jumps over the lazy dog", Field.Store.NO, Field.Index.ANALYZED));
-      writer.addDocument(doc);
-      writer.optimize();
-      writer.close();
-      searcher = new IndexSearcher(directory, true);
-    } catch (Exception e) {
-      fail(e.toString());
-    }
-  }
-
-  @Override
-  protected void tearDown() throws Exception {
-    searcher.close();
-    super.tearDown();
-  }
-
-  private Term newTerm(String value) { return new Term(FN, value); }
-
-  private int  regexQueryNrHits(String regex, RegexCapabilities capability) throws Exception {
-    RegexQuery query = new RegexQuery( newTerm(regex));
-    
-    if ( capability != null )
-      query.setRegexImplementation(capability);
-    
-    return searcher.search(query, null, 1000).totalHits;
-  }
-
-  private int  spanRegexQueryNrHits(String regex1, String regex2, int slop, boolean ordered) throws Exception {
-    SpanRegexQuery srq1 = new SpanRegexQuery( newTerm(regex1));
-    SpanRegexQuery srq2 = new SpanRegexQuery( newTerm(regex2));
-    SpanNearQuery query = new SpanNearQuery( new SpanQuery[]{srq1, srq2}, slop, ordered);
-    
-    return searcher.search(query, null, 1000).totalHits;
-  }
-
-  public void testMatchAll() throws Exception {
-    TermEnum terms = new RegexQuery(new Term(FN, "jum.")).getEnum(searcher.getIndexReader());
-    // no term should match
-    assertNull(terms.term());
-    assertFalse(terms.next());
-  }
-
-  public void testRegex1() throws Exception {
-    assertEquals(1, regexQueryNrHits("^q.[aeiou]c.*$", null));
-  }
-
-  public void testRegex2() throws Exception {
-    assertEquals(0, regexQueryNrHits("^.[aeiou]c.*$", null));
-  }
-
-  public void testRegex3() throws Exception {
-    assertEquals(0, regexQueryNrHits("^q.[aeiou]c$", null));
-  }
-
-  public void testSpanRegex1() throws Exception {
-    assertEquals(1, spanRegexQueryNrHits("^q.[aeiou]c.*$", "dog", 6, true));
-  }
-
-  public void testSpanRegex2() throws Exception {
-    assertEquals(0, spanRegexQueryNrHits("^q.[aeiou]c.*$", "dog", 5, true));
-  }
-
-  public void testEquals() throws Exception {
-    RegexQuery query1 = new RegexQuery( newTerm("foo.*"));
-    query1.setRegexImplementation(new JakartaRegexpCapabilities());
-
-    RegexQuery query2 = new RegexQuery( newTerm("foo.*"));
-    assertFalse(query1.equals(query2));
-  }
-  
-  public void testJakartaCaseSensativeFail() throws Exception {
-    assertEquals(0, regexQueryNrHits("^.*DOG.*$", null));
-  }
-
-  public void testJavaUtilCaseSensativeFail() throws Exception {
-    assertEquals(0, regexQueryNrHits("^.*DOG.*$", null));
-  }
-  
-  public void testJakartaCaseInsensative() throws Exception {
-    assertEquals(1, regexQueryNrHits("^.*DOG.*$", new JakartaRegexpCapabilities(JakartaRegexpCapabilities.FLAG_MATCH_CASEINDEPENDENT)));
-  }
-  
-  public void testJavaUtilCaseInsensative() throws Exception {
-    assertEquals(1, regexQueryNrHits("^.*DOG.*$", new JavaUtilRegexCapabilities(JavaUtilRegexCapabilities.FLAG_CASE_INSENSITIVE)));
-  }
-
-}
-
diff --git a/lucene/contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java b/lucene/contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
deleted file mode 100644
index 7fd16dc..0000000
--- a/lucene/contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
+++ /dev/null
@@ -1,128 +0,0 @@
-package org.apache.lucene.search.regex;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.SimpleAnalyzer;
-import org.apache.lucene.analysis.standard.StandardAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MultiSearcher;
-import org.apache.lucene.search.spans.SpanFirstQuery;
-import org.apache.lucene.search.spans.SpanNearQuery;
-import org.apache.lucene.search.spans.SpanQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.LockObtainFailedException;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.LuceneTestCase;
-
-public class TestSpanRegexQuery extends LuceneTestCase {
-  
-  Directory indexStoreA = new RAMDirectory();
-
-  Directory indexStoreB = new RAMDirectory();
-
-  public void testSpanRegex() throws Exception {
-    RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new SimpleAnalyzer(TEST_VERSION_CURRENT)));
-    Document doc = new Document();
-    // doc.add(new Field("field", "the quick brown fox jumps over the lazy dog",
-    // Field.Store.NO, Field.Index.ANALYZED));
-    // writer.addDocument(doc);
-    // doc = new Document();
-    doc.add(new Field("field", "auto update", Field.Store.NO,
-        Field.Index.ANALYZED));
-    writer.addDocument(doc);
-    doc = new Document();
-    doc.add(new Field("field", "first auto update", Field.Store.NO,
-        Field.Index.ANALYZED));
-    writer.addDocument(doc);
-    writer.optimize();
-    writer.close();
-
-    IndexSearcher searcher = new IndexSearcher(directory, true);
-    SpanRegexQuery srq = new SpanRegexQuery(new Term("field", "aut.*"));
-    SpanFirstQuery sfq = new SpanFirstQuery(srq, 1);
-    // SpanNearQuery query = new SpanNearQuery(new SpanQuery[] {srq, stq}, 6,
-    // true);
-    int numHits = searcher.search(sfq, null, 1000).totalHits;
-    assertEquals(1, numHits);
-  }
-
-  public void testSpanRegexBug() throws CorruptIndexException, IOException {
-    createRAMDirectories();
-
-    SpanRegexQuery srq = new SpanRegexQuery(new Term("field", "a.*"));
-    SpanRegexQuery stq = new SpanRegexQuery(new Term("field", "b.*"));
-    SpanNearQuery query = new SpanNearQuery(new SpanQuery[] { srq, stq }, 6,
-        true);
-
-    // 1. Search the same store which works
-    IndexSearcher[] arrSearcher = new IndexSearcher[2];
-    arrSearcher[0] = new IndexSearcher(indexStoreA, true);
-    arrSearcher[1] = new IndexSearcher(indexStoreB, true);
-    MultiSearcher searcher = new MultiSearcher(arrSearcher);
-    int numHits = searcher.search(query, null, 1000).totalHits;
-    arrSearcher[0].close();
-    arrSearcher[1].close();
-
-    // Will fail here
-    // We expect 2 but only one matched
-    // The rewriter function only write it once on the first IndexSearcher
-    // So it's using term: a1 b1 to search on the second IndexSearcher
-    // As a result, it won't match the document in the second IndexSearcher
-    assertEquals(2, numHits);
-    indexStoreA.close();
-    indexStoreB.close();
-  }
-
-  private void createRAMDirectories() throws CorruptIndexException,
-      LockObtainFailedException, IOException {
-    // creating a document to store
-    Document lDoc = new Document();
-    lDoc.add(new Field("field", "a1 b1", Field.Store.NO,
-        Field.Index.ANALYZED_NO_NORMS));
-
-    // creating a document to store
-    Document lDoc2 = new Document();
-    lDoc2.add(new Field("field", "a2 b2", Field.Store.NO,
-        Field.Index.ANALYZED_NO_NORMS));
-
-    // creating first index writer
-    IndexWriter writerA = new IndexWriter(indexStoreA, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.CREATE));
-    writerA.addDocument(lDoc);
-    writerA.optimize();
-    writerA.close();
-
-    // creating second index writer
-    IndexWriter writerB = new IndexWriter(indexStoreB, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.CREATE));
-    writerB.addDocument(lDoc2);
-    writerB.optimize();
-    writerB.close();
-  }
-}
diff --git a/lucene/docs/contributions.html b/lucene/docs/contributions.html
index 55ce4ed..ddbdb1d 100644
--- a/lucene/docs/contributions.html
+++ b/lucene/docs/contributions.html
@@ -153,9 +153,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -180,9 +177,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/demo.html b/lucene/docs/demo.html
index 045b44b..db7c848 100644
--- a/lucene/docs/demo.html
+++ b/lucene/docs/demo.html
@@ -153,9 +153,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -180,9 +177,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/demo2.html b/lucene/docs/demo2.html
index 9db2d45..0386692 100644
--- a/lucene/docs/demo2.html
+++ b/lucene/docs/demo2.html
@@ -153,9 +153,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -180,9 +177,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/demo3.html b/lucene/docs/demo3.html
index 597cd89..258b043 100644
--- a/lucene/docs/demo3.html
+++ b/lucene/docs/demo3.html
@@ -153,9 +153,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -180,9 +177,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/demo4.html b/lucene/docs/demo4.html
index 4f2a072..46e9790 100644
--- a/lucene/docs/demo4.html
+++ b/lucene/docs/demo4.html
@@ -153,9 +153,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -180,9 +177,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/fileformats.html b/lucene/docs/fileformats.html
index ad2d1bc..1919ce1 100644
--- a/lucene/docs/fileformats.html
+++ b/lucene/docs/fileformats.html
@@ -153,9 +153,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -180,9 +177,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
@@ -439,7 +433,6 @@ document.write("Last Published: " + document.lastModified);
 	        written to the index (they can still be read, but on
 	        merge the new segment will write them,
 	        uncompressed). See issue LUCENE-1960 for details.
-
             </p>
 </div>
 
diff --git a/lucene/docs/gettingstarted.html b/lucene/docs/gettingstarted.html
index 38207c0..5f68fad 100644
--- a/lucene/docs/gettingstarted.html
+++ b/lucene/docs/gettingstarted.html
@@ -153,9 +153,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -180,9 +177,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/index.html b/lucene/docs/index.html
index 3123ad3..93b6d92 100644
--- a/lucene/docs/index.html
+++ b/lucene/docs/index.html
@@ -151,9 +151,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -178,9 +175,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/linkmap.html b/lucene/docs/linkmap.html
index b4414ce..f447575 100644
--- a/lucene/docs/linkmap.html
+++ b/lucene/docs/linkmap.html
@@ -151,9 +151,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -178,9 +175,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
@@ -367,12 +361,6 @@ document.write("Last Published: " + document.lastModified);
 		    
 <ul>
 <li>
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>&nbsp;&nbsp;___________________&nbsp;&nbsp;<em>javadoc-contrib-fast-vector-highlighter</em>
-</li>
-</ul>
-		    
-<ul>
-<li>
 <a href="api/contrib-highlighter/index.html">Highlighter</a>&nbsp;&nbsp;___________________&nbsp;&nbsp;<em>javadoc-contrib-highlighter</em>
 </li>
 </ul>
@@ -418,12 +406,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>&nbsp;&nbsp;___________________&nbsp;&nbsp;<em>javadoc-contrib-queryparser</em>
 </li>
 </ul>
-		    
-<ul>
-<li>
-<a href="api/contrib-regex/index.html">Regex</a>&nbsp;&nbsp;___________________&nbsp;&nbsp;<em>javadoc-contrib-regex</em>
-</li>
-</ul>
 			
 		    
 <ul>
diff --git a/lucene/docs/linkmap.pdf b/lucene/docs/linkmap.pdf
index e833b38..099d597 100644
--- a/lucene/docs/linkmap.pdf
+++ b/lucene/docs/linkmap.pdf
@@ -20,23 +20,20 @@ This is a map of the complete site and its structure.
                                     ?? Bdb ___________________ javadoc-contrib-bdb
                                     ?? Bdb-je ___________________ javadoc-contrib-bdb-je
                                     ?? Benchmark ___________________ javadoc-contrib-benchmark
-                                    ?? Fast Vector
-                                        Highlighter ___________________ javadoc-contrib-fast-vector-highlight
                                     ?? Highlighter ___________________ javadoc-contrib-highlighter
                                     ?? ICU ___________________ javadoc-contrib-icu
                                     ?? Instantiated ___________________ javadoc-contrib-instantiated
+                                    ?? Lucli ___________________ javadoc-contrib-lucli
+                                    ?? Memory ___________________ javadoc-contrib-memory
 
                    Copyright © 2006 The Apache Software Foundation. All rights reserved.
                                                                                               Site Linkmap Table of Contents
 
-                  ?? Lucli ___________________ javadoc-contrib-lucli
-                  ?? Memory ___________________ javadoc-contrib-memory
                   ?? Miscellaneous ___________________ javadoc-contrib-misc
                   ?? Queries ___________________ javadoc-contrib-queries
                   ?? Query Parser
 
                       Framework ___________________ javadoc-contrib-queryparser
-                  ?? Regex ___________________ javadoc-contrib-regex
                   ?? Remote ___________________ javadoc-contrib-remote
                   ?? Spatial ___________________ javadoc-contrib-spatial
                   ?? Spellchecker ___________________ javadoc-contrib-spellchecker
diff --git a/lucene/docs/lucene-contrib/index.html b/lucene/docs/lucene-contrib/index.html
index 7da3976..2d6cbef 100644
--- a/lucene/docs/lucene-contrib/index.html
+++ b/lucene/docs/lucene-contrib/index.html
@@ -153,9 +153,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="../api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="../api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="../api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -180,9 +177,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="../api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="../api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="../api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
@@ -282,9 +276,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="#highlighter">highlighter</a>
 </li>
 <li>
-<a href="#fast-vector-highlighter">fast-vector-highlighter</a>
-</li>
-<li>
 <a href="#icu">icu</a>
 </li>
 <li>
@@ -303,7 +294,7 @@ document.write("Last Published: " + document.lastModified);
 <a href="#queryparser">queryparser</a>
 </li>
 <li>
-<a href="#regex">regex</a>
+<a href="#queries">queries</a>
 </li>
 <li>
 <a href="#remote">remote</a>
@@ -405,84 +396,78 @@ document.write("Last Published: " + document.lastModified);
 <p>A set of classes for highlighting matching terms in search results.</p>
 <p>See <a href="../api/contrib-highlighter/index.html">highlighter javadoc</a>
 </p>
-<a name="N10080"></a><a name="fast-vector-highlighter"></a>
-<h3 class="boxed">fast-vector-highlighter</h3>
-<p>An alternative set of classes for highlighting matching terms in search results that relies on stored term vectors.
-                This highlighter can be much faster than the standard highlighter, especially on large fields.</p>
-<p>See <a href="../api/contrib-fast-vector-highlighter/index.html">fast-vector-highlighter javadoc</a>
-</p>
-<a name="N1008F"></a><a name="icu"></a>
+<a name="N10080"></a><a name="icu"></a>
 <h3 class="boxed">icu</h3>
 <p>Provides integration with ICU (International Components for Unicode) for
                 stronger Unicode and internationalization support. </p>
 <p>See <a href="../api/contrib-icu/index.html">icu javadoc</a>
 </p>
-<a name="N1009E"></a><a name="instantiated"></a>
+<a name="N1008F"></a><a name="instantiated"></a>
 <h3 class="boxed">instantiated</h3>
 <p>RAM-based index that enables much faster searching than RAMDirectory in certain situations.</p>
 <p>See <a href="../api/contrib-instantiated/index.html">instantiated javadoc</a>
 </p>
-<a name="N100AD"></a><a name="lucli"></a>
+<a name="N1009E"></a><a name="lucli"></a>
 <h3 class="boxed">lucli</h3>
 <p>An application that allows Lucene index manipulation from the command-line.</p>
 <p>See <a href="../api/contrib-lucli/index.html">lucli javadoc</a>
 </p>
-<a name="N100BC"></a><a name="memory"></a>
+<a name="N100AD"></a><a name="memory"></a>
 <h3 class="boxed">memory</h3>
 <p>High-performance single-document main memory index.</p>
 <p>See <a href="../api/contrib-memory/index.html">memory javadoc</a>
 </p>
-<a name="N100CB"></a><a name="misc"></a>
+<a name="N100BC"></a><a name="misc"></a>
 <h3 class="boxed">misc</h3>
 <p>A variety of miscellaneous files, including QueryParsers, and other alternate Lucene class implementations and tools.</p>
 <p>See <a href="../api/contrib-misc/index.html">misc javadoc</a>
 </p>
-<a name="N100DA"></a><a name="queryparser"></a>
+<a name="N100CB"></a><a name="queryparser"></a>
 <h3 class="boxed">queryparser</h3>
 <p>A new Lucene query parser implementation, which matches the syntax of the core QueryParser but offers a more modular architecture to enable customization.</p>
 <p>See <a href="../api/contrib-queryparser/index.html">queryparser javadoc</a>
 </p>
-<a name="N100E9"></a><a name="regex"></a>
-<h3 class="boxed">regex</h3>
-<p>Queries with additional regex matching capabilities.</p>
-<p>See <a href="../api/contrib-regex/index.html">regex javadoc</a>
+<a name="N100DA"></a><a name="queries"></a>
+<h3 class="boxed">queries</h3>
+<p>Additional queries for Lucene.</p>
+<p>See <a href="../api/contrib-queries/index.html">queries javadoc</a>
 </p>
-<a name="N100F8"></a><a name="remote"></a>
+<a name="N100E9"></a><a name="remote"></a>
 <h3 class="boxed">remote</h3>
 <p>Classes to help use Lucene with RMI.</p>
 <p>See <a href="../api/contrib-remote/index.html">remote javadoc</a>
 </p>
-<a name="N10107"></a><a name="spatial"></a>
+<a name="N100F8"></a><a name="spatial"></a>
 <h3 class="boxed">spatial</h3>
 <p>Classes to help with efficient distance based sorting.</p>
 <p>See <a href="../api/contrib-spatial/index.html">spatial javadoc</a>
 </p>
-<a name="N10116"></a><a name="spellchecker"></a>
+<a name="N10107"></a><a name="spellchecker"></a>
 <h3 class="boxed">spellchecker</h3>
 <p>Provides tools for spellchecking and suggestions with Lucene.</p>
 <p>See <a href="../api/contrib-spellchecker/index.html">spellchecker javadoc</a>
 </p>
-<a name="N10125"></a><a name="surround"></a>
+<a name="N10116"></a><a name="surround"></a>
 <h3 class="boxed">surround</h3>
 <p>A QueryParser that supports the Span family of queries as well as pre and infix notation.</p>
 <p>See <a href="../api/contrib-surround/index.html">surround javadoc</a>
 </p>
-<a name="N10134"></a><a name="swing"></a>
+<a name="N10125"></a><a name="swing"></a>
 <h3 class="boxed">swing</h3>
 <p>Swing components designed to integrate with Lucene.</p>
 <p>See <a href="../api/contrib-swing/index.html">swing javadoc</a>
 </p>
-<a name="N10143"></a><a name="wikipedia"></a>
+<a name="N10134"></a><a name="wikipedia"></a>
 <h3 class="boxed">wikipedia</h3>
 <p>Tools for working with wikipedia content.</p>
 <p>See <a href="../api/contrib-wikipedia/index.html">wikipedia javadoc</a>
 </p>
-<a name="N10152"></a><a name="wordnet"></a>
+<a name="N10143"></a><a name="wordnet"></a>
 <h3 class="boxed">wordnet</h3>
 <p>Tools to help utilize wordnet synonyms with Lucene</p>
 <p>See <a href="../api/contrib-wordnet/index.html">wordnet javadoc</a>
 </p>
-<a name="N10161"></a><a name="xml-query-parser"></a>
+<a name="N10152"></a><a name="xml-query-parser"></a>
 <h3 class="boxed">xml-query-parser</h3>
 <p>A QueryParser that can read queries written in an XML format.</p>
 <p>See <a href="../api/contrib-wordnet/index.html">xml-query-parser javadoc</a>
diff --git a/lucene/docs/lucene-contrib/index.pdf b/lucene/docs/lucene-contrib/index.pdf
index e087422..41eed6e 100644
--- a/lucene/docs/lucene-contrib/index.pdf
+++ b/lucene/docs/lucene-contrib/index.pdf
@@ -8,22 +8,21 @@ Table of contents
     1.3 benchmark..................................................................................................................... 2
     1.4 db................................................................................................................................... 3
     1.5 highlighter......................................................................................................................3
-    1.6 fast-vector-highlighter................................................................................................... 3
-    1.7 icu.................................................................................................................................. 3
-    1.8 instantiated.....................................................................................................................3
-    1.9 lucli................................................................................................................................ 3
-    1.10 memory........................................................................................................................ 3
-    1.11 misc..............................................................................................................................4
-    1.12 queryparser...................................................................................................................4
-    1.13 regex.............................................................................................................................4
-    1.14 remote.......................................................................................................................... 4
-    1.15 spatial........................................................................................................................... 4
-    1.16 spellchecker................................................................................................................. 4
-    1.17 surround....................................................................................................................... 4
-    1.18 swing............................................................................................................................5
-    1.19 wikipedia......................................................................................................................5
-    1.20 wordnet........................................................................................................................ 5
-    1.21 xml-query-parser..........................................................................................................5
+    1.6 icu.................................................................................................................................. 3
+    1.7 instantiated.....................................................................................................................3
+    1.8 lucli................................................................................................................................ 3
+    1.9 memory..........................................................................................................................3
+    1.10 misc..............................................................................................................................3
+    1.11 queryparser...................................................................................................................4
+    1.12 queries..........................................................................................................................4
+    1.13 remote.......................................................................................................................... 4
+    1.14 spatial........................................................................................................................... 4
+    1.15 spellchecker................................................................................................................. 4
+    1.16 surround....................................................................................................................... 4
+    1.17 swing............................................................................................................................4
+    1.18 wikipedia......................................................................................................................5
+    1.19 wordnet........................................................................................................................ 5
+    1.20 xml-query-parser..........................................................................................................5
 
                    Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Lucene Contrib
@@ -86,81 +85,76 @@ See db javadoc
 A set of classes for highlighting matching terms in search results.
 See highlighter javadoc
 
-1.6. fast-vector-highlighter
-An alternative set of classes for highlighting matching terms in search results that relies on
-stored term vectors. This highlighter can be much faster than the standard highlighter,
-especially on large fields.
-See fast-vector-highlighter javadoc
-
-1.7. icu
+1.6. icu
 Provides integration with ICU (International Components for Unicode) for stronger Unicode
 and internationalization support.
 See icu javadoc
 
-1.8. instantiated
+1.7. instantiated
 RAM-based index that enables much faster searching than RAMDirectory in certain
 situations.
 See instantiated javadoc
 
-1.9. lucli
+1.8. lucli
 An application that allows Lucene index manipulation from the command-line.
 See lucli javadoc
 
-1.10. memory
+1.9. memory
+High-performance single-document main memory index.
+See memory javadoc
+
+1.10. misc
+A variety of miscellaneous files, including QueryParsers, and other alternate Lucene class
 
 Page 3
 
         Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Lucene Contrib
 
-High-performance single-document main memory index.
-See memory javadoc
-
-1.11. misc
-A variety of miscellaneous files, including QueryParsers, and other alternate Lucene class
 implementations and tools.
 See misc javadoc
 
-1.12. queryparser
+1.11. queryparser
 A new Lucene query parser implementation, which matches the syntax of the core
 QueryParser but offers a more modular architecture to enable customization.
 See queryparser javadoc
 
-1.13. regex
-Queries with additional regex matching capabilities.
-See regex javadoc
+1.12. queries
+Additional queries for Lucene.
+See queries javadoc
 
-1.14. remote
+1.13. remote
 Classes to help use Lucene with RMI.
 See remote javadoc
 
-1.15. spatial
+1.14. spatial
 Classes to help with efficient distance based sorting.
 See spatial javadoc
 
-1.16. spellchecker
+1.15. spellchecker
 Provides tools for spellchecking and suggestions with Lucene.
 See spellchecker javadoc
 
-1.17. surround
+1.16. surround
+A QueryParser that supports the Span family of queries as well as pre and infix notation.
+See surround javadoc
+
+1.17. swing
+Swing components designed to integrate with Lucene.
 
                                                                        Page 4
 
 Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Lucene Contrib
 
-A QueryParser that supports the Span family of queries as well as pre and infix notation.
-See surround javadoc
-1.18. swing
-Swing components designed to integrate with Lucene.
 See swing javadoc
-1.19. wikipedia
+1.18. wikipedia
 Tools for working with wikipedia content.
 See wikipedia javadoc
-1.20. wordnet
+1.19. wordnet
 Tools to help utilize wordnet synonyms with Lucene
 See wordnet javadoc
-1.21. xml-query-parser
+1.20. xml-query-parser
 A QueryParser that can read queries written in an XML format.
 See xml-query-parser javadoc
 
diff --git a/lucene/docs/queryparsersyntax.html b/lucene/docs/queryparsersyntax.html
index 6228c84..17cf145 100644
--- a/lucene/docs/queryparsersyntax.html
+++ b/lucene/docs/queryparsersyntax.html
@@ -153,9 +153,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -180,9 +177,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/scoring.html b/lucene/docs/scoring.html
index 0079c07..b11a7f9 100644
--- a/lucene/docs/scoring.html
+++ b/lucene/docs/scoring.html
@@ -153,9 +153,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -180,9 +177,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/systemrequirements.html b/lucene/docs/systemrequirements.html
index aea1ece..f8e85ce 100644
--- a/lucene/docs/systemrequirements.html
+++ b/lucene/docs/systemrequirements.html
@@ -151,9 +151,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-benchmark/index.html">Benchmark</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-fast-vector-highlighter/index.html">Fast Vector Highlighter</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-highlighter/index.html">Highlighter</a>
 </div>
 <div class="menuitem">
@@ -178,9 +175,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-queryparser/index.html">Query Parser Framework</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-regex/index.html">Regex</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-remote/index.html">Remote</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/src/site/src/documentation/content/xdocs/lucene-contrib/index.xml b/lucene/src/site/src/documentation/content/xdocs/lucene-contrib/index.xml
index f1f9c90..7bf93d4 100644
--- a/lucene/src/site/src/documentation/content/xdocs/lucene-contrib/index.xml
+++ b/lucene/src/site/src/documentation/content/xdocs/lucene-contrib/index.xml
@@ -74,12 +74,6 @@
                 <p>A set of classes for highlighting matching terms in search results.</p>
                 <p>See <a href="../api/contrib-highlighter/index.html">highlighter javadoc</a></p>
             </section>
-            
-            <section id="fast-vector-highlighter"><title>fast-vector-highlighter</title>
-                <p>An alternative set of classes for highlighting matching terms in search results that relies on stored term vectors.
-                This highlighter can be much faster than the standard highlighter, especially on large fields.</p>
-                <p>See <a href="../api/contrib-fast-vector-highlighter/index.html">fast-vector-highlighter javadoc</a></p>
-            </section>
 
             <section id="icu"><title>icu</title>
                 <p>Provides integration with ICU (International Components for Unicode) for
@@ -112,9 +106,9 @@
                 <p>See <a href="../api/contrib-queryparser/index.html">queryparser javadoc</a></p>
             </section>                          
  
-            <section id="regex"><title>regex</title>
-                <p>Queries with additional regex matching capabilities.</p>
-                <p>See <a href="../api/contrib-regex/index.html">regex javadoc</a></p>
+            <section id="queries"><title>queries</title>
+                <p>Additional queries for Lucene.</p>
+                <p>See <a href="../api/contrib-queries/index.html">queries javadoc</a></p>
             </section> 
             
             <section id="remote"><title>remote</title>
diff --git a/lucene/src/site/src/documentation/content/xdocs/site.xml b/lucene/src/site/src/documentation/content/xdocs/site.xml
index f65e960..14c079b 100755
--- a/lucene/src/site/src/documentation/content/xdocs/site.xml
+++ b/lucene/src/site/src/documentation/content/xdocs/site.xml
@@ -58,7 +58,6 @@ See http://forrest.apache.org/docs/linking.html for more info
 		    <javadoc-contrib-bdb label="Bdb" href="ext:javadocs-contrib-bdb"/>
 		    <javadoc-contrib-bdb-je label="Bdb-je" href="ext:javadocs-contrib-bdb-je"/>
 		    <javadoc-contrib-benchmark label="Benchmark" href="ext:javadocs-contrib-benchmark"/>
-		    <javadoc-contrib-fast-vector-highlighter label="Fast Vector Highlighter" href="ext:javadocs-contrib-fast-vector-highlighter"/>
 		    <javadoc-contrib-highlighter label="Highlighter" href="ext:javadocs-contrib-highlighter"/>
 		    <javadoc-contrib-icu label="ICU" href="ext:javadocs-contrib-icu"/>
 		    <javadoc-contrib-instantiated label="Instantiated" href="ext:javadocs-contrib-instantiated"/>
@@ -67,7 +66,6 @@ See http://forrest.apache.org/docs/linking.html for more info
 		    <javadoc-contrib-misc label="Miscellaneous" href="ext:javadocs-contrib-misc"/>
 		    <javadoc-contrib-queries label="Queries" href="ext:javadocs-contrib-queries"/>
                     <javadoc-contrib-queryparser label="Query Parser Framework" href="ext:javadocs-contrib-queryparser"/>
-		    <javadoc-contrib-regex label="Regex" href="ext:javadocs-contrib-regex"/>
 <!-- This package has currently no content			
 		    <javadoc-contrib-similarity label="Similarity" href="ext:javadocs-contrib-similarity"/>
 -->			
@@ -112,7 +110,6 @@ See http://forrest.apache.org/docs/linking.html for more info
 	<javadocs-contrib-bdb href="api/contrib-bdb/index.html"/>
 	<javadocs-contrib-bdb-je href="api/contrib-bdb-je/index.html"/>
 	<javadocs-contrib-benchmark href="api/contrib-benchmark/index.html"/>
-	<javadocs-contrib-fast-vector-highlighter href="api/contrib-fast-vector-highlighter/index.html"/>
 	<javadocs-contrib-highlighter href="api/contrib-highlighter/index.html"/>
     <javadocs-contrib-icu href="api/contrib-icu/index.html"/>
 	<javadocs-contrib-instantiated href="api/contrib-instantiated/index.html"/>
@@ -121,7 +118,6 @@ See http://forrest.apache.org/docs/linking.html for more info
 	<javadocs-contrib-misc href="api/contrib-misc/index.html"/>
 	<javadocs-contrib-queries href="api/contrib-queries/index.html"/>
         <javadocs-contrib-queryparser href="api/contrib-queryparser/index.html"/>
-	<javadocs-contrib-regex href="api/contrib-regex/index.html"/>
 	<javadocs-contrib-remote href="api/contrib-remote/index.html"/>
 	<javadocs-contrib-similarity href="api/contrib-similarity/index.html"/>
 	<javadocs-contrib-spatial href="api/contrib-spatial/index.html"/>

