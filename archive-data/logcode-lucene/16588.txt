GitDiffStart: 3c905b5ae803cc2fee0a1ff40901da11055d9919 | Sun Jan 16 10:45:38 2011 +0000
diff --git a/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java b/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
index 91d0d2e..72dad36 100644
--- a/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -121,29 +121,6 @@ final class DocumentsWriter {
   private AtomicInteger numDocsInRAM = new AtomicInteger(0);
   private AtomicLong ramUsed = new AtomicLong(0);
 
-  static class DocState {
-    DocumentsWriter docWriter;
-    Analyzer analyzer;
-    int maxFieldLength;
-    PrintStream infoStream;
-    Similarity similarity;
-    int docID;
-    Document doc;
-    String maxTermPrefix;
-
-    // Only called by asserts
-    public boolean testPoint(String name) {
-      return docWriter.indexWriter.testPoint(name);
-    }
-
-    public void clear() {
-      // don't hold onto doc nor analyzer, in case it is
-      // largish:
-      doc = null;
-      analyzer = null;
-    }
-  }
-
   // How much RAM we can use before flushing.  This is 0 if
   // we are flushing by doc count instead.
   private long ramBufferSize = (long) (IndexWriterConfig.DEFAULT_RAM_BUFFER_SIZE_MB*1024*1024);
@@ -155,7 +132,7 @@ final class DocumentsWriter {
   private final FieldInfos fieldInfos;
 
   final BufferedDeletes bufferedDeletes;
-  final SegmentDeletes pendingDeletes;
+  SegmentDeletes pendingDeletes;
   final IndexingChain chain;
 
   final DocumentsWriterPerThreadPool perThreadPool;
@@ -175,13 +152,19 @@ final class DocumentsWriter {
   boolean deleteQueries(final Query... queries) throws IOException {
     Iterator<ThreadState> threadsIterator = perThreadPool.getActivePerThreadsIterator();
 
-    boolean added = false;
+    boolean deleted = false;
     while (threadsIterator.hasNext()) {
-      threadsIterator.next().perThread.deleteQueries(queries);
-      added = true;
+      ThreadState state = threadsIterator.next();
+      state.lock();
+      try {
+        state.perThread.deleteQueries(queries);
+        deleted = true;
+      } finally {
+        state.unlock();
+      }
     }
 
-    if (!added) {
+    if (!deleted) {
       synchronized(this) {
         for (Query query : queries) {
           pendingDeletes.addQuery(query, SegmentDeletes.MAX_INT);
@@ -199,13 +182,19 @@ final class DocumentsWriter {
   boolean deleteTerms(final Term... terms) throws IOException {
     Iterator<ThreadState> threadsIterator = perThreadPool.getActivePerThreadsIterator();
 
-    boolean added = false;
+    boolean deleted = false;
     while (threadsIterator.hasNext()) {
-      threadsIterator.next().perThread.deleteTerms(terms);
-      added = true;
+      ThreadState state = threadsIterator.next();
+      deleted = true;
+      state.lock();
+      try {
+        state.perThread.deleteTerms(terms);
+      } finally {
+        state.unlock();
+      }
     }
 
-    if (!added) {
+    if (!deleted) {
       synchronized(this) {
         for (Term term : terms) {
           pendingDeletes.addTerm(term, SegmentDeletes.MAX_INT);
@@ -220,6 +209,26 @@ final class DocumentsWriter {
     return deleteTerms(term);
   }
 
+  boolean deleteTerm(final Term term, ThreadState exclude) {
+    Iterator<ThreadState> threadsIterator = perThreadPool.getActivePerThreadsIterator();
+
+    boolean deleted = false;
+    while (threadsIterator.hasNext()) {
+      deleted = true;
+      ThreadState state = threadsIterator.next();
+      if (state != exclude) {
+        state.lock();
+        try {
+          state.perThread.deleteTerm(term);
+        } finally {
+          state.unlock();
+        }
+      }
+    }
+
+    return deleted;
+  }
+
   public FieldInfos getFieldInfos() {
     return fieldInfos;
   }
@@ -371,25 +380,32 @@ final class DocumentsWriter {
       dwpt.addDocument(doc, analyzer);
 
       if (delTerm != null) {
-        deleteTerm(delTerm);
+        dwpt.deleteTerm(delTerm);
       }
       dwpt.commitDocument();
       numDocsInRAM.incrementAndGet();
 
       newSegment = finishAddDocument(dwpt, perThreadRAMUsedBeforeAdd);
-      if (newSegment != null) {
-        perThreadPool.clearThreadBindings(perThread);
+      if (newSegment != null && dwpt.pendingDeletes.any()) {
+        bufferedDeletes.pushDeletes(dwpt.pendingDeletes, newSegment);
+        dwpt.pendingDeletes = new SegmentDeletes();
       }
-
     } finally {
       perThread.unlock();
     }
 
     if (newSegment != null) {
+      perThreadPool.clearThreadBindings(perThread);
       finishFlushedSegment(newSegment);
       return true;
     }
 
+    // delete term from other DWPTs later, so that this thread
+    // doesn't have to lock multiple DWPTs at the same time
+    if (delTerm != null) {
+      deleteTerm(delTerm, perThread);
+    }
+
     return false;
   }
 
@@ -416,14 +432,32 @@ final class DocumentsWriter {
     return newSegment;
   }
 
+  private final void pushToLastSegment(SegmentDeletes segmentDeletes) {
+    synchronized(indexWriter) {
+      // Lock order: DW -> BD
+      if (segmentDeletes.any()) {
+        if (indexWriter.segmentInfos.size() > 0) {
+          if (infoStream != null) {
+            message("flush: push buffered deletes to previously flushed segment " + indexWriter.segmentInfos.lastElement());
+          }
+          bufferedDeletes.pushDeletes(segmentDeletes, indexWriter.segmentInfos.lastElement(), true);
+        } else {
+          if (infoStream != null) {
+            message("flush: drop buffered deletes: no segments");
+          }
+          // We can safely discard these deletes: since
+          // there are no segments, the deletions cannot
+          // affect anything.
+        }
+      }
+    }
+  }
+
   final boolean flushAllThreads(final boolean flushDeletes)
     throws IOException {
 
     if (flushDeletes) {
-      if (indexWriter.segmentInfos.size() > 0 && pendingDeletes.any()) {
-        bufferedDeletes.pushDeletes(pendingDeletes, indexWriter.segmentInfos.lastElement(), true);
-        pendingDeletes.clear();
-      }
+      pushToLastSegment(pendingDeletes);
     }
 
     Iterator<ThreadState> threadsIterator = perThreadPool.getActivePerThreadsIterator();
@@ -450,13 +484,16 @@ final class DocumentsWriter {
           newSegment = dwpt.flush();
 
           if (newSegment != null) {
-            IndexWriter.setDiagnostics(newSegment, "flush");
-            dwpt.pushDeletes(newSegment, indexWriter.segmentInfos);
             anythingFlushed = true;
             perThreadPool.clearThreadBindings(perThread);
+            if (dwpt.pendingDeletes.any()) {
+              bufferedDeletes.pushDeletes(dwpt.pendingDeletes, newSegment);
+              dwpt.pendingDeletes = new SegmentDeletes();
+            }
           }
-        } else if (flushDeletes) {
-          dwpt.pushDeletes(null, indexWriter.segmentInfos);
+        }
+        else if (flushDeletes && dwpt.pendingDeletes.any()) {
+          pushToLastSegment(dwpt.pendingDeletes);
         }
       } finally {
         perThread.unlock();
@@ -485,6 +522,10 @@ final class DocumentsWriter {
   }
 
   void finishFlushedSegment(SegmentInfo newSegment) throws IOException {
+    assert newSegment != null;
+
+    IndexWriter.setDiagnostics(newSegment, "flush");
+
     if (indexWriter.useCompoundFile(newSegment)) {
       String compoundFileName = IndexFileNames.segmentFileName(newSegment.name, "", IndexFileNames.COMPOUND_FILE_EXTENSION);
       message("creating compound file " + compoundFileName);
diff --git a/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java b/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
index 0fc743f..594da66 100644
--- a/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
+++ b/lucene/src/java/org/apache/lucene/index/DocumentsWriterPerThread.java
@@ -81,7 +81,7 @@ public class DocumentsWriterPerThread {
   };
 
   // Deletes for our still-in-RAM (to be flushed next) segment
-  private SegmentDeletes pendingDeletes = new SegmentDeletes();
+  SegmentDeletes pendingDeletes = new SegmentDeletes();
 
   static class DocState {
     final DocumentsWriterPerThread docWriter;
@@ -170,6 +170,7 @@ public class DocumentsWriterPerThread {
   }
 
   public void addDocument(Document doc, Analyzer analyzer) throws IOException {
+    assert writer.testPoint("DocumentsWriterPerThread addDocument start");
     docState.doc = doc;
     docState.analyzer = analyzer;
     docState.docID = numDocsInRAM;
@@ -206,35 +207,9 @@ public class DocumentsWriterPerThread {
     }
   }
 
-  void pushDeletes(SegmentInfo newSegment, SegmentInfos segmentInfos) {
-    // Lock order: DW -> BD
-    if (pendingDeletes.any()) {
-      if (newSegment != null) {
-        if (infoStream != null) {
-          message("flush: push buffered deletes to newSegment");
-        }
-        parent.bufferedDeletes.pushDeletes(pendingDeletes, newSegment);
-      } else if (segmentInfos.size() > 0) {
-        if (infoStream != null) {
-          message("flush: push buffered deletes to previously flushed segment " + segmentInfos.lastElement());
-        }
-        parent.bufferedDeletes.pushDeletes(pendingDeletes, segmentInfos.lastElement(), true);
-      } else {
-        if (infoStream != null) {
-          message("flush: drop buffered deletes: no segments");
-        }
-        // We can safely discard these deletes: since
-        // there are no segments, the deletions cannot
-        // affect anything.
-      }
-      pendingDeletes = new SegmentDeletes();
-    }
-  }
-
-
   // Buffer a specific docID for deletion.  Currently only
   // used when we hit a exception when adding a document
-  synchronized void deleteDocID(int docIDUpto) {
+  void deleteDocID(int docIDUpto) {
     pendingDeletes.addDocID(docIDUpto);
     // NOTE: we do not trigger flush here.  This is
     // potentially a RAM leak, if you have an app that tries
@@ -247,13 +222,13 @@ public class DocumentsWriterPerThread {
     // confounding exception).
   }
 
-  synchronized void deleteQueries(Query... queries) {
+  void deleteQueries(Query... queries) {
     for (Query query : queries) {
       pendingDeletes.addQuery(query, numDocsInRAM);
     }
   }
 
-  synchronized void deleteQuery(Query query) {
+  void deleteQuery(Query query) {
     pendingDeletes.addQuery(query, numDocsInRAM);
   }
 
@@ -263,7 +238,7 @@ public class DocumentsWriterPerThread {
     }
   }
 
-  synchronized void deleteTerm(Term term) {
+  void deleteTerm(Term term) {
     pendingDeletes.addTerm(term, numDocsInRAM);
   }
 
diff --git a/lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java b/lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
index 9a40ea2..83ba2c4 100644
--- a/lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
+++ b/lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions.java
@@ -245,7 +245,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     _TestUtil.checkIndex(dir);
     dir.close();
   }
-  
+
   // LUCENE-1198
   private static final class MockIndexWriter2 extends IndexWriter {
 
@@ -257,12 +257,12 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
 
     @Override
     boolean testPoint(String name) {
-      if (doFail && name.equals("DocumentsWriter.ThreadState.init start"))
+      if (doFail && name.equals("DocumentsWriterPerThread addDocument start"))
         throw new RuntimeException("intentionally failing");
       return true;
     }
   }
-  
+
   private class CrashingFilter extends TokenFilter {
     String fieldName;
     int count;
@@ -334,7 +334,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     w.addDocument(doc);
     w.close();
     dir.close();
-  }    
+  }
 
   private static final class MockIndexWriter3 extends IndexWriter {
 
@@ -354,7 +354,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       return true;
     }
   }
-  
+
 
   // LUCENE-1210
   public void testExceptionOnMergeInit() throws IOException {
@@ -379,7 +379,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     w.close();
     dir.close();
   }
-  
+
   // LUCENE-1072
   public void testExceptionFromTokenStream() throws IOException {
     Directory dir = newDirectory();
@@ -470,9 +470,9 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
         boolean sawAppend = false;
         boolean sawFlush = false;
         for (int i = 0; i < trace.length; i++) {
-          if ("org.apache.lucene.index.FreqProxTermsWriter".equals(trace[i].getClassName()) && "appendPostings".equals(trace[i].getMethodName()))
+          if ("org.apache.lucene.index.FreqProxTermsWriterPerField".equals(trace[i].getClassName()) && "flush".equals(trace[i].getMethodName()))
             sawAppend = true;
-          if ("doFlush".equals(trace[i].getMethodName()))
+          if ("flush".equals(trace[i].getMethodName()))
             sawFlush = true;
         }
 
@@ -680,7 +680,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
 
         for(int t=0;t<NUM_THREAD;t++)
           threads[t].join();
-            
+
         writer.close();
       }
 
@@ -727,7 +727,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       dir.close();
     }
   }
-  
+
   // Throws IOException during MockDirectoryWrapper.sync
   private static class FailOnlyInSync extends MockDirectoryWrapper.Failure {
     boolean didFail;
@@ -744,7 +744,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       }
     }
   }
-  
+
   // TODO: these are also in TestIndexWriter... add a simple doc-writing method
   // like this to LuceneTestCase?
   private void addDoc(IndexWriter writer) throws IOException
@@ -753,7 +753,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       doc.add(newField("content", "aaa", Field.Store.NO, Field.Index.ANALYZED));
       writer.addDocument(doc);
   }
-  
+
   // LUCENE-1044: test exception during sync
   public void testExceptionDuringSync() throws IOException {
     MockDirectoryWrapper dir = newDirectory();
@@ -790,7 +790,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     reader.close();
     dir.close();
   }
-  
+
   private static class FailOnlyInCommit extends MockDirectoryWrapper.Failure {
 
     boolean fail1, fail2;
@@ -818,7 +818,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       }
     }
   }
-  
+
   // LUCENE-1214
   public void testExceptionsDuringCommit() throws Throwable {
     MockDirectoryWrapper dir = newDirectory();
@@ -841,7 +841,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     w.rollback();
     dir.close();
   }
-  
+
   public void testOptimizeExceptions() throws IOException {
     Directory startDir = newDirectory();
     IndexWriterConfig conf = newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setMaxBufferedDocs(2);
@@ -873,7 +873,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     }
     startDir.close();
   }
-  
+
   // LUCENE-1429
   public void testOutOfMemoryErrorCausesCloseToFail() throws Exception {
 
@@ -902,7 +902,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     writer.close();
     dir.close();
   }
-  
+
   // LUCENE-1347
   private static final class MockIndexWriter4 extends IndexWriter {
 
@@ -919,7 +919,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       return true;
     }
   }
-  
+
   // LUCENE-1347
   public void testRollbackExceptionHang() throws Throwable {
     Directory dir = newDirectory();
@@ -933,12 +933,12 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     } catch (RuntimeException re) {
       // expected
     }
-    
+
     w.doFail = false;
     w.rollback();
     dir.close();
   }
-  
+
   // LUCENE-1044: Simulate checksum error in segments_N
   public void testSegmentsChecksumError() throws IOException {
     Directory dir = newDirectory();
@@ -977,7 +977,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
     reader.close();
     dir.close();
   }
-  
+
   // Simulate a corrupt index by removing last byte of
   // latest segments file and make sure we get an
   // IOException trying to open the index:
@@ -1024,7 +1024,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       }
       dir.close();
   }
-  
+
   // Simulate a corrupt index by removing one of the cfs
   // files and make sure we get an IOException trying to
   // open the index:
@@ -1073,7 +1073,7 @@ public class TestIndexWriterExceptions extends LuceneTestCase {
       }
       dir.close();
   }
-  
+
   // Simulate a writer that crashed while writing segments
   // file: make sure we can still open the index (ie,
   // gracefully fallback to the previous segments file),
diff --git a/lucene/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java b/lucene/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
index e793088..958204a 100644
--- a/lucene/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
+++ b/lucene/src/test/org/apache/lucene/index/TestPerSegmentDeletes.java
@@ -85,7 +85,7 @@ public class TestPerSegmentDeletes extends LuceneTestCase {
     // merge segments 0 and 1
     // which should apply the delete id:2
     writer.deleteDocuments(new Term("id", "2"));
-    writer.flush(false, false);
+    writer.flush(false, true);
     fsmp.doMerge = true;
     fsmp.start = 0;
     fsmp.length = 2;

