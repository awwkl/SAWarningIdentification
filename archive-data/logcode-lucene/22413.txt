GitDiffStart: cff5767e443411c78b199fbf9686d1ec672e38f0 | Wed Jul 4 15:16:38 2007 +0000
diff --git a/CHANGES.txt b/CHANGES.txt
index 0383f80..184ee27 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -105,6 +105,13 @@ API Changes
     to be public because it implements the public interface TermPositionVector.
     (Michael Busch)
 
+14. LUCENE-843: Added IndexWriter.setRAMBufferSizeMB(...) to have
+    IndexWriter flush whenever the buffered documents are using more
+    than the specified amount of RAM.  Also added new APIs to Token
+    that allow one to set a char[] plus offset and length to specify a
+    token (to avoid creating a new String() for each Token).  (Mike
+    McCandless)
+ 
 Bug fixes
 
  1. LUCENE-804: Fixed build.xml to pack a fully compilable src dist.  (Doron Cohen)
@@ -267,6 +274,12 @@ Optimizations
     lists. For average AND queries the speedup is about 20%, for queries that 
     contain very frequent and very unique terms the speedup can be over 80%.
     (Michael Busch)
+
+ 8. LUCENE-843: Substantial optimizations to improve how IndexWriter
+    uses RAM for buffering documents and to speed up indexing (2X-8X
+    faster).  A single shared hash table now records the in-memory
+    postings per unique term and is directly flushed into a single
+    segment.  (Mike McCandless)
  
 Documentation
 
diff --git a/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java b/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java
index 22662db..c54794b 100644
--- a/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java
+++ b/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java
@@ -77,6 +77,7 @@ public class TestParser extends TestCase {
 				line=d.readLine();
 			}			
 			d.close();
+                        writer.close();
 		}
 		reader=IndexReader.open(dir);
 		searcher=new IndexSearcher(reader);
diff --git a/docs/fileformats.html b/docs/fileformats.html
index 212d3a4..799924e 100644
--- a/docs/fileformats.html
+++ b/docs/fileformats.html
@@ -380,10 +380,18 @@ document.write("Last Published: " + document.lastModified);
                 But note that once a commit has occurred, pre-2.1
                 Lucene will not be able to read the index.
             </p>
+<p>
+                In version 2.3, the file format was changed to allow
+		segments to share a single set of doc store (vectors &amp;
+		stored fields) files.  This allows for faster indexing
+		in certain cases.  The change is fully backwards
+		compatible (in the same way as the lock-less commits
+		change in 2.1).
+            </p>
 </div>
 
         
-<a name="N10032"></a><a name="Definitions"></a>
+<a name="N10035"></a><a name="Definitions"></a>
 <h2 class="boxed">Definitions</h2>
 <div class="section">
 <p>
@@ -424,7 +432,7 @@ document.write("Last Published: " + document.lastModified);
                 strings, the first naming the field, and the second naming text
                 within the field.
             </p>
-<a name="N10052"></a><a name="Inverted Indexing"></a>
+<a name="N10055"></a><a name="Inverted Indexing"></a>
 <h3 class="boxed">Inverted Indexing</h3>
 <p>
                     The index stores statistics about terms in order
@@ -434,7 +442,7 @@ document.write("Last Published: " + document.lastModified);
                     it.  This is the inverse of the natural relationship, in which
                     documents list terms.
                 </p>
-<a name="N1005E"></a><a name="Types of Fields"></a>
+<a name="N10061"></a><a name="Types of Fields"></a>
 <h3 class="boxed">Types of Fields</h3>
 <p>
                     In Lucene, fields may be <i>stored</i>, in which
@@ -448,7 +456,7 @@ document.write("Last Published: " + document.lastModified);
                     to be indexed literally.
                 </p>
 <p>See the <a href="http://lucene.apache.org/java/docs/api/org/apache/lucene/document/Field.html">Field</a> java docs for more information on Fields.</p>
-<a name="N1007B"></a><a name="Segments"></a>
+<a name="N1007E"></a><a name="Segments"></a>
 <h3 class="boxed">Segments</h3>
 <p>
                     Lucene indexes may be composed of multiple sub-indexes, or
@@ -474,7 +482,7 @@ document.write("Last Published: " + document.lastModified);
                     Searches may involve multiple segments and/or multiple indexes, each
                     index potentially composed of a set of segments.
                 </p>
-<a name="N10099"></a><a name="Document Numbers"></a>
+<a name="N1009C"></a><a name="Document Numbers"></a>
 <h3 class="boxed">Document Numbers</h3>
 <p>
                     Internally, Lucene refers to documents by an integer <i>document
@@ -529,7 +537,7 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N100C0"></a><a name="Overview"></a>
+<a name="N100C3"></a><a name="Overview"></a>
 <h2 class="boxed">Overview</h2>
 <div class="section">
 <p>
@@ -626,7 +634,7 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N10103"></a><a name="File Naming"></a>
+<a name="N10106"></a><a name="File Naming"></a>
 <h2 class="boxed">File Naming</h2>
 <div class="section">
 <p>
@@ -654,10 +662,10 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N10112"></a><a name="Primitive Types"></a>
+<a name="N10115"></a><a name="Primitive Types"></a>
 <h2 class="boxed">Primitive Types</h2>
 <div class="section">
-<a name="N10117"></a><a name="Byte"></a>
+<a name="N1011A"></a><a name="Byte"></a>
 <h3 class="boxed">Byte</h3>
 <p>
                     The most primitive type
@@ -665,7 +673,7 @@ document.write("Last Published: " + document.lastModified);
                     other data types are defined as sequences
                     of bytes, so file formats are byte-order independent.
                 </p>
-<a name="N10120"></a><a name="UInt32"></a>
+<a name="N10123"></a><a name="UInt32"></a>
 <h3 class="boxed">UInt32</h3>
 <p>
                     32-bit unsigned integers are written as four
@@ -675,7 +683,7 @@ document.write("Last Published: " + document.lastModified);
                     UInt32    --&gt; &lt;Byte&gt;<sup>4</sup>
                 
 </p>
-<a name="N1012F"></a><a name="Uint64"></a>
+<a name="N10132"></a><a name="Uint64"></a>
 <h3 class="boxed">Uint64</h3>
 <p>
                     64-bit unsigned integers are written as eight
@@ -684,7 +692,7 @@ document.write("Last Published: " + document.lastModified);
 <p>UInt64    --&gt; &lt;Byte&gt;<sup>8</sup>
                 
 </p>
-<a name="N1013E"></a><a name="VInt"></a>
+<a name="N10141"></a><a name="VInt"></a>
 <h3 class="boxed">VInt</h3>
 <p>
                     A variable-length format for positive integers is
@@ -1234,7 +1242,7 @@ document.write("Last Published: " + document.lastModified);
                     This provides compression while still being
                     efficient to decode.
                 </p>
-<a name="N10423"></a><a name="Chars"></a>
+<a name="N10426"></a><a name="Chars"></a>
 <h3 class="boxed">Chars</h3>
 <p>
                     Lucene writes unicode
@@ -1243,7 +1251,7 @@ document.write("Last Published: " + document.lastModified);
                         UTF-8 encoding"</a>
                     .
                 </p>
-<a name="N10430"></a><a name="String"></a>
+<a name="N10433"></a><a name="String"></a>
 <h3 class="boxed">String</h3>
 <p>
                     Lucene writes strings as a VInt representing the length, followed by
@@ -1255,13 +1263,13 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N1043D"></a><a name="Per-Index Files"></a>
+<a name="N10440"></a><a name="Per-Index Files"></a>
 <h2 class="boxed">Per-Index Files</h2>
 <div class="section">
 <p>
                 The files in this section exist one-per-index.
             </p>
-<a name="N10445"></a><a name="Segments File"></a>
+<a name="N10448"></a><a name="Segments File"></a>
 <h3 class="boxed">Segments File</h3>
 <p>
                     The active segments in the index are stored in the
@@ -1316,16 +1324,24 @@ document.write("Last Published: " + document.lastModified);
                 
 </p>
 <p>
-                    Format, NameCounter, SegCount, SegSize, NumField --&gt; Int32
+                    
+<b>2.3 and above:</b>
+                    Segments --&gt; Format, Version, NameCounter, SegCount, &lt;SegName, SegSize, DelGen, DocStoreOffset, [DocStoreSegment, DocStoreIsCompoundFile], HasSingleNormFile, NumField,
+                    NormGen<sup>NumField</sup>,
+                    IsCompoundFile&gt;<sup>SegCount</sup>
+                
+</p>
+<p>
+                    Format, NameCounter, SegCount, SegSize, NumField, DocStoreOffset --&gt; Int32
                 </p>
 <p>
                     Version, DelGen, NormGen --&gt; Int64
                 </p>
 <p>
-                    SegName --&gt; String
+                    SegName, DocStoreSegment --&gt; String
                 </p>
 <p>
-                    IsCompoundFile, HasSingleNormFile --&gt; Int8
+                    IsCompoundFile, HasSingleNormFile, DocStoreIsCompoundFile --&gt; Int8
                 </p>
 <p>
                     Format is -1 as of Lucene 1.4 and -3 (SemgentInfos.FORMAT_SINGLE_NORM_FILE) as of Lucene 2.1.
@@ -1380,7 +1396,28 @@ document.write("Last Published: " + document.lastModified);
                     are stored as separate <tt>.fN</tt> files.  See
                     "Normalization Factors" below for details.
                 </p>
-<a name="N104A9"></a><a name="Lock File"></a>
+<p>
+		    DocStoreOffset, DocStoreSegment,
+                    DocStoreIsCompoundFile: If DocStoreOffset is -1,
+                    this segment has its own doc store (stored fields
+                    values and term vectors) files and DocStoreSegment
+                    and DocStoreIsCompoundFile are not stored.  In
+                    this case all files for stored field values
+                    (<tt>*.fdt</tt> and <tt>*.fdx</tt>) and term
+                    vectors (<tt>*.tvf</tt>, <tt>*.tvd</tt> and
+                    <tt>*.tvx</tt>) will be stored with this segment.
+                    Otherwise, DocStoreSegment is the name of the
+                    segment that has the shared doc store files;
+                    DocStoreIsCompoundFile is 1 if that segment is
+                    stored in compound file format (as a <tt>.cfx</tt>
+                    file); and DocStoreOffset is the starting document
+                    in the shared doc store files where this segment's
+                    documents begin.  In this case, this segment does
+                    not store its own doc store files but instead
+                    shares a single set of these files with other
+                    segments.
+                </p>
+<a name="N104CD"></a><a name="Lock File"></a>
 <h3 class="boxed">Lock File</h3>
 <p>
                     The write lock, which is stored in the index
@@ -1398,7 +1435,7 @@ document.write("Last Published: " + document.lastModified);
                     Note that prior to version 2.1, Lucene also used a
                     commit lock. This was removed in 2.1.
                 </p>
-<a name="N104B5"></a><a name="Deletable File"></a>
+<a name="N104D9"></a><a name="Deletable File"></a>
 <h3 class="boxed">Deletable File</h3>
 <p>
                     Prior to Lucene 2.1 there was a file "deletable"
@@ -1407,7 +1444,7 @@ document.write("Last Published: " + document.lastModified);
                     the files that are deletable, instead, so no file
                     is written.
                 </p>
-<a name="N104BE"></a><a name="Compound Files"></a>
+<a name="N104E2"></a><a name="Compound Files"></a>
 <h3 class="boxed">Compound Files</h3>
 <p>Starting with Lucene 1.4 the compound file format became default. This
                     is simply a container for all files described in the next section
@@ -1424,17 +1461,24 @@ document.write("Last Published: " + document.lastModified);
 <p>FileName --&gt; String</p>
 <p>FileData --&gt; raw file data</p>
 <p>The raw file data is the data from the individual files named above.</p>
+<p>Starting with Lucene 2.3, doc store files (stored
+		field values and term vectors) can be shared in a
+		single set of files for more than one segment.  When
+		compound file is enabled, these shared files will be
+		added into a single compound file (same format as
+		above) but with the extension <tt>.cfx</tt>.
+		</p>
 </div>
 
         
-<a name="N104E0"></a><a name="Per-Segment Files"></a>
+<a name="N1050A"></a><a name="Per-Segment Files"></a>
 <h2 class="boxed">Per-Segment Files</h2>
 <div class="section">
 <p>
                 The remaining files are all per-segment, and are
                 thus defined by suffix.
             </p>
-<a name="N104E8"></a><a name="Fields"></a>
+<a name="N10512"></a><a name="Fields"></a>
 <h3 class="boxed">Fields</h3>
 <p>
                     
@@ -1653,7 +1697,7 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N105A3"></a><a name="Term Dictionary"></a>
+<a name="N105CD"></a><a name="Term Dictionary"></a>
 <h3 class="boxed">Term Dictionary</h3>
 <p>
                     The term dictionary is represented as two files:
@@ -1839,7 +1883,7 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N10623"></a><a name="Frequencies"></a>
+<a name="N1064D"></a><a name="Frequencies"></a>
 <h3 class="boxed">Frequencies</h3>
 <p>
                     The .frq file contains the lists of documents
@@ -1957,7 +2001,7 @@ document.write("Last Published: " + document.lastModified);
                    entry in level-1. In the example has entry 15 on level 1 a pointer to entry 15 on level 0 and entry 31 on level 1 a pointer
                    to entry 31 on level 0.                   
                 </p>
-<a name="N106A5"></a><a name="Positions"></a>
+<a name="N106CF"></a><a name="Positions"></a>
 <h3 class="boxed">Positions</h3>
 <p>
                     The .prx file contains the lists of positions that
@@ -2023,7 +2067,7 @@ document.write("Last Published: " + document.lastModified);
                     Payload. If PayloadLength is not stored, then this Payload has the same
                     length as the Payload at the previous position.
                 </p>
-<a name="N106E1"></a><a name="Normalization Factors"></a>
+<a name="N1070B"></a><a name="Normalization Factors"></a>
 <h3 class="boxed">Normalization Factors</h3>
 <p>
                     
@@ -2127,7 +2171,7 @@ document.write("Last Published: " + document.lastModified);
 <b>2.1 and above:</b>
                     Separate norm files are created (when adequate) for both compound and non compound segments.
                 </p>
-<a name="N1074A"></a><a name="Term Vectors"></a>
+<a name="N10774"></a><a name="Term Vectors"></a>
 <h3 class="boxed">Term Vectors</h3>
 <ol>
                     
@@ -2253,7 +2297,7 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N107DD"></a><a name="Deleted Documents"></a>
+<a name="N10807"></a><a name="Deleted Documents"></a>
 <h3 class="boxed">Deleted Documents</h3>
 <p>The .del file is
                     optional, and only exists when a segment contains deletions.
@@ -2325,7 +2369,7 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N10820"></a><a name="Limitations"></a>
+<a name="N1084A"></a><a name="Limitations"></a>
 <h2 class="boxed">Limitations</h2>
 <div class="section">
 <p>There
diff --git a/docs/fileformats.pdf b/docs/fileformats.pdf
index 7661594..2e90a93 100644
--- a/docs/fileformats.pdf
+++ b/docs/fileformats.pdf
@@ -6,7 +6,7 @@ Table of contents
 
    1 Index File Formats............................................................................................................. 3
    2 Definitions..........................................................................................................................3
-    2.1 Inverted Indexing...........................................................................................................3
+    2.1 Inverted Indexing...........................................................................................................4
     2.2 Types of Fields.............................................................................................................. 4
     2.3 Segments........................................................................................................................4
     2.4 Document Numbers.......................................................................................................4
@@ -19,23 +19,23 @@ Table of contents
     5.4 VInt................................................................................................................................6
     5.5 Chars..............................................................................................................................7
     5.6 String............................................................................................................................. 7
-   6 Per-Index Files................................................................................................................... 7
+   6 Per-Index Files................................................................................................................... 8
     6.1 Segments File................................................................................................................ 8
     6.2 Lock File........................................................................................................................9
-    6.3 Deletable File.................................................................................................................9
-    6.4 Compound Files.............................................................................................................9
+    6.3 Deletable File...............................................................................................................10
+    6.4 Compound Files...........................................................................................................10
    7 Per-Segment Files............................................................................................................ 10
     7.1 Fields........................................................................................................................... 10
 
                    Copyright © 2006 The Apache Software Foundation. All rights reserved.
                                                                                                             Apache Lucene - Index File Formats
 
- 7.2 Term Dictionary.......................................................................................................... 11
- 7.3 Frequencies..................................................................................................................13
+ 7.2 Term Dictionary.......................................................................................................... 12
+ 7.3 Frequencies..................................................................................................................14
  7.4 Positions...................................................................................................................... 15
  7.5 Normalization Factors................................................................................................. 16
- 7.6 Term Vectors............................................................................................................... 16
- 7.7 Deleted Documents..................................................................................................... 18
+ 7.6 Term Vectors............................................................................................................... 17
+ 7.7 Deleted Documents..................................................................................................... 19
 8 Limitations....................................................................................................................... 19
 
                                                                        Page 2
@@ -68,6 +68,11 @@ or adding/deleting of docs. When the new segments file is saved (committed), it
 written in the new file format (meaning no specific "upgrade" process is needed). But note
 that once a commit has occurred, pre-2.1 Lucene will not be able to read the index.
 
+In version 2.3, the file format was changed to allow segments to share a single set of doc
+store (vectors & stored fields) files. This allows for faster indexing in certain cases. The
+change is fully backwards compatible (in the same way as the lock-less commits change in
+2.1).
+
 2. Definitions
 
 The fundamental concepts in Lucene are index, document, field and term.
@@ -79,17 +84,17 @@ An index contains a sequence of documents.
 
 The same string in two different fields is considered a different term. Thus terms are
 represented as a pair of strings, the first naming the field, and the second naming text within
-the field.
-
-2.1. Inverted Indexing
-
-The index stores statistics about terms in order to make term-based search more efficient.
 
 Page 3
 
         Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
+the field.
+
+2.1. Inverted Indexing
+
+The index stores statistics about terms in order to make term-based search more efficient.
 Lucene's index falls into the family of indexes known as an inverted index. This is because it
 can list, for a term, the documents that contain it. This is the inverse of the natural
 relationship, in which documents list terms.
@@ -127,19 +132,20 @@ numbers outside of Lucene. In particular, numbers may change in the following si
 ?? The numbers stored in each segment are unique only within the segment, and must be
 
     converted before they can be used in a larger context. The standard technique is to
-    allocate each segment a range of values, based on the range of numbers used in that
-    segment. To convert a document number from a segment to an external value, the
-    segment's base document number is added. To convert an external value back to a
-    segment-specific value, the segment is identified by the range that the external value is in,
 
                                                                        Page 4
 
 Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
+    allocate each segment a range of values, based on the range of numbers used in that
+    segment. To convert a document number from a segment to an external value, the
+    segment's base document number is added. To convert an external value back to a
+    segment-specific value, the segment is identified by the range that the external value is in,
     and the segment's base value is subtracted. For example two five document segments
     might be combined, so that the first segment has a base value of zero, and the second of
     five. Document three from the second segment would have an external value of eight.
+
 ?? When documents are deleted, gaps are created in the numbering. These are eventually
     removed as the index evolves through merging. Deleted documents are dropped when
     segments are merged. A freshly-merged segment thus has no gaps in its numbering.
@@ -147,6 +153,7 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
 3. Overview
 
 Each segment index maintains the following:
+
 ?? Field names. This contains the set of field names used in the index.
 ?? Stored Field values. This contains, for each document, a list of attribute-value pairs,
 
@@ -154,33 +161,38 @@ Each segment index maintains the following:
     the document, such as its title, url, or an identifier to access a database. The set of stored
     fields are what is returned for each hit when searching. This is keyed by document
     number.
+
 ?? Term dictionary. A dictionary containing all of the terms used in all of the indexed fields
     of all of the documents. The dictionary also contains the number of documents which
     contain the term, and pointers to the term's frequency and proximity data.
+
 ?? Term Frequency data. For each term in the dictionary, the numbers of all the documents
     that contain that term, and the frequency of the term in that document.
+
 ?? Term Proximity data. For each term in the dictionary, the positions that the term occurs in
     each document.
+
 ?? Normalization factors. For each field in each document, a value is stored that is
     multiplied into the score for hits on that field.
+
 ?? Term Vectors. For each field in each document, the term vector (sometimes called
     document vector) may be stored. A term vector consists of term text and term frequency.
     To add Term Vectors to your index see the Field constructors
+
 ?? Deleted documents. An optional file indicating which documents are deleted.
 
 Details on each of these are provided in subsequent sections.
 
 4. File Naming
 
-All files belonging to a segment have the same name with varying extensions. The extensions
-correspond to the different file formats described below. When using the Compound File
-format (default in 1.4 and greater) these files are collapsed into a single .cfs file (see below
-
 Page 5
 
         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-                                             Apache Lucene - Index File Formats
+Apache Lucene - Index File Formats
 
+All files belonging to a segment have the same name with varying extensions. The extensions
+correspond to the different file formats described below. When using the Compound File
+format (default in 1.4 and greater) these files are collapsed into a single .cfs file (see below
 for details)
 
 Typically, all segments in an index are stored in a single directory, although this is not
@@ -200,11 +212,15 @@ The most primitive type is an eight-bit byte. Files are accessed as sequences of
 other data types are defined as sequences of bytes, so file formats are byte-order independent.
 
 5.2. UInt32
+
 32-bit unsigned integers are written as four bytes, high-order bytes first.
+
 UInt32 --> <Byte>4
 
 5.3. Uint64
+
 64-bit unsigned integers are written as eight bytes, high-order bytes first.
+
 UInt64 --> <Byte>8
 
 5.4. VInt
@@ -217,14 +233,14 @@ on.
 
 VInt Encoding Example
 
-Value               First byte  Second byte  Third byte
+                                                                       Page 6
 
-0 00000000
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
 
-                                                                              Page 6
+Value   First byte                  Second byte                   Third byte
 
-       Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
+0 00000000
 
 1 00000001
 
@@ -234,19 +250,19 @@ Value               First byte  Second byte  Third byte
 
 127 01111111
 
-128     10000000                                00000001
+128     10000000                    00000001
 
-129     10000001                                00000001
+129     10000001                    00000001
 
-130     10000010                                00000001
+130     10000010                    00000001
 
 ...
 
-16,383  11111111                                01111111
+16,383  11111111                    01111111
 
-16,384  10000000                                10000000          00000001
+16,384  10000000                    10000000                      00000001
 
-16,385  10000001                                10000000          00000001
+16,385  10000001                    10000000                      00000001
 
  ...
 
@@ -259,15 +275,15 @@ Lucene writes unicode character sequences using Java's "modified UTF-8 encoding"
 Lucene writes strings as a VInt representing the length, followed by the character data.
 String --> VInt, Chars
 
-6. Per-Index Files
-
-The files in this section exist one-per-index.
-
 Page 7
 
         Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
+6. Per-Index Files
+
+The files in this section exist one-per-index.
+
 6.1. Segments File
 
 The active segments in the index are stored in the segment info file, segments_N. There may
@@ -291,19 +307,28 @@ SegCount
 SegSize, DelGen, HasSingleNormFile, NumField, NormGenNumField,
 IsCompoundFile>SegCount
 
-Format, NameCounter, SegCount, SegSize, NumField --> Int32
+2.3 and above: Segments --> Format, Version, NameCounter, SegCount, <SegName,
+SegSize, DelGen, DocStoreOffset, [DocStoreSegment, DocStoreIsCompoundFile],
+HasSingleNormFile, NumField, NormGenNumField, IsCompoundFile>SegCount
+
+Format, NameCounter, SegCount, SegSize, NumField, DocStoreOffset --> Int32
 
 Version, DelGen, NormGen --> Int64
 
-SegName --> String
+SegName, DocStoreSegment --> String
 
-IsCompoundFile, HasSingleNormFile --> Int8
+IsCompoundFile, HasSingleNormFile, DocStoreIsCompoundFile --> Int8
 
 Format is -1 as of Lucene 1.4 and -3 (SemgentInfos.FORMAT_SINGLE_NORM_FILE) as
 of Lucene 2.1.
 
 Version counts how often the index has been changed by adding or deleting documents.
 
+                                                                       Page 8
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
 NameCounter is used to generate names for new segment files.
 
 SegName is the name of the segment, and is used as the file name prefix for all of the files
@@ -313,12 +338,6 @@ SegSize is the number of documents contained in the segment index.
 
 DelGen is the generation count of the separate deletes file. If this is -1, there are no separate
 deletes. If it is 0, this is a pre-2.1 segment and you must check filesystem for the existence of
-
-                                                                       Page 8
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 _X.del. Anything above zero means there are separate deletes (_X_N.del).
 
 NumField is the size of the array for NormGen, or -1 if there are no NormGens stored.
@@ -336,6 +355,16 @@ If HasSingleNormFile is 1, then the field norms are written as a single joined f
 extension .nrm); if it is 0 then each field's norms are stored as separate .fN files. See
 "Normalization Factors" below for details.
 
+DocStoreOffset, DocStoreSegment, DocStoreIsCompoundFile: If DocStoreOffset is -1, this
+segment has its own doc store (stored fields values and term vectors) files and
+DocStoreSegment and DocStoreIsCompoundFile are not stored. In this case all files for
+stored field values (*.fdt and *.fdx) and term vectors (*.tvf, *.tvd and *.tvx) will be stored
+with this segment. Otherwise, DocStoreSegment is the name of the segment that has the
+shared doc store files; DocStoreIsCompoundFile is 1 if that segment is stored in compound
+file format (as a .cfx file); and DocStoreOffset is the starting document in the shared doc
+store files where this segment's documents begin. In this case, this segment does not store its
+own doc store files but instead shares a single set of these files with other segments.
+
 6.2. Lock File
 
 The write lock, which is stored in the index directory by default, is named "write.lock". If the
@@ -343,35 +372,33 @@ lock directory is different from the index directory then the write lock will be
 "XXXX-write.lock" where XXXX is a unique prefix derived from the full path to the index
 directory. When this file is present, a writer is currently modifying the index (adding or
 removing documents). This lock file ensures that only one writer is modifying the index at a
-time.
 
+Page 9
+
+        Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
+time.
 Note that prior to version 2.1, Lucene also used a commit lock. This was removed in 2.1.
 
 6.3. Deletable File
-
 Prior to Lucene 2.1 there was a file "deletable" that contained details about files that need to
 be deleted. As of 2.1, a writer dynamically computes the files that are deletable, instead, so
 no file is written.
 
 6.4. Compound Files
-
 Starting with Lucene 1.4 the compound file format became default. This is simply a container
 for all files described in the next section (except for the .del file).
-
 Compound (.cfs) --> FileCount, <DataOffset, FileName> FileCount , FileData FileCount
-
 FileCount --> VInt
-
 DataOffset --> Long
-
-Page 9
-
-        Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 FileName --> String
 FileData --> raw file data
 The raw file data is the data from the individual files named above.
+Starting with Lucene 2.3, doc store files (stored field values and term vectors) can be shared
+in a single set of files for more than one segment. When compound file is enabled, these
+shared files will be added into a single compound file (same format as above) but with the
+extension .cfx.
 
 7. Per-Segment Files
 
@@ -382,6 +409,12 @@ Field Info
 Field names are stored in the field info file, with suffix .fnm.
 FieldInfos (.fnm) --> FieldsCount, <FieldName, FieldBits> FieldsCount
 FieldsCount --> VInt
+
+                                                                       Page 10
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
 FieldName --> String
 FieldBits --> Byte
 
@@ -400,12 +433,6 @@ Stored fields are represented by two files:
     accessed. The position of document n 's field data is the Uint64 at n*8 in this file.
 2. The field data, or .fdt file.
     This contains the stored fields of each document, as follows:
-
-                                                                       Page 10
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
     FieldData (.fdt) --> <DocFieldData> SegSize
     DocFieldData --> FieldCount, <FieldNum, Bits, Value> FieldCount
     FieldCount --> VInt
@@ -417,6 +444,12 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
     non-tokenized fields.
     Lucene >= 1.9:
     Bits --> Byte
+
+Page 11
+
+         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
     ?? low order bit is one for tokenized fields
     ?? second bit is one for fields containing binary data
     ?? third bit is one for fields with compression option enabled (if compression is enabled,
@@ -436,33 +469,24 @@ The term dictionary is represented as two files:
     TIVersion --> UInt32
     TermCount --> UInt64
     IndexInterval --> UInt32
-
-Page 11
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
     SkipInterval --> UInt32
-
     MaxSkipLevels --> UInt32
-
     TermInfos --> <TermInfo> TermCount
-
     TermInfo --> <Term, DocFreq, FreqDelta, ProxDelta, SkipDelta>
-
     Term --> <PrefixLength, Suffix, FieldNum>
-
     Suffix --> String
-
     PrefixLength, DocFreq, FreqDelta, ProxDelta, SkipDelta
     --> VInt
-
     This file is sorted by Term. Terms are ordered first lexicographically by the term's field
     name, and within that lexicographically by the term's text.
-
     TIVersion names the version of the format of this file and is -2 in Lucene 1.4.
-
     Term text prefixes are shared. The PrefixLength is the number of initial characters from
+
+                                                                       Page 12
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
     the previous term which must be pre-pended to a term's suffix in order to form the term's
     text. Thus, if the previous term's text was "bone" and the term is "boy", the PrefixLength
     is two and the suffix is "y".
@@ -489,11 +513,6 @@ Page 11
     "tis" file. This is designed to be read entirely into memory and used to provide random
     access to the "tis" file.
 
-                                                                       Page 12
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
     The structure of this file is very similar to the .tis file, with the addition of one item per
     record, the IndexDelta.
 
@@ -514,6 +533,12 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
 
     IndexDelta determines the position of this term's TermInfo within the .tis file. In
     particular, it is the difference between the position of this term's entry in that file and the
+
+Page 13
+
+         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
     position of the previous term's entry.
 
     SkipInterval is the fraction of TermDocs stored in skip tables. It is used to accelerate
@@ -541,11 +566,6 @@ SkipData --> <<SkipLevelLength, SkipLevel> NumSkipLevels-1, SkipLevel> <SkipDatu
 
 SkipLevel --> <SkipDatum> DocFreq/(SkipInterval^(Level + 1))
 
-Page 13
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 SkipDatum --> DocSkip,PayloadLength?,FreqSkip,ProxSkip,SkipChildLevelPointer?
 
 DocDelta,Freq,DocSkip,PayloadLength,FreqSkip,ProxSkip --> VInt
@@ -566,6 +586,11 @@ in document eleven would be the following sequence of VInts:
 
 15, 8, 3
 
+                                                                       Page 14
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
 DocSkip records the document number before every SkipInterval th document in TermFreqs.
 If payloads are disabled for the term's field, then DocSkip represents the difference from the
 previous value in the sequence. If payloads are enabled for the term's field, then DocSkip/2
@@ -590,12 +615,6 @@ DocFreq/(SkipInterval^(Level + 1)), whereas the lowest skip level is Level=0.
 Example: SkipInterval = 4, MaxSkipLevels = 2, DocFreq = 35. Then skip level 0 has 8
 SkipData entries, containing the 3rd, 7th, 11th, 15th, 19th, 23rd, 27th, and 31st document
 numbers in TermFreqs. Skip level 1 has 2 SkipData entries, containing the 15th and 31st
-
-                                                                       Page 14
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 document numbers in TermFreqs.
 The SkipData entries on all upper levels > 0 contain a SkipChildLevelPointer referencing the
 corresponding SkipData entry in level-1. In the example has entry 15 on level 1 a pointer to
@@ -615,6 +634,11 @@ Payload --> <PayloadLength?,PayloadData>
 
 PositionDelta --> VInt
 
+Page 15
+
+         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
 PayloadLength --> VInt
 
 PayloadData --> bytePayloadLength
@@ -641,11 +665,6 @@ PayloadData is metadata associated with the current term position. If PayloadLen
 at the current position, then it indicates the length of this Payload. If PayloadLength is not
 stored, then this Payload has the same length as the Payload at the previous position.
 
-Page 15
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 7.5. Normalization Factors
 
 Pre-2.1: There's a norm file for each indexed field with a byte for each document. The
@@ -666,6 +685,11 @@ Version --> Byte
 
 NormsHeader has 4 bytes, last of which is the format version for this file, currently -1.
 
+                                                                       Page 16
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
 Each byte encodes a floating point value. Bits 0-2 contain the 3-bit mantissa, and bits 3-8
 contain the 5-bit exponent.
 
@@ -689,11 +713,6 @@ compound segments.
 Term Vector support is an optional on a field by field basis. It consists of 4 files.
 1. The Document Index or .tvx file.
 
-                                                                       Page 16
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
     This contains, for each document, a pointer to the document data in the Document (.tvd)
     file.
 
@@ -714,6 +733,11 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
 
     TVDVersion --> Int
 
+Page 17
+
+         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
     NumFields --> VInt
 
     FieldNums --> <FieldNumDelta> NumFields
@@ -727,6 +751,7 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
     The .tvd file is used to map out the fields that have term vectors stored and where the
     field information is in the .tvf file.
 3. The Field or .tvf file.
+
     This file contains, for each field that has a term vector stored, a list of the terms, their
     frequencies and, optionally, position and offest information.
 
@@ -740,11 +765,6 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
 
     TermFreqs --> <TermText, TermFreq, Positions?, Offsets?> NumTerms
 
-Page 17
-
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
     TermText --> <PrefixLength, Suffix>
 
     PrefixLength --> VInt
@@ -767,8 +787,15 @@ Page 17
          the term's text. Thus, if the previous term's text was "bone" and the term is "boy", the
          PrefixLength is two and the suffix is "y".
     ?? Positions are stored as delta encoded VInts. This means we only store the difference
+
+                                                                       Page 18
+
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Apache Lucene - Index File Formats
+
          of the current position from the last position
     ?? Offsets are stored as delta encoded VInts. The first VInt is the startOffset, the second
+
          is the endOffset.
 
 7.7. Deleted Documents
@@ -793,33 +820,38 @@ DGap --> VInt
 NonzeroByte --> Byte
 
 Format is Optional. -1 indicates DGaps. Non-negative value indicates Bits, and that Format is
-
-                                                                       Page 18
-
-Copyright © 2006 The Apache Software Foundation. All rights reserved.
-Apache Lucene - Index File Formats
-
 excluded.
+
 ByteCount indicates the number of bytes in Bits. It is typically (SegSize/8)+1.
+
 BitCount indicates the number of bits that are currently set in Bits.
+
 Bits contains one bit for each document indexed. When the bit corresponding to a document
 number is set, that document is marked as deleted. Bit ordering is from least to most
 significant. Thus, if Bits contains two bytes, 0x00 and 0x02, then document 9 is marked as
 deleted.
+
 DGaps represents sparse bit-vectors more efficiently than Bits. It is made of DGaps on
 indexes of nonzero bytes in Bits, and the nonzero bytes themselves. The number of nonzero
 bytes in Bits (NonzeroBytesCount) is not stored.
+
 For example, if there are 8000 bits and only bits 10,12,32 are set, DGaps would be used:
+
 (VInt) 1 , (byte) 20 , (VInt) 3 , (Byte) 1
 
 8. Limitations
 
+Page 19
+
+         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+                                                                                                                Apache Lucene - Index File Formats
+
 There are a few places where these file formats limit the maximum number of terms and
 documents to a 32-bit quantity, or to approximately 4 billion. This is not today a problem,
 but, in the long term, probably will be. These should therefore be replaced with either UInt64
 values, or better yet, with VInt values which have no limit.
 
-Page 19
+                                                                       Page 20
 
-         Copyright © 2006 The Apache Software Foundation. All rights reserved.
+Copyright © 2006 The Apache Software Foundation. All rights reserved.
 
\ No newline at end of file
diff --git a/src/java/org/apache/lucene/analysis/Token.java b/src/java/org/apache/lucene/analysis/Token.java
index acb7a8b..c32f8e2 100644
--- a/src/java/org/apache/lucene/analysis/Token.java
+++ b/src/java/org/apache/lucene/analysis/Token.java
@@ -55,6 +55,13 @@ public class Token implements Cloneable {
   
   Payload payload;
   
+  // For better indexing speed, use termBuffer (and
+  // termBufferOffset/termBufferLength) instead of termText
+  // to save new'ing a String per token
+  char[] termBuffer;
+  int termBufferOffset;
+  int termBufferLength;
+
   private int positionIncrement = 1;
 
   /** Constructs a Token with the given term text, and start & end offsets.
@@ -65,6 +72,17 @@ public class Token implements Cloneable {
     endOffset = end;
   }
 
+  /** Constructs a Token with the given term text buffer
+   *  starting at offset for length lenth, and start & end offsets.
+   *  The type defaults to "word." */
+  public Token(char[] text, int offset, int length, int start, int end) {
+    termBuffer = text;
+    termBufferOffset = offset;
+    termBufferLength = length;
+    startOffset = start;
+    endOffset = end;
+  }
+
   /** Constructs a Token with the given text, start and end offsets, & type. */
   public Token(String text, int start, int end, String typ) {
     termText = text;
@@ -73,6 +91,19 @@ public class Token implements Cloneable {
     type = typ;
   }
 
+  /** Constructs a Token with the given term text buffer
+   *  starting at offset for length lenth, and start & end
+   *  offsets, & type. */
+  public Token(char[] text, int offset, int length, int start, int end, String typ) {
+    termBuffer = text;
+    termBufferOffset = offset;
+    termBufferLength = length;
+    startOffset = start;
+    endOffset = end;
+    type = typ;
+  }
+
+
   /** Set the position increment.  This determines the position of this token
    * relative to the previous Token in a {@link TokenStream}, used in phrase
    * searching.
@@ -117,6 +148,19 @@ public class Token implements Cloneable {
 
   /** Returns the Token's term text. */
   public final String termText() { return termText; }
+  public final char[] termBuffer() { return termBuffer; }
+  public final int termBufferOffset() { return termBufferOffset; }
+  public final int termBufferLength() { return termBufferLength; }
+
+  public void setStartOffset(int offset) {this.startOffset = offset;}
+  public void setEndOffset(int offset) {this.endOffset = offset;}
+
+  public final void setTermBuffer(char[] buffer, int offset, int length) {
+    this.termBuffer = buffer;
+    this.termBufferOffset = offset;
+    this.termBufferLength = length;
+  }
+    
 
   /** Returns this Token's starting offset, the position of the first character
     corresponding to this token in the source text.
diff --git a/src/java/org/apache/lucene/index/DocumentsWriter.java b/src/java/org/apache/lucene/index/DocumentsWriter.java
new file mode 100644
index 0000000..a868905
--- /dev/null
+++ b/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -0,0 +1,2847 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.search.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.RAMOutputStream;
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.io.Reader;
+import java.util.Arrays;
+import java.util.List;
+import java.util.HashMap;
+import java.util.ArrayList;
+import java.text.NumberFormat;
+import java.util.Collections;
+
+/**
+ * This class accepts multiple added documents and directly
+ * writes a single segment file.  It does this more
+ * efficiently than creating a single segment per document
+ * (with DocumentWriter) and doing standard merges on those
+ * segments.
+ *
+ * When a document is added, its stored fields (if any) and
+ * term vectors (if any) are immediately written to the
+ * Directory (ie these do not consume RAM).  The freq/prox
+ * postings are accumulated into a Postings hash table keyed
+ * by term.  Each entry in this hash table holds a separate
+ * byte stream (allocated as incrementally growing slices
+ * into large shared byte[] arrays) for freq and prox, that
+ * contains the postings data for multiple documents.  If
+ * vectors are enabled, each unique term for each document
+ * also allocates a PostingVector instance to similarly
+ * track the offsets & positions byte stream.
+ *
+ * Once the Postings hash is full (ie is consuming the
+ * allowed RAM) or the number of added docs is large enough
+ * (in the case we are flushing by doc count instead of RAM
+ * usage), we create a real segment and flush it to disk and
+ * reset the Postings hash.
+ *
+ * In adding a document we first organize all of its fields
+ * by field name.  We then process field by field, and
+ * record the Posting hash per-field.  After each field we
+ * flush its term vectors.  When it's time to flush the full
+ * segment we first sort the fields by name, and then go
+ * field by field and sorts its postings.
+ *
+ *
+ * Threads:
+ *
+ * Multiple threads are allowed into addDocument at once.
+ * There is an initial synchronized call to getThreadState
+ * which allocates a ThreadState for this thread.  The same
+ * thread will get the same ThreadState over time (thread
+ * affinity) so that if there are consistent patterns (for
+ * example each thread is indexing a different content
+ * source) then we make better use of RAM.  Then
+ * processDocument is called on that ThreadState without
+ * synchronization (most of the "heavy lifting" is in this
+ * call).  Finally the synchronized "finishDocument" is
+ * called to flush changes to the directory.
+ *
+ * Each ThreadState instance has its own Posting hash. Once
+ * we're using too much RAM, we flush all Posting hashes to
+ * a segment by merging the docIDs in the posting lists for
+ * the same term across multiple thread states (see
+ * writeSegment and appendPostings).
+ *
+ * When flush is called by IndexWriter, or, we flush
+ * internally when autoCommit=false, we forcefully idle all
+ * threads and flush only once they are all idle.  This
+ * means you can call flush with a given thread even while
+ * other threads are actively adding/deleting documents.
+ */
+
+final class DocumentsWriter {
+
+  private IndexWriter writer;
+  private Directory directory;
+
+  private FieldInfos fieldInfos = new FieldInfos(); // All fields we've seen
+  private IndexOutput tvx, tvf, tvd;              // To write term vectors
+  private FieldsWriter fieldsWriter;              // To write stored fields
+
+  private String segment;                         // Current segment we are working on
+  private String docStoreSegment;                 // Current doc-store segment we are writing
+  private int docStoreOffset;                     // Current starting doc-store offset of current segment
+
+  private int nextDocID;                          // Next docID to be added
+  private int numDocsInRAM;                       // # docs buffered in RAM
+  private int nextWriteDocID;                     // Next docID to be written
+
+  // Max # ThreadState instances; if there are more threads
+  // than this they share ThreadStates
+  private final static int MAX_THREAD_STATE = 5;
+  private ThreadState[] threadStates = new ThreadState[0];
+  private final HashMap threadBindings = new HashMap();
+  private int numWaiting;
+  private ThreadState[] waitingThreadStates = new ThreadState[1];
+  private int pauseThreads;                       // Non-zero when we need all threads to
+                                                  // pause (eg to flush)
+  private boolean flushPending;                   // True when a thread has decided to flush
+  private boolean postingsIsFull;                 // True when it's time to write segment
+
+  private PrintStream infoStream;
+
+  // How much RAM we can use before flushing.  This is 0 if
+  // we are flushing by doc count instead.
+  private long ramBufferSize = (long) (IndexWriter.DEFAULT_RAM_BUFFER_SIZE_MB*1024*1024);
+
+  // Flush @ this number of docs.  If rarmBufferSize is
+  // non-zero we will flush by RAM usage instead.
+  private int maxBufferedDocs = IndexWriter.DEFAULT_MAX_BUFFERED_DOCS;
+
+  private BufferedNorms[] norms = new BufferedNorms[0];   // Holds norms until we flush
+
+  DocumentsWriter(Directory directory, IndexWriter writer) throws IOException {
+    this.directory = directory;
+    this.writer = writer;
+
+    postingsFreeList = new Posting[0];
+  }
+
+  /** If non-null, various details of indexing are printed
+   *  here. */
+  void setInfoStream(PrintStream infoStream) {
+    this.infoStream = infoStream;
+  }
+
+  /** Set how much RAM we can use before flushing. */
+  void setRAMBufferSizeMB(double mb) {
+    ramBufferSize = (long) (mb*1024*1024);
+  }
+
+  double getRAMBufferSizeMB() {
+    return ramBufferSize/1024./1024.;
+  }
+
+  /** Set max buffered docs, which means we will flush by
+   *  doc count instead of by RAM usage. */
+  void setMaxBufferedDocs(int count) {
+    maxBufferedDocs = count;
+    ramBufferSize = 0;
+  }
+
+  int getMaxBufferedDocs() {
+    return maxBufferedDocs;
+  }
+
+  /** Get current segment name we are writing. */
+  String getSegment() {
+    return segment;
+  }
+
+  /** Returns how many docs are currently buffered in RAM. */
+  int getNumDocsInRAM() {
+    return numDocsInRAM;
+  }
+
+  /** Returns the current doc store segment we are writing
+   *  to.  This will be the same as segment when autoCommit
+   *  * is true. */
+  String getDocStoreSegment() {
+    return docStoreSegment;
+  }
+
+  /** Returns the doc offset into the shared doc store for
+   *  the current buffered docs. */
+  int getDocStoreOffset() {
+    return docStoreOffset;
+  }
+
+  /** Closes the current open doc stores an returns the doc
+   *  store segment name.  This returns null if there are *
+   *  no buffered documents. */
+  String closeDocStore() throws IOException {
+
+    assert allThreadsIdle();
+
+    List flushedFiles = files();
+
+    if (infoStream != null)
+      infoStream.println("\ncloseDocStore: " + flushedFiles.size() + " files to flush to segment " + docStoreSegment);
+
+    if (flushedFiles.size() > 0) {
+      files = null;
+
+      if (tvx != null) {
+        // At least one doc in this run had term vectors enabled
+        assert docStoreSegment != null;
+        tvx.close();
+        tvf.close();
+        tvd.close();
+        tvx = null;
+      }
+
+      if (fieldsWriter != null) {
+        assert docStoreSegment != null;
+        fieldsWriter.close();
+        fieldsWriter = null;
+      }
+
+      String s = docStoreSegment;
+      docStoreSegment = null;
+      docStoreOffset = 0;
+      return s;
+    } else {
+      return null;
+    }
+  }
+
+  private List files = null;                      // Cached list of files we've created
+
+  /* Returns list of files in use by this instance,
+   * including any flushed segments. */
+  List files() {
+
+    if (files != null)
+      return files;
+
+    files = new ArrayList();
+
+    // Stored fields:
+    if (fieldsWriter != null) {
+      assert docStoreSegment != null;
+      files.add(docStoreSegment + "." + IndexFileNames.FIELDS_EXTENSION);
+      files.add(docStoreSegment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION);
+    }
+
+    // Vectors:
+    if (tvx != null) {
+      assert docStoreSegment != null;
+      files.add(docStoreSegment + "." + IndexFileNames.VECTORS_INDEX_EXTENSION);
+      files.add(docStoreSegment + "." + IndexFileNames.VECTORS_FIELDS_EXTENSION);
+      files.add(docStoreSegment + "." + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);
+    }
+
+    return files;
+  }
+
+  /** Called if we hit an exception when adding docs,
+   *  flushing, etc.  This resets our state, discarding any
+   *  * docs added since last flush. */
+  void abort() throws IOException {
+
+    // Forcefully remove waiting ThreadStates from line
+    for(int i=0;i<numWaiting;i++)
+      waitingThreadStates[i].isIdle = true;
+    numWaiting = 0;
+
+    pauseAllThreads();
+
+    try {
+
+      // Discard pending norms:
+      final int numField = fieldInfos.size();
+      for (int i=0;i<numField;i++) {
+        FieldInfo fi = fieldInfos.fieldInfo(i);
+        if (fi.isIndexed && !fi.omitNorms) {
+          BufferedNorms n = norms[i];
+          if (n != null) {
+            n.out.reset();
+            n.reset();
+          }
+        }
+      }
+
+      // Reset vectors writer
+      if (tvx != null) {
+        tvx.close();
+        tvf.close();
+        tvd.close();
+        tvx = null;
+      }
+
+      // Reset fields writer
+      if (fieldsWriter != null) {
+        fieldsWriter.close();
+        fieldsWriter = null;
+      }
+
+      // Reset all postings data
+      resetPostingsData();
+
+      // Clear vectors & fields from ThreadStates
+      for(int i=0;i<threadStates.length;i++) {
+        ThreadState state = threadStates[i];
+        if (state.localFieldsWriter != null) {
+          state.localFieldsWriter.close();
+          state.localFieldsWriter = null;
+        }
+        state.tvfLocal.reset();
+        state.fdtLocal.reset();
+      }
+
+      files = null;
+    } finally {
+      resumeAllThreads();
+    }
+  }
+
+  /** Reset after a flush */
+  private void resetPostingsData() throws IOException {
+    // All ThreadStates should be idle when we are called
+    assert allThreadsIdle();
+    for(int i=0;i<threadStates.length;i++) {
+      threadStates[i].resetPostings();
+      threadStates[i].numThreads = 0;
+    }
+    threadBindings.clear();
+    numBytesUsed = 0;
+    balanceRAM();
+    postingsIsFull = false;
+    flushPending = false;
+    segment = null;
+    hasNorms = false;
+    numDocsInRAM = 0;
+    nextDocID = 0;
+    nextWriteDocID = 0;
+    files = null;
+  }
+
+  synchronized void pauseAllThreads() {
+    pauseThreads++;
+    if (1 == pauseThreads) {
+      while(!allThreadsIdle()) {
+        try {
+          wait();
+        } catch (InterruptedException e) {
+        }
+      }
+    }
+  }
+
+  synchronized void resumeAllThreads() {
+    pauseThreads--;
+    assert pauseThreads >= 0;
+    if (0 == pauseThreads)
+      notifyAll();
+  }
+
+  private boolean allThreadsIdle() {
+    for(int i=0;i<threadStates.length;i++)
+      if (!threadStates[i].isIdle)
+        return false;
+    return true;
+  }
+
+  private boolean hasNorms;                       // Whether any norms were seen since last flush
+
+  List newFiles;
+
+  /** Flush all pending docs to a new segment */
+  int flush(boolean closeDocStore) throws IOException {
+
+    assert allThreadsIdle();
+
+    if (segment == null)
+      // In case we are asked to flush an empty segment
+      segment = writer.newSegmentName();
+
+    newFiles = new ArrayList();
+
+    docStoreOffset += numDocsInRAM;
+
+    if (closeDocStore) {
+      assert docStoreSegment != null;
+      assert docStoreSegment.equals(segment);
+      newFiles.addAll(files());
+      closeDocStore();
+    }
+    
+    int docCount;
+
+    assert numDocsInRAM > 0;
+
+    if (infoStream != null)
+      infoStream.println("\nflush postings as segment " + segment + " numDocs=" + numDocsInRAM);
+    
+    boolean success = false;
+
+    try {
+
+      fieldInfos.write(directory, segment + ".fnm");
+
+      docCount = numDocsInRAM;
+
+      newFiles.addAll(writeSegment());
+
+      success = true;
+
+    } finally {
+      if (!success)
+        abort();
+    }
+
+    return docCount;
+  }
+
+  /** Build compound file for the segment we just flushed */
+  void createCompoundFile(String segment) throws IOException
+  {
+    CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, segment + "." + IndexFileNames.COMPOUND_FILE_EXTENSION);
+    final int size = newFiles.size();
+    for(int i=0;i<size;i++)
+      cfsWriter.addFile((String) newFiles.get(i));
+      
+    // Perform the merge
+    cfsWriter.close();
+  }
+
+  /** Set flushPending if it is not already set and returns
+   *  whether it was set. This is used by IndexWriter to *
+   *  trigger a single flush even when multiple threads are
+   *  * trying to do so. */
+  synchronized boolean setFlushPending() {
+    if (flushPending)
+      return false;
+    else {
+      flushPending = true;
+      return true;
+    }
+  }
+
+  synchronized void clearFlushPending() {
+    flushPending = false;
+  }
+
+  /** Per-thread state.  We keep a separate Posting hash and
+   *  other state for each thread and then merge postings *
+   *  hashes from all threads when writing the segment. */
+  private final class ThreadState {
+
+    Posting[] postingsFreeList;           // Free Posting instances
+    int postingsFreeCount;
+
+    RAMOutputStream tvfLocal = new RAMOutputStream();    // Term vectors for one doc
+    RAMOutputStream fdtLocal = new RAMOutputStream();    // Stored fields for one doc
+    FieldsWriter localFieldsWriter;       // Fields for one doc
+
+    long[] vectorFieldPointers;
+    int[] vectorFieldNumbers;
+
+    boolean isIdle = true;                // Whether we are in use
+    int numThreads = 1;                   // Number of threads that use this instance
+
+    int docID;                            // docID we are now working on
+    int numStoredFields;                  // How many stored fields in current doc
+    float docBoost;                       // Boost for current doc
+
+    FieldData[] fieldDataArray;           // Fields touched by current doc
+    int numFieldData;                     // How many fields in current doc
+    int numVectorFields;                  // How many vector fields in current doc
+
+    FieldData[] allFieldDataArray = new FieldData[10]; // All FieldData instances
+    int numAllFieldData;
+    FieldData[] fieldDataHash;            // Hash FieldData instances by field name
+    int fieldDataHashMask;
+
+    boolean doFlushAfter;
+
+    public ThreadState() {
+      fieldDataArray = new FieldData[8];
+
+      fieldDataHash = new FieldData[16];
+      fieldDataHashMask = 15;
+
+      vectorFieldPointers = new long[10];
+      vectorFieldNumbers = new int[10];
+      postingsFreeList = new Posting[256];
+      postingsFreeCount = 0;
+    }
+
+    /** Clear the postings hash and return objects back to
+     *  shared pool */
+    public void resetPostings() throws IOException {
+      if (localFieldsWriter != null) {
+        localFieldsWriter.close();
+        localFieldsWriter = null;
+      }
+      maxPostingsVectors = 0;
+      doFlushAfter = false;
+      postingsPool.reset();
+      charPool.reset();
+      recyclePostings(postingsFreeList, postingsFreeCount);
+      postingsFreeCount = 0;
+      for(int i=0;i<numAllFieldData;i++) {
+        FieldData fp = allFieldDataArray[i];
+        if (fp.numPostings > 0)
+          fp.resetPostingArrays();
+      }
+    }
+
+    /** Move all per-document state that was accumulated in
+     *  the ThreadState into the "real" stores. */
+    public void writeDocument() throws IOException {
+
+      // Append stored fields to the real FieldsWriter:
+      fieldsWriter.flushDocument(fdtLocal);
+      fdtLocal.reset();
+
+      // Append term vectors to the real outputs:
+      if (tvx != null) {
+        tvx.writeLong(tvd.getFilePointer());
+        if (numVectorFields > 0) {
+          tvd.writeVInt(numVectorFields);
+          for(int i=0;i<numVectorFields;i++)
+            tvd.writeVInt(vectorFieldNumbers[i]);
+          assert 0 == vectorFieldPointers[0];
+          tvd.writeVLong(tvf.getFilePointer());
+          long lastPos = vectorFieldPointers[0];
+          for(int i=1;i<numVectorFields;i++) {
+            long pos = vectorFieldPointers[i];
+            tvd.writeVLong(pos-lastPos);
+            lastPos = pos;
+          }
+          tvfLocal.writeTo(tvf);
+          tvfLocal.reset();
+        }
+      }
+
+      // Append norms for the fields we saw:
+      for(int i=0;i<numFieldData;i++) {
+        FieldData fp = fieldDataArray[i];
+        if (fp.doNorms) {
+          BufferedNorms bn = norms[fp.fieldInfo.number];
+          assert bn != null;
+          assert bn.upto <= docID;
+          bn.fill(docID);
+          float norm = fp.boost * writer.getSimilarity().lengthNorm(fp.fieldInfo.name, fp.length);
+          bn.add(norm);
+        }
+      }
+
+      if (postingsIsFull && !flushPending) {
+        flushPending = true;
+        doFlushAfter = true;
+      }
+    }
+
+    /** Initializes shared state for this new document */
+    void init(Document doc, int docID) throws IOException {
+
+      this.docID = docID;
+      docBoost = doc.getBoost();
+      numStoredFields = 0;
+      numFieldData = 0;
+      numVectorFields = 0;
+
+      List docFields = doc.getFields();
+      final int numDocFields = docFields.size();
+      boolean docHasVectors = false;
+
+      // Absorb any new fields first seen in this document.
+      // Also absorb any changes to fields we had already
+      // seen before (eg suddenly turning on norms or
+      // vectors, etc.):
+
+      for(int i=0;i<numDocFields;i++) {
+        Fieldable field = (Fieldable) docFields.get(i);
+
+        FieldInfo fi = fieldInfos.add(field.name(), field.isIndexed(), field.isTermVectorStored(),
+                                      field.isStorePositionWithTermVector(), field.isStoreOffsetWithTermVector(),
+                                      field.getOmitNorms(), false);
+        numStoredFields += field.isStored() ? 1:0;
+        if (fi.isIndexed && !fi.omitNorms) {
+          // Maybe grow our buffered norms
+          if (norms.length <= fi.number) {
+            int newSize = (int) ((1+fi.number)*1.25);
+            BufferedNorms[] newNorms = new BufferedNorms[newSize];
+            System.arraycopy(norms, 0, newNorms, 0, norms.length);
+            norms = newNorms;
+          }
+          
+          if (norms[fi.number] == null)
+            norms[fi.number] = new BufferedNorms();
+
+          hasNorms = true;
+        }
+
+        // Make sure we have a FieldData allocated
+        int hashPos = fi.name.hashCode() & fieldDataHashMask;
+        FieldData fp = fieldDataHash[hashPos];
+        while(fp != null && !fp.fieldInfo.name.equals(fi.name))
+          fp = fp.next;
+
+        if (fp == null) {
+
+          fp = new FieldData(fi);
+          fp.next = fieldDataHash[hashPos];
+          fieldDataHash[hashPos] = fp;
+
+          if (numAllFieldData == allFieldDataArray.length) {
+            int newSize = (int) (allFieldDataArray.length*1.5);
+
+            FieldData newArray[] = new FieldData[newSize];
+            System.arraycopy(allFieldDataArray, 0, newArray, 0, numAllFieldData);
+            allFieldDataArray = newArray;
+
+            // Rehash
+            newSize = fieldDataHash.length*2;
+            newArray = new FieldData[newSize];
+            fieldDataHashMask = newSize-1;
+            for(int j=0;j<fieldDataHash.length;j++) {
+              FieldData fp0 = fieldDataHash[j];
+              while(fp0 != null) {
+                hashPos = fp0.fieldInfo.name.hashCode() & fieldDataHashMask;
+                FieldData nextFP0 = fp0.next;
+                fp0.next = newArray[hashPos];
+                newArray[hashPos] = fp0;
+                fp0 = nextFP0;
+              }
+            }
+            fieldDataHash = newArray;
+          }
+          allFieldDataArray[numAllFieldData++] = fp;
+        } else {
+          assert fp.fieldInfo == fi;
+        }
+
+        if (docID != fp.lastDocID) {
+
+          // First time we're seeing this field for this doc
+          fp.lastDocID = docID;
+          fp.fieldCount = 0;
+          fp.doVectors = fp.doVectorPositions = fp.doVectorOffsets = false;
+          fp.doNorms = fi.isIndexed && !fi.omitNorms;
+
+          if (numFieldData == fieldDataArray.length) {
+            int newSize = fieldDataArray.length*2;
+            FieldData newArray[] = new FieldData[newSize];
+            System.arraycopy(fieldDataArray, 0, newArray, 0, numFieldData);
+            fieldDataArray = newArray;
+
+          }
+          fieldDataArray[numFieldData++] = fp;
+        }
+
+        if (field.isTermVectorStored()) {
+          if (!fp.doVectors) {
+            if (numVectorFields++ == vectorFieldPointers.length) {
+              final int newSize = (int) (numVectorFields*1.5);
+              vectorFieldPointers = new long[newSize];
+              vectorFieldNumbers = new int[newSize];
+            }
+          }
+          fp.doVectors = true;
+          docHasVectors = true;
+
+          fp.doVectorPositions |= field.isStorePositionWithTermVector();
+          fp.doVectorOffsets |= field.isStoreOffsetWithTermVector();
+        }
+
+        if (fp.fieldCount == fp.docFields.length) {
+          Fieldable[] newArray = new Fieldable[fp.docFields.length*2];
+          System.arraycopy(fp.docFields, 0, newArray, 0, fp.docFields.length);
+          fp.docFields = newArray;
+        }
+
+        // Lazily allocate arrays for postings:
+        if (field.isIndexed() && fp.postingsHash == null)
+          fp.initPostingArrays();
+
+        fp.docFields[fp.fieldCount++] = field;
+      }
+
+      final int numFields = fieldInfos.size();
+
+      // Maybe init the local & global fieldsWriter
+      if (localFieldsWriter == null) {
+        if (fieldsWriter == null) {
+          assert docStoreSegment == null;
+          assert segment != null;
+          docStoreSegment = segment;
+          fieldsWriter = new FieldsWriter(directory, docStoreSegment, fieldInfos);
+          files = null;
+        }
+        localFieldsWriter = new FieldsWriter(null, fdtLocal, fieldInfos);
+      }
+
+      // First time we see a doc that has field(s) with
+      // stored vectors, we init our tvx writer
+      if (docHasVectors) {
+        if (tvx == null) {
+          assert docStoreSegment != null;
+          tvx = directory.createOutput(docStoreSegment + TermVectorsWriter.TVX_EXTENSION);
+          tvx.writeInt(TermVectorsWriter.FORMAT_VERSION);
+          tvd = directory.createOutput(docStoreSegment +  TermVectorsWriter.TVD_EXTENSION);
+          tvd.writeInt(TermVectorsWriter.FORMAT_VERSION);
+          tvf = directory.createOutput(docStoreSegment +  TermVectorsWriter.TVF_EXTENSION);
+          tvf.writeInt(TermVectorsWriter.FORMAT_VERSION);
+          files = null;
+
+          // We must "catch up" for all docIDs that had no
+          // vectors before this one
+          for(int i=0;i<docID;i++)
+            tvx.writeLong(0);
+        }
+
+        numVectorFields = 0;
+      }
+    }
+
+    /** Do in-place sort of Posting array */
+    final void doPostingSort(Posting[] postings, int numPosting) {
+      quickSort(postings, 0, numPosting-1);
+    }
+
+    final void quickSort(Posting[] postings, int lo, int hi) {
+      if (lo >= hi)
+        return;
+
+      int mid = (lo + hi) / 2;
+
+      if (comparePostings(postings[lo], postings[mid]) > 0) {
+        Posting tmp = postings[lo];
+        postings[lo] = postings[mid];
+        postings[mid] = tmp;
+      }
+
+      if (comparePostings(postings[mid], postings[hi]) > 0) {
+        Posting tmp = postings[mid];
+        postings[mid] = postings[hi];
+        postings[hi] = tmp;
+
+        if (comparePostings(postings[lo], postings[mid]) > 0) {
+          Posting tmp2 = postings[lo];
+          postings[lo] = postings[mid];
+          postings[mid] = tmp2;
+        }
+      }
+
+      int left = lo + 1;
+      int right = hi - 1;
+
+      if (left >= right)
+        return;
+
+      Posting partition = postings[mid];
+
+      for (; ;) {
+        while (comparePostings(postings[right], partition) > 0)
+          --right;
+
+        while (left < right && comparePostings(postings[left], partition) <= 0)
+          ++left;
+
+        if (left < right) {
+          Posting tmp = postings[left];
+          postings[left] = postings[right];
+          postings[right] = tmp;
+          --right;
+        } else {
+          break;
+        }
+      }
+
+      quickSort(postings, lo, left);
+      quickSort(postings, left + 1, hi);
+    }
+
+    /** Do in-place sort of PostingVector array */
+    final void doVectorSort(PostingVector[] postings, int numPosting) {
+      quickSort(postings, 0, numPosting-1);
+    }
+
+    final void quickSort(PostingVector[] postings, int lo, int hi) {
+      if (lo >= hi)
+        return;
+
+      int mid = (lo + hi) / 2;
+
+      if (comparePostings(postings[lo].p, postings[mid].p) > 0) {
+        PostingVector tmp = postings[lo];
+        postings[lo] = postings[mid];
+        postings[mid] = tmp;
+      }
+
+      if (comparePostings(postings[mid].p, postings[hi].p) > 0) {
+        PostingVector tmp = postings[mid];
+        postings[mid] = postings[hi];
+        postings[hi] = tmp;
+
+        if (comparePostings(postings[lo].p, postings[mid].p) > 0) {
+          PostingVector tmp2 = postings[lo];
+          postings[lo] = postings[mid];
+          postings[mid] = tmp2;
+        }
+      }
+
+      int left = lo + 1;
+      int right = hi - 1;
+
+      if (left >= right)
+        return;
+
+      PostingVector partition = postings[mid];
+
+      for (; ;) {
+        while (comparePostings(postings[right].p, partition.p) > 0)
+          --right;
+
+        while (left < right && comparePostings(postings[left].p, partition.p) <= 0)
+          ++left;
+
+        if (left < right) {
+          PostingVector tmp = postings[left];
+          postings[left] = postings[right];
+          postings[right] = tmp;
+          --right;
+        } else {
+          break;
+        }
+      }
+
+      quickSort(postings, lo, left);
+      quickSort(postings, left + 1, hi);
+    }
+
+    /** If there are fields we've seen but did not see again
+     *  in the last run, then free them up.  Also reduce
+     *  postings hash size. */
+    void trimFields() {
+
+      int upto = 0;
+      for(int i=0;i<numAllFieldData;i++) {
+        FieldData fp = allFieldDataArray[i];
+        if (fp.lastDocID == -1) {
+          // This field was not seen since the previous
+          // flush, so, free up its resources now
+
+          // Unhash
+          final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;
+          FieldData last = null;
+          FieldData fp0 = fieldDataHash[hashPos];
+          while(fp0 != fp) {
+            last = fp0;
+            fp0 = fp0.next;
+          }
+          assert fp0 != null;
+
+          if (last == null)
+            fieldDataHash[hashPos] = fp.next;
+          else
+            last.next = fp.next;
+
+          if (infoStream != null)
+            infoStream.println("  remove field=" + fp.fieldInfo.name);
+
+        } else {
+          // Reset
+          fp.lastDocID = -1;
+          allFieldDataArray[upto++] = fp;
+          
+          if (fp.numPostings > 0 && ((float) fp.numPostings) / fp.postingsHashSize < 0.2) {
+            int hashSize = fp.postingsHashSize;
+
+            // Reduce hash so it's between 25-50% full
+            while (fp.numPostings < hashSize/2 && hashSize >= 2)
+              hashSize /= 2;
+            hashSize *= 2;
+
+            if (hashSize != fp.postingsHash.length)
+              fp.rehashPostings(hashSize);
+          }
+        }
+      }
+
+      // If we didn't see any norms for this field since
+      // last flush, free it
+      for(int i=0;i<norms.length;i++) {
+        BufferedNorms n = norms[i];
+        if (n != null && n.upto == 0)
+          norms[i] = null;
+      }
+
+      numAllFieldData = upto;
+
+      // Also pare back PostingsVectors if it's excessively
+      // large
+      if (maxPostingsVectors * 1.5 < postingsVectors.length) {
+        final int newSize;
+        if (0 == maxPostingsVectors)
+          newSize = 1;
+        else
+          newSize = (int) (1.5*maxPostingsVectors);
+        PostingVector[] newArray = new PostingVector[newSize];
+        System.arraycopy(postingsVectors, 0, newArray, 0, newSize);
+        postingsVectors = newArray;
+      }
+    }
+
+    /** Tokenizes the fields of a document into Postings */
+    void processDocument(Analyzer analyzer)
+      throws IOException {
+
+      final int numFields = numFieldData;
+
+      fdtLocal.writeVInt(numStoredFields);
+
+      if (tvx != null)
+        // If we are writing vectors then we must visit
+        // fields in sorted order so they are written in
+        // sorted order.  TODO: we actually only need to
+        // sort the subset of fields that have vectors
+        // enabled; we could save [small amount of] CPU
+        // here.
+        Arrays.sort(fieldDataArray, 0, numFields);
+
+      // We process the document one field at a time
+      for(int i=0;i<numFields;i++)
+        fieldDataArray[i].processField(analyzer);
+
+      if (numBytesUsed > 0.95 * ramBufferSize)
+        balanceRAM();
+    }
+
+    final ByteBlockPool postingsPool = new ByteBlockPool();
+    final ByteBlockPool vectorsPool = new ByteBlockPool();
+    final CharBlockPool charPool = new CharBlockPool();
+
+    // Current posting we are working on
+    Posting p;
+    PostingVector vector;
+
+    // USE ONLY FOR DEBUGGING!
+    /*
+      public String getPostingText() {
+      char[] text = charPool.buffers[p.textStart >> CHAR_BLOCK_SHIFT];
+      int upto = p.textStart & CHAR_BLOCK_MASK;
+      while(text[upto] != 0xffff)
+      upto++;
+      return new String(text, p.textStart, upto-(p.textStart & BYTE_BLOCK_MASK));
+      }
+    */
+
+    /** Test whether the text for current Posting p equals
+     *  current tokenText. */
+    boolean postingEquals(final String tokenString, final char[] tokenText,
+                          final int tokenTextLen, final int tokenTextOffset) {
+
+      final char[] text = charPool.buffers[p.textStart >> CHAR_BLOCK_SHIFT];
+      assert text != null;
+      int pos = p.textStart & CHAR_BLOCK_MASK;
+
+      if (tokenText == null) {
+        // Compare to String
+        for(int i=0;i<tokenTextLen;i++)
+          if (tokenString.charAt(i) != text[pos++])
+            return false;
+        return text[pos] == 0xffff;
+      } else {
+        int tokenPos = tokenTextOffset;
+        final int stopAt = tokenTextLen+tokenPos;
+        for(;tokenPos<stopAt;pos++,tokenPos++)
+          if (tokenText[tokenPos] != text[pos])
+            return false;
+        return 0xffff == text[pos];
+      }
+    }
+
+    /** Compares term text for two Posting instance and
+     *  returns -1 if p1 < p2; 1 if p1 > p2; else 0.
+     */
+    int comparePostings(Posting p1, Posting p2) {
+      final char[] text1 = charPool.buffers[p1.textStart >> CHAR_BLOCK_SHIFT];
+      int pos1 = p1.textStart & CHAR_BLOCK_MASK;
+      final char[] text2 = charPool.buffers[p2.textStart >> CHAR_BLOCK_SHIFT];
+      int pos2 = p2.textStart & CHAR_BLOCK_MASK;
+      while(true) {
+        final char c1 = text1[pos1++];
+        final char c2 = text2[pos2++];
+        if (c1 < c2)
+          if (0xffff == c2)
+            return 1;
+          else
+            return -1;
+        else if (c2 < c1)
+          if (0xffff == c1)
+            return -1;
+          else
+            return 1;
+        else if (0xffff == c1)
+          return 0;
+      }
+    }
+
+    /** Write vInt into freq stream of current Posting */
+    public void writeFreqVInt(int i) {
+      int upto = 0;
+      while ((i & ~0x7F) != 0) {
+        writeFreqByte((byte)((i & 0x7f) | 0x80));
+        i >>>= 7;
+      }
+      writeFreqByte((byte) i);
+    }
+
+    /** Write vInt into prox stream of current Posting */
+    public void writeProxVInt(int i) {
+      int upto = 0;
+      while ((i & ~0x7F) != 0) {
+        writeProxByte((byte)((i & 0x7f) | 0x80));
+        i >>>= 7;
+      }
+      writeProxByte((byte) i);
+    }
+
+    /** Write byte into freq stream of current Posting */
+    byte[] freq;
+    int freqUpto;
+    public void writeFreqByte(byte b) {
+      assert freq != null;
+      if (freq[freqUpto] != 0) {
+        freqUpto = postingsPool.allocSlice(freq, freqUpto);
+        freq = postingsPool.buffer;
+        p.freqUpto = postingsPool.byteOffset;
+      }
+      freq[freqUpto++] = b;
+    }
+
+    /** Write byte into prox stream of current Posting */
+    byte[] prox;
+    int proxUpto;
+    public void writeProxByte(byte b) {
+      assert prox != null;
+      if (prox[proxUpto] != 0) {
+        proxUpto = postingsPool.allocSlice(prox, proxUpto);
+        prox = postingsPool.buffer;
+        p.proxUpto = postingsPool.byteOffset;
+        assert prox != null;
+      }
+      prox[proxUpto++] = b;
+      assert proxUpto != prox.length;
+    }
+
+    /** Currently only used to copy a payload into the prox
+     *  stream. */
+    public void writeProxBytes(byte[] b, int offset, int len) {
+      final int offsetEnd = offset + len;
+      while(offset < offsetEnd) {
+        if (prox[proxUpto] != 0) {
+          // End marker
+          proxUpto = postingsPool.allocSlice(prox, proxUpto);
+          prox = postingsPool.buffer;
+          p.proxUpto = postingsPool.byteOffset;
+        }
+
+        prox[proxUpto++] = b[offset++];
+        assert proxUpto != prox.length;
+      }
+    }
+
+    /** Write vInt into offsets stream of current
+     *  PostingVector */
+    public void writeOffsetVInt(int i) {
+      int upto = 0;
+      while ((i & ~0x7F) != 0) {
+        writeOffsetByte((byte)((i & 0x7f) | 0x80));
+        i >>>= 7;
+      }
+      writeOffsetByte((byte) i);
+    }
+
+    byte[] offsets;
+    int offsetUpto;
+
+    /** Write byte into offsets stream of current
+     *  PostingVector */
+    public void writeOffsetByte(byte b) {
+      assert offsets != null;
+      if (offsets[offsetUpto] != 0) {
+        offsetUpto = vectorsPool.allocSlice(offsets, offsetUpto);
+        offsets = vectorsPool.buffer;
+        vector.offsetUpto = vectorsPool.byteOffset;
+      }
+      offsets[offsetUpto++] = b;
+    }
+
+    /** Write vInt into pos stream of current
+     *  PostingVector */
+    public void writePosVInt(int i) {
+      int upto = 0;
+      while ((i & ~0x7F) != 0) {
+        writePosByte((byte)((i & 0x7f) | 0x80));
+        i >>>= 7;
+      }
+      writePosByte((byte) i);
+    }
+
+    byte[] pos;
+    int posUpto;
+
+    /** Write byte into pos stream of current
+     *  PostingVector */
+    public void writePosByte(byte b) {
+      assert pos != null;
+      if (pos[posUpto] != 0) {
+        posUpto = vectorsPool.allocSlice(pos, posUpto);
+        pos = vectorsPool.buffer;
+        vector.posUpto = vectorsPool.byteOffset;
+      }
+      pos[posUpto++] = b;
+    }
+
+    PostingVector[] postingsVectors = new PostingVector[1];
+    int maxPostingsVectors;
+
+    // Used to read a string value for a field
+    ReusableStringReader stringReader = new ReusableStringReader();
+
+    /** Holds data associated with a single field, including
+     *  the Postings hash.  A document may have many *
+     *  occurrences for a given field name; we gather all *
+     *  such occurrences here (in docFields) so that we can
+     *  * process the entire field at once. */
+    private final class FieldData implements Comparable {
+
+      ThreadState threadState;
+      FieldInfo fieldInfo;
+
+      int fieldCount;
+      Fieldable[] docFields = new Fieldable[1];
+
+      int lastDocID = -1;
+      FieldData next;
+
+      boolean doNorms;
+      boolean doVectors;
+      boolean doVectorPositions;
+      boolean doVectorOffsets;
+
+      int numPostings;
+      
+      Posting[] postingsHash;
+      int postingsHashSize;
+      int postingsHashHalfSize;
+      int postingsHashMask;
+
+      int position;
+      int length;
+      int offset;
+      float boost;
+      int postingsVectorsUpto;
+
+      public FieldData(FieldInfo fieldInfo) {
+        this.fieldInfo = fieldInfo;
+        threadState = ThreadState.this;
+      }
+
+      void resetPostingArrays() {
+        recyclePostings(this.postingsHash, numPostings);
+        Arrays.fill(postingsHash, 0, postingsHash.length, null);
+        numPostings = 0;
+      }
+
+      void initPostingArrays() {
+        // Target hash fill factor of <= 50%
+        // NOTE: must be a power of two for hash collision
+        // strategy to work correctly
+        postingsHashSize = 4;
+        postingsHashHalfSize = 2;
+        postingsHashMask = postingsHashSize-1;
+        postingsHash = new Posting[postingsHashSize];
+      }
+
+      /** So Arrays.sort can sort us. */
+      public int compareTo(Object o) {
+        return fieldInfo.name.compareTo(((FieldData) o).fieldInfo.name);
+      }
+
+      /** Collapse the hash table & sort in-place. */
+      public Posting[] sortPostings() {
+        int upto = 0;
+        for(int i=0;i<postingsHashSize;i++)
+          if (postingsHash[i] != null)
+            postingsHash[upto++] = postingsHash[i];
+
+        assert upto == numPostings;
+        doPostingSort(postingsHash, upto);
+        return postingsHash;
+      }
+
+      /** Process all occurrences of one field in the document. */
+      public void processField(Analyzer analyzer) throws IOException {
+        length = 0;
+        position = 0;
+        offset = 0;
+        boost = docBoost;
+
+        final int startNumPostings = numPostings;
+        final int maxFieldLength = writer.getMaxFieldLength();
+
+        final int limit = fieldCount;
+        final Fieldable[] docFieldsFinal = docFields;
+
+        // Walk through all occurrences in this doc for this field:
+        for(int j=0;j<limit;j++) {
+          Fieldable field = docFieldsFinal[j];
+
+          if (field.isIndexed())
+            invertField(field, analyzer, maxFieldLength);
+
+          if (field.isStored())
+            localFieldsWriter.writeField(fieldInfo, field);
+
+          docFieldsFinal[j] = null;
+        }
+
+        if (postingsVectorsUpto > 0) {
+          // Add term vectors for this field
+          writeVectors(fieldInfo);
+          if (postingsVectorsUpto > maxPostingsVectors)
+            maxPostingsVectors = postingsVectorsUpto;
+          postingsVectorsUpto = 0;
+          vectorsPool.reset();
+        }
+      }
+
+      int offsetEnd;
+      Token token;
+      Token localToken = new Token("", 0, 0);
+
+      /* Invert one occurrence of one field in the document */
+      public void invertField(Fieldable field, Analyzer analyzer, final int maxFieldLength) throws IOException {
+
+        if (length>0)
+          position += analyzer.getPositionIncrementGap(fieldInfo.name);
+
+        if (!field.isTokenized()) {		  // un-tokenized field
+          token = localToken;
+          String stringValue = field.stringValue();
+          token.setTermText(stringValue);
+          token.setStartOffset(offset);
+          token.setEndOffset(offset + stringValue.length());
+          addPosition();
+          offset += stringValue.length();
+          length++;
+        } else {                                  // tokenized field
+          final TokenStream stream;
+          final TokenStream streamValue = field.tokenStreamValue();
+
+          if (streamValue != null) 
+            stream = streamValue;
+          else {
+            // the field does not have a TokenStream,
+            // so we have to obtain one from the analyzer
+            final Reader reader;			  // find or make Reader
+            final Reader readerValue = field.readerValue();
+
+            if (readerValue != null)
+              reader = readerValue;
+            else {
+              String stringValue = field.stringValue();
+              if (stringValue == null)
+                throw new IllegalArgumentException("field must have either TokenStream, String or Reader value");
+              stringReader.init(stringValue);
+              reader = stringReader;
+            }
+          
+            // Tokenize field and add to postingTable
+            stream = analyzer.tokenStream(fieldInfo.name, reader);
+          }
+
+          // reset the TokenStream to the first token
+          stream.reset();
+
+          try {
+            offsetEnd = offset-1;
+            for (token = stream.next(); token != null; token = stream.next()) {
+              position += (token.getPositionIncrement() - 1);
+              addPosition();
+              if (++length >= maxFieldLength) {
+                if (infoStream != null)
+                  infoStream.println("maxFieldLength " +maxFieldLength+ " reached for field " + fieldInfo.name + ", ignoring following tokens");
+                break;
+              }
+            }
+            offset = offsetEnd+1;
+          } finally {
+            stream.close();
+          }
+        }
+
+        boost *= field.getBoost();
+      }
+
+      /** Only called when term vectors are enabled.  This
+       *  is called the first time we see a given term for
+       *  each * document, to allocate a PostingVector
+       *  instance that * is used to record data needed to
+       *  write the posting * vectors. */
+      private PostingVector addNewVector() {
+
+        if (postingsVectorsUpto == postingsVectors.length) {
+          final int newSize;
+          if (postingsVectors.length < 2)
+            newSize = 2;
+          else
+            newSize = (int) (1.5*postingsVectors.length);
+          PostingVector[] newArray = new PostingVector[newSize];
+          System.arraycopy(postingsVectors, 0, newArray, 0, postingsVectors.length);
+          postingsVectors = newArray;
+        }
+        
+        p.vector = postingsVectors[postingsVectorsUpto];
+        if (p.vector == null)
+          p.vector = postingsVectors[postingsVectorsUpto] = new PostingVector();
+
+        postingsVectorsUpto++;
+
+        final PostingVector v = p.vector;
+        v.p = p;
+
+        final int firstSize = levelSizeArray[0];
+
+        if (doVectorPositions) {
+          final int upto = vectorsPool.newSlice(firstSize);
+          v.posStart = v.posUpto = vectorsPool.byteOffset + upto;
+        }
+
+        if (doVectorOffsets) {
+          final int upto = vectorsPool.newSlice(firstSize);
+          v.offsetStart = v.offsetUpto = vectorsPool.byteOffset + upto;
+        }
+
+        return v;
+      }
+
+      int offsetStartCode;
+      int offsetStart;
+
+      /** This is the hotspot of indexing: it's called once
+       *  for every term of every document.  Its job is to *
+       *  update the postings byte stream (Postings hash) *
+       *  based on the occurence of a single term. */
+      private void addPosition() {
+
+        final Payload payload = token.getPayload();
+
+        final String tokenString;
+        final int tokenTextLen;
+        final int tokenTextOffset;
+
+        // Get the text of this term.  Term can either
+        // provide a String token or offset into a char[]
+        // array
+        final char[] tokenText = token.termBuffer();
+
+        int code = 0;
+        int code2 = 0;
+
+        if (tokenText == null) {
+
+          // Fallback to String token
+          tokenString = token.termText();
+          tokenTextLen = tokenString.length();
+          tokenTextOffset = 0;
+
+          // Compute hashcode.
+          int downto = tokenTextLen;
+          while (downto > 0)
+            code = (code*31) + tokenString.charAt(--downto);
+          
+          // System.out.println("  addPosition: field=" + fieldInfo.name + " string=" + tokenString + " pos=" + position + " offsetStart=" + (offset+token.startOffset()) + " offsetEnd=" + (offset+token.endOffset()) + " docID=" + docID + " doPos=" + doVectorPositions + " doOffset=" + doVectorOffsets);
+
+        } else {
+          tokenString = null;
+          tokenTextLen = token.termBufferLength();
+          tokenTextOffset = token.termBufferOffset();
+
+          // Compute hashcode
+          int downto = tokenTextLen+tokenTextOffset;
+          while (downto > tokenTextOffset)
+            code = (code*31) + tokenText[--downto];
+
+          // System.out.println("  addPosition: buffer=" + new String(tokenText, tokenTextOffset, tokenTextLen) + " pos=" + position + " offsetStart=" + (offset+token.startOffset()) + " offsetEnd=" + (offset + token.endOffset()) + " docID=" + docID + " doPos=" + doVectorPositions + " doOffset=" + doVectorOffsets);
+        }
+
+        int hashPos = code & postingsHashMask;
+
+        // Locate Posting in hash
+        p = postingsHash[hashPos];
+
+        if (p != null && !postingEquals(tokenString, tokenText, tokenTextLen, tokenTextOffset)) {
+          // Conflict: keep searching different locations in
+          // the hash table.
+          final int inc = code*1347|1;
+          do {
+            code += inc;
+            hashPos = code & postingsHashMask;
+            p = postingsHash[hashPos];
+          } while (p != null && !postingEquals(tokenString, tokenText, tokenTextLen, tokenTextOffset));
+        }
+        
+        final int proxCode;
+
+        if (p != null) {       // term seen since last flush
+
+          if (docID != p.lastDocID) { // term not yet seen in this doc
+            
+            // System.out.println("    seen before (new docID=" + docID + ") freqUpto=" + p.freqUpto +" proxUpto=" + p.proxUpto);
+
+            assert p.docFreq > 0;
+
+            // Now that we know doc freq for previous doc,
+            // write it & lastDocCode
+            freqUpto = p.freqUpto & BYTE_BLOCK_MASK;
+            freq = postingsPool.buffers[p.freqUpto >> BYTE_BLOCK_SHIFT];
+            if (1 == p.docFreq)
+              writeFreqVInt(p.lastDocCode|1);
+            else {
+              writeFreqVInt(p.lastDocCode);
+              writeFreqVInt(p.docFreq);
+            }
+            p.freqUpto = freqUpto + (p.freqUpto & BYTE_BLOCK_NOT_MASK);
+
+            if (doVectors) {
+              vector = addNewVector();
+              if (doVectorOffsets) {
+                offsetStartCode = offsetStart = offset + token.startOffset();
+                offsetEnd = offset + token.endOffset();
+              }
+            }
+
+            proxCode = position;
+
+            p.docFreq = 1;
+
+            // Store code so we can write this after we're
+            // done with this new doc
+            p.lastDocCode = (docID-p.lastDocID) << 1;
+            p.lastDocID = docID;
+
+          } else {                                // term already seen in this doc
+            // System.out.println("    seen before (same docID=" + docID + ") proxUpto=" + p.proxUpto);
+            p.docFreq++;
+
+            proxCode = position-p.lastPosition;
+
+            if (doVectors) {
+              vector = p.vector;
+              if (vector == null)
+                vector = addNewVector();
+              if (doVectorOffsets) {
+                offsetStart = offset + token.startOffset();
+                offsetEnd = offset + token.endOffset();
+                offsetStartCode = offsetStart-vector.lastOffset;
+              }
+            }
+          }
+        } else {					  // term not seen before
+          // System.out.println("    never seen docID=" + docID);
+
+          // Refill?
+          if (0 == postingsFreeCount) {
+            postingsFreeCount = postingsFreeList.length;
+            getPostings(postingsFreeList);
+          }
+
+          // Pull next free Posting from free list
+          p = postingsFreeList[--postingsFreeCount];
+
+          final int textLen1 = 1+tokenTextLen;
+          if (textLen1 + charPool.byteUpto > CHAR_BLOCK_SIZE)
+            charPool.nextBuffer();
+          final char[] text = charPool.buffer;
+          final int textUpto = charPool.byteUpto;
+          p.textStart = textUpto + charPool.byteOffset;
+          charPool.byteUpto += textLen1;
+
+          if (tokenString == null)
+            System.arraycopy(tokenText, tokenTextOffset, text, textUpto, tokenTextLen);
+          else
+            tokenString.getChars(0, tokenTextLen, text, textUpto);
+
+          text[textUpto+tokenTextLen] = 0xffff;
+          
+          assert postingsHash[hashPos] == null;
+
+          postingsHash[hashPos] = p;
+          numPostings++;
+
+          if (numPostings == postingsHashHalfSize)
+            rehashPostings(2*postingsHashSize);
+
+          // Init first slice for freq & prox streams
+          final int firstSize = levelSizeArray[0];
+
+          final int upto1 = postingsPool.newSlice(firstSize);
+          p.freqStart = p.freqUpto = postingsPool.byteOffset + upto1;
+
+          final int upto2 = postingsPool.newSlice(firstSize);
+          p.proxStart = p.proxUpto = postingsPool.byteOffset + upto2;
+
+          p.lastDocCode = docID << 1;
+          p.lastDocID = docID;
+          p.docFreq = 1;
+
+          if (doVectors) {
+            vector = addNewVector();
+            if (doVectorOffsets) {
+              offsetStart = offsetStartCode = offset + token.startOffset();
+              offsetEnd = offset + token.endOffset();
+            }
+          }
+
+          proxCode = position;
+        }
+
+        proxUpto = p.proxUpto & BYTE_BLOCK_MASK;
+        prox = postingsPool.buffers[p.proxUpto >> BYTE_BLOCK_SHIFT];
+        assert prox != null;
+
+        if (payload != null && payload.length > 0) {
+          writeProxVInt((proxCode<<1)|1);
+          writeProxVInt(payload.length);
+          writeProxBytes(payload.data, payload.offset, payload.length);
+          fieldInfo.storePayloads = true;
+        } else
+          writeProxVInt(proxCode<<1);
+
+        p.proxUpto = proxUpto + (p.proxUpto & BYTE_BLOCK_NOT_MASK);
+
+        p.lastPosition = position++;
+
+        if (doVectorPositions) {
+          posUpto = vector.posUpto & BYTE_BLOCK_MASK;
+          pos = vectorsPool.buffers[vector.posUpto >> BYTE_BLOCK_SHIFT];
+          writePosVInt(proxCode);
+          vector.posUpto = posUpto + (vector.posUpto & BYTE_BLOCK_NOT_MASK);
+        }
+
+        if (doVectorOffsets) {
+          offsetUpto = vector.offsetUpto & BYTE_BLOCK_MASK;
+          offsets = vectorsPool.buffers[vector.offsetUpto >> BYTE_BLOCK_SHIFT];
+          writeOffsetVInt(offsetStartCode);
+          writeOffsetVInt(offsetEnd-offsetStart);
+          vector.lastOffset = offsetEnd;
+          vector.offsetUpto = offsetUpto + (vector.offsetUpto & BYTE_BLOCK_NOT_MASK);
+        }
+      }
+
+      /** Called when postings hash is too small (> 50%
+       *  occupied) or too large (< 20% occupied). */
+      void rehashPostings(final int newSize) {
+
+        postingsHashMask = newSize-1;
+
+        Posting[] newHash = new Posting[newSize];
+        for(int i=0;i<postingsHashSize;i++) {
+          Posting p0 = postingsHash[i];
+          if (p0 != null) {
+            final int start = p0.textStart & CHAR_BLOCK_MASK;
+            final char[] text = charPool.buffers[p0.textStart >> CHAR_BLOCK_SHIFT];
+            int pos = start;
+            while(text[pos] != 0xffff)
+              pos++;
+            int code = 0;
+            while (pos > start)
+              code = (code*31) + text[--pos];
+
+            int hashPos = code & postingsHashMask;
+            assert hashPos >= 0;
+            if (newHash[hashPos] != null) {
+              final int inc = code*1347|1;
+              do {
+                code += inc;
+                hashPos = code & postingsHashMask;
+              } while (newHash[hashPos] != null);
+            }
+            newHash[hashPos] = p0;
+          }
+        }
+
+        postingsHash = newHash;
+        postingsHashSize = newSize;
+        postingsHashHalfSize = newSize/2;
+      }
+      
+      final ByteSliceReader vectorSliceReader = new ByteSliceReader();
+
+      /** Called once per field per document if term vectors
+       *  are enabled, to write the vectors to *
+       *  RAMOutputStream, which is then quickly flushed to
+       *  * the real term vectors files in the Directory. */
+      void writeVectors(FieldInfo fieldInfo) throws IOException {
+
+        assert fieldInfo.storeTermVector;
+
+        vectorFieldNumbers[numVectorFields] = fieldInfo.number;
+        vectorFieldPointers[numVectorFields] = tvfLocal.getFilePointer();
+        numVectorFields++;
+
+        final int numPostingsVectors = postingsVectorsUpto;
+
+        tvfLocal.writeVInt(numPostingsVectors);
+        byte bits = 0x0;
+        if (doVectorPositions)
+          bits |= TermVectorsWriter.STORE_POSITIONS_WITH_TERMVECTOR;
+        if (doVectorOffsets) 
+          bits |= TermVectorsWriter.STORE_OFFSET_WITH_TERMVECTOR;
+        tvfLocal.writeByte(bits);
+
+        doVectorSort(postingsVectors, numPostingsVectors);
+
+        Posting lastPosting = null;
+
+        final ByteSliceReader reader = vectorSliceReader;
+
+        for(int j=0;j<numPostingsVectors;j++) {
+          PostingVector vector = postingsVectors[j];
+          Posting posting = vector.p;
+          final int freq = posting.docFreq;
+          
+          final int prefix;
+          final char[] text2 = charPool.buffers[posting.textStart >> CHAR_BLOCK_SHIFT];
+          final int start2 = posting.textStart & CHAR_BLOCK_MASK;
+          int pos2 = start2;
+
+          // Compute common prefix between last term and
+          // this term
+          if (lastPosting == null)
+            prefix = 0;
+          else {
+            final char[] text1 = charPool.buffers[lastPosting.textStart >> CHAR_BLOCK_SHIFT];
+            final int start1 = lastPosting.textStart & CHAR_BLOCK_MASK;
+            int pos1 = start1;
+            while(true) {
+              final char c1 = text1[pos1];
+              final char c2 = text2[pos2];
+              if (c1 != c2 || c1 == 0xffff) {
+                prefix = pos1-start1;
+                break;
+              }
+              pos1++;
+              pos2++;
+            }
+          }
+          lastPosting = posting;
+
+          // Compute length
+          while(text2[pos2] != 0xffff)
+            pos2++;
+
+          final int suffix = pos2 - start2 - prefix;
+          tvfLocal.writeVInt(prefix);
+          tvfLocal.writeVInt(suffix);
+          tvfLocal.writeChars(text2, start2 + prefix, suffix);
+          tvfLocal.writeVInt(freq);
+
+          if (doVectorPositions) {
+            reader.init(vectorsPool, vector.posStart, vector.posUpto);
+            reader.writeTo(tvfLocal);
+          }
+
+          if (doVectorOffsets) {
+            reader.init(vectorsPool, vector.offsetStart, vector.offsetUpto);
+            reader.writeTo(tvfLocal);
+          }
+        }
+      }
+    }
+  }
+
+  private static final byte defaultNorm = Similarity.encodeNorm(1.0f);
+
+  /** Write norms in the "true" segment format.  This is
+   *  called only during commit, to create the .nrm file. */
+  void writeNorms(String segmentName, int totalNumDoc) throws IOException {
+
+    IndexOutput normsOut = directory.createOutput(segmentName + "." + IndexFileNames.NORMS_EXTENSION);
+
+    try {
+      normsOut.writeBytes(SegmentMerger.NORMS_HEADER, 0, SegmentMerger.NORMS_HEADER.length);
+
+      final int numField = fieldInfos.size();
+
+      for (int fieldIdx=0;fieldIdx<numField;fieldIdx++) {
+        FieldInfo fi = fieldInfos.fieldInfo(fieldIdx);
+        if (fi.isIndexed && !fi.omitNorms) {
+          BufferedNorms n = norms[fieldIdx];
+          final long v;
+          if (n == null)
+            v = 0;
+          else {
+            v = n.out.getFilePointer();
+            n.out.writeTo(normsOut);
+            n.reset();
+          }
+          if (v < totalNumDoc)
+            fillBytes(normsOut, defaultNorm, (int) (totalNumDoc-v));
+        }
+      }
+    } finally {
+      normsOut.close();
+    }
+  }
+
+  private DefaultSkipListWriter skipListWriter = null;
+
+  private boolean currentFieldStorePayloads;
+
+  /** Creates a segment from all Postings in the Postings
+   *  hashes across all ThreadStates & FieldDatas. */
+  private List writeSegment() throws IOException {
+
+    assert allThreadsIdle();
+
+    assert nextDocID == numDocsInRAM;
+
+    final String segmentName;
+
+    segmentName = segment;
+
+    TermInfosWriter termsOut = new TermInfosWriter(directory, segmentName, fieldInfos,
+                                                   writer.getTermIndexInterval());
+
+    IndexOutput freqOut = directory.createOutput(segmentName + ".frq");
+    IndexOutput proxOut = directory.createOutput(segmentName + ".prx");
+
+    // Gather all FieldData's that have postings, across all
+    // ThreadStates
+    ArrayList allFields = new ArrayList();
+    assert allThreadsIdle();
+    for(int i=0;i<threadStates.length;i++) {
+      ThreadState state = threadStates[i];
+      state.trimFields();
+      final int numFields = state.numAllFieldData;
+      for(int j=0;j<numFields;j++) {
+        ThreadState.FieldData fp = state.allFieldDataArray[j];
+        if (fp.numPostings > 0)
+          allFields.add(fp);
+      }
+    }
+
+    // Sort by field name
+    Collections.sort(allFields);
+    final int numAllFields = allFields.size();
+
+    skipListWriter = new DefaultSkipListWriter(termsOut.skipInterval,
+                                               termsOut.maxSkipLevels,
+                                               numDocsInRAM, freqOut, proxOut);
+
+    int start = 0;
+    while(start < numAllFields) {
+
+      final String fieldName = ((ThreadState.FieldData) allFields.get(start)).fieldInfo.name;
+
+      int end = start+1;
+      while(end < numAllFields && ((ThreadState.FieldData) allFields.get(end)).fieldInfo.name.equals(fieldName))
+        end++;
+      
+      ThreadState.FieldData[] fields = new ThreadState.FieldData[end-start];
+      for(int i=start;i<end;i++)
+        fields[i-start] = (ThreadState.FieldData) allFields.get(i);
+
+      // If this field has postings then add them to the
+      // segment
+      appendPostings(fields, termsOut, freqOut, proxOut);
+
+      for(int i=0;i<fields.length;i++)
+        fields[i].resetPostingArrays();
+
+      start = end;
+    }
+
+    freqOut.close();
+    proxOut.close();
+    termsOut.close();
+    
+    // Record all files we have flushed
+    List flushedFiles = new ArrayList();
+    flushedFiles.add(segmentFileName(IndexFileNames.FIELD_INFOS_EXTENSION));
+    flushedFiles.add(segmentFileName(IndexFileNames.FREQ_EXTENSION));
+    flushedFiles.add(segmentFileName(IndexFileNames.PROX_EXTENSION));
+    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_EXTENSION));
+    flushedFiles.add(segmentFileName(IndexFileNames.TERMS_INDEX_EXTENSION));
+
+    if (hasNorms) {
+      writeNorms(segmentName, numDocsInRAM);
+      hasNorms = false;
+      flushedFiles.add(segmentFileName(IndexFileNames.NORMS_EXTENSION));
+    }
+
+    if (infoStream != null) {
+      final long newSegmentSize = segmentSize(segmentName);
+      String message = "  oldRAMSize=" + numBytesUsed + " newFlushedSize=" + newSegmentSize + " docs/MB=" + nf.format(numDocsInRAM/(newSegmentSize/1024./1024.)) + " new/old=" + nf.format(100.0*newSegmentSize/numBytesUsed) + "%";
+      infoStream.println(message);
+    }
+
+    resetPostingsData();
+
+    nextDocID = 0;
+    nextWriteDocID = 0;
+    numDocsInRAM = 0;
+    files = null;
+
+    // Maybe downsize postingsFreeList array
+    if (postingsFreeList.length > 1.5*postingsFreeCount) {
+      int newSize = postingsFreeList.length;
+      while(newSize > 1.25*postingsFreeCount) {
+        newSize = (int) (newSize*0.8);
+      }
+      Posting[] newArray = new Posting[newSize];
+      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);
+      postingsFreeList = newArray;
+    }
+
+    return flushedFiles;
+  }
+
+  /** Returns the name of the file with this extension, on
+   *  the current segment we are working on. */
+  private String segmentFileName(String extension) {
+    return segment + "." + extension;
+  }
+
+  private final TermInfo termInfo = new TermInfo(); // minimize consing
+
+  /** Used to merge the postings from multiple ThreadStates
+   * when creating a segment */
+  final static class FieldMergeState {
+
+    private ThreadState.FieldData field;
+
+    private Posting[] postings;
+
+    private Posting p;
+    private  char[] text;
+    private int textOffset;
+
+    private int postingUpto = -1;
+
+    private ByteSliceReader freq = new ByteSliceReader();
+    private ByteSliceReader prox = new ByteSliceReader();
+
+    private int lastDocID;
+    private int docID;
+    private int termFreq;
+
+    boolean nextTerm() throws IOException {
+      postingUpto++;
+      if (postingUpto == field.numPostings)
+        return false;
+
+      p = postings[postingUpto];
+      docID = 0;
+
+      text = field.threadState.charPool.buffers[p.textStart >> CHAR_BLOCK_SHIFT];
+      textOffset = p.textStart & CHAR_BLOCK_MASK;
+
+      if (p.freqUpto > p.freqStart)
+        freq.init(field.threadState.postingsPool, p.freqStart, p.freqUpto);
+      else
+        freq.bufferOffset = freq.upto = freq.endIndex = 0;
+
+      prox.init(field.threadState.postingsPool, p.proxStart, p.proxUpto);
+
+      // Should always be true
+      boolean result = nextDoc();
+      assert result;
+
+      return true;
+    }
+
+    public boolean nextDoc() throws IOException {
+      if (freq.bufferOffset + freq.upto == freq.endIndex) {
+        if (p.lastDocCode != -1) {
+          // Return last doc
+          docID = p.lastDocID;
+          termFreq = p.docFreq;
+          p.lastDocCode = -1;
+          return true;
+        } else 
+          // EOF
+          return false;
+      }
+
+      final int code = freq.readVInt();
+      docID += code >>> 1;
+      if ((code & 1) != 0)
+        termFreq = 1;
+      else
+        termFreq = freq.readVInt();
+
+      return true;
+    }
+  }
+
+  int compareText(final char[] text1, int pos1, final char[] text2, int pos2) {
+    while(true) {
+      final char c1 = text1[pos1++];
+      final char c2 = text2[pos2++];
+      if (c1 < c2)
+        if (0xffff == c2)
+          return 1;
+        else
+          return -1;
+      else if (c2 < c1)
+        if (0xffff == c1)
+          return -1;
+        else
+          return 1;
+      else if (0xffff == c1)
+        return 0;
+    }
+  }
+
+  /* Walk through all unique text tokens (Posting
+   * instances) found in this field and serialize them
+   * into a single RAM segment. */
+  void appendPostings(ThreadState.FieldData[] fields,
+                      TermInfosWriter termsOut,
+                      IndexOutput freqOut,
+                      IndexOutput proxOut)
+    throws CorruptIndexException, IOException {
+
+    final String fieldName = fields[0].fieldInfo.name;
+    int numFields = fields.length;
+
+    final FieldMergeState[] mergeStates = new FieldMergeState[numFields];
+
+    for(int i=0;i<numFields;i++) {
+      FieldMergeState fms = mergeStates[i] = new FieldMergeState();
+      fms.field = fields[i];
+      fms.postings = fms.field.sortPostings();
+
+      assert fms.field.fieldInfo == fields[0].fieldInfo;
+
+      // Should always be true
+      boolean result = fms.nextTerm();
+      assert result;
+    }
+
+    Posting lastPosting = null;
+    final int skipInterval = termsOut.skipInterval;
+    currentFieldStorePayloads = fields[0].fieldInfo.storePayloads;
+
+    FieldMergeState[] termStates = new FieldMergeState[numFields];
+
+    while(numFields > 0) {
+
+      // Get the next term to merge
+      termStates[0] = mergeStates[0];
+      int numToMerge = 1;
+
+      for(int i=1;i<numFields;i++) {
+        final char[] text = mergeStates[i].text;
+        final int textOffset = mergeStates[i].textOffset;
+        final int cmp = compareText(text, textOffset, termStates[0].text, termStates[0].textOffset);
+
+        if (cmp < 0) {
+          termStates[0] = mergeStates[i];
+          numToMerge = 1;
+        } else if (cmp == 0)
+          termStates[numToMerge++] = mergeStates[i];
+      }
+
+      int df = 0;
+      int lastPayloadLength = -1;
+
+      int lastDoc = 0;
+
+      final char[] text = termStates[0].text;
+      final int start = termStates[0].textOffset;
+      int pos = start;
+      while(text[pos] != 0xffff)
+        pos++;
+
+      // TODO: can we avoid 2 new objects here?
+      Term term = new Term(fieldName, new String(text, start, pos-start));
+
+      long freqPointer = freqOut.getFilePointer();
+      long proxPointer = proxOut.getFilePointer();
+
+      skipListWriter.resetSkip();
+
+      // Now termStates has numToMerge FieldMergeStates
+      // which all share the same term.  Now we must
+      // interleave the docID streams.
+      while(numToMerge > 0) {
+        
+        if ((++df % skipInterval) == 0) {
+          skipListWriter.setSkipData(lastDoc, currentFieldStorePayloads, lastPayloadLength);
+          skipListWriter.bufferSkip(df);
+        }
+
+        FieldMergeState minState = termStates[0];
+        for(int i=1;i<numToMerge;i++)
+          if (termStates[i].docID < minState.docID)
+            minState = termStates[i];
+
+        final int doc = minState.docID;
+        final int termDocFreq = minState.termFreq;
+
+        assert doc < numDocsInRAM;
+        assert doc > lastDoc || df == 1;
+
+        final int newDocCode = (doc-lastDoc)<<1;
+        lastDoc = doc;
+
+        final ByteSliceReader prox = minState.prox;
+
+        // Carefully copy over the prox + payload info,
+        // changing the format to match Lucene's segment
+        // format.
+        for(int j=0;j<termDocFreq;j++) {
+          final int code = prox.readVInt();
+          if (currentFieldStorePayloads) {
+            final int payloadLength;
+            if ((code & 1) != 0) {
+              // This position has a payload
+              payloadLength = prox.readVInt();
+            } else
+              payloadLength = 0;
+            if (payloadLength != lastPayloadLength) {
+              proxOut.writeVInt(code|1);
+              proxOut.writeVInt(payloadLength);
+              lastPayloadLength = payloadLength;
+            } else
+              proxOut.writeVInt(code & (~1));
+            if (payloadLength > 0)
+              copyBytes(prox, proxOut, payloadLength);
+          } else {
+            assert 0 == (code & 1);
+            proxOut.writeVInt(code>>1);
+          }
+        }
+
+        if (1 == termDocFreq) {
+          freqOut.writeVInt(newDocCode|1);
+        } else {
+          freqOut.writeVInt(newDocCode);
+          freqOut.writeVInt(termDocFreq);
+        }
+
+        if (!minState.nextDoc()) {
+
+          // Remove from termStates
+          int upto = 0;
+          for(int i=0;i<numToMerge;i++)
+            if (termStates[i] != minState)
+              termStates[upto++] = termStates[i];
+          numToMerge--;
+          assert upto == numToMerge;
+
+          // Advance this state to the next term
+
+          if (!minState.nextTerm()) {
+            // OK, no more terms, so remove from mergeStates
+            // as well
+            upto = 0;
+            for(int i=0;i<numFields;i++)
+              if (mergeStates[i] != minState)
+                mergeStates[upto++] = mergeStates[i];
+            numFields--;
+            assert upto == numFields;
+          }
+        }
+      }
+
+      assert df > 0;
+
+      // Done merging this term
+
+      long skipPointer = skipListWriter.writeSkip(freqOut);
+
+      // Write term
+      termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));
+      termsOut.add(term, termInfo);
+    }
+  }
+
+  /** Returns a free (idle) ThreadState that may be used for
+   * indexing this one document.  This call also pauses if a
+   * flush is pending. */
+  synchronized ThreadState getThreadState(Document doc) throws IOException {
+
+    // First, find a thread state.  If this thread already
+    // has affinity to a specific ThreadState, use that one
+    // again.
+    ThreadState state = (ThreadState) threadBindings.get(Thread.currentThread());
+    if (state == null) {
+      // First time this thread has called us since last flush
+      ThreadState minThreadState = null;
+      for(int i=0;i<threadStates.length;i++) {
+        ThreadState ts = threadStates[i];
+        if (minThreadState == null || ts.numThreads < minThreadState.numThreads)
+          minThreadState = ts;
+      }
+      if (minThreadState != null && (minThreadState.numThreads == 0 || threadStates.length == MAX_THREAD_STATE)) {
+        state = minThreadState;
+        state.numThreads++;
+      } else {
+        // Just create a new "private" thread state
+        ThreadState[] newArray = new ThreadState[1+threadStates.length];
+        if (threadStates.length > 0)
+          System.arraycopy(threadStates, 0, newArray, 0, threadStates.length);
+        threadStates = newArray;
+        state = threadStates[threadStates.length-1] = new ThreadState();
+      }
+      threadBindings.put(Thread.currentThread(), state);
+    }
+
+    // Next, wait until my thread state is idle (in case
+    // it's shared with other threads) and for threads to
+    // not be paused nor a flush pending:
+    while(!state.isIdle || pauseThreads != 0 || flushPending)
+      try {
+        wait();
+      } catch (InterruptedException e) {}
+
+    if (segment == null)
+      segment = writer.newSegmentName();
+
+    numDocsInRAM++;
+
+    // We must at this point commit to flushing to ensure we
+    // always get N docs when we flush by doc count, even if
+    // > 1 thread is adding documents:
+    /* new merge policy
+    if (!flushPending && maxBufferedDocs > 0 && numDocsInRAM >= maxBufferedDocs) {
+    */
+    if (!flushPending && ramBufferSize == 0 && numDocsInRAM >= maxBufferedDocs) {
+      flushPending = true;
+      state.doFlushAfter = true;
+    } else
+      state.doFlushAfter = false;
+
+    state.isIdle = false;
+
+    boolean success = false;
+    try {
+      state.init(doc, nextDocID++);
+      success = true;
+    } finally {
+      if (!success) {
+        state.isIdle = true;
+        if (state.doFlushAfter) {
+          state.doFlushAfter = false;
+          flushPending = false;
+        }
+        abort();
+      }
+    }
+
+    return state;
+  }
+
+  /** Returns true if the caller (IndexWriter) should now
+   * flush. */
+  boolean addDocument(Document doc, Analyzer analyzer)
+    throws CorruptIndexException, IOException {
+
+    // This call is synchronized but fast
+    final ThreadState state = getThreadState(doc);
+    boolean success = false;
+    try {
+      // This call is not synchronized and does all the work
+      state.processDocument(analyzer);
+      // This call synchronized but fast
+      finishDocument(state);
+      success = true;
+    } finally {
+      if (!success) {
+        state.isIdle = true;
+        abort();
+      }
+    }
+    return state.doFlushAfter;
+  }
+
+  /** Does the synchronized work to finish/flush the
+   * inverted document. */
+  private synchronized void finishDocument(ThreadState state) throws IOException {
+
+    // Now write the indexed document to the real files.
+
+    if (nextWriteDocID == state.docID) {
+      // It's my turn, so write everything now:
+      state.isIdle = true;
+      nextWriteDocID++;
+      state.writeDocument();
+
+      // If any states were waiting on me, sweep through and
+      // flush those that are enabled by my write.
+      if (numWaiting > 0) {
+        while(true) {
+          int upto = 0;
+          for(int i=0;i<numWaiting;i++) {
+            ThreadState s = waitingThreadStates[i];
+            if (s.docID == nextWriteDocID) {
+              s.isIdle = true;
+              nextWriteDocID++;
+              s.writeDocument();
+            } else
+              // Compact as we go
+              waitingThreadStates[upto++] = waitingThreadStates[i];
+          }
+          if (upto == numWaiting) 
+            break;
+          numWaiting = upto;
+        }
+      }
+
+      // Now notify any incoming calls to addDocument
+      // (above) that are waiting on our line to
+      // shrink
+      notifyAll();
+
+    } else {
+      // Another thread got a docID before me, but, it
+      // hasn't finished its processing.  So add myself to
+      // the line but don't hold up this thread.
+      if (numWaiting == waitingThreadStates.length) {
+        ThreadState[] newWaiting = new ThreadState[2*waitingThreadStates.length];
+        System.arraycopy(waitingThreadStates, 0, newWaiting, 0, numWaiting);
+        waitingThreadStates = newWaiting;
+      }
+      waitingThreadStates[numWaiting++] = state;
+    }
+  }
+
+  long getRAMUsed() {
+    return numBytesUsed;
+  }
+
+  long numBytesAlloc;
+  long numBytesUsed;
+
+  NumberFormat nf = NumberFormat.getInstance();
+
+  /* Used only when writing norms to fill in default norm
+   * value into the holes in docID stream for those docs
+   * that didn't have this field. */
+  static void fillBytes(IndexOutput out, byte b, int numBytes) throws IOException {
+    for(int i=0;i<numBytes;i++)
+      out.writeByte(b);
+  }
+
+  byte[] copyByteBuffer = new byte[4096];
+
+  /** Copy numBytes from srcIn to destIn */
+  void copyBytes(IndexInput srcIn, IndexOutput destIn, long numBytes) throws IOException {
+    // TODO: we could do this more efficiently (save a copy)
+    // because it's always from a ByteSliceReader ->
+    // IndexOutput
+    while(numBytes > 0) {
+      final int chunk;
+      if (numBytes > 4096)
+        chunk = 4096;
+      else
+        chunk = (int) numBytes;
+      srcIn.readBytes(copyByteBuffer, 0, chunk);
+      destIn.writeBytes(copyByteBuffer, 0, chunk);
+      numBytes -= chunk;
+    }
+  }
+
+  /* Stores norms, buffered in RAM, until they are flushed
+   * to a partial segment. */
+  private static class BufferedNorms {
+
+    RAMOutputStream out;
+    int upto;
+
+    BufferedNorms() {
+      out = new RAMOutputStream();
+    }
+
+    void add(float norm) throws IOException {
+      byte b = Similarity.encodeNorm(norm);
+      out.writeByte(b);
+      upto++;
+    }
+
+    void reset() {
+      out.reset();
+      upto = 0;
+    }
+
+    void fill(int docID) throws IOException {
+      // Must now fill in docs that didn't have this
+      // field.  Note that this is how norms can consume
+      // tremendous storage when the docs have widely
+      // varying different fields, because we are not
+      // storing the norms sparsely (see LUCENE-830)
+      if (upto < docID) {
+        fillBytes(out, defaultNorm, docID-upto);
+        upto = docID;
+      }
+    }
+  }
+
+  /* Simple StringReader that can be reset to a new string;
+   * we use this when tokenizing the string value from a
+   * Field. */
+  private final static class ReusableStringReader extends Reader {
+    int upto;
+    int left;
+    String s;
+    void init(String s) {
+      this.s = s;
+      left = s.length();
+      this.upto = 0;
+    }
+    public int read(char[] c) {
+      return read(c, 0, c.length);
+    }
+    public int read(char[] c, int off, int len) {
+      if (left > len) {
+        s.getChars(upto, upto+len, c, off);
+        upto += len;
+        left -= len;
+        return len;
+      } else if (0 == left) {
+        return -1;
+      } else {
+        s.getChars(upto, upto+left, c, off);
+        int r = left;
+        left = 0;
+        upto = s.length();
+        return r;
+      }
+    }
+    public void close() {};
+  }
+
+  /* IndexInput that knows how to read the byte slices written
+   * by Posting and PostingVector.  We read the bytes in
+   * each slice until we hit the end of that slice at which
+   * point we read the forwarding address of the next slice
+   * and then jump to it.*/
+  private final static class ByteSliceReader extends IndexInput {
+    ByteBlockPool pool;
+    int bufferUpto;
+    byte[] buffer;
+    public int upto;
+    int limit;
+    int level;
+    public int bufferOffset;
+
+    public int endIndex;
+
+    public void init(ByteBlockPool pool, int startIndex, int endIndex) {
+
+      assert endIndex-startIndex > 0;
+
+      this.pool = pool;
+      this.endIndex = endIndex;
+
+      level = 0;
+      bufferUpto = startIndex / BYTE_BLOCK_SIZE;
+      bufferOffset = bufferUpto * BYTE_BLOCK_SIZE;
+      buffer = pool.buffers[bufferUpto];
+      upto = startIndex & BYTE_BLOCK_MASK;
+
+      final int firstSize = levelSizeArray[0];
+
+      if (startIndex+firstSize >= endIndex) {
+        // There is only this one slice to read
+        limit = endIndex & BYTE_BLOCK_MASK;
+      } else
+        limit = upto+firstSize-4;
+    }
+
+    public byte readByte() {
+      // Assert that we are not @ EOF
+      assert upto + bufferOffset < endIndex;
+      if (upto == limit)
+        nextSlice();
+      return buffer[upto++];
+    }
+
+    public long writeTo(IndexOutput out) throws IOException {
+      long size = 0;
+      while(true) {
+        if (limit + bufferOffset == endIndex) {
+          assert endIndex - bufferOffset >= upto;
+          out.writeBytes(buffer, upto, limit-upto);
+          size += limit-upto;
+          break;
+        } else {
+          out.writeBytes(buffer, upto, limit-upto);
+          size += limit-upto;
+          nextSlice();
+        }
+      }
+
+      return size;
+    }
+
+    public void nextSlice() {
+
+      // Skip to our next slice
+      final int nextIndex = ((buffer[limit]&0xff)<<24) + ((buffer[1+limit]&0xff)<<16) + ((buffer[2+limit]&0xff)<<8) + (buffer[3+limit]&0xff);
+
+      level = nextLevelArray[level];
+      final int newSize = levelSizeArray[level];
+
+      bufferUpto = nextIndex / BYTE_BLOCK_SIZE;
+      bufferOffset = bufferUpto * BYTE_BLOCK_SIZE;
+
+      buffer = pool.buffers[bufferUpto];
+      upto = nextIndex & BYTE_BLOCK_MASK;
+
+      if (nextIndex + newSize >= endIndex) {
+        // We are advancing to the final slice
+        assert endIndex - nextIndex > 0;
+        limit = endIndex - bufferOffset;
+      } else {
+        // This is not the final slice (subtract 4 for the
+        // forwarding address at the end of this new slice)
+        limit = upto+newSize-4;
+      }
+    }
+
+    public void readBytes(byte[] b, int offset, int len) {
+      while(len > 0) {
+        final int numLeft = limit-upto;
+        if (numLeft < len) {
+          // Read entire slice
+          System.arraycopy(buffer, upto, b, offset, numLeft);
+          offset += numLeft;
+          len -= numLeft;
+          nextSlice();
+        } else {
+          // This slice is the last one
+          System.arraycopy(buffer, upto, b, offset, len);
+          upto += len;
+          break;
+        }
+      }
+    }
+
+    public long getFilePointer() {throw new RuntimeException("not implemented");}
+    public long length() {throw new RuntimeException("not implemented");}
+    public void seek(long pos) {throw new RuntimeException("not implemented");}
+    public void close() {throw new RuntimeException("not implemented");}
+  }
+
+  // Size of each slice.  These arrays should be at most 16
+  // elements.  First array is just a compact way to encode
+  // X+1 with a max.  Second array is the length of each
+  // slice, ie first slice is 5 bytes, next slice is 14
+  // bytes, etc.
+  final static int[] nextLevelArray = {1, 2, 3, 4, 5, 6, 7, 8, 9, 9};
+  final static int[] levelSizeArray = {5, 14, 20, 30, 40, 40, 80, 80, 120, 200};
+
+  /* Class that Posting and PostingVector use to write byte
+   * streams into shared fixed-size byte[] arrays.  The idea
+   * is to allocate slices of increasing lengths For
+   * example, the first slice is 5 bytes, the next slice is
+   * 14, etc.  We start by writing our bytes into the first
+   * 5 bytes.  When we hit the end of the slice, we allocate
+   * the next slice and then write the address of the new
+   * slice into the last 4 bytes of the previous slice (the
+   * "forwarding address").
+   *
+   * Each slice is filled with 0's initially, and we mark
+   * the end with a non-zero byte.  This way the methods
+   * that are writing into the slice don't need to record
+   * its length and instead allocate a new slice once they
+   * hit a non-zero byte. */
+  private final class ByteBlockPool {
+
+    public byte[][] buffers = new byte[10][];
+
+    int bufferUpto = -1;                        // Which buffer we are upto
+    public int byteUpto = BYTE_BLOCK_SIZE;             // Where we are in head buffer
+
+    public byte[] buffer;                              // Current head buffer
+    public int byteOffset = -BYTE_BLOCK_SIZE;          // Current head offset
+
+    public void reset() {
+      recycleByteBlocks(buffers, 1+bufferUpto);
+      bufferUpto = -1;
+      byteUpto = BYTE_BLOCK_SIZE;
+      byteOffset = -BYTE_BLOCK_SIZE;
+    }
+
+    public void nextBuffer() {
+      bufferUpto++;
+      if (bufferUpto == buffers.length) {
+        byte[][] newBuffers = new byte[(int) (bufferUpto*1.5)][];
+        System.arraycopy(buffers, 0, newBuffers, 0, bufferUpto);
+        buffers = newBuffers;
+      }
+      buffer = buffers[bufferUpto] = getByteBlock();
+      Arrays.fill(buffer, (byte) 0);
+
+      byteUpto = 0;
+      byteOffset += BYTE_BLOCK_SIZE;
+    }
+
+    public int newSlice(final int size) {
+      if (byteUpto > BYTE_BLOCK_SIZE-size)
+        nextBuffer();
+      final int upto = byteUpto;
+      byteUpto += size;
+      buffer[byteUpto-1] = 16;
+      return upto;
+    }
+
+    public int allocSlice(final byte[] slice, final int upto) {
+
+      final int level = slice[upto] & 15;
+      final int newLevel = nextLevelArray[level];
+      final int newSize = levelSizeArray[newLevel];
+
+      // Maybe allocate another block
+      if (byteUpto > BYTE_BLOCK_SIZE-newSize)
+        nextBuffer();
+
+      final int newUpto = byteUpto;
+      final int offset = newUpto + byteOffset;
+      byteUpto += newSize;
+
+      // Copy forward the past 3 bytes (which we are about
+      // to overwrite with the forwarding address):
+      buffer[newUpto] = slice[upto-3];
+      buffer[newUpto+1] = slice[upto-2];
+      buffer[newUpto+2] = slice[upto-1];
+
+      // Write forwarding address at end of last slice:
+      slice[upto-3] = (byte) (offset >>> 24);
+      slice[upto-2] = (byte) (offset >>> 16);
+      slice[upto-1] = (byte) (offset >>> 8);
+      slice[upto] = (byte) offset;
+        
+      // Write new level:
+      buffer[byteUpto-1] = (byte) (16|newLevel);
+
+      return newUpto+3;
+    }
+  }
+
+  private final class CharBlockPool {
+
+    public char[][] buffers = new char[10][];
+    int numBuffer;
+
+    int bufferUpto = -1;                        // Which buffer we are upto
+    public int byteUpto = CHAR_BLOCK_SIZE;             // Where we are in head buffer
+
+    public char[] buffer;                              // Current head buffer
+    public int byteOffset = -CHAR_BLOCK_SIZE;          // Current head offset
+
+    public void reset() {
+      recycleCharBlocks(buffers, 1+bufferUpto);
+      bufferUpto = -1;
+      byteUpto = CHAR_BLOCK_SIZE;
+      byteOffset = -CHAR_BLOCK_SIZE;
+    }
+
+    public void nextBuffer() {
+      bufferUpto++;
+      if (bufferUpto == buffers.length) {
+        char[][] newBuffers = new char[(int) (bufferUpto*1.5)][];
+        System.arraycopy(buffers, 0, newBuffers, 0, bufferUpto);
+        buffers = newBuffers;
+      }
+      buffer = buffers[bufferUpto] = getCharBlock();
+
+      byteUpto = 0;
+      byteOffset += CHAR_BLOCK_SIZE;
+    }
+  }
+
+  // Used only when infoStream != null
+  private long segmentSize(String segmentName) throws IOException {
+    assert infoStream != null;
+    
+    long size = directory.fileLength(segmentName + ".tii") +
+      directory.fileLength(segmentName + ".tis") +
+      directory.fileLength(segmentName + ".frq") +
+      directory.fileLength(segmentName + ".prx");
+
+    final String normFileName = segmentName + ".nrm";
+    if (directory.fileExists(normFileName))
+      size += directory.fileLength(normFileName);
+
+    return size;
+  }
+
+  final private static int POINTER_NUM_BYTE = 4;
+  final private static int INT_NUM_BYTE = 4;
+  final private static int CHAR_NUM_BYTE = 2;
+  final private static int OBJECT_HEADER_NUM_BYTE = 8;
+
+  final static int POSTING_NUM_BYTE = OBJECT_HEADER_NUM_BYTE + 9*INT_NUM_BYTE + POINTER_NUM_BYTE;
+
+  // Holds free pool of Posting instances
+  private Posting[] postingsFreeList;
+  private int postingsFreeCount;
+
+  /* Allocate more Postings from shared pool */
+  private synchronized void getPostings(Posting[] postings) {
+    numBytesUsed += postings.length * POSTING_NUM_BYTE;
+    final int numToCopy;
+    if (postingsFreeCount < postings.length)
+      numToCopy = postingsFreeCount;
+    else
+      numToCopy = postings.length;
+    final int start = postingsFreeCount-numToCopy;
+    System.arraycopy(postingsFreeList, start,
+                     postings, 0, numToCopy);
+    postingsFreeCount -= numToCopy;
+
+    // Directly allocate the remainder if any
+    if (numToCopy < postings.length) {
+      numBytesAlloc += (postings.length - numToCopy) * POSTING_NUM_BYTE;
+      balanceRAM();
+      for(int i=numToCopy;i<postings.length;i++)
+        postings[i] = new Posting();
+    }
+  }
+
+  private synchronized void recyclePostings(Posting[] postings, int numPostings) {
+    // Move all Postings from this ThreadState back to our
+    // free list
+    if (postingsFreeCount + numPostings > postingsFreeList.length) {
+      final int newSize = (int) (1.25 * (postingsFreeCount + numPostings));
+      Posting[] newArray = new Posting[newSize];
+      System.arraycopy(postingsFreeList, 0, newArray, 0, postingsFreeCount);
+      postingsFreeList = newArray;
+    }
+    System.arraycopy(postings, 0, postingsFreeList, postingsFreeCount, numPostings);
+    postingsFreeCount += numPostings;
+    numBytesUsed -= numPostings * POSTING_NUM_BYTE;
+  }
+
+  /* Initial chunks size of the shared byte[] blocks used to
+     store postings data */
+  final static int BYTE_BLOCK_SHIFT = 15;
+  final static int BYTE_BLOCK_SIZE = (int) Math.pow(2.0, BYTE_BLOCK_SHIFT);
+  final static int BYTE_BLOCK_MASK = BYTE_BLOCK_SIZE - 1;
+  final static int BYTE_BLOCK_NOT_MASK = ~BYTE_BLOCK_MASK;
+
+  private ArrayList freeByteBlocks = new ArrayList();
+
+  /* Allocate another byte[] from the shared pool */
+  synchronized byte[] getByteBlock() {
+    final int size = freeByteBlocks.size();
+    final byte[] b;
+    if (0 == size) {
+      numBytesAlloc += BYTE_BLOCK_SIZE;
+      balanceRAM();
+      b = new byte[BYTE_BLOCK_SIZE];
+    } else
+      b = (byte[]) freeByteBlocks.remove(size-1);
+    numBytesUsed += BYTE_BLOCK_SIZE;
+    return b;
+  }
+
+  /* Return a byte[] to the pool */
+  synchronized void recycleByteBlocks(byte[][] blocks, int numBlocks) {
+    for(int i=0;i<numBlocks;i++)
+      freeByteBlocks.add(blocks[i]);
+    numBytesUsed -= numBlocks * BYTE_BLOCK_SIZE;
+  }
+
+  /* Initial chunk size of the shared char[] blocks used to
+     store term text */
+  final static int CHAR_BLOCK_SHIFT = 14;
+  final static int CHAR_BLOCK_SIZE = (int) Math.pow(2.0, CHAR_BLOCK_SHIFT);
+  final static int CHAR_BLOCK_MASK = CHAR_BLOCK_SIZE - 1;
+
+  private ArrayList freeCharBlocks = new ArrayList();
+
+  /* Allocate another char[] from the shared pool */
+  synchronized char[] getCharBlock() {
+    final int size = freeCharBlocks.size();
+    final char[] c;
+    if (0 == size) {
+      numBytesAlloc += CHAR_BLOCK_SIZE * CHAR_NUM_BYTE;
+      balanceRAM();
+      c = new char[BYTE_BLOCK_SIZE];
+    } else
+      c = (char[]) freeCharBlocks.remove(size-1);
+    numBytesUsed += CHAR_BLOCK_SIZE * CHAR_NUM_BYTE;
+    return c;
+  }
+
+  /* Return a char[] to the pool */
+  synchronized void recycleCharBlocks(char[][] blocks, int numBlocks) {
+    for(int i=0;i<numBlocks;i++)
+      freeCharBlocks.add(blocks[i]);
+    numBytesUsed -= numBlocks * CHAR_BLOCK_SIZE * CHAR_NUM_BYTE;
+  }
+
+  String toMB(long v) {
+    return nf.format(v/1024./1024.);
+  }
+
+  /* We have three pools of RAM: Postings, byte blocks
+   * (holds freq/prox posting data) and char blocks (holds
+   * characters in the term).  Different docs require
+   * varying amount of storage from these three classes.
+   * For example, docs with many unique single-occurrence
+   * short terms will use up the Postings RAM and hardly any
+   * of the other two.  Whereas docs with very large terms
+   * will use alot of char blocks RAM and relatively less of
+   * the other two.  This method just frees allocations from
+   * the pools once we are over-budget, which balances the
+   * pools to match the current docs. */
+  private synchronized void balanceRAM() {
+
+    if (ramBufferSize == 0.0 || postingsIsFull)
+      return;
+
+    // We free our allocations if we've allocated 5% over
+    // our allowed RAM buffer
+    final long freeTrigger = (long) (1.05 * ramBufferSize);
+    final long freeLevel = (long) (0.95 * ramBufferSize);
+    
+    // We flush when we've used our target usage
+    final long flushTrigger = (long) ramBufferSize;
+
+    if (numBytesAlloc > freeTrigger) {
+      if (infoStream != null)
+        infoStream.println("  RAM: now balance allocations: usedMB=" + toMB(numBytesUsed) +
+                           " vs trigger=" + toMB(flushTrigger) +
+                           " allocMB=" + toMB(numBytesAlloc) +
+                           " vs trigger=" + toMB(freeTrigger) +
+                           " postingsFree=" + toMB(postingsFreeCount*POSTING_NUM_BYTE) +
+                           " byteBlockFree=" + toMB(freeByteBlocks.size()*BYTE_BLOCK_SIZE) +
+                           " charBlockFree=" + toMB(freeCharBlocks.size()*CHAR_BLOCK_SIZE*CHAR_NUM_BYTE));
+
+      // When we've crossed 100% of our target Postings
+      // RAM usage, try to free up until we're back down
+      // to 95%
+      final long startBytesAlloc = numBytesAlloc;
+
+      final int postingsFreeChunk = (int) (BYTE_BLOCK_SIZE / POSTING_NUM_BYTE);
+
+      int iter = 0;
+
+      // We free equally from each pool in 64 KB
+      // chunks until we are below our threshold
+      // (freeLevel)
+
+      while(numBytesAlloc > freeLevel) {
+        if (0 == freeByteBlocks.size() && 0 == freeCharBlocks.size() && 0 == postingsFreeCount) {
+          // Nothing else to free -- must flush now.
+          postingsIsFull = true;
+          if (infoStream != null)
+            infoStream.println("    nothing to free; now set postingsIsFull");
+          break;
+        }
+
+        if ((0 == iter % 3) && freeByteBlocks.size() > 0) {
+          freeByteBlocks.remove(freeByteBlocks.size()-1);
+          numBytesAlloc -= BYTE_BLOCK_SIZE;
+        }
+
+        if ((1 == iter % 3) && freeCharBlocks.size() > 0) {
+          freeCharBlocks.remove(freeCharBlocks.size()-1);
+          numBytesAlloc -= CHAR_BLOCK_SIZE * CHAR_NUM_BYTE;
+        }
+
+        if ((2 == iter % 3) && postingsFreeCount > 0) {
+          final int numToFree;
+          if (postingsFreeCount >= postingsFreeChunk)
+            numToFree = postingsFreeChunk;
+          else
+            numToFree = postingsFreeCount;
+          Arrays.fill(postingsFreeList, postingsFreeCount-numToFree, postingsFreeCount, null);
+          postingsFreeCount -= numToFree;
+          numBytesAlloc -= numToFree * POSTING_NUM_BYTE;
+        }
+
+        iter++;
+      }
+      
+      if (infoStream != null)
+        infoStream.println("    after free: freedMB=" + nf.format((startBytesAlloc-numBytesAlloc)/1024./1024.) + " usedMB=" + nf.format(numBytesUsed/1024./1024.) + " allocMB=" + nf.format(numBytesAlloc/1024./1024.));
+      
+    } else {
+      // If we have not crossed the 100% mark, but have
+      // crossed the 95% mark of RAM we are actually
+      // using, go ahead and flush.  This prevents
+      // over-allocating and then freeing, with every
+      // flush.
+      if (numBytesUsed > flushTrigger) {
+        if (infoStream != null)
+          infoStream.println("  RAM: now flush @ usedMB=" + nf.format(numBytesUsed/1024./1024.) +
+                             " allocMB=" + nf.format(numBytesAlloc/1024./1024.) +
+                             " triggerMB=" + nf.format(flushTrigger/1024./1024.));
+
+        postingsIsFull = true;
+      }
+    }
+  }
+
+  /* Used to track postings for a single term.  One of these
+   * exists per unique term seen since the last flush. */
+  private final static class Posting {
+    int textStart;                                  // Address into char[] blocks where our text is stored
+    int docFreq;                                    // # times this term occurs in the current doc
+    int freqStart;                                  // Address of first byte[] slice for freq
+    int freqUpto;                                   // Next write address for freq
+    int proxStart;                                  // Address of first byte[] slice
+    int proxUpto;                                   // Next write address for prox
+    int lastDocID;                                  // Last docID where this term occurred
+    int lastDocCode;                                // Code for prior doc
+    int lastPosition;                               // Last position where this term occurred
+    PostingVector vector;                           // Corresponding PostingVector instance
+  }
+
+  /* Used to track data for term vectors.  One of these
+   * exists per unique term seen in each field in the
+   * document. */
+  private final static class PostingVector {
+    Posting p;                                      // Corresponding Posting instance for this term
+    int lastOffset;                                 // Last offset we saw
+    int offsetStart;                                // Address of first slice for offsets
+    int offsetUpto;                                 // Next write address for offsets
+    int posStart;                                   // Address of first slice for positions
+    int posUpto;                                    // Next write address for positions
+  }
+}
diff --git a/src/java/org/apache/lucene/index/FieldInfo.java b/src/java/org/apache/lucene/index/FieldInfo.java
index 416c995..bd93f31 100644
--- a/src/java/org/apache/lucene/index/FieldInfo.java
+++ b/src/java/org/apache/lucene/index/FieldInfo.java
@@ -43,4 +43,9 @@ final class FieldInfo {
     this.omitNorms = omitNorms;
     this.storePayloads = storePayloads;
   }
+
+  public Object clone() {
+    return new FieldInfo(name, isIndexed, number, storeTermVector, storePositionWithTermVector,
+                         storeOffsetWithTermVector, omitNorms, storePayloads);
+  }
 }
diff --git a/src/java/org/apache/lucene/index/FieldInfos.java b/src/java/org/apache/lucene/index/FieldInfos.java
index e638d8a..9baf155 100644
--- a/src/java/org/apache/lucene/index/FieldInfos.java
+++ b/src/java/org/apache/lucene/index/FieldInfos.java
@@ -62,6 +62,20 @@ final class FieldInfos {
     }
   }
 
+  /**
+   * Returns a deep clone of this FieldInfos instance.
+   */
+  public Object clone() {
+    FieldInfos fis = new FieldInfos();
+    final int numField = byNumber.size();
+    for(int i=0;i<numField;i++) {
+      FieldInfo fi = (FieldInfo) ((FieldInfo) byNumber.get(i)).clone();
+      fis.byNumber.add(fi);
+      fis.byName.put(fi.name, fi);
+    }
+    return fis;
+  }
+
   /** Adds field info for a Document. */
   public void add(Document doc) {
     List fields = doc.getFields();
diff --git a/src/java/org/apache/lucene/index/FieldsReader.java b/src/java/org/apache/lucene/index/FieldsReader.java
index deb7d85..c36bbcc 100644
--- a/src/java/org/apache/lucene/index/FieldsReader.java
+++ b/src/java/org/apache/lucene/index/FieldsReader.java
@@ -51,19 +51,39 @@ final class FieldsReader {
   private int size;
   private boolean closed;
 
+  // The docID offset where our docs begin in the index
+  // file.  This will be 0 if we have our own private file.
+  private int docStoreOffset;
+
   private ThreadLocal fieldsStreamTL = new ThreadLocal();
 
   FieldsReader(Directory d, String segment, FieldInfos fn) throws IOException {
-    this(d, segment, fn, BufferedIndexInput.BUFFER_SIZE);
+    this(d, segment, fn, BufferedIndexInput.BUFFER_SIZE, -1, 0);
   }
 
   FieldsReader(Directory d, String segment, FieldInfos fn, int readBufferSize) throws IOException {
+    this(d, segment, fn, readBufferSize, -1, 0);
+  }
+
+  FieldsReader(Directory d, String segment, FieldInfos fn, int readBufferSize, int docStoreOffset, int size) throws IOException {
     fieldInfos = fn;
 
     cloneableFieldsStream = d.openInput(segment + ".fdt", readBufferSize);
     fieldsStream = (IndexInput)cloneableFieldsStream.clone();
     indexStream = d.openInput(segment + ".fdx", readBufferSize);
-    size = (int) (indexStream.length() / 8);
+
+    if (docStoreOffset != -1) {
+      // We read only a slice out of this shared fields file
+      this.docStoreOffset = docStoreOffset;
+      this.size = size;
+
+      // Verify the file is long enough to hold all of our
+      // docs
+      assert ((int) (indexStream.length()/8)) >= size + this.docStoreOffset;
+    } else {
+      this.docStoreOffset = 0;
+      this.size = (int) (indexStream.length() / 8);
+    }
   }
 
   /**
@@ -100,7 +120,7 @@ final class FieldsReader {
   }
 
   final Document doc(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
-    indexStream.seek(n * 8L);
+    indexStream.seek((n + docStoreOffset) * 8L);
     long position = indexStream.readLong();
     fieldsStream.seek(position);
 
diff --git a/src/java/org/apache/lucene/index/FieldsWriter.java b/src/java/org/apache/lucene/index/FieldsWriter.java
index 9681e0a..6fdba68 100644
--- a/src/java/org/apache/lucene/index/FieldsWriter.java
+++ b/src/java/org/apache/lucene/index/FieldsWriter.java
@@ -24,6 +24,7 @@ import java.util.zip.Deflater;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.RAMOutputStream;
 import org.apache.lucene.store.IndexOutput;
 
 final class FieldsWriter
@@ -38,15 +39,92 @@ final class FieldsWriter
 
     private IndexOutput indexStream;
 
+    private boolean doClose;
+
     FieldsWriter(Directory d, String segment, FieldInfos fn) throws IOException {
         fieldInfos = fn;
         fieldsStream = d.createOutput(segment + ".fdt");
         indexStream = d.createOutput(segment + ".fdx");
+        doClose = true;
+    }
+
+    FieldsWriter(IndexOutput fdx, IndexOutput fdt, FieldInfos fn) throws IOException {
+        fieldInfos = fn;
+        fieldsStream = fdt;
+        indexStream = fdx;
+        doClose = false;
+    }
+
+    // Writes the contents of buffer into the fields stream
+    // and adds a new entry for this document into the index
+    // stream.  This assumes the buffer was already written
+    // in the correct fields format.
+    void flushDocument(RAMOutputStream buffer) throws IOException {
+      indexStream.writeLong(fieldsStream.getFilePointer());
+      buffer.writeTo(fieldsStream);
+    }
+
+    void flush() throws IOException {
+      indexStream.flush();
+      fieldsStream.flush();
     }
 
     final void close() throws IOException {
+      if (doClose) {
         fieldsStream.close();
         indexStream.close();
+      }
+    }
+
+    final void writeField(FieldInfo fi, Fieldable field) throws IOException {
+      // if the field as an instanceof FieldsReader.FieldForMerge, we're in merge mode
+      // and field.binaryValue() already returns the compressed value for a field
+      // with isCompressed()==true, so we disable compression in that case
+      boolean disableCompression = (field instanceof FieldsReader.FieldForMerge);
+      fieldsStream.writeVInt(fi.number);
+      byte bits = 0;
+      if (field.isTokenized())
+        bits |= FieldsWriter.FIELD_IS_TOKENIZED;
+      if (field.isBinary())
+        bits |= FieldsWriter.FIELD_IS_BINARY;
+      if (field.isCompressed())
+        bits |= FieldsWriter.FIELD_IS_COMPRESSED;
+                
+      fieldsStream.writeByte(bits);
+                
+      if (field.isCompressed()) {
+        // compression is enabled for the current field
+        byte[] data = null;
+                  
+        if (disableCompression) {
+          // optimized case for merging, the data
+          // is already compressed
+          data = field.binaryValue();
+        } else {
+          // check if it is a binary field
+          if (field.isBinary()) {
+            data = compress(field.binaryValue());
+          }
+          else {
+            data = compress(field.stringValue().getBytes("UTF-8"));
+          }
+        }
+        final int len = data.length;
+        fieldsStream.writeVInt(len);
+        fieldsStream.writeBytes(data, len);
+      }
+      else {
+        // compression is disabled for the current field
+        if (field.isBinary()) {
+          byte[] data = field.binaryValue();
+          final int len = data.length;
+          fieldsStream.writeVInt(len);
+          fieldsStream.writeBytes(data, len);
+        }
+        else {
+          fieldsStream.writeString(field.stringValue());
+        }
+      }
     }
 
     final void addDocument(Document doc) throws IOException {
@@ -64,57 +142,8 @@ final class FieldsWriter
         fieldIterator = doc.getFields().iterator();
         while (fieldIterator.hasNext()) {
             Fieldable field = (Fieldable) fieldIterator.next();
-            // if the field as an instanceof FieldsReader.FieldForMerge, we're in merge mode
-            // and field.binaryValue() already returns the compressed value for a field
-            // with isCompressed()==true, so we disable compression in that case
-            boolean disableCompression = (field instanceof FieldsReader.FieldForMerge);
-            if (field.isStored()) {
-                fieldsStream.writeVInt(fieldInfos.fieldNumber(field.name()));
-
-                byte bits = 0;
-                if (field.isTokenized())
-                    bits |= FieldsWriter.FIELD_IS_TOKENIZED;
-                if (field.isBinary())
-                    bits |= FieldsWriter.FIELD_IS_BINARY;
-                if (field.isCompressed())
-                    bits |= FieldsWriter.FIELD_IS_COMPRESSED;
-                
-                fieldsStream.writeByte(bits);
-                
-                if (field.isCompressed()) {
-                  // compression is enabled for the current field
-                  byte[] data = null;
-                  
-                  if (disableCompression) {
-                      // optimized case for merging, the data
-                      // is already compressed
-                      data = field.binaryValue();
-                  } else {
-                      // check if it is a binary field
-                      if (field.isBinary()) {
-                        data = compress(field.binaryValue());
-                      }
-                      else {
-                        data = compress(field.stringValue().getBytes("UTF-8"));
-                      }
-                  }
-                  final int len = data.length;
-                  fieldsStream.writeVInt(len);
-                  fieldsStream.writeBytes(data, len);
-                }
-                else {
-                  // compression is disabled for the current field
-                  if (field.isBinary()) {
-                    byte[] data = field.binaryValue();
-                    final int len = data.length;
-                    fieldsStream.writeVInt(len);
-                    fieldsStream.writeBytes(data, len);
-                  }
-                  else {
-                    fieldsStream.writeString(field.stringValue());
-                  }
-                }
-            }
+            if (field.isStored())
+              writeField(fieldInfos.fieldInfo(field.name()), field);
         }
     }
 
diff --git a/src/java/org/apache/lucene/index/IndexFileDeleter.java b/src/java/org/apache/lucene/index/IndexFileDeleter.java
index b837db4..2675517 100644
--- a/src/java/org/apache/lucene/index/IndexFileDeleter.java
+++ b/src/java/org/apache/lucene/index/IndexFileDeleter.java
@@ -97,6 +97,7 @@ final class IndexFileDeleter {
   private PrintStream infoStream;
   private Directory directory;
   private IndexDeletionPolicy policy;
+  private DocumentsWriter docWriter;
 
   void setInfoStream(PrintStream infoStream) {
     this.infoStream = infoStream;
@@ -116,10 +117,12 @@ final class IndexFileDeleter {
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
    */
-  public IndexFileDeleter(Directory directory, IndexDeletionPolicy policy, SegmentInfos segmentInfos, PrintStream infoStream)
+  public IndexFileDeleter(Directory directory, IndexDeletionPolicy policy, SegmentInfos segmentInfos, PrintStream infoStream, DocumentsWriter docWriter)
     throws CorruptIndexException, IOException {
 
+    this.docWriter = docWriter;
     this.infoStream = infoStream;
+
     this.policy = policy;
     this.directory = directory;
 
@@ -294,7 +297,7 @@ final class IndexFileDeleter {
   public void checkpoint(SegmentInfos segmentInfos, boolean isCommit) throws IOException {
 
     if (infoStream != null) {
-      message("now checkpoint \"" + segmentInfos.getCurrentSegmentFileName() + "\" [isCommit = " + isCommit + "]");
+      message("now checkpoint \"" + segmentInfos.getCurrentSegmentFileName() + "\" [" + segmentInfos.size() + " segments " + "; isCommit = " + isCommit + "]");
     }
 
     // Try again now to delete any previously un-deletable
@@ -310,6 +313,8 @@ final class IndexFileDeleter {
 
     // Incref the files:
     incRef(segmentInfos, isCommit);
+    if (docWriter != null)
+      incRef(docWriter.files());
 
     if (isCommit) {
       // Append to our commits list:
@@ -325,9 +330,8 @@ final class IndexFileDeleter {
     // DecRef old files from the last checkpoint, if any:
     int size = lastFiles.size();
     if (size > 0) {
-      for(int i=0;i<size;i++) {
+      for(int i=0;i<size;i++)
         decRef((List) lastFiles.get(i));
-      }
       lastFiles.clear();
     }
 
@@ -340,6 +344,8 @@ final class IndexFileDeleter {
           lastFiles.add(segmentInfo.files());
         }
       }
+      if (docWriter != null)
+        lastFiles.add(docWriter.files());
     }
   }
 
diff --git a/src/java/org/apache/lucene/index/IndexFileNames.java b/src/java/org/apache/lucene/index/IndexFileNames.java
index 3f60986..c60df74 100644
--- a/src/java/org/apache/lucene/index/IndexFileNames.java
+++ b/src/java/org/apache/lucene/index/IndexFileNames.java
@@ -38,18 +38,54 @@ final class IndexFileNames {
   /** Extension of norms file */
   static final String NORMS_EXTENSION = "nrm";
 
+  /** Extension of freq postings file */
+  static final String FREQ_EXTENSION = "frq";
+
+  /** Extension of prox postings file */
+  static final String PROX_EXTENSION = "prx";
+
+  /** Extension of terms file */
+  static final String TERMS_EXTENSION = "tis";
+
+  /** Extension of terms index file */
+  static final String TERMS_INDEX_EXTENSION = "tii";
+
+  /** Extension of stored fields index file */
+  static final String FIELDS_INDEX_EXTENSION = "fdx";
+
+  /** Extension of stored fields file */
+  static final String FIELDS_EXTENSION = "fdt";
+
+  /** Extension of vectors fields file */
+  static final String VECTORS_FIELDS_EXTENSION = "tvf";
+
+  /** Extension of vectors documents file */
+  static final String VECTORS_DOCUMENTS_EXTENSION = "tvd";
+
+  /** Extension of vectors index file */
+  static final String VECTORS_INDEX_EXTENSION = "tvx";
+
   /** Extension of compound file */
   static final String COMPOUND_FILE_EXTENSION = "cfs";
 
+  /** Extension of compound file for doc store files*/
+  static final String COMPOUND_FILE_STORE_EXTENSION = "cfx";
+
   /** Extension of deletes */
   static final String DELETES_EXTENSION = "del";
 
+  /** Extension of field infos */
+  static final String FIELD_INFOS_EXTENSION = "fnm";
+
   /** Extension of plain norms */
   static final String PLAIN_NORMS_EXTENSION = "f";
 
   /** Extension of separate norms */
   static final String SEPARATE_NORMS_EXTENSION = "s";
 
+  /** Extension of gen file */
+  static final String GEN_EXTENSION = "gen";
+
   /**
    * This array contains all filename extensions used by
    * Lucene's index files, with two exceptions, namely the
@@ -59,25 +95,72 @@ final class IndexFileNames {
    * filename extension.
    */
   static final String INDEX_EXTENSIONS[] = new String[] {
-      "cfs", "fnm", "fdx", "fdt", "tii", "tis", "frq", "prx", "del",
-      "tvx", "tvd", "tvf", "gen", "nrm" 
+    COMPOUND_FILE_EXTENSION,
+    FIELD_INFOS_EXTENSION,
+    FIELDS_INDEX_EXTENSION,
+    FIELDS_EXTENSION,
+    TERMS_INDEX_EXTENSION,
+    TERMS_EXTENSION,
+    FREQ_EXTENSION,
+    PROX_EXTENSION,
+    DELETES_EXTENSION,
+    VECTORS_INDEX_EXTENSION,
+    VECTORS_DOCUMENTS_EXTENSION,
+    VECTORS_FIELDS_EXTENSION,
+    GEN_EXTENSION,
+    NORMS_EXTENSION,
+    COMPOUND_FILE_STORE_EXTENSION,
   };
 
   /** File extensions that are added to a compound file
    * (same as above, minus "del", "gen", "cfs"). */
   static final String[] INDEX_EXTENSIONS_IN_COMPOUND_FILE = new String[] {
-      "fnm", "fdx", "fdt", "tii", "tis", "frq", "prx",
-      "tvx", "tvd", "tvf", "nrm" 
+    FIELD_INFOS_EXTENSION,
+    FIELDS_INDEX_EXTENSION,
+    FIELDS_EXTENSION,
+    TERMS_INDEX_EXTENSION,
+    TERMS_EXTENSION,
+    FREQ_EXTENSION,
+    PROX_EXTENSION,
+    VECTORS_INDEX_EXTENSION,
+    VECTORS_DOCUMENTS_EXTENSION,
+    VECTORS_FIELDS_EXTENSION,
+    NORMS_EXTENSION
+  };
+
+  static final String[] STORE_INDEX_EXTENSIONS = new String[] {
+    VECTORS_INDEX_EXTENSION,
+    VECTORS_FIELDS_EXTENSION,
+    VECTORS_DOCUMENTS_EXTENSION,
+    FIELDS_INDEX_EXTENSION,
+    FIELDS_EXTENSION
+  };
+
+  static final String[] NON_STORE_INDEX_EXTENSIONS = new String[] {
+    FIELD_INFOS_EXTENSION,
+    FREQ_EXTENSION,
+    PROX_EXTENSION,
+    TERMS_EXTENSION,
+    TERMS_INDEX_EXTENSION,
+    NORMS_EXTENSION
   };
   
   /** File extensions of old-style index files */
   static final String COMPOUND_EXTENSIONS[] = new String[] {
-    "fnm", "frq", "prx", "fdx", "fdt", "tii", "tis"
+    FIELD_INFOS_EXTENSION,
+    FREQ_EXTENSION,
+    PROX_EXTENSION,
+    FIELDS_INDEX_EXTENSION,
+    FIELDS_EXTENSION,
+    TERMS_INDEX_EXTENSION,
+    TERMS_EXTENSION
   };
   
   /** File extensions for term vector support */
   static final String VECTOR_EXTENSIONS[] = new String[] {
-    "tvx", "tvd", "tvf"
+    VECTORS_INDEX_EXTENSION,
+    VECTORS_DOCUMENTS_EXTENSION,
+    VECTORS_FIELDS_EXTENSION
   };
 
   /**
diff --git a/src/java/org/apache/lucene/index/IndexModifier.java b/src/java/org/apache/lucene/index/IndexModifier.java
index 354985c..8bdd230 100644
--- a/src/java/org/apache/lucene/index/IndexModifier.java
+++ b/src/java/org/apache/lucene/index/IndexModifier.java
@@ -203,7 +203,8 @@ public class IndexModifier {
       indexWriter = new IndexWriter(directory, analyzer, false);
       indexWriter.setInfoStream(infoStream);
       indexWriter.setUseCompoundFile(useCompoundFile);
-      indexWriter.setMaxBufferedDocs(maxBufferedDocs);
+      if (maxBufferedDocs != 0)
+        indexWriter.setMaxBufferedDocs(maxBufferedDocs);
       indexWriter.setMaxFieldLength(maxFieldLength);
       indexWriter.setMergeFactor(mergeFactor);
     }
diff --git a/src/java/org/apache/lucene/index/IndexReader.java b/src/java/org/apache/lucene/index/IndexReader.java
index 32aa9b9..30315a1 100644
--- a/src/java/org/apache/lucene/index/IndexReader.java
+++ b/src/java/org/apache/lucene/index/IndexReader.java
@@ -783,7 +783,7 @@ public abstract class IndexReader {
         // KeepOnlyLastCommitDeleter:
         IndexFileDeleter deleter =  new IndexFileDeleter(directory,
                                                          deletionPolicy == null ? new KeepOnlyLastCommitDeletionPolicy() : deletionPolicy,
-                                                         segmentInfos, null);
+                                                         segmentInfos, null, null);
 
         // Checkpoint the state we are about to change, in
         // case we have to roll back:
diff --git a/src/java/org/apache/lucene/index/IndexWriter.java b/src/java/org/apache/lucene/index/IndexWriter.java
index 4934a5d..322fce6 100644
--- a/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/src/java/org/apache/lucene/index/IndexWriter.java
@@ -61,14 +61,19 @@ import java.util.Map.Entry;
   When finished adding, deleting and updating documents, <a href="#close()"><b>close</b></a> should be called.</p>
 
   <p>These changes are buffered in memory and periodically
-  flushed to the {@link Directory} (during the above method calls).  A flush is triggered when there are
-  enough buffered deletes (see {@link
-  #setMaxBufferedDeleteTerms}) or enough added documents
-  (see {@link #setMaxBufferedDocs}) since the last flush,
-  whichever is sooner.  You can also force a flush by
-  calling {@link #flush}.  When a flush occurs, both pending
-  deletes and added documents are flushed to the index.  A
-  flush may also trigger one or more segment merges.</p>
+  flushed to the {@link Directory} (during the above method
+  calls).  A flush is triggered when there are enough
+  buffered deletes (see {@link #setMaxBufferedDeleteTerms})
+  or enough added documents since the last flush, whichever
+  is sooner.  For the added documents, flushing is triggered
+  either by RAM usage of the documents (see {@link
+  #setRAMBufferSizeMB}) or the number of added documents
+  (this is the default; see {@link #setMaxBufferedDocs}).
+  For best indexing speed you should flush by RAM usage with
+  a large RAM buffer.  You can also force a flush by calling
+  {@link #flush}.  When a flush occurs, both pending deletes
+  and added documents are flushed to the index.  A flush may
+  also trigger one or more segment merges.</p>
 
   <a name="autoCommit"></a>
   <p>The optional <code>autoCommit</code> argument to the
@@ -181,7 +186,20 @@ public class IndexWriter {
   /**
    * Default value is 10. Change using {@link #setMaxBufferedDocs(int)}.
    */
+
   public final static int DEFAULT_MAX_BUFFERED_DOCS = 10;
+  /* new merge policy
+  public final static int DEFAULT_MAX_BUFFERED_DOCS = 0;
+  */
+
+  /**
+   * Default value is 0 MB (which means flush only by doc
+   * count).  Change using {@link #setRAMBufferSizeMB}.
+   */
+  public final static double DEFAULT_RAM_BUFFER_SIZE_MB = 0.0;
+  /* new merge policy
+  public final static double DEFAULT_RAM_BUFFER_SIZE_MB = 16.0;
+  */
 
   /**
    * Default value is 1000. Change using {@link #setMaxBufferedDeleteTerms(int)}.
@@ -224,8 +242,7 @@ public class IndexWriter {
   private boolean autoCommit = true;              // false if we should commit only on close
 
   SegmentInfos segmentInfos = new SegmentInfos();       // the segments
-  SegmentInfos ramSegmentInfos = new SegmentInfos();    // the segments in ramDirectory
-  private final RAMDirectory ramDirectory = new RAMDirectory(); // for temp segs
+  private DocumentsWriter docWriter;
   private IndexFileDeleter deleter;
 
   private Lock writeLock;
@@ -621,11 +638,14 @@ public class IndexWriter {
         rollbackSegmentInfos = (SegmentInfos) segmentInfos.clone();
       }
 
+      docWriter = new DocumentsWriter(directory, this);
+      docWriter.setInfoStream(infoStream);
+
       // Default deleter (for backwards compatibility) is
       // KeepOnlyLastCommitDeleter:
       deleter = new IndexFileDeleter(directory,
                                      deletionPolicy == null ? new KeepOnlyLastCommitDeletionPolicy() : deletionPolicy,
-                                     segmentInfos, infoStream);
+                                     segmentInfos, infoStream, docWriter);
 
     } catch (IOException e) {
       this.writeLock.release();
@@ -683,31 +703,64 @@ public class IndexWriter {
     return maxFieldLength;
   }
 
-  /** Determines the minimal number of documents required before the buffered
-   * in-memory documents are merged and a new Segment is created.
-   * Since Documents are merged in a {@link org.apache.lucene.store.RAMDirectory},
-   * large value gives faster indexing.  At the same time, mergeFactor limits
-   * the number of files open in a FSDirectory.
+  /** Determines the minimal number of documents required
+   * before the buffered in-memory documents are flushed as
+   * a new Segment.  Large values generally gives faster
+   * indexing.
    *
-   * <p> The default value is 10.
+   * <p>When this is set, the writer will flush every
+   * maxBufferedDocs added documents and never flush by RAM
+   * usage.</p>
    *
-   * @throws IllegalArgumentException if maxBufferedDocs is smaller than 2
+   * <p> The default value is 0 (writer flushes by RAM
+   * usage).</p>
+   *
+   * @throws IllegalArgumentException if maxBufferedDocs is
+   * smaller than 2
+   * @see #setRAMBufferSizeMB
    */
   public void setMaxBufferedDocs(int maxBufferedDocs) {
     ensureOpen();
     if (maxBufferedDocs < 2)
       throw new IllegalArgumentException("maxBufferedDocs must at least be 2");
-    this.minMergeDocs = maxBufferedDocs;
+    docWriter.setMaxBufferedDocs(maxBufferedDocs);
   }
 
   /**
-   * Returns the number of buffered added documents that will
+   * Returns 0 if this writer is flushing by RAM usage, else
+   * returns the number of buffered added documents that will
    * trigger a flush.
    * @see #setMaxBufferedDocs
    */
   public int getMaxBufferedDocs() {
     ensureOpen();
-    return minMergeDocs;
+    return docWriter.getMaxBufferedDocs();
+  }
+
+  /** Determines the amount of RAM that may be used for
+   * buffering added documents before they are flushed as a
+   * new Segment.  Generally for faster indexing performance
+   * it's best to flush by RAM usage instead of document
+   * count and use as large a RAM buffer as you can.
+   *
+   * <p>When this is set, the writer will flush whenever
+   * buffered documents use this much RAM.</p>
+   *
+   * <p> The default value is {@link #DEFAULT_RAM_BUFFER_SIZE_MB}.</p>
+   */
+  public void setRAMBufferSizeMB(double mb) {
+    if (mb <= 0.0)
+      throw new IllegalArgumentException("ramBufferSize should be > 0.0 MB");
+    docWriter.setRAMBufferSizeMB(mb);
+  }
+
+  /**
+   * Returns 0.0 if this writer is flushing by document
+   * count, else returns the value set by {@link
+   * #setRAMBufferSizeMB}.
+   */
+  public double getRAMBufferSizeMB() {
+    return docWriter.getRAMBufferSizeMB();
   }
 
   /**
@@ -788,6 +841,7 @@ public class IndexWriter {
   public void setInfoStream(PrintStream infoStream) {
     ensureOpen();
     this.infoStream = infoStream;
+    docWriter.setInfoStream(infoStream);
     deleter.setInfoStream(infoStream);
   }
 
@@ -871,7 +925,7 @@ public class IndexWriter {
    */
   public synchronized void close() throws CorruptIndexException, IOException {
     if (!closed) {
-      flushRamSegments();
+      flush(true, true);
 
       if (commitPending) {
         segmentInfos.write(directory);         // now commit changes
@@ -880,18 +934,79 @@ public class IndexWriter {
         rollbackSegmentInfos = null;
       }
 
-      ramDirectory.close();
       if (writeLock != null) {
         writeLock.release();                          // release write lock
         writeLock = null;
       }
       closed = true;
+      docWriter = null;
 
       if(closeDir)
         directory.close();
     }
   }
 
+  /** Tells the docWriter to close its currently open shared
+   *  doc stores (stored fields & vectors files). */
+  private void flushDocStores() throws IOException {
+
+    List files = docWriter.files();
+
+    if (files.size() > 0) {
+      String docStoreSegment;
+
+      boolean success = false;
+      try {
+        docStoreSegment = docWriter.closeDocStore();
+        success = true;
+      } finally {
+        if (!success)
+          docWriter.abort();
+      }
+
+      if (useCompoundFile && docStoreSegment != null) {
+        // Now build compound doc store file
+        checkpoint();
+
+        success = false;
+
+        final int numSegments = segmentInfos.size();
+
+        try {
+          CompoundFileWriter cfsWriter = new CompoundFileWriter(directory, docStoreSegment + "." + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION);
+          final int size = files.size();
+          for(int i=0;i<size;i++)
+            cfsWriter.addFile((String) files.get(i));
+      
+          // Perform the merge
+          cfsWriter.close();
+
+          for(int i=0;i<numSegments;i++) {
+            SegmentInfo si = segmentInfos.info(i);
+            if (si.getDocStoreOffset() != -1 &&
+                si.getDocStoreSegment().equals(docStoreSegment))
+              si.setDocStoreIsCompoundFile(true);
+          }
+          checkpoint();
+          success = true;
+        } finally {
+          if (!success) {
+            // Rollback to no compound file
+            for(int i=0;i<numSegments;i++) {
+              SegmentInfo si = segmentInfos.info(i);
+              if (si.getDocStoreOffset() != -1 &&
+                  si.getDocStoreSegment().equals(docStoreSegment))
+                si.setDocStoreIsCompoundFile(false);
+            }
+            deleter.refresh();
+          }
+        }
+
+        deleter.checkpoint(segmentInfos, false);
+      }
+    }
+  }
+
   /** Release the write lock, if needed. */
   protected void finalize() throws Throwable {
     try {
@@ -916,11 +1031,10 @@ public class IndexWriter {
     return analyzer;
   }
 
-
   /** Returns the number of documents currently in this index. */
   public synchronized int docCount() {
     ensureOpen();
-    int count = ramSegmentInfos.size();
+    int count = docWriter.getNumDocsInRAM();
     for (int i = 0; i < segmentInfos.size(); i++) {
       SegmentInfo si = segmentInfos.info(i);
       count += si.docCount;
@@ -998,22 +1112,8 @@ public class IndexWriter {
    */
   public void addDocument(Document doc, Analyzer analyzer) throws CorruptIndexException, IOException {
     ensureOpen();
-    SegmentInfo newSegmentInfo = buildSingleDocSegment(doc, analyzer);
-    synchronized (this) {
-      ramSegmentInfos.addElement(newSegmentInfo);
-      maybeFlushRamSegments();
-    }
-  }
-
-  SegmentInfo buildSingleDocSegment(Document doc, Analyzer analyzer)
-      throws CorruptIndexException, IOException {
-    DocumentWriter dw = new DocumentWriter(ramDirectory, analyzer, this);
-    dw.setInfoStream(infoStream);
-    String segmentName = newRamSegmentName();
-    dw.addDocument(segmentName, doc);
-    SegmentInfo si = new SegmentInfo(segmentName, 1, ramDirectory, false, false);
-    si.setNumFields(dw.getNumFields());
-    return si;
+    if (docWriter.addDocument(doc, analyzer))
+      flush(true, false);
   }
 
   /**
@@ -1025,7 +1125,7 @@ public class IndexWriter {
   public synchronized void deleteDocuments(Term term) throws CorruptIndexException, IOException {
     ensureOpen();
     bufferDeleteTerm(term);
-    maybeFlushRamSegments();
+    maybeFlush();
   }
 
   /**
@@ -1041,7 +1141,7 @@ public class IndexWriter {
     for (int i = 0; i < terms.length; i++) {
       bufferDeleteTerm(terms[i]);
     }
-    maybeFlushRamSegments();
+    maybeFlush();
   }
 
   /**
@@ -1077,16 +1177,13 @@ public class IndexWriter {
   public void updateDocument(Term term, Document doc, Analyzer analyzer)
       throws CorruptIndexException, IOException {
     ensureOpen();
-    SegmentInfo newSegmentInfo = buildSingleDocSegment(doc, analyzer);
     synchronized (this) {
       bufferDeleteTerm(term);
-      ramSegmentInfos.addElement(newSegmentInfo);
-      maybeFlushRamSegments();
     }
-  }
-
-  final synchronized String newRamSegmentName() {
-    return "_ram_" + Integer.toString(ramSegmentInfos.counter++, Character.MAX_RADIX);
+    if (docWriter.addDocument(doc, analyzer))
+      flush(true, false);
+    else
+      maybeFlush();
   }
 
   // for test purpose
@@ -1095,8 +1192,8 @@ public class IndexWriter {
   }
 
   // for test purpose
-  final synchronized int getRamSegmentCount(){
-    return ramSegmentInfos.size();
+  final synchronized int getNumBufferedDocuments(){
+    return docWriter.getNumDocsInRAM();
   }
 
   // for test purpose
@@ -1108,7 +1205,7 @@ public class IndexWriter {
     }
   }
 
-  final synchronized String newSegmentName() {
+  final String newSegmentName() {
     return "_" + Integer.toString(segmentInfos.counter++, Character.MAX_RADIX);
   }
 
@@ -1125,17 +1222,10 @@ public class IndexWriter {
    */
   private int mergeFactor = DEFAULT_MERGE_FACTOR;
 
-  /** Determines the minimal number of documents required before the buffered
-   * in-memory documents are merging and a new Segment is created.
-   * Since Documents are merged in a {@link org.apache.lucene.store.RAMDirectory},
-   * large value gives faster indexing.  At the same time, mergeFactor limits
-   * the number of files open in a FSDirectory.
-   *
-   * <p> The default value is {@link #DEFAULT_MAX_BUFFERED_DOCS}.
-
+  /** Determines amount of RAM usage by the buffered docs at
+   * which point we trigger a flush to the index.
    */
-  private int minMergeDocs = DEFAULT_MAX_BUFFERED_DOCS;
-
+  private double ramBufferSize = DEFAULT_RAM_BUFFER_SIZE_MB*1024F*1024F;
 
   /** Determines the largest number of documents ever merged by addDocument().
    * Small values (e.g., less than 10,000) are best for interactive indexing,
@@ -1151,6 +1241,7 @@ public class IndexWriter {
 
    */
   private PrintStream infoStream = null;
+
   private static PrintStream defaultInfoStream = null;
 
   /** Merges all segments together into a single segment,
@@ -1219,16 +1310,16 @@ public class IndexWriter {
   */
   public synchronized void optimize() throws CorruptIndexException, IOException {
     ensureOpen();
-    flushRamSegments();
+    flush();
     while (segmentInfos.size() > 1 ||
            (segmentInfos.size() == 1 &&
             (SegmentReader.hasDeletions(segmentInfos.info(0)) ||
              SegmentReader.hasSeparateNorms(segmentInfos.info(0)) ||
              segmentInfos.info(0).dir != directory ||
              (useCompoundFile &&
-              (!SegmentReader.usesCompoundFile(segmentInfos.info(0))))))) {
+              !segmentInfos.info(0).getUseCompoundFile())))) {
       int minSegment = segmentInfos.size() - mergeFactor;
-      mergeSegments(segmentInfos, minSegment < 0 ? 0 : minSegment, segmentInfos.size());
+      mergeSegments(minSegment < 0 ? 0 : minSegment, segmentInfos.size());
     }
   }
 
@@ -1245,7 +1336,7 @@ public class IndexWriter {
     localRollbackSegmentInfos = (SegmentInfos) segmentInfos.clone();
     localAutoCommit = autoCommit;
     if (localAutoCommit) {
-      flushRamSegments();
+      flush();
       // Turn off auto-commit during our local transaction:
       autoCommit = false;
     } else
@@ -1335,16 +1426,18 @@ public class IndexWriter {
       segmentInfos.clear();
       segmentInfos.addAll(rollbackSegmentInfos);
 
+      docWriter.abort();
+
       // Ask deleter to locate unreferenced files & remove
       // them:
       deleter.checkpoint(segmentInfos, false);
       deleter.refresh();
 
-      ramSegmentInfos = new SegmentInfos();
       bufferedDeleteTerms.clear();
       numBufferedDeleteTerms = 0;
 
       commitPending = false;
+      docWriter.abort();
       close();
 
     } else {
@@ -1439,7 +1532,7 @@ public class IndexWriter {
         for (int base = start; base < segmentInfos.size(); base++) {
           int end = Math.min(segmentInfos.size(), base+mergeFactor);
           if (end-base > 1) {
-            mergeSegments(segmentInfos, base, end);
+            mergeSegments(base, end);
           }
         }
       }
@@ -1479,7 +1572,7 @@ public class IndexWriter {
     // segments in S may not since they could come from multiple indexes.
     // Here is the merge algorithm for addIndexesNoOptimize():
     //
-    // 1 Flush ram segments.
+    // 1 Flush ram.
     // 2 Consider a combined sequence with segments from T followed
     //   by segments from S (same as current addIndexes(Directory[])).
     // 3 Assume the highest level for segments in S is h. Call
@@ -1500,13 +1593,18 @@ public class IndexWriter {
     // copy a segment, which may cause doc count to change because deleted
     // docs are garbage collected.
 
-    // 1 flush ram segments
+    // 1 flush ram
 
     ensureOpen();
-    flushRamSegments();
+    flush();
 
     // 2 copy segment infos and find the highest level from dirs
-    int startUpperBound = minMergeDocs;
+    int startUpperBound = docWriter.getMaxBufferedDocs();
+
+    /* new merge policy
+    if (startUpperBound == 0)
+      startUpperBound = 10;
+    */
 
     boolean success = false;
 
@@ -1566,7 +1664,7 @@ public class IndexWriter {
 
         // copy those segments from S
         for (int i = segmentCount - numSegmentsToCopy; i < segmentCount; i++) {
-          mergeSegments(segmentInfos, i, i + 1);
+          mergeSegments(i, i + 1);
         }
         if (checkNonDecreasingLevels(segmentCount - numSegmentsToCopy)) {
           success = true;
@@ -1575,7 +1673,7 @@ public class IndexWriter {
       }
 
       // invariants do not hold, simply merge those segments
-      mergeSegments(segmentInfos, segmentCount - numTailSegments, segmentCount);
+      mergeSegments(segmentCount - numTailSegments, segmentCount);
 
       // maybe merge segments again if necessary
       if (segmentInfos.info(segmentInfos.size() - 1).docCount > startUpperBound) {
@@ -1637,7 +1735,8 @@ public class IndexWriter {
         }
 
         segmentInfos.setSize(0);                      // pop old infos & add new
-        info = new SegmentInfo(mergedName, docCount, directory, false, true);
+        info = new SegmentInfo(mergedName, docCount, directory, false, true,
+                               -1, null, false);
         segmentInfos.addElement(info);
 
         success = true;
@@ -1720,29 +1819,19 @@ public class IndexWriter {
    * buffered added documents or buffered deleted terms are
    * large enough.
    */
-  protected final void maybeFlushRamSegments() throws CorruptIndexException, IOException {
-    // A flush is triggered if enough new documents are buffered or
-    // if enough delete terms are buffered
-    if (ramSegmentInfos.size() >= minMergeDocs || numBufferedDeleteTerms >= maxBufferedDeleteTerms) {
-      flushRamSegments();
-    }
+  protected final synchronized void maybeFlush() throws CorruptIndexException, IOException {
+    // We only check for flush due to number of buffered
+    // delete terms, because triggering of a flush due to
+    // too many added documents is handled by
+    // DocumentsWriter
+    if (numBufferedDeleteTerms >= maxBufferedDeleteTerms && docWriter.setFlushPending())
+      flush(true, false);
   }
 
-  /** Expert:  Flushes all RAM-resident segments (buffered documents), then may merge segments. */
-  private final synchronized void flushRamSegments() throws CorruptIndexException, IOException {
-    flushRamSegments(true);
-  }
-    
-  /** Expert:  Flushes all RAM-resident segments (buffered documents), 
-   *           then may merge segments if triggerMerge==true. */
-  protected final synchronized void flushRamSegments(boolean triggerMerge) 
-      throws CorruptIndexException, IOException {
-    if (ramSegmentInfos.size() > 0 || bufferedDeleteTerms.size() > 0) {
-      mergeSegments(ramSegmentInfos, 0, ramSegmentInfos.size());
-      if (triggerMerge) maybeMergeSegments(minMergeDocs);
-    }
+  public final synchronized void flush() throws CorruptIndexException, IOException {  
+    flush(true, false);
   }
-  
+
   /**
    * Flush all in-memory buffered updates (adds and deletes)
    * to the Directory. 
@@ -1751,9 +1840,158 @@ public class IndexWriter {
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
    */
-  public final synchronized void flush() throws CorruptIndexException, IOException {
+  public final synchronized void flush(boolean triggerMerge, boolean flushDocStores) throws CorruptIndexException, IOException {
     ensureOpen();
-    flushRamSegments();
+
+    // Make sure no threads are actively adding a document
+    docWriter.pauseAllThreads();
+
+    try {
+
+      SegmentInfo newSegment = null;
+
+      final int numDocs = docWriter.getNumDocsInRAM();
+
+      // Always flush docs if there are any
+      boolean flushDocs = numDocs > 0;
+
+      // With autoCommit=true we always must flush the doc
+      // stores when we flush
+      flushDocStores |= autoCommit;
+      String docStoreSegment = docWriter.getDocStoreSegment();
+      if (docStoreSegment == null)
+        flushDocStores = false;
+
+      // Always flush deletes if there are any delete terms.
+      // TODO: when autoCommit=false we don't have to flush
+      // deletes with every flushed segment; we can save
+      // CPU/IO by buffering longer & flushing deletes only
+      // when they are full or writer is being closed.  We
+      // have to fix the "applyDeletesSelectively" logic to
+      // apply to more than just the last flushed segment
+      boolean flushDeletes = bufferedDeleteTerms.size() > 0;
+
+      if (infoStream != null)
+        infoStream.println("  flush: flushDocs=" + flushDocs +
+                           " flushDeletes=" + flushDeletes +
+                           " flushDocStores=" + flushDocStores +
+                           " numDocs=" + numDocs);
+
+      int docStoreOffset = docWriter.getDocStoreOffset();
+      boolean docStoreIsCompoundFile = false;
+
+      // Check if the doc stores must be separately flushed
+      // because other segments, besides the one we are about
+      // to flush, reference it
+      if (flushDocStores && (!flushDocs || !docWriter.getSegment().equals(docWriter.getDocStoreSegment()))) {
+        // We must separately flush the doc store
+        if (infoStream != null)
+          infoStream.println("  flush shared docStore segment " + docStoreSegment);
+      
+        flushDocStores();
+        flushDocStores = false;
+        docStoreIsCompoundFile = useCompoundFile;
+      }
+
+      String segment = docWriter.getSegment();
+
+      if (flushDocs || flushDeletes) {
+
+        SegmentInfos rollback = null;
+
+        if (flushDeletes)
+          rollback = (SegmentInfos) segmentInfos.clone();
+
+        boolean success = false;
+
+        try {
+          if (flushDocs) {
+
+            if (0 == docStoreOffset && flushDocStores) {
+              // This means we are flushing private doc stores
+              // with this segment, so it will not be shared
+              // with other segments
+              assert docStoreSegment != null;
+              assert docStoreSegment.equals(segment);
+              docStoreOffset = -1;
+              docStoreIsCompoundFile = false;
+              docStoreSegment = null;
+            }
+
+            int flushedDocCount = docWriter.flush(flushDocStores);
+          
+            newSegment = new SegmentInfo(segment,
+                                         flushedDocCount,
+                                         directory, false, true,
+                                         docStoreOffset, docStoreSegment,
+                                         docStoreIsCompoundFile);
+            segmentInfos.addElement(newSegment);
+          }
+
+          if (flushDeletes) {
+            // we should be able to change this so we can
+            // buffer deletes longer and then flush them to
+            // multiple flushed segments, when
+            // autoCommit=false
+            applyDeletes(flushDocs);
+            doAfterFlush();
+          }
+
+          checkpoint();
+          success = true;
+        } finally {
+          if (!success) {
+            if (flushDeletes) {
+              // Fully replace the segmentInfos since flushed
+              // deletes could have changed any of the
+              // SegmentInfo instances:
+              segmentInfos.clear();
+              segmentInfos.addAll(rollback);
+            } else {
+              // Remove segment we added, if any:
+              if (newSegment != null && 
+                  segmentInfos.size() > 0 && 
+                  segmentInfos.info(segmentInfos.size()-1) == newSegment)
+                segmentInfos.remove(segmentInfos.size()-1);
+            }
+            if (flushDocs)
+              docWriter.abort();
+            deleter.checkpoint(segmentInfos, false);
+            deleter.refresh();
+          }
+        }
+
+        deleter.checkpoint(segmentInfos, autoCommit);
+
+        if (flushDocs && useCompoundFile) {
+          success = false;
+          try {
+            docWriter.createCompoundFile(segment);
+            newSegment.setUseCompoundFile(true);
+            checkpoint();
+            success = true;
+          } finally {
+            if (!success) {
+              newSegment.setUseCompoundFile(false);
+              deleter.refresh();
+            }
+          }
+
+          deleter.checkpoint(segmentInfos, autoCommit);
+        }
+
+        /* new merge policy
+        if (0 == docWriter.getMaxBufferedDocs())
+          maybeMergeSegments(mergeFactor * numDocs / 2);
+        else
+          maybeMergeSegments(docWriter.getMaxBufferedDocs());
+        */
+        maybeMergeSegments(docWriter.getMaxBufferedDocs());
+      }
+    } finally {
+      docWriter.clearFlushPending();
+      docWriter.resumeAllThreads();
+    }
   }
 
   /** Expert:  Return the total size of all index files currently cached in memory.
@@ -1761,15 +1999,15 @@ public class IndexWriter {
    */
   public final long ramSizeInBytes() {
     ensureOpen();
-    return ramDirectory.sizeInBytes();
+    return docWriter.getRAMUsed();
   }
 
   /** Expert:  Return the number of documents whose segments are currently cached in memory.
-   * Useful when calling flushRamSegments()
+   * Useful when calling flush()
    */
   public final synchronized int numRamDocs() {
     ensureOpen();
-    return ramSegmentInfos.size();
+    return docWriter.getNumDocsInRAM();
   }
   
   /** Incremental segment merger.  */
@@ -1777,6 +2015,10 @@ public class IndexWriter {
     long lowerBound = -1;
     long upperBound = startUpperBound;
 
+    /* new merge policy
+    if (upperBound == 0) upperBound = 10;
+    */
+
     while (upperBound < maxMergeDocs) {
       int minSegment = segmentInfos.size();
       int maxSegment = -1;
@@ -1808,7 +2050,7 @@ public class IndexWriter {
         while (numSegments >= mergeFactor) {
           // merge the leftmost* mergeFactor segments
 
-          int docCount = mergeSegments(segmentInfos, minSegment, minSegment + mergeFactor);
+          int docCount = mergeSegments(minSegment, minSegment + mergeFactor);
           numSegments -= mergeFactor;
 
           if (docCount > upperBound) {
@@ -1837,39 +2079,108 @@ public class IndexWriter {
    * Merges the named range of segments, replacing them in the stack with a
    * single segment.
    */
-  private final int mergeSegments(SegmentInfos sourceSegments, int minSegment, int end)
+
+  private final int mergeSegments(int minSegment, int end)
     throws CorruptIndexException, IOException {
 
-    // We may be called solely because there are deletes
-    // pending, in which case doMerge is false:
-    boolean doMerge = end > 0;
     final String mergedName = newSegmentName();
+    
     SegmentMerger merger = null;
-
-    final List ramSegmentsToDelete = new ArrayList();
-
     SegmentInfo newSegment = null;
 
     int mergedDocCount = 0;
-    boolean anyDeletes = (bufferedDeleteTerms.size() != 0);
 
     // This is try/finally to make sure merger's readers are closed:
     try {
 
-      if (doMerge) {
-        if (infoStream != null) infoStream.print("merging segments");
-        merger = new SegmentMerger(this, mergedName);
-
-        for (int i = minSegment; i < end; i++) {
-          SegmentInfo si = sourceSegments.info(i);
-          if (infoStream != null)
-            infoStream.print(" " + si.name + " (" + si.docCount + " docs)");
-          IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE); // no need to set deleter (yet)
-          merger.add(reader);
-          if (reader.directory() == this.ramDirectory) {
-            ramSegmentsToDelete.add(si);
-          }
-        }
+      if (infoStream != null) infoStream.print("merging segments");
+
+      // Check whether this merge will allow us to skip
+      // merging the doc stores (stored field & vectors).
+      // This is a very substantial optimization (saves tons
+      // of IO) that can only be applied with
+      // autoCommit=false.
+
+      Directory lastDir = directory;
+      String lastDocStoreSegment = null;
+      boolean mergeDocStores = false;
+      boolean doFlushDocStore = false;
+      int next = -1;
+
+      // Test each segment to be merged
+      for (int i = minSegment; i < end; i++) {
+        SegmentInfo si = segmentInfos.info(i);
+
+        // If it has deletions we must merge the doc stores
+        if (si.hasDeletions())
+          mergeDocStores = true;
+
+        // If it has its own (private) doc stores we must
+        // merge the doc stores
+        if (-1 == si.getDocStoreOffset())
+          mergeDocStores = true;
+
+        // If it has a different doc store segment than
+        // previous segments, we must merge the doc stores
+        String docStoreSegment = si.getDocStoreSegment();
+        if (docStoreSegment == null)
+          mergeDocStores = true;
+        else if (lastDocStoreSegment == null)
+          lastDocStoreSegment = docStoreSegment;
+        else if (!lastDocStoreSegment.equals(docStoreSegment))
+          mergeDocStores = true;
+
+        // Segments' docScoreOffsets must be in-order,
+        // contiguous.  For the default merge policy now
+        // this will always be the case but for an arbitrary
+        // merge policy this may not be the case
+        if (-1 == next)
+          next = si.getDocStoreOffset() + si.docCount;
+        else if (next != si.getDocStoreOffset())
+          mergeDocStores = true;
+        else
+          next = si.getDocStoreOffset() + si.docCount;
+      
+        // If the segment comes from a different directory
+        // we must merge
+        if (lastDir != si.dir)
+          mergeDocStores = true;
+
+        // If the segment is referencing the current "live"
+        // doc store outputs then we must merge
+        if (si.getDocStoreOffset() != -1 && si.getDocStoreSegment().equals(docWriter.getDocStoreSegment()))
+          doFlushDocStore = true;
+      }
+
+      final int docStoreOffset;
+      final String docStoreSegment;
+      final boolean docStoreIsCompoundFile;
+      if (mergeDocStores) {
+        docStoreOffset = -1;
+        docStoreSegment = null;
+        docStoreIsCompoundFile = false;
+      } else {
+        SegmentInfo si = segmentInfos.info(minSegment);        
+        docStoreOffset = si.getDocStoreOffset();
+        docStoreSegment = si.getDocStoreSegment();
+        docStoreIsCompoundFile = si.getDocStoreIsCompoundFile();
+      }
+
+      if (mergeDocStores && doFlushDocStore)
+        // SegmentMerger intends to merge the doc stores
+        // (stored fields, vectors), and at least one of the
+        // segments to be merged refers to the currently
+        // live doc stores.
+        flushDocStores();
+
+      merger = new SegmentMerger(this, mergedName);
+
+      for (int i = minSegment; i < end; i++) {
+        SegmentInfo si = segmentInfos.info(i);
+        if (infoStream != null)
+          infoStream.print(" " + si.name + " (" + si.docCount + " docs)");
+        IndexReader reader = SegmentReader.get(si, MERGE_READ_BUFFER_SIZE, mergeDocStores); // no need to set deleter (yet)
+        merger.add(reader);
       }
 
       SegmentInfos rollback = null;
@@ -1879,65 +2190,32 @@ public class IndexWriter {
       // if we hit exception when doing the merge:
       try {
 
-        if (doMerge) {
-          mergedDocCount = merger.merge();
-
-          if (infoStream != null) {
-            infoStream.println(" into "+mergedName+" ("+mergedDocCount+" docs)");
-          }
+        mergedDocCount = merger.merge(mergeDocStores);
 
-          newSegment = new SegmentInfo(mergedName, mergedDocCount,
-                                       directory, false, true);
+        if (infoStream != null) {
+          infoStream.println(" into "+mergedName+" ("+mergedDocCount+" docs)");
         }
+
+        newSegment = new SegmentInfo(mergedName, mergedDocCount,
+                                     directory, false, true,
+                                     docStoreOffset,
+                                     docStoreSegment,
+                                     docStoreIsCompoundFile);
         
-        if (sourceSegments != ramSegmentInfos || anyDeletes) {
-          // Now save the SegmentInfo instances that
-          // we are replacing:
-          rollback = (SegmentInfos) segmentInfos.clone();
-        }
+        rollback = (SegmentInfos) segmentInfos.clone();
 
-        if (doMerge) {
-          if (sourceSegments == ramSegmentInfos) {
-            segmentInfos.addElement(newSegment);
-          } else {
-            for (int i = end-1; i > minSegment; i--)     // remove old infos & add new
-              sourceSegments.remove(i);
+        for (int i = end-1; i > minSegment; i--)     // remove old infos & add new
+          segmentInfos.remove(i);
 
-            segmentInfos.set(minSegment, newSegment);
-          }
-        }
+        segmentInfos.set(minSegment, newSegment);
 
-        if (sourceSegments == ramSegmentInfos) {
-          maybeApplyDeletes(doMerge);
-          doAfterFlush();
-        }
-        
         checkpoint();
 
         success = true;
 
       } finally {
-
-        if (success) {
-          // The non-ram-segments case is already committed
-          // (above), so all the remains for ram segments case
-          // is to clear the ram segments:
-          if (sourceSegments == ramSegmentInfos) {
-            ramSegmentInfos.removeAllElements();
-          }
-        } else {
-
-          // Must rollback so our state matches index:
-          if (sourceSegments == ramSegmentInfos && !anyDeletes) {
-            // Simple case: newSegment may or may not have
-            // been added to the end of our segment infos,
-            // so just check & remove if so:
-            if (newSegment != null && 
-                segmentInfos.size() > 0 && 
-                segmentInfos.info(segmentInfos.size()-1) == newSegment) {
-              segmentInfos.remove(segmentInfos.size()-1);
-            }
-          } else if (rollback != null) {
+        if (!success) {
+          if (rollback != null) {
             // Rollback the individual SegmentInfo
             // instances, but keep original SegmentInfos
             // instance (so we don't try to write again the
@@ -1952,16 +2230,13 @@ public class IndexWriter {
       }
     } finally {
       // close readers before we attempt to delete now-obsolete segments
-      if (doMerge) merger.closeReaders();
+      merger.closeReaders();
     }
 
-    // Delete the RAM segments
-    deleter.deleteDirect(ramDirectory, ramSegmentsToDelete);
-
     // Give deleter a chance to remove files now.
     deleter.checkpoint(segmentInfos, autoCommit);
 
-    if (useCompoundFile && doMerge) {
+    if (useCompoundFile) {
 
       boolean success = false;
 
@@ -1988,19 +2263,23 @@ public class IndexWriter {
   }
 
   // Called during flush to apply any buffered deletes.  If
-  // doMerge is true then a new segment was just created and
-  // flushed from the ram segments.
-  private final void maybeApplyDeletes(boolean doMerge) throws CorruptIndexException, IOException {
+  // flushedNewSegment is true then a new segment was just
+  // created and flushed from the ram segments, so we will
+  // selectively apply the deletes to that new segment.
+  private final void applyDeletes(boolean flushedNewSegment) throws CorruptIndexException, IOException {
 
     if (bufferedDeleteTerms.size() > 0) {
       if (infoStream != null)
         infoStream.println("flush " + numBufferedDeleteTerms + " buffered deleted terms on "
                            + segmentInfos.size() + " segments.");
 
-      if (doMerge) {
+      if (flushedNewSegment) {
         IndexReader reader = null;
         try {
-          reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1));
+          // Open readers w/o opening the stored fields /
+          // vectors because these files may still be held
+          // open for writing by docWriter
+          reader = SegmentReader.get(segmentInfos.info(segmentInfos.size() - 1), false);
 
           // Apply delete terms to the segment just flushed from ram
           // apply appropriately so that a delete term is only applied to
@@ -2018,14 +2297,14 @@ public class IndexWriter {
       }
 
       int infosEnd = segmentInfos.size();
-      if (doMerge) {
+      if (flushedNewSegment) {
         infosEnd--;
       }
 
       for (int i = 0; i < infosEnd; i++) {
         IndexReader reader = null;
         try {
-          reader = SegmentReader.get(segmentInfos.info(i));
+          reader = SegmentReader.get(segmentInfos.info(i), false);
 
           // Apply delete terms to disk segments
           // except the one just flushed from ram.
@@ -2049,7 +2328,12 @@ public class IndexWriter {
 
   private final boolean checkNonDecreasingLevels(int start) {
     int lowerBound = -1;
-    int upperBound = minMergeDocs;
+    int upperBound = docWriter.getMaxBufferedDocs();
+
+    /* new merge policy
+    if (upperBound == 0)
+      upperBound = 10;
+    */
 
     for (int i = segmentInfos.size() - 1; i >= start; i--) {
       int docCount = segmentInfos.info(i).docCount;
@@ -2098,10 +2382,11 @@ public class IndexWriter {
   // well as the disk segments.
   private void bufferDeleteTerm(Term term) {
     Num num = (Num) bufferedDeleteTerms.get(term);
+    int numDoc = docWriter.getNumDocsInRAM();
     if (num == null) {
-      bufferedDeleteTerms.put(term, new Num(ramSegmentInfos.size()));
+      bufferedDeleteTerms.put(term, new Num(numDoc));
     } else {
-      num.setNum(ramSegmentInfos.size());
+      num.setNum(numDoc);
     }
     numBufferedDeleteTerms++;
   }
diff --git a/src/java/org/apache/lucene/index/SegmentInfo.java b/src/java/org/apache/lucene/index/SegmentInfo.java
index 36a503f..4cfa95f 100644
--- a/src/java/org/apache/lucene/index/SegmentInfo.java
+++ b/src/java/org/apache/lucene/index/SegmentInfo.java
@@ -65,6 +65,12 @@ final class SegmentInfo {
   private List files;                             // cached list of files that this segment uses
                                                   // in the Directory
 
+  private int docStoreOffset;                     // if this segment shares stored fields & vectors, this
+                                                  // offset is where in that file this segment's docs begin
+  private String docStoreSegment;                 // name used to derive fields/vectors file we share with
+                                                  // other segments
+  private boolean docStoreIsCompoundFile;         // whether doc store files are stored in compound file (*.cfx)
+
   public SegmentInfo(String name, int docCount, Directory dir) {
     this.name = name;
     this.docCount = docCount;
@@ -73,13 +79,25 @@ final class SegmentInfo {
     isCompoundFile = CHECK_DIR;
     preLockless = true;
     hasSingleNormFile = false;
+    docStoreOffset = -1;
+    docStoreSegment = name;
+    docStoreIsCompoundFile = false;
   }
 
   public SegmentInfo(String name, int docCount, Directory dir, boolean isCompoundFile, boolean hasSingleNormFile) { 
+    this(name, docCount, dir, isCompoundFile, hasSingleNormFile, -1, null, false);
+  }
+
+  public SegmentInfo(String name, int docCount, Directory dir, boolean isCompoundFile, boolean hasSingleNormFile,
+                     int docStoreOffset, String docStoreSegment, boolean docStoreIsCompoundFile) { 
     this(name, docCount, dir);
     this.isCompoundFile = (byte) (isCompoundFile ? YES : NO);
     this.hasSingleNormFile = hasSingleNormFile;
     preLockless = false;
+    this.docStoreOffset = docStoreOffset;
+    this.docStoreSegment = docStoreSegment;
+    this.docStoreIsCompoundFile = docStoreIsCompoundFile;
+    assert docStoreOffset == -1 || docStoreSegment != null;
   }
 
   /**
@@ -92,6 +110,8 @@ final class SegmentInfo {
     dir = src.dir;
     preLockless = src.preLockless;
     delGen = src.delGen;
+    docStoreOffset = src.docStoreOffset;
+    docStoreIsCompoundFile = src.docStoreIsCompoundFile;
     if (src.normGen == null) {
       normGen = null;
     } else {
@@ -116,6 +136,20 @@ final class SegmentInfo {
     docCount = input.readInt();
     if (format <= SegmentInfos.FORMAT_LOCKLESS) {
       delGen = input.readLong();
+      if (format <= SegmentInfos.FORMAT_SHARED_DOC_STORE) {
+        docStoreOffset = input.readInt();
+        if (docStoreOffset != -1) {
+          docStoreSegment = input.readString();
+          docStoreIsCompoundFile = (1 == input.readByte());
+        } else {
+          docStoreSegment = name;
+          docStoreIsCompoundFile = false;
+        }
+      } else {
+        docStoreOffset = -1;
+        docStoreSegment = name;
+        docStoreIsCompoundFile = false;
+      }
       if (format <= SegmentInfos.FORMAT_SINGLE_NORM_FILE) {
         hasSingleNormFile = (1 == input.readByte());
       } else {
@@ -138,6 +172,9 @@ final class SegmentInfo {
       isCompoundFile = CHECK_DIR;
       preLockless = true;
       hasSingleNormFile = false;
+      docStoreOffset = -1;
+      docStoreIsCompoundFile = false;
+      docStoreSegment = null;
     }
   }
   
@@ -368,6 +405,28 @@ final class SegmentInfo {
       return dir.fileExists(name + "." + IndexFileNames.COMPOUND_FILE_EXTENSION);
     }
   }
+
+  int getDocStoreOffset() {
+    return docStoreOffset;
+  }
+  
+  boolean getDocStoreIsCompoundFile() {
+    return docStoreIsCompoundFile;
+  }
+  
+  void setDocStoreIsCompoundFile(boolean v) {
+    docStoreIsCompoundFile = v;
+    files = null;
+  }
+  
+  String getDocStoreSegment() {
+    return docStoreSegment;
+  }
+  
+  void setDocStoreOffset(int offset) {
+    docStoreOffset = offset;
+    files = null;
+  }
   
   /**
    * Save this segment's info.
@@ -377,6 +436,12 @@ final class SegmentInfo {
     output.writeString(name);
     output.writeInt(docCount);
     output.writeLong(delGen);
+    output.writeInt(docStoreOffset);
+    if (docStoreOffset != -1) {
+      output.writeString(docStoreSegment);
+      output.writeByte((byte) (docStoreIsCompoundFile ? 1:0));
+    }
+
     output.writeByte((byte) (hasSingleNormFile ? 1:0));
     if (normGen == null) {
       output.writeInt(NO);
@@ -389,6 +454,11 @@ final class SegmentInfo {
     output.writeByte(isCompoundFile);
   }
 
+  private void addIfExists(List files, String fileName) throws IOException {
+    if (dir.fileExists(fileName))
+      files.add(fileName);
+  }
+
   /*
    * Return all files referenced by this SegmentInfo.  The
    * returns List is a locally cached List so you should not
@@ -409,13 +479,28 @@ final class SegmentInfo {
     if (useCompoundFile) {
       files.add(name + "." + IndexFileNames.COMPOUND_FILE_EXTENSION);
     } else {
-      for (int i = 0; i < IndexFileNames.INDEX_EXTENSIONS_IN_COMPOUND_FILE.length; i++) {
-        String ext = IndexFileNames.INDEX_EXTENSIONS_IN_COMPOUND_FILE[i];
-        String fileName = name + "." + ext;
-        if (dir.fileExists(fileName)) {
-          files.add(fileName);
-        }
+      final String[] exts = IndexFileNames.NON_STORE_INDEX_EXTENSIONS;
+      for(int i=0;i<exts.length;i++)
+        addIfExists(files, name + "." + exts[i]);
+    }
+
+    if (docStoreOffset != -1) {
+      // We are sharing doc stores (stored fields, term
+      // vectors) with other segments
+      assert docStoreSegment != null;
+      if (docStoreIsCompoundFile) {
+        files.add(docStoreSegment + "." + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION);
+      } else {
+        final String[] exts = IndexFileNames.STORE_INDEX_EXTENSIONS;
+        for(int i=0;i<exts.length;i++)
+          addIfExists(files, docStoreSegment + "." + exts[i]);
       }
+    } else if (!useCompoundFile) {
+      // We are not sharing, and, these files were not
+      // included in the compound file
+      final String[] exts = IndexFileNames.STORE_INDEX_EXTENSIONS;
+      for(int i=0;i<exts.length;i++)
+        addIfExists(files, name + "." + exts[i]);
     }
 
     String delFileName = IndexFileNames.fileNameFromGeneration(name, "." + IndexFileNames.DELETES_EXTENSION, delGen);
diff --git a/src/java/org/apache/lucene/index/SegmentInfos.java b/src/java/org/apache/lucene/index/SegmentInfos.java
index 148da67..b2ccfce 100644
--- a/src/java/org/apache/lucene/index/SegmentInfos.java
+++ b/src/java/org/apache/lucene/index/SegmentInfos.java
@@ -51,8 +51,12 @@ final class SegmentInfos extends Vector {
    */
   public static final int FORMAT_SINGLE_NORM_FILE = -3;
 
+  /** This format allows multiple segments to share a single
+   * vectors and stored fields file. */
+  public static final int FORMAT_SHARED_DOC_STORE = -4;
+
   /* This must always point to the most recent file format. */
-  private static final int CURRENT_FORMAT = FORMAT_SINGLE_NORM_FILE;
+  private static final int CURRENT_FORMAT = FORMAT_SHARED_DOC_STORE;
   
   public int counter = 0;    // used to name new segments
   /**
diff --git a/src/java/org/apache/lucene/index/SegmentMerger.java b/src/java/org/apache/lucene/index/SegmentMerger.java
index c0e84c0..dbeae02 100644
--- a/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -52,6 +52,12 @@ final class SegmentMerger {
   
   private int mergedDocs;
 
+  // Whether we should merge doc stores (stored fields and
+  // vectors files).  When all segments we are merging
+  // already share the same doc store files, we don't need
+  // to merge the doc stores.
+  private boolean mergeDocStores;
+
   /** This ctor used only by test code.
    * 
    * @param dir The Directory to merge the other segments into
@@ -92,18 +98,32 @@ final class SegmentMerger {
    * @throws IOException if there is a low-level IO error
    */
   final int merge() throws CorruptIndexException, IOException {
-    int value;
-    
+    return merge(true);
+  }
+
+  /**
+   * Merges the readers specified by the {@link #add} method
+   * into the directory passed to the constructor.
+   * @param mergeDocStores if false, we will not merge the
+   * stored fields nor vectors files
+   * @return The number of documents that were merged
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   */
+  final int merge(boolean mergeDocStores) throws CorruptIndexException, IOException {
+
+    this.mergeDocStores = mergeDocStores;
+
     mergedDocs = mergeFields();
     mergeTerms();
     mergeNorms();
 
-    if (fieldInfos.hasVectors())
+    if (mergeDocStores && fieldInfos.hasVectors())
       mergeVectors();
 
     return mergedDocs;
   }
-  
+
   /**
    * close all IndexReaders that have been added.
    * Should not be called before merge().
@@ -126,7 +146,10 @@ final class SegmentMerger {
     
     // Basic files
     for (int i = 0; i < IndexFileNames.COMPOUND_EXTENSIONS.length; i++) {
-      files.add(segment + "." + IndexFileNames.COMPOUND_EXTENSIONS[i]);
+      String ext = IndexFileNames.COMPOUND_EXTENSIONS[i];
+      if (mergeDocStores || (!ext.equals(IndexFileNames.FIELDS_EXTENSION) &&
+                            !ext.equals(IndexFileNames.FIELDS_INDEX_EXTENSION)))
+        files.add(segment + "." + ext);
     }
 
     // Fieldable norm files
@@ -139,7 +162,7 @@ final class SegmentMerger {
     }
 
     // Vector files
-    if (fieldInfos.hasVectors()) {
+    if (fieldInfos.hasVectors() && mergeDocStores) {
       for (int i = 0; i < IndexFileNames.VECTOR_EXTENSIONS.length; i++) {
         files.add(segment + "." + IndexFileNames.VECTOR_EXTENSIONS[i]);
       }
@@ -173,7 +196,20 @@ final class SegmentMerger {
    * @throws IOException if there is a low-level IO error
    */
   private final int mergeFields() throws CorruptIndexException, IOException {
-    fieldInfos = new FieldInfos();		  // merge field names
+
+    if (!mergeDocStores) {
+      // When we are not merging by doc stores, that means
+      // all segments were written as part of a single
+      // autoCommit=false IndexWriter session, so their field
+      // name -> number mapping are the same.  So, we start
+      // with the fieldInfos of the last segment in this
+      // case, to keep that numbering.
+      final SegmentReader sr = (SegmentReader) readers.elementAt(readers.size()-1);
+      fieldInfos = (FieldInfos) sr.fieldInfos.clone();
+    } else {
+      fieldInfos = new FieldInfos();		  // merge field names
+    }
+
     int docCount = 0;
     for (int i = 0; i < readers.size(); i++) {
       IndexReader reader = (IndexReader) readers.elementAt(i);
@@ -187,30 +223,40 @@ final class SegmentMerger {
     }
     fieldInfos.write(directory, segment + ".fnm");
 
-    FieldsWriter fieldsWriter = // merge field values
-            new FieldsWriter(directory, segment, fieldInfos);
-    
-    // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're
-    // in  merge mode, we use this FieldSelector
-    FieldSelector fieldSelectorMerge = new FieldSelector() {
-      public FieldSelectorResult accept(String fieldName) {
-        return FieldSelectorResult.LOAD_FOR_MERGE;
-      }        
-    };
+    if (mergeDocStores) {
+
+      FieldsWriter fieldsWriter = // merge field values
+        new FieldsWriter(directory, segment, fieldInfos);
     
-    try {
-      for (int i = 0; i < readers.size(); i++) {
-        IndexReader reader = (IndexReader) readers.elementAt(i);
-        int maxDoc = reader.maxDoc();
-        for (int j = 0; j < maxDoc; j++)
-          if (!reader.isDeleted(j)) {               // skip deleted docs
-            fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));
-            docCount++;
-          }
+      // for merging we don't want to compress/uncompress the data, so to tell the FieldsReader that we're
+      // in  merge mode, we use this FieldSelector
+      FieldSelector fieldSelectorMerge = new FieldSelector() {
+          public FieldSelectorResult accept(String fieldName) {
+            return FieldSelectorResult.LOAD_FOR_MERGE;
+          }        
+        };
+
+      try {
+        for (int i = 0; i < readers.size(); i++) {
+          IndexReader reader = (IndexReader) readers.elementAt(i);
+          int maxDoc = reader.maxDoc();
+          for (int j = 0; j < maxDoc; j++)
+            if (!reader.isDeleted(j)) {               // skip deleted docs
+              fieldsWriter.addDocument(reader.document(j, fieldSelectorMerge));
+              docCount++;
+            }
+        }
+      } finally {
+        fieldsWriter.close();
       }
-    } finally {
-      fieldsWriter.close();
-    }
+
+    } else
+      // If we are skipping the doc stores, that means there
+      // are no deletions in any of these segments, so we
+      // just sum numDocs() of each segment to get total docCount
+      for (int i = 0; i < readers.size(); i++)
+        docCount += ((IndexReader) readers.elementAt(i)).numDocs();
+
     return docCount;
   }
 
@@ -355,6 +401,7 @@ final class SegmentMerger {
     for (int i = 0; i < n; i++) {
       SegmentMergeInfo smi = smis[i];
       TermPositions postings = smi.getPositions();
+      assert postings != null;
       int base = smi.base;
       int[] docMap = smi.getDocMap();
       postings.seek(smi.termEnum);
diff --git a/src/java/org/apache/lucene/index/SegmentReader.java b/src/java/org/apache/lucene/index/SegmentReader.java
index 66dfeef..2ac0886 100644
--- a/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/src/java/org/apache/lucene/index/SegmentReader.java
@@ -60,6 +60,7 @@ class SegmentReader extends IndexReader {
 
   // Compound File Reader when based on a compound file segment
   CompoundFileReader cfsReader = null;
+  CompoundFileReader storeCFSReader = null;
 
   private class Norm {
     public Norm(IndexInput in, int number, long normSeek)
@@ -128,7 +129,15 @@ class SegmentReader extends IndexReader {
    * @throws IOException if there is a low-level IO error
    */
   public static SegmentReader get(SegmentInfo si) throws CorruptIndexException, IOException {
-    return get(si.dir, si, null, false, false, BufferedIndexInput.BUFFER_SIZE);
+    return get(si.dir, si, null, false, false, BufferedIndexInput.BUFFER_SIZE, true);
+  }
+
+  /**
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   */
+  public static SegmentReader get(SegmentInfo si, boolean doOpenStores) throws CorruptIndexException, IOException {
+    return get(si.dir, si, null, false, false, BufferedIndexInput.BUFFER_SIZE, doOpenStores);
   }
 
   /**
@@ -136,7 +145,15 @@ class SegmentReader extends IndexReader {
    * @throws IOException if there is a low-level IO error
    */
   public static SegmentReader get(SegmentInfo si, int readBufferSize) throws CorruptIndexException, IOException {
-    return get(si.dir, si, null, false, false, readBufferSize);
+    return get(si.dir, si, null, false, false, readBufferSize, true);
+  }
+
+  /**
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   */
+  public static SegmentReader get(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {
+    return get(si.dir, si, null, false, false, readBufferSize, doOpenStores);
   }
 
   /**
@@ -145,7 +162,7 @@ class SegmentReader extends IndexReader {
    */
   public static SegmentReader get(SegmentInfos sis, SegmentInfo si,
                                   boolean closeDir) throws CorruptIndexException, IOException {
-    return get(si.dir, si, sis, closeDir, true, BufferedIndexInput.BUFFER_SIZE);
+    return get(si.dir, si, sis, closeDir, true, BufferedIndexInput.BUFFER_SIZE, true);
   }
 
   /**
@@ -157,6 +174,19 @@ class SegmentReader extends IndexReader {
                                   boolean closeDir, boolean ownDir,
                                   int readBufferSize)
     throws CorruptIndexException, IOException {
+    return get(dir, si, sis, closeDir, ownDir, readBufferSize, true);
+  }
+
+  /**
+   * @throws CorruptIndexException if the index is corrupt
+   * @throws IOException if there is a low-level IO error
+   */
+  public static SegmentReader get(Directory dir, SegmentInfo si,
+                                  SegmentInfos sis,
+                                  boolean closeDir, boolean ownDir,
+                                  int readBufferSize,
+                                  boolean doOpenStores)
+    throws CorruptIndexException, IOException {
     SegmentReader instance;
     try {
       instance = (SegmentReader)IMPL.newInstance();
@@ -164,11 +194,11 @@ class SegmentReader extends IndexReader {
       throw new RuntimeException("cannot load SegmentReader class: " + e, e);
     }
     instance.init(dir, sis, closeDir, ownDir);
-    instance.initialize(si, readBufferSize);
+    instance.initialize(si, readBufferSize, doOpenStores);
     return instance;
   }
 
-  private void initialize(SegmentInfo si, int readBufferSize) throws CorruptIndexException, IOException {
+  private void initialize(SegmentInfo si, int readBufferSize, boolean doOpenStores) throws CorruptIndexException, IOException {
     segment = si.name;
     this.si = si;
 
@@ -178,17 +208,45 @@ class SegmentReader extends IndexReader {
       // Use compound file directory for some files, if it exists
       Directory cfsDir = directory();
       if (si.getUseCompoundFile()) {
-        cfsReader = new CompoundFileReader(directory(), segment + ".cfs", readBufferSize);
+        cfsReader = new CompoundFileReader(directory(), segment + "." + IndexFileNames.COMPOUND_FILE_EXTENSION, readBufferSize);
         cfsDir = cfsReader;
       }
 
+      final Directory storeDir;
+
+      if (doOpenStores) {
+        if (si.getDocStoreOffset() != -1) {
+          if (si.getDocStoreIsCompoundFile()) {
+            storeCFSReader = new CompoundFileReader(directory(), si.getDocStoreSegment() + "." + IndexFileNames.COMPOUND_FILE_STORE_EXTENSION, readBufferSize);
+            storeDir = storeCFSReader;
+          } else {
+            storeDir = directory();
+          }
+        } else {
+          storeDir = cfsDir;
+        }
+      } else
+        storeDir = null;
+
       // No compound file exists - use the multi-file format
       fieldInfos = new FieldInfos(cfsDir, segment + ".fnm");
-      fieldsReader = new FieldsReader(cfsDir, segment, fieldInfos, readBufferSize);
 
-      // Verify two sources of "maxDoc" agree:
-      if (fieldsReader.size() != si.docCount) {
-        throw new CorruptIndexException("doc counts differ for segment " + si.name + ": fieldsReader shows " + fieldsReader.size() + " but segmentInfo shows " + si.docCount);
+      final String fieldsSegment;
+      final Directory dir;
+
+      if (si.getDocStoreOffset() != -1)
+        fieldsSegment = si.getDocStoreSegment();
+      else
+        fieldsSegment = segment;
+
+      if (doOpenStores) {
+        fieldsReader = new FieldsReader(storeDir, fieldsSegment, fieldInfos, readBufferSize,
+                                        si.getDocStoreOffset(), si.docCount);
+
+        // Verify two sources of "maxDoc" agree:
+        if (si.getDocStoreOffset() == -1 && fieldsReader.size() != si.docCount) {
+          throw new CorruptIndexException("doc counts differ for segment " + si.name + ": fieldsReader shows " + fieldsReader.size() + " but segmentInfo shows " + si.docCount);
+        }
       }
 
       tis = new TermInfosReader(cfsDir, segment, fieldInfos, readBufferSize);
@@ -209,8 +267,13 @@ class SegmentReader extends IndexReader {
       proxStream = cfsDir.openInput(segment + ".prx", readBufferSize);
       openNorms(cfsDir, readBufferSize);
 
-      if (fieldInfos.hasVectors()) { // open term vector files only as needed
-        termVectorsReaderOrig = new TermVectorsReader(cfsDir, segment, fieldInfos, readBufferSize);
+      if (doOpenStores && fieldInfos.hasVectors()) { // open term vector files only as needed
+        final String vectorsSegment;
+        if (si.getDocStoreOffset() != -1)
+          vectorsSegment = si.getDocStoreSegment();
+        else
+          vectorsSegment = segment;
+        termVectorsReaderOrig = new TermVectorsReader(storeDir, vectorsSegment, fieldInfos, readBufferSize, si.getDocStoreOffset(), si.docCount);
       }
       success = true;
     } finally {
@@ -273,6 +336,9 @@ class SegmentReader extends IndexReader {
 
     if (cfsReader != null)
       cfsReader.close();
+
+    if (storeCFSReader != null)
+      storeCFSReader.close();
   }
 
   static boolean hasDeletions(SegmentInfo si) throws IOException {
diff --git a/src/java/org/apache/lucene/index/TermVectorsReader.java b/src/java/org/apache/lucene/index/TermVectorsReader.java
index 8c40c83..a0eeeed 100644
--- a/src/java/org/apache/lucene/index/TermVectorsReader.java
+++ b/src/java/org/apache/lucene/index/TermVectorsReader.java
@@ -33,6 +33,10 @@ class TermVectorsReader implements Cloneable {
   private IndexInput tvd;
   private IndexInput tvf;
   private int size;
+
+  // The docID offset where our docs begin in the index
+  // file.  This will be 0 if we have our own private file.
+  private int docStoreOffset;
   
   private int tvdFormat;
   private int tvfFormat;
@@ -44,6 +48,11 @@ class TermVectorsReader implements Cloneable {
 
   TermVectorsReader(Directory d, String segment, FieldInfos fieldInfos, int readBufferSize)
     throws CorruptIndexException, IOException {
+    this(d, segment, fieldInfos, BufferedIndexInput.BUFFER_SIZE, -1, 0);
+  }
+    
+  TermVectorsReader(Directory d, String segment, FieldInfos fieldInfos, int readBufferSize, int docStoreOffset, int size)
+    throws CorruptIndexException, IOException {
     if (d.fileExists(segment + TermVectorsWriter.TVX_EXTENSION)) {
       tvx = d.openInput(segment + TermVectorsWriter.TVX_EXTENSION, readBufferSize);
       checkValidFormat(tvx);
@@ -51,7 +60,16 @@ class TermVectorsReader implements Cloneable {
       tvdFormat = checkValidFormat(tvd);
       tvf = d.openInput(segment + TermVectorsWriter.TVF_EXTENSION, readBufferSize);
       tvfFormat = checkValidFormat(tvf);
-      size = (int) tvx.length() / 8;
+      if (-1 == docStoreOffset) {
+        this.docStoreOffset = 0;
+        this.size = (int) (tvx.length() / 8);
+      } else {
+        this.docStoreOffset = docStoreOffset;
+        this.size = size;
+        // Verify the file is long enough to hold all of our
+        // docs
+        assert ((int) (tvx.length()/8)) >= size + docStoreOffset;
+      }
     }
 
     this.fieldInfos = fieldInfos;
@@ -102,7 +120,7 @@ class TermVectorsReader implements Cloneable {
       //We don't need to do this in other seeks because we already have the
       // file pointer
       //that was written in another file
-      tvx.seek((docNum * 8L) + TermVectorsWriter.FORMAT_SIZE);
+      tvx.seek(((docNum + docStoreOffset) * 8L) + TermVectorsWriter.FORMAT_SIZE);
       //System.out.println("TVX Pointer: " + tvx.getFilePointer());
       long position = tvx.readLong();
 
@@ -154,7 +172,7 @@ class TermVectorsReader implements Cloneable {
     // Check if no term vectors are available for this segment at all
     if (tvx != null) {
       //We need to offset by
-      tvx.seek((docNum * 8L) + TermVectorsWriter.FORMAT_SIZE);
+      tvx.seek(((docNum + docStoreOffset) * 8L) + TermVectorsWriter.FORMAT_SIZE);
       long position = tvx.readLong();
 
       tvd.seek(position);
diff --git a/src/java/org/apache/lucene/store/IndexOutput.java b/src/java/org/apache/lucene/store/IndexOutput.java
index 0a14227..6b193c3 100644
--- a/src/java/org/apache/lucene/store/IndexOutput.java
+++ b/src/java/org/apache/lucene/store/IndexOutput.java
@@ -125,6 +125,31 @@ public abstract class IndexOutput {
     }
   }
 
+  /** Writes a sequence of UTF-8 encoded characters from a char[].
+   * @param s the source of the characters
+   * @param start the first character in the sequence
+   * @param length the number of characters in the sequence
+   * @see IndexInput#readChars(char[],int,int)
+   */
+  public void writeChars(char[] s, int start, int length)
+    throws IOException {
+    final int end = start + length;
+    for (int i = start; i < end; i++) {
+      final int code = (int)s[i];
+      if (code >= 0x01 && code <= 0x7F)
+	writeByte((byte)code);
+      else if (((code >= 0x80) && (code <= 0x7FF)) || code == 0) {
+	writeByte((byte)(0xC0 | (code >> 6)));
+	writeByte((byte)(0x80 | (code & 0x3F)));
+      } else {
+	writeByte((byte)(0xE0 | (code >>> 12)));
+	writeByte((byte)(0x80 | ((code >> 6) & 0x3F)));
+	writeByte((byte)(0x80 | (code & 0x3F)));
+      }
+    }
+  }
+
+
   /** Forces any buffered output to be written. */
   public abstract void flush() throws IOException;
 
diff --git a/src/site/src/documentation/content/xdocs/fileformats.xml b/src/site/src/documentation/content/xdocs/fileformats.xml
index 60a8bd9..b98d191 100644
--- a/src/site/src/documentation/content/xdocs/fileformats.xml
+++ b/src/site/src/documentation/content/xdocs/fileformats.xml
@@ -60,6 +60,15 @@
                 Lucene will not be able to read the index.
             </p>
 
+            <p>
+                In version 2.3, the file format was changed to allow
+		segments to share a single set of doc store (vectors &amp;
+		stored fields) files.  This allows for faster indexing
+		in certain cases.  The change is fully backwards
+		compatible (in the same way as the lock-less commits
+		change in 2.1).
+            </p>
+
         </section>
 
         <section id="Definitions"><title>Definitions</title>
@@ -809,9 +818,15 @@
                     NormGen<sup>NumField</sup>,
                     IsCompoundFile&gt;<sup>SegCount</sup>
                 </p>
+                <p>
+                    <b>2.3 and above:</b>
+                    Segments --&gt; Format, Version, NameCounter, SegCount, &lt;SegName, SegSize, DelGen, DocStoreOffset, [DocStoreSegment, DocStoreIsCompoundFile], HasSingleNormFile, NumField,
+                    NormGen<sup>NumField</sup>,
+                    IsCompoundFile&gt;<sup>SegCount</sup>
+                </p>
 
                 <p>
-                    Format, NameCounter, SegCount, SegSize, NumField --&gt; Int32
+                    Format, NameCounter, SegCount, SegSize, NumField, DocStoreOffset --&gt; Int32
                 </p>
 
                 <p>
@@ -819,11 +834,11 @@
                 </p>
 
                 <p>
-                    SegName --&gt; String
+                    SegName, DocStoreSegment --&gt; String
                 </p>
 
                 <p>
-                    IsCompoundFile, HasSingleNormFile --&gt; Int8
+                    IsCompoundFile, HasSingleNormFile, DocStoreIsCompoundFile --&gt; Int8
                 </p>
 
                 <p>
@@ -889,6 +904,29 @@
                     "Normalization Factors" below for details.
                 </p>
 
+                <p>
+		    DocStoreOffset, DocStoreSegment,
+                    DocStoreIsCompoundFile: If DocStoreOffset is -1,
+                    this segment has its own doc store (stored fields
+                    values and term vectors) files and DocStoreSegment
+                    and DocStoreIsCompoundFile are not stored.  In
+                    this case all files for stored field values
+                    (<tt>*.fdt</tt> and <tt>*.fdx</tt>) and term
+                    vectors (<tt>*.tvf</tt>, <tt>*.tvd</tt> and
+                    <tt>*.tvx</tt>) will be stored with this segment.
+                    Otherwise, DocStoreSegment is the name of the
+                    segment that has the shared doc store files;
+                    DocStoreIsCompoundFile is 1 if that segment is
+                    stored in compound file format (as a <tt>.cfx</tt>
+                    file); and DocStoreOffset is the starting document
+                    in the shared doc store files where this segment's
+                    documents begin.  In this case, this segment does
+                    not store its own doc store files but instead
+                    shares a single set of these files with other
+                    segments.
+                </p>
+		
+
             </section>
 
             <section id="Lock File"><title>Lock File</title>
@@ -947,6 +985,14 @@
                 <p>FileData --&gt; raw file data</p>
                 <p>The raw file data is the data from the individual files named above.</p>
 
+		<p>Starting with Lucene 2.3, doc store files (stored
+		field values and term vectors) can be shared in a
+		single set of files for more than one segment.  When
+		compound file is enabled, these shared files will be
+		added into a single compound file (same format as
+		above) but with the extension <tt>.cfx</tt>.
+		</p>
+
             </section>
 
         </section>
diff --git a/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java b/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
index 70ed2b6..26933b8 100644
--- a/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
+++ b/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
@@ -106,8 +106,12 @@ public class TestBackwardsCompatibility extends TestCase
     rmDir(dirName);
   }
 
+  final String[] oldNames = {"prelockless.cfs",
+                             "prelockless.nocfs",
+                             "presharedstores.cfs",
+                             "presharedstores.nocfs"};
+
   public void testSearchOldIndex() throws IOException {
-    String[] oldNames = {"prelockless.cfs", "prelockless.nocfs"};
     for(int i=0;i<oldNames.length;i++) {
       String dirName = "src/test/org/apache/lucene/index/index." + oldNames[i];
       unzip(dirName, oldNames[i]);
@@ -117,7 +121,6 @@ public class TestBackwardsCompatibility extends TestCase
   }
 
   public void testIndexOldIndexNoAdds() throws IOException {
-    String[] oldNames = {"prelockless.cfs", "prelockless.nocfs"};
     for(int i=0;i<oldNames.length;i++) {
       String dirName = "src/test/org/apache/lucene/index/index." + oldNames[i];
       unzip(dirName, oldNames[i]);
@@ -131,7 +134,6 @@ public class TestBackwardsCompatibility extends TestCase
   }
 
   public void testIndexOldIndex() throws IOException {
-    String[] oldNames = {"prelockless.cfs", "prelockless.nocfs"};
     for(int i=0;i<oldNames.length;i++) {
       String dirName = "src/test/org/apache/lucene/index/index." + oldNames[i];
       unzip(dirName, oldNames[i]);
@@ -312,8 +314,9 @@ public class TestBackwardsCompatibility extends TestCase
         Directory dir = FSDirectory.getDirectory(fullDir(outputDir));
 
         boolean autoCommit = 0 == pass;
-      
+ 
         IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true);
+        writer.setRAMBufferSizeMB(16.0);
         //IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
         for(int i=0;i<35;i++) {
           addDoc(writer, i);
@@ -337,8 +340,8 @@ public class TestBackwardsCompatibility extends TestCase
         // figure out which field number corresponds to
         // "content", and then set our expected file names below
         // accordingly:
-        CompoundFileReader cfsReader = new CompoundFileReader(dir, "_2.cfs");
-        FieldInfos fieldInfos = new FieldInfos(cfsReader, "_2.fnm");
+        CompoundFileReader cfsReader = new CompoundFileReader(dir, "_0.cfs");
+        FieldInfos fieldInfos = new FieldInfos(cfsReader, "_0.fnm");
         int contentFieldIndex = -1;
         for(int i=0;i<fieldInfos.size();i++) {
           FieldInfo fi = fieldInfos.fieldInfo(i);
@@ -351,17 +354,15 @@ public class TestBackwardsCompatibility extends TestCase
         assertTrue("could not locate the 'content' field number in the _2.cfs segment", contentFieldIndex != -1);
 
         // Now verify file names:
-        String[] expected = {"_0.cfs",
-                             "_0_1.del",
-                             "_1.cfs",
-                             "_2.cfs",
-                             "_2_1.s" + contentFieldIndex,
-                             "_3.cfs",
-                             "segments_a",
-                             "segments.gen"};
-        if (!autoCommit) {
-          expected[6] = "segments_3";
-        }
+        String[] expected;
+        expected = new String[] {"_0.cfs",
+                    "_0_1.del",
+                    "_0_1.s" + contentFieldIndex,
+                    "segments_4",
+                    "segments.gen"};
+
+        if (!autoCommit)
+          expected[3] = "segments_3";
 
         String[] actual = dir.list();
         Arrays.sort(expected);
diff --git a/src/test/org/apache/lucene/index/TestDeletionPolicy.java b/src/test/org/apache/lucene/index/TestDeletionPolicy.java
index cf49f8f..2746cf6 100644
--- a/src/test/org/apache/lucene/index/TestDeletionPolicy.java
+++ b/src/test/org/apache/lucene/index/TestDeletionPolicy.java
@@ -256,6 +256,7 @@ public class TestDeletionPolicy extends TestCase
       Directory dir = new RAMDirectory();
 
       IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+      writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       for(int i=0;i<107;i++) {
         addDoc(writer);
@@ -318,6 +319,7 @@ public class TestDeletionPolicy extends TestCase
       Directory dir = new RAMDirectory();
 
       IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+      writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       for(int i=0;i<107;i++) {
         addDoc(writer);
@@ -365,6 +367,7 @@ public class TestDeletionPolicy extends TestCase
 
       for(int j=0;j<N+1;j++) {
         IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+        writer.setMaxBufferedDocs(10);
         writer.setUseCompoundFile(useCompoundFile);
         for(int i=0;i<17;i++) {
           addDoc(writer);
@@ -525,6 +528,7 @@ public class TestDeletionPolicy extends TestCase
 
       Directory dir = new RAMDirectory();
       IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+      writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       writer.close();
       Term searchTerm = new Term("content", "aaa");        
@@ -533,6 +537,7 @@ public class TestDeletionPolicy extends TestCase
       for(int i=0;i<N+1;i++) {
 
         writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, policy);
+        writer.setMaxBufferedDocs(10);
         writer.setUseCompoundFile(useCompoundFile);
         for(int j=0;j<17;j++) {
           addDoc(writer);
diff --git a/src/test/org/apache/lucene/index/TestIndexFileDeleter.java b/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
index 320470a..e65ee3e 100644
--- a/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
+++ b/src/test/org/apache/lucene/index/TestIndexFileDeleter.java
@@ -51,6 +51,7 @@ public class TestIndexFileDeleter extends TestCase
     Directory dir = new RAMDirectory();
 
     IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+    writer.setMaxBufferedDocs(10);
     int i;
     for(i=0;i<35;i++) {
       addDoc(writer, i);
diff --git a/src/test/org/apache/lucene/index/TestIndexModifier.java b/src/test/org/apache/lucene/index/TestIndexModifier.java
index a16874a0..29d99d6 100644
--- a/src/test/org/apache/lucene/index/TestIndexModifier.java
+++ b/src/test/org/apache/lucene/index/TestIndexModifier.java
@@ -74,6 +74,9 @@ public class TestIndexModifier extends TestCase {
     //  Lucene defaults:
     assertNull(i.getInfoStream());
     assertTrue(i.getUseCompoundFile());
+    /* new merge policy
+    assertEquals(0, i.getMaxBufferedDocs());
+    */
     assertEquals(10, i.getMaxBufferedDocs());
     assertEquals(10000, i.getMaxFieldLength());
     assertEquals(10, i.getMergeFactor());
diff --git a/src/test/org/apache/lucene/index/TestIndexReader.java b/src/test/org/apache/lucene/index/TestIndexReader.java
index b24f71c..a3df3a2 100644
--- a/src/test/org/apache/lucene/index/TestIndexReader.java
+++ b/src/test/org/apache/lucene/index/TestIndexReader.java
@@ -803,7 +803,7 @@ public class TestIndexReader extends TestCase
           String[] startFiles = dir.list();
           SegmentInfos infos = new SegmentInfos();
           infos.read(dir);
-          IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null);
+          IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
           String[] endFiles = dir.list();
 
           Arrays.sort(startFiles);
diff --git a/src/test/org/apache/lucene/index/TestIndexWriter.java b/src/test/org/apache/lucene/index/TestIndexWriter.java
index 947303c..f13be74 100644
--- a/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -20,6 +20,7 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import java.io.File;
 import java.util.Arrays;
+import java.util.Random;
 
 import junit.framework.TestCase;
 
@@ -478,7 +479,7 @@ public class TestIndexWriter extends TestCase
       String[] startFiles = dir.list();
       SegmentInfos infos = new SegmentInfos();
       infos.read(dir);
-      IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null);
+      IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
       String[] endFiles = dir.list();
 
       Arrays.sort(startFiles);
@@ -859,6 +860,7 @@ public class TestIndexWriter extends TestCase
     public void testCommitOnCloseAbort() throws IOException {
       Directory dir = new RAMDirectory();      
       IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      writer.setMaxBufferedDocs(10);
       for (int i = 0; i < 14; i++) {
         addDoc(writer);
       }
@@ -871,6 +873,7 @@ public class TestIndexWriter extends TestCase
       searcher.close();
 
       writer = new IndexWriter(dir, false, new WhitespaceAnalyzer(), false);
+      writer.setMaxBufferedDocs(10);
       for(int j=0;j<17;j++) {
         addDoc(writer);
       }
@@ -895,6 +898,7 @@ public class TestIndexWriter extends TestCase
       // Now make sure we can re-open the index, add docs,
       // and all is good:
       writer = new IndexWriter(dir, false, new WhitespaceAnalyzer(), false);
+      writer.setMaxBufferedDocs(10);
       for(int i=0;i<12;i++) {
         for(int j=0;j<17;j++) {
           addDoc(writer);
@@ -962,6 +966,7 @@ public class TestIndexWriter extends TestCase
     public void testCommitOnCloseOptimize() throws IOException {
       RAMDirectory dir = new RAMDirectory();      
       IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      writer.setMaxBufferedDocs(10);
       for(int j=0;j<17;j++) {
         addDocWithIndex(writer, j);
       }
@@ -1002,6 +1007,255 @@ public class TestIndexWriter extends TestCase
       reader.close();
     }
 
+    public void testIndexNoDocuments() throws IOException {
+      RAMDirectory dir = new RAMDirectory();      
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      writer.flush();
+      writer.close();
+
+      IndexReader reader = IndexReader.open(dir);
+      assertEquals(0, reader.maxDoc());
+      assertEquals(0, reader.numDocs());
+      reader.close();
+
+      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), false);
+      writer.flush();
+      writer.close();
+
+      reader = IndexReader.open(dir);
+      assertEquals(0, reader.maxDoc());
+      assertEquals(0, reader.numDocs());
+      reader.close();
+    }
+
+    public void testManyFields() throws IOException {
+      RAMDirectory dir = new RAMDirectory();      
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      writer.setMaxBufferedDocs(10);
+      for(int j=0;j<100;j++) {
+        Document doc = new Document();
+        doc.add(new Field("a"+j, "aaa" + j, Field.Store.YES, Field.Index.TOKENIZED));
+        doc.add(new Field("b"+j, "aaa" + j, Field.Store.YES, Field.Index.TOKENIZED));
+        doc.add(new Field("c"+j, "aaa" + j, Field.Store.YES, Field.Index.TOKENIZED));
+        doc.add(new Field("d"+j, "aaa", Field.Store.YES, Field.Index.TOKENIZED));
+        doc.add(new Field("e"+j, "aaa", Field.Store.YES, Field.Index.TOKENIZED));
+        doc.add(new Field("f"+j, "aaa", Field.Store.YES, Field.Index.TOKENIZED));
+        writer.addDocument(doc);
+      }
+      writer.close();
+
+      IndexReader reader = IndexReader.open(dir);
+      assertEquals(100, reader.maxDoc());
+      assertEquals(100, reader.numDocs());
+      for(int j=0;j<100;j++) {
+        assertEquals(1, reader.docFreq(new Term("a"+j, "aaa"+j)));
+        assertEquals(1, reader.docFreq(new Term("b"+j, "aaa"+j)));
+        assertEquals(1, reader.docFreq(new Term("c"+j, "aaa"+j)));
+        assertEquals(1, reader.docFreq(new Term("d"+j, "aaa")));
+        assertEquals(1, reader.docFreq(new Term("e"+j, "aaa")));
+        assertEquals(1, reader.docFreq(new Term("f"+j, "aaa")));
+      }
+      reader.close();
+      dir.close();
+    }
+
+    public void testSmallRAMBuffer() throws IOException {
+      RAMDirectory dir = new RAMDirectory();      
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      writer.setRAMBufferSizeMB(0.000001);
+      int lastNumFile = dir.list().length;
+      for(int j=0;j<9;j++) {
+        Document doc = new Document();
+        doc.add(new Field("field", "aaa" + j, Field.Store.YES, Field.Index.TOKENIZED));
+        writer.addDocument(doc);
+        int numFile = dir.list().length;
+        // Verify that with a tiny RAM buffer we see new
+        // segment after every doc
+        assertTrue(numFile > lastNumFile);
+        lastNumFile = numFile;
+      }
+      writer.close();
+      dir.close();
+    }
+
+    // Make sure it's OK to change RAM buffer size and
+    // maxBufferedDocs in a write session
+    public void testChangingRAMBuffer() throws IOException {
+      RAMDirectory dir = new RAMDirectory();      
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      writer.setMaxBufferedDocs(10);
+      int lastNumFile = dir.list().length;
+      long lastGen = -1;
+      for(int j=1;j<52;j++) {
+        Document doc = new Document();
+        doc.add(new Field("field", "aaa" + j, Field.Store.YES, Field.Index.TOKENIZED));
+        writer.addDocument(doc);
+        long gen = SegmentInfos.generationFromSegmentsFileName(SegmentInfos.getCurrentSegmentFileName(dir.list()));
+        if (j == 1)
+          lastGen = gen;
+        else if (j < 10)
+          // No new files should be created
+          assertEquals(gen, lastGen);
+        else if (10 == j) {
+          assertTrue(gen > lastGen);
+          lastGen = gen;
+          writer.setRAMBufferSizeMB(0.000001);
+        } else if (j < 20) {
+          assertTrue(gen > lastGen);
+          lastGen = gen;
+        } else if (20 == j) {
+          writer.setRAMBufferSizeMB(16);
+          lastGen = gen;
+        } else if (j < 30) {
+          assertEquals(gen, lastGen);
+        } else if (30 == j) {
+          writer.setRAMBufferSizeMB(0.000001);
+        } else if (j < 40) {
+          assertTrue(gen> lastGen);
+          lastGen = gen;
+        } else if (40 == j) {
+          writer.setMaxBufferedDocs(10);
+          lastGen = gen;
+        } else if (j < 50) {
+          assertEquals(gen, lastGen);
+          writer.setMaxBufferedDocs(10);
+        } else if (50 == j) {
+          assertTrue(gen > lastGen);
+        }
+      }
+      writer.close();
+      dir.close();
+    }
+
+    public void testDiverseDocs() throws IOException {
+      RAMDirectory dir = new RAMDirectory();      
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      // writer.setInfoStream(System.out);
+      long t0 = System.currentTimeMillis();
+      writer.setRAMBufferSizeMB(0.5);
+      Random rand = new Random(31415);
+      for(int i=0;i<3;i++) {
+        // First, docs where every term is unique (heavy on
+        // Posting instances)
+        for(int j=0;j<100;j++) {
+          Document doc = new Document();
+          for(int k=0;k<100;k++) {
+            doc.add(new Field("field", Integer.toString(rand.nextInt()), Field.Store.YES, Field.Index.TOKENIZED));
+          }
+          writer.addDocument(doc);
+        }
+
+        // Next, many single term docs where only one term
+        // occurs (heavy on byte blocks)
+        for(int j=0;j<100;j++) {
+          Document doc = new Document();
+          doc.add(new Field("field", "aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa", Field.Store.YES, Field.Index.TOKENIZED));
+          writer.addDocument(doc);
+        }
+
+        // Next, many single term docs where only one term
+        // occurs but the terms are very long (heavy on
+        // char[] arrays)
+        for(int j=0;j<100;j++) {
+          StringBuffer b = new StringBuffer();
+          String x = Integer.toString(j) + ".";
+          for(int k=0;k<1000;k++)
+            b.append(x);
+          String longTerm = b.toString();
+
+          Document doc = new Document();
+          doc.add(new Field("field", longTerm, Field.Store.YES, Field.Index.TOKENIZED));
+          writer.addDocument(doc);
+        }
+      }
+      writer.close();
+
+      long t1 = System.currentTimeMillis();
+      IndexSearcher searcher = new IndexSearcher(dir);
+      Hits hits = searcher.search(new TermQuery(new Term("field", "aaa")));
+      assertEquals(300, hits.length());
+      searcher.close();
+
+      dir.close();
+    }
+
+    public void testEnablingNorms() throws IOException {
+      RAMDirectory dir = new RAMDirectory();      
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      writer.setMaxBufferedDocs(10);
+      // Enable norms for only 1 doc, pre flush
+      for(int j=0;j<10;j++) {
+        Document doc = new Document();
+        Field f = new Field("field", "aaa", Field.Store.YES, Field.Index.TOKENIZED); 
+        if (j != 8) {
+          f.setOmitNorms(true);
+        }
+        doc.add(f);
+        writer.addDocument(doc);
+      }
+      writer.close();
+
+      Term searchTerm = new Term("field", "aaa");
+
+      IndexSearcher searcher = new IndexSearcher(dir);
+      Hits hits = searcher.search(new TermQuery(searchTerm));
+      assertEquals(10, hits.length());
+      searcher.close();
+
+      writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      writer.setMaxBufferedDocs(10);
+      // Enable norms for only 1 doc, post flush
+      for(int j=0;j<27;j++) {
+        Document doc = new Document();
+        Field f = new Field("field", "aaa", Field.Store.YES, Field.Index.TOKENIZED); 
+        if (j != 26) {
+          f.setOmitNorms(true);
+        }
+        doc.add(f);
+        writer.addDocument(doc);
+      }
+      writer.close();
+      searcher = new IndexSearcher(dir);
+      hits = searcher.search(new TermQuery(searchTerm));
+      assertEquals(27, hits.length());
+      searcher.close();
+
+      IndexReader reader = IndexReader.open(dir);
+      reader.close();
+
+      dir.close();
+    }
+
+    public void testHighFreqTerm() throws IOException {
+      RAMDirectory dir = new RAMDirectory();      
+      IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);
+      writer.setRAMBufferSizeMB(0.01);
+      writer.setMaxFieldLength(100000000);
+      // Massive doc that has 128 K a's
+      StringBuffer b = new StringBuffer(1024*1024);
+      for(int i=0;i<4096;i++) {
+        b.append(" a a a a a a a a");
+        b.append(" a a a a a a a a");
+        b.append(" a a a a a a a a");
+        b.append(" a a a a a a a a");
+      }
+      Document doc = new Document();
+      doc.add(new Field("field", b.toString(), Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+      writer.addDocument(doc);
+      writer.close();
+
+      IndexReader reader = IndexReader.open(dir);
+      assertEquals(1, reader.maxDoc());
+      assertEquals(1, reader.numDocs());
+      Term t = new Term("field", "a");
+      assertEquals(1, reader.docFreq(t));
+      TermDocs td = reader.termDocs(t);
+      td.next();
+      assertEquals(128*1024, td.freq());
+      reader.close();
+      dir.close();
+    }
+
     // Make sure that a Directory implementation that does
     // not use LockFactory at all (ie overrides makeLock and
     // implements its own private locking) works OK.  This
diff --git a/src/test/org/apache/lucene/index/TestIndexWriterDelete.java b/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
index 62f22a1..21a566c 100644
--- a/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
+++ b/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
@@ -110,7 +110,7 @@ public class TestIndexWriterDelete extends TestCase {
       }
       modifier.flush();
 
-      assertEquals(0, modifier.getRamSegmentCount());
+      assertEquals(0, modifier.getNumBufferedDocuments());
       assertTrue(0 < modifier.getSegmentCount());
 
       if (!autoCommit) {
@@ -452,7 +452,7 @@ public class TestIndexWriterDelete extends TestCase {
           String[] startFiles = dir.list();
           SegmentInfos infos = new SegmentInfos();
           infos.read(dir);
-          IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null);
+          IndexFileDeleter d = new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
           String[] endFiles = dir.list();
 
           Arrays.sort(startFiles);
diff --git a/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java b/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
index 4475eca..8fa04a1 100755
--- a/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
+++ b/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
@@ -57,7 +57,7 @@ public class TestIndexWriterMergePolicy extends TestCase {
     for (int i = 0; i < 100; i++) {
       addDoc(writer);
       checkInvariants(writer);
-      if (writer.getRamSegmentCount() + writer.getSegmentCount() >= 18) {
+      if (writer.getNumBufferedDocuments() + writer.getSegmentCount() >= 18) {
         noOverMerge = true;
       }
     }
@@ -195,7 +195,7 @@ public class TestIndexWriterMergePolicy extends TestCase {
     int mergeFactor = writer.getMergeFactor();
     int maxMergeDocs = writer.getMaxMergeDocs();
 
-    int ramSegmentCount = writer.getRamSegmentCount();
+    int ramSegmentCount = writer.getNumBufferedDocuments();
     assertTrue(ramSegmentCount < maxBufferedDocs);
 
     int lowerBound = -1;
diff --git a/src/test/org/apache/lucene/index/TestLazyProxSkipping.java b/src/test/org/apache/lucene/index/TestLazyProxSkipping.java
index 617ff52..0199487 100755
--- a/src/test/org/apache/lucene/index/TestLazyProxSkipping.java
+++ b/src/test/org/apache/lucene/index/TestLazyProxSkipping.java
@@ -50,7 +50,7 @@ public class TestLazyProxSkipping extends TestCase {
         
         Directory directory = new RAMDirectory();
         IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
-        
+        writer.setMaxBufferedDocs(10);
         for (int i = 0; i < numDocs; i++) {
             Document doc = new Document();
             String content;
diff --git a/src/test/org/apache/lucene/index/TestPayloads.java b/src/test/org/apache/lucene/index/TestPayloads.java
index adac6716..a9d1791 100644
--- a/src/test/org/apache/lucene/index/TestPayloads.java
+++ b/src/test/org/apache/lucene/index/TestPayloads.java
@@ -467,7 +467,8 @@ public class TestPayloads extends TestCase {
                             d.add(new Field(field, new PoolingPayloadTokenStream(pool)));
                             writer.addDocument(d);
                         }
-                    } catch (IOException e) {
+                    } catch (Exception e) {
+                        e.printStackTrace();
                         fail(e.toString());
                     }
                 }
@@ -480,7 +481,6 @@ public class TestPayloads extends TestCase {
                 ingesters[i].join();
             } catch (InterruptedException e) {}
         }
-        
         writer.close();
         IndexReader reader = IndexReader.open(dir);
         TermEnum terms = reader.terms();
diff --git a/src/test/org/apache/lucene/index/TestStressIndexing.java b/src/test/org/apache/lucene/index/TestStressIndexing.java
index 41fb629..72b00fd 100644
--- a/src/test/org/apache/lucene/index/TestStressIndexing.java
+++ b/src/test/org/apache/lucene/index/TestStressIndexing.java
@@ -74,8 +74,6 @@ public class TestStressIndexing extends TestCase {
           count++;
         }
         
-        modifier.close();
-
       } catch (Exception e) {
         System.out.println(e.toString());
         e.printStackTrace();
@@ -125,6 +123,9 @@ public class TestStressIndexing extends TestCase {
     IndexerThread indexerThread = new IndexerThread(modifier);
     indexerThread.start();
       
+    IndexerThread indexerThread2 = new IndexerThread(modifier);
+    indexerThread2.start();
+      
     // Two searchers that constantly just re-instantiate the searcher:
     SearcherThread searcherThread1 = new SearcherThread(directory);
     searcherThread1.start();
@@ -133,9 +134,14 @@ public class TestStressIndexing extends TestCase {
     searcherThread2.start();
 
     indexerThread.join();
+    indexerThread2.join();
     searcherThread1.join();
     searcherThread2.join();
+
+    modifier.close();
+
     assertTrue("hit unexpected exception in indexer", !indexerThread.failed);
+    assertTrue("hit unexpected exception in indexer 2", !indexerThread2.failed);
     assertTrue("hit unexpected exception in search1", !searcherThread1.failed);
     assertTrue("hit unexpected exception in search2", !searcherThread2.failed);
     //System.out.println("    Writer: " + indexerThread.count + " iterations");
diff --git a/src/test/org/apache/lucene/index/index.presharedstores.cfs.zip b/src/test/org/apache/lucene/index/index.presharedstores.cfs.zip
new file mode 100644
index 0000000..c952b80
Binary files /dev/null and b/src/test/org/apache/lucene/index/index.presharedstores.cfs.zip differ
diff --git a/src/test/org/apache/lucene/index/index.presharedstores.nocfs.zip b/src/test/org/apache/lucene/index/index.presharedstores.nocfs.zip
new file mode 100644
index 0000000..e0a4240
Binary files /dev/null and b/src/test/org/apache/lucene/index/index.presharedstores.nocfs.zip differ
diff --git a/src/test/org/apache/lucene/search/TestTermVectors.java b/src/test/org/apache/lucene/search/TestTermVectors.java
index d0a94cf..67c260d 100644
--- a/src/test/org/apache/lucene/search/TestTermVectors.java
+++ b/src/test/org/apache/lucene/search/TestTermVectors.java
@@ -291,6 +291,80 @@ public class TestTermVectors extends TestCase {
         Field.Index.TOKENIZED, Field.TermVector.YES));
     //System.out.println("Document: " + doc);
   }
-  
-  
+
+  // Test only a few docs having vectors
+  public void testRareVectors() throws IOException {
+    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true);
+    for(int i=0;i<100;i++) {
+      Document doc = new Document();
+      doc.add(new Field("field", English.intToEnglish(i),
+                        Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.NO));
+      writer.addDocument(doc);
+    }
+    for(int i=0;i<10;i++) {
+      Document doc = new Document();
+      doc.add(new Field("field", English.intToEnglish(100+i),
+                        Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+      writer.addDocument(doc);
+    }
+
+    writer.close();
+    searcher = new IndexSearcher(directory);
+
+    Query query = new TermQuery(new Term("field", "hundred"));
+    Hits hits = searcher.search(query);
+    assertEquals(10, hits.length());
+    for (int i = 0; i < hits.length(); i++) {
+      TermFreqVector [] vector = searcher.reader.getTermFreqVectors(hits.id(i));
+      assertTrue(vector != null);
+      assertTrue(vector.length == 1);
+    }
+  }
+
+
+  // In a single doc, for the same field, mix the term
+  // vectors up
+  public void testMixedVectrosVectors() throws IOException {
+    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true);
+    Document doc = new Document();
+    doc.add(new Field("field", "one",
+                      Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.NO));
+    doc.add(new Field("field", "one",
+                      Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.YES));
+    doc.add(new Field("field", "one",
+                      Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS));
+    doc.add(new Field("field", "one",
+                      Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_OFFSETS));
+    doc.add(new Field("field", "one",
+                      Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
+    writer.addDocument(doc);
+    writer.close();
+
+    searcher = new IndexSearcher(directory);
+
+    Query query = new TermQuery(new Term("field", "one"));
+    Hits hits = searcher.search(query);
+    assertEquals(1, hits.length());
+
+    TermFreqVector [] vector = searcher.reader.getTermFreqVectors(hits.id(0));
+    assertTrue(vector != null);
+    assertTrue(vector.length == 1);
+    TermPositionVector tfv = (TermPositionVector) vector[0];
+    assertTrue(tfv.getField().equals("field"));
+    String[] terms = tfv.getTerms();
+    assertEquals(1, terms.length);
+    assertEquals(terms[0], "one");
+    assertEquals(5, tfv.getTermFrequencies()[0]);
+
+    int[] positions = tfv.getTermPositions(0);
+    assertEquals(5, positions.length);
+    for(int i=0;i<5;i++)
+      assertEquals(i, positions[i]);
+    TermVectorOffsetInfo[] offsets = tfv.getOffsets(0);
+    assertEquals(5, offsets.length);
+    for(int i=0;i<5;i++) {
+      assertEquals(4*i, offsets[i].getStartOffset());
+      assertEquals(4*i+3, offsets[i].getEndOffset());
+    }
+  }
 }

