GitDiffStart: 1accec983e402f575b652f347442e22cf6dac0bc | Tue Nov 19 18:35:54 2013 +0000
diff --git a/TODO b/TODO
index 9034b0c..7077e41 100644
--- a/TODO
+++ b/TODO
@@ -2,6 +2,7 @@ nocommit this!
 
 TODO
   - associations
+    - can we do index time detection of invalid mixing?
   - cutover taxo writer/reader to pathToString/stringToPath
   - wrap an IW instead of extending one?  or, FacetDocument?
   - re-enable ALL_BUT_DIM somehow?
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/DocumentBuilder.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/DocumentBuilder.java
new file mode 100644
index 0000000..ce6e028
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/DocumentBuilder.java
@@ -0,0 +1,365 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.SortedSetDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.index.IndexDocument;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.IndexableFieldType;
+import org.apache.lucene.index.StorableField;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/** Pass the {@link #Document} to index to {@link #build},
+ *  to translate any added {@link FacetField}s into
+ *  indexable and storable fields.  It's safe to share a
+ *  single instance of this across multiple threads. */
+
+public class DocumentBuilder {
+  private final TaxonomyWriter taxoWriter;
+  private final FacetsConfig config;
+
+  public DocumentBuilder(TaxonomyWriter taxoWriter, FacetsConfig config) {
+    this.taxoWriter = taxoWriter;
+    this.config = config;
+  }
+
+  public IndexDocument build(IndexDocument doc) throws IOException {
+    // Find all FacetFields, collated by the actual field:
+    Map<String,List<FacetField>> byField = new HashMap<String,List<FacetField>>();
+
+    // ... and also all SortedSetDocValuesFacetFields:
+    Map<String,List<SortedSetDocValuesFacetField>> dvByField = new HashMap<String,List<SortedSetDocValuesFacetField>>();
+
+    // ... and also all AssociationFacetFields
+    Map<String,List<AssociationFacetField>> assocByField = new HashMap<String,List<AssociationFacetField>>();
+
+    for(IndexableField field : doc.indexableFields()) {
+      if (field.fieldType() == FacetField.TYPE) {
+        FacetField facetField = (FacetField) field;
+        FacetsConfig.DimConfig dimConfig = config.getDimConfig(facetField.dim);
+        String indexFieldName = dimConfig.indexFieldName;
+        List<FacetField> fields = byField.get(indexFieldName);
+        if (fields == null) {
+          fields = new ArrayList<FacetField>();
+          byField.put(indexFieldName, fields);
+        }
+        fields.add(facetField);
+      }
+
+      if (field.fieldType() == SortedSetDocValuesFacetField.TYPE) {
+        SortedSetDocValuesFacetField facetField = (SortedSetDocValuesFacetField) field;
+        FacetsConfig.DimConfig dimConfig = config.getDimConfig(facetField.dim);
+        String indexFieldName = dimConfig.indexFieldName;
+        List<SortedSetDocValuesFacetField> fields = dvByField.get(indexFieldName);
+        if (fields == null) {
+          fields = new ArrayList<SortedSetDocValuesFacetField>();
+          dvByField.put(indexFieldName, fields);
+        }
+        fields.add(facetField);
+      }
+
+      if (field.fieldType() == AssociationFacetField.TYPE) {
+        AssociationFacetField facetField = (AssociationFacetField) field;
+        FacetsConfig.DimConfig dimConfig = config.getDimConfig(facetField.dim);
+
+        // nocommit how to use a different default name for assocs?
+        String indexFieldName = dimConfig.indexFieldName;
+        List<AssociationFacetField> fields = assocByField.get(indexFieldName);
+        if (fields == null) {
+          fields = new ArrayList<AssociationFacetField>();
+          assocByField.put(indexFieldName, fields);
+        }
+        fields.add(facetField);
+      }
+    }
+
+    List<Field> addedIndexedFields = new ArrayList<Field>();
+    List<Field> addedStoredFields = new ArrayList<Field>();
+
+    processFacetFields(byField, addedIndexedFields, addedStoredFields);
+    processSSDVFacetFields(dvByField, addedIndexedFields, addedStoredFields);
+    processAssocFacetFields(assocByField, addedIndexedFields, addedStoredFields);
+
+    //System.out.println("add stored: " + addedStoredFields);
+
+    final List<IndexableField> allIndexedFields = new ArrayList<IndexableField>();
+    for(IndexableField field : doc.indexableFields()) {
+      IndexableFieldType ft = field.fieldType();
+      if (ft != FacetField.TYPE && ft != SortedSetDocValuesFacetField.TYPE && ft != AssociationFacetField.TYPE) {
+        allIndexedFields.add(field);
+      }
+    }
+    allIndexedFields.addAll(addedIndexedFields);
+
+    final List<StorableField> allStoredFields = new ArrayList<StorableField>();
+    for(StorableField field : doc.storableFields()) {
+      allStoredFields.add(field);
+    }
+    allStoredFields.addAll(addedStoredFields);
+
+    //System.out.println("all indexed: " + allIndexedFields);
+    //System.out.println("all stored: " + allStoredFields);
+
+    return new IndexDocument() {
+        @Override
+        public Iterable<IndexableField> indexableFields() {
+          return allIndexedFields;
+        }
+
+        @Override
+        public Iterable<StorableField> storableFields() {
+          return allStoredFields;
+        }
+      };
+  }
+
+  private void processFacetFields(Map<String,List<FacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
+
+    for(Map.Entry<String,List<FacetField>> ent : byField.entrySet()) {
+
+      // nocommit maybe we can somehow catch singleValued
+      // dim appearing more than once?
+
+      String indexFieldName = ent.getKey();
+      //System.out.println("  fields=" + ent.getValue());
+
+      IntsRef ordinals = new IntsRef(32);
+      for(FacetField facetField : ent.getValue()) {
+
+        FacetsConfig.DimConfig ft = config.getDimConfig(facetField.dim);
+        if (facetField.path.length > 1 && ft.hierarchical == false) {
+          throw new IllegalArgumentException("dimension \"" + facetField.dim + "\" is not hierarchical yet has " + facetField.path.length + " components");
+        }
+      
+        FacetLabel cp = FacetLabel.create(facetField.dim, facetField.path);
+
+        int ordinal = taxoWriter.addCategory(cp);
+        ordinals.ints[ordinals.length++] = ordinal;
+        //System.out.println("  add cp=" + cp);
+
+        if (ft.hierarchical && ft.multiValued) {
+          // Add all parents too:
+          int parent = taxoWriter.getParent(ordinal);
+          while (parent > 0) {
+            if (ordinals.ints.length == ordinals.length) {
+              ordinals.grow(ordinals.length+1);
+            }
+            ordinals.ints[ordinals.length++] = parent;
+            parent = taxoWriter.getParent(parent);
+          }
+        }
+
+        // Drill down:
+        for(int i=2;i<=cp.length;i++) {
+          addedIndexedFields.add(new StringField(indexFieldName, pathToString(cp.components, i), Field.Store.NO));
+        }
+      }
+
+      // Facet counts:
+      // DocValues are considered stored fields:
+      addedStoredFields.add(new BinaryDocValuesField(indexFieldName, dedupAndEncode(ordinals)));
+    }
+  }
+
+  private void processSSDVFacetFields(Map<String,List<SortedSetDocValuesFacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
+    //System.out.println("process SSDV: " + byField);
+    for(Map.Entry<String,List<SortedSetDocValuesFacetField>> ent : byField.entrySet()) {
+
+      String indexFieldName = ent.getKey();
+      //System.out.println("  field=" + indexFieldName);
+
+      for(SortedSetDocValuesFacetField facetField : ent.getValue()) {
+        FacetLabel cp = new FacetLabel(facetField.dim, facetField.label);
+        String fullPath = pathToString(cp.components, cp.length);
+        //System.out.println("add " + fullPath);
+
+        // For facet counts:
+        addedStoredFields.add(new SortedSetDocValuesField(indexFieldName, new BytesRef(fullPath)));
+
+        // For drill-down:
+        addedIndexedFields.add(new StringField(indexFieldName, fullPath, Field.Store.NO));
+      }
+    }
+  }
+
+  private void processAssocFacetFields(Map<String,List<AssociationFacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
+    for(Map.Entry<String,List<AssociationFacetField>> ent : byField.entrySet()) {
+      byte[] bytes = new byte[16];
+      int upto = 0;
+      String indexFieldName = ent.getKey();
+      for(AssociationFacetField field : ent.getValue()) {
+        // NOTE: we don't add parents for associations
+        // nocommit is that right?  maybe we are supposed to
+        // add to taxo writer, and just not index the parent
+        // ords?
+        int ordinal = taxoWriter.addCategory(FacetLabel.create(field.dim, field.path));
+        if (upto + 4 > bytes.length) {
+          bytes = ArrayUtil.grow(bytes, upto+4);
+        }
+        // big-endian:
+        bytes[upto++] = (byte) (ordinal >> 24);
+        bytes[upto++] = (byte) (ordinal >> 16);
+        bytes[upto++] = (byte) (ordinal >> 8);
+        bytes[upto++] = (byte) ordinal;
+        if (upto + field.assoc.length > bytes.length) {
+          bytes = ArrayUtil.grow(bytes, upto+field.assoc.length);
+        }
+        System.arraycopy(field.assoc.bytes, field.assoc.offset, bytes, upto, field.assoc.length);
+        upto += field.assoc.length;
+      }
+      addedStoredFields.add(new BinaryDocValuesField(indexFieldName, new BytesRef(bytes, 0, upto)));
+    }
+  }
+
+  /** Encodes ordinals into a BytesRef; expert: subclass can
+   *  override this to change encoding. */
+  protected BytesRef dedupAndEncode(IntsRef ordinals) {
+    Arrays.sort(ordinals.ints, ordinals.offset, ordinals.length);
+    byte[] bytes = new byte[5*ordinals.length];
+    int lastOrd = -1;
+    int upto = 0;
+    for(int i=0;i<ordinals.length;i++) {
+      int ord = ordinals.ints[ordinals.offset+i];
+      // ord could be == lastOrd, so we must dedup:
+      if (ord > lastOrd) {
+        int delta;
+        if (lastOrd == -1) {
+          delta = ord;
+        } else {
+          delta = ord - lastOrd;
+        }
+        if ((delta & ~0x7F) == 0) {
+          bytes[upto] = (byte) delta;
+          upto++;
+        } else if ((delta & ~0x3FFF) == 0) {
+          bytes[upto] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 1] = (byte) (delta & 0x7F);
+          upto += 2;
+        } else if ((delta & ~0x1FFFFF) == 0) {
+          bytes[upto] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
+          bytes[upto + 1] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 2] = (byte) (delta & 0x7F);
+          upto += 3;
+        } else if ((delta & ~0xFFFFFFF) == 0) {
+          bytes[upto] = (byte) (0x80 | ((delta & 0xFE00000) >> 21));
+          bytes[upto + 1] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
+          bytes[upto + 2] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 3] = (byte) (delta & 0x7F);
+          upto += 4;
+        } else {
+          bytes[upto] = (byte) (0x80 | ((delta & 0xF0000000) >> 28));
+          bytes[upto + 1] = (byte) (0x80 | ((delta & 0xFE00000) >> 21));
+          bytes[upto + 2] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
+          bytes[upto + 3] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 4] = (byte) (delta & 0x7F);
+          upto += 5;
+        }
+        lastOrd = ord;
+      }
+    }
+    return new BytesRef(bytes, 0, upto);
+  }
+
+  // nocommit move all of this to Util?
+
+  // Joins the path components together:
+  private static final char DELIM_CHAR = '\u001F';
+
+  // Escapes any occurrence of the path component inside the label:
+  private static final char ESCAPE_CHAR = '\u001E';
+
+  /** Turns a path into a string without stealing any
+   *  characters. */
+  public static String pathToString(String dim, String[] path) {
+    String[] fullPath = new String[1+path.length];
+    fullPath[0] = dim;
+    System.arraycopy(path, 0, fullPath, 1, path.length);
+    return pathToString(fullPath, fullPath.length);
+  }
+
+  public static String pathToString(String[] path) {
+    return pathToString(path, path.length);
+  }
+
+  public static String pathToString(String[] path, int length) {
+    StringBuilder sb = new StringBuilder();
+    for(int i=0;i<length;i++) {
+      String s = path[i];
+      int numChars = s.length();
+      for(int j=0;j<numChars;j++) {
+        char ch = s.charAt(j);
+        if (ch == DELIM_CHAR || ch == ESCAPE_CHAR) {
+          sb.append(ESCAPE_CHAR);
+        }
+        sb.append(ch);
+      }
+      sb.append(DELIM_CHAR);
+    }
+
+    // Trim off last DELIM_CHAR:
+    sb.setLength(sb.length()-1);
+    return sb.toString();
+  }
+
+  /** Turns a result from previous call to {@link
+   *  #pathToString} back into the original {@code String[]}
+   *  without stealing any characters. */
+  public static String[] stringToPath(String s) {
+    List<String> parts = new ArrayList<String>();
+    int length = s.length();
+    char[] buffer = new char[length];
+
+    int upto = 0;
+    boolean lastEscape = false;
+    for(int i=0;i<length;i++) {
+      char ch = s.charAt(i);
+      if (lastEscape) {
+        buffer[upto++] = ch;
+        lastEscape = false;
+      } else if (ch == ESCAPE_CHAR) {
+        lastEscape = true;
+      } else if (ch == DELIM_CHAR) {
+        parts.add(new String(buffer, 0, upto));
+        upto = 0;
+      } else {
+        buffer[upto++] = ch;
+      }
+    }
+    parts.add(new String(buffer, 0, upto));
+    assert !lastEscape;
+    return parts.toArray(new String[parts.size()]);
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/FacetIndexWriter.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/FacetIndexWriter.java
deleted file mode 100644
index 8380d44..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/FacetIndexWriter.java
+++ /dev/null
@@ -1,370 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.SortedSetDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.index.IndexDocument;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.IndexableFieldType;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-public class FacetIndexWriter extends IndexWriter {
-
-  private final TaxonomyWriter taxoWriter;
-  private final FacetsConfig config;
-
-  public FacetIndexWriter(Directory d, IndexWriterConfig conf, TaxonomyWriter taxoWriter, FacetsConfig config) throws IOException {
-    super(d, conf);
-    this.taxoWriter = taxoWriter;
-    this.config = config;
-  }
-
-  // nocommit maybe we could somehow "own" TaxonomyWriter
-  // too?  commit it in commit, close it in close, etc?
-
-  // nocommit also updateDocument, addDocument, addDocuments
-
-  @Override
-  public void addDocument(final IndexDocument doc) throws IOException {
-
-    // Find all FacetFields, collated by the actual field:
-    Map<String,List<FacetField>> byField = new HashMap<String,List<FacetField>>();
-
-    // ... and also all SortedSetDocValuesFacetFields:
-    Map<String,List<SortedSetDocValuesFacetField>> dvByField = new HashMap<String,List<SortedSetDocValuesFacetField>>();
-
-    // ... and also all AssociationFacetFields
-    Map<String,List<AssociationFacetField>> assocByField = new HashMap<String,List<AssociationFacetField>>();
-
-    for(IndexableField field : doc.indexableFields()) {
-      if (field.fieldType() == FacetField.TYPE) {
-        FacetField facetField = (FacetField) field;
-        FacetsConfig.DimConfig dimConfig = config.getDimConfig(facetField.dim);
-        String indexFieldName = dimConfig.indexFieldName;
-        List<FacetField> fields = byField.get(indexFieldName);
-        if (fields == null) {
-          fields = new ArrayList<FacetField>();
-          byField.put(indexFieldName, fields);
-        }
-        fields.add(facetField);
-      }
-
-      if (field.fieldType() == SortedSetDocValuesFacetField.TYPE) {
-        SortedSetDocValuesFacetField facetField = (SortedSetDocValuesFacetField) field;
-        FacetsConfig.DimConfig dimConfig = config.getDimConfig(facetField.dim);
-        String indexFieldName = dimConfig.indexFieldName;
-        List<SortedSetDocValuesFacetField> fields = dvByField.get(indexFieldName);
-        if (fields == null) {
-          fields = new ArrayList<SortedSetDocValuesFacetField>();
-          dvByField.put(indexFieldName, fields);
-        }
-        fields.add(facetField);
-      }
-
-      if (field.fieldType() == AssociationFacetField.TYPE) {
-        AssociationFacetField facetField = (AssociationFacetField) field;
-        FacetsConfig.DimConfig dimConfig = config.getDimConfig(facetField.dim);
-
-        // nocommit how to use a different default name for assocs?
-        String indexFieldName = dimConfig.indexFieldName;
-        List<AssociationFacetField> fields = assocByField.get(indexFieldName);
-        if (fields == null) {
-          fields = new ArrayList<AssociationFacetField>();
-          assocByField.put(indexFieldName, fields);
-        }
-        fields.add(facetField);
-      }
-    }
-
-    List<Field> addedIndexedFields = new ArrayList<Field>();
-    List<Field> addedStoredFields = new ArrayList<Field>();
-
-    processFacetFields(byField, addedIndexedFields, addedStoredFields);
-    processSSDVFacetFields(dvByField, addedIndexedFields, addedStoredFields);
-    processAssocFacetFields(assocByField, addedIndexedFields, addedStoredFields);
-
-    //System.out.println("add stored: " + addedStoredFields);
-
-    final List<IndexableField> allIndexedFields = new ArrayList<IndexableField>();
-    for(IndexableField field : doc.indexableFields()) {
-      IndexableFieldType ft = field.fieldType();
-      if (ft != FacetField.TYPE && ft != SortedSetDocValuesFacetField.TYPE && ft != AssociationFacetField.TYPE) {
-        allIndexedFields.add(field);
-      }
-    }
-    allIndexedFields.addAll(addedIndexedFields);
-
-    final List<StorableField> allStoredFields = new ArrayList<StorableField>();
-    for(StorableField field : doc.storableFields()) {
-      allStoredFields.add(field);
-    }
-    allStoredFields.addAll(addedStoredFields);
-
-    //System.out.println("all indexed: " + allIndexedFields);
-    //System.out.println("all stored: " + allStoredFields);
-
-    super.addDocument(new IndexDocument() {
-        @Override
-        public Iterable<IndexableField> indexableFields() {
-          return allIndexedFields;
-        }
-
-        @Override
-        public Iterable<StorableField> storableFields() {
-          return allStoredFields;
-        }
-      });
-  }
-
-  private void processFacetFields(Map<String,List<FacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
-
-    for(Map.Entry<String,List<FacetField>> ent : byField.entrySet()) {
-
-      // nocommit maybe we can somehow catch singleValued
-      // dim appearing more than once?
-
-      String indexFieldName = ent.getKey();
-      //System.out.println("  fields=" + ent.getValue());
-
-      IntsRef ordinals = new IntsRef(32);
-      for(FacetField facetField : ent.getValue()) {
-
-        FacetsConfig.DimConfig ft = config.getDimConfig(facetField.dim);
-        if (facetField.path.length > 1 && ft.hierarchical == false) {
-          throw new IllegalArgumentException("dimension \"" + facetField.dim + "\" is not hierarchical yet has " + facetField.path.length + " components");
-        }
-      
-        FacetLabel cp = FacetLabel.create(facetField.dim, facetField.path);
-
-        int ordinal = taxoWriter.addCategory(cp);
-        ordinals.ints[ordinals.length++] = ordinal;
-        //System.out.println("  add cp=" + cp);
-
-        if (ft.hierarchical && ft.multiValued) {
-          // Add all parents too:
-          int parent = taxoWriter.getParent(ordinal);
-          while (parent > 0) {
-            if (ordinals.ints.length == ordinals.length) {
-              ordinals.grow(ordinals.length+1);
-            }
-            ordinals.ints[ordinals.length++] = parent;
-            parent = taxoWriter.getParent(parent);
-          }
-        }
-
-        // Drill down:
-        for(int i=2;i<=cp.length;i++) {
-          addedIndexedFields.add(new StringField(indexFieldName, pathToString(cp.components, i), Field.Store.NO));
-        }
-      }
-
-      // Facet counts:
-      // DocValues are considered stored fields:
-      addedStoredFields.add(new BinaryDocValuesField(indexFieldName, dedupAndEncode(ordinals)));
-    }
-  }
-
-  private void processSSDVFacetFields(Map<String,List<SortedSetDocValuesFacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
-    //System.out.println("process SSDV: " + byField);
-    for(Map.Entry<String,List<SortedSetDocValuesFacetField>> ent : byField.entrySet()) {
-
-      String indexFieldName = ent.getKey();
-      //System.out.println("  field=" + indexFieldName);
-
-      for(SortedSetDocValuesFacetField facetField : ent.getValue()) {
-        FacetLabel cp = new FacetLabel(facetField.dim, facetField.label);
-        String fullPath = pathToString(cp.components, cp.length);
-        //System.out.println("add " + fullPath);
-
-        // For facet counts:
-        addedStoredFields.add(new SortedSetDocValuesField(indexFieldName, new BytesRef(fullPath)));
-
-        // For drill-down:
-        addedIndexedFields.add(new StringField(indexFieldName, fullPath, Field.Store.NO));
-      }
-    }
-  }
-
-  private void processAssocFacetFields(Map<String,List<AssociationFacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
-    for(Map.Entry<String,List<AssociationFacetField>> ent : byField.entrySet()) {
-      byte[] bytes = new byte[16];
-      int upto = 0;
-      String indexFieldName = ent.getKey();
-      for(AssociationFacetField field : ent.getValue()) {
-        // NOTE: we don't add parents for associations
-        // nocommit is that right?  maybe we are supposed to
-        // add to taxo writer, and just not index the parent
-        // ords?
-        int ordinal = taxoWriter.addCategory(FacetLabel.create(field.dim, field.path));
-        if (upto + 4 > bytes.length) {
-          bytes = ArrayUtil.grow(bytes, upto+4);
-        }
-        // big-endian:
-        bytes[upto++] = (byte) (ordinal >> 24);
-        bytes[upto++] = (byte) (ordinal >> 16);
-        bytes[upto++] = (byte) (ordinal >> 8);
-        bytes[upto++] = (byte) ordinal;
-        if (upto + field.assoc.length > bytes.length) {
-          bytes = ArrayUtil.grow(bytes, upto+field.assoc.length);
-        }
-        System.arraycopy(field.assoc.bytes, field.assoc.offset, bytes, upto, field.assoc.length);
-        upto += field.assoc.length;
-      }
-      addedStoredFields.add(new BinaryDocValuesField(indexFieldName, new BytesRef(bytes, 0, upto)));
-    }
-  }
-
-  // nocommit open this up
-  /** We can open this up if/when we really need
-   *  pluggability on the encoding. */
-  private final BytesRef dedupAndEncode(IntsRef ordinals) {
-    Arrays.sort(ordinals.ints, ordinals.offset, ordinals.length);
-    byte[] bytes = new byte[5*ordinals.length];
-    int lastOrd = -1;
-    int upto = 0;
-    for(int i=0;i<ordinals.length;i++) {
-      int ord = ordinals.ints[ordinals.offset+i];
-      // ord could be == lastOrd, so we must dedup:
-      if (ord > lastOrd) {
-        int delta;
-        if (lastOrd == -1) {
-          delta = ord;
-        } else {
-          delta = ord - lastOrd;
-        }
-        if ((delta & ~0x7F) == 0) {
-          bytes[upto] = (byte) delta;
-          upto++;
-        } else if ((delta & ~0x3FFF) == 0) {
-          bytes[upto] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
-          bytes[upto + 1] = (byte) (delta & 0x7F);
-          upto += 2;
-        } else if ((delta & ~0x1FFFFF) == 0) {
-          bytes[upto] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
-          bytes[upto + 1] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
-          bytes[upto + 2] = (byte) (delta & 0x7F);
-          upto += 3;
-        } else if ((delta & ~0xFFFFFFF) == 0) {
-          bytes[upto] = (byte) (0x80 | ((delta & 0xFE00000) >> 21));
-          bytes[upto + 1] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
-          bytes[upto + 2] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
-          bytes[upto + 3] = (byte) (delta & 0x7F);
-          upto += 4;
-        } else {
-          bytes[upto] = (byte) (0x80 | ((delta & 0xF0000000) >> 28));
-          bytes[upto + 1] = (byte) (0x80 | ((delta & 0xFE00000) >> 21));
-          bytes[upto + 2] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
-          bytes[upto + 3] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
-          bytes[upto + 4] = (byte) (delta & 0x7F);
-          upto += 5;
-        }
-        lastOrd = ord;
-      }
-    }
-    return new BytesRef(bytes, 0, upto);
-  }
-
-  // nocommit move these constants / methods to Util?
-
-  // Joins the path components together:
-  private static final char DELIM_CHAR = '\u001F';
-
-  // Escapes any occurrence of the path component inside the label:
-  private static final char ESCAPE_CHAR = '\u001E';
-
-  /** Turns a path into a string without stealing any
-   *  characters. */
-  public static String pathToString(String dim, String[] path) {
-    String[] fullPath = new String[1+path.length];
-    fullPath[0] = dim;
-    System.arraycopy(path, 0, fullPath, 1, path.length);
-    return pathToString(fullPath, fullPath.length);
-  }
-
-  public static String pathToString(String[] path) {
-    return pathToString(path, path.length);
-  }
-
-  public static String pathToString(String[] path, int length) {
-    StringBuilder sb = new StringBuilder();
-    for(int i=0;i<length;i++) {
-      String s = path[i];
-      int numChars = s.length();
-      for(int j=0;j<numChars;j++) {
-        char ch = s.charAt(j);
-        if (ch == DELIM_CHAR || ch == ESCAPE_CHAR) {
-          sb.append(ESCAPE_CHAR);
-        }
-        sb.append(ch);
-      }
-      sb.append(DELIM_CHAR);
-    }
-
-    // Trim off last DELIM_CHAR:
-    sb.setLength(sb.length()-1);
-    return sb.toString();
-  }
-
-  /** Turns a result from previous call to {@link
-   *  #pathToString} back into the original {@code String[]}
-   *  without stealing any characters. */
-  public static String[] stringToPath(String s) {
-    List<String> parts = new ArrayList<String>();
-    int length = s.length();
-    char[] buffer = new char[length];
-
-    int upto = 0;
-    boolean lastEscape = false;
-    for(int i=0;i<length;i++) {
-      char ch = s.charAt(i);
-      if (lastEscape) {
-        buffer[upto++] = ch;
-        lastEscape = false;
-      } else if (ch == ESCAPE_CHAR) {
-        lastEscape = true;
-      } else if (ch == DELIM_CHAR) {
-        parts.add(new String(buffer, 0, upto));
-        upto = 0;
-      } else {
-        buffer[upto++] = ch;
-      }
-    }
-    parts.add(new String(buffer, 0, upto));
-    assert !lastEscape;
-    return parts.toArray(new String[parts.size()]);
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillDownQuery.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillDownQuery.java
index 3f2186a..645f2b2 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillDownQuery.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillDownQuery.java
@@ -53,7 +53,7 @@ import org.apache.lucene.search.TermQuery;
 public final class SimpleDrillDownQuery extends Query {
 
   private static Term term(String field, String dim, String[] path) {
-    return new Term(field, FacetIndexWriter.pathToString(dim, path));
+    return new Term(field, FacetDocument.pathToString(dim, path));
   }
 
   private final FacetsConfig config;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts.java
index d00fc03..e011c78 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts.java
@@ -230,7 +230,7 @@ public class SortedSetDocValuesFacetCounts extends Facets {
       throw new IllegalArgumentException("path must be length=1");
     }
 
-    int ord = (int) dv.lookupTerm(new BytesRef(FacetIndexWriter.pathToString(dim, path)));
+    int ord = (int) dv.lookupTerm(new BytesRef(FacetDocument.pathToString(dim, path)));
     if (ord < 0) {
       return -1;
     }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesReaderState.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesReaderState.java
index 75a4d37..03e1ebb 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesReaderState.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesReaderState.java
@@ -110,7 +110,7 @@ public final class SortedSetDocValuesReaderState {
     // support arbitrary hierarchy:
     for(int ord=0;ord<valueCount;ord++) {
       dv.lookupOrd(ord, spare);
-      String[] components = FacetIndexWriter.stringToPath(spare.utf8ToString());
+      String[] components = FacetDocument.stringToPath(spare.utf8ToString());
       if (components.length != 2) {
         throw new IllegalArgumentException("this class can only handle 2 level hierarchy (dim/value); got: " + Arrays.toString(components) + " " + spare.utf8ToString());
       }
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java
index 82a6cd6..845ddd6 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java
@@ -89,35 +89,36 @@ public class TestSimpleDrillSideways extends FacetTestCase {
     FacetsConfig config = new FacetsConfig();
     config.setHierarchical("Publish Date");
 
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, config);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, config);
 
     Document doc = new Document();
     doc.add(new FacetField("Author", "Bob"));
     doc.add(new FacetField("Publish Date", "2010", "10", "15"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new FacetField("Author", "Lisa"));
     doc.add(new FacetField("Publish Date", "2010", "10", "20"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new FacetField("Author", "Lisa"));
     doc.add(new FacetField("Publish Date", "2012", "1", "1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new FacetField("Author", "Susan"));
     doc.add(new FacetField("Publish Date", "2012", "1", "7"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new FacetField("Author", "Frank"));
     doc.add(new FacetField("Publish Date", "1999", "5", "5"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    IndexSearcher searcher = newSearcher(writer.getReader());
     writer.close();
 
     //System.out.println("searcher=" + searcher);
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSortedSetDocValuesFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSortedSetDocValuesFacets.java
index 37b1f28..a36ae7e 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSortedSetDocValuesFacets.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSortedSetDocValuesFacets.java
@@ -46,24 +46,25 @@ public class TestSortedSetDocValuesFacets extends FacetTestCase {
     Directory dir = newDirectory();
 
     FacetsConfig config = new FacetsConfig();
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), null, config);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetDocument facetDoc = new FacetDocument(null, config);
 
     Document doc = new Document();
     doc.add(new SortedSetDocValuesFacetField("a", "foo"));
     doc.add(new SortedSetDocValuesFacetField("a", "bar"));
     doc.add(new SortedSetDocValuesFacetField("a", "zoo"));
     doc.add(new SortedSetDocValuesFacetField("b", "baz"));
-    writer.addDocument(doc);
+    writer.addDocument(facetDoc.build(doc));
     if (random().nextBoolean()) {
       writer.commit();
     }
 
     doc = new Document();
     doc.add(new SortedSetDocValuesFacetField("a", "foo"));
-    writer.addDocument(doc);
+    writer.addDocument(facetDoc.build(doc));
 
     // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    IndexSearcher searcher = newSearcher(writer.getReader());
     writer.close();
 
     // Per-top-reader state:
@@ -94,24 +95,25 @@ public class TestSortedSetDocValuesFacets extends FacetTestCase {
     assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
     Directory dir = newDirectory();
 
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), null, new FacetsConfig());
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(null, new FacetsConfig());
 
     Document doc = new Document();
     doc.add(new SortedSetDocValuesFacetField("a", "foo"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
-    IndexReader r = DirectoryReader.open(writer, true);
+    IndexReader r = writer.getReader();
     SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(r);
 
     doc = new Document();
     doc.add(new SortedSetDocValuesFacetField("a", "bar"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new SortedSetDocValuesFacetField("a", "baz"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    IndexSearcher searcher = newSearcher(writer.getReader());
 
     SimpleFacetsCollector c = new SimpleFacetsCollector();
 
@@ -135,11 +137,12 @@ public class TestSortedSetDocValuesFacets extends FacetTestCase {
     assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
     Directory dir = newDirectory();
 
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), null, new FacetsConfig());
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(null, new FacetsConfig());
 
     Document doc = new Document();
     doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     if (random().nextBoolean()) {
       writer.commit();
@@ -148,7 +151,7 @@ public class TestSortedSetDocValuesFacets extends FacetTestCase {
     doc = new Document();
     doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
     doc.add(new SortedSetDocValuesFacetField("b", "bar1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     if (random().nextBoolean()) {
       writer.commit();
@@ -158,10 +161,10 @@ public class TestSortedSetDocValuesFacets extends FacetTestCase {
     doc.add(new SortedSetDocValuesFacetField("a", "foo3"));
     doc.add(new SortedSetDocValuesFacetField("b", "bar2"));
     doc.add(new SortedSetDocValuesFacetField("c", "baz1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    IndexSearcher searcher = newSearcher(writer.getReader());
     writer.close();
 
     // Per-top-reader state:
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetAssociations.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetAssociations.java
index a0642fa..f393e73 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetAssociations.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetAssociations.java
@@ -56,7 +56,7 @@ public class TestTaxonomyFacetAssociations extends FacetTestCase {
   private static final FacetLabel afloat = new FacetLabel("float", "a");
   private static final FacetLabel bfloat = new FacetLabel("float", "b");
   private static final FacetsConfig config = new FacetsConfig();
-  
+
   @BeforeClass
   public static void beforeClass() throws Exception {
     dir = newDirectory();
@@ -69,8 +69,8 @@ public class TestTaxonomyFacetAssociations extends FacetTestCase {
     config.setIndexFieldName("int", "$facets.int");
     config.setIndexFieldName("float", "$facets.float");
 
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter writer = new FacetIndexWriter(dir, iwc, taxoWriter, config);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, config);
 
     // index documents, 50% have only 'b' and all have 'a'
     for (int i = 0; i < 110; i++) {
@@ -85,11 +85,11 @@ public class TestTaxonomyFacetAssociations extends FacetTestCase {
           doc.add(new AssociationFacetField(0.2f, "float", "b"));
         }
       }
-      writer.addDocument(doc);
+      writer.addDocument(builder.build(doc));
     }
     
     taxoWriter.close();
-    reader = DirectoryReader.open(writer, true);
+    reader = writer.getReader();
     writer.close();
     taxoReader = new DirectoryTaxonomyReader(taxoDir);
   }
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts.java
index d547b10..f37785e 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts.java
@@ -61,35 +61,36 @@ public class TestTaxonomyFacetCounts extends FacetTestCase {
     FacetsConfig config = new FacetsConfig();
     config.setHierarchical("Publish Date");
 
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, config);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, config);
 
     Document doc = new Document();
     doc.add(new FacetField("Author", "Bob"));
     doc.add(new FacetField("Publish Date", "2010", "10", "15"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new FacetField("Author", "Lisa"));
     doc.add(new FacetField("Publish Date", "2010", "10", "20"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new FacetField("Author", "Lisa"));
     doc.add(new FacetField("Publish Date", "2012", "1", "1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new FacetField("Author", "Susan"));
     doc.add(new FacetField("Publish Date", "2012", "1", "7"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new FacetField("Author", "Frank"));
     doc.add(new FacetField("Publish Date", "1999", "5", "5"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    IndexSearcher searcher = newSearcher(writer.getReader());
     writer.close();
 
     // NRT open
@@ -147,11 +148,12 @@ public class TestTaxonomyFacetCounts extends FacetTestCase {
     // main index:
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
 
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, new FacetsConfig());
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, new FacetsConfig());
 
     Document doc = new Document();
     doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     if (random().nextBoolean()) {
       writer.commit();
@@ -160,7 +162,7 @@ public class TestTaxonomyFacetCounts extends FacetTestCase {
     doc = new Document();
     doc.add(new FacetField("a", "foo2"));
     doc.add(new FacetField("b", "bar1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     if (random().nextBoolean()) {
       writer.commit();
@@ -170,10 +172,10 @@ public class TestTaxonomyFacetCounts extends FacetTestCase {
     doc.add(new FacetField("a", "foo3"));
     doc.add(new FacetField("b", "bar2"));
     doc.add(new FacetField("c", "baz1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    IndexSearcher searcher = newSearcher(writer.getReader());
     writer.close();
 
     // NRT open
@@ -209,14 +211,15 @@ public class TestTaxonomyFacetCounts extends FacetTestCase {
 
     FacetsConfig config = new FacetsConfig();
     config.setIndexFieldName("a", "$facets2");
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, config);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, config);
 
     Document doc = new Document();
     doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    IndexSearcher searcher = newSearcher(writer.getReader());
     writer.close();
 
     // NRT open
@@ -280,12 +283,13 @@ public class TestTaxonomyFacetCounts extends FacetTestCase {
         }
       });
     TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    IndexWriter writer = new FacetIndexWriter(dir, iwc, taxoWriter, new FacetsConfig());
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, new FacetsConfig());
 
     Document doc = new Document();
     doc.add(newTextField("field", "text", Field.Store.NO));
     doc.add(new FacetField("a", "path"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
     writer.close();
     taxoWriter.close();
     dir.close();
@@ -296,20 +300,20 @@ public class TestTaxonomyFacetCounts extends FacetTestCase {
     Directory dir = newDirectory();
     Directory taxoDir = newDirectory();
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
     FacetsConfig config = new FacetsConfig();
     config.setHierarchical("a");
     config.setMultiValued("a");
-    IndexWriter writer = new FacetIndexWriter(dir, iwc, taxoWriter, config);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, config);
 
     Document doc = new Document();
     doc.add(newTextField("field", "text", Field.Store.NO));
     doc.add(new FacetField("a", "path", "x"));
     doc.add(new FacetField("a", "path", "y"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    IndexSearcher searcher = newSearcher(writer.getReader());
     writer.close();
 
     // NRT open
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetSumValueSource.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetSumValueSource.java
index 9970ddb..3a1b861 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetSumValueSource.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetSumValueSource.java
@@ -63,37 +63,38 @@ public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
     // main index:
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
 
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, new FacetsConfig());
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, new FacetsConfig());
 
     // Reused across documents, to add the necessary facet
     // fields:
     Document doc = new Document();
     doc.add(new IntField("num", 10, Field.Store.NO));
     doc.add(new FacetField("Author", "Bob"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new IntField("num", 20, Field.Store.NO));
     doc.add(new FacetField("Author", "Lisa"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new IntField("num", 30, Field.Store.NO));
     doc.add(new FacetField("Author", "Lisa"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new IntField("num", 40, Field.Store.NO));
     doc.add(new FacetField("Author", "Susan"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     doc = new Document();
     doc.add(new IntField("num", 45, Field.Store.NO));
     doc.add(new FacetField("Author", "Frank"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    IndexSearcher searcher = newSearcher(writer.getReader());
     writer.close();
 
     // NRT open
@@ -129,12 +130,13 @@ public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
     // main index:
     DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
 
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, new FacetsConfig());
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, new FacetsConfig());
 
     Document doc = new Document();
     doc.add(new IntField("num", 10, Field.Store.NO));
     doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     if (random().nextBoolean()) {
       writer.commit();
@@ -144,7 +146,7 @@ public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
     doc.add(new IntField("num", 20, Field.Store.NO));
     doc.add(new FacetField("a", "foo2"));
     doc.add(new FacetField("b", "bar1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     if (random().nextBoolean()) {
       writer.commit();
@@ -155,10 +157,10 @@ public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
     doc.add(new FacetField("a", "foo3"));
     doc.add(new FacetField("b", "bar2"));
     doc.add(new FacetField("c", "baz1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    IndexSearcher searcher = newSearcher(writer.getReader());
     writer.close();
 
     // NRT open
@@ -196,15 +198,16 @@ public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
     FacetsConfig config = new FacetsConfig();
     config.setIndexFieldName("a", "$facets2");
 
-    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, config);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DocumentBuilder builder = new DocumentBuilder(taxoWriter, config);
 
     Document doc = new Document();
     doc.add(new IntField("num", 10, Field.Store.NO));
     doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(doc);
+    writer.addDocument(builder.build(doc));
 
     // NRT open
-    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    IndexSearcher searcher = newSearcher(writer.getReader());
     writer.close();
 
     // NRT open

