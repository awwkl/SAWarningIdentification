GitDiffStart: 80743b6e78da7b26b2d1d6685e8ee821914b065d | Tue Nov 26 19:53:22 2013 +0000
diff --git a/TODO b/TODO
index 5976523..05d0a78 100644
--- a/TODO
+++ b/TODO
@@ -2,6 +2,7 @@ nocommit this!
 
 TODO
   - allow path.length==0?
+  - make a variant/sugar of FacetsConfig.build that just updates an existing doc?
   - need test coverage of utility search methods
   - move DocumentBuilder.build -> FacetsConfig.build
   - getSpecificValue for a dim isn't reliable
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FacetSource.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FacetSource.java
index 81b65d5..1e94543 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FacetSource.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FacetSource.java
@@ -20,7 +20,8 @@ package org.apache.lucene.benchmark.byTask.feeds;
 import java.io.IOException;
 import java.util.List;
 
-import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetsConfig;
 
 /**
  * Source items for facets.
@@ -34,7 +35,9 @@ public abstract class FacetSource extends ContentItemsSource {
    * account for multi-threading, as multiple threads can call this method
    * simultaneously.
    */
-  public abstract void getNextFacets(List<FacetLabel> facets) throws NoMoreDataException, IOException;
+  public abstract void getNextFacets(List<FacetField> facets) throws NoMoreDataException, IOException;
+
+  public abstract void configure(FacetsConfig config);
 
   @Override
   public void resetInputs() throws IOException {
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/RandomFacetSource.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/RandomFacetSource.java
index c492fef..8f9894f 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/RandomFacetSource.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/RandomFacetSource.java
@@ -22,7 +22,8 @@ import java.util.List;
 import java.util.Random;
 
 import org.apache.lucene.benchmark.byTask.utils.Config;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetsConfig;
 
 /**
  * Simple implementation of a random facet source
@@ -31,6 +32,9 @@ import org.apache.lucene.facet.taxonomy.FacetLabel;
  * <ul>
  * <li><b>rand.seed</b> - defines the seed to initialize {@link Random} with
  * (default: <b>13</b>).
+ * <li><b>max.doc.facet.dims</b> - Max number of random dimensions to
+ * create (default: <b>5</b>); actual number of dimensions
+ * would be anything between 1 and that number.
  * <li><b>max.doc.facets</b> - maximal #facets per doc (default: <b>10</b>).
  * Actual number of facets in a certain doc would be anything between 1 and that
  * number.
@@ -44,22 +48,33 @@ public class RandomFacetSource extends FacetSource {
   private Random random;
   private int maxDocFacets;
   private int maxFacetDepth;
+  private int maxDims;
   private int maxValue = maxDocFacets * maxFacetDepth;
   
   @Override
-  public void getNextFacets(List<FacetLabel> facets) throws NoMoreDataException, IOException {
+  public void getNextFacets(List<FacetField> facets) throws NoMoreDataException, IOException {
     facets.clear();
     int numFacets = 1 + random.nextInt(maxDocFacets); // at least one facet to each doc
     for (int i = 0; i < numFacets; i++) {
       int depth = 1 + random.nextInt(maxFacetDepth); // depth 0 is not useful
-      String[] components = new String[depth];
-      for (int k = 0; k < depth; k++) {
+
+      String dim = Integer.toString(random.nextInt(maxDims));
+      String[] components = new String[depth-1];
+      for (int k = 0; k < depth-1; k++) {
         components[k] = Integer.toString(random.nextInt(maxValue));
         addItem();
       }
-      FacetLabel cp = new FacetLabel(components);
-      facets.add(cp);
-      addBytes(cp.toString().length()); // very rough approximation
+      FacetField ff = new FacetField(dim, components);
+      facets.add(ff);
+      addBytes(ff.toString().length()); // very rough approximation
+    }
+  }
+
+  @Override
+  public void configure(FacetsConfig config) {
+    for(int i=0;i<maxDims;i++) {
+      config.setHierarchical(Integer.toString(i), true);
+      config.setMultiValued(Integer.toString(i), true);
     }
   }
 
@@ -73,6 +88,7 @@ public class RandomFacetSource extends FacetSource {
     super.setConfig(config);
     random = new Random(config.get("rand.seed", 13));
     maxDocFacets = config.get("max.doc.facets", 10);
+    maxDims = config.get("max.doc.facets.dims", 5);
     maxFacetDepth = config.get("max.facet.depth", 3);
     maxValue = maxDocFacets * maxFacetDepth;
   }
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java
index f73b731..a748773 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AddFacetedDocTask.java
@@ -22,8 +22,14 @@ import java.util.List;
 
 import org.apache.lucene.benchmark.byTask.PerfRunData;
 import org.apache.lucene.benchmark.byTask.feeds.FacetSource;
-import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetsConfig;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.index.IndexDocument;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.StorableField;
 
 /**
  * Add a faceted document.
@@ -44,8 +50,8 @@ import org.apache.lucene.facet.taxonomy.FacetLabel;
  */
 public class AddFacetedDocTask extends AddDocTask {
 
-  private final List<FacetLabel> facets = new ArrayList<FacetLabel>();
-  private FacetFields facetFields;
+  private final List<FacetField> facets = new ArrayList<FacetField>();
+  private FacetsConfig config;
   
   public AddFacetedDocTask(PerfRunData runData) {
     super(runData);
@@ -54,19 +60,22 @@ public class AddFacetedDocTask extends AddDocTask {
   @Override
   public void setup() throws Exception {
     super.setup();
-    if (facetFields == null) {
+    if (config == null) {
       boolean withFacets = getRunData().getConfig().get("with.facets", true);
       if (withFacets) {
+        // nocommit is this called once?  are we adding same
+        // facets over and over!?
         FacetSource facetsSource = getRunData().getFacetSource();
-        facetFields = withFacets ? new FacetFields(getRunData().getTaxonomyWriter()) : null;
+        config = new FacetsConfig(getRunData().getTaxonomyWriter());
         facetsSource.getNextFacets(facets);
+        facetsSource.configure(config);
       }
     }
   }
 
   @Override
   protected String getLogMessage(int recsCount) {
-    if (facetFields == null) {
+    if (config == null) {
       return super.getLogMessage(recsCount);
     }
     return super.getLogMessage(recsCount)+ " with facets";
@@ -74,10 +83,21 @@ public class AddFacetedDocTask extends AddDocTask {
   
   @Override
   public int doLogic() throws Exception {
-    if (facetFields != null) {
-      facetFields.addFields(doc, facets);
+    if (config != null) {
+      // nocommit hokey:
+      Document doc2 = new Document();
+      for(FacetField ff : facets) {
+        doc2.add(ff);
+      }
+      IndexDocument doc3 = config.build(doc2);
+      for(StorableField field : doc3.storableFields()) {
+        doc.add((Field) field);
+      }
+      for(IndexableField field : doc3.indexableFields()) {
+        doc.add((Field) field);
+      }
+      
     }
     return super.doLogic();
   }
-
 }
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
index 696dba2..adb55d6 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/AssociationsFacetsExample.java
@@ -23,14 +23,14 @@ import java.util.List;
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.simple.Facets;
-import org.apache.lucene.facet.simple.FacetsConfig;
-import org.apache.lucene.facet.simple.FloatAssociationFacetField;
-import org.apache.lucene.facet.simple.IntAssociationFacetField;
-import org.apache.lucene.facet.simple.SimpleFacetResult;
-import org.apache.lucene.facet.simple.SimpleFacetsCollector;
-import org.apache.lucene.facet.simple.TaxonomyFacetSumFloatAssociations;
-import org.apache.lucene.facet.simple.TaxonomyFacetSumIntAssociations;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FloatAssociationFacetField;
+import org.apache.lucene.facet.IntAssociationFacetField;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.TaxonomyFacetSumFloatAssociations;
+import org.apache.lucene.facet.TaxonomyFacetSumIntAssociations;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
@@ -98,13 +98,13 @@ public class AssociationsFacetsExample {
   }
 
   /** User runs a query and aggregates facets by summing their association values. */
-  private List<SimpleFacetResult> sumAssociations() throws IOException {
+  private List<FacetResult> sumAssociations() throws IOException {
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
     FacetsConfig config = getConfig(null);
     
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
+    FacetsCollector sfc = new FacetsCollector();
     
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
@@ -116,7 +116,7 @@ public class AssociationsFacetsExample {
     Facets genre = new TaxonomyFacetSumFloatAssociations("$genre", taxoReader, config, sfc);
 
     // Retrieve results
-    List<SimpleFacetResult> results = new ArrayList<SimpleFacetResult>();
+    List<FacetResult> results = new ArrayList<FacetResult>();
     results.add(tags.getTopChildren(10, "tags"));
     results.add(genre.getTopChildren(10, "genre"));
 
@@ -127,7 +127,7 @@ public class AssociationsFacetsExample {
   }
   
   /** Runs summing association example. */
-  public List<SimpleFacetResult> runSumAssociations() throws IOException {
+  public List<FacetResult> runSumAssociations() throws IOException {
     index();
     return sumAssociations();
   }
@@ -136,7 +136,7 @@ public class AssociationsFacetsExample {
   public static void main(String[] args) throws Exception {
     System.out.println("Sum associations example:");
     System.out.println("-------------------------");
-    List<SimpleFacetResult> results = new AssociationsFacetsExample().runSumAssociations();
+    List<FacetResult> results = new AssociationsFacetsExample().runSumAssociations();
     System.out.println("tags: " + results.get(0));
     System.out.println("genre: " + results.get(1));
   }
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java
index f1dd58b..04f3b8c 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/ExpressionAggregationFacetsExample.java
@@ -13,12 +13,12 @@ import org.apache.lucene.document.TextField;
 import org.apache.lucene.expressions.Expression;
 import org.apache.lucene.expressions.SimpleBindings;
 import org.apache.lucene.expressions.js.JavascriptCompiler;
-import org.apache.lucene.facet.simple.FacetField;
-import org.apache.lucene.facet.simple.Facets;
-import org.apache.lucene.facet.simple.FacetsConfig;
-import org.apache.lucene.facet.simple.SimpleFacetResult;
-import org.apache.lucene.facet.simple.SimpleFacetsCollector;
-import org.apache.lucene.facet.simple.TaxonomyFacetSumValueSource;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.TaxonomyFacetSumValueSource;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
@@ -86,7 +86,7 @@ public class ExpressionAggregationFacetsExample {
   }
 
   /** User runs a query and aggregates facets. */
-  private SimpleFacetResult search() throws IOException, ParseException {
+  private FacetResult search() throws IOException, ParseException {
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
@@ -100,7 +100,7 @@ public class ExpressionAggregationFacetsExample {
     bindings.add(new SortField("popularity", SortField.Type.LONG)); // the value of the 'popularity' field
 
     // Aggregates the facet values
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector(true);
+    FacetsCollector sfc = new FacetsCollector(true);
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
@@ -110,7 +110,7 @@ public class ExpressionAggregationFacetsExample {
 
     // Retrieve results
     Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, expr.getValueSource(bindings));
-    SimpleFacetResult result = facets.getTopChildren(10, "A");
+    FacetResult result = facets.getTopChildren(10, "A");
     
     indexReader.close();
     taxoReader.close();
@@ -119,7 +119,7 @@ public class ExpressionAggregationFacetsExample {
   }
   
   /** Runs the search example. */
-  public SimpleFacetResult runSearch() throws IOException, ParseException {
+  public FacetResult runSearch() throws IOException, ParseException {
     index();
     return search();
   }
@@ -128,8 +128,7 @@ public class ExpressionAggregationFacetsExample {
   public static void main(String[] args) throws Exception {
     System.out.println("Facet counting example:");
     System.out.println("-----------------------");
-    SimpleFacetResult result = new ExpressionAggregationFacetsExample().runSearch();
+    FacetResult result = new ExpressionAggregationFacetsExample().runSearch();
     System.out.println(result);
   }
-  
 }
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java
index 9504d23..32e98b8 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/MultiCategoryListsFacetsExample.java
@@ -25,12 +25,12 @@ import java.util.Map;
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.simple.FacetField;
-import org.apache.lucene.facet.simple.Facets;
-import org.apache.lucene.facet.simple.FacetsConfig;
-import org.apache.lucene.facet.simple.FastTaxonomyFacetCounts;
-import org.apache.lucene.facet.simple.SimpleFacetResult;
-import org.apache.lucene.facet.simple.SimpleFacetsCollector;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FastTaxonomyFacetCounts;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsCollector;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
@@ -103,13 +103,13 @@ public class MultiCategoryListsFacetsExample {
   }
 
   /** User runs a query and counts facets. */
-  private List<SimpleFacetResult> search() throws IOException {
+  private List<FacetResult> search() throws IOException {
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
     FacetsConfig config = getConfig(null);
 
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
+    FacetsCollector sfc = new FacetsCollector();
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
@@ -118,7 +118,7 @@ public class MultiCategoryListsFacetsExample {
     searcher.search(new MatchAllDocsQuery(), sfc);
 
     // Retrieve results
-    List<SimpleFacetResult> results = new ArrayList<SimpleFacetResult>();
+    List<FacetResult> results = new ArrayList<FacetResult>();
 
     // Count both "Publish Date" and "Author" dimensions
     Facets author = new FastTaxonomyFacetCounts("author", taxoReader, config, sfc);
@@ -134,7 +134,7 @@ public class MultiCategoryListsFacetsExample {
   }
 
   /** Runs the search example. */
-  public List<SimpleFacetResult> runSearch() throws IOException {
+  public List<FacetResult> runSearch() throws IOException {
     index();
     return search();
   }
@@ -143,7 +143,7 @@ public class MultiCategoryListsFacetsExample {
   public static void main(String[] args) throws Exception {
     System.out.println("Facet counting over multiple category lists example:");
     System.out.println("-----------------------");
-    List<SimpleFacetResult> results = new MultiCategoryListsFacetsExample().runSearch();
+    List<FacetResult> results = new MultiCategoryListsFacetsExample().runSearch();
     System.out.println("Author: " + results.get(0));
     System.out.println("Publish Date: " + results.get(1));
   }
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
index 1c3c631..7108281 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/RangeFacetsExample.java
@@ -26,13 +26,13 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.LongField;
 import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.facet.simple.Facets;
-import org.apache.lucene.facet.simple.FacetsConfig;
-import org.apache.lucene.facet.simple.LongRange;
-import org.apache.lucene.facet.simple.RangeFacetCounts;
-import org.apache.lucene.facet.simple.SimpleDrillDownQuery;
-import org.apache.lucene.facet.simple.SimpleFacetResult;
-import org.apache.lucene.facet.simple.SimpleFacetsCollector;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.LongRange;
+import org.apache.lucene.facet.RangeFacetCounts;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsCollector;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -86,10 +86,10 @@ public class RangeFacetsExample implements Closeable {
   }
 
   /** User runs a query and counts facets. */
-  public SimpleFacetResult search() throws IOException {
+  public FacetResult search() throws IOException {
 
     // Aggregates the facet counts
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
+    FacetsCollector sfc = new FacetsCollector();
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
@@ -109,7 +109,7 @@ public class RangeFacetsExample implements Closeable {
 
     // Passing no baseQuery means we drill down on all
     // documents ("browse only"):
-    SimpleDrillDownQuery q = new SimpleDrillDownQuery(getConfig());
+    DrillDownQuery q = new DrillDownQuery(getConfig());
 
     // Use FieldCacheRangeFilter; this will use
     // NumericDocValues:
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java
index f1a0569..4cd1a88 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleFacetsExample.java
@@ -23,13 +23,13 @@ import java.util.List;
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.simple.FacetField;
-import org.apache.lucene.facet.simple.Facets;
-import org.apache.lucene.facet.simple.FacetsConfig;
-import org.apache.lucene.facet.simple.FastTaxonomyFacetCounts;
-import org.apache.lucene.facet.simple.SimpleDrillDownQuery;
-import org.apache.lucene.facet.simple.SimpleFacetResult;
-import org.apache.lucene.facet.simple.SimpleFacetsCollector;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FastTaxonomyFacetCounts;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsCollector;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
@@ -99,13 +99,13 @@ public class SimpleFacetsExample {
   }
 
   /** User runs a query and counts facets. */
-  private List<SimpleFacetResult> search() throws IOException {
+  private List<FacetResult> search() throws IOException {
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
     FacetsConfig config = getConfig(null);
 
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
+    FacetsCollector sfc = new FacetsCollector();
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
@@ -114,7 +114,7 @@ public class SimpleFacetsExample {
     searcher.search(new MatchAllDocsQuery(), sfc);
 
     // Retrieve results
-    List<SimpleFacetResult> results = new ArrayList<SimpleFacetResult>();
+    List<FacetResult> results = new ArrayList<FacetResult>();
 
     // Count both "Publish Date" and "Author" dimensions
     Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, sfc);
@@ -128,7 +128,7 @@ public class SimpleFacetsExample {
   }
   
   /** User drills down on 'Publish Date/2010'. */
-  private SimpleFacetResult drillDown() throws IOException {
+  private FacetResult drillDown() throws IOException {
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
@@ -136,16 +136,16 @@ public class SimpleFacetsExample {
 
     // Passing no baseQuery means we drill down on all
     // documents ("browse only"):
-    SimpleDrillDownQuery q = new SimpleDrillDownQuery(config);
+    DrillDownQuery q = new DrillDownQuery(config);
 
     // Now user drills down on Publish Date/2010:
     q.add("Publish Date", "2010");
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
+    FacetsCollector sfc = new FacetsCollector();
     searcher.search(q, sfc);
 
     // Retrieve results
     Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, sfc);
-    SimpleFacetResult result = facets.getTopChildren(10, "Author");
+    FacetResult result = facets.getTopChildren(10, "Author");
 
     indexReader.close();
     taxoReader.close();
@@ -154,13 +154,13 @@ public class SimpleFacetsExample {
   }
 
   /** Runs the search example. */
-  public List<SimpleFacetResult> runSearch() throws IOException {
+  public List<FacetResult> runSearch() throws IOException {
     index();
     return search();
   }
   
   /** Runs the drill-down example. */
-  public SimpleFacetResult runDrillDown() throws IOException {
+  public FacetResult runDrillDown() throws IOException {
     index();
     return drillDown();
   }
@@ -170,7 +170,7 @@ public class SimpleFacetsExample {
     System.out.println("Facet counting example:");
     System.out.println("-----------------------");
     SimpleFacetsExample example = new SimpleFacetsExample();
-    List<SimpleFacetResult> results = example.runSearch();
+    List<FacetResult> results = example.runSearch();
     System.out.println("Author: " + results.get(0));
     System.out.println("Publish Date: " + results.get(1));
 
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
index b42117a..2058ba3 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/SimpleSortedSetFacetsExample.java
@@ -23,14 +23,14 @@ import java.util.List;
 
 import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.simple.Facets;
-import org.apache.lucene.facet.simple.FacetsConfig;
-import org.apache.lucene.facet.simple.SimpleDrillDownQuery;
-import org.apache.lucene.facet.simple.SimpleFacetResult;
-import org.apache.lucene.facet.simple.SimpleFacetsCollector;
-import org.apache.lucene.facet.simple.SortedSetDocValuesFacetCounts;
-import org.apache.lucene.facet.simple.SortedSetDocValuesFacetField;
-import org.apache.lucene.facet.simple.SortedSetDocValuesReaderState;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.SortedSetDocValuesFacetCounts;
+import org.apache.lucene.facet.SortedSetDocValuesFacetField;
+import org.apache.lucene.facet.SortedSetDocValuesReaderState;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexWriter;
@@ -89,14 +89,14 @@ public class SimpleSortedSetFacetsExample {
   }
 
   /** User runs a query and counts facets. */
-  private List<SimpleFacetResult> search() throws IOException {
+  private List<FacetResult> search() throws IOException {
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
     SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(indexReader);
     FacetsConfig config = getConfig();
 
     // Aggregatses the facet counts
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
+    FacetsCollector sfc = new FacetsCollector();
 
     // MatchAllDocsQuery is for "browsing" (counts facets
     // for all non-deleted docs in the index); normally
@@ -107,7 +107,7 @@ public class SimpleSortedSetFacetsExample {
     // Retrieve results
     Facets facets = new SortedSetDocValuesFacetCounts(state, sfc);
 
-    List<SimpleFacetResult> results = new ArrayList<SimpleFacetResult>();
+    List<FacetResult> results = new ArrayList<FacetResult>();
     results.add(facets.getTopChildren(10, "Author"));
     results.add(facets.getTopChildren(10, "Publish Year"));
     indexReader.close();
@@ -116,34 +116,34 @@ public class SimpleSortedSetFacetsExample {
   }
   
   /** User drills down on 'Publish Year/2010'. */
-  private SimpleFacetResult drillDown() throws IOException {
+  private FacetResult drillDown() throws IOException {
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
     SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(indexReader);
     FacetsConfig config = getConfig();
 
     // Now user drills down on Publish Year/2010:
-    SimpleDrillDownQuery q = new SimpleDrillDownQuery(config);
+    DrillDownQuery q = new DrillDownQuery(config);
     q.add("Publish Year", "2010");
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
+    FacetsCollector sfc = new FacetsCollector();
     searcher.search(q, sfc);
 
     // Retrieve results
     Facets facets = new SortedSetDocValuesFacetCounts(state, sfc);
-    SimpleFacetResult result = facets.getTopChildren(10, "Author");
+    FacetResult result = facets.getTopChildren(10, "Author");
     indexReader.close();
     
     return result;
   }
 
   /** Runs the search example. */
-  public List<SimpleFacetResult> runSearch() throws IOException {
+  public List<FacetResult> runSearch() throws IOException {
     index();
     return search();
   }
   
   /** Runs the drill-down example. */
-  public SimpleFacetResult runDrillDown() throws IOException {
+  public FacetResult runDrillDown() throws IOException {
     index();
     return drillDown();
   }
@@ -153,7 +153,7 @@ public class SimpleSortedSetFacetsExample {
     System.out.println("Facet counting example:");
     System.out.println("-----------------------");
     SimpleSortedSetFacetsExample example = new SimpleSortedSetFacetsExample();
-    List<SimpleFacetResult> results = example.runSearch();
+    List<FacetResult> results = example.runSearch();
     System.out.println("Author: " + results.get(0));
     System.out.println("Publish Year: " + results.get(0));
 
diff --git a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationsFacetsExample.java b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationsFacetsExample.java
index 1455a36..85133e5 100644
--- a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationsFacetsExample.java
+++ b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationsFacetsExample.java
@@ -19,7 +19,7 @@ package org.apache.lucene.demo.facet;
 
 import java.util.List;
 
-import org.apache.lucene.facet.simple.SimpleFacetResult;
+import org.apache.lucene.facet.FacetResult;
 import org.apache.lucene.util.LuceneTestCase;
 import org.junit.Test;
 
@@ -27,7 +27,7 @@ public class TestAssociationsFacetsExample extends LuceneTestCase {
   
   @Test
   public void testExamples() throws Exception {
-    List<SimpleFacetResult> res = new AssociationsFacetsExample().runSumAssociations();
+    List<FacetResult> res = new AssociationsFacetsExample().runSumAssociations();
     assertEquals("Wrong number of results", 2, res.size());
     assertEquals("value=6 childCount=2\n  lucene (4)\n  solr (2)\n", res.get(0).toString());
     assertEquals("value=1.96 childCount=2\n  computing (1.62)\n  software (0.34)\n", res.get(1).toString());
diff --git a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestExpressionAggregationFacetsExample.java b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestExpressionAggregationFacetsExample.java
index 2b0a59a..0b4e552 100644
--- a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestExpressionAggregationFacetsExample.java
+++ b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestExpressionAggregationFacetsExample.java
@@ -20,7 +20,7 @@ package org.apache.lucene.demo.facet;
 import java.util.List;
 import java.util.Locale;
 
-import org.apache.lucene.facet.simple.SimpleFacetResult;
+import org.apache.lucene.facet.FacetResult;
 import org.apache.lucene.util.LuceneTestCase;
 import org.junit.Test;
 
@@ -28,7 +28,7 @@ public class TestExpressionAggregationFacetsExample extends LuceneTestCase {
 
   @Test
   public void testSimple() throws Exception {
-    SimpleFacetResult result = new ExpressionAggregationFacetsExample().runSearch();
+    FacetResult result = new ExpressionAggregationFacetsExample().runSearch();
     assertEquals("value=3.9681187 childCount=2\n  B (2.236068)\n  C (1.7320508)\n", result.toString());
   }
 }
diff --git a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCategoryListsFacetsExample.java b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCategoryListsFacetsExample.java
index af2b89f..070f0e7 100644
--- a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCategoryListsFacetsExample.java
+++ b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCategoryListsFacetsExample.java
@@ -19,7 +19,7 @@ package org.apache.lucene.demo.facet;
 
 import java.util.List;
 
-import org.apache.lucene.facet.simple.SimpleFacetResult;
+import org.apache.lucene.facet.FacetResult;
 import org.apache.lucene.util.LuceneTestCase;
 import org.junit.Test;
 
@@ -27,7 +27,7 @@ public class TestMultiCategoryListsFacetsExample extends LuceneTestCase {
 
   @Test
   public void testExample() throws Exception {
-    List<SimpleFacetResult> results = new MultiCategoryListsFacetsExample().runSearch();
+    List<FacetResult> results = new MultiCategoryListsFacetsExample().runSearch();
     assertEquals(2, results.size());
     assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", results.get(0).toString());
     assertEquals("value=5 childCount=3\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", results.get(1).toString());
diff --git a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestRangeFacetsExample.java b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestRangeFacetsExample.java
index 5865f8d..6fe08ad 100644
--- a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestRangeFacetsExample.java
+++ b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestRangeFacetsExample.java
@@ -19,7 +19,7 @@ package org.apache.lucene.demo.facet;
 
 import java.util.List;
 
-import org.apache.lucene.facet.simple.SimpleFacetResult;
+import org.apache.lucene.facet.FacetResult;
 import org.apache.lucene.search.TopDocs;
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.LuceneTestCase;
@@ -32,7 +32,7 @@ public class TestRangeFacetsExample extends LuceneTestCase {
   public void testSimple() throws Exception {
     RangeFacetsExample example = new RangeFacetsExample();
     example.index();
-    SimpleFacetResult result = example.search();
+    FacetResult result = example.search();
     assertEquals("value=100 childCount=3\n  Past hour (4)\n  Past six hours (22)\n  Past day (87)\n", result.toString());
     example.close();
   }
diff --git a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleFacetsExample.java b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleFacetsExample.java
index 4e66121..dc6e4f4 100644
--- a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleFacetsExample.java
+++ b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleFacetsExample.java
@@ -19,7 +19,7 @@ package org.apache.lucene.demo.facet;
 
 import java.util.List;
 
-import org.apache.lucene.facet.simple.SimpleFacetResult;
+import org.apache.lucene.facet.FacetResult;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.util.LuceneTestCase;
 import org.junit.Test;
@@ -28,7 +28,7 @@ public class TestSimpleFacetsExample extends LuceneTestCase {
 
   @Test
   public void testSimple() throws Exception {
-    List<SimpleFacetResult> results = new SimpleFacetsExample().runSearch();
+    List<FacetResult> results = new SimpleFacetsExample().runSearch();
     assertEquals(2, results.size());
     assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", results.get(0).toString());
     assertEquals("value=5 childCount=3\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", results.get(1).toString());
@@ -36,7 +36,7 @@ public class TestSimpleFacetsExample extends LuceneTestCase {
 
   @Test
   public void testDrillDown() throws Exception {
-    SimpleFacetResult result = new SimpleFacetsExample().runDrillDown();
+    FacetResult result = new SimpleFacetsExample().runDrillDown();
     assertEquals("value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", result.toString());
   }
 }
diff --git a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleSortedSetFacetsExample.java b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleSortedSetFacetsExample.java
index 9e3e46c..6908dfe 100644
--- a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleSortedSetFacetsExample.java
+++ b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleSortedSetFacetsExample.java
@@ -19,7 +19,7 @@ package org.apache.lucene.demo.facet;
 
 import java.util.List;
 
-import org.apache.lucene.facet.simple.SimpleFacetResult;
+import org.apache.lucene.facet.FacetResult;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util.LuceneTestCase;
@@ -32,7 +32,7 @@ public class TestSimpleSortedSetFacetsExample extends LuceneTestCase {
 
   @Test
   public void testSimple() throws Exception {
-    List<SimpleFacetResult> results = new SimpleSortedSetFacetsExample().runSearch();
+    List<FacetResult> results = new SimpleSortedSetFacetsExample().runSearch();
     assertEquals(2, results.size());
     assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Frank (1)\n  Susan (1)\n", results.get(0).toString());
     assertEquals("value=5 childCount=3\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", results.get(1).toString());
@@ -40,7 +40,7 @@ public class TestSimpleSortedSetFacetsExample extends LuceneTestCase {
 
   @Test
   public void testDrillDown() throws Exception {
-    SimpleFacetResult result = new SimpleSortedSetFacetsExample().runDrillDown();
+    FacetResult result = new SimpleSortedSetFacetsExample().runDrillDown();
     assertEquals("value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", result.toString());
   }
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/AssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/AssociationFacetField.java
new file mode 100644
index 0000000..4c92fe9
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/AssociationFacetField.java
@@ -0,0 +1,62 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.util.BytesRef;
+
+/** Associates an arbitrary byte[] with the added facet
+ *  path. */
+public class AssociationFacetField extends Field {
+  static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setIndexed(true);
+    TYPE.freeze();
+  }
+  protected final String dim;
+  protected final String[] path;
+  protected final BytesRef assoc;
+
+  public AssociationFacetField(BytesRef assoc, String dim, String... path) {
+    super("dummy", TYPE);
+    this.dim = dim;
+    this.assoc = assoc;
+    if (path.length == 0) {
+      throw new IllegalArgumentException("path must have at least one element");
+    }
+    this.path = path;
+  }
+
+  private static BytesRef intToBytesRef(int v) {
+    byte[] bytes = new byte[4];
+    // big-endian:
+    bytes[0] = (byte) (v >> 24);
+    bytes[1] = (byte) (v >> 16);
+    bytes[2] = (byte) (v >> 8);
+    bytes[3] = (byte) v;
+    return new BytesRef(bytes);
+  }
+
+  @Override
+  public String toString() {
+    return "AssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " bytes=" + assoc + ")";
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/CachedOrdinalsReader.java b/lucene/facet/src/java/org/apache/lucene/facet/CachedOrdinalsReader.java
new file mode 100644
index 0000000..bec0095
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/CachedOrdinalsReader.java
@@ -0,0 +1,147 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.WeakHashMap;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/**
+ * A per-segment cache of documents' facet ordinals. Every
+ * {@link CachedOrds} holds the ordinals in a raw {@code
+ * int[]}, and therefore consumes as much RAM as the total
+ * number of ordinals found in the segment, but saves the
+ * CPU cost of decoding ordinals during facet counting.
+ * 
+ * <p>
+ * <b>NOTE:</b> every {@link CachedOrds} is limited to 2.1B
+ * total ordinals. If that is a limitation for you then
+ * consider limiting the segment size to fewer documents, or
+ * use an alternative cache which pages through the category
+ * ordinals.
+ * 
+ * <p>
+ * <b>NOTE:</b> when using this cache, it is advised to use
+ * a {@link DocValuesFormat} that does not cache the data in
+ * memory, at least for the category lists fields, or
+ * otherwise you'll be doing double-caching.
+ *
+ * <p>
+ * <b>NOTE:</b> create one instance of this and re-use it
+ * for all facet implementations (the cache is per-instance,
+ * not static).
+ */
+public class CachedOrdinalsReader extends OrdinalsReader {
+
+  private final OrdinalsReader source;
+
+  private final Map<Object,CachedOrds> ordsCache = new WeakHashMap<Object,CachedOrds>();
+
+  public CachedOrdinalsReader(OrdinalsReader source) {
+    this.source = source;
+  }
+
+  private synchronized CachedOrds getCachedOrds(AtomicReaderContext context) throws IOException {
+    Object cacheKey = context.reader().getCoreCacheKey();
+    CachedOrds ords = ordsCache.get(cacheKey);
+    if (ords == null) {
+      ords = new CachedOrds(source.getReader(context), context.reader().maxDoc());
+      ordsCache.put(cacheKey, ords);
+    }
+
+    return ords;
+  }
+
+  @Override
+  public String getIndexFieldName() {
+    return source.getIndexFieldName();
+  }
+
+  @Override
+  public OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException {
+    final CachedOrds cachedOrds = getCachedOrds(context);
+    return new OrdinalsSegmentReader() {
+      @Override
+      public void get(int docID, IntsRef ordinals) {
+        ordinals.ints = cachedOrds.ordinals;
+        ordinals.offset = cachedOrds.offsets[docID];
+        ordinals.length = cachedOrds.offsets[docID+1] - ordinals.offset;
+      }
+    };
+  }
+
+  /** Holds the cached ordinals in two paralel {@code int[]} arrays. */
+  public static final class CachedOrds {
+    
+    public final int[] offsets;
+    public final int[] ordinals;
+
+    /**
+     * Creates a new {@link CachedOrds} from the {@link BinaryDocValues}.
+     * Assumes that the {@link BinaryDocValues} is not {@code null}.
+     */
+    public CachedOrds(OrdinalsSegmentReader source, int maxDoc) throws IOException {
+      final BytesRef buf = new BytesRef();
+
+      offsets = new int[maxDoc + 1];
+      int[] ords = new int[maxDoc]; // let's assume one ordinal per-document as an initial size
+
+      // this aggregator is limited to Integer.MAX_VALUE total ordinals.
+      long totOrds = 0;
+      final IntsRef values = new IntsRef(32);
+      for (int docID = 0; docID < maxDoc; docID++) {
+        offsets[docID] = (int) totOrds;
+        source.get(docID, values);
+        long nextLength = totOrds + values.length;
+        if (nextLength > ords.length) {
+          if (nextLength > ArrayUtil.MAX_ARRAY_LENGTH) {
+            throw new IllegalStateException("too many ordinals (>= " + nextLength + ") to cache");
+          }
+          ords = ArrayUtil.grow(ords, (int) nextLength);
+        }
+        System.arraycopy(values.ints, 0, ords, (int) totOrds, values.length);
+        totOrds = nextLength;
+      }
+      offsets[maxDoc] = (int) totOrds;
+      
+      // if ords array is bigger by more than 10% of what we really need, shrink it
+      if ((double) totOrds / ords.length < 0.9) { 
+        this.ordinals = new int[(int) totOrds];
+        System.arraycopy(ords, 0, this.ordinals, 0, (int) totOrds);
+      } else {
+        this.ordinals = ords;
+      }
+    }
+  }
+
+  /** How many bytes is this cache using? */
+  public synchronized long ramBytesUsed() {
+    long bytes = 0;
+    for(CachedOrds ords : ordsCache.values()) {
+      bytes += RamUsageEstimator.sizeOf(ords);
+    }
+
+    return bytes;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/DocValuesOrdinalsReader.java b/lucene/facet/src/java/org/apache/lucene/facet/DocValuesOrdinalsReader.java
new file mode 100644
index 0000000..0f91f3b
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/DocValuesOrdinalsReader.java
@@ -0,0 +1,97 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/** Decodes ordinals previously indexed into a BinaryDocValues field */
+
+public class DocValuesOrdinalsReader extends OrdinalsReader {
+  private final String field;
+
+  public DocValuesOrdinalsReader() {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME);
+  }
+
+  public DocValuesOrdinalsReader(String field) {
+    this.field = field;
+  }
+
+  @Override
+  public OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException {
+    BinaryDocValues values0 = context.reader().getBinaryDocValues(field);
+    if (values0 == null) {
+      values0 = BinaryDocValues.EMPTY;
+    }
+
+    final BinaryDocValues values = values0;
+
+    return new OrdinalsSegmentReader() {
+      private final BytesRef bytes = new BytesRef(32);
+
+      @Override
+      public void get(int docID, IntsRef ordinals) throws IOException {
+        values.get(docID, bytes);
+        decode(bytes, ordinals);
+      }
+    };
+  }
+
+  @Override
+  public String getIndexFieldName() {
+    return field;
+  }
+
+  /** Subclass & override if you change the encoding. */
+  protected void decode(BytesRef buf, IntsRef ordinals) {
+
+    // grow the buffer up front, even if by a large number of values (buf.length)
+    // that saves the need to check inside the loop for every decoded value if
+    // the buffer needs to grow.
+    if (ordinals.ints.length < buf.length) {
+      ordinals.ints = ArrayUtil.grow(ordinals.ints, buf.length);
+    }
+
+    ordinals.offset = 0;
+    ordinals.length = 0;
+
+    // it is better if the decoding is inlined like so, and not e.g.
+    // in a utility method
+    int upto = buf.offset + buf.length;
+    int value = 0;
+    int offset = buf.offset;
+    int prev = 0;
+    while (offset < upto) {
+      byte b = buf.bytes[offset++];
+      if (b >= 0) {
+        ordinals.ints[ordinals.length] = ((value << 7) | b) + prev;
+        value = 0;
+        prev = ordinals.ints[ordinals.length];
+        ordinals.length++;
+      } else {
+        value = (value << 7) | (b & 0x7F);
+      }
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/DoubleRange.java b/lucene/facet/src/java/org/apache/lucene/facet/DoubleRange.java
new file mode 100644
index 0000000..8ec025a
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/DoubleRange.java
@@ -0,0 +1,71 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.DoubleDocValuesField; // javadocs
+
+/** Represents a range over double values indexed as {@link
+ *  DoubleDocValuesField}.  */
+public final class DoubleRange extends Range {
+  private final double minIncl;
+  private final double maxIncl;
+
+  public final double min;
+  public final double max;
+  public final boolean minInclusive;
+  public final boolean maxInclusive;
+
+  /** Create a DoubleRange. */
+  public DoubleRange(String label, double min, boolean minInclusive, double max, boolean maxInclusive) {
+    super(label);
+    this.min = min;
+    this.max = max;
+    this.minInclusive = minInclusive;
+    this.maxInclusive = maxInclusive;
+
+    // TODO: if DoubleDocValuesField used
+    // NumericUtils.doubleToSortableLong format (instead of
+    // Double.doubleToRawLongBits) we could do comparisons
+    // in long space 
+
+    if (Double.isNaN(min)) {
+      throw new IllegalArgumentException("min cannot be NaN");
+    }
+    if (!minInclusive) {
+      min = Math.nextUp(min);
+    }
+
+    if (Double.isNaN(max)) {
+      throw new IllegalArgumentException("max cannot be NaN");
+    }
+    if (!maxInclusive) {
+      // Why no Math.nextDown?
+      max = Math.nextAfter(max, Double.NEGATIVE_INFINITY);
+    }
+
+    this.minIncl = min;
+    this.maxIncl = max;
+  }
+
+  @Override
+  public boolean accept(long value) {
+    double doubleValue = Double.longBitsToDouble(value);
+    return doubleValue >= minIncl && doubleValue <= maxIncl;
+  }
+}
+
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java b/lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java
new file mode 100644
index 0000000..c789a14
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/DrillDownQuery.java
@@ -0,0 +1,225 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.FilteredQuery;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+
+/**
+ * A {@link Query} for drill-down over {@link FacetLabel categories}. You
+ * should call {@link #add(FacetLabel...)} for every group of categories you
+ * want to drill-down over. Each category in the group is {@code OR'ed} with
+ * the others, and groups are {@code AND'ed}.
+ * <p>
+ * <b>NOTE:</b> if you choose to create your own {@link Query} by calling
+ * {@link #term}, it is recommended to wrap it with {@link ConstantScoreQuery}
+ * and set the {@link ConstantScoreQuery#setBoost(float) boost} to {@code 0.0f},
+ * so that it does not affect the scores of the documents.
+ * 
+ * @lucene.experimental
+ */
+public final class DrillDownQuery extends Query {
+
+  public static Term term(String field, String dim, String... path) {
+    return new Term(field, FacetsConfig.pathToString(dim, path));
+  }
+
+  private final FacetsConfig config;
+  private final BooleanQuery query;
+  private final Map<String,Integer> drillDownDims = new LinkedHashMap<String,Integer>();
+
+  /** Used by clone() */
+  DrillDownQuery(FacetsConfig config, BooleanQuery query, Map<String,Integer> drillDownDims) {
+    this.query = query.clone();
+    this.drillDownDims.putAll(drillDownDims);
+    this.config = config;
+  }
+
+  /** Used by DrillSideways */
+  DrillDownQuery(FacetsConfig config, Filter filter, DrillDownQuery other) {
+    query = new BooleanQuery(true); // disable coord
+
+    BooleanClause[] clauses = other.query.getClauses();
+    if (clauses.length == other.drillDownDims.size()) {
+      throw new IllegalArgumentException("cannot apply filter unless baseQuery isn't null; pass ConstantScoreQuery instead");
+    }
+    assert clauses.length == 1+other.drillDownDims.size(): clauses.length + " vs " + (1+other.drillDownDims.size());
+    drillDownDims.putAll(other.drillDownDims);
+    query.add(new FilteredQuery(clauses[0].getQuery(), filter), Occur.MUST);
+    for(int i=1;i<clauses.length;i++) {
+      query.add(clauses[i].getQuery(), Occur.MUST);
+    }
+    this.config = config;
+  }
+
+  /** Used by DrillSideways */
+  DrillDownQuery(FacetsConfig config, Query baseQuery, List<Query> clauses, Map<String,Integer> drillDownDims) {
+    this.query = new BooleanQuery(true);
+    if (baseQuery != null) {
+      query.add(baseQuery, Occur.MUST);      
+    }
+    for(Query clause : clauses) {
+      query.add(clause, Occur.MUST);
+    }
+    this.drillDownDims.putAll(drillDownDims);
+    this.config = config;
+  }
+
+  /**
+   * Creates a new {@code DrillDownQuery} without a base query, 
+   * to perform a pure browsing query (equivalent to using
+   * {@link MatchAllDocsQuery} as base).
+   */
+  public DrillDownQuery(FacetsConfig config) {
+    this(config, null);
+  }
+  
+  /**
+   * Creates a new {@code DrillDownQuery} over the given base query. Can be
+   * {@code null}, in which case the result {@link Query} from
+   * {@link #rewrite(IndexReader)} will be a pure browsing query, filtering on
+   * the added categories only.
+   */
+  public DrillDownQuery(FacetsConfig config, Query baseQuery) {
+    query = new BooleanQuery(true); // disable coord
+    if (baseQuery != null) {
+      query.add(baseQuery, Occur.MUST);
+    }
+    this.config = config;
+  }
+
+  /** Merges (ORs) a new path into an existing AND'd
+   *  clause. */ 
+  private void merge(String dim, String[] path) {
+    int index = drillDownDims.get(dim);
+    if (query.getClauses().length == drillDownDims.size()+1) {
+      index++;
+    }
+    ConstantScoreQuery q = (ConstantScoreQuery) query.clauses().get(index).getQuery();
+    if ((q.getQuery() instanceof BooleanQuery) == false) {
+      // App called .add(dim, customQuery) and then tried to
+      // merge a facet label in:
+      throw new RuntimeException("cannot merge with custom Query");
+    }
+    String indexedField = config.getDimConfig(dim).indexFieldName;
+
+    BooleanQuery bq = (BooleanQuery) q.getQuery();
+    bq.add(new TermQuery(term(indexedField, dim, path)), Occur.SHOULD);
+  }
+
+  /** Adds one dimension of drill downs; if you pass the same
+   *  dimension again, it's OR'd with the previous
+   *  constraints on that dimension, and all dimensions are
+   *  AND'd against each other and the base query. */
+  // nocommit can we remove FacetLabel here?
+  public void add(String dim, String... path) {
+
+    if (drillDownDims.containsKey(dim)) {
+      merge(dim, path);
+      return;
+    }
+    String indexedField = config.getDimConfig(dim).indexFieldName;
+
+    BooleanQuery bq = new BooleanQuery(true); // disable coord
+    // nocommit too anal?
+    /*
+    if (path.length == 0) {
+      throw new IllegalArgumentException("must have at least one facet label under dim");
+    }
+    */
+    bq.add(new TermQuery(term(indexedField, dim, path)), Occur.SHOULD);
+
+    add(dim, bq);
+  }
+
+  /** Expert: add a custom drill-down subQuery.  Use this
+   *  when you have a separate way to drill-down on the
+   *  dimension than the indexed facet ordinals. */
+  public void add(String dim, Query subQuery) {
+
+    // TODO: we should use FilteredQuery?
+
+    // So scores of the drill-down query don't have an
+    // effect:
+    final ConstantScoreQuery drillDownQuery = new ConstantScoreQuery(subQuery);
+    drillDownQuery.setBoost(0.0f);
+
+    query.add(drillDownQuery, Occur.MUST);
+
+    drillDownDims.put(dim, drillDownDims.size());
+  }
+
+  @Override
+  public DrillDownQuery clone() {
+    return new DrillDownQuery(config, query, drillDownDims);
+  }
+  
+  @Override
+  public int hashCode() {
+    final int prime = 31;
+    int result = super.hashCode();
+    return prime * result + query.hashCode();
+  }
+  
+  @Override
+  public boolean equals(Object obj) {
+    if (!(obj instanceof DrillDownQuery)) {
+      return false;
+    }
+    
+    DrillDownQuery other = (DrillDownQuery) obj;
+    return query.equals(other.query) && super.equals(other);
+  }
+  
+  @Override
+  public Query rewrite(IndexReader r) throws IOException {
+    if (query.clauses().size() == 0) {
+      return new MatchAllDocsQuery();
+    }
+    return query;
+  }
+
+  @Override
+  public String toString(String field) {
+    return query.toString(field);
+  }
+
+  BooleanQuery getBooleanQuery() {
+    return query;
+  }
+
+  Map<String,Integer> getDims() {
+    return drillDownDims;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java b/lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java
new file mode 100644
index 0000000..25800fd
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/DrillSideways.java
@@ -0,0 +1,446 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.FieldDoc;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MultiCollector;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.TopFieldCollector;
+import org.apache.lucene.search.TopScoreDocCollector;
+import org.apache.lucene.search.Weight;
+
+/**     
+ * Computes drill down and sideways counts for the provided
+ * {@link DrillDownQuery}.  Drill sideways counts include
+ * alternative values/aggregates for the drill-down
+ * dimensions so that a dimension does not disappear after
+ * the user drills down into it.
+ *
+ * <p> Use one of the static search
+ * methods to do the search, and then get the hits and facet
+ * results from the returned {@link DrillSidewaysResult}.
+ *
+ * <p><b>NOTE</b>: this allocates one {@link
+ * FacetsCollector} for each drill-down, plus one.  If your
+ * index has high number of facet labels then this will
+ * multiply your memory usage.
+ *
+ * @lucene.experimental
+ */
+
+public class DrillSideways {
+
+  protected final IndexSearcher searcher;
+  protected final TaxonomyReader taxoReader;
+  protected final SortedSetDocValuesReaderState state;
+  protected final FacetsConfig config;
+
+  /**
+   * Create a new {@code DrillSideways} instance, assuming the categories were
+   * indexed with {@link FacetFields}.
+   */
+  public DrillSideways(IndexSearcher searcher, FacetsConfig config, TaxonomyReader taxoReader) {
+    this(searcher, config, taxoReader, null);
+  }
+    
+  /**
+   * Create a new {@code DrillSideways} instance, assuming the categories were
+   * indexed with {@link SortedSetDocValuesFacetFields}.
+   */
+  public DrillSideways(IndexSearcher searcher, FacetsConfig config, SortedSetDocValuesReaderState state) {
+    this(searcher, config, null, state);
+  }
+
+  /**
+   * Create a new {@code DrillSideways} instance, where some
+   * dimensions are sorted set facets and others are
+   * taxononmy facets.
+   */
+  public DrillSideways(IndexSearcher searcher, FacetsConfig config, TaxonomyReader taxoReader, SortedSetDocValuesReaderState state) {
+    this.searcher = searcher;
+    this.config = config;
+    this.taxoReader = taxoReader;
+    this.state = state;
+  }
+
+  /** Subclass can override to customize per-dim Facets
+   *  impl. */
+  protected Facets buildFacetsResult(FacetsCollector drillDowns, FacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {
+
+    Facets drillDownFacets;
+    Map<String,Facets> drillSidewaysFacets = new HashMap<String,Facets>();
+
+    if (taxoReader != null) {
+      drillDownFacets = new FastTaxonomyFacetCounts(taxoReader, config, drillDowns);
+      if (drillSideways != null) {
+        for(int i=0;i<drillSideways.length;i++) {
+          drillSidewaysFacets.put(drillSidewaysDims[i],
+                                  new FastTaxonomyFacetCounts(taxoReader, config, drillSideways[i]));
+        }
+      }
+    } else {
+      drillDownFacets = new SortedSetDocValuesFacetCounts(state, drillDowns);
+      if (drillSideways != null) {
+        for(int i=0;i<drillSideways.length;i++) {
+          drillSidewaysFacets.put(drillSidewaysDims[i],
+                                  new SortedSetDocValuesFacetCounts(state, drillSideways[i]));
+        }
+      }
+    }
+
+    if (drillSidewaysFacets.isEmpty()) {
+      return drillDownFacets;
+    } else {
+      return new MultiFacets(drillSidewaysFacets, drillDownFacets);
+    }
+  }
+
+  /**
+   * Search, collecting hits with a {@link Collector}, and
+   * computing drill down and sideways counts.
+   */
+  @SuppressWarnings({"rawtypes","unchecked"})
+  public DrillSidewaysResult search(DrillDownQuery query, Collector hitCollector) throws IOException {
+
+    Map<String,Integer> drillDownDims = query.getDims();
+
+    FacetsCollector drillDownCollector = new FacetsCollector();
+    
+    if (drillDownDims.isEmpty()) {
+      // There are no drill-down dims, so there is no
+      // drill-sideways to compute:
+      searcher.search(query, MultiCollector.wrap(hitCollector, drillDownCollector));
+      return new DrillSidewaysResult(buildFacetsResult(drillDownCollector, null, null), null);
+    }
+
+    BooleanQuery ddq = query.getBooleanQuery();
+    BooleanClause[] clauses = ddq.getClauses();
+
+    Query baseQuery;
+    int startClause;
+    if (clauses.length == drillDownDims.size()) {
+      // TODO: we could optimize this pure-browse case by
+      // making a custom scorer instead:
+      baseQuery = new MatchAllDocsQuery();
+      startClause = 0;
+    } else {
+      assert clauses.length == 1+drillDownDims.size();
+      baseQuery = clauses[0].getQuery();
+      startClause = 1;
+    }
+
+    FacetsCollector[] drillSidewaysCollectors = new FacetsCollector[drillDownDims.size()];
+
+    int idx = 0;
+    for(String dim : drillDownDims.keySet()) {
+      drillSidewaysCollectors[idx++] = new FacetsCollector();
+    }
+
+    boolean useCollectorMethod = scoreSubDocsAtOnce();
+
+    Term[][] drillDownTerms = null;
+
+    if (!useCollectorMethod) {
+      // Optimistic: assume subQueries of the DDQ are either
+      // TermQuery or BQ OR of TermQuery; if this is wrong
+      // then we detect it and fallback to the mome general
+      // but slower DrillSidewaysCollector:
+      drillDownTerms = new Term[clauses.length-startClause][];
+      for(int i=startClause;i<clauses.length;i++) {
+        Query q = clauses[i].getQuery();
+
+        // DrillDownQuery always wraps each subQuery in
+        // ConstantScoreQuery:
+        assert q instanceof ConstantScoreQuery;
+
+        q = ((ConstantScoreQuery) q).getQuery();
+
+        if (q instanceof TermQuery) {
+          drillDownTerms[i-startClause] = new Term[] {((TermQuery) q).getTerm()};
+        } else if (q instanceof BooleanQuery) {
+          BooleanQuery q2 = (BooleanQuery) q;
+          BooleanClause[] clauses2 = q2.getClauses();
+          drillDownTerms[i-startClause] = new Term[clauses2.length];
+          for(int j=0;j<clauses2.length;j++) {
+            if (clauses2[j].getQuery() instanceof TermQuery) {
+              drillDownTerms[i-startClause][j] = ((TermQuery) clauses2[j].getQuery()).getTerm();
+            } else {
+              useCollectorMethod = true;
+              break;
+            }
+          }
+        } else {
+          useCollectorMethod = true;
+        }
+      }
+    }
+
+    if (useCollectorMethod) {
+      // TODO: maybe we could push the "collector method"
+      // down into the optimized scorer to have a tighter
+      // integration ... and so TermQuery clauses could
+      // continue to run "optimized"
+      collectorMethod(query, baseQuery, startClause, hitCollector, drillDownCollector, drillSidewaysCollectors);
+    } else {
+      DrillSidewaysQuery dsq = new DrillSidewaysQuery(baseQuery, drillDownCollector, drillSidewaysCollectors, drillDownTerms);
+      searcher.search(dsq, hitCollector);
+    }
+
+    return new DrillSidewaysResult(buildFacetsResult(drillDownCollector, drillSidewaysCollectors, drillDownDims.keySet().toArray(new String[drillDownDims.size()])), null);
+  }
+
+  /** Uses the more general but slower method of sideways
+   *  counting. This method allows an arbitrary subQuery to
+   *  implement the drill down for a given dimension. */
+  private void collectorMethod(DrillDownQuery ddq, Query baseQuery, int startClause, Collector hitCollector, Collector drillDownCollector, Collector[] drillSidewaysCollectors) throws IOException {
+
+    BooleanClause[] clauses = ddq.getBooleanQuery().getClauses();
+
+    Map<String,Integer> drillDownDims = ddq.getDims();
+
+    BooleanQuery topQuery = new BooleanQuery(true);
+    final DrillSidewaysCollector collector = new DrillSidewaysCollector(hitCollector, drillDownCollector, drillSidewaysCollectors,
+                                                                                    drillDownDims);
+
+    // TODO: if query is already a BQ we could copy that and
+    // add clauses to it, instead of doing BQ inside BQ
+    // (should be more efficient)?  Problem is this can
+    // affect scoring (coord) ... too bad we can't disable
+    // coord on a clause by clause basis:
+    topQuery.add(baseQuery, BooleanClause.Occur.MUST);
+
+    // NOTE: in theory we could just make a single BQ, with
+    // +query a b c minShouldMatch=2, but in this case,
+    // annoyingly, BS2 wraps a sub-scorer that always
+    // returns 2 as the .freq(), not how many of the
+    // SHOULD clauses matched:
+    BooleanQuery subQuery = new BooleanQuery(true);
+
+    Query wrappedSubQuery = new QueryWrapper(subQuery,
+                                             new SetWeight() {
+                                               @Override
+                                               public void set(Weight w) {
+                                                 collector.setWeight(w, -1);
+                                               }
+                                             });
+    Query constantScoreSubQuery = new ConstantScoreQuery(wrappedSubQuery);
+
+    // Don't impact score of original query:
+    constantScoreSubQuery.setBoost(0.0f);
+
+    topQuery.add(constantScoreSubQuery, BooleanClause.Occur.MUST);
+
+    // Unfortunately this sub-BooleanQuery
+    // will never get BS1 because today BS1 only works
+    // if topScorer=true... and actually we cannot use BS1
+    // anyways because we need subDocsScoredAtOnce:
+    int dimIndex = 0;
+    for(int i=startClause;i<clauses.length;i++) {
+      Query q = clauses[i].getQuery();
+      // DrillDownQuery always wraps each subQuery in
+      // ConstantScoreQuery:
+      assert q instanceof ConstantScoreQuery;
+      q = ((ConstantScoreQuery) q).getQuery();
+
+      final int finalDimIndex = dimIndex;
+      subQuery.add(new QueryWrapper(q,
+                                    new SetWeight() {
+                                      @Override
+                                      public void set(Weight w) {
+                                        collector.setWeight(w, finalDimIndex);
+                                      }
+                                    }),
+                   BooleanClause.Occur.SHOULD);
+      dimIndex++;
+    }
+
+    // TODO: we could better optimize the "just one drill
+    // down" case w/ a separate [specialized]
+    // collector...
+    int minShouldMatch = drillDownDims.size()-1;
+    if (minShouldMatch == 0) {
+      // Must add another "fake" clause so BQ doesn't erase
+      // itself by rewriting to the single clause:
+      Query end = new MatchAllDocsQuery();
+      end.setBoost(0.0f);
+      subQuery.add(end, BooleanClause.Occur.SHOULD);
+      minShouldMatch++;
+    }
+
+    subQuery.setMinimumNumberShouldMatch(minShouldMatch);
+
+    // System.out.println("EXE " + topQuery);
+
+    // Collects against the passed-in
+    // drillDown/SidewaysCollectors as a side effect:
+    searcher.search(topQuery, collector);
+  }
+
+  /**
+   * Search, sorting by {@link Sort}, and computing
+   * drill down and sideways counts.
+   */
+  public DrillSidewaysResult search(DrillDownQuery query,
+                                          Filter filter, FieldDoc after, int topN, Sort sort, boolean doDocScores,
+                                          boolean doMaxScore) throws IOException {
+    if (filter != null) {
+      query = new DrillDownQuery(config, filter, query);
+    }
+    if (sort != null) {
+      int limit = searcher.getIndexReader().maxDoc();
+      if (limit == 0) {
+        limit = 1; // the collector does not alow numHits = 0
+      }
+      topN = Math.min(topN, limit);
+      final TopFieldCollector hitCollector = TopFieldCollector.create(sort,
+                                                                      topN,
+                                                                      after,
+                                                                      true,
+                                                                      doDocScores,
+                                                                      doMaxScore,
+                                                                      true);
+      DrillSidewaysResult r = search(query, hitCollector);
+      return new DrillSidewaysResult(r.facets, hitCollector.topDocs());
+    } else {
+      return search(after, query, topN);
+    }
+  }
+
+  /**
+   * Search, sorting by score, and computing
+   * drill down and sideways counts.
+   */
+  public DrillSidewaysResult search(DrillDownQuery query, int topN) throws IOException {
+    return search(null, query, topN);
+  }
+
+  /**
+   * Search, sorting by score, and computing
+   * drill down and sideways counts.
+   */
+  public DrillSidewaysResult search(ScoreDoc after,
+                                          DrillDownQuery query, int topN) throws IOException {
+    int limit = searcher.getIndexReader().maxDoc();
+    if (limit == 0) {
+      limit = 1; // the collector does not alow numHits = 0
+    }
+    topN = Math.min(topN, limit);
+    TopScoreDocCollector hitCollector = TopScoreDocCollector.create(topN, after, true);
+    DrillSidewaysResult r = search(query, hitCollector);
+    return new DrillSidewaysResult(r.facets, hitCollector.topDocs());
+  }
+
+  /** Override this and return true if your collector
+   *  (e.g., ToParentBlockJoinCollector) expects all
+   *  sub-scorers to be positioned on the document being
+   *  collected.  This will cause some performance loss;
+   *  default is false.  Note that if you return true from
+   *  this method (in a subclass) be sure your collector
+   *  also returns false from {@link
+   *  Collector#acceptsDocsOutOfOrder}: this will trick
+   *  BooleanQuery into also scoring all subDocs at once. */
+  protected boolean scoreSubDocsAtOnce() {
+    return false;
+  }
+
+  public static class DrillSidewaysResult {
+    /** Combined drill down & sideways results. */
+    public final Facets facets;
+
+    /** Hits. */
+    public final TopDocs hits;
+
+    public DrillSidewaysResult(Facets facets, TopDocs hits) {
+      this.facets = facets;
+      this.hits = hits;
+    }
+  }
+  private interface SetWeight {
+    public void set(Weight w);
+  }
+
+  /** Just records which Weight was given out for the
+   *  (possibly rewritten) Query. */
+  private static class QueryWrapper extends Query {
+    private final Query originalQuery;
+    private final SetWeight setter;
+
+    public QueryWrapper(Query originalQuery, SetWeight setter) {
+      this.originalQuery = originalQuery;
+      this.setter = setter;
+    }
+
+    @Override
+    public Weight createWeight(final IndexSearcher searcher) throws IOException {
+      Weight w = originalQuery.createWeight(searcher);
+      setter.set(w);
+      return w;
+    }
+
+    @Override
+    public Query rewrite(IndexReader reader) throws IOException {
+      Query rewritten = originalQuery.rewrite(reader);
+      if (rewritten != originalQuery) {
+        return new QueryWrapper(rewritten, setter);
+      } else {
+        return this;
+      }
+    }
+
+    @Override
+    public String toString(String s) {
+      return originalQuery.toString(s);
+    }
+
+    @Override
+    public boolean equals(Object o) {
+      if (!(o instanceof QueryWrapper)) return false;
+      final QueryWrapper other = (QueryWrapper) o;
+      return super.equals(o) && originalQuery.equals(other.originalQuery);
+    }
+
+    @Override
+    public int hashCode() {
+      return super.hashCode() * 31 + originalQuery.hashCode();
+    }
+  }
+}
+
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysCollector.java b/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysCollector.java
new file mode 100644
index 0000000..2006f1d
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysCollector.java
@@ -0,0 +1,188 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.IdentityHashMap;
+import java.util.Map;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer.ChildScorer;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+
+/** Collector that scrutinizes each hit to determine if it
+ *  passed all constraints (a true hit) or if it missed
+ *  exactly one dimension (a near-miss, to count for
+ *  drill-sideways counts on that dimension). */
+class DrillSidewaysCollector extends Collector {
+
+  private final Collector hitCollector;
+  private final Collector drillDownCollector;
+  private final Collector[] drillSidewaysCollectors;
+  private final Scorer[] subScorers;
+  private final int exactCount;
+
+  // Maps Weight to either -1 (mainQuery) or to integer
+  // index of the dims drillDown.  We needs this when
+  // visiting the child scorers to correlate back to the
+  // right scorers:
+  private final Map<Weight,Integer> weightToIndex = new IdentityHashMap<Weight,Integer>();
+
+  private Scorer mainScorer;
+
+  public DrillSidewaysCollector(Collector hitCollector, Collector drillDownCollector, Collector[] drillSidewaysCollectors,
+                                      Map<String,Integer> dims) {
+    this.hitCollector = hitCollector;
+    this.drillDownCollector = drillDownCollector;
+    this.drillSidewaysCollectors = drillSidewaysCollectors;
+    subScorers = new Scorer[dims.size()];
+
+    if (dims.size() == 1) {
+      // When we have only one dim, we insert the
+      // MatchAllDocsQuery, bringing the clause count to
+      // 2:
+      exactCount = 2;
+    } else {
+      exactCount = dims.size();
+    }
+  }
+
+  @Override
+  public void collect(int doc) throws IOException {
+    //System.out.println("collect doc=" + doc + " main.freq=" + mainScorer.freq() + " main.doc=" + mainScorer.docID() + " exactCount=" + exactCount);
+      
+    if (mainScorer == null) {
+      // This segment did not have any docs with any
+      // drill-down field & value:
+      return;
+    }
+
+    if (mainScorer.freq() == exactCount) {
+      // All sub-clauses from the drill-down filters
+      // matched, so this is a "real" hit, so we first
+      // collect in both the hitCollector and the
+      // drillDown collector:
+      //System.out.println("  hit " + drillDownCollector);
+      hitCollector.collect(doc);
+      if (drillDownCollector != null) {
+        drillDownCollector.collect(doc);
+      }
+
+      // Also collect across all drill-sideways counts so
+      // we "merge in" drill-down counts for this
+      // dimension.
+      for(int i=0;i<subScorers.length;i++) {
+        // This cannot be null, because it was a hit,
+        // meaning all drill-down dims matched, so all
+        // dims must have non-null scorers:
+        assert subScorers[i] != null;
+        int subDoc = subScorers[i].docID();
+        assert subDoc == doc;
+        drillSidewaysCollectors[i].collect(doc);
+      }
+
+    } else {
+      boolean found = false;
+      for(int i=0;i<subScorers.length;i++) {
+        if (subScorers[i] == null) {
+          // This segment did not have any docs with this
+          // drill-down field & value:
+          drillSidewaysCollectors[i].collect(doc);
+          assert allMatchesFrom(i+1, doc);
+          found = true;
+          break;
+        }
+        int subDoc = subScorers[i].docID();
+        //System.out.println("  i=" + i + " sub: " + subDoc);
+        if (subDoc != doc) {
+          //System.out.println("  +ds[" + i + "]");
+          assert subDoc > doc: "subDoc=" + subDoc + " doc=" + doc;
+          drillSidewaysCollectors[i].collect(doc);
+          assert allMatchesFrom(i+1, doc);
+          found = true;
+          break;
+        }
+      }
+      assert found;
+    }
+  }
+
+  // Only used by assert:
+  private boolean allMatchesFrom(int startFrom, int doc) {
+    for(int i=startFrom;i<subScorers.length;i++) {
+      assert subScorers[i].docID() == doc;
+    }
+    return true;
+  }
+
+  @Override
+  public boolean acceptsDocsOutOfOrder() {
+    // We actually could accept docs out of order, but, we
+    // need to force BooleanScorer2 so that the
+    // sub-scorers are "on" each docID we are collecting:
+    return false;
+  }
+
+  @Override
+  public void setNextReader(AtomicReaderContext leaf) throws IOException {
+    //System.out.println("DS.setNextReader reader=" + leaf.reader());
+    hitCollector.setNextReader(leaf);
+    if (drillDownCollector != null) {
+      drillDownCollector.setNextReader(leaf);
+    }
+    for(Collector dsc : drillSidewaysCollectors) {
+      dsc.setNextReader(leaf);
+    }
+  }
+
+  void setWeight(Weight weight, int index) {
+    assert !weightToIndex.containsKey(weight);
+    weightToIndex.put(weight, index);
+  }
+
+  private void findScorers(Scorer scorer) {
+    Integer index = weightToIndex.get(scorer.getWeight());
+    if (index != null) {
+      if (index.intValue() == -1) {
+        mainScorer = scorer;
+      } else {
+        subScorers[index] = scorer;
+      }
+    }
+    for(ChildScorer child : scorer.getChildren()) {
+      findScorers(child.child);
+    }
+  }
+
+  @Override
+  public void setScorer(Scorer scorer) throws IOException {
+    mainScorer = null;
+    Arrays.fill(subScorers, null);
+    findScorers(scorer);
+    hitCollector.setScorer(scorer);
+    if (drillDownCollector != null) {
+      drillDownCollector.setScorer(scorer);
+    }
+    for(Collector dsc : drillSidewaysCollectors) {
+      dsc.setScorer(scorer);
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysQuery.java b/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysQuery.java
new file mode 100644
index 0000000..221628d
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysQuery.java
@@ -0,0 +1,198 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Explanation;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.Bits;
+
+/** Only purpose is to punch through and return a
+ *  SimpleDrillSidewaysScorer */ 
+
+class DrillSidewaysQuery extends Query {
+  final Query baseQuery;
+  final Collector drillDownCollector;
+  final Collector[] drillSidewaysCollectors;
+  final Term[][] drillDownTerms;
+
+  DrillSidewaysQuery(Query baseQuery, Collector drillDownCollector, Collector[] drillSidewaysCollectors, Term[][] drillDownTerms) {
+    this.baseQuery = baseQuery;
+    this.drillDownCollector = drillDownCollector;
+    this.drillSidewaysCollectors = drillSidewaysCollectors;
+    this.drillDownTerms = drillDownTerms;
+  }
+
+  @Override
+  public String toString(String field) {
+    return "DrillSidewaysQuery";
+  }
+
+  @Override
+  public Query rewrite(IndexReader reader) throws IOException {
+    Query newQuery = baseQuery;
+    while(true) {
+      Query rewrittenQuery = newQuery.rewrite(reader);
+      if (rewrittenQuery == newQuery) {
+        break;
+      }
+      newQuery = rewrittenQuery;
+    }
+    if (newQuery == baseQuery) {
+      return this;
+    } else {
+      return new DrillSidewaysQuery(newQuery, drillDownCollector, drillSidewaysCollectors, drillDownTerms);
+    }
+  }
+  
+  @Override
+  public Weight createWeight(IndexSearcher searcher) throws IOException {
+    final Weight baseWeight = baseQuery.createWeight(searcher);
+
+    return new Weight() {
+      @Override
+      public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+        return baseWeight.explain(context, doc);
+      }
+
+      @Override
+      public Query getQuery() {
+        return baseQuery;
+      }
+
+      @Override
+      public float getValueForNormalization() throws IOException {
+        return baseWeight.getValueForNormalization();
+      }
+
+      @Override
+      public void normalize(float norm, float topLevelBoost) {
+        baseWeight.normalize(norm, topLevelBoost);
+      }
+
+      @Override
+      public boolean scoresDocsOutOfOrder() {
+        // TODO: would be nice if AssertingIndexSearcher
+        // confirmed this for us
+        return false;
+      }
+
+      @Override
+      public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
+                           boolean topScorer, Bits acceptDocs) throws IOException {
+
+        DrillSidewaysScorer.DocsEnumsAndFreq[] dims = new DrillSidewaysScorer.DocsEnumsAndFreq[drillDownTerms.length];
+        TermsEnum termsEnum = null;
+        String lastField = null;
+        int nullCount = 0;
+        for(int dim=0;dim<dims.length;dim++) {
+          dims[dim] = new DrillSidewaysScorer.DocsEnumsAndFreq();
+          dims[dim].sidewaysCollector = drillSidewaysCollectors[dim];
+          String field = drillDownTerms[dim][0].field();
+          dims[dim].dim = drillDownTerms[dim][0].text();
+          if (lastField == null || !lastField.equals(field)) {
+            AtomicReader reader = context.reader();
+            Terms terms = reader.terms(field);
+            if (terms != null) {
+              termsEnum = terms.iterator(null);
+            } else {
+              termsEnum = null;
+            }
+            lastField = field;
+          }
+          dims[dim].docsEnums = new DocsEnum[drillDownTerms[dim].length];
+          if (termsEnum == null) {
+            nullCount++;
+            continue;
+          }
+          for(int i=0;i<drillDownTerms[dim].length;i++) {
+            if (termsEnum.seekExact(drillDownTerms[dim][i].bytes())) {
+              DocsEnum docsEnum = termsEnum.docs(null, null, 0);
+              if (docsEnum != null) {
+                dims[dim].docsEnums[i] = docsEnum;
+                dims[dim].maxCost = Math.max(dims[dim].maxCost, docsEnum.cost());
+              }
+            }
+          }
+        }
+
+        if (nullCount > 1 || (nullCount == 1 && dims.length == 1)) {
+          return null;
+        }
+
+        // Sort drill-downs by most restrictive first:
+        Arrays.sort(dims);
+
+        // TODO: it could be better if we take acceptDocs
+        // into account instead of baseScorer?
+        Scorer baseScorer = baseWeight.scorer(context, scoreDocsInOrder, false, acceptDocs);
+
+        if (baseScorer == null) {
+          return null;
+        }
+
+        return new DrillSidewaysScorer(this, context,
+                                             baseScorer,
+                                             drillDownCollector, dims);
+      }
+    };
+  }
+
+  // TODO: these should do "deeper" equals/hash on the 2-D drillDownTerms array
+
+  @Override
+  public int hashCode() {
+    final int prime = 31;
+    int result = super.hashCode();
+    result = prime * result + ((baseQuery == null) ? 0 : baseQuery.hashCode());
+    result = prime * result
+        + ((drillDownCollector == null) ? 0 : drillDownCollector.hashCode());
+    result = prime * result + Arrays.hashCode(drillDownTerms);
+    result = prime * result + Arrays.hashCode(drillSidewaysCollectors);
+    return result;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) return true;
+    if (!super.equals(obj)) return false;
+    if (getClass() != obj.getClass()) return false;
+    DrillSidewaysQuery other = (DrillSidewaysQuery) obj;
+    if (baseQuery == null) {
+      if (other.baseQuery != null) return false;
+    } else if (!baseQuery.equals(other.baseQuery)) return false;
+    if (drillDownCollector == null) {
+      if (other.drillDownCollector != null) return false;
+    } else if (!drillDownCollector.equals(other.drillDownCollector)) return false;
+    if (!Arrays.equals(drillDownTerms, other.drillDownTerms)) return false;
+    if (!Arrays.equals(drillSidewaysCollectors, other.drillSidewaysCollectors)) return false;
+    return true;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java b/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java
new file mode 100644
index 0000000..263f9aa
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/DrillSidewaysScorer.java
@@ -0,0 +1,654 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.FixedBitSet;
+
+class DrillSidewaysScorer extends Scorer {
+
+  //private static boolean DEBUG = false;
+
+  private final Collector drillDownCollector;
+
+  private final DocsEnumsAndFreq[] dims;
+
+  // DrillDown DocsEnums:
+  private final Scorer baseScorer;
+
+  private final AtomicReaderContext context;
+
+  private static final int CHUNK = 2048;
+  private static final int MASK = CHUNK-1;
+
+  private int collectDocID = -1;
+  private float collectScore;
+
+  DrillSidewaysScorer(Weight w, AtomicReaderContext context, Scorer baseScorer, Collector drillDownCollector,
+                            DocsEnumsAndFreq[] dims) {
+    super(w);
+    this.dims = dims;
+    this.context = context;
+    this.baseScorer = baseScorer;
+    this.drillDownCollector = drillDownCollector;
+  }
+
+  @Override
+  public void score(Collector collector) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("\nscore: reader=" + context.reader());
+    //}
+    //System.out.println("score r=" + context.reader());
+    collector.setScorer(this);
+    if (drillDownCollector != null) {
+      drillDownCollector.setScorer(this);
+      drillDownCollector.setNextReader(context);
+    }
+    for(DocsEnumsAndFreq dim : dims) {
+      dim.sidewaysCollector.setScorer(this);
+      dim.sidewaysCollector.setNextReader(context);
+    }
+
+    // TODO: if we ever allow null baseScorer ... it will
+    // mean we DO score docs out of order ... hmm, or if we
+    // change up the order of the conjuntions below
+    assert baseScorer != null;
+
+    // Position all scorers to their first matching doc:
+    baseScorer.nextDoc();
+    for(DocsEnumsAndFreq dim : dims) {
+      for (DocsEnum docsEnum : dim.docsEnums) {
+        if (docsEnum != null) {
+          docsEnum.nextDoc();
+        }
+      }
+    }
+
+    final int numDims = dims.length;
+
+    DocsEnum[][] docsEnums = new DocsEnum[numDims][];
+    Collector[] sidewaysCollectors = new Collector[numDims];
+    long drillDownCost = 0;
+    for(int dim=0;dim<numDims;dim++) {
+      docsEnums[dim] = dims[dim].docsEnums;
+      sidewaysCollectors[dim] = dims[dim].sidewaysCollector;
+      for (DocsEnum de : dims[dim].docsEnums) {
+        if (de != null) {
+          drillDownCost += de.cost();
+        }
+      }
+    }
+
+    long baseQueryCost = baseScorer.cost();
+
+    /*
+    System.out.println("\nbaseDocID=" + baseScorer.docID() + " est=" + estBaseHitCount);
+    System.out.println("  maxDoc=" + context.reader().maxDoc());
+    System.out.println("  maxCost=" + maxCost);
+    System.out.println("  dims[0].freq=" + dims[0].freq);
+    if (numDims > 1) {
+      System.out.println("  dims[1].freq=" + dims[1].freq);
+    }
+    */
+
+    if (baseQueryCost < drillDownCost/10) {
+      //System.out.println("baseAdvance");
+      doBaseAdvanceScoring(collector, docsEnums, sidewaysCollectors);
+    } else if (numDims > 1 && (dims[1].maxCost < baseQueryCost/10)) {
+      //System.out.println("drillDownAdvance");
+      doDrillDownAdvanceScoring(collector, docsEnums, sidewaysCollectors);
+    } else {
+      //System.out.println("union");
+      doUnionScoring(collector, docsEnums, sidewaysCollectors);
+    }
+  }
+
+  /** Used when drill downs are highly constraining vs
+   *  baseQuery. */
+  private void doDrillDownAdvanceScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
+    final int maxDoc = context.reader().maxDoc();
+    final int numDims = dims.length;
+
+    //if (DEBUG) {
+    //  System.out.println("  doDrillDownAdvanceScoring");
+    //}
+
+    // TODO: maybe a class like BS, instead of parallel arrays
+    int[] filledSlots = new int[CHUNK];
+    int[] docIDs = new int[CHUNK];
+    float[] scores = new float[CHUNK];
+    int[] missingDims = new int[CHUNK];
+    int[] counts = new int[CHUNK];
+
+    docIDs[0] = -1;
+    int nextChunkStart = CHUNK;
+
+    final FixedBitSet seen = new FixedBitSet(CHUNK);
+
+    while (true) {
+      //if (DEBUG) {
+      //  System.out.println("\ncycle nextChunkStart=" + nextChunkStart + " docIds[0]=" + docIDs[0]);
+      //}
+
+      // First dim:
+      //if (DEBUG) {
+      //  System.out.println("  dim0");
+      //}
+      for(DocsEnum docsEnum : docsEnums[0]) {
+        if (docsEnum == null) {
+          continue;
+        }
+        int docID = docsEnum.docID();
+        while (docID < nextChunkStart) {
+          int slot = docID & MASK;
+
+          if (docIDs[slot] != docID) {
+            seen.set(slot);
+            // Mark slot as valid:
+            //if (DEBUG) {
+            //  System.out.println("    set docID=" + docID + " id=" + context.reader().document(docID).get("id"));
+            //}
+            docIDs[slot] = docID;
+            missingDims[slot] = 1;
+            counts[slot] = 1;
+          }
+
+          docID = docsEnum.nextDoc();
+        }
+      }
+
+      // Second dim:
+      //if (DEBUG) {
+      //  System.out.println("  dim1");
+      //}
+      for(DocsEnum docsEnum : docsEnums[1]) {
+        if (docsEnum == null) {
+          continue;
+        }
+        int docID = docsEnum.docID();
+        while (docID < nextChunkStart) {
+          int slot = docID & MASK;
+
+          if (docIDs[slot] != docID) {
+            // Mark slot as valid:
+            seen.set(slot);
+            //if (DEBUG) {
+            //  System.out.println("    set docID=" + docID + " missingDim=0 id=" + context.reader().document(docID).get("id"));
+            //}
+            docIDs[slot] = docID;
+            missingDims[slot] = 0;
+            counts[slot] = 1;
+          } else {
+            // TODO: single-valued dims will always be true
+            // below; we could somehow specialize
+            if (missingDims[slot] >= 1) {
+              missingDims[slot] = 2;
+              counts[slot] = 2;
+              //if (DEBUG) {
+              //  System.out.println("    set docID=" + docID + " missingDim=2 id=" + context.reader().document(docID).get("id"));
+              //}
+            } else {
+              counts[slot] = 1;
+              //if (DEBUG) {
+              //  System.out.println("    set docID=" + docID + " missingDim=" + missingDims[slot] + " id=" + context.reader().document(docID).get("id"));
+              //}
+            }
+          }
+
+          docID = docsEnum.nextDoc();
+        }
+      }
+
+      // After this we can "upgrade" to conjunction, because
+      // any doc not seen by either dim 0 or dim 1 cannot be
+      // a hit or a near miss:
+
+      //if (DEBUG) {
+      //  System.out.println("  baseScorer");
+      //}
+
+      // Fold in baseScorer, using advance:
+      int filledCount = 0;
+      int slot0 = 0;
+      while (slot0 < CHUNK && (slot0 = seen.nextSetBit(slot0)) != -1) {
+        int ddDocID = docIDs[slot0];
+        assert ddDocID != -1;
+
+        int baseDocID = baseScorer.docID();
+        if (baseDocID < ddDocID) {
+          baseDocID = baseScorer.advance(ddDocID);
+        }
+        if (baseDocID == ddDocID) {
+          //if (DEBUG) {
+          //  System.out.println("    keep docID=" + ddDocID + " id=" + context.reader().document(ddDocID).get("id"));
+          //}
+          scores[slot0] = baseScorer.score();
+          filledSlots[filledCount++] = slot0;
+          counts[slot0]++;
+        } else {
+          //if (DEBUG) {
+          //  System.out.println("    no docID=" + ddDocID + " id=" + context.reader().document(ddDocID).get("id"));
+          //}
+          docIDs[slot0] = -1;
+
+          // TODO: we could jump slot0 forward to the
+          // baseDocID ... but we'd need to set docIDs for
+          // intervening slots to -1
+        }
+        slot0++;
+      }
+      seen.clear(0, CHUNK);
+
+      if (filledCount == 0) {
+        if (nextChunkStart >= maxDoc) {
+          break;
+        }
+        nextChunkStart += CHUNK;
+        continue;
+      }
+      
+      // TODO: factor this out & share w/ union scorer,
+      // except we start from dim=2 instead:
+      for(int dim=2;dim<numDims;dim++) {
+        //if (DEBUG) {
+        //  System.out.println("  dim=" + dim + " [" + dims[dim].dim + "]");
+        //}
+        for(DocsEnum docsEnum : docsEnums[dim]) {
+          if (docsEnum == null) {
+            continue;
+          }
+          int docID = docsEnum.docID();
+          while (docID < nextChunkStart) {
+            int slot = docID & MASK;
+            if (docIDs[slot] == docID && counts[slot] >= dim) {
+              // TODO: single-valued dims will always be true
+              // below; we could somehow specialize
+              if (missingDims[slot] >= dim) {
+                //if (DEBUG) {
+                //  System.out.println("    set docID=" + docID + " count=" + (dim+2));
+                //}
+                missingDims[slot] = dim+1;
+                counts[slot] = dim+2;
+              } else {
+                //if (DEBUG) {
+                //  System.out.println("    set docID=" + docID + " missing count=" + (dim+1));
+                //}
+                counts[slot] = dim+1;
+              }
+            }
+            // TODO: sometimes use advance?
+            docID = docsEnum.nextDoc();
+          }
+        }
+      }
+
+      // Collect:
+      //if (DEBUG) {
+      //  System.out.println("  now collect: " + filledCount + " hits");
+      //}
+      for(int i=0;i<filledCount;i++) {
+        int slot = filledSlots[i];
+        collectDocID = docIDs[slot];
+        collectScore = scores[slot];
+        //if (DEBUG) {
+        //  System.out.println("    docID=" + docIDs[slot] + " count=" + counts[slot]);
+        //}
+        if (counts[slot] == 1+numDims) {
+          collectHit(collector, sidewaysCollectors);
+        } else if (counts[slot] == numDims) {
+          collectNearMiss(sidewaysCollectors, missingDims[slot]);
+        }
+      }
+
+      if (nextChunkStart >= maxDoc) {
+        break;
+      }
+
+      nextChunkStart += CHUNK;
+    }
+  }
+
+  /** Used when base query is highly constraining vs the
+   *  drilldowns; in this case we just .next() on base and
+   *  .advance() on the dims. */
+  private void doBaseAdvanceScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("  doBaseAdvanceScoring");
+    //}
+    int docID = baseScorer.docID();
+
+    final int numDims = dims.length;
+
+    nextDoc: while (docID != NO_MORE_DOCS) {
+      int failedDim = -1;
+      for(int dim=0;dim<numDims;dim++) {
+        // TODO: should we sort this 2nd dimension of
+        // docsEnums from most frequent to least?
+        boolean found = false;
+        for(DocsEnum docsEnum : docsEnums[dim]) {
+          if (docsEnum == null) {
+            continue;
+          }
+          if (docsEnum.docID() < docID) {
+            docsEnum.advance(docID);
+          }
+          if (docsEnum.docID() == docID) {
+            found = true;
+            break;
+          }
+        }
+        if (!found) {
+          if (failedDim != -1) {
+            // More than one dim fails on this document, so
+            // it's neither a hit nor a near-miss; move to
+            // next doc:
+            docID = baseScorer.nextDoc();
+            continue nextDoc;
+          } else {
+            failedDim = dim;
+          }
+        }
+      }
+
+      collectDocID = docID;
+
+      // TODO: we could score on demand instead since we are
+      // daat here:
+      collectScore = baseScorer.score();
+
+      if (failedDim == -1) {
+        collectHit(collector, sidewaysCollectors);
+      } else {
+        collectNearMiss(sidewaysCollectors, failedDim);
+      }
+
+      docID = baseScorer.nextDoc();
+    }
+  }
+
+  private void collectHit(Collector collector, Collector[] sidewaysCollectors) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("      hit");
+    //}
+
+    collector.collect(collectDocID);
+    if (drillDownCollector != null) {
+      drillDownCollector.collect(collectDocID);
+    }
+
+    // TODO: we could "fix" faceting of the sideways counts
+    // to do this "union" (of the drill down hits) in the
+    // end instead:
+
+    // Tally sideways counts:
+    for(int dim=0;dim<sidewaysCollectors.length;dim++) {
+      sidewaysCollectors[dim].collect(collectDocID);
+    }
+  }
+
+  private void collectNearMiss(Collector[] sidewaysCollectors, int dim) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("      missingDim=" + dim);
+    //}
+    sidewaysCollectors[dim].collect(collectDocID);
+  }
+
+  private void doUnionScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("  doUnionScoring");
+    //}
+
+    final int maxDoc = context.reader().maxDoc();
+    final int numDims = dims.length;
+
+    // TODO: maybe a class like BS, instead of parallel arrays
+    int[] filledSlots = new int[CHUNK];
+    int[] docIDs = new int[CHUNK];
+    float[] scores = new float[CHUNK];
+    int[] missingDims = new int[CHUNK];
+    int[] counts = new int[CHUNK];
+
+    docIDs[0] = -1;
+
+    // NOTE: this is basically a specialized version of
+    // BooleanScorer, to the minShouldMatch=N-1 case, but
+    // carefully tracking which dimension failed to match
+
+    int nextChunkStart = CHUNK;
+
+    while (true) {
+      //if (DEBUG) {
+      //  System.out.println("\ncycle nextChunkStart=" + nextChunkStart + " docIds[0]=" + docIDs[0]);
+      //}
+      int filledCount = 0;
+      int docID = baseScorer.docID();
+      //if (DEBUG) {
+      //  System.out.println("  base docID=" + docID);
+      //}
+      while (docID < nextChunkStart) {
+        int slot = docID & MASK;
+        //if (DEBUG) {
+        //  System.out.println("    docIDs[slot=" + slot + "]=" + docID + " id=" + context.reader().document(docID).get("id"));
+        //}
+
+        // Mark slot as valid:
+        assert docIDs[slot] != docID: "slot=" + slot + " docID=" + docID;
+        docIDs[slot] = docID;
+        scores[slot] = baseScorer.score();
+        filledSlots[filledCount++] = slot;
+        missingDims[slot] = 0;
+        counts[slot] = 1;
+
+        docID = baseScorer.nextDoc();
+      }
+
+      if (filledCount == 0) {
+        if (nextChunkStart >= maxDoc) {
+          break;
+        }
+        nextChunkStart += CHUNK;
+        continue;
+      }
+
+      // First drill-down dim, basically adds SHOULD onto
+      // the baseQuery:
+      //if (DEBUG) {
+      //  System.out.println("  dim=0 [" + dims[0].dim + "]");
+      //}
+      for(DocsEnum docsEnum : docsEnums[0]) {
+        if (docsEnum == null) {
+          continue;
+        }
+        docID = docsEnum.docID();
+        //if (DEBUG) {
+        //  System.out.println("    start docID=" + docID);
+        //}
+        while (docID < nextChunkStart) {
+          int slot = docID & MASK;
+          if (docIDs[slot] == docID) {
+            //if (DEBUG) {
+            //  System.out.println("      set docID=" + docID + " count=2");
+            //}
+            missingDims[slot] = 1;
+            counts[slot] = 2;
+          }
+          docID = docsEnum.nextDoc();
+        }
+      }
+
+      for(int dim=1;dim<numDims;dim++) {
+        //if (DEBUG) {
+        //  System.out.println("  dim=" + dim + " [" + dims[dim].dim + "]");
+        //}
+        for(DocsEnum docsEnum : docsEnums[dim]) {
+          if (docsEnum == null) {
+            continue;
+          }
+          docID = docsEnum.docID();
+          //if (DEBUG) {
+          //  System.out.println("    start docID=" + docID);
+          //}
+          while (docID < nextChunkStart) {
+            int slot = docID & MASK;
+            if (docIDs[slot] == docID && counts[slot] >= dim) {
+              // This doc is still in the running...
+              // TODO: single-valued dims will always be true
+              // below; we could somehow specialize
+              if (missingDims[slot] >= dim) {
+                //if (DEBUG) {
+                //  System.out.println("      set docID=" + docID + " count=" + (dim+2));
+                //}
+                missingDims[slot] = dim+1;
+                counts[slot] = dim+2;
+              } else {
+                //if (DEBUG) {
+                //  System.out.println("      set docID=" + docID + " missing count=" + (dim+1));
+                //}
+                counts[slot] = dim+1;
+              }
+            }
+            docID = docsEnum.nextDoc();
+          }
+
+          // TODO: sometimes use advance?
+
+          /*
+            int docBase = nextChunkStart - CHUNK;
+            for(int i=0;i<filledCount;i++) {
+              int slot = filledSlots[i];
+              docID = docBase + filledSlots[i];
+              if (docIDs[slot] == docID && counts[slot] >= dim) {
+                // This doc is still in the running...
+                int ddDocID = docsEnum.docID();
+                if (ddDocID < docID) {
+                  ddDocID = docsEnum.advance(docID);
+                }
+                if (ddDocID == docID) {
+                  if (missingDims[slot] >= dim && counts[slot] == allMatchCount) {
+                  //if (DEBUG) {
+                  //    System.out.println("    set docID=" + docID + " count=" + (dim+2));
+                   // }
+                    missingDims[slot] = dim+1;
+                    counts[slot] = dim+2;
+                  } else {
+                  //if (DEBUG) {
+                  //    System.out.println("    set docID=" + docID + " missing count=" + (dim+1));
+                   // }
+                    counts[slot] = dim+1;
+                  }
+                }
+              }
+            }            
+          */
+        }
+      }
+
+      // Collect:
+      //if (DEBUG) {
+      //  System.out.println("  now collect: " + filledCount + " hits");
+      //}
+      for(int i=0;i<filledCount;i++) {
+        // NOTE: This is actually in-order collection,
+        // because we only accept docs originally returned by
+        // the baseScorer (ie that Scorer is AND'd)
+        int slot = filledSlots[i];
+        collectDocID = docIDs[slot];
+        collectScore = scores[slot];
+        //if (DEBUG) {
+        //  System.out.println("    docID=" + docIDs[slot] + " count=" + counts[slot]);
+        //}
+        //System.out.println("  collect doc=" + collectDocID + " main.freq=" + (counts[slot]-1) + " main.doc=" + collectDocID + " exactCount=" + numDims);
+        if (counts[slot] == 1+numDims) {
+          //System.out.println("    hit");
+          collectHit(collector, sidewaysCollectors);
+        } else if (counts[slot] == numDims) {
+          //System.out.println("    sw");
+          collectNearMiss(sidewaysCollectors, missingDims[slot]);
+        }
+      }
+
+      if (nextChunkStart >= maxDoc) {
+        break;
+      }
+
+      nextChunkStart += CHUNK;
+    }
+  }
+
+  @Override
+  public int docID() {
+    return collectDocID;
+  }
+
+  @Override
+  public float score() {
+    return collectScore;
+  }
+
+  @Override
+  public int freq() {
+    return 1+dims.length;
+  }
+
+  @Override
+  public int nextDoc() {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public int advance(int target) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public long cost() {
+    return baseScorer.cost();
+  }
+
+  @Override
+  public Collection<ChildScorer> getChildren() {
+    return Collections.singletonList(new ChildScorer(baseScorer, "MUST"));
+  }
+
+  static class DocsEnumsAndFreq implements Comparable<DocsEnumsAndFreq> {
+    DocsEnum[] docsEnums;
+    // Max cost for all docsEnums for this dim:
+    long maxCost;
+    Collector sidewaysCollector;
+    String dim;
+
+    @Override
+    public int compareTo(DocsEnumsAndFreq other) {
+      if (maxCost < other.maxCost) {
+        return -1;
+      } else if (maxCost > other.maxCost) {
+        return 1;
+      } else {
+        return 0;
+      }
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/FacetField.java
new file mode 100644
index 0000000..b4a1845
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/FacetField.java
@@ -0,0 +1,49 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+
+/** Add an instance of this to your Document for every facet
+ *  label. */
+public class FacetField extends Field {
+  static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setIndexed(true);
+    TYPE.freeze();
+  }
+  public final String dim;
+  public final String[] path;
+
+  public FacetField(String dim, String... path) {
+    super("dummy", TYPE);
+    this.dim = dim;
+    if (path.length == 0) {
+      throw new IllegalArgumentException("path must have at least one element");
+    }
+    this.path = path;
+  }
+
+  @Override
+  public String toString() {
+    return "FacetField(dim=" + dim + " path=" + Arrays.toString(path) + ")";
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FacetResult.java b/lucene/facet/src/java/org/apache/lucene/facet/FacetResult.java
new file mode 100644
index 0000000..58bdaf4
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/FacetResult.java
@@ -0,0 +1,69 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+import java.util.List;
+
+public final class FacetResult {
+
+  /** Total value for this path (sum of all child counts, or
+   *  sum of all child values), even those not included in
+   *  the topN. */
+  public final Number value;
+
+  /** How many labels were populated under the requested
+   *  path. */
+  public final int childCount;
+
+  /** Child counts. */
+  public final LabelAndValue[] labelValues;
+
+  public FacetResult(Number value, LabelAndValue[] labelValues, int childCount) {
+    this.value = value;
+    this.labelValues = labelValues;
+    this.childCount = childCount;
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    sb.append("value=");
+    sb.append(value);
+    sb.append(" childCount=");
+    sb.append(childCount);
+    sb.append('\n');
+    for(LabelAndValue labelValue : labelValues) {
+      sb.append("  " + labelValue + "\n");
+    }
+    return sb.toString();
+  }
+
+  @Override
+  public boolean equals(Object _other) {
+    if ((_other instanceof FacetResult) == false) {
+      return false;
+    }
+    FacetResult other = (FacetResult) _other;
+    return value.equals(other.value) &&
+      childCount == other.childCount &&
+      Arrays.equals(labelValues, other.labelValues);
+  }
+
+  // nocommit hashCode
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/Facets.java b/lucene/facet/src/java/org/apache/lucene/facet/Facets.java
new file mode 100644
index 0000000..4e66979
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/Facets.java
@@ -0,0 +1,121 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.FilteredQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MultiCollector;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.TopFieldCollector;
+import org.apache.lucene.search.TopFieldDocs;
+import org.apache.lucene.search.TopScoreDocCollector;
+
+public abstract class Facets {
+  /** Returns the topN child labels under the specified
+   *  path.  Returns null if the specified path doesn't
+   *  exist. */
+  public abstract FacetResult getTopChildren(int topN, String dim, String... path) throws IOException;
+
+  /** Return the count for a specific path.  Returns -1 if
+   *  this path doesn't exist, else the count. */
+  public abstract Number getSpecificValue(String dim, String... path) throws IOException;
+
+  /** Returns topN labels for any dimension that had hits,
+   *  sorted by the number of hits that dimension matched;
+   *  this is used for "sparse" faceting, where many
+   *  different dimensions were indexed depending on the
+   *  type of document. */
+  public abstract List<FacetResult> getAllDims(int topN) throws IOException;
+
+  // nocommit where to move?
+
+  /** Utility method, to search for top hits by score
+   *  ({@link IndexSearcher#search(Query,int)}), but
+   *  also collect results into a {@link
+   *  FacetsCollector} for faceting. */
+  public static TopDocs search(IndexSearcher searcher, Query q, int topN, FacetsCollector sfc) throws IOException {
+    // nocommit can we pass the "right" boolean for
+    // in-order...?  we'd need access to the protected
+    // IS.search methods taking Weight... could use
+    // reflection...
+    TopScoreDocCollector hitsCollector = TopScoreDocCollector.create(topN, false);
+    searcher.search(q, MultiCollector.wrap(hitsCollector, sfc));
+    return hitsCollector.topDocs();
+  }
+
+  // nocommit where to move?
+
+  /** Utility method, to search for top hits by score with a filter
+   *  ({@link IndexSearcher#search(Query,Filter,int)}), but
+   *  also collect results into a {@link
+   *  FacetsCollector} for faceting. */
+  public static TopDocs search(IndexSearcher searcher, Query q, Filter filter, int topN, FacetsCollector sfc) throws IOException {
+    if (filter != null) {
+      q = new FilteredQuery(q, filter);
+    }
+    return search(searcher, q, topN, sfc);
+  }
+
+  // nocommit where to move?
+
+  /** Utility method, to search for top hits by a custom
+   *  {@link Sort} with a filter
+   *  ({@link IndexSearcher#search(Query,Filter,int,Sort)}), but
+   *  also collect results into a {@link
+   *  FacetsCollector} for faceting. */
+  public static TopFieldDocs search(IndexSearcher searcher, Query q, Filter filter, int topN, Sort sort, FacetsCollector sfc) throws IOException {
+    return search(searcher, q, filter, topN, sort, false, false, sfc);
+  }
+
+  // nocommit where to move?
+
+  /** Utility method, to search for top hits by a custom
+   *  {@link Sort} with a filter
+   *  ({@link IndexSearcher#search(Query,Filter,int,Sort,boolean,boolean)}), but
+   *  also collect results into a {@link
+   *  FacetsCollector} for faceting. */
+  public static TopFieldDocs search(IndexSearcher searcher, Query q, Filter filter, int topN, Sort sort, boolean doDocScores, boolean doMaxScore, FacetsCollector sfc) throws IOException {
+    int limit = searcher.getIndexReader().maxDoc();
+    if (limit == 0) {
+      limit = 1;
+    }
+    topN = Math.min(topN, limit);
+
+    boolean fillFields = true;
+    TopFieldCollector hitsCollector = TopFieldCollector.create(sort, topN,
+                                                               null,
+                                                               fillFields,
+                                                               doDocScores,
+                                                               doMaxScore,
+                                                               false);
+    if (filter != null) {
+      q = new FilteredQuery(q, filter);
+    }
+    searcher.search(q, MultiCollector.wrap(hitsCollector, sfc));
+    return (TopFieldDocs) hitsCollector.topDocs();
+  }
+
+  // nocommit need searchAfter variants too
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java b/lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java
new file mode 100644
index 0000000..715ee0e
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/FacetsCollector.java
@@ -0,0 +1,127 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.FixedBitSet;
+
+// nocommit javadocs
+public final class FacetsCollector extends Collector {
+
+  private AtomicReaderContext context;
+  private Scorer scorer;
+  private FixedBitSet bits;
+  private int totalHits;
+  private float[] scores;
+  private final boolean keepScores;
+  private final List<MatchingDocs> matchingDocs = new ArrayList<MatchingDocs>();
+  
+  /**
+   * Holds the documents that were matched in the {@link AtomicReaderContext}.
+   * If scores were required, then {@code scores} is not null.
+   */
+  public final static class MatchingDocs {
+    
+    public final AtomicReaderContext context;
+    public final FixedBitSet bits;
+    public final float[] scores;
+    public final int totalHits;
+    
+    public MatchingDocs(AtomicReaderContext context, FixedBitSet bits, int totalHits, float[] scores) {
+      this.context = context;
+      this.bits = bits;
+      this.scores = scores;
+      this.totalHits = totalHits;
+    }
+  }
+
+  public FacetsCollector() {
+    this(false);
+  }
+
+  public FacetsCollector(boolean keepScores) {
+    this.keepScores = keepScores;
+  }
+
+  public boolean getKeepScores() {
+    return keepScores;
+  }
+  
+  /**
+   * Returns the documents matched by the query, one {@link MatchingDocs} per
+   * visited segment.
+   */
+  public List<MatchingDocs> getMatchingDocs() {
+    if (bits != null) {
+      matchingDocs.add(new MatchingDocs(this.context, bits, totalHits, scores));
+      bits = null;
+      scores = null;
+      context = null;
+    }
+
+    return matchingDocs;
+  }
+    
+  @Override
+  public final boolean acceptsDocsOutOfOrder() {
+    // nocommit why not true?
+    return false;
+  }
+
+  @Override
+  public final void collect(int doc) throws IOException {
+    bits.set(doc);
+    if (keepScores) {
+      if (totalHits >= scores.length) {
+        float[] newScores = new float[ArrayUtil.oversize(totalHits + 1, 4)];
+        System.arraycopy(scores, 0, newScores, 0, totalHits);
+        scores = newScores;
+      }
+      scores[totalHits] = scorer.score();
+    }
+    totalHits++;
+  }
+
+  @Override
+  public final void setScorer(Scorer scorer) throws IOException {
+    this.scorer = scorer;
+  }
+    
+  @Override
+  public final void setNextReader(AtomicReaderContext context) throws IOException {
+    if (bits != null) {
+      matchingDocs.add(new MatchingDocs(this.context, bits, totalHits, scores));
+    }
+    bits = new FixedBitSet(context.reader().maxDoc());
+    totalHits = 0;
+    if (keepScores) {
+      scores = new float[64]; // some initial size
+    }
+    this.context = context;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java b/lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java
new file mode 100644
index 0000000..59cfe5d
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/FacetsConfig.java
@@ -0,0 +1,517 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.SortedSetDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.index.IndexDocument;
+import org.apache.lucene.index.IndexableField;
+import org.apache.lucene.index.IndexableFieldType;
+import org.apache.lucene.index.StorableField;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/** By default a dimension is flat, single valued and does
+ *  not require count for the dimension; use
+ *  the setters in this class to change these settings for
+ *  any dims.
+ *
+ *  <p><b>NOTE</b>: this configuration is not saved into the
+ *  index, but it's vital, and up to the application to
+ *  ensure, that at search time the provided FacetsConfig
+ *  matches what was used during indexing.
+ *
+ *  @lucene.experimental */
+public class FacetsConfig {
+
+  public static final String DEFAULT_INDEX_FIELD_NAME = "$facets";
+
+  private final Map<String,DimConfig> fieldTypes = new ConcurrentHashMap<String,DimConfig>();
+
+  // Used only for best-effort detection of app mixing
+  // int/float/bytes in a single indexed field:
+  private final Map<String,String> assocDimTypes = new ConcurrentHashMap<String,String>();
+
+  private final TaxonomyWriter taxoWriter;
+
+  /** @lucene.internal */
+  // nocommit expose this to the user, vs the setters?
+  public static final class DimConfig {
+    /** True if this dimension is hierarchical. */
+    boolean hierarchical;
+
+    /** True if this dimension is multi-valued. */
+    boolean multiValued;
+
+    /** True if the count/aggregate for the entire dimension
+     *  is required, which is unusual (default is false). */
+    boolean requireDimCount;
+
+    /** Actual field where this dimension's facet labels
+     *  should be indexed */
+    String indexFieldName = DEFAULT_INDEX_FIELD_NAME;
+  }
+
+  public FacetsConfig() {
+    this(null);
+  }
+
+  public FacetsConfig(TaxonomyWriter taxoWriter) {
+    this.taxoWriter = taxoWriter;
+  }
+
+  public final static DimConfig DEFAULT_DIM_CONFIG = new DimConfig();
+
+  public DimConfig getDimConfig(String dimName) {
+    DimConfig ft = fieldTypes.get(dimName);
+    if (ft == null) {
+      ft = DEFAULT_DIM_CONFIG;
+    }
+    return ft;
+  }
+
+  // nocommit maybe setDimConfig instead?
+  public synchronized void setHierarchical(String dimName, boolean v) {
+    DimConfig ft = fieldTypes.get(dimName);
+    if (ft == null) {
+      ft = new DimConfig();
+      fieldTypes.put(dimName, ft);
+    }
+    ft.hierarchical = v;
+  }
+
+  public synchronized void setMultiValued(String dimName, boolean v) {
+    DimConfig ft = fieldTypes.get(dimName);
+    if (ft == null) {
+      ft = new DimConfig();
+      fieldTypes.put(dimName, ft);
+    }
+    ft.multiValued = v;
+  }
+
+  public synchronized void setRequireDimCount(String dimName, boolean v) {
+    DimConfig ft = fieldTypes.get(dimName);
+    if (ft == null) {
+      ft = new DimConfig();
+      fieldTypes.put(dimName, ft);
+    }
+    ft.requireDimCount = v;
+  }
+
+  public synchronized void setIndexFieldName(String dimName, String indexFieldName) {
+    DimConfig ft = fieldTypes.get(dimName);
+    if (ft == null) {
+      ft = new DimConfig();
+      fieldTypes.put(dimName, ft);
+    }
+    ft.indexFieldName = indexFieldName;
+  }
+
+  Map<String,DimConfig> getDimConfigs() {
+    return fieldTypes;
+  }
+
+  private static void checkSeen(Set<String> seenDims, String dim) {
+    if (seenDims.contains(dim)) {
+      throw new IllegalArgumentException("dimension \"" + dim + "\" is not multiValued, but it appears more than once in this document");
+    }
+    seenDims.add(dim);
+  }
+
+  /** Translates any added {@link FacetField}s into normal
+   *  fields for indexing */
+  public IndexDocument build(IndexDocument doc) throws IOException {
+    // Find all FacetFields, collated by the actual field:
+    Map<String,List<FacetField>> byField = new HashMap<String,List<FacetField>>();
+
+    // ... and also all SortedSetDocValuesFacetFields:
+    Map<String,List<SortedSetDocValuesFacetField>> dvByField = new HashMap<String,List<SortedSetDocValuesFacetField>>();
+
+    // ... and also all AssociationFacetFields
+    Map<String,List<AssociationFacetField>> assocByField = new HashMap<String,List<AssociationFacetField>>();
+
+    Set<String> seenDims = new HashSet<String>();
+
+    for(IndexableField field : doc.indexableFields()) {
+      if (field.fieldType() == FacetField.TYPE) {
+        FacetField facetField = (FacetField) field;
+        FacetsConfig.DimConfig dimConfig = getDimConfig(facetField.dim);
+        if (dimConfig.multiValued == false) {
+          checkSeen(seenDims, facetField.dim);
+        }
+        String indexFieldName = dimConfig.indexFieldName;
+        List<FacetField> fields = byField.get(indexFieldName);
+        if (fields == null) {
+          fields = new ArrayList<FacetField>();
+          byField.put(indexFieldName, fields);
+        }
+        fields.add(facetField);
+      }
+
+      if (field.fieldType() == SortedSetDocValuesFacetField.TYPE) {
+        SortedSetDocValuesFacetField facetField = (SortedSetDocValuesFacetField) field;
+        FacetsConfig.DimConfig dimConfig = getDimConfig(facetField.dim);
+        if (dimConfig.multiValued == false) {
+          checkSeen(seenDims, facetField.dim);
+        }
+        String indexFieldName = dimConfig.indexFieldName;
+        List<SortedSetDocValuesFacetField> fields = dvByField.get(indexFieldName);
+        if (fields == null) {
+          fields = new ArrayList<SortedSetDocValuesFacetField>();
+          dvByField.put(indexFieldName, fields);
+        }
+        fields.add(facetField);
+      }
+
+      if (field.fieldType() == AssociationFacetField.TYPE) {
+        AssociationFacetField facetField = (AssociationFacetField) field;
+        FacetsConfig.DimConfig dimConfig = getDimConfig(facetField.dim);
+        if (dimConfig.multiValued == false) {
+          checkSeen(seenDims, facetField.dim);
+        }
+        if (dimConfig.hierarchical) {
+          throw new IllegalArgumentException("AssociationFacetField cannot be hierarchical (dim=\"" + facetField.dim + "\")");
+        }
+        if (dimConfig.requireDimCount) {
+          throw new IllegalArgumentException("AssociationFacetField cannot requireDimCount (dim=\"" + facetField.dim + "\")");
+        }
+
+        String indexFieldName = dimConfig.indexFieldName;
+        List<AssociationFacetField> fields = assocByField.get(indexFieldName);
+        if (fields == null) {
+          fields = new ArrayList<AssociationFacetField>();
+          assocByField.put(indexFieldName, fields);
+        }
+        fields.add(facetField);
+
+        // Best effort: detect mis-matched types in same
+        // indexed field:
+        String type;
+        if (facetField instanceof IntAssociationFacetField) {
+          type = "int";
+        } else if (facetField instanceof FloatAssociationFacetField) {
+          type = "float";
+        } else {
+          type = "bytes";
+        }
+        // NOTE: not thread safe, but this is just best effort:
+        String curType = assocDimTypes.get(indexFieldName);
+        if (curType == null) {
+          assocDimTypes.put(indexFieldName, type);
+        } else if (!curType.equals(type)) {
+          throw new IllegalArgumentException("mixing incompatible types of AssocationFacetField (" + curType + " and " + type + ") in indexed field \"" + indexFieldName + "\"; use FacetsConfig to change the indexFieldName for each dimension");
+        }
+      }
+    }
+
+    List<Field> addedIndexedFields = new ArrayList<Field>();
+    List<Field> addedStoredFields = new ArrayList<Field>();
+
+    processFacetFields(byField, addedIndexedFields, addedStoredFields);
+    processSSDVFacetFields(dvByField, addedIndexedFields, addedStoredFields);
+    processAssocFacetFields(assocByField, addedIndexedFields, addedStoredFields);
+
+    //System.out.println("add stored: " + addedStoredFields);
+
+    final List<IndexableField> allIndexedFields = new ArrayList<IndexableField>();
+    for(IndexableField field : doc.indexableFields()) {
+      IndexableFieldType ft = field.fieldType();
+      if (ft != FacetField.TYPE && ft != SortedSetDocValuesFacetField.TYPE && ft != AssociationFacetField.TYPE) {
+        allIndexedFields.add(field);
+      }
+    }
+    allIndexedFields.addAll(addedIndexedFields);
+
+    final List<StorableField> allStoredFields = new ArrayList<StorableField>();
+    for(StorableField field : doc.storableFields()) {
+      allStoredFields.add(field);
+    }
+    allStoredFields.addAll(addedStoredFields);
+
+    //System.out.println("all indexed: " + allIndexedFields);
+    //System.out.println("all stored: " + allStoredFields);
+
+    return new IndexDocument() {
+        @Override
+        public Iterable<IndexableField> indexableFields() {
+          return allIndexedFields;
+        }
+
+        @Override
+        public Iterable<StorableField> storableFields() {
+          return allStoredFields;
+        }
+      };
+  }
+
+  private void processFacetFields(Map<String,List<FacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
+
+    for(Map.Entry<String,List<FacetField>> ent : byField.entrySet()) {
+
+      String indexFieldName = ent.getKey();
+      //System.out.println("  fields=" + ent.getValue());
+
+      IntsRef ordinals = new IntsRef(32);
+      for(FacetField facetField : ent.getValue()) {
+
+        FacetsConfig.DimConfig ft = getDimConfig(facetField.dim);
+        if (facetField.path.length > 1 && ft.hierarchical == false) {
+          throw new IllegalArgumentException("dimension \"" + facetField.dim + "\" is not hierarchical yet has " + facetField.path.length + " components");
+        }
+      
+        FacetLabel cp = FacetLabel.create(facetField.dim, facetField.path);
+
+        checkTaxoWriter();
+        int ordinal = taxoWriter.addCategory(cp);
+        if (ordinals.length == ordinals.ints.length) {
+          ordinals.grow(ordinals.length+1);
+        }
+        ordinals.ints[ordinals.length++] = ordinal;
+        //System.out.println("  add cp=" + cp);
+
+        if (ft.multiValued && (ft.hierarchical || ft.requireDimCount)) {
+          // Add all parents too:
+          int parent = taxoWriter.getParent(ordinal);
+          while (parent > 0) {
+            if (ordinals.ints.length == ordinals.length) {
+              ordinals.grow(ordinals.length+1);
+            }
+            ordinals.ints[ordinals.length++] = parent;
+            parent = taxoWriter.getParent(parent);
+          }
+
+          if (ft.requireDimCount == false) {
+            // Remove last (dimension) ord:
+            ordinals.length--;
+          }
+        }
+
+        // Drill down:
+        for(int i=1;i<=cp.length;i++) {
+          addedIndexedFields.add(new StringField(indexFieldName, pathToString(cp.components, i), Field.Store.NO));
+        }
+      }
+
+      // Facet counts:
+      // DocValues are considered stored fields:
+      addedStoredFields.add(new BinaryDocValuesField(indexFieldName, dedupAndEncode(ordinals)));
+    }
+  }
+
+  private void processSSDVFacetFields(Map<String,List<SortedSetDocValuesFacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
+    //System.out.println("process SSDV: " + byField);
+    for(Map.Entry<String,List<SortedSetDocValuesFacetField>> ent : byField.entrySet()) {
+
+      String indexFieldName = ent.getKey();
+      //System.out.println("  field=" + indexFieldName);
+
+      for(SortedSetDocValuesFacetField facetField : ent.getValue()) {
+        FacetLabel cp = new FacetLabel(facetField.dim, facetField.label);
+        String fullPath = pathToString(cp.components, cp.length);
+        //System.out.println("add " + fullPath);
+
+        // For facet counts:
+        addedStoredFields.add(new SortedSetDocValuesField(indexFieldName, new BytesRef(fullPath)));
+
+        // For drill-down:
+        addedIndexedFields.add(new StringField(indexFieldName, fullPath, Field.Store.NO));
+        addedIndexedFields.add(new StringField(indexFieldName, facetField.dim, Field.Store.NO));
+      }
+    }
+  }
+
+  private void processAssocFacetFields(Map<String,List<AssociationFacetField>> byField,
+                                       List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
+    for(Map.Entry<String,List<AssociationFacetField>> ent : byField.entrySet()) {
+      byte[] bytes = new byte[16];
+      int upto = 0;
+      String indexFieldName = ent.getKey();
+      for(AssociationFacetField field : ent.getValue()) {
+        // NOTE: we don't add parents for associations
+        // nocommit is that right?  maybe we are supposed to
+        // add to taxo writer, and just not index the parent
+        // ords?
+        checkTaxoWriter();
+        int ordinal = taxoWriter.addCategory(FacetLabel.create(field.dim, field.path));
+        if (upto + 4 > bytes.length) {
+          bytes = ArrayUtil.grow(bytes, upto+4);
+        }
+        // big-endian:
+        bytes[upto++] = (byte) (ordinal >> 24);
+        bytes[upto++] = (byte) (ordinal >> 16);
+        bytes[upto++] = (byte) (ordinal >> 8);
+        bytes[upto++] = (byte) ordinal;
+        if (upto + field.assoc.length > bytes.length) {
+          bytes = ArrayUtil.grow(bytes, upto+field.assoc.length);
+        }
+        System.arraycopy(field.assoc.bytes, field.assoc.offset, bytes, upto, field.assoc.length);
+        upto += field.assoc.length;
+      }
+      addedStoredFields.add(new BinaryDocValuesField(indexFieldName, new BytesRef(bytes, 0, upto)));
+    }
+  }
+
+  /** Encodes ordinals into a BytesRef; expert: subclass can
+   *  override this to change encoding. */
+  protected BytesRef dedupAndEncode(IntsRef ordinals) {
+    Arrays.sort(ordinals.ints, ordinals.offset, ordinals.length);
+    byte[] bytes = new byte[5*ordinals.length];
+    int lastOrd = -1;
+    int upto = 0;
+    for(int i=0;i<ordinals.length;i++) {
+      int ord = ordinals.ints[ordinals.offset+i];
+      // ord could be == lastOrd, so we must dedup:
+      if (ord > lastOrd) {
+        int delta;
+        if (lastOrd == -1) {
+          delta = ord;
+        } else {
+          delta = ord - lastOrd;
+        }
+        if ((delta & ~0x7F) == 0) {
+          bytes[upto] = (byte) delta;
+          upto++;
+        } else if ((delta & ~0x3FFF) == 0) {
+          bytes[upto] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 1] = (byte) (delta & 0x7F);
+          upto += 2;
+        } else if ((delta & ~0x1FFFFF) == 0) {
+          bytes[upto] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
+          bytes[upto + 1] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 2] = (byte) (delta & 0x7F);
+          upto += 3;
+        } else if ((delta & ~0xFFFFFFF) == 0) {
+          bytes[upto] = (byte) (0x80 | ((delta & 0xFE00000) >> 21));
+          bytes[upto + 1] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
+          bytes[upto + 2] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 3] = (byte) (delta & 0x7F);
+          upto += 4;
+        } else {
+          bytes[upto] = (byte) (0x80 | ((delta & 0xF0000000) >> 28));
+          bytes[upto + 1] = (byte) (0x80 | ((delta & 0xFE00000) >> 21));
+          bytes[upto + 2] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
+          bytes[upto + 3] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
+          bytes[upto + 4] = (byte) (delta & 0x7F);
+          upto += 5;
+        }
+        lastOrd = ord;
+      }
+    }
+    return new BytesRef(bytes, 0, upto);
+  }
+
+  private void checkTaxoWriter() {
+    if (taxoWriter == null) {
+      throw new IllegalStateException("a valid TaxonomyWriter must be provided to the constructor (got null), when using FacetField or AssociationFacetField");
+    }
+  }
+
+  // Joins the path components together:
+  private static final char DELIM_CHAR = '\u001F';
+
+  // Escapes any occurrence of the path component inside the label:
+  private static final char ESCAPE_CHAR = '\u001E';
+
+  /** Turns a path into a string without stealing any
+   *  characters. */
+  public static String pathToString(String dim, String[] path) {
+    String[] fullPath = new String[1+path.length];
+    fullPath[0] = dim;
+    System.arraycopy(path, 0, fullPath, 1, path.length);
+    return pathToString(fullPath, fullPath.length);
+  }
+
+  public static String pathToString(String[] path) {
+    return pathToString(path, path.length);
+  }
+
+  public static String pathToString(String[] path, int length) {
+    // nocommit .... too anal?  shouldn't we allow drill
+    // down on just dim, to get all docs that have that
+    // dim...?
+    /*
+    if (path.length < 2) {
+      throw new IllegalArgumentException("path length must be > 0 (dim=" + path[0] + ")");
+    }
+    */
+    if (length == 0) {
+      return "";
+    }
+    StringBuilder sb = new StringBuilder();
+    for(int i=0;i<length;i++) {
+      String s = path[i];
+      int numChars = s.length();
+      for(int j=0;j<numChars;j++) {
+        char ch = s.charAt(j);
+        if (ch == DELIM_CHAR || ch == ESCAPE_CHAR) {
+          sb.append(ESCAPE_CHAR);
+        }
+        sb.append(ch);
+      }
+      sb.append(DELIM_CHAR);
+    }
+
+    // Trim off last DELIM_CHAR:
+    sb.setLength(sb.length()-1);
+    return sb.toString();
+  }
+
+  /** Turns a result from previous call to {@link
+   *  #pathToString} back into the original {@code String[]}
+   *  without stealing any characters. */
+  public static String[] stringToPath(String s) {
+    List<String> parts = new ArrayList<String>();
+    int length = s.length();
+    char[] buffer = new char[length];
+
+    int upto = 0;
+    boolean lastEscape = false;
+    for(int i=0;i<length;i++) {
+      char ch = s.charAt(i);
+      if (lastEscape) {
+        buffer[upto++] = ch;
+        lastEscape = false;
+      } else if (ch == ESCAPE_CHAR) {
+        lastEscape = true;
+      } else if (ch == DELIM_CHAR) {
+        parts.add(new String(buffer, 0, upto));
+        upto = 0;
+      } else {
+        buffer[upto++] = ch;
+      }
+    }
+    parts.add(new String(buffer, 0, upto));
+    assert !lastEscape;
+    return parts.toArray(new String[parts.size()]);
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FastTaxonomyFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/FastTaxonomyFacetCounts.java
new file mode 100644
index 0000000..569b403
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/FastTaxonomyFacetCounts.java
@@ -0,0 +1,190 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+// nocommit jdoc that this assumes/requires the default encoding
+public class FastTaxonomyFacetCounts extends TaxonomyFacets {
+  private final int[] counts;
+
+  public FastTaxonomyFacetCounts(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
+  }
+
+  public FastTaxonomyFacetCounts(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    counts = new int[taxoReader.getSize()];
+    count(fc.getMatchingDocs());
+  }
+
+  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
+    for(MatchingDocs hits : matchingDocs) {
+      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
+      if (dv == null) { // this reader does not have DocValues for the requested category list
+        continue;
+      }
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      BytesRef scratch = new BytesRef();
+      //System.out.println("count seg=" + hits.context.reader());
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        //System.out.println("  doc=" + doc);
+        dv.get(doc, scratch);
+        byte[] bytes = scratch.bytes;
+        int end = scratch.offset + scratch.length;
+        int ord = 0;
+        int offset = scratch.offset;
+        int prev = 0;
+        while (offset < end) {
+          byte b = bytes[offset++];
+          if (b >= 0) {
+            prev = ord = ((ord << 7) | b) + prev;
+            assert ord < counts.length: "ord=" + ord + " vs maxOrd=" + counts.length;
+            //System.out.println("    ord=" + ord);
+            ++counts[ord];
+            ord = 0;
+          } else {
+            ord = (ord << 7) | (b & 0x7F);
+          }
+        }
+        ++doc;
+      }
+    }
+
+    // nocommit we could do this lazily instead:
+
+    // Rollup any necessary dims:
+    for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
+      String dim = ent.getKey();
+      FacetsConfig.DimConfig ft = ent.getValue();
+      if (ft.hierarchical && ft.multiValued == false) {
+        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
+        // It can be -1 if this field was declared in the
+        // config but never indexed:
+        if (dimRootOrd > 0) {
+          counts[dimRootOrd] += rollup(children[dimRootOrd]);
+        }
+      }
+    }
+  }
+
+  private int rollup(int ord) {
+    int sum = 0;
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      int childValue = counts[ord] + rollup(children[ord]);
+      counts[ord] = childValue;
+      sum += childValue;
+      ord = siblings[ord];
+    }
+    return sum;
+  }
+
+  /** Return the count for a specific path.  Returns -1 if
+   *  this path doesn't exist, else the count. */
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    verifyDim(dim);
+    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
+    if (ord < 0) {
+      return -1;
+    }
+    return counts[ord];
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    FacetsConfig.DimConfig dimConfig = verifyDim(dim);
+    //System.out.println("ftfc.getTopChildren topN=" + topN);
+    FacetLabel cp = FacetLabel.create(dim, path);
+    int dimOrd = taxoReader.getOrdinal(cp);
+    if (dimOrd == -1) {
+      //System.out.println("no ord for dim=" + dim + " path=" + path);
+      return null;
+    }
+
+    TopOrdAndIntQueue q = new TopOrdAndIntQueue(Math.min(taxoReader.getSize(), topN));
+    
+    int bottomCount = 0;
+
+    int ord = children[dimOrd];
+    int totCount = 0;
+    int childCount = 0;
+
+    TopOrdAndIntQueue.OrdAndValue reuse = null;
+    while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      //System.out.println("  check ord=" + ord + " label=" + taxoReader.getPath(ord) + " topN=" + topN);
+      if (counts[ord] > 0) {
+        totCount += counts[ord];
+        childCount++;
+        if (counts[ord] > bottomCount) {
+          if (reuse == null) {
+            reuse = new TopOrdAndIntQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = counts[ord];
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomCount = q.top().value;
+          }
+        }
+      }
+
+      ord = siblings[ord];
+    }
+
+    if (totCount == 0) {
+      //System.out.println("  no matches");
+      return null;
+    }
+
+    if (dimConfig.multiValued) {
+      if (dimConfig.requireDimCount) {
+        totCount = counts[dimOrd];
+      } else {
+        // Our sum'd count is not correct, in general:
+        totCount = -1;
+      }
+    } else {
+      // Our sum'd dim count is accurate, so we keep it
+    }
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
+      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
+      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
+    }
+
+    return new FacetResult(totCount, labelValues, childCount);
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FloatAssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/FloatAssociationFacetField.java
new file mode 100644
index 0000000..e18e984
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/FloatAssociationFacetField.java
@@ -0,0 +1,48 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.util.BytesRef;
+
+/** Associates an arbitrary float with the added facet
+ *  path, encoding the float into a 4-byte BytesRef. */
+public class FloatAssociationFacetField extends AssociationFacetField {
+
+  /** Utility ctor: associates an int value (translates it
+   *  to 4-byte BytesRef). */
+  public FloatAssociationFacetField(float assoc, String dim, String... path) {
+    super(floatToBytesRef(assoc), dim, path);
+  }
+
+  public static BytesRef floatToBytesRef(float v) {
+    return IntAssociationFacetField.intToBytesRef(Float.floatToIntBits(v));
+  }
+
+  public static float bytesRefToFloat(BytesRef b) {
+    return Float.intBitsToFloat(IntAssociationFacetField.bytesRefToInt(b));
+  }
+
+  @Override
+  public String toString() {
+    return "FloatAssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " value=" + bytesRefToFloat(assoc) + ")";
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FloatRange.java b/lucene/facet/src/java/org/apache/lucene/facet/FloatRange.java
new file mode 100644
index 0000000..f25ee9c
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/FloatRange.java
@@ -0,0 +1,70 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.FloatDocValuesField; // javadocs
+
+/** Represents a range over float values indexed as {@link
+ *  FloatDocValuesField}.  */
+public final class FloatRange extends Range {
+  private final float minIncl;
+  private final float maxIncl;
+
+  public final float min;
+  public final float max;
+  public final boolean minInclusive;
+  public final boolean maxInclusive;
+
+  /** Create a FloatRange. */
+  public FloatRange(String label, float min, boolean minInclusive, float max, boolean maxInclusive) {
+    super(label);
+    this.min = min;
+    this.max = max;
+    this.minInclusive = minInclusive;
+    this.maxInclusive = maxInclusive;
+
+    // TODO: if FloatDocValuesField used
+    // NumericUtils.floatToSortableInt format (instead of
+    // Float.floatToRawIntBits) we could do comparisons
+    // in int space 
+
+    if (Float.isNaN(min)) {
+      throw new IllegalArgumentException("min cannot be NaN");
+    }
+    if (!minInclusive) {
+      min = Math.nextUp(min);
+    }
+
+    if (Float.isNaN(max)) {
+      throw new IllegalArgumentException("max cannot be NaN");
+    }
+    if (!maxInclusive) {
+      // Why no Math.nextDown?
+      max = Math.nextAfter(max, Float.NEGATIVE_INFINITY);
+    }
+
+    this.minIncl = min;
+    this.maxIncl = max;
+  }
+
+  @Override
+  public boolean accept(long value) {
+    float floatValue = Float.intBitsToFloat((int) value);
+    return floatValue >= minIncl && floatValue <= maxIncl;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/IntAssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/IntAssociationFacetField.java
new file mode 100644
index 0000000..62b925b
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/IntAssociationFacetField.java
@@ -0,0 +1,57 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.util.BytesRef;
+
+/** Associates an arbitrary int with the added facet
+ *  path, encoding the int into a 4-byte BytesRef. */
+public class IntAssociationFacetField extends AssociationFacetField {
+
+  /** Utility ctor: associates an int value (translates it
+   *  to 4-byte BytesRef). */
+  public IntAssociationFacetField(int assoc, String dim, String... path) {
+    super(intToBytesRef(assoc), dim, path);
+  }
+
+  public static BytesRef intToBytesRef(int v) {
+    byte[] bytes = new byte[4];
+    // big-endian:
+    bytes[0] = (byte) (v >> 24);
+    bytes[1] = (byte) (v >> 16);
+    bytes[2] = (byte) (v >> 8);
+    bytes[3] = (byte) v;
+    return new BytesRef(bytes);
+  }
+
+  public static int bytesRefToInt(BytesRef b) {
+    return ((b.bytes[b.offset]&0xFF) << 24) |
+      ((b.bytes[b.offset+1]&0xFF) << 16) |
+      ((b.bytes[b.offset+2]&0xFF) << 8) |
+      (b.bytes[b.offset+3]&0xFF);
+  }
+
+  @Override
+  public String toString() {
+    return "IntAssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " value=" + bytesRefToInt(assoc) + ")";
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/LabelAndValue.java b/lucene/facet/src/java/org/apache/lucene/facet/LabelAndValue.java
new file mode 100644
index 0000000..3de3628
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/LabelAndValue.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public final class LabelAndValue {
+  // nocommit BytesRef?
+  public final String label;
+
+  /** Value associated with this label. */
+  public final Number value;
+
+  public LabelAndValue(String label, Number value) {
+    this.label = label;
+    this.value = value;
+  }
+
+  @Override
+  public String toString() {
+    return label + " (" + value + ")";
+  }
+
+  @Override
+  public boolean equals(Object _other) {
+    if ((_other instanceof LabelAndValue) == false) {
+      return false;
+    }
+    LabelAndValue other = (LabelAndValue) _other;
+    return label.equals(other.label) && value.equals(other.value);
+  }
+
+  // nocommit hashCode
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/LongRange.java b/lucene/facet/src/java/org/apache/lucene/facet/LongRange.java
new file mode 100644
index 0000000..bb83d80
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/LongRange.java
@@ -0,0 +1,60 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.document.NumericDocValuesField; // javadocs
+
+/** Represents a range over long values indexed as {@link
+ *  NumericDocValuesField}.  */
+public final class LongRange extends Range {
+  private final long minIncl;
+  private final long maxIncl;
+
+  public final long min;
+  public final long max;
+  public final boolean minInclusive;
+  public final boolean maxInclusive;
+
+  // nocommit can we require fewer args? (same for
+  // Double/FloatRange too)
+
+  /** Create a LongRange. */
+  public LongRange(String label, long min, boolean minInclusive, long max, boolean maxInclusive) {
+    super(label);
+    this.min = min;
+    this.max = max;
+    this.minInclusive = minInclusive;
+    this.maxInclusive = maxInclusive;
+
+    if (!minInclusive && min != Long.MAX_VALUE) {
+      min++;
+    }
+
+    if (!maxInclusive && max != Long.MIN_VALUE) {
+      max--;
+    }
+
+    this.minIncl = min;
+    this.maxIncl = max;
+  }
+
+  @Override
+  public boolean accept(long value) {
+    return value >= minIncl && value <= maxIncl;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/MultiFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/MultiFacets.java
new file mode 100644
index 0000000..95a8086
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/MultiFacets.java
@@ -0,0 +1,66 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+/** Maps specified dims to provided Facets impls; else, uses
+ *  the default Facets impl. */
+public class MultiFacets extends Facets {
+  private final Map<String,Facets> dimToFacets;
+  private final Facets defaultFacets;
+
+  public MultiFacets(Map<String,Facets> dimToFacets) {
+    this(dimToFacets = dimToFacets, null);
+  }
+
+  public MultiFacets(Map<String,Facets> dimToFacets, Facets defaultFacets) {
+    this.dimToFacets = dimToFacets;
+    this.defaultFacets = defaultFacets;
+  }
+
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    Facets facets = dimToFacets.get(dim);
+    if (facets == null) {
+      if (defaultFacets == null) {
+        throw new IllegalArgumentException("invalid dim \"" + dim + "\"");
+      }
+      facets = defaultFacets;
+    }
+    return facets.getTopChildren(topN, dim, path);
+  }
+
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    Facets facets = dimToFacets.get(dim);
+    if (facets == null) {
+      if (defaultFacets == null) {
+        throw new IllegalArgumentException("invalid dim \"" + dim + "\"");
+      }
+      facets = defaultFacets;
+    }
+    return facets.getSpecificValue(dim, path);
+  }
+
+  public List<FacetResult> getAllDims(int topN) throws IOException {
+    // nocommit can/should we impl this?  ie, sparse
+    // faceting after drill sideways
+    throw new UnsupportedOperationException();
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/OrdinalsReader.java b/lucene/facet/src/java/org/apache/lucene/facet/OrdinalsReader.java
new file mode 100644
index 0000000..666fd70
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/OrdinalsReader.java
@@ -0,0 +1,39 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.util.IntsRef;
+
+/** Provides per-document ordinals. */
+
+public abstract class OrdinalsReader {
+
+  public static abstract class OrdinalsSegmentReader {
+    /** Get the ordinals for this document.  ordinals.offset
+     *  must always be 0! */
+    public abstract void get(int doc, IntsRef ordinals) throws IOException;
+  }
+
+  /** Set current atomic reader. */
+  public abstract OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException;
+
+  public abstract String getIndexFieldName();
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/Range.java b/lucene/facet/src/java/org/apache/lucene/facet/Range.java
new file mode 100644
index 0000000..cc34a97
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/Range.java
@@ -0,0 +1,33 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Represents a single labelled range, one facet label in
+ *  the facets computed by {@link RangeAccumulator}.
+ *
+ *  @lucene.experimental */
+
+public abstract class Range {
+  public final String label;
+
+  protected Range(String label) {
+    this.label = label;
+  }
+
+  public abstract boolean accept(long value);
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/RangeFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/RangeFacetCounts.java
new file mode 100644
index 0000000..c742125
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/RangeFacetCounts.java
@@ -0,0 +1,110 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.valuesource.LongFieldSource;
+
+/**
+ * accumulates counts for provided ranges.
+ */
+public class RangeFacetCounts extends Facets {
+  private final Range[] ranges;
+  private final int[] counts;
+  private final String field;
+  private int totCount;
+
+  public RangeFacetCounts(String field, FacetsCollector hits, Range... ranges) throws IOException {
+    this(field, new LongFieldSource(field), hits, ranges);
+  }
+
+  public RangeFacetCounts(String field, ValueSource valueSource, FacetsCollector hits, Range... ranges) throws IOException {
+    this.ranges = ranges;
+    this.field = field;
+    counts = new int[ranges.length];
+    count(valueSource, hits.getMatchingDocs());
+  }
+
+  private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
+
+    // TODO: test if this is faster (in the past it was
+    // faster to do MatchingDocs on the inside) ... see
+    // patches on LUCENE-4965):
+    for (MatchingDocs hits : matchingDocs) {
+      FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
+      final int length = hits.bits.length();
+      int doc = 0;
+      totCount += hits.totalHits;
+      while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
+        // Skip missing docs:
+        if (fv.exists(doc)) {
+          
+          long v = fv.longVal(doc);
+
+          // TODO: if all ranges are non-overlapping, we
+          // should instead do a bin-search up front
+          // (really, a specialized case of the interval
+          // tree)
+          // TODO: use interval tree instead of linear search:
+          for (int j = 0; j < ranges.length; j++) {
+            if (ranges[j].accept(v)) {
+              counts[j]++;
+            }
+          }
+        }
+
+        doc++;
+      }
+    }
+  }
+
+  // nocommit all args are ... unused ... this doesn't "fit"
+  // very well:
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) {
+    if (dim.equals(field) == false) {
+      throw new IllegalArgumentException("invalid dim \"" + dim + "\"; should be \"" + field + "\"");
+    }
+    LabelAndValue[] labelValues = new LabelAndValue[counts.length];
+    for(int i=0;i<counts.length;i++) {
+      // nocommit can we add the range into this?
+      labelValues[i] = new LabelAndValue(ranges[i].label, counts[i]);
+    }
+
+    return new FacetResult(totCount, labelValues, labelValues.length);
+  }
+
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    // nocommit we could impl this?
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public List<FacetResult> getAllDims(int topN) throws IOException {
+    return Collections.singletonList(getTopChildren(topN, null));
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/SearcherTaxonomyManager.java b/lucene/facet/src/java/org/apache/lucene/facet/SearcherTaxonomyManager.java
new file mode 100644
index 0000000..c4b303e
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/SearcherTaxonomyManager.java
@@ -0,0 +1,123 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.ReferenceManager;
+import org.apache.lucene.search.SearcherFactory;
+import org.apache.lucene.search.SearcherManager;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Manages near-real-time reopen of both an IndexSearcher
+ * and a TaxonomyReader.
+ *
+ * <p><b>NOTE</b>: If you call {@link
+ * DirectoryTaxonomyWriter#replaceTaxonomy} then you must
+ * open a new {@code SearcherTaxonomyManager} afterwards.
+ */
+public class SearcherTaxonomyManager extends ReferenceManager<SearcherTaxonomyManager.SearcherAndTaxonomy> {
+
+  /** Holds a matched pair of {@link IndexSearcher} and
+   *  {@link TaxonomyReader} */
+  public static class SearcherAndTaxonomy {
+    public final IndexSearcher searcher;
+    public final DirectoryTaxonomyReader taxonomyReader;
+
+    /** Create a SearcherAndTaxonomy */
+    public SearcherAndTaxonomy(IndexSearcher searcher, DirectoryTaxonomyReader taxonomyReader) {
+      this.searcher = searcher;
+      this.taxonomyReader = taxonomyReader;
+    }
+  }
+
+  private final SearcherFactory searcherFactory;
+  private final long taxoEpoch;
+  private final DirectoryTaxonomyWriter taxoWriter;
+
+  /** Creates near-real-time searcher and taxonomy reader
+   *  from the corresponding writers. */
+  public SearcherTaxonomyManager(IndexWriter writer, boolean applyAllDeletes, SearcherFactory searcherFactory, DirectoryTaxonomyWriter taxoWriter) throws IOException {
+    if (searcherFactory == null) {
+      searcherFactory = new SearcherFactory();
+    }
+    this.searcherFactory = searcherFactory;
+    this.taxoWriter = taxoWriter;
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    current = new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, DirectoryReader.open(writer, applyAllDeletes)),
+                                      taxoReader);
+    taxoEpoch = taxoWriter.getTaxonomyEpoch();
+  }
+
+  @Override
+  protected void decRef(SearcherAndTaxonomy ref) throws IOException {
+    ref.searcher.getIndexReader().decRef();
+
+    // This decRef can fail, and then in theory we should
+    // tryIncRef the searcher to put back the ref count
+    // ... but 1) the below decRef should only fail because
+    // it decRef'd to 0 and closed and hit some IOException
+    // during close, in which case 2) very likely the
+    // searcher was also just closed by the above decRef and
+    // a tryIncRef would fail:
+    ref.taxonomyReader.decRef();
+  }
+
+  @Override
+  protected boolean tryIncRef(SearcherAndTaxonomy ref) throws IOException {
+    if (ref.searcher.getIndexReader().tryIncRef()) {
+      if (ref.taxonomyReader.tryIncRef()) {
+        return true;
+      } else {
+        ref.searcher.getIndexReader().decRef();
+      }
+    }
+    return false;
+  }
+
+  @Override
+  protected SearcherAndTaxonomy refreshIfNeeded(SearcherAndTaxonomy ref) throws IOException {
+    // Must re-open searcher first, otherwise we may get a
+    // new reader that references ords not yet known to the
+    // taxonomy reader:
+    final IndexReader r = ref.searcher.getIndexReader();
+    final IndexReader newReader = DirectoryReader.openIfChanged((DirectoryReader) r);
+    if (newReader == null) {
+      return null;
+    } else {
+      DirectoryTaxonomyReader tr = TaxonomyReader.openIfChanged(ref.taxonomyReader);
+      if (tr == null) {
+        ref.taxonomyReader.incRef();
+        tr = ref.taxonomyReader;
+      } else if (taxoWriter.getTaxonomyEpoch() != taxoEpoch) {
+        IOUtils.close(newReader, tr);
+        throw new IllegalStateException("DirectoryTaxonomyWriter.replaceTaxonomy was called, which is not allowed when using SearcherTaxonomyManager");
+      }
+
+      return new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, newReader), tr);
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts.java
new file mode 100644
index 0000000..616f32e
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetCounts.java
@@ -0,0 +1,296 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.SortedSetDocValuesReaderState.OrdRange;
+import org.apache.lucene.facet.SortedSetDocValuesReaderState;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.ReaderUtil;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.PriorityQueue;
+
+/** Compute facets counts from previously
+ *  indexed {@link SortedSetDocValuesFacetField},
+ *  without require a separate taxonomy index.  Faceting is
+ *  a bit slower (~25%), and there is added cost on every
+ *  {@link IndexReader} open to create a new {@link
+ *  SortedSetDocValuesReaderState}.  Furthermore, this does
+ *  not support hierarchical facets; only flat (dimension +
+ *  label) facets, but it uses quite a bit less RAM to do
+ *  so.
+ *
+ *  After creating this class, invoke {@link #getDim} or
+ *  {@link #getAllDims} to retrieve facet results. */
+
+public class SortedSetDocValuesFacetCounts extends Facets {
+
+  final SortedSetDocValuesReaderState state;
+  final SortedSetDocValues dv;
+  final String field;
+  final int[] counts;
+
+  /** Sparse faceting: returns any dimension that had any
+   *  hits, topCount labels per dimension. */
+  public SortedSetDocValuesFacetCounts(SortedSetDocValuesReaderState state, FacetsCollector hits)
+      throws IOException {
+    this.state = state;
+    this.field = state.getField();
+    counts = new int[state.getSize()];
+    dv = state.getDocValues();
+    //System.out.println("field=" + field);
+    count(hits.getMatchingDocs());
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    if (path.length > 0) {
+      throw new IllegalArgumentException("path should be 0 length");
+    }
+    OrdRange ordRange = state.getOrdRange(dim);
+    if (ordRange == null) {
+      throw new IllegalArgumentException("dimension \"" + dim + "\" was not indexed");
+    }
+    return getDim(dim, ordRange, topN);
+  }
+
+  private final FacetResult getDim(String dim, OrdRange ordRange, int topN) {
+
+    TopOrdAndIntQueue q = null;
+
+    int bottomCount = 0;
+
+    int dimCount = 0;
+    int childCount = 0;
+
+    TopOrdAndIntQueue.OrdAndValue reuse = null;
+    //System.out.println("getDim : " + ordRange.start + " - " + ordRange.end);
+    for(int ord=ordRange.start; ord<=ordRange.end; ord++) {
+      //System.out.println("  ord=" + ord + " count=" + counts[ord]);
+      if (counts[ord] > 0) {
+        dimCount += counts[ord];
+        childCount++;
+        if (counts[ord] > bottomCount) {
+          if (reuse == null) {
+            reuse = new TopOrdAndIntQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = counts[ord];
+          if (q == null) {
+            // Lazy init, so we don't create this for the
+            // sparse case unnecessarily
+            q = new TopOrdAndIntQueue(topN);
+          }
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomCount = q.top().value;
+          }
+        }
+      }
+    }
+
+    if (q == null) {
+      return null;
+    }
+
+    BytesRef scratch = new BytesRef();
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
+      dv.lookupOrd(ordAndValue.ord, scratch);
+      String[] parts = FacetsConfig.stringToPath(scratch.utf8ToString());
+      labelValues[i] = new LabelAndValue(parts[1], ordAndValue.value);
+    }
+
+    return new FacetResult(dimCount, labelValues, childCount);
+  }
+
+  /** Does all the "real work" of tallying up the counts. */
+  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("ssdv count");
+
+    MultiDocValues.OrdinalMap ordinalMap;
+
+    // nocommit not quite right?  really, we need a way to
+    // verify that this ordinalMap "matches" the leaves in
+    // matchingDocs...
+    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {
+      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;
+    } else {
+      ordinalMap = null;
+    }
+
+    for(MatchingDocs hits : matchingDocs) {
+
+      AtomicReader reader = hits.context.reader();
+      //System.out.println("  reader=" + reader);
+      // LUCENE-5090: make sure the provided reader context "matches"
+      // the top-level reader passed to the
+      // SortedSetDocValuesReaderState, else cryptic
+      // AIOOBE can happen:
+      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {
+        throw new IllegalStateException("the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader");
+      }
+      
+      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);
+      if (segValues == null) {
+        // nocommit in trunk this was a "return" which is
+        // wrong; make a failing test
+        continue;
+      }
+
+      final int maxDoc = reader.maxDoc();
+      assert maxDoc == hits.bits.length();
+      //System.out.println("  dv=" + dv);
+
+      // nocommit, yet another option is to count all segs
+      // first, only in seg-ord space, and then do a
+      // merge-sort-PQ in the end to only "resolve to
+      // global" those seg ords that can compete, if we know
+      // we just want top K?  ie, this is the same algo
+      // that'd be used for merging facets across shards
+      // (distributed faceting).  but this has much higher
+      // temp ram req'ts (sum of number of ords across all
+      // segs)
+      if (ordinalMap != null) {
+        int segOrd = hits.context.ord;
+
+        int numSegOrds = (int) segValues.getValueCount();
+
+        if (hits.totalHits < numSegOrds/10) {
+          //System.out.println("    remap as-we-go");
+          // Remap every ord to global ord as we iterate:
+          int doc = 0;
+          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
+            //System.out.println("    doc=" + doc);
+            segValues.setDocument(doc);
+            int term = (int) segValues.nextOrd();
+            while (term != SortedSetDocValues.NO_MORE_ORDS) {
+              //System.out.println("      segOrd=" + segOrd + " ord=" + term + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, term));
+              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;
+              term = (int) segValues.nextOrd();
+            }
+            ++doc;
+          }
+        } else {
+          //System.out.println("    count in seg ord first");
+
+          // First count in seg-ord space:
+          final int[] segCounts = new int[numSegOrds];
+          int doc = 0;
+          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
+            //System.out.println("    doc=" + doc);
+            segValues.setDocument(doc);
+            int term = (int) segValues.nextOrd();
+            while (term != SortedSetDocValues.NO_MORE_ORDS) {
+              //System.out.println("      ord=" + term);
+              segCounts[term]++;
+              term = (int) segValues.nextOrd();
+            }
+            ++doc;
+          }
+
+          // Then, migrate to global ords:
+          for(int ord=0;ord<numSegOrds;ord++) {
+            int count = segCounts[ord];
+            if (count != 0) {
+              //System.out.println("    migrate segOrd=" + segOrd + " ord=" + ord + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, ord));
+              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;
+            }
+          }
+        }
+      } else {
+        // No ord mapping (e.g., single segment index):
+        // just aggregate directly into counts:
+
+        int doc = 0;
+        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
+          segValues.setDocument(doc);
+          int term = (int) segValues.nextOrd();
+          while (term != SortedSetDocValues.NO_MORE_ORDS) {
+            counts[term]++;
+            term = (int) segValues.nextOrd();
+          }
+          ++doc;
+        }
+      }
+    }
+  }
+
+  @Override
+  public Number getSpecificValue(String dim, String... path) {
+    if (path.length != 1) {
+      throw new IllegalArgumentException("path must be length=1");
+    }
+    // nocommit this is not thread safe in general?  add
+    // jdocs that app must instantiate & use from same thread?
+    int ord = (int) dv.lookupTerm(new BytesRef(FacetsConfig.pathToString(dim, path)));
+    if (ord < 0) {
+      return -1;
+    }
+
+    return counts[ord];
+  }
+
+  @Override
+  public List<FacetResult> getAllDims(int topN) throws IOException {
+
+    List<FacetResult> results = new ArrayList<FacetResult>();
+    for(Map.Entry<String,OrdRange> ent : state.getPrefixToOrdRange().entrySet()) {
+      FacetResult fr = getDim(ent.getKey(), ent.getValue(), topN);
+      if (fr != null) {
+        results.add(fr);
+      }
+    }
+
+    // Sort by highest count:
+    Collections.sort(results,
+                     new Comparator<FacetResult>() {
+                       @Override
+                       public int compare(FacetResult a, FacetResult b) {
+                         if (a.value.intValue() > b.value.intValue()) {
+                           return -1;
+                         } else if (b.value.intValue() > a.value.intValue()) {
+                           return 1;
+                         } else {
+                           return 0;
+                         }
+                       }
+                     });
+
+    return results;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetField.java
new file mode 100644
index 0000000..0ea6662
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesFacetField.java
@@ -0,0 +1,46 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+
+/** Add an instance of this to your Document for every facet
+ *  label to be indexed via SortedSetDocValues. */
+public class SortedSetDocValuesFacetField extends Field {
+  static final FieldType TYPE = new FieldType();
+  static {
+    TYPE.setIndexed(true);
+    TYPE.freeze();
+  }
+  final String dim;
+  final String label;
+
+  public SortedSetDocValuesFacetField(String dim, String label) {
+    super("dummy", TYPE);
+    this.dim = dim;
+    this.label = label;
+  }
+
+  @Override
+  public String toString() {
+    return "SortedSetDocValuesFacetField(dim=" + dim + " label=" + label + ")";
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesReaderState.java b/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesReaderState.java
new file mode 100644
index 0000000..0ad1759
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/SortedSetDocValuesReaderState.java
@@ -0,0 +1,148 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.regex.Pattern;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.CompositeReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.index.SortedSetDocValues;
+import org.apache.lucene.util.BytesRef;
+
+/** Wraps a {@link IndexReader} and resolves ords
+ *  using existing {@link SortedSetDocValues} APIs without a
+ *  separate taxonomy index.  This only supports flat facets
+ *  (dimension + label), and it makes faceting a bit
+ *  slower, adds some cost at reopen time, but avoids
+ *  managing the separate taxonomy index.  It also requires
+ *  less RAM than the taxonomy index, as it manages the flat
+ *  (2-level) hierarchy more efficiently.  In addition, the
+ *  tie-break during faceting is now meaningful (in label
+ *  sorted order).
+ *
+ *  <p><b>NOTE</b>: creating an instance of this class is
+ *  somewhat costly, as it computes per-segment ordinal maps,
+ *  so you should create it once and re-use that one instance
+ *  for a given {@link IndexReader}. */
+
+public final class SortedSetDocValuesReaderState {
+
+  private final String field;
+  private final AtomicReader topReader;
+  private final int valueCount;
+  public final IndexReader origReader;
+
+  /** Holds start/end range of ords, which maps to one
+   *  dimension (someday we may generalize it to map to
+   *  hierarchies within one dimension). */
+  public static final class OrdRange {
+    /** Start of range, inclusive: */
+    public final int start;
+    /** End of range, inclusive: */
+    public final int end;
+
+    /** Start and end are inclusive. */
+    public OrdRange(int start, int end) {
+      this.start = start;
+      this.end = end;
+    }
+  }
+
+  private final Map<String,OrdRange> prefixToOrdRange = new HashMap<String,OrdRange>();
+
+  public SortedSetDocValuesReaderState(IndexReader reader) throws IOException {
+    this(reader, FacetsConfig.DEFAULT_INDEX_FIELD_NAME);
+  }
+
+  /** Create an instance, scanning the {@link
+   *  SortedSetDocValues} from the provided reader, with
+   *  default {@link FacetIndexingParams}. */
+  public SortedSetDocValuesReaderState(IndexReader reader, String field) throws IOException {
+
+    this.field = field;
+    this.origReader = reader;
+
+    // We need this to create thread-safe MultiSortedSetDV
+    // per collector:
+    topReader = SlowCompositeReaderWrapper.wrap(reader);
+    SortedSetDocValues dv = topReader.getSortedSetDocValues(field);
+    if (dv == null) {
+      throw new IllegalArgumentException("field \"" + field + "\" was not indexed with SortedSetDocValues");
+    }
+    if (dv.getValueCount() > Integer.MAX_VALUE) {
+      throw new IllegalArgumentException("can only handle valueCount < Integer.MAX_VALUE; got " + dv.getValueCount());
+    }
+    valueCount = (int) dv.getValueCount();
+
+    // TODO: we can make this more efficient if eg we can be
+    // "involved" when OrdinalMap is being created?  Ie see
+    // each term/ord it's assigning as it goes...
+    String lastDim = null;
+    int startOrd = -1;
+    BytesRef spare = new BytesRef();
+
+    // TODO: this approach can work for full hierarchy?;
+    // TaxoReader can't do this since ords are not in
+    // "sorted order" ... but we should generalize this to
+    // support arbitrary hierarchy:
+    for(int ord=0;ord<valueCount;ord++) {
+      dv.lookupOrd(ord, spare);
+      String[] components = FacetsConfig.stringToPath(spare.utf8ToString());
+      if (components.length != 2) {
+        throw new IllegalArgumentException("this class can only handle 2 level hierarchy (dim/value); got: " + Arrays.toString(components) + " " + spare.utf8ToString());
+      }
+      if (!components[0].equals(lastDim)) {
+        if (lastDim != null) {
+          prefixToOrdRange.put(lastDim, new OrdRange(startOrd, ord-1));
+        }
+        startOrd = ord;
+        lastDim = components[0];
+      }
+    }
+
+    if (lastDim != null) {
+      prefixToOrdRange.put(lastDim, new OrdRange(startOrd, valueCount-1));
+    }
+  }
+
+  public SortedSetDocValues getDocValues() throws IOException {
+    return topReader.getSortedSetDocValues(field);
+  }
+
+  public Map<String,OrdRange> getPrefixToOrdRange() {
+    return prefixToOrdRange;
+  }
+
+  public OrdRange getOrdRange(String dim) {
+    return prefixToOrdRange.get(dim);
+  }
+
+  public String getField() {
+    return field;
+  }
+
+  public int getSize() {
+    return valueCount;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetCounts.java
new file mode 100644
index 0000000..c4e593d
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetCounts.java
@@ -0,0 +1,170 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IntsRef;
+
+/** Reads from any {@link OrdinalsReader}; use {@link
+ *  FastTaxonomyFacetCounts} if you are just using the
+ *  default encoding from {@link BinaryDocValues}. */
+
+// nocommit remove & add specialized Cached variation only?
+public class TaxonomyFacetCounts extends TaxonomyFacets {
+  private final OrdinalsReader ordinalsReader;
+  private final int[] counts;
+
+  public TaxonomyFacetCounts(OrdinalsReader ordinalsReader, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(ordinalsReader.getIndexFieldName(), taxoReader, config);
+    this.ordinalsReader = ordinalsReader;
+    counts = new int[taxoReader.getSize()];
+    count(fc.getMatchingDocs());
+  }
+
+  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
+    IntsRef scratch  = new IntsRef();
+    for(MatchingDocs hits : matchingDocs) {
+      OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        ords.get(doc, scratch);
+        for(int i=0;i<scratch.length;i++) {
+          counts[scratch.ints[scratch.offset+i]]++;
+        }
+        ++doc;
+      }
+    }
+
+    // nocommit we could do this lazily instead:
+
+    // Rollup any necessary dims:
+    for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
+      String dim = ent.getKey();
+      FacetsConfig.DimConfig ft = ent.getValue();
+      if (ft.hierarchical && ft.multiValued == false) {
+        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
+        // It can be -1 if this field was declared in the
+        // config but never indexed:
+        if (dimRootOrd > 0) {
+          counts[dimRootOrd] += rollup(children[dimRootOrd]);
+        }
+      }
+    }
+  }
+
+  private int rollup(int ord) {
+    int sum = 0;
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      int childValue = counts[ord] + rollup(children[ord]);
+      counts[ord] = childValue;
+      sum += childValue;
+      ord = siblings[ord];
+    }
+    return sum;
+  }
+
+  /** Return the count for a specific path.  Returns -1 if
+   *  this path doesn't exist, else the count. */
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    verifyDim(dim);
+    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
+    if (ord < 0) {
+      return -1;
+    }
+    return counts[ord];
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    FacetsConfig.DimConfig dimConfig = verifyDim(dim);
+    FacetLabel cp = FacetLabel.create(dim, path);
+    int dimOrd = taxoReader.getOrdinal(cp);
+    if (dimOrd == -1) {
+      //System.out.println("no ord for path=" + path);
+      return null;
+    }
+
+    TopOrdAndIntQueue q = new TopOrdAndIntQueue(Math.min(taxoReader.getSize(), topN));
+    
+    int bottomCount = 0;
+
+    int ord = children[dimOrd];
+    int totCount = 0;
+    int childCount = 0;
+
+    TopOrdAndIntQueue.OrdAndValue reuse = null;
+    while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      if (counts[ord] > 0) {
+        totCount += counts[ord];
+        childCount++;
+        if (counts[ord] > bottomCount) {
+          if (reuse == null) {
+            reuse = new TopOrdAndIntQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = counts[ord];
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomCount = q.top().value;
+          }
+        }
+      }
+
+      ord = siblings[ord];
+    }
+
+    if (totCount == 0) {
+      return null;
+    }
+
+    if (dimConfig.multiValued) {
+      if (dimConfig.requireDimCount) {
+        totCount = counts[dimOrd];
+      } else {
+        // Our sum'd count is not correct, in general:
+        totCount = -1;
+      }
+    } else {
+      // Our sum'd dim count is accurate, so we keep it
+    }
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
+      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
+      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
+    }
+
+    return new FacetResult(totCount, labelValues, childCount);
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumFloatAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumFloatAssociations.java
new file mode 100644
index 0000000..369c0a2
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumFloatAssociations.java
@@ -0,0 +1,149 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+// nocommit jdoc that this assumes/requires the default encoding
+public class TaxonomyFacetSumFloatAssociations extends TaxonomyFacets {
+  private final float[] values;
+
+  public TaxonomyFacetSumFloatAssociations(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
+  }
+
+  public TaxonomyFacetSumFloatAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    values = new float[taxoReader.getSize()];
+    sumValues(fc.getMatchingDocs());
+  }
+
+  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
+    for(MatchingDocs hits : matchingDocs) {
+      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
+      if (dv == null) { // this reader does not have DocValues for the requested category list
+        continue;
+      }
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      BytesRef scratch = new BytesRef();
+      //System.out.println("count seg=" + hits.context.reader());
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        //System.out.println("  doc=" + doc);
+        // nocommit use OrdinalsReader?  but, add a
+        // BytesRef getAssociation()?
+        dv.get(doc, scratch);
+        byte[] bytes = scratch.bytes;
+        int end = scratch.offset + scratch.length;
+        int offset = scratch.offset;
+        while (offset < end) {
+          int ord = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          int value = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          values[ord] += Float.intBitsToFloat(value);
+        }
+        ++doc;
+      }
+    }
+  }
+
+  /** Return the count for a specific path.  Returns -1 if
+   *  this path doesn't exist, else the count. */
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    verifyDim(dim);
+    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
+    if (ord < 0) {
+      return -1;
+    }
+    return values[ord];
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    FacetsConfig.DimConfig dimConfig = verifyDim(dim);
+    FacetLabel cp = FacetLabel.create(dim, path);
+    int dimOrd = taxoReader.getOrdinal(cp);
+    if (dimOrd == -1) {
+      //System.out.println("no ord for path=" + path);
+      return null;
+    }
+
+    TopOrdAndFloatQueue q = new TopOrdAndFloatQueue(Math.min(taxoReader.getSize(), topN));
+    float bottomValue = 0;
+
+    int ord = children[dimOrd];
+    float sumValue = 0;
+    int childCount = 0;
+    TopOrdAndFloatQueue.OrdAndValue reuse = null;
+    while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      if (values[ord] > 0) {
+        sumValue += values[ord];
+        childCount++;
+        if (values[ord] > bottomValue) {
+          if (reuse == null) {
+            reuse = new TopOrdAndFloatQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = values[ord];
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomValue = q.top().value;
+          }
+        }
+      }
+
+      ord = siblings[ord];
+    }
+
+    if (sumValue == 0) {
+      //System.out.println("totCount=0 for path=" + path);
+      return null;
+    }
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndFloatQueue.OrdAndValue ordAndValue = q.pop();
+      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
+      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
+    }
+
+    return new FacetResult(sumValue, labelValues, childCount);
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumIntAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumIntAssociations.java
new file mode 100644
index 0000000..aaaba78
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumIntAssociations.java
@@ -0,0 +1,150 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+
+// nocommit jdoc that this assumes/requires the default encoding
+public class TaxonomyFacetSumIntAssociations extends TaxonomyFacets {
+  private final int[] values;
+
+  public TaxonomyFacetSumIntAssociations(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
+  }
+
+  public TaxonomyFacetSumIntAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector fc) throws IOException {
+    super(indexFieldName, taxoReader, config);
+    values = new int[taxoReader.getSize()];
+    sumValues(fc.getMatchingDocs());
+  }
+
+  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
+    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
+    for(MatchingDocs hits : matchingDocs) {
+      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
+      if (dv == null) { // this reader does not have DocValues for the requested category list
+        continue;
+      }
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      BytesRef scratch = new BytesRef();
+      //System.out.println("count seg=" + hits.context.reader());
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        //System.out.println("  doc=" + doc);
+        // nocommit use OrdinalsReader?  but, add a
+        // BytesRef getAssociation()?
+        dv.get(doc, scratch);
+        byte[] bytes = scratch.bytes;
+        int end = scratch.offset + scratch.length;
+        int offset = scratch.offset;
+        while (offset < end) {
+          int ord = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          int value = ((bytes[offset]&0xFF) << 24) |
+            ((bytes[offset+1]&0xFF) << 16) |
+            ((bytes[offset+2]&0xFF) << 8) |
+            (bytes[offset+3]&0xFF);
+          offset += 4;
+          values[ord] += value;
+        }
+        ++doc;
+      }
+    }
+  }
+
+  /** Return the count for a specific path.  Returns -1 if
+   *  this path doesn't exist, else the count. */
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    verifyDim(dim);
+    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
+    if (ord < 0) {
+      return -1;
+    }
+    return values[ord];
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    verifyDim(dim);
+    FacetLabel cp = FacetLabel.create(dim, path);
+    int dimOrd = taxoReader.getOrdinal(cp);
+    if (dimOrd == -1) {
+      //System.out.println("no ord for path=" + path);
+      return null;
+    }
+
+    TopOrdAndIntQueue q = new TopOrdAndIntQueue(Math.min(taxoReader.getSize(), topN));
+    int bottomValue = 0;
+
+    int ord = children[dimOrd];
+    long sumValue = 0;
+    int childCount = 0;
+
+    TopOrdAndIntQueue.OrdAndValue reuse = null;
+    while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      if (values[ord] > 0) {
+        sumValue += values[ord];
+        childCount++;
+        if (values[ord] > bottomValue) {
+          if (reuse == null) {
+            reuse = new TopOrdAndIntQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = values[ord];
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomValue = q.top().value;
+          }
+        }
+      }
+
+      ord = siblings[ord];
+    }
+
+    if (sumValue == 0) {
+      //System.out.println("totCount=0 for path=" + path);
+      return null;
+    }
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
+      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
+      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
+    }
+
+    return new FacetResult(sumValue, labelValues, childCount);
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumValueSource.java b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumValueSource.java
new file mode 100644
index 0000000..c1e8a3c
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacetSumValueSource.java
@@ -0,0 +1,235 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.facet.FacetsCollector.MatchingDocs;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IntsRef;
+
+/** Aggregates sum of values from a {@link ValueSource}, for
+ *  each facet label. */
+
+public class TaxonomyFacetSumValueSource extends TaxonomyFacets {
+  private final float[] values;
+  private final OrdinalsReader ordinalsReader;
+
+  /** Aggreggates float facet values from the provided
+   *  {@link ValueSource}, pulling ordinals using {@link
+   *  DocValuesOrdinalsReader} against the default indexed
+   *  facet field {@link
+   *  FacetsConfig#DEFAULT_INDEX_FIELD_NAME}. */
+  public TaxonomyFacetSumValueSource(TaxonomyReader taxoReader, FacetsConfig config,
+                                     FacetsCollector fc, ValueSource valueSource) throws IOException {
+    this(new DocValuesOrdinalsReader(FacetsConfig.DEFAULT_INDEX_FIELD_NAME), taxoReader, config, fc, valueSource);
+  }
+
+  /** Aggreggates float facet values from the provided
+   *  {@link ValueSource}, and pulls ordinals from the
+   *  provided {@link OrdinalsReader}. */
+  public TaxonomyFacetSumValueSource(OrdinalsReader ordinalsReader, TaxonomyReader taxoReader,
+                                     FacetsConfig config, FacetsCollector fc, ValueSource valueSource) throws IOException {
+    super(ordinalsReader.getIndexFieldName(), taxoReader, config);
+    this.ordinalsReader = ordinalsReader;
+    values = new float[taxoReader.getSize()];
+    sumValues(fc.getMatchingDocs(), fc.getKeepScores(), valueSource);
+  }
+
+  private static final class FakeScorer extends Scorer {
+    float score;
+    int docID;
+    FakeScorer() { super(null); }
+    @Override public float score() throws IOException { return score; }
+    @Override public int freq() throws IOException { throw new UnsupportedOperationException(); }
+    @Override public int docID() { return docID; }
+    @Override public int nextDoc() throws IOException { throw new UnsupportedOperationException(); }
+    @Override public int advance(int target) throws IOException { throw new UnsupportedOperationException(); }
+    @Override public long cost() { return 0; }
+  }
+
+  private final void sumValues(List<MatchingDocs> matchingDocs, boolean keepScores, ValueSource valueSource) throws IOException {
+    final FakeScorer scorer = new FakeScorer();
+    Map<String, Scorer> context = new HashMap<String, Scorer>();
+    if (keepScores) {
+      context.put("scorer", scorer);
+    }
+    IntsRef scratch = new IntsRef();
+    for(MatchingDocs hits : matchingDocs) {
+      OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
+      FixedBitSet bits = hits.bits;
+    
+      final int length = hits.bits.length();
+      int doc = 0;
+      int scoresIdx = 0;
+      float[] scores = hits.scores;
+
+      FunctionValues functionValues = valueSource.getValues(context, hits.context);
+      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
+        ords.get(doc, scratch);
+        if (keepScores) {
+          scorer.docID = doc;
+          scorer.score = scores[scoresIdx++];
+        }
+        float value = (float) functionValues.doubleVal(doc);
+        for(int i=0;i<scratch.length;i++) {
+          values[scratch.ints[i]] += value;
+        }
+        ++doc;
+      }
+    }
+
+    // nocommit we could do this lazily instead:
+
+    // Rollup any necessary dims:
+    for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
+      String dim = ent.getKey();
+      FacetsConfig.DimConfig ft = ent.getValue();
+      if (ft.hierarchical && ft.multiValued == false) {
+        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
+        assert dimRootOrd > 0;
+        values[dimRootOrd] += rollup(children[dimRootOrd]);
+      }
+    }
+  }
+
+  private float rollup(int ord) {
+    float sum = 0;
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      float childValue = values[ord] + rollup(children[ord]);
+      values[ord] = childValue;
+      sum += childValue;
+      ord = siblings[ord];
+    }
+    return sum;
+  }
+
+  @Override
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    verifyDim(dim);
+    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
+    if (ord < 0) {
+      return -1;
+    }
+    return values[ord];
+  }
+
+  @Override
+  public FacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    if (topN <= 0) {
+      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
+    }
+    FacetsConfig.DimConfig dimConfig = verifyDim(dim);
+    FacetLabel cp = FacetLabel.create(dim, path);
+    int dimOrd = taxoReader.getOrdinal(cp);
+    if (dimOrd == -1) {
+      System.out.println("  no dim ord " + dim);
+      return null;
+    }
+
+    TopOrdAndFloatQueue q = new TopOrdAndFloatQueue(Math.min(taxoReader.getSize(), topN));
+    float bottomValue = 0;
+
+    int ord = children[dimOrd];
+    float sumValues = 0;
+    int childCount = 0;
+
+    TopOrdAndFloatQueue.OrdAndValue reuse = null;
+    while(ord != TaxonomyReader.INVALID_ORDINAL) {
+      if (values[ord] > 0) {
+        sumValues += values[ord];
+        childCount++;
+        if (values[ord] > bottomValue) {
+          if (reuse == null) {
+            reuse = new TopOrdAndFloatQueue.OrdAndValue();
+          }
+          reuse.ord = ord;
+          reuse.value = values[ord];
+          reuse = q.insertWithOverflow(reuse);
+          if (q.size() == topN) {
+            bottomValue = q.top().value;
+          }
+        }
+      }
+
+      ord = siblings[ord];
+    }
+
+    if (sumValues == 0) {
+      System.out.println("  no sum");
+      return null;
+    }
+
+    if (dimConfig.multiValued) {
+      if (dimConfig.requireDimCount) {
+        sumValues = values[dimOrd];
+      } else {
+        // Our sum'd count is not correct, in general:
+        sumValues = -1;
+      }
+    } else {
+      // Our sum'd dim count is accurate, so we keep it
+    }
+
+    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
+    for(int i=labelValues.length-1;i>=0;i--) {
+      TopOrdAndFloatQueue.OrdAndValue ordAndValue = q.pop();
+      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
+      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
+    }
+
+    return new FacetResult(sumValues, labelValues, childCount);
+  }
+
+  /** {@link ValueSource} that returns the score for each
+   *  hit; use this to aggregate the sum of all hit scores
+   *  for each facet label.  */
+  public static class ScoreValueSource extends ValueSource {
+    @Override
+    public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
+      final Scorer scorer = (Scorer) context.get("scorer");
+      if (scorer == null) {
+        throw new IllegalStateException("scores are missing; be sure to pass keepScores=true to FacetsCollector");
+      }
+      return new DoubleDocValues(this) {
+        @Override
+        public double doubleVal(int document) {
+          try {
+            return scorer.score();
+          } catch (IOException exception) {
+            throw new RuntimeException(exception);
+          }
+        }
+      };
+    }
+
+    @Override public boolean equals(Object o) { return o == this; }
+    @Override public int hashCode() { return System.identityHashCode(this); }
+    @Override public String description() { return "score()"; }
+    };
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacets.java
new file mode 100644
index 0000000..a03d413
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/TaxonomyFacets.java
@@ -0,0 +1,92 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/** Base class for all taxonomy-based facets impls. */
+abstract class TaxonomyFacets extends Facets {
+  protected final String indexFieldName;
+  protected final TaxonomyReader taxoReader;
+  protected final FacetsConfig config;
+  protected final int[] children;
+  protected final int[] parents;
+  protected final int[] siblings;
+
+  /** Sole parameter is the field name that holds the facet
+   *  counts. */
+  protected TaxonomyFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config) throws IOException {
+    this.indexFieldName = indexFieldName;
+    this.taxoReader = taxoReader;
+    this.config = config;
+    ParallelTaxonomyArrays pta = taxoReader.getParallelTaxonomyArrays();
+    children = pta.children();
+    parents = pta.parents();
+    siblings = pta.siblings();
+  }
+
+  protected FacetsConfig.DimConfig verifyDim(String dim) {
+    FacetsConfig.DimConfig dimConfig = config.getDimConfig(dim);
+    if (!dimConfig.indexFieldName.equals(indexFieldName)) {
+      // nocommit get test case to cover this:
+      throw new IllegalArgumentException("dimension \"" + dim + "\" was not indexed into field \"" + indexFieldName);
+    }
+    return dimConfig;
+  }
+
+  @Override
+  public List<FacetResult> getAllDims(int topN) throws IOException {
+    int ord = children[TaxonomyReader.ROOT_ORDINAL];
+    List<FacetResult> results = new ArrayList<FacetResult>();
+    while (ord != TaxonomyReader.INVALID_ORDINAL) {
+      String dim = taxoReader.getPath(ord).components[0];
+      FacetsConfig.DimConfig dimConfig = config.getDimConfig(dim);
+      if (dimConfig.indexFieldName.equals(indexFieldName)) {
+        FacetResult result = getTopChildren(topN, dim);
+        if (result != null) {
+          results.add(result);
+        }
+      }
+      ord = siblings[ord];
+    }
+
+    // Sort by highest value, tie break by value:
+    Collections.sort(results,
+                     new Comparator<FacetResult>() {
+                       @Override
+                       public int compare(FacetResult a, FacetResult b) {
+                         if (a.value.doubleValue() > b.value.doubleValue()) {
+                           return -1;
+                         } else if (b.value.doubleValue() > a.value.doubleValue()) {
+                           return 1;
+                         } else {
+                           return 0;
+                         }
+                       }
+                     });
+
+    return results;
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndFloatQueue.java b/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndFloatQueue.java
new file mode 100644
index 0000000..9514c06
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndFloatQueue.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.PriorityQueue;
+
+// nocommit make value a double and merge with TopOrdCountValueQueue?
+
+/** Keeps highest results, first by largest float value,
+ *  then tie break by smallest ord. */
+class TopOrdAndFloatQueue extends PriorityQueue<TopOrdAndFloatQueue.OrdAndValue> {
+
+  public static final class OrdAndValue {
+    int ord;
+    float value;
+  }
+
+  public TopOrdAndFloatQueue(int topN) {
+    super(topN, false);
+  }
+
+  @Override
+  protected boolean lessThan(OrdAndValue a, OrdAndValue b) {
+    if (a.value < b.value) {
+      return true;
+    } else if (a.value > b.value) {
+      return false;
+    } else {
+      return a.ord > b.ord;
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndIntQueue.java b/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndIntQueue.java
new file mode 100644
index 0000000..f1c8cde
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/TopOrdAndIntQueue.java
@@ -0,0 +1,45 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.PriorityQueue;
+
+/** Keeps highest results, first by largest int value,
+ *  then tie break by smallest ord. */
+class TopOrdAndIntQueue extends PriorityQueue<TopOrdAndIntQueue.OrdAndValue> {
+
+  public static final class OrdAndValue {
+    int ord;
+    int value;
+  }
+
+  public TopOrdAndIntQueue(int topN) {
+    super(topN, false);
+  }
+
+  @Override
+  protected boolean lessThan(OrdAndValue a, OrdAndValue b) {
+    if (a.value < b.value) {
+      return true;
+    } else if (a.value > b.value) {
+      return false;
+    } else {
+      return a.ord > b.ord;
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/AssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/AssociationFacetField.java
deleted file mode 100644
index 376c09e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/AssociationFacetField.java
+++ /dev/null
@@ -1,62 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.util.BytesRef;
-
-/** Associates an arbitrary byte[] with the added facet
- *  path. */
-public class AssociationFacetField extends Field {
-  static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setIndexed(true);
-    TYPE.freeze();
-  }
-  protected final String dim;
-  protected final String[] path;
-  protected final BytesRef assoc;
-
-  public AssociationFacetField(BytesRef assoc, String dim, String... path) {
-    super("dummy", TYPE);
-    this.dim = dim;
-    this.assoc = assoc;
-    if (path.length == 0) {
-      throw new IllegalArgumentException("path must have at least one element");
-    }
-    this.path = path;
-  }
-
-  private static BytesRef intToBytesRef(int v) {
-    byte[] bytes = new byte[4];
-    // big-endian:
-    bytes[0] = (byte) (v >> 24);
-    bytes[1] = (byte) (v >> 16);
-    bytes[2] = (byte) (v >> 8);
-    bytes[3] = (byte) v;
-    return new BytesRef(bytes);
-  }
-
-  @Override
-  public String toString() {
-    return "AssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " bytes=" + assoc + ")";
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/CachedOrdinalsReader.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/CachedOrdinalsReader.java
deleted file mode 100644
index 7c8db30..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/CachedOrdinalsReader.java
+++ /dev/null
@@ -1,147 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.WeakHashMap;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/**
- * A per-segment cache of documents' facet ordinals. Every
- * {@link CachedOrds} holds the ordinals in a raw {@code
- * int[]}, and therefore consumes as much RAM as the total
- * number of ordinals found in the segment, but saves the
- * CPU cost of decoding ordinals during facet counting.
- * 
- * <p>
- * <b>NOTE:</b> every {@link CachedOrds} is limited to 2.1B
- * total ordinals. If that is a limitation for you then
- * consider limiting the segment size to fewer documents, or
- * use an alternative cache which pages through the category
- * ordinals.
- * 
- * <p>
- * <b>NOTE:</b> when using this cache, it is advised to use
- * a {@link DocValuesFormat} that does not cache the data in
- * memory, at least for the category lists fields, or
- * otherwise you'll be doing double-caching.
- *
- * <p>
- * <b>NOTE:</b> create one instance of this and re-use it
- * for all facet implementations (the cache is per-instance,
- * not static).
- */
-public class CachedOrdinalsReader extends OrdinalsReader {
-
-  private final OrdinalsReader source;
-
-  private final Map<Object,CachedOrds> ordsCache = new WeakHashMap<Object,CachedOrds>();
-
-  public CachedOrdinalsReader(OrdinalsReader source) {
-    this.source = source;
-  }
-
-  private synchronized CachedOrds getCachedOrds(AtomicReaderContext context) throws IOException {
-    Object cacheKey = context.reader().getCoreCacheKey();
-    CachedOrds ords = ordsCache.get(cacheKey);
-    if (ords == null) {
-      ords = new CachedOrds(source.getReader(context), context.reader().maxDoc());
-      ordsCache.put(cacheKey, ords);
-    }
-
-    return ords;
-  }
-
-  @Override
-  public String getIndexFieldName() {
-    return source.getIndexFieldName();
-  }
-
-  @Override
-  public OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException {
-    final CachedOrds cachedOrds = getCachedOrds(context);
-    return new OrdinalsSegmentReader() {
-      @Override
-      public void get(int docID, IntsRef ordinals) {
-        ordinals.ints = cachedOrds.ordinals;
-        ordinals.offset = cachedOrds.offsets[docID];
-        ordinals.length = cachedOrds.offsets[docID+1] - ordinals.offset;
-      }
-    };
-  }
-
-  /** Holds the cached ordinals in two paralel {@code int[]} arrays. */
-  public static final class CachedOrds {
-    
-    public final int[] offsets;
-    public final int[] ordinals;
-
-    /**
-     * Creates a new {@link CachedOrds} from the {@link BinaryDocValues}.
-     * Assumes that the {@link BinaryDocValues} is not {@code null}.
-     */
-    public CachedOrds(OrdinalsSegmentReader source, int maxDoc) throws IOException {
-      final BytesRef buf = new BytesRef();
-
-      offsets = new int[maxDoc + 1];
-      int[] ords = new int[maxDoc]; // let's assume one ordinal per-document as an initial size
-
-      // this aggregator is limited to Integer.MAX_VALUE total ordinals.
-      long totOrds = 0;
-      final IntsRef values = new IntsRef(32);
-      for (int docID = 0; docID < maxDoc; docID++) {
-        offsets[docID] = (int) totOrds;
-        source.get(docID, values);
-        long nextLength = totOrds + values.length;
-        if (nextLength > ords.length) {
-          if (nextLength > ArrayUtil.MAX_ARRAY_LENGTH) {
-            throw new IllegalStateException("too many ordinals (>= " + nextLength + ") to cache");
-          }
-          ords = ArrayUtil.grow(ords, (int) nextLength);
-        }
-        System.arraycopy(values.ints, 0, ords, (int) totOrds, values.length);
-        totOrds = nextLength;
-      }
-      offsets[maxDoc] = (int) totOrds;
-      
-      // if ords array is bigger by more than 10% of what we really need, shrink it
-      if ((double) totOrds / ords.length < 0.9) { 
-        this.ordinals = new int[(int) totOrds];
-        System.arraycopy(ords, 0, this.ordinals, 0, (int) totOrds);
-      } else {
-        this.ordinals = ords;
-      }
-    }
-  }
-
-  /** How many bytes is this cache using? */
-  public synchronized long ramBytesUsed() {
-    long bytes = 0;
-    for(CachedOrds ords : ordsCache.values()) {
-      bytes += RamUsageEstimator.sizeOf(ords);
-    }
-
-    return bytes;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/DocValuesOrdinalsReader.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/DocValuesOrdinalsReader.java
deleted file mode 100644
index 07d1ac6..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/DocValuesOrdinalsReader.java
+++ /dev/null
@@ -1,97 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/** Decodes ordinals previously indexed into a BinaryDocValues field */
-
-public class DocValuesOrdinalsReader extends OrdinalsReader {
-  private final String field;
-
-  public DocValuesOrdinalsReader() {
-    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME);
-  }
-
-  public DocValuesOrdinalsReader(String field) {
-    this.field = field;
-  }
-
-  @Override
-  public OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException {
-    BinaryDocValues values0 = context.reader().getBinaryDocValues(field);
-    if (values0 == null) {
-      values0 = BinaryDocValues.EMPTY;
-    }
-
-    final BinaryDocValues values = values0;
-
-    return new OrdinalsSegmentReader() {
-      private final BytesRef bytes = new BytesRef(32);
-
-      @Override
-      public void get(int docID, IntsRef ordinals) throws IOException {
-        values.get(docID, bytes);
-        decode(bytes, ordinals);
-      }
-    };
-  }
-
-  @Override
-  public String getIndexFieldName() {
-    return field;
-  }
-
-  /** Subclass & override if you change the encoding. */
-  protected void decode(BytesRef buf, IntsRef ordinals) {
-
-    // grow the buffer up front, even if by a large number of values (buf.length)
-    // that saves the need to check inside the loop for every decoded value if
-    // the buffer needs to grow.
-    if (ordinals.ints.length < buf.length) {
-      ordinals.ints = ArrayUtil.grow(ordinals.ints, buf.length);
-    }
-
-    ordinals.offset = 0;
-    ordinals.length = 0;
-
-    // it is better if the decoding is inlined like so, and not e.g.
-    // in a utility method
-    int upto = buf.offset + buf.length;
-    int value = 0;
-    int offset = buf.offset;
-    int prev = 0;
-    while (offset < upto) {
-      byte b = buf.bytes[offset++];
-      if (b >= 0) {
-        ordinals.ints[ordinals.length] = ((value << 7) | b) + prev;
-        value = 0;
-        prev = ordinals.ints[ordinals.length];
-        ordinals.length++;
-      } else {
-        value = (value << 7) | (b & 0x7F);
-      }
-    }
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/DoubleRange.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/DoubleRange.java
deleted file mode 100644
index 5fbea85..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/DoubleRange.java
+++ /dev/null
@@ -1,71 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.DoubleDocValuesField; // javadocs
-
-/** Represents a range over double values indexed as {@link
- *  DoubleDocValuesField}.  */
-public final class DoubleRange extends Range {
-  private final double minIncl;
-  private final double maxIncl;
-
-  public final double min;
-  public final double max;
-  public final boolean minInclusive;
-  public final boolean maxInclusive;
-
-  /** Create a DoubleRange. */
-  public DoubleRange(String label, double min, boolean minInclusive, double max, boolean maxInclusive) {
-    super(label);
-    this.min = min;
-    this.max = max;
-    this.minInclusive = minInclusive;
-    this.maxInclusive = maxInclusive;
-
-    // TODO: if DoubleDocValuesField used
-    // NumericUtils.doubleToSortableLong format (instead of
-    // Double.doubleToRawLongBits) we could do comparisons
-    // in long space 
-
-    if (Double.isNaN(min)) {
-      throw new IllegalArgumentException("min cannot be NaN");
-    }
-    if (!minInclusive) {
-      min = Math.nextUp(min);
-    }
-
-    if (Double.isNaN(max)) {
-      throw new IllegalArgumentException("max cannot be NaN");
-    }
-    if (!maxInclusive) {
-      // Why no Math.nextDown?
-      max = Math.nextAfter(max, Double.NEGATIVE_INFINITY);
-    }
-
-    this.minIncl = min;
-    this.maxIncl = max;
-  }
-
-  @Override
-  public boolean accept(long value) {
-    double doubleValue = Double.longBitsToDouble(value);
-    return doubleValue >= minIncl && doubleValue <= maxIncl;
-  }
-}
-
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/FacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/FacetField.java
deleted file mode 100644
index f41a79d..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/FacetField.java
+++ /dev/null
@@ -1,49 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-
-/** Add an instance of this to your Document for every facet
- *  label. */
-public class FacetField extends Field {
-  static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setIndexed(true);
-    TYPE.freeze();
-  }
-  public final String dim;
-  public final String[] path;
-
-  public FacetField(String dim, String... path) {
-    super("dummy", TYPE);
-    this.dim = dim;
-    if (path.length == 0) {
-      throw new IllegalArgumentException("path must have at least one element");
-    }
-    this.path = path;
-  }
-
-  @Override
-  public String toString() {
-    return "FacetField(dim=" + dim + " path=" + Arrays.toString(path) + ")";
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/Facets.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/Facets.java
deleted file mode 100644
index f6f5fb7..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/Facets.java
+++ /dev/null
@@ -1,121 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.FilteredQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MultiCollector;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.TopFieldCollector;
-import org.apache.lucene.search.TopFieldDocs;
-import org.apache.lucene.search.TopScoreDocCollector;
-
-public abstract class Facets {
-  /** Returns the topN child labels under the specified
-   *  path.  Returns null if the specified path doesn't
-   *  exist. */
-  public abstract SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException;
-
-  /** Return the count for a specific path.  Returns -1 if
-   *  this path doesn't exist, else the count. */
-  public abstract Number getSpecificValue(String dim, String... path) throws IOException;
-
-  /** Returns topN labels for any dimension that had hits,
-   *  sorted by the number of hits that dimension matched;
-   *  this is used for "sparse" faceting, where many
-   *  different dimensions were indexed depending on the
-   *  type of document. */
-  public abstract List<SimpleFacetResult> getAllDims(int topN) throws IOException;
-
-  // nocommit where to move?
-
-  /** Utility method, to search for top hits by score
-   *  ({@link IndexSearcher#search(Query,int)}), but
-   *  also collect results into a {@link
-   *  SimpleFacetsCollector} for faceting. */
-  public static TopDocs search(IndexSearcher searcher, Query q, int topN, SimpleFacetsCollector sfc) throws IOException {
-    // nocommit can we pass the "right" boolean for
-    // in-order...?  we'd need access to the protected
-    // IS.search methods taking Weight... could use
-    // reflection...
-    TopScoreDocCollector hitsCollector = TopScoreDocCollector.create(topN, false);
-    searcher.search(q, MultiCollector.wrap(hitsCollector, sfc));
-    return hitsCollector.topDocs();
-  }
-
-  // nocommit where to move?
-
-  /** Utility method, to search for top hits by score with a filter
-   *  ({@link IndexSearcher#search(Query,Filter,int)}), but
-   *  also collect results into a {@link
-   *  SimpleFacetsCollector} for faceting. */
-  public static TopDocs search(IndexSearcher searcher, Query q, Filter filter, int topN, SimpleFacetsCollector sfc) throws IOException {
-    if (filter != null) {
-      q = new FilteredQuery(q, filter);
-    }
-    return search(searcher, q, topN, sfc);
-  }
-
-  // nocommit where to move?
-
-  /** Utility method, to search for top hits by a custom
-   *  {@link Sort} with a filter
-   *  ({@link IndexSearcher#search(Query,Filter,int,Sort)}), but
-   *  also collect results into a {@link
-   *  SimpleFacetsCollector} for faceting. */
-  public static TopFieldDocs search(IndexSearcher searcher, Query q, Filter filter, int topN, Sort sort, SimpleFacetsCollector sfc) throws IOException {
-    return search(searcher, q, filter, topN, sort, false, false, sfc);
-  }
-
-  // nocommit where to move?
-
-  /** Utility method, to search for top hits by a custom
-   *  {@link Sort} with a filter
-   *  ({@link IndexSearcher#search(Query,Filter,int,Sort,boolean,boolean)}), but
-   *  also collect results into a {@link
-   *  SimpleFacetsCollector} for faceting. */
-  public static TopFieldDocs search(IndexSearcher searcher, Query q, Filter filter, int topN, Sort sort, boolean doDocScores, boolean doMaxScore, SimpleFacetsCollector sfc) throws IOException {
-    int limit = searcher.getIndexReader().maxDoc();
-    if (limit == 0) {
-      limit = 1;
-    }
-    topN = Math.min(topN, limit);
-
-    boolean fillFields = true;
-    TopFieldCollector hitsCollector = TopFieldCollector.create(sort, topN,
-                                                               null,
-                                                               fillFields,
-                                                               doDocScores,
-                                                               doMaxScore,
-                                                               false);
-    if (filter != null) {
-      q = new FilteredQuery(q, filter);
-    }
-    searcher.search(q, MultiCollector.wrap(hitsCollector, sfc));
-    return (TopFieldDocs) hitsCollector.topDocs();
-  }
-
-  // nocommit need searchAfter variants too
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/FacetsConfig.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/FacetsConfig.java
deleted file mode 100644
index 5f72a90..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/FacetsConfig.java
+++ /dev/null
@@ -1,517 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.ConcurrentHashMap;
-
-import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.SortedSetDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.index.IndexDocument;
-import org.apache.lucene.index.IndexableField;
-import org.apache.lucene.index.IndexableFieldType;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/** By default a dimension is flat, single valued and does
- *  not require count for the dimension; use
- *  the setters in this class to change these settings for
- *  any dims.
- *
- *  <p><b>NOTE</b>: this configuration is not saved into the
- *  index, but it's vital, and up to the application to
- *  ensure, that at search time the provided FacetsConfig
- *  matches what was used during indexing.
- *
- *  @lucene.experimental */
-public class FacetsConfig {
-
-  public static final String DEFAULT_INDEX_FIELD_NAME = "$facets";
-
-  private final Map<String,DimConfig> fieldTypes = new ConcurrentHashMap<String,DimConfig>();
-
-  // Used only for best-effort detection of app mixing
-  // int/float/bytes in a single indexed field:
-  private final Map<String,String> assocDimTypes = new ConcurrentHashMap<String,String>();
-
-  private final TaxonomyWriter taxoWriter;
-
-  /** @lucene.internal */
-  // nocommit expose this to the user, vs the setters?
-  public static final class DimConfig {
-    /** True if this dimension is hierarchical. */
-    boolean hierarchical;
-
-    /** True if this dimension is multi-valued. */
-    boolean multiValued;
-
-    /** True if the count/aggregate for the entire dimension
-     *  is required, which is unusual (default is false). */
-    boolean requireDimCount;
-
-    /** Actual field where this dimension's facet labels
-     *  should be indexed */
-    String indexFieldName = DEFAULT_INDEX_FIELD_NAME;
-  }
-
-  public FacetsConfig() {
-    this(null);
-  }
-
-  public FacetsConfig(TaxonomyWriter taxoWriter) {
-    this.taxoWriter = taxoWriter;
-  }
-
-  public final static DimConfig DEFAULT_DIM_CONFIG = new DimConfig();
-
-  public DimConfig getDimConfig(String dimName) {
-    DimConfig ft = fieldTypes.get(dimName);
-    if (ft == null) {
-      ft = DEFAULT_DIM_CONFIG;
-    }
-    return ft;
-  }
-
-  // nocommit maybe setDimConfig instead?
-  public synchronized void setHierarchical(String dimName, boolean v) {
-    DimConfig ft = fieldTypes.get(dimName);
-    if (ft == null) {
-      ft = new DimConfig();
-      fieldTypes.put(dimName, ft);
-    }
-    ft.hierarchical = v;
-  }
-
-  public synchronized void setMultiValued(String dimName, boolean v) {
-    DimConfig ft = fieldTypes.get(dimName);
-    if (ft == null) {
-      ft = new DimConfig();
-      fieldTypes.put(dimName, ft);
-    }
-    ft.multiValued = v;
-  }
-
-  public synchronized void setRequireDimCount(String dimName, boolean v) {
-    DimConfig ft = fieldTypes.get(dimName);
-    if (ft == null) {
-      ft = new DimConfig();
-      fieldTypes.put(dimName, ft);
-    }
-    ft.requireDimCount = v;
-  }
-
-  public synchronized void setIndexFieldName(String dimName, String indexFieldName) {
-    DimConfig ft = fieldTypes.get(dimName);
-    if (ft == null) {
-      ft = new DimConfig();
-      fieldTypes.put(dimName, ft);
-    }
-    ft.indexFieldName = indexFieldName;
-  }
-
-  Map<String,DimConfig> getDimConfigs() {
-    return fieldTypes;
-  }
-
-  private static void checkSeen(Set<String> seenDims, String dim) {
-    if (seenDims.contains(dim)) {
-      throw new IllegalArgumentException("dimension \"" + dim + "\" is not multiValued, but it appears more than once in this document");
-    }
-    seenDims.add(dim);
-  }
-
-  /** Translates any added {@link FacetField}s into normal
-   *  fields for indexing */
-  public IndexDocument build(IndexDocument doc) throws IOException {
-    // Find all FacetFields, collated by the actual field:
-    Map<String,List<FacetField>> byField = new HashMap<String,List<FacetField>>();
-
-    // ... and also all SortedSetDocValuesFacetFields:
-    Map<String,List<SortedSetDocValuesFacetField>> dvByField = new HashMap<String,List<SortedSetDocValuesFacetField>>();
-
-    // ... and also all AssociationFacetFields
-    Map<String,List<AssociationFacetField>> assocByField = new HashMap<String,List<AssociationFacetField>>();
-
-    Set<String> seenDims = new HashSet<String>();
-
-    for(IndexableField field : doc.indexableFields()) {
-      if (field.fieldType() == FacetField.TYPE) {
-        FacetField facetField = (FacetField) field;
-        FacetsConfig.DimConfig dimConfig = getDimConfig(facetField.dim);
-        if (dimConfig.multiValued == false) {
-          checkSeen(seenDims, facetField.dim);
-        }
-        String indexFieldName = dimConfig.indexFieldName;
-        List<FacetField> fields = byField.get(indexFieldName);
-        if (fields == null) {
-          fields = new ArrayList<FacetField>();
-          byField.put(indexFieldName, fields);
-        }
-        fields.add(facetField);
-      }
-
-      if (field.fieldType() == SortedSetDocValuesFacetField.TYPE) {
-        SortedSetDocValuesFacetField facetField = (SortedSetDocValuesFacetField) field;
-        FacetsConfig.DimConfig dimConfig = getDimConfig(facetField.dim);
-        if (dimConfig.multiValued == false) {
-          checkSeen(seenDims, facetField.dim);
-        }
-        String indexFieldName = dimConfig.indexFieldName;
-        List<SortedSetDocValuesFacetField> fields = dvByField.get(indexFieldName);
-        if (fields == null) {
-          fields = new ArrayList<SortedSetDocValuesFacetField>();
-          dvByField.put(indexFieldName, fields);
-        }
-        fields.add(facetField);
-      }
-
-      if (field.fieldType() == AssociationFacetField.TYPE) {
-        AssociationFacetField facetField = (AssociationFacetField) field;
-        FacetsConfig.DimConfig dimConfig = getDimConfig(facetField.dim);
-        if (dimConfig.multiValued == false) {
-          checkSeen(seenDims, facetField.dim);
-        }
-        if (dimConfig.hierarchical) {
-          throw new IllegalArgumentException("AssociationFacetField cannot be hierarchical (dim=\"" + facetField.dim + "\")");
-        }
-        if (dimConfig.requireDimCount) {
-          throw new IllegalArgumentException("AssociationFacetField cannot requireDimCount (dim=\"" + facetField.dim + "\")");
-        }
-
-        String indexFieldName = dimConfig.indexFieldName;
-        List<AssociationFacetField> fields = assocByField.get(indexFieldName);
-        if (fields == null) {
-          fields = new ArrayList<AssociationFacetField>();
-          assocByField.put(indexFieldName, fields);
-        }
-        fields.add(facetField);
-
-        // Best effort: detect mis-matched types in same
-        // indexed field:
-        String type;
-        if (facetField instanceof IntAssociationFacetField) {
-          type = "int";
-        } else if (facetField instanceof FloatAssociationFacetField) {
-          type = "float";
-        } else {
-          type = "bytes";
-        }
-        // NOTE: not thread safe, but this is just best effort:
-        String curType = assocDimTypes.get(indexFieldName);
-        if (curType == null) {
-          assocDimTypes.put(indexFieldName, type);
-        } else if (!curType.equals(type)) {
-          throw new IllegalArgumentException("mixing incompatible types of AssocationFacetField (" + curType + " and " + type + ") in indexed field \"" + indexFieldName + "\"; use FacetsConfig to change the indexFieldName for each dimension");
-        }
-      }
-    }
-
-    List<Field> addedIndexedFields = new ArrayList<Field>();
-    List<Field> addedStoredFields = new ArrayList<Field>();
-
-    processFacetFields(byField, addedIndexedFields, addedStoredFields);
-    processSSDVFacetFields(dvByField, addedIndexedFields, addedStoredFields);
-    processAssocFacetFields(assocByField, addedIndexedFields, addedStoredFields);
-
-    //System.out.println("add stored: " + addedStoredFields);
-
-    final List<IndexableField> allIndexedFields = new ArrayList<IndexableField>();
-    for(IndexableField field : doc.indexableFields()) {
-      IndexableFieldType ft = field.fieldType();
-      if (ft != FacetField.TYPE && ft != SortedSetDocValuesFacetField.TYPE && ft != AssociationFacetField.TYPE) {
-        allIndexedFields.add(field);
-      }
-    }
-    allIndexedFields.addAll(addedIndexedFields);
-
-    final List<StorableField> allStoredFields = new ArrayList<StorableField>();
-    for(StorableField field : doc.storableFields()) {
-      allStoredFields.add(field);
-    }
-    allStoredFields.addAll(addedStoredFields);
-
-    //System.out.println("all indexed: " + allIndexedFields);
-    //System.out.println("all stored: " + allStoredFields);
-
-    return new IndexDocument() {
-        @Override
-        public Iterable<IndexableField> indexableFields() {
-          return allIndexedFields;
-        }
-
-        @Override
-        public Iterable<StorableField> storableFields() {
-          return allStoredFields;
-        }
-      };
-  }
-
-  private void processFacetFields(Map<String,List<FacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
-
-    for(Map.Entry<String,List<FacetField>> ent : byField.entrySet()) {
-
-      String indexFieldName = ent.getKey();
-      //System.out.println("  fields=" + ent.getValue());
-
-      IntsRef ordinals = new IntsRef(32);
-      for(FacetField facetField : ent.getValue()) {
-
-        FacetsConfig.DimConfig ft = getDimConfig(facetField.dim);
-        if (facetField.path.length > 1 && ft.hierarchical == false) {
-          throw new IllegalArgumentException("dimension \"" + facetField.dim + "\" is not hierarchical yet has " + facetField.path.length + " components");
-        }
-      
-        FacetLabel cp = FacetLabel.create(facetField.dim, facetField.path);
-
-        checkTaxoWriter();
-        int ordinal = taxoWriter.addCategory(cp);
-        if (ordinals.length == ordinals.ints.length) {
-          ordinals.grow(ordinals.length+1);
-        }
-        ordinals.ints[ordinals.length++] = ordinal;
-        //System.out.println("  add cp=" + cp);
-
-        if (ft.multiValued && (ft.hierarchical || ft.requireDimCount)) {
-          // Add all parents too:
-          int parent = taxoWriter.getParent(ordinal);
-          while (parent > 0) {
-            if (ordinals.ints.length == ordinals.length) {
-              ordinals.grow(ordinals.length+1);
-            }
-            ordinals.ints[ordinals.length++] = parent;
-            parent = taxoWriter.getParent(parent);
-          }
-
-          if (ft.requireDimCount == false) {
-            // Remove last (dimension) ord:
-            ordinals.length--;
-          }
-        }
-
-        // Drill down:
-        for(int i=1;i<=cp.length;i++) {
-          addedIndexedFields.add(new StringField(indexFieldName, pathToString(cp.components, i), Field.Store.NO));
-        }
-      }
-
-      // Facet counts:
-      // DocValues are considered stored fields:
-      addedStoredFields.add(new BinaryDocValuesField(indexFieldName, dedupAndEncode(ordinals)));
-    }
-  }
-
-  private void processSSDVFacetFields(Map<String,List<SortedSetDocValuesFacetField>> byField, List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
-    //System.out.println("process SSDV: " + byField);
-    for(Map.Entry<String,List<SortedSetDocValuesFacetField>> ent : byField.entrySet()) {
-
-      String indexFieldName = ent.getKey();
-      //System.out.println("  field=" + indexFieldName);
-
-      for(SortedSetDocValuesFacetField facetField : ent.getValue()) {
-        FacetLabel cp = new FacetLabel(facetField.dim, facetField.label);
-        String fullPath = pathToString(cp.components, cp.length);
-        //System.out.println("add " + fullPath);
-
-        // For facet counts:
-        addedStoredFields.add(new SortedSetDocValuesField(indexFieldName, new BytesRef(fullPath)));
-
-        // For drill-down:
-        addedIndexedFields.add(new StringField(indexFieldName, fullPath, Field.Store.NO));
-        addedIndexedFields.add(new StringField(indexFieldName, facetField.dim, Field.Store.NO));
-      }
-    }
-  }
-
-  private void processAssocFacetFields(Map<String,List<AssociationFacetField>> byField,
-                                       List<Field> addedIndexedFields, List<Field> addedStoredFields) throws IOException {
-    for(Map.Entry<String,List<AssociationFacetField>> ent : byField.entrySet()) {
-      byte[] bytes = new byte[16];
-      int upto = 0;
-      String indexFieldName = ent.getKey();
-      for(AssociationFacetField field : ent.getValue()) {
-        // NOTE: we don't add parents for associations
-        // nocommit is that right?  maybe we are supposed to
-        // add to taxo writer, and just not index the parent
-        // ords?
-        checkTaxoWriter();
-        int ordinal = taxoWriter.addCategory(FacetLabel.create(field.dim, field.path));
-        if (upto + 4 > bytes.length) {
-          bytes = ArrayUtil.grow(bytes, upto+4);
-        }
-        // big-endian:
-        bytes[upto++] = (byte) (ordinal >> 24);
-        bytes[upto++] = (byte) (ordinal >> 16);
-        bytes[upto++] = (byte) (ordinal >> 8);
-        bytes[upto++] = (byte) ordinal;
-        if (upto + field.assoc.length > bytes.length) {
-          bytes = ArrayUtil.grow(bytes, upto+field.assoc.length);
-        }
-        System.arraycopy(field.assoc.bytes, field.assoc.offset, bytes, upto, field.assoc.length);
-        upto += field.assoc.length;
-      }
-      addedStoredFields.add(new BinaryDocValuesField(indexFieldName, new BytesRef(bytes, 0, upto)));
-    }
-  }
-
-  /** Encodes ordinals into a BytesRef; expert: subclass can
-   *  override this to change encoding. */
-  protected BytesRef dedupAndEncode(IntsRef ordinals) {
-    Arrays.sort(ordinals.ints, ordinals.offset, ordinals.length);
-    byte[] bytes = new byte[5*ordinals.length];
-    int lastOrd = -1;
-    int upto = 0;
-    for(int i=0;i<ordinals.length;i++) {
-      int ord = ordinals.ints[ordinals.offset+i];
-      // ord could be == lastOrd, so we must dedup:
-      if (ord > lastOrd) {
-        int delta;
-        if (lastOrd == -1) {
-          delta = ord;
-        } else {
-          delta = ord - lastOrd;
-        }
-        if ((delta & ~0x7F) == 0) {
-          bytes[upto] = (byte) delta;
-          upto++;
-        } else if ((delta & ~0x3FFF) == 0) {
-          bytes[upto] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
-          bytes[upto + 1] = (byte) (delta & 0x7F);
-          upto += 2;
-        } else if ((delta & ~0x1FFFFF) == 0) {
-          bytes[upto] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
-          bytes[upto + 1] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
-          bytes[upto + 2] = (byte) (delta & 0x7F);
-          upto += 3;
-        } else if ((delta & ~0xFFFFFFF) == 0) {
-          bytes[upto] = (byte) (0x80 | ((delta & 0xFE00000) >> 21));
-          bytes[upto + 1] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
-          bytes[upto + 2] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
-          bytes[upto + 3] = (byte) (delta & 0x7F);
-          upto += 4;
-        } else {
-          bytes[upto] = (byte) (0x80 | ((delta & 0xF0000000) >> 28));
-          bytes[upto + 1] = (byte) (0x80 | ((delta & 0xFE00000) >> 21));
-          bytes[upto + 2] = (byte) (0x80 | ((delta & 0x1FC000) >> 14));
-          bytes[upto + 3] = (byte) (0x80 | ((delta & 0x3F80) >> 7));
-          bytes[upto + 4] = (byte) (delta & 0x7F);
-          upto += 5;
-        }
-        lastOrd = ord;
-      }
-    }
-    return new BytesRef(bytes, 0, upto);
-  }
-
-  private void checkTaxoWriter() {
-    if (taxoWriter == null) {
-      throw new IllegalStateException("a valid TaxonomyWriter must be provided to the constructor (got null), when using FacetField or AssociationFacetField");
-    }
-  }
-
-  // Joins the path components together:
-  private static final char DELIM_CHAR = '\u001F';
-
-  // Escapes any occurrence of the path component inside the label:
-  private static final char ESCAPE_CHAR = '\u001E';
-
-  /** Turns a path into a string without stealing any
-   *  characters. */
-  public static String pathToString(String dim, String[] path) {
-    String[] fullPath = new String[1+path.length];
-    fullPath[0] = dim;
-    System.arraycopy(path, 0, fullPath, 1, path.length);
-    return pathToString(fullPath, fullPath.length);
-  }
-
-  public static String pathToString(String[] path) {
-    return pathToString(path, path.length);
-  }
-
-  public static String pathToString(String[] path, int length) {
-    // nocommit .... too anal?  shouldn't we allow drill
-    // down on just dim, to get all docs that have that
-    // dim...?
-    /*
-    if (path.length < 2) {
-      throw new IllegalArgumentException("path length must be > 0 (dim=" + path[0] + ")");
-    }
-    */
-    if (length == 0) {
-      return "";
-    }
-    StringBuilder sb = new StringBuilder();
-    for(int i=0;i<length;i++) {
-      String s = path[i];
-      int numChars = s.length();
-      for(int j=0;j<numChars;j++) {
-        char ch = s.charAt(j);
-        if (ch == DELIM_CHAR || ch == ESCAPE_CHAR) {
-          sb.append(ESCAPE_CHAR);
-        }
-        sb.append(ch);
-      }
-      sb.append(DELIM_CHAR);
-    }
-
-    // Trim off last DELIM_CHAR:
-    sb.setLength(sb.length()-1);
-    return sb.toString();
-  }
-
-  /** Turns a result from previous call to {@link
-   *  #pathToString} back into the original {@code String[]}
-   *  without stealing any characters. */
-  public static String[] stringToPath(String s) {
-    List<String> parts = new ArrayList<String>();
-    int length = s.length();
-    char[] buffer = new char[length];
-
-    int upto = 0;
-    boolean lastEscape = false;
-    for(int i=0;i<length;i++) {
-      char ch = s.charAt(i);
-      if (lastEscape) {
-        buffer[upto++] = ch;
-        lastEscape = false;
-      } else if (ch == ESCAPE_CHAR) {
-        lastEscape = true;
-      } else if (ch == DELIM_CHAR) {
-        parts.add(new String(buffer, 0, upto));
-        upto = 0;
-      } else {
-        buffer[upto++] = ch;
-      }
-    }
-    parts.add(new String(buffer, 0, upto));
-    assert !lastEscape;
-    return parts.toArray(new String[parts.size()]);
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/FastTaxonomyFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/FastTaxonomyFacetCounts.java
deleted file mode 100644
index 1e59fec..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/FastTaxonomyFacetCounts.java
+++ /dev/null
@@ -1,190 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.simple.SimpleFacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-
-// nocommit jdoc that this assumes/requires the default encoding
-public class FastTaxonomyFacetCounts extends TaxonomyFacets {
-  private final int[] counts;
-
-  public FastTaxonomyFacetCounts(TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
-    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
-  }
-
-  public FastTaxonomyFacetCounts(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
-    super(indexFieldName, taxoReader, config);
-    counts = new int[taxoReader.getSize()];
-    count(fc.getMatchingDocs());
-  }
-
-  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
-    for(MatchingDocs hits : matchingDocs) {
-      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
-      if (dv == null) { // this reader does not have DocValues for the requested category list
-        continue;
-      }
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      BytesRef scratch = new BytesRef();
-      //System.out.println("count seg=" + hits.context.reader());
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        //System.out.println("  doc=" + doc);
-        dv.get(doc, scratch);
-        byte[] bytes = scratch.bytes;
-        int end = scratch.offset + scratch.length;
-        int ord = 0;
-        int offset = scratch.offset;
-        int prev = 0;
-        while (offset < end) {
-          byte b = bytes[offset++];
-          if (b >= 0) {
-            prev = ord = ((ord << 7) | b) + prev;
-            assert ord < counts.length: "ord=" + ord + " vs maxOrd=" + counts.length;
-            //System.out.println("    ord=" + ord);
-            ++counts[ord];
-            ord = 0;
-          } else {
-            ord = (ord << 7) | (b & 0x7F);
-          }
-        }
-        ++doc;
-      }
-    }
-
-    // nocommit we could do this lazily instead:
-
-    // Rollup any necessary dims:
-    for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
-      String dim = ent.getKey();
-      FacetsConfig.DimConfig ft = ent.getValue();
-      if (ft.hierarchical && ft.multiValued == false) {
-        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
-        // It can be -1 if this field was declared in the
-        // config but never indexed:
-        if (dimRootOrd > 0) {
-          counts[dimRootOrd] += rollup(children[dimRootOrd]);
-        }
-      }
-    }
-  }
-
-  private int rollup(int ord) {
-    int sum = 0;
-    while (ord != TaxonomyReader.INVALID_ORDINAL) {
-      int childValue = counts[ord] + rollup(children[ord]);
-      counts[ord] = childValue;
-      sum += childValue;
-      ord = siblings[ord];
-    }
-    return sum;
-  }
-
-  /** Return the count for a specific path.  Returns -1 if
-   *  this path doesn't exist, else the count. */
-  @Override
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    verifyDim(dim);
-    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
-    if (ord < 0) {
-      return -1;
-    }
-    return counts[ord];
-  }
-
-  @Override
-  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    if (topN <= 0) {
-      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
-    }
-    FacetsConfig.DimConfig dimConfig = verifyDim(dim);
-    //System.out.println("ftfc.getTopChildren topN=" + topN);
-    FacetLabel cp = FacetLabel.create(dim, path);
-    int dimOrd = taxoReader.getOrdinal(cp);
-    if (dimOrd == -1) {
-      //System.out.println("no ord for dim=" + dim + " path=" + path);
-      return null;
-    }
-
-    TopOrdAndIntQueue q = new TopOrdAndIntQueue(Math.min(taxoReader.getSize(), topN));
-    
-    int bottomCount = 0;
-
-    int ord = children[dimOrd];
-    int totCount = 0;
-    int childCount = 0;
-
-    TopOrdAndIntQueue.OrdAndValue reuse = null;
-    while(ord != TaxonomyReader.INVALID_ORDINAL) {
-      //System.out.println("  check ord=" + ord + " label=" + taxoReader.getPath(ord) + " topN=" + topN);
-      if (counts[ord] > 0) {
-        totCount += counts[ord];
-        childCount++;
-        if (counts[ord] > bottomCount) {
-          if (reuse == null) {
-            reuse = new TopOrdAndIntQueue.OrdAndValue();
-          }
-          reuse.ord = ord;
-          reuse.value = counts[ord];
-          reuse = q.insertWithOverflow(reuse);
-          if (q.size() == topN) {
-            bottomCount = q.top().value;
-          }
-        }
-      }
-
-      ord = siblings[ord];
-    }
-
-    if (totCount == 0) {
-      //System.out.println("  no matches");
-      return null;
-    }
-
-    if (dimConfig.multiValued) {
-      if (dimConfig.requireDimCount) {
-        totCount = counts[dimOrd];
-      } else {
-        // Our sum'd count is not correct, in general:
-        totCount = -1;
-      }
-    } else {
-      // Our sum'd dim count is accurate, so we keep it
-    }
-
-    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
-    for(int i=labelValues.length-1;i>=0;i--) {
-      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
-      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
-      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
-    }
-
-    return new SimpleFacetResult(totCount, labelValues, childCount);
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/FloatAssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/FloatAssociationFacetField.java
deleted file mode 100644
index eb6c1c5..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/FloatAssociationFacetField.java
+++ /dev/null
@@ -1,48 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.util.BytesRef;
-
-/** Associates an arbitrary float with the added facet
- *  path, encoding the float into a 4-byte BytesRef. */
-public class FloatAssociationFacetField extends AssociationFacetField {
-
-  /** Utility ctor: associates an int value (translates it
-   *  to 4-byte BytesRef). */
-  public FloatAssociationFacetField(float assoc, String dim, String... path) {
-    super(floatToBytesRef(assoc), dim, path);
-  }
-
-  public static BytesRef floatToBytesRef(float v) {
-    return IntAssociationFacetField.intToBytesRef(Float.floatToIntBits(v));
-  }
-
-  public static float bytesRefToFloat(BytesRef b) {
-    return Float.intBitsToFloat(IntAssociationFacetField.bytesRefToInt(b));
-  }
-
-  @Override
-  public String toString() {
-    return "FloatAssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " value=" + bytesRefToFloat(assoc) + ")";
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/FloatRange.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/FloatRange.java
deleted file mode 100644
index 4793dc7..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/FloatRange.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.FloatDocValuesField; // javadocs
-
-/** Represents a range over float values indexed as {@link
- *  FloatDocValuesField}.  */
-public final class FloatRange extends Range {
-  private final float minIncl;
-  private final float maxIncl;
-
-  public final float min;
-  public final float max;
-  public final boolean minInclusive;
-  public final boolean maxInclusive;
-
-  /** Create a FloatRange. */
-  public FloatRange(String label, float min, boolean minInclusive, float max, boolean maxInclusive) {
-    super(label);
-    this.min = min;
-    this.max = max;
-    this.minInclusive = minInclusive;
-    this.maxInclusive = maxInclusive;
-
-    // TODO: if FloatDocValuesField used
-    // NumericUtils.floatToSortableInt format (instead of
-    // Float.floatToRawIntBits) we could do comparisons
-    // in int space 
-
-    if (Float.isNaN(min)) {
-      throw new IllegalArgumentException("min cannot be NaN");
-    }
-    if (!minInclusive) {
-      min = Math.nextUp(min);
-    }
-
-    if (Float.isNaN(max)) {
-      throw new IllegalArgumentException("max cannot be NaN");
-    }
-    if (!maxInclusive) {
-      // Why no Math.nextDown?
-      max = Math.nextAfter(max, Float.NEGATIVE_INFINITY);
-    }
-
-    this.minIncl = min;
-    this.maxIncl = max;
-  }
-
-  @Override
-  public boolean accept(long value) {
-    float floatValue = Float.intBitsToFloat((int) value);
-    return floatValue >= minIncl && floatValue <= maxIncl;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/IntAssociationFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/IntAssociationFacetField.java
deleted file mode 100644
index f946d5f..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/IntAssociationFacetField.java
+++ /dev/null
@@ -1,57 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.util.BytesRef;
-
-/** Associates an arbitrary int with the added facet
- *  path, encoding the int into a 4-byte BytesRef. */
-public class IntAssociationFacetField extends AssociationFacetField {
-
-  /** Utility ctor: associates an int value (translates it
-   *  to 4-byte BytesRef). */
-  public IntAssociationFacetField(int assoc, String dim, String... path) {
-    super(intToBytesRef(assoc), dim, path);
-  }
-
-  public static BytesRef intToBytesRef(int v) {
-    byte[] bytes = new byte[4];
-    // big-endian:
-    bytes[0] = (byte) (v >> 24);
-    bytes[1] = (byte) (v >> 16);
-    bytes[2] = (byte) (v >> 8);
-    bytes[3] = (byte) v;
-    return new BytesRef(bytes);
-  }
-
-  public static int bytesRefToInt(BytesRef b) {
-    return ((b.bytes[b.offset]&0xFF) << 24) |
-      ((b.bytes[b.offset+1]&0xFF) << 16) |
-      ((b.bytes[b.offset+2]&0xFF) << 8) |
-      (b.bytes[b.offset+3]&0xFF);
-  }
-
-  @Override
-  public String toString() {
-    return "IntAssociationFacetField(dim=" + dim + " path=" + Arrays.toString(path) + " value=" + bytesRefToInt(assoc) + ")";
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/LabelAndValue.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/LabelAndValue.java
deleted file mode 100644
index 9bce0d4..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/LabelAndValue.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public final class LabelAndValue {
-  // nocommit BytesRef?
-  public final String label;
-
-  /** Value associated with this label. */
-  public final Number value;
-
-  public LabelAndValue(String label, Number value) {
-    this.label = label;
-    this.value = value;
-  }
-
-  @Override
-  public String toString() {
-    return label + " (" + value + ")";
-  }
-
-  @Override
-  public boolean equals(Object _other) {
-    if ((_other instanceof LabelAndValue) == false) {
-      return false;
-    }
-    LabelAndValue other = (LabelAndValue) _other;
-    return label.equals(other.label) && value.equals(other.value);
-  }
-
-  // nocommit hashCode
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/LongRange.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/LongRange.java
deleted file mode 100644
index eb14dc5..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/LongRange.java
+++ /dev/null
@@ -1,60 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.NumericDocValuesField; // javadocs
-
-/** Represents a range over long values indexed as {@link
- *  NumericDocValuesField}.  */
-public final class LongRange extends Range {
-  private final long minIncl;
-  private final long maxIncl;
-
-  public final long min;
-  public final long max;
-  public final boolean minInclusive;
-  public final boolean maxInclusive;
-
-  // nocommit can we require fewer args? (same for
-  // Double/FloatRange too)
-
-  /** Create a LongRange. */
-  public LongRange(String label, long min, boolean minInclusive, long max, boolean maxInclusive) {
-    super(label);
-    this.min = min;
-    this.max = max;
-    this.minInclusive = minInclusive;
-    this.maxInclusive = maxInclusive;
-
-    if (!minInclusive && min != Long.MAX_VALUE) {
-      min++;
-    }
-
-    if (!maxInclusive && max != Long.MIN_VALUE) {
-      max--;
-    }
-
-    this.minIncl = min;
-    this.maxIncl = max;
-  }
-
-  @Override
-  public boolean accept(long value) {
-    return value >= minIncl && value <= maxIncl;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/MultiFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/MultiFacets.java
deleted file mode 100644
index bccbbec..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/MultiFacets.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-/** Maps specified dims to provided Facets impls; else, uses
- *  the default Facets impl. */
-public class MultiFacets extends Facets {
-  private final Map<String,Facets> dimToFacets;
-  private final Facets defaultFacets;
-
-  public MultiFacets(Map<String,Facets> dimToFacets) {
-    this(dimToFacets = dimToFacets, null);
-  }
-
-  public MultiFacets(Map<String,Facets> dimToFacets, Facets defaultFacets) {
-    this.dimToFacets = dimToFacets;
-    this.defaultFacets = defaultFacets;
-  }
-
-  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    Facets facets = dimToFacets.get(dim);
-    if (facets == null) {
-      if (defaultFacets == null) {
-        throw new IllegalArgumentException("invalid dim \"" + dim + "\"");
-      }
-      facets = defaultFacets;
-    }
-    return facets.getTopChildren(topN, dim, path);
-  }
-
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    Facets facets = dimToFacets.get(dim);
-    if (facets == null) {
-      if (defaultFacets == null) {
-        throw new IllegalArgumentException("invalid dim \"" + dim + "\"");
-      }
-      facets = defaultFacets;
-    }
-    return facets.getSpecificValue(dim, path);
-  }
-
-  public List<SimpleFacetResult> getAllDims(int topN) throws IOException {
-    // nocommit can/should we impl this?  ie, sparse
-    // faceting after drill sideways
-    throw new UnsupportedOperationException();
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/OrdinalsReader.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/OrdinalsReader.java
deleted file mode 100644
index 83e5d6a..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/OrdinalsReader.java
+++ /dev/null
@@ -1,39 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/** Provides per-document ordinals. */
-
-public abstract class OrdinalsReader {
-
-  public static abstract class OrdinalsSegmentReader {
-    /** Get the ordinals for this document.  ordinals.offset
-     *  must always be 0! */
-    public abstract void get(int doc, IntsRef ordinals) throws IOException;
-  }
-
-  /** Set current atomic reader. */
-  public abstract OrdinalsSegmentReader getReader(AtomicReaderContext context) throws IOException;
-
-  public abstract String getIndexFieldName();
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/Range.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/Range.java
deleted file mode 100644
index c3f5d3b..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/Range.java
+++ /dev/null
@@ -1,33 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Represents a single labelled range, one facet label in
- *  the facets computed by {@link RangeAccumulator}.
- *
- *  @lucene.experimental */
-
-public abstract class Range {
-  public final String label;
-
-  protected Range(String label) {
-    this.label = label;
-  }
-
-  public abstract boolean accept(long value);
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/RangeFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/RangeFacetCounts.java
deleted file mode 100644
index 65121bc..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/RangeFacetCounts.java
+++ /dev/null
@@ -1,110 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.facet.simple.SimpleFacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.valuesource.LongFieldSource;
-
-/**
- * accumulates counts for provided ranges.
- */
-public class RangeFacetCounts extends Facets {
-  private final Range[] ranges;
-  private final int[] counts;
-  private final String field;
-  private int totCount;
-
-  public RangeFacetCounts(String field, SimpleFacetsCollector hits, Range... ranges) throws IOException {
-    this(field, new LongFieldSource(field), hits, ranges);
-  }
-
-  public RangeFacetCounts(String field, ValueSource valueSource, SimpleFacetsCollector hits, Range... ranges) throws IOException {
-    this.ranges = ranges;
-    this.field = field;
-    counts = new int[ranges.length];
-    count(valueSource, hits.getMatchingDocs());
-  }
-
-  private void count(ValueSource valueSource, List<MatchingDocs> matchingDocs) throws IOException {
-
-    // TODO: test if this is faster (in the past it was
-    // faster to do MatchingDocs on the inside) ... see
-    // patches on LUCENE-4965):
-    for (MatchingDocs hits : matchingDocs) {
-      FunctionValues fv = valueSource.getValues(Collections.emptyMap(), hits.context);
-      final int length = hits.bits.length();
-      int doc = 0;
-      totCount += hits.totalHits;
-      while (doc < length && (doc = hits.bits.nextSetBit(doc)) != -1) {
-        // Skip missing docs:
-        if (fv.exists(doc)) {
-          
-          long v = fv.longVal(doc);
-
-          // TODO: if all ranges are non-overlapping, we
-          // should instead do a bin-search up front
-          // (really, a specialized case of the interval
-          // tree)
-          // TODO: use interval tree instead of linear search:
-          for (int j = 0; j < ranges.length; j++) {
-            if (ranges[j].accept(v)) {
-              counts[j]++;
-            }
-          }
-        }
-
-        doc++;
-      }
-    }
-  }
-
-  // nocommit all args are ... unused ... this doesn't "fit"
-  // very well:
-
-  @Override
-  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) {
-    if (dim.equals(field) == false) {
-      throw new IllegalArgumentException("invalid dim \"" + dim + "\"; should be \"" + field + "\"");
-    }
-    LabelAndValue[] labelValues = new LabelAndValue[counts.length];
-    for(int i=0;i<counts.length;i++) {
-      // nocommit can we add the range into this?
-      labelValues[i] = new LabelAndValue(ranges[i].label, counts[i]);
-    }
-
-    return new SimpleFacetResult(totCount, labelValues, labelValues.length);
-  }
-
-  @Override
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    // nocommit we could impl this?
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public List<SimpleFacetResult> getAllDims(int topN) throws IOException {
-    return Collections.singletonList(getTopChildren(topN, null));
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SearcherTaxonomyManager.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SearcherTaxonomyManager.java
deleted file mode 100644
index 6245c11..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SearcherTaxonomyManager.java
+++ /dev/null
@@ -1,123 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.ReferenceManager;
-import org.apache.lucene.search.SearcherFactory;
-import org.apache.lucene.search.SearcherManager;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Manages near-real-time reopen of both an IndexSearcher
- * and a TaxonomyReader.
- *
- * <p><b>NOTE</b>: If you call {@link
- * DirectoryTaxonomyWriter#replaceTaxonomy} then you must
- * open a new {@code SearcherTaxonomyManager} afterwards.
- */
-public class SearcherTaxonomyManager extends ReferenceManager<SearcherTaxonomyManager.SearcherAndTaxonomy> {
-
-  /** Holds a matched pair of {@link IndexSearcher} and
-   *  {@link TaxonomyReader} */
-  public static class SearcherAndTaxonomy {
-    public final IndexSearcher searcher;
-    public final DirectoryTaxonomyReader taxonomyReader;
-
-    /** Create a SearcherAndTaxonomy */
-    public SearcherAndTaxonomy(IndexSearcher searcher, DirectoryTaxonomyReader taxonomyReader) {
-      this.searcher = searcher;
-      this.taxonomyReader = taxonomyReader;
-    }
-  }
-
-  private final SearcherFactory searcherFactory;
-  private final long taxoEpoch;
-  private final DirectoryTaxonomyWriter taxoWriter;
-
-  /** Creates near-real-time searcher and taxonomy reader
-   *  from the corresponding writers. */
-  public SearcherTaxonomyManager(IndexWriter writer, boolean applyAllDeletes, SearcherFactory searcherFactory, DirectoryTaxonomyWriter taxoWriter) throws IOException {
-    if (searcherFactory == null) {
-      searcherFactory = new SearcherFactory();
-    }
-    this.searcherFactory = searcherFactory;
-    this.taxoWriter = taxoWriter;
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    current = new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, DirectoryReader.open(writer, applyAllDeletes)),
-                                      taxoReader);
-    taxoEpoch = taxoWriter.getTaxonomyEpoch();
-  }
-
-  @Override
-  protected void decRef(SearcherAndTaxonomy ref) throws IOException {
-    ref.searcher.getIndexReader().decRef();
-
-    // This decRef can fail, and then in theory we should
-    // tryIncRef the searcher to put back the ref count
-    // ... but 1) the below decRef should only fail because
-    // it decRef'd to 0 and closed and hit some IOException
-    // during close, in which case 2) very likely the
-    // searcher was also just closed by the above decRef and
-    // a tryIncRef would fail:
-    ref.taxonomyReader.decRef();
-  }
-
-  @Override
-  protected boolean tryIncRef(SearcherAndTaxonomy ref) throws IOException {
-    if (ref.searcher.getIndexReader().tryIncRef()) {
-      if (ref.taxonomyReader.tryIncRef()) {
-        return true;
-      } else {
-        ref.searcher.getIndexReader().decRef();
-      }
-    }
-    return false;
-  }
-
-  @Override
-  protected SearcherAndTaxonomy refreshIfNeeded(SearcherAndTaxonomy ref) throws IOException {
-    // Must re-open searcher first, otherwise we may get a
-    // new reader that references ords not yet known to the
-    // taxonomy reader:
-    final IndexReader r = ref.searcher.getIndexReader();
-    final IndexReader newReader = DirectoryReader.openIfChanged((DirectoryReader) r);
-    if (newReader == null) {
-      return null;
-    } else {
-      DirectoryTaxonomyReader tr = TaxonomyReader.openIfChanged(ref.taxonomyReader);
-      if (tr == null) {
-        ref.taxonomyReader.incRef();
-        tr = ref.taxonomyReader;
-      } else if (taxoWriter.getTaxonomyEpoch() != taxoEpoch) {
-        IOUtils.close(newReader, tr);
-        throw new IllegalStateException("DirectoryTaxonomyWriter.replaceTaxonomy was called, which is not allowed when using SearcherTaxonomyManager");
-      }
-
-      return new SearcherAndTaxonomy(SearcherManager.getSearcher(searcherFactory, newReader), tr);
-    }
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillDownQuery.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillDownQuery.java
deleted file mode 100644
index 8ad3d17..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillDownQuery.java
+++ /dev/null
@@ -1,225 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.FilteredQuery;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-
-/**
- * A {@link Query} for drill-down over {@link FacetLabel categories}. You
- * should call {@link #add(FacetLabel...)} for every group of categories you
- * want to drill-down over. Each category in the group is {@code OR'ed} with
- * the others, and groups are {@code AND'ed}.
- * <p>
- * <b>NOTE:</b> if you choose to create your own {@link Query} by calling
- * {@link #term}, it is recommended to wrap it with {@link ConstantScoreQuery}
- * and set the {@link ConstantScoreQuery#setBoost(float) boost} to {@code 0.0f},
- * so that it does not affect the scores of the documents.
- * 
- * @lucene.experimental
- */
-public final class SimpleDrillDownQuery extends Query {
-
-  public static Term term(String field, String dim, String... path) {
-    return new Term(field, FacetsConfig.pathToString(dim, path));
-  }
-
-  private final FacetsConfig config;
-  private final BooleanQuery query;
-  private final Map<String,Integer> drillDownDims = new LinkedHashMap<String,Integer>();
-
-  /** Used by clone() */
-  SimpleDrillDownQuery(FacetsConfig config, BooleanQuery query, Map<String,Integer> drillDownDims) {
-    this.query = query.clone();
-    this.drillDownDims.putAll(drillDownDims);
-    this.config = config;
-  }
-
-  /** Used by DrillSideways */
-  SimpleDrillDownQuery(FacetsConfig config, Filter filter, SimpleDrillDownQuery other) {
-    query = new BooleanQuery(true); // disable coord
-
-    BooleanClause[] clauses = other.query.getClauses();
-    if (clauses.length == other.drillDownDims.size()) {
-      throw new IllegalArgumentException("cannot apply filter unless baseQuery isn't null; pass ConstantScoreQuery instead");
-    }
-    assert clauses.length == 1+other.drillDownDims.size(): clauses.length + " vs " + (1+other.drillDownDims.size());
-    drillDownDims.putAll(other.drillDownDims);
-    query.add(new FilteredQuery(clauses[0].getQuery(), filter), Occur.MUST);
-    for(int i=1;i<clauses.length;i++) {
-      query.add(clauses[i].getQuery(), Occur.MUST);
-    }
-    this.config = config;
-  }
-
-  /** Used by DrillSideways */
-  SimpleDrillDownQuery(FacetsConfig config, Query baseQuery, List<Query> clauses, Map<String,Integer> drillDownDims) {
-    this.query = new BooleanQuery(true);
-    if (baseQuery != null) {
-      query.add(baseQuery, Occur.MUST);      
-    }
-    for(Query clause : clauses) {
-      query.add(clause, Occur.MUST);
-    }
-    this.drillDownDims.putAll(drillDownDims);
-    this.config = config;
-  }
-
-  /**
-   * Creates a new {@code SimpleDrillDownQuery} without a base query, 
-   * to perform a pure browsing query (equivalent to using
-   * {@link MatchAllDocsQuery} as base).
-   */
-  public SimpleDrillDownQuery(FacetsConfig config) {
-    this(config, null);
-  }
-  
-  /**
-   * Creates a new {@code SimpleDrillDownQuery} over the given base query. Can be
-   * {@code null}, in which case the result {@link Query} from
-   * {@link #rewrite(IndexReader)} will be a pure browsing query, filtering on
-   * the added categories only.
-   */
-  public SimpleDrillDownQuery(FacetsConfig config, Query baseQuery) {
-    query = new BooleanQuery(true); // disable coord
-    if (baseQuery != null) {
-      query.add(baseQuery, Occur.MUST);
-    }
-    this.config = config;
-  }
-
-  /** Merges (ORs) a new path into an existing AND'd
-   *  clause. */ 
-  private void merge(String dim, String[] path) {
-    int index = drillDownDims.get(dim);
-    if (query.getClauses().length == drillDownDims.size()+1) {
-      index++;
-    }
-    ConstantScoreQuery q = (ConstantScoreQuery) query.clauses().get(index).getQuery();
-    if ((q.getQuery() instanceof BooleanQuery) == false) {
-      // App called .add(dim, customQuery) and then tried to
-      // merge a facet label in:
-      throw new RuntimeException("cannot merge with custom Query");
-    }
-    String indexedField = config.getDimConfig(dim).indexFieldName;
-
-    BooleanQuery bq = (BooleanQuery) q.getQuery();
-    bq.add(new TermQuery(term(indexedField, dim, path)), Occur.SHOULD);
-  }
-
-  /** Adds one dimension of drill downs; if you pass the same
-   *  dimension again, it's OR'd with the previous
-   *  constraints on that dimension, and all dimensions are
-   *  AND'd against each other and the base query. */
-  // nocommit can we remove FacetLabel here?
-  public void add(String dim, String... path) {
-
-    if (drillDownDims.containsKey(dim)) {
-      merge(dim, path);
-      return;
-    }
-    String indexedField = config.getDimConfig(dim).indexFieldName;
-
-    BooleanQuery bq = new BooleanQuery(true); // disable coord
-    // nocommit too anal?
-    /*
-    if (path.length == 0) {
-      throw new IllegalArgumentException("must have at least one facet label under dim");
-    }
-    */
-    bq.add(new TermQuery(term(indexedField, dim, path)), Occur.SHOULD);
-
-    add(dim, bq);
-  }
-
-  /** Expert: add a custom drill-down subQuery.  Use this
-   *  when you have a separate way to drill-down on the
-   *  dimension than the indexed facet ordinals. */
-  public void add(String dim, Query subQuery) {
-
-    // TODO: we should use FilteredQuery?
-
-    // So scores of the drill-down query don't have an
-    // effect:
-    final ConstantScoreQuery drillDownQuery = new ConstantScoreQuery(subQuery);
-    drillDownQuery.setBoost(0.0f);
-
-    query.add(drillDownQuery, Occur.MUST);
-
-    drillDownDims.put(dim, drillDownDims.size());
-  }
-
-  @Override
-  public SimpleDrillDownQuery clone() {
-    return new SimpleDrillDownQuery(config, query, drillDownDims);
-  }
-  
-  @Override
-  public int hashCode() {
-    final int prime = 31;
-    int result = super.hashCode();
-    return prime * result + query.hashCode();
-  }
-  
-  @Override
-  public boolean equals(Object obj) {
-    if (!(obj instanceof SimpleDrillDownQuery)) {
-      return false;
-    }
-    
-    SimpleDrillDownQuery other = (SimpleDrillDownQuery) obj;
-    return query.equals(other.query) && super.equals(other);
-  }
-  
-  @Override
-  public Query rewrite(IndexReader r) throws IOException {
-    if (query.clauses().size() == 0) {
-      return new MatchAllDocsQuery();
-    }
-    return query;
-  }
-
-  @Override
-  public String toString(String field) {
-    return query.toString(field);
-  }
-
-  BooleanQuery getBooleanQuery() {
-    return query;
-  }
-
-  Map<String,Integer> getDims() {
-    return drillDownDims;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSideways.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSideways.java
deleted file mode 100644
index 24b5142..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSideways.java
+++ /dev/null
@@ -1,446 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.FieldDoc;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.MultiCollector;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.TopFieldCollector;
-import org.apache.lucene.search.TopScoreDocCollector;
-import org.apache.lucene.search.Weight;
-
-/**     
- * Computes drill down and sideways counts for the provided
- * {@link DrillDownQuery}.  Drill sideways counts include
- * alternative values/aggregates for the drill-down
- * dimensions so that a dimension does not disappear after
- * the user drills down into it.
- *
- * <p> Use one of the static search
- * methods to do the search, and then get the hits and facet
- * results from the returned {@link DrillSidewaysResult}.
- *
- * <p><b>NOTE</b>: this allocates one {@link
- * FacetsCollector} for each drill-down, plus one.  If your
- * index has high number of facet labels then this will
- * multiply your memory usage.
- *
- * @lucene.experimental
- */
-
-public class SimpleDrillSideways {
-
-  protected final IndexSearcher searcher;
-  protected final TaxonomyReader taxoReader;
-  protected final SortedSetDocValuesReaderState state;
-  protected final FacetsConfig config;
-
-  /**
-   * Create a new {@code DrillSideways} instance, assuming the categories were
-   * indexed with {@link FacetFields}.
-   */
-  public SimpleDrillSideways(IndexSearcher searcher, FacetsConfig config, TaxonomyReader taxoReader) {
-    this(searcher, config, taxoReader, null);
-  }
-    
-  /**
-   * Create a new {@code DrillSideways} instance, assuming the categories were
-   * indexed with {@link SortedSetDocValuesFacetFields}.
-   */
-  public SimpleDrillSideways(IndexSearcher searcher, FacetsConfig config, SortedSetDocValuesReaderState state) {
-    this(searcher, config, null, state);
-  }
-
-  /**
-   * Create a new {@code DrillSideways} instance, where some
-   * dimensions are sorted set facets and others are
-   * taxononmy facets.
-   */
-  public SimpleDrillSideways(IndexSearcher searcher, FacetsConfig config, TaxonomyReader taxoReader, SortedSetDocValuesReaderState state) {
-    this.searcher = searcher;
-    this.config = config;
-    this.taxoReader = taxoReader;
-    this.state = state;
-  }
-
-  /** Subclass can override to customize per-dim Facets
-   *  impl. */
-  protected Facets buildFacetsResult(SimpleFacetsCollector drillDowns, SimpleFacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {
-
-    Facets drillDownFacets;
-    Map<String,Facets> drillSidewaysFacets = new HashMap<String,Facets>();
-
-    if (taxoReader != null) {
-      drillDownFacets = new FastTaxonomyFacetCounts(taxoReader, config, drillDowns);
-      if (drillSideways != null) {
-        for(int i=0;i<drillSideways.length;i++) {
-          drillSidewaysFacets.put(drillSidewaysDims[i],
-                                  new FastTaxonomyFacetCounts(taxoReader, config, drillSideways[i]));
-        }
-      }
-    } else {
-      drillDownFacets = new SortedSetDocValuesFacetCounts(state, drillDowns);
-      if (drillSideways != null) {
-        for(int i=0;i<drillSideways.length;i++) {
-          drillSidewaysFacets.put(drillSidewaysDims[i],
-                                  new SortedSetDocValuesFacetCounts(state, drillSideways[i]));
-        }
-      }
-    }
-
-    if (drillSidewaysFacets.isEmpty()) {
-      return drillDownFacets;
-    } else {
-      return new MultiFacets(drillSidewaysFacets, drillDownFacets);
-    }
-  }
-
-  /**
-   * Search, collecting hits with a {@link Collector}, and
-   * computing drill down and sideways counts.
-   */
-  @SuppressWarnings({"rawtypes","unchecked"})
-  public SimpleDrillSidewaysResult search(SimpleDrillDownQuery query, Collector hitCollector) throws IOException {
-
-    Map<String,Integer> drillDownDims = query.getDims();
-
-    SimpleFacetsCollector drillDownCollector = new SimpleFacetsCollector();
-    
-    if (drillDownDims.isEmpty()) {
-      // There are no drill-down dims, so there is no
-      // drill-sideways to compute:
-      searcher.search(query, MultiCollector.wrap(hitCollector, drillDownCollector));
-      return new SimpleDrillSidewaysResult(buildFacetsResult(drillDownCollector, null, null), null);
-    }
-
-    BooleanQuery ddq = query.getBooleanQuery();
-    BooleanClause[] clauses = ddq.getClauses();
-
-    Query baseQuery;
-    int startClause;
-    if (clauses.length == drillDownDims.size()) {
-      // TODO: we could optimize this pure-browse case by
-      // making a custom scorer instead:
-      baseQuery = new MatchAllDocsQuery();
-      startClause = 0;
-    } else {
-      assert clauses.length == 1+drillDownDims.size();
-      baseQuery = clauses[0].getQuery();
-      startClause = 1;
-    }
-
-    SimpleFacetsCollector[] drillSidewaysCollectors = new SimpleFacetsCollector[drillDownDims.size()];
-
-    int idx = 0;
-    for(String dim : drillDownDims.keySet()) {
-      drillSidewaysCollectors[idx++] = new SimpleFacetsCollector();
-    }
-
-    boolean useCollectorMethod = scoreSubDocsAtOnce();
-
-    Term[][] drillDownTerms = null;
-
-    if (!useCollectorMethod) {
-      // Optimistic: assume subQueries of the DDQ are either
-      // TermQuery or BQ OR of TermQuery; if this is wrong
-      // then we detect it and fallback to the mome general
-      // but slower DrillSidewaysCollector:
-      drillDownTerms = new Term[clauses.length-startClause][];
-      for(int i=startClause;i<clauses.length;i++) {
-        Query q = clauses[i].getQuery();
-
-        // DrillDownQuery always wraps each subQuery in
-        // ConstantScoreQuery:
-        assert q instanceof ConstantScoreQuery;
-
-        q = ((ConstantScoreQuery) q).getQuery();
-
-        if (q instanceof TermQuery) {
-          drillDownTerms[i-startClause] = new Term[] {((TermQuery) q).getTerm()};
-        } else if (q instanceof BooleanQuery) {
-          BooleanQuery q2 = (BooleanQuery) q;
-          BooleanClause[] clauses2 = q2.getClauses();
-          drillDownTerms[i-startClause] = new Term[clauses2.length];
-          for(int j=0;j<clauses2.length;j++) {
-            if (clauses2[j].getQuery() instanceof TermQuery) {
-              drillDownTerms[i-startClause][j] = ((TermQuery) clauses2[j].getQuery()).getTerm();
-            } else {
-              useCollectorMethod = true;
-              break;
-            }
-          }
-        } else {
-          useCollectorMethod = true;
-        }
-      }
-    }
-
-    if (useCollectorMethod) {
-      // TODO: maybe we could push the "collector method"
-      // down into the optimized scorer to have a tighter
-      // integration ... and so TermQuery clauses could
-      // continue to run "optimized"
-      collectorMethod(query, baseQuery, startClause, hitCollector, drillDownCollector, drillSidewaysCollectors);
-    } else {
-      SimpleDrillSidewaysQuery dsq = new SimpleDrillSidewaysQuery(baseQuery, drillDownCollector, drillSidewaysCollectors, drillDownTerms);
-      searcher.search(dsq, hitCollector);
-    }
-
-    return new SimpleDrillSidewaysResult(buildFacetsResult(drillDownCollector, drillSidewaysCollectors, drillDownDims.keySet().toArray(new String[drillDownDims.size()])), null);
-  }
-
-  /** Uses the more general but slower method of sideways
-   *  counting. This method allows an arbitrary subQuery to
-   *  implement the drill down for a given dimension. */
-  private void collectorMethod(SimpleDrillDownQuery ddq, Query baseQuery, int startClause, Collector hitCollector, Collector drillDownCollector, Collector[] drillSidewaysCollectors) throws IOException {
-
-    BooleanClause[] clauses = ddq.getBooleanQuery().getClauses();
-
-    Map<String,Integer> drillDownDims = ddq.getDims();
-
-    BooleanQuery topQuery = new BooleanQuery(true);
-    final SimpleDrillSidewaysCollector collector = new SimpleDrillSidewaysCollector(hitCollector, drillDownCollector, drillSidewaysCollectors,
-                                                                                    drillDownDims);
-
-    // TODO: if query is already a BQ we could copy that and
-    // add clauses to it, instead of doing BQ inside BQ
-    // (should be more efficient)?  Problem is this can
-    // affect scoring (coord) ... too bad we can't disable
-    // coord on a clause by clause basis:
-    topQuery.add(baseQuery, BooleanClause.Occur.MUST);
-
-    // NOTE: in theory we could just make a single BQ, with
-    // +query a b c minShouldMatch=2, but in this case,
-    // annoyingly, BS2 wraps a sub-scorer that always
-    // returns 2 as the .freq(), not how many of the
-    // SHOULD clauses matched:
-    BooleanQuery subQuery = new BooleanQuery(true);
-
-    Query wrappedSubQuery = new QueryWrapper(subQuery,
-                                             new SetWeight() {
-                                               @Override
-                                               public void set(Weight w) {
-                                                 collector.setWeight(w, -1);
-                                               }
-                                             });
-    Query constantScoreSubQuery = new ConstantScoreQuery(wrappedSubQuery);
-
-    // Don't impact score of original query:
-    constantScoreSubQuery.setBoost(0.0f);
-
-    topQuery.add(constantScoreSubQuery, BooleanClause.Occur.MUST);
-
-    // Unfortunately this sub-BooleanQuery
-    // will never get BS1 because today BS1 only works
-    // if topScorer=true... and actually we cannot use BS1
-    // anyways because we need subDocsScoredAtOnce:
-    int dimIndex = 0;
-    for(int i=startClause;i<clauses.length;i++) {
-      Query q = clauses[i].getQuery();
-      // DrillDownQuery always wraps each subQuery in
-      // ConstantScoreQuery:
-      assert q instanceof ConstantScoreQuery;
-      q = ((ConstantScoreQuery) q).getQuery();
-
-      final int finalDimIndex = dimIndex;
-      subQuery.add(new QueryWrapper(q,
-                                    new SetWeight() {
-                                      @Override
-                                      public void set(Weight w) {
-                                        collector.setWeight(w, finalDimIndex);
-                                      }
-                                    }),
-                   BooleanClause.Occur.SHOULD);
-      dimIndex++;
-    }
-
-    // TODO: we could better optimize the "just one drill
-    // down" case w/ a separate [specialized]
-    // collector...
-    int minShouldMatch = drillDownDims.size()-1;
-    if (minShouldMatch == 0) {
-      // Must add another "fake" clause so BQ doesn't erase
-      // itself by rewriting to the single clause:
-      Query end = new MatchAllDocsQuery();
-      end.setBoost(0.0f);
-      subQuery.add(end, BooleanClause.Occur.SHOULD);
-      minShouldMatch++;
-    }
-
-    subQuery.setMinimumNumberShouldMatch(minShouldMatch);
-
-    // System.out.println("EXE " + topQuery);
-
-    // Collects against the passed-in
-    // drillDown/SidewaysCollectors as a side effect:
-    searcher.search(topQuery, collector);
-  }
-
-  /**
-   * Search, sorting by {@link Sort}, and computing
-   * drill down and sideways counts.
-   */
-  public SimpleDrillSidewaysResult search(SimpleDrillDownQuery query,
-                                          Filter filter, FieldDoc after, int topN, Sort sort, boolean doDocScores,
-                                          boolean doMaxScore) throws IOException {
-    if (filter != null) {
-      query = new SimpleDrillDownQuery(config, filter, query);
-    }
-    if (sort != null) {
-      int limit = searcher.getIndexReader().maxDoc();
-      if (limit == 0) {
-        limit = 1; // the collector does not alow numHits = 0
-      }
-      topN = Math.min(topN, limit);
-      final TopFieldCollector hitCollector = TopFieldCollector.create(sort,
-                                                                      topN,
-                                                                      after,
-                                                                      true,
-                                                                      doDocScores,
-                                                                      doMaxScore,
-                                                                      true);
-      SimpleDrillSidewaysResult r = search(query, hitCollector);
-      return new SimpleDrillSidewaysResult(r.facets, hitCollector.topDocs());
-    } else {
-      return search(after, query, topN);
-    }
-  }
-
-  /**
-   * Search, sorting by score, and computing
-   * drill down and sideways counts.
-   */
-  public SimpleDrillSidewaysResult search(SimpleDrillDownQuery query, int topN) throws IOException {
-    return search(null, query, topN);
-  }
-
-  /**
-   * Search, sorting by score, and computing
-   * drill down and sideways counts.
-   */
-  public SimpleDrillSidewaysResult search(ScoreDoc after,
-                                          SimpleDrillDownQuery query, int topN) throws IOException {
-    int limit = searcher.getIndexReader().maxDoc();
-    if (limit == 0) {
-      limit = 1; // the collector does not alow numHits = 0
-    }
-    topN = Math.min(topN, limit);
-    TopScoreDocCollector hitCollector = TopScoreDocCollector.create(topN, after, true);
-    SimpleDrillSidewaysResult r = search(query, hitCollector);
-    return new SimpleDrillSidewaysResult(r.facets, hitCollector.topDocs());
-  }
-
-  /** Override this and return true if your collector
-   *  (e.g., ToParentBlockJoinCollector) expects all
-   *  sub-scorers to be positioned on the document being
-   *  collected.  This will cause some performance loss;
-   *  default is false.  Note that if you return true from
-   *  this method (in a subclass) be sure your collector
-   *  also returns false from {@link
-   *  Collector#acceptsDocsOutOfOrder}: this will trick
-   *  BooleanQuery into also scoring all subDocs at once. */
-  protected boolean scoreSubDocsAtOnce() {
-    return false;
-  }
-
-  public static class SimpleDrillSidewaysResult {
-    /** Combined drill down & sideways results. */
-    public final Facets facets;
-
-    /** Hits. */
-    public final TopDocs hits;
-
-    public SimpleDrillSidewaysResult(Facets facets, TopDocs hits) {
-      this.facets = facets;
-      this.hits = hits;
-    }
-  }
-  private interface SetWeight {
-    public void set(Weight w);
-  }
-
-  /** Just records which Weight was given out for the
-   *  (possibly rewritten) Query. */
-  private static class QueryWrapper extends Query {
-    private final Query originalQuery;
-    private final SetWeight setter;
-
-    public QueryWrapper(Query originalQuery, SetWeight setter) {
-      this.originalQuery = originalQuery;
-      this.setter = setter;
-    }
-
-    @Override
-    public Weight createWeight(final IndexSearcher searcher) throws IOException {
-      Weight w = originalQuery.createWeight(searcher);
-      setter.set(w);
-      return w;
-    }
-
-    @Override
-    public Query rewrite(IndexReader reader) throws IOException {
-      Query rewritten = originalQuery.rewrite(reader);
-      if (rewritten != originalQuery) {
-        return new QueryWrapper(rewritten, setter);
-      } else {
-        return this;
-      }
-    }
-
-    @Override
-    public String toString(String s) {
-      return originalQuery.toString(s);
-    }
-
-    @Override
-    public boolean equals(Object o) {
-      if (!(o instanceof QueryWrapper)) return false;
-      final QueryWrapper other = (QueryWrapper) o;
-      return super.equals(o) && originalQuery.equals(other.originalQuery);
-    }
-
-    @Override
-    public int hashCode() {
-      return super.hashCode() * 31 + originalQuery.hashCode();
-    }
-  }
-}
-
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysCollector.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysCollector.java
deleted file mode 100644
index c8421b4..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysCollector.java
+++ /dev/null
@@ -1,188 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.IdentityHashMap;
-import java.util.Map;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer.ChildScorer;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Weight;
-
-/** Collector that scrutinizes each hit to determine if it
- *  passed all constraints (a true hit) or if it missed
- *  exactly one dimension (a near-miss, to count for
- *  drill-sideways counts on that dimension). */
-class SimpleDrillSidewaysCollector extends Collector {
-
-  private final Collector hitCollector;
-  private final Collector drillDownCollector;
-  private final Collector[] drillSidewaysCollectors;
-  private final Scorer[] subScorers;
-  private final int exactCount;
-
-  // Maps Weight to either -1 (mainQuery) or to integer
-  // index of the dims drillDown.  We needs this when
-  // visiting the child scorers to correlate back to the
-  // right scorers:
-  private final Map<Weight,Integer> weightToIndex = new IdentityHashMap<Weight,Integer>();
-
-  private Scorer mainScorer;
-
-  public SimpleDrillSidewaysCollector(Collector hitCollector, Collector drillDownCollector, Collector[] drillSidewaysCollectors,
-                                      Map<String,Integer> dims) {
-    this.hitCollector = hitCollector;
-    this.drillDownCollector = drillDownCollector;
-    this.drillSidewaysCollectors = drillSidewaysCollectors;
-    subScorers = new Scorer[dims.size()];
-
-    if (dims.size() == 1) {
-      // When we have only one dim, we insert the
-      // MatchAllDocsQuery, bringing the clause count to
-      // 2:
-      exactCount = 2;
-    } else {
-      exactCount = dims.size();
-    }
-  }
-
-  @Override
-  public void collect(int doc) throws IOException {
-    //System.out.println("collect doc=" + doc + " main.freq=" + mainScorer.freq() + " main.doc=" + mainScorer.docID() + " exactCount=" + exactCount);
-      
-    if (mainScorer == null) {
-      // This segment did not have any docs with any
-      // drill-down field & value:
-      return;
-    }
-
-    if (mainScorer.freq() == exactCount) {
-      // All sub-clauses from the drill-down filters
-      // matched, so this is a "real" hit, so we first
-      // collect in both the hitCollector and the
-      // drillDown collector:
-      //System.out.println("  hit " + drillDownCollector);
-      hitCollector.collect(doc);
-      if (drillDownCollector != null) {
-        drillDownCollector.collect(doc);
-      }
-
-      // Also collect across all drill-sideways counts so
-      // we "merge in" drill-down counts for this
-      // dimension.
-      for(int i=0;i<subScorers.length;i++) {
-        // This cannot be null, because it was a hit,
-        // meaning all drill-down dims matched, so all
-        // dims must have non-null scorers:
-        assert subScorers[i] != null;
-        int subDoc = subScorers[i].docID();
-        assert subDoc == doc;
-        drillSidewaysCollectors[i].collect(doc);
-      }
-
-    } else {
-      boolean found = false;
-      for(int i=0;i<subScorers.length;i++) {
-        if (subScorers[i] == null) {
-          // This segment did not have any docs with this
-          // drill-down field & value:
-          drillSidewaysCollectors[i].collect(doc);
-          assert allMatchesFrom(i+1, doc);
-          found = true;
-          break;
-        }
-        int subDoc = subScorers[i].docID();
-        //System.out.println("  i=" + i + " sub: " + subDoc);
-        if (subDoc != doc) {
-          //System.out.println("  +ds[" + i + "]");
-          assert subDoc > doc: "subDoc=" + subDoc + " doc=" + doc;
-          drillSidewaysCollectors[i].collect(doc);
-          assert allMatchesFrom(i+1, doc);
-          found = true;
-          break;
-        }
-      }
-      assert found;
-    }
-  }
-
-  // Only used by assert:
-  private boolean allMatchesFrom(int startFrom, int doc) {
-    for(int i=startFrom;i<subScorers.length;i++) {
-      assert subScorers[i].docID() == doc;
-    }
-    return true;
-  }
-
-  @Override
-  public boolean acceptsDocsOutOfOrder() {
-    // We actually could accept docs out of order, but, we
-    // need to force BooleanScorer2 so that the
-    // sub-scorers are "on" each docID we are collecting:
-    return false;
-  }
-
-  @Override
-  public void setNextReader(AtomicReaderContext leaf) throws IOException {
-    //System.out.println("DS.setNextReader reader=" + leaf.reader());
-    hitCollector.setNextReader(leaf);
-    if (drillDownCollector != null) {
-      drillDownCollector.setNextReader(leaf);
-    }
-    for(Collector dsc : drillSidewaysCollectors) {
-      dsc.setNextReader(leaf);
-    }
-  }
-
-  void setWeight(Weight weight, int index) {
-    assert !weightToIndex.containsKey(weight);
-    weightToIndex.put(weight, index);
-  }
-
-  private void findScorers(Scorer scorer) {
-    Integer index = weightToIndex.get(scorer.getWeight());
-    if (index != null) {
-      if (index.intValue() == -1) {
-        mainScorer = scorer;
-      } else {
-        subScorers[index] = scorer;
-      }
-    }
-    for(ChildScorer child : scorer.getChildren()) {
-      findScorers(child.child);
-    }
-  }
-
-  @Override
-  public void setScorer(Scorer scorer) throws IOException {
-    mainScorer = null;
-    Arrays.fill(subScorers, null);
-    findScorers(scorer);
-    hitCollector.setScorer(scorer);
-    if (drillDownCollector != null) {
-      drillDownCollector.setScorer(scorer);
-    }
-    for(Collector dsc : drillSidewaysCollectors) {
-      dsc.setScorer(scorer);
-    }
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysQuery.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysQuery.java
deleted file mode 100644
index 34a6813..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysQuery.java
+++ /dev/null
@@ -1,198 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Explanation;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Weight;
-import org.apache.lucene.util.Bits;
-
-/** Only purpose is to punch through and return a
- *  SimpleDrillSidewaysScorer */ 
-
-class SimpleDrillSidewaysQuery extends Query {
-  final Query baseQuery;
-  final Collector drillDownCollector;
-  final Collector[] drillSidewaysCollectors;
-  final Term[][] drillDownTerms;
-
-  SimpleDrillSidewaysQuery(Query baseQuery, Collector drillDownCollector, Collector[] drillSidewaysCollectors, Term[][] drillDownTerms) {
-    this.baseQuery = baseQuery;
-    this.drillDownCollector = drillDownCollector;
-    this.drillSidewaysCollectors = drillSidewaysCollectors;
-    this.drillDownTerms = drillDownTerms;
-  }
-
-  @Override
-  public String toString(String field) {
-    return "DrillSidewaysQuery";
-  }
-
-  @Override
-  public Query rewrite(IndexReader reader) throws IOException {
-    Query newQuery = baseQuery;
-    while(true) {
-      Query rewrittenQuery = newQuery.rewrite(reader);
-      if (rewrittenQuery == newQuery) {
-        break;
-      }
-      newQuery = rewrittenQuery;
-    }
-    if (newQuery == baseQuery) {
-      return this;
-    } else {
-      return new SimpleDrillSidewaysQuery(newQuery, drillDownCollector, drillSidewaysCollectors, drillDownTerms);
-    }
-  }
-  
-  @Override
-  public Weight createWeight(IndexSearcher searcher) throws IOException {
-    final Weight baseWeight = baseQuery.createWeight(searcher);
-
-    return new Weight() {
-      @Override
-      public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
-        return baseWeight.explain(context, doc);
-      }
-
-      @Override
-      public Query getQuery() {
-        return baseQuery;
-      }
-
-      @Override
-      public float getValueForNormalization() throws IOException {
-        return baseWeight.getValueForNormalization();
-      }
-
-      @Override
-      public void normalize(float norm, float topLevelBoost) {
-        baseWeight.normalize(norm, topLevelBoost);
-      }
-
-      @Override
-      public boolean scoresDocsOutOfOrder() {
-        // TODO: would be nice if AssertingIndexSearcher
-        // confirmed this for us
-        return false;
-      }
-
-      @Override
-      public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
-                           boolean topScorer, Bits acceptDocs) throws IOException {
-
-        SimpleDrillSidewaysScorer.DocsEnumsAndFreq[] dims = new SimpleDrillSidewaysScorer.DocsEnumsAndFreq[drillDownTerms.length];
-        TermsEnum termsEnum = null;
-        String lastField = null;
-        int nullCount = 0;
-        for(int dim=0;dim<dims.length;dim++) {
-          dims[dim] = new SimpleDrillSidewaysScorer.DocsEnumsAndFreq();
-          dims[dim].sidewaysCollector = drillSidewaysCollectors[dim];
-          String field = drillDownTerms[dim][0].field();
-          dims[dim].dim = drillDownTerms[dim][0].text();
-          if (lastField == null || !lastField.equals(field)) {
-            AtomicReader reader = context.reader();
-            Terms terms = reader.terms(field);
-            if (terms != null) {
-              termsEnum = terms.iterator(null);
-            } else {
-              termsEnum = null;
-            }
-            lastField = field;
-          }
-          dims[dim].docsEnums = new DocsEnum[drillDownTerms[dim].length];
-          if (termsEnum == null) {
-            nullCount++;
-            continue;
-          }
-          for(int i=0;i<drillDownTerms[dim].length;i++) {
-            if (termsEnum.seekExact(drillDownTerms[dim][i].bytes())) {
-              DocsEnum docsEnum = termsEnum.docs(null, null, 0);
-              if (docsEnum != null) {
-                dims[dim].docsEnums[i] = docsEnum;
-                dims[dim].maxCost = Math.max(dims[dim].maxCost, docsEnum.cost());
-              }
-            }
-          }
-        }
-
-        if (nullCount > 1 || (nullCount == 1 && dims.length == 1)) {
-          return null;
-        }
-
-        // Sort drill-downs by most restrictive first:
-        Arrays.sort(dims);
-
-        // TODO: it could be better if we take acceptDocs
-        // into account instead of baseScorer?
-        Scorer baseScorer = baseWeight.scorer(context, scoreDocsInOrder, false, acceptDocs);
-
-        if (baseScorer == null) {
-          return null;
-        }
-
-        return new SimpleDrillSidewaysScorer(this, context,
-                                             baseScorer,
-                                             drillDownCollector, dims);
-      }
-    };
-  }
-
-  // TODO: these should do "deeper" equals/hash on the 2-D drillDownTerms array
-
-  @Override
-  public int hashCode() {
-    final int prime = 31;
-    int result = super.hashCode();
-    result = prime * result + ((baseQuery == null) ? 0 : baseQuery.hashCode());
-    result = prime * result
-        + ((drillDownCollector == null) ? 0 : drillDownCollector.hashCode());
-    result = prime * result + Arrays.hashCode(drillDownTerms);
-    result = prime * result + Arrays.hashCode(drillSidewaysCollectors);
-    return result;
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (this == obj) return true;
-    if (!super.equals(obj)) return false;
-    if (getClass() != obj.getClass()) return false;
-    SimpleDrillSidewaysQuery other = (SimpleDrillSidewaysQuery) obj;
-    if (baseQuery == null) {
-      if (other.baseQuery != null) return false;
-    } else if (!baseQuery.equals(other.baseQuery)) return false;
-    if (drillDownCollector == null) {
-      if (other.drillDownCollector != null) return false;
-    } else if (!drillDownCollector.equals(other.drillDownCollector)) return false;
-    if (!Arrays.equals(drillDownTerms, other.drillDownTerms)) return false;
-    if (!Arrays.equals(drillSidewaysCollectors, other.drillSidewaysCollectors)) return false;
-    return true;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysScorer.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysScorer.java
deleted file mode 100644
index 6be41af..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysScorer.java
+++ /dev/null
@@ -1,654 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Collections;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Weight;
-import org.apache.lucene.util.FixedBitSet;
-
-class SimpleDrillSidewaysScorer extends Scorer {
-
-  //private static boolean DEBUG = false;
-
-  private final Collector drillDownCollector;
-
-  private final DocsEnumsAndFreq[] dims;
-
-  // DrillDown DocsEnums:
-  private final Scorer baseScorer;
-
-  private final AtomicReaderContext context;
-
-  private static final int CHUNK = 2048;
-  private static final int MASK = CHUNK-1;
-
-  private int collectDocID = -1;
-  private float collectScore;
-
-  SimpleDrillSidewaysScorer(Weight w, AtomicReaderContext context, Scorer baseScorer, Collector drillDownCollector,
-                            DocsEnumsAndFreq[] dims) {
-    super(w);
-    this.dims = dims;
-    this.context = context;
-    this.baseScorer = baseScorer;
-    this.drillDownCollector = drillDownCollector;
-  }
-
-  @Override
-  public void score(Collector collector) throws IOException {
-    //if (DEBUG) {
-    //  System.out.println("\nscore: reader=" + context.reader());
-    //}
-    //System.out.println("score r=" + context.reader());
-    collector.setScorer(this);
-    if (drillDownCollector != null) {
-      drillDownCollector.setScorer(this);
-      drillDownCollector.setNextReader(context);
-    }
-    for(DocsEnumsAndFreq dim : dims) {
-      dim.sidewaysCollector.setScorer(this);
-      dim.sidewaysCollector.setNextReader(context);
-    }
-
-    // TODO: if we ever allow null baseScorer ... it will
-    // mean we DO score docs out of order ... hmm, or if we
-    // change up the order of the conjuntions below
-    assert baseScorer != null;
-
-    // Position all scorers to their first matching doc:
-    baseScorer.nextDoc();
-    for(DocsEnumsAndFreq dim : dims) {
-      for (DocsEnum docsEnum : dim.docsEnums) {
-        if (docsEnum != null) {
-          docsEnum.nextDoc();
-        }
-      }
-    }
-
-    final int numDims = dims.length;
-
-    DocsEnum[][] docsEnums = new DocsEnum[numDims][];
-    Collector[] sidewaysCollectors = new Collector[numDims];
-    long drillDownCost = 0;
-    for(int dim=0;dim<numDims;dim++) {
-      docsEnums[dim] = dims[dim].docsEnums;
-      sidewaysCollectors[dim] = dims[dim].sidewaysCollector;
-      for (DocsEnum de : dims[dim].docsEnums) {
-        if (de != null) {
-          drillDownCost += de.cost();
-        }
-      }
-    }
-
-    long baseQueryCost = baseScorer.cost();
-
-    /*
-    System.out.println("\nbaseDocID=" + baseScorer.docID() + " est=" + estBaseHitCount);
-    System.out.println("  maxDoc=" + context.reader().maxDoc());
-    System.out.println("  maxCost=" + maxCost);
-    System.out.println("  dims[0].freq=" + dims[0].freq);
-    if (numDims > 1) {
-      System.out.println("  dims[1].freq=" + dims[1].freq);
-    }
-    */
-
-    if (baseQueryCost < drillDownCost/10) {
-      //System.out.println("baseAdvance");
-      doBaseAdvanceScoring(collector, docsEnums, sidewaysCollectors);
-    } else if (numDims > 1 && (dims[1].maxCost < baseQueryCost/10)) {
-      //System.out.println("drillDownAdvance");
-      doDrillDownAdvanceScoring(collector, docsEnums, sidewaysCollectors);
-    } else {
-      //System.out.println("union");
-      doUnionScoring(collector, docsEnums, sidewaysCollectors);
-    }
-  }
-
-  /** Used when drill downs are highly constraining vs
-   *  baseQuery. */
-  private void doDrillDownAdvanceScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
-    final int maxDoc = context.reader().maxDoc();
-    final int numDims = dims.length;
-
-    //if (DEBUG) {
-    //  System.out.println("  doDrillDownAdvanceScoring");
-    //}
-
-    // TODO: maybe a class like BS, instead of parallel arrays
-    int[] filledSlots = new int[CHUNK];
-    int[] docIDs = new int[CHUNK];
-    float[] scores = new float[CHUNK];
-    int[] missingDims = new int[CHUNK];
-    int[] counts = new int[CHUNK];
-
-    docIDs[0] = -1;
-    int nextChunkStart = CHUNK;
-
-    final FixedBitSet seen = new FixedBitSet(CHUNK);
-
-    while (true) {
-      //if (DEBUG) {
-      //  System.out.println("\ncycle nextChunkStart=" + nextChunkStart + " docIds[0]=" + docIDs[0]);
-      //}
-
-      // First dim:
-      //if (DEBUG) {
-      //  System.out.println("  dim0");
-      //}
-      for(DocsEnum docsEnum : docsEnums[0]) {
-        if (docsEnum == null) {
-          continue;
-        }
-        int docID = docsEnum.docID();
-        while (docID < nextChunkStart) {
-          int slot = docID & MASK;
-
-          if (docIDs[slot] != docID) {
-            seen.set(slot);
-            // Mark slot as valid:
-            //if (DEBUG) {
-            //  System.out.println("    set docID=" + docID + " id=" + context.reader().document(docID).get("id"));
-            //}
-            docIDs[slot] = docID;
-            missingDims[slot] = 1;
-            counts[slot] = 1;
-          }
-
-          docID = docsEnum.nextDoc();
-        }
-      }
-
-      // Second dim:
-      //if (DEBUG) {
-      //  System.out.println("  dim1");
-      //}
-      for(DocsEnum docsEnum : docsEnums[1]) {
-        if (docsEnum == null) {
-          continue;
-        }
-        int docID = docsEnum.docID();
-        while (docID < nextChunkStart) {
-          int slot = docID & MASK;
-
-          if (docIDs[slot] != docID) {
-            // Mark slot as valid:
-            seen.set(slot);
-            //if (DEBUG) {
-            //  System.out.println("    set docID=" + docID + " missingDim=0 id=" + context.reader().document(docID).get("id"));
-            //}
-            docIDs[slot] = docID;
-            missingDims[slot] = 0;
-            counts[slot] = 1;
-          } else {
-            // TODO: single-valued dims will always be true
-            // below; we could somehow specialize
-            if (missingDims[slot] >= 1) {
-              missingDims[slot] = 2;
-              counts[slot] = 2;
-              //if (DEBUG) {
-              //  System.out.println("    set docID=" + docID + " missingDim=2 id=" + context.reader().document(docID).get("id"));
-              //}
-            } else {
-              counts[slot] = 1;
-              //if (DEBUG) {
-              //  System.out.println("    set docID=" + docID + " missingDim=" + missingDims[slot] + " id=" + context.reader().document(docID).get("id"));
-              //}
-            }
-          }
-
-          docID = docsEnum.nextDoc();
-        }
-      }
-
-      // After this we can "upgrade" to conjunction, because
-      // any doc not seen by either dim 0 or dim 1 cannot be
-      // a hit or a near miss:
-
-      //if (DEBUG) {
-      //  System.out.println("  baseScorer");
-      //}
-
-      // Fold in baseScorer, using advance:
-      int filledCount = 0;
-      int slot0 = 0;
-      while (slot0 < CHUNK && (slot0 = seen.nextSetBit(slot0)) != -1) {
-        int ddDocID = docIDs[slot0];
-        assert ddDocID != -1;
-
-        int baseDocID = baseScorer.docID();
-        if (baseDocID < ddDocID) {
-          baseDocID = baseScorer.advance(ddDocID);
-        }
-        if (baseDocID == ddDocID) {
-          //if (DEBUG) {
-          //  System.out.println("    keep docID=" + ddDocID + " id=" + context.reader().document(ddDocID).get("id"));
-          //}
-          scores[slot0] = baseScorer.score();
-          filledSlots[filledCount++] = slot0;
-          counts[slot0]++;
-        } else {
-          //if (DEBUG) {
-          //  System.out.println("    no docID=" + ddDocID + " id=" + context.reader().document(ddDocID).get("id"));
-          //}
-          docIDs[slot0] = -1;
-
-          // TODO: we could jump slot0 forward to the
-          // baseDocID ... but we'd need to set docIDs for
-          // intervening slots to -1
-        }
-        slot0++;
-      }
-      seen.clear(0, CHUNK);
-
-      if (filledCount == 0) {
-        if (nextChunkStart >= maxDoc) {
-          break;
-        }
-        nextChunkStart += CHUNK;
-        continue;
-      }
-      
-      // TODO: factor this out & share w/ union scorer,
-      // except we start from dim=2 instead:
-      for(int dim=2;dim<numDims;dim++) {
-        //if (DEBUG) {
-        //  System.out.println("  dim=" + dim + " [" + dims[dim].dim + "]");
-        //}
-        for(DocsEnum docsEnum : docsEnums[dim]) {
-          if (docsEnum == null) {
-            continue;
-          }
-          int docID = docsEnum.docID();
-          while (docID < nextChunkStart) {
-            int slot = docID & MASK;
-            if (docIDs[slot] == docID && counts[slot] >= dim) {
-              // TODO: single-valued dims will always be true
-              // below; we could somehow specialize
-              if (missingDims[slot] >= dim) {
-                //if (DEBUG) {
-                //  System.out.println("    set docID=" + docID + " count=" + (dim+2));
-                //}
-                missingDims[slot] = dim+1;
-                counts[slot] = dim+2;
-              } else {
-                //if (DEBUG) {
-                //  System.out.println("    set docID=" + docID + " missing count=" + (dim+1));
-                //}
-                counts[slot] = dim+1;
-              }
-            }
-            // TODO: sometimes use advance?
-            docID = docsEnum.nextDoc();
-          }
-        }
-      }
-
-      // Collect:
-      //if (DEBUG) {
-      //  System.out.println("  now collect: " + filledCount + " hits");
-      //}
-      for(int i=0;i<filledCount;i++) {
-        int slot = filledSlots[i];
-        collectDocID = docIDs[slot];
-        collectScore = scores[slot];
-        //if (DEBUG) {
-        //  System.out.println("    docID=" + docIDs[slot] + " count=" + counts[slot]);
-        //}
-        if (counts[slot] == 1+numDims) {
-          collectHit(collector, sidewaysCollectors);
-        } else if (counts[slot] == numDims) {
-          collectNearMiss(sidewaysCollectors, missingDims[slot]);
-        }
-      }
-
-      if (nextChunkStart >= maxDoc) {
-        break;
-      }
-
-      nextChunkStart += CHUNK;
-    }
-  }
-
-  /** Used when base query is highly constraining vs the
-   *  drilldowns; in this case we just .next() on base and
-   *  .advance() on the dims. */
-  private void doBaseAdvanceScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
-    //if (DEBUG) {
-    //  System.out.println("  doBaseAdvanceScoring");
-    //}
-    int docID = baseScorer.docID();
-
-    final int numDims = dims.length;
-
-    nextDoc: while (docID != NO_MORE_DOCS) {
-      int failedDim = -1;
-      for(int dim=0;dim<numDims;dim++) {
-        // TODO: should we sort this 2nd dimension of
-        // docsEnums from most frequent to least?
-        boolean found = false;
-        for(DocsEnum docsEnum : docsEnums[dim]) {
-          if (docsEnum == null) {
-            continue;
-          }
-          if (docsEnum.docID() < docID) {
-            docsEnum.advance(docID);
-          }
-          if (docsEnum.docID() == docID) {
-            found = true;
-            break;
-          }
-        }
-        if (!found) {
-          if (failedDim != -1) {
-            // More than one dim fails on this document, so
-            // it's neither a hit nor a near-miss; move to
-            // next doc:
-            docID = baseScorer.nextDoc();
-            continue nextDoc;
-          } else {
-            failedDim = dim;
-          }
-        }
-      }
-
-      collectDocID = docID;
-
-      // TODO: we could score on demand instead since we are
-      // daat here:
-      collectScore = baseScorer.score();
-
-      if (failedDim == -1) {
-        collectHit(collector, sidewaysCollectors);
-      } else {
-        collectNearMiss(sidewaysCollectors, failedDim);
-      }
-
-      docID = baseScorer.nextDoc();
-    }
-  }
-
-  private void collectHit(Collector collector, Collector[] sidewaysCollectors) throws IOException {
-    //if (DEBUG) {
-    //  System.out.println("      hit");
-    //}
-
-    collector.collect(collectDocID);
-    if (drillDownCollector != null) {
-      drillDownCollector.collect(collectDocID);
-    }
-
-    // TODO: we could "fix" faceting of the sideways counts
-    // to do this "union" (of the drill down hits) in the
-    // end instead:
-
-    // Tally sideways counts:
-    for(int dim=0;dim<sidewaysCollectors.length;dim++) {
-      sidewaysCollectors[dim].collect(collectDocID);
-    }
-  }
-
-  private void collectNearMiss(Collector[] sidewaysCollectors, int dim) throws IOException {
-    //if (DEBUG) {
-    //  System.out.println("      missingDim=" + dim);
-    //}
-    sidewaysCollectors[dim].collect(collectDocID);
-  }
-
-  private void doUnionScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
-    //if (DEBUG) {
-    //  System.out.println("  doUnionScoring");
-    //}
-
-    final int maxDoc = context.reader().maxDoc();
-    final int numDims = dims.length;
-
-    // TODO: maybe a class like BS, instead of parallel arrays
-    int[] filledSlots = new int[CHUNK];
-    int[] docIDs = new int[CHUNK];
-    float[] scores = new float[CHUNK];
-    int[] missingDims = new int[CHUNK];
-    int[] counts = new int[CHUNK];
-
-    docIDs[0] = -1;
-
-    // NOTE: this is basically a specialized version of
-    // BooleanScorer, to the minShouldMatch=N-1 case, but
-    // carefully tracking which dimension failed to match
-
-    int nextChunkStart = CHUNK;
-
-    while (true) {
-      //if (DEBUG) {
-      //  System.out.println("\ncycle nextChunkStart=" + nextChunkStart + " docIds[0]=" + docIDs[0]);
-      //}
-      int filledCount = 0;
-      int docID = baseScorer.docID();
-      //if (DEBUG) {
-      //  System.out.println("  base docID=" + docID);
-      //}
-      while (docID < nextChunkStart) {
-        int slot = docID & MASK;
-        //if (DEBUG) {
-        //  System.out.println("    docIDs[slot=" + slot + "]=" + docID + " id=" + context.reader().document(docID).get("id"));
-        //}
-
-        // Mark slot as valid:
-        assert docIDs[slot] != docID: "slot=" + slot + " docID=" + docID;
-        docIDs[slot] = docID;
-        scores[slot] = baseScorer.score();
-        filledSlots[filledCount++] = slot;
-        missingDims[slot] = 0;
-        counts[slot] = 1;
-
-        docID = baseScorer.nextDoc();
-      }
-
-      if (filledCount == 0) {
-        if (nextChunkStart >= maxDoc) {
-          break;
-        }
-        nextChunkStart += CHUNK;
-        continue;
-      }
-
-      // First drill-down dim, basically adds SHOULD onto
-      // the baseQuery:
-      //if (DEBUG) {
-      //  System.out.println("  dim=0 [" + dims[0].dim + "]");
-      //}
-      for(DocsEnum docsEnum : docsEnums[0]) {
-        if (docsEnum == null) {
-          continue;
-        }
-        docID = docsEnum.docID();
-        //if (DEBUG) {
-        //  System.out.println("    start docID=" + docID);
-        //}
-        while (docID < nextChunkStart) {
-          int slot = docID & MASK;
-          if (docIDs[slot] == docID) {
-            //if (DEBUG) {
-            //  System.out.println("      set docID=" + docID + " count=2");
-            //}
-            missingDims[slot] = 1;
-            counts[slot] = 2;
-          }
-          docID = docsEnum.nextDoc();
-        }
-      }
-
-      for(int dim=1;dim<numDims;dim++) {
-        //if (DEBUG) {
-        //  System.out.println("  dim=" + dim + " [" + dims[dim].dim + "]");
-        //}
-        for(DocsEnum docsEnum : docsEnums[dim]) {
-          if (docsEnum == null) {
-            continue;
-          }
-          docID = docsEnum.docID();
-          //if (DEBUG) {
-          //  System.out.println("    start docID=" + docID);
-          //}
-          while (docID < nextChunkStart) {
-            int slot = docID & MASK;
-            if (docIDs[slot] == docID && counts[slot] >= dim) {
-              // This doc is still in the running...
-              // TODO: single-valued dims will always be true
-              // below; we could somehow specialize
-              if (missingDims[slot] >= dim) {
-                //if (DEBUG) {
-                //  System.out.println("      set docID=" + docID + " count=" + (dim+2));
-                //}
-                missingDims[slot] = dim+1;
-                counts[slot] = dim+2;
-              } else {
-                //if (DEBUG) {
-                //  System.out.println("      set docID=" + docID + " missing count=" + (dim+1));
-                //}
-                counts[slot] = dim+1;
-              }
-            }
-            docID = docsEnum.nextDoc();
-          }
-
-          // TODO: sometimes use advance?
-
-          /*
-            int docBase = nextChunkStart - CHUNK;
-            for(int i=0;i<filledCount;i++) {
-              int slot = filledSlots[i];
-              docID = docBase + filledSlots[i];
-              if (docIDs[slot] == docID && counts[slot] >= dim) {
-                // This doc is still in the running...
-                int ddDocID = docsEnum.docID();
-                if (ddDocID < docID) {
-                  ddDocID = docsEnum.advance(docID);
-                }
-                if (ddDocID == docID) {
-                  if (missingDims[slot] >= dim && counts[slot] == allMatchCount) {
-                  //if (DEBUG) {
-                  //    System.out.println("    set docID=" + docID + " count=" + (dim+2));
-                   // }
-                    missingDims[slot] = dim+1;
-                    counts[slot] = dim+2;
-                  } else {
-                  //if (DEBUG) {
-                  //    System.out.println("    set docID=" + docID + " missing count=" + (dim+1));
-                   // }
-                    counts[slot] = dim+1;
-                  }
-                }
-              }
-            }            
-          */
-        }
-      }
-
-      // Collect:
-      //if (DEBUG) {
-      //  System.out.println("  now collect: " + filledCount + " hits");
-      //}
-      for(int i=0;i<filledCount;i++) {
-        // NOTE: This is actually in-order collection,
-        // because we only accept docs originally returned by
-        // the baseScorer (ie that Scorer is AND'd)
-        int slot = filledSlots[i];
-        collectDocID = docIDs[slot];
-        collectScore = scores[slot];
-        //if (DEBUG) {
-        //  System.out.println("    docID=" + docIDs[slot] + " count=" + counts[slot]);
-        //}
-        //System.out.println("  collect doc=" + collectDocID + " main.freq=" + (counts[slot]-1) + " main.doc=" + collectDocID + " exactCount=" + numDims);
-        if (counts[slot] == 1+numDims) {
-          //System.out.println("    hit");
-          collectHit(collector, sidewaysCollectors);
-        } else if (counts[slot] == numDims) {
-          //System.out.println("    sw");
-          collectNearMiss(sidewaysCollectors, missingDims[slot]);
-        }
-      }
-
-      if (nextChunkStart >= maxDoc) {
-        break;
-      }
-
-      nextChunkStart += CHUNK;
-    }
-  }
-
-  @Override
-  public int docID() {
-    return collectDocID;
-  }
-
-  @Override
-  public float score() {
-    return collectScore;
-  }
-
-  @Override
-  public int freq() {
-    return 1+dims.length;
-  }
-
-  @Override
-  public int nextDoc() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public int advance(int target) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public long cost() {
-    return baseScorer.cost();
-  }
-
-  @Override
-  public Collection<ChildScorer> getChildren() {
-    return Collections.singletonList(new ChildScorer(baseScorer, "MUST"));
-  }
-
-  static class DocsEnumsAndFreq implements Comparable<DocsEnumsAndFreq> {
-    DocsEnum[] docsEnums;
-    // Max cost for all docsEnums for this dim:
-    long maxCost;
-    Collector sidewaysCollector;
-    String dim;
-
-    @Override
-    public int compareTo(DocsEnumsAndFreq other) {
-      if (maxCost < other.maxCost) {
-        return -1;
-      } else if (maxCost > other.maxCost) {
-        return 1;
-      } else {
-        return 0;
-      }
-    }
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleFacetResult.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleFacetResult.java
deleted file mode 100644
index 58a1a22..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleFacetResult.java
+++ /dev/null
@@ -1,69 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-import java.util.List;
-
-public final class SimpleFacetResult {
-
-  /** Total value for this path (sum of all child counts, or
-   *  sum of all child values), even those not included in
-   *  the topN. */
-  public final Number value;
-
-  /** How many labels were populated under the requested
-   *  path. */
-  public final int childCount;
-
-  /** Child counts. */
-  public final LabelAndValue[] labelValues;
-
-  public SimpleFacetResult(Number value, LabelAndValue[] labelValues, int childCount) {
-    this.value = value;
-    this.labelValues = labelValues;
-    this.childCount = childCount;
-  }
-
-  @Override
-  public String toString() {
-    StringBuilder sb = new StringBuilder();
-    sb.append("value=");
-    sb.append(value);
-    sb.append(" childCount=");
-    sb.append(childCount);
-    sb.append('\n');
-    for(LabelAndValue labelValue : labelValues) {
-      sb.append("  " + labelValue + "\n");
-    }
-    return sb.toString();
-  }
-
-  @Override
-  public boolean equals(Object _other) {
-    if ((_other instanceof SimpleFacetResult) == false) {
-      return false;
-    }
-    SimpleFacetResult other = (SimpleFacetResult) _other;
-    return value.equals(other.value) &&
-      childCount == other.childCount &&
-      Arrays.equals(labelValues, other.labelValues);
-  }
-
-  // nocommit hashCode
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleFacetsCollector.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleFacetsCollector.java
deleted file mode 100644
index b17d6c0..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleFacetsCollector.java
+++ /dev/null
@@ -1,127 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.FixedBitSet;
-
-// nocommit javadocs
-public final class SimpleFacetsCollector extends Collector {
-
-  private AtomicReaderContext context;
-  private Scorer scorer;
-  private FixedBitSet bits;
-  private int totalHits;
-  private float[] scores;
-  private final boolean keepScores;
-  private final List<MatchingDocs> matchingDocs = new ArrayList<MatchingDocs>();
-  
-  /**
-   * Holds the documents that were matched in the {@link AtomicReaderContext}.
-   * If scores were required, then {@code scores} is not null.
-   */
-  public final static class MatchingDocs {
-    
-    public final AtomicReaderContext context;
-    public final FixedBitSet bits;
-    public final float[] scores;
-    public final int totalHits;
-    
-    public MatchingDocs(AtomicReaderContext context, FixedBitSet bits, int totalHits, float[] scores) {
-      this.context = context;
-      this.bits = bits;
-      this.scores = scores;
-      this.totalHits = totalHits;
-    }
-  }
-
-  public SimpleFacetsCollector() {
-    this(false);
-  }
-
-  public SimpleFacetsCollector(boolean keepScores) {
-    this.keepScores = keepScores;
-  }
-
-  public boolean getKeepScores() {
-    return keepScores;
-  }
-  
-  /**
-   * Returns the documents matched by the query, one {@link MatchingDocs} per
-   * visited segment.
-   */
-  public List<MatchingDocs> getMatchingDocs() {
-    if (bits != null) {
-      matchingDocs.add(new MatchingDocs(this.context, bits, totalHits, scores));
-      bits = null;
-      scores = null;
-      context = null;
-    }
-
-    return matchingDocs;
-  }
-    
-  @Override
-  public final boolean acceptsDocsOutOfOrder() {
-    // nocommit why not true?
-    return false;
-  }
-
-  @Override
-  public final void collect(int doc) throws IOException {
-    bits.set(doc);
-    if (keepScores) {
-      if (totalHits >= scores.length) {
-        float[] newScores = new float[ArrayUtil.oversize(totalHits + 1, 4)];
-        System.arraycopy(scores, 0, newScores, 0, totalHits);
-        scores = newScores;
-      }
-      scores[totalHits] = scorer.score();
-    }
-    totalHits++;
-  }
-
-  @Override
-  public final void setScorer(Scorer scorer) throws IOException {
-    this.scorer = scorer;
-  }
-    
-  @Override
-  public final void setNextReader(AtomicReaderContext context) throws IOException {
-    if (bits != null) {
-      matchingDocs.add(new MatchingDocs(this.context, bits, totalHits, scores));
-    }
-    bits = new FixedBitSet(context.reader().maxDoc());
-    totalHits = 0;
-    if (keepScores) {
-      scores = new float[64]; // some initial size
-    }
-    this.context = context;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts.java
deleted file mode 100644
index 112ac2a..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetCounts.java
+++ /dev/null
@@ -1,296 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.simple.SimpleFacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.simple.SortedSetDocValuesReaderState.OrdRange;
-import org.apache.lucene.facet.simple.SortedSetDocValuesReaderState;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiDocValues.MultiSortedSetDocValues;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.ReaderUtil;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.PriorityQueue;
-
-/** Compute facets counts from previously
- *  indexed {@link SortedSetDocValuesFacetField},
- *  without require a separate taxonomy index.  Faceting is
- *  a bit slower (~25%), and there is added cost on every
- *  {@link IndexReader} open to create a new {@link
- *  SortedSetDocValuesReaderState}.  Furthermore, this does
- *  not support hierarchical facets; only flat (dimension +
- *  label) facets, but it uses quite a bit less RAM to do
- *  so.
- *
- *  After creating this class, invoke {@link #getDim} or
- *  {@link #getAllDims} to retrieve facet results. */
-
-public class SortedSetDocValuesFacetCounts extends Facets {
-
-  final SortedSetDocValuesReaderState state;
-  final SortedSetDocValues dv;
-  final String field;
-  final int[] counts;
-
-  /** Sparse faceting: returns any dimension that had any
-   *  hits, topCount labels per dimension. */
-  public SortedSetDocValuesFacetCounts(SortedSetDocValuesReaderState state, SimpleFacetsCollector hits)
-      throws IOException {
-    this.state = state;
-    this.field = state.getField();
-    counts = new int[state.getSize()];
-    dv = state.getDocValues();
-    //System.out.println("field=" + field);
-    count(hits.getMatchingDocs());
-  }
-
-  @Override
-  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    if (topN <= 0) {
-      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
-    }
-    if (path.length > 0) {
-      throw new IllegalArgumentException("path should be 0 length");
-    }
-    OrdRange ordRange = state.getOrdRange(dim);
-    if (ordRange == null) {
-      throw new IllegalArgumentException("dimension \"" + dim + "\" was not indexed");
-    }
-    return getDim(dim, ordRange, topN);
-  }
-
-  private final SimpleFacetResult getDim(String dim, OrdRange ordRange, int topN) {
-
-    TopOrdAndIntQueue q = null;
-
-    int bottomCount = 0;
-
-    int dimCount = 0;
-    int childCount = 0;
-
-    TopOrdAndIntQueue.OrdAndValue reuse = null;
-    //System.out.println("getDim : " + ordRange.start + " - " + ordRange.end);
-    for(int ord=ordRange.start; ord<=ordRange.end; ord++) {
-      //System.out.println("  ord=" + ord + " count=" + counts[ord]);
-      if (counts[ord] > 0) {
-        dimCount += counts[ord];
-        childCount++;
-        if (counts[ord] > bottomCount) {
-          if (reuse == null) {
-            reuse = new TopOrdAndIntQueue.OrdAndValue();
-          }
-          reuse.ord = ord;
-          reuse.value = counts[ord];
-          if (q == null) {
-            // Lazy init, so we don't create this for the
-            // sparse case unnecessarily
-            q = new TopOrdAndIntQueue(topN);
-          }
-          reuse = q.insertWithOverflow(reuse);
-          if (q.size() == topN) {
-            bottomCount = q.top().value;
-          }
-        }
-      }
-    }
-
-    if (q == null) {
-      return null;
-    }
-
-    BytesRef scratch = new BytesRef();
-
-    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
-    for(int i=labelValues.length-1;i>=0;i--) {
-      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
-      dv.lookupOrd(ordAndValue.ord, scratch);
-      String[] parts = FacetsConfig.stringToPath(scratch.utf8ToString());
-      labelValues[i] = new LabelAndValue(parts[1], ordAndValue.value);
-    }
-
-    return new SimpleFacetResult(dimCount, labelValues, childCount);
-  }
-
-  /** Does all the "real work" of tallying up the counts. */
-  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
-    //System.out.println("ssdv count");
-
-    MultiDocValues.OrdinalMap ordinalMap;
-
-    // nocommit not quite right?  really, we need a way to
-    // verify that this ordinalMap "matches" the leaves in
-    // matchingDocs...
-    if (dv instanceof MultiSortedSetDocValues && matchingDocs.size() > 1) {
-      ordinalMap = ((MultiSortedSetDocValues) dv).mapping;
-    } else {
-      ordinalMap = null;
-    }
-
-    for(MatchingDocs hits : matchingDocs) {
-
-      AtomicReader reader = hits.context.reader();
-      //System.out.println("  reader=" + reader);
-      // LUCENE-5090: make sure the provided reader context "matches"
-      // the top-level reader passed to the
-      // SortedSetDocValuesReaderState, else cryptic
-      // AIOOBE can happen:
-      if (ReaderUtil.getTopLevelContext(hits.context).reader() != state.origReader) {
-        throw new IllegalStateException("the SortedSetDocValuesReaderState provided to this class does not match the reader being searched; you must create a new SortedSetDocValuesReaderState every time you open a new IndexReader");
-      }
-      
-      SortedSetDocValues segValues = reader.getSortedSetDocValues(field);
-      if (segValues == null) {
-        // nocommit in trunk this was a "return" which is
-        // wrong; make a failing test
-        continue;
-      }
-
-      final int maxDoc = reader.maxDoc();
-      assert maxDoc == hits.bits.length();
-      //System.out.println("  dv=" + dv);
-
-      // nocommit, yet another option is to count all segs
-      // first, only in seg-ord space, and then do a
-      // merge-sort-PQ in the end to only "resolve to
-      // global" those seg ords that can compete, if we know
-      // we just want top K?  ie, this is the same algo
-      // that'd be used for merging facets across shards
-      // (distributed faceting).  but this has much higher
-      // temp ram req'ts (sum of number of ords across all
-      // segs)
-      if (ordinalMap != null) {
-        int segOrd = hits.context.ord;
-
-        int numSegOrds = (int) segValues.getValueCount();
-
-        if (hits.totalHits < numSegOrds/10) {
-          //System.out.println("    remap as-we-go");
-          // Remap every ord to global ord as we iterate:
-          int doc = 0;
-          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
-            //System.out.println("    doc=" + doc);
-            segValues.setDocument(doc);
-            int term = (int) segValues.nextOrd();
-            while (term != SortedSetDocValues.NO_MORE_ORDS) {
-              //System.out.println("      segOrd=" + segOrd + " ord=" + term + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, term));
-              counts[(int) ordinalMap.getGlobalOrd(segOrd, term)]++;
-              term = (int) segValues.nextOrd();
-            }
-            ++doc;
-          }
-        } else {
-          //System.out.println("    count in seg ord first");
-
-          // First count in seg-ord space:
-          final int[] segCounts = new int[numSegOrds];
-          int doc = 0;
-          while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
-            //System.out.println("    doc=" + doc);
-            segValues.setDocument(doc);
-            int term = (int) segValues.nextOrd();
-            while (term != SortedSetDocValues.NO_MORE_ORDS) {
-              //System.out.println("      ord=" + term);
-              segCounts[term]++;
-              term = (int) segValues.nextOrd();
-            }
-            ++doc;
-          }
-
-          // Then, migrate to global ords:
-          for(int ord=0;ord<numSegOrds;ord++) {
-            int count = segCounts[ord];
-            if (count != 0) {
-              //System.out.println("    migrate segOrd=" + segOrd + " ord=" + ord + " globalOrd=" + ordinalMap.getGlobalOrd(segOrd, ord));
-              counts[(int) ordinalMap.getGlobalOrd(segOrd, ord)] += count;
-            }
-          }
-        }
-      } else {
-        // No ord mapping (e.g., single segment index):
-        // just aggregate directly into counts:
-
-        int doc = 0;
-        while (doc < maxDoc && (doc = hits.bits.nextSetBit(doc)) != -1) {
-          segValues.setDocument(doc);
-          int term = (int) segValues.nextOrd();
-          while (term != SortedSetDocValues.NO_MORE_ORDS) {
-            counts[term]++;
-            term = (int) segValues.nextOrd();
-          }
-          ++doc;
-        }
-      }
-    }
-  }
-
-  @Override
-  public Number getSpecificValue(String dim, String... path) {
-    if (path.length != 1) {
-      throw new IllegalArgumentException("path must be length=1");
-    }
-    // nocommit this is not thread safe in general?  add
-    // jdocs that app must instantiate & use from same thread?
-    int ord = (int) dv.lookupTerm(new BytesRef(FacetsConfig.pathToString(dim, path)));
-    if (ord < 0) {
-      return -1;
-    }
-
-    return counts[ord];
-  }
-
-  @Override
-  public List<SimpleFacetResult> getAllDims(int topN) throws IOException {
-
-    List<SimpleFacetResult> results = new ArrayList<SimpleFacetResult>();
-    for(Map.Entry<String,OrdRange> ent : state.getPrefixToOrdRange().entrySet()) {
-      SimpleFacetResult fr = getDim(ent.getKey(), ent.getValue(), topN);
-      if (fr != null) {
-        results.add(fr);
-      }
-    }
-
-    // Sort by highest count:
-    Collections.sort(results,
-                     new Comparator<SimpleFacetResult>() {
-                       @Override
-                       public int compare(SimpleFacetResult a, SimpleFacetResult b) {
-                         if (a.value.intValue() > b.value.intValue()) {
-                           return -1;
-                         } else if (b.value.intValue() > a.value.intValue()) {
-                           return 1;
-                         } else {
-                           return 0;
-                         }
-                       }
-                     });
-
-    return results;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetField.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetField.java
deleted file mode 100644
index 098bda3..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesFacetField.java
+++ /dev/null
@@ -1,46 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-
-/** Add an instance of this to your Document for every facet
- *  label to be indexed via SortedSetDocValues. */
-public class SortedSetDocValuesFacetField extends Field {
-  static final FieldType TYPE = new FieldType();
-  static {
-    TYPE.setIndexed(true);
-    TYPE.freeze();
-  }
-  final String dim;
-  final String label;
-
-  public SortedSetDocValuesFacetField(String dim, String label) {
-    super("dummy", TYPE);
-    this.dim = dim;
-    this.label = label;
-  }
-
-  @Override
-  public String toString() {
-    return "SortedSetDocValuesFacetField(dim=" + dim + " label=" + label + ")";
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesReaderState.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesReaderState.java
deleted file mode 100644
index 1ebc049..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SortedSetDocValuesReaderState.java
+++ /dev/null
@@ -1,148 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.regex.Pattern;
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.CompositeReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.index.SortedSetDocValues;
-import org.apache.lucene.util.BytesRef;
-
-/** Wraps a {@link IndexReader} and resolves ords
- *  using existing {@link SortedSetDocValues} APIs without a
- *  separate taxonomy index.  This only supports flat facets
- *  (dimension + label), and it makes faceting a bit
- *  slower, adds some cost at reopen time, but avoids
- *  managing the separate taxonomy index.  It also requires
- *  less RAM than the taxonomy index, as it manages the flat
- *  (2-level) hierarchy more efficiently.  In addition, the
- *  tie-break during faceting is now meaningful (in label
- *  sorted order).
- *
- *  <p><b>NOTE</b>: creating an instance of this class is
- *  somewhat costly, as it computes per-segment ordinal maps,
- *  so you should create it once and re-use that one instance
- *  for a given {@link IndexReader}. */
-
-public final class SortedSetDocValuesReaderState {
-
-  private final String field;
-  private final AtomicReader topReader;
-  private final int valueCount;
-  public final IndexReader origReader;
-
-  /** Holds start/end range of ords, which maps to one
-   *  dimension (someday we may generalize it to map to
-   *  hierarchies within one dimension). */
-  public static final class OrdRange {
-    /** Start of range, inclusive: */
-    public final int start;
-    /** End of range, inclusive: */
-    public final int end;
-
-    /** Start and end are inclusive. */
-    public OrdRange(int start, int end) {
-      this.start = start;
-      this.end = end;
-    }
-  }
-
-  private final Map<String,OrdRange> prefixToOrdRange = new HashMap<String,OrdRange>();
-
-  public SortedSetDocValuesReaderState(IndexReader reader) throws IOException {
-    this(reader, FacetsConfig.DEFAULT_INDEX_FIELD_NAME);
-  }
-
-  /** Create an instance, scanning the {@link
-   *  SortedSetDocValues} from the provided reader, with
-   *  default {@link FacetIndexingParams}. */
-  public SortedSetDocValuesReaderState(IndexReader reader, String field) throws IOException {
-
-    this.field = field;
-    this.origReader = reader;
-
-    // We need this to create thread-safe MultiSortedSetDV
-    // per collector:
-    topReader = SlowCompositeReaderWrapper.wrap(reader);
-    SortedSetDocValues dv = topReader.getSortedSetDocValues(field);
-    if (dv == null) {
-      throw new IllegalArgumentException("field \"" + field + "\" was not indexed with SortedSetDocValues");
-    }
-    if (dv.getValueCount() > Integer.MAX_VALUE) {
-      throw new IllegalArgumentException("can only handle valueCount < Integer.MAX_VALUE; got " + dv.getValueCount());
-    }
-    valueCount = (int) dv.getValueCount();
-
-    // TODO: we can make this more efficient if eg we can be
-    // "involved" when OrdinalMap is being created?  Ie see
-    // each term/ord it's assigning as it goes...
-    String lastDim = null;
-    int startOrd = -1;
-    BytesRef spare = new BytesRef();
-
-    // TODO: this approach can work for full hierarchy?;
-    // TaxoReader can't do this since ords are not in
-    // "sorted order" ... but we should generalize this to
-    // support arbitrary hierarchy:
-    for(int ord=0;ord<valueCount;ord++) {
-      dv.lookupOrd(ord, spare);
-      String[] components = FacetsConfig.stringToPath(spare.utf8ToString());
-      if (components.length != 2) {
-        throw new IllegalArgumentException("this class can only handle 2 level hierarchy (dim/value); got: " + Arrays.toString(components) + " " + spare.utf8ToString());
-      }
-      if (!components[0].equals(lastDim)) {
-        if (lastDim != null) {
-          prefixToOrdRange.put(lastDim, new OrdRange(startOrd, ord-1));
-        }
-        startOrd = ord;
-        lastDim = components[0];
-      }
-    }
-
-    if (lastDim != null) {
-      prefixToOrdRange.put(lastDim, new OrdRange(startOrd, valueCount-1));
-    }
-  }
-
-  public SortedSetDocValues getDocValues() throws IOException {
-    return topReader.getSortedSetDocValues(field);
-  }
-
-  public Map<String,OrdRange> getPrefixToOrdRange() {
-    return prefixToOrdRange;
-  }
-
-  public OrdRange getOrdRange(String dim) {
-    return prefixToOrdRange.get(dim);
-  }
-
-  public String getField() {
-    return field;
-  }
-
-  public int getSize() {
-    return valueCount;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetCounts.java
deleted file mode 100644
index 84ee692..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetCounts.java
+++ /dev/null
@@ -1,170 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.simple.SimpleFacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.IntsRef;
-
-/** Reads from any {@link OrdinalsReader}; use {@link
- *  FastTaxonomyFacetCounts} if you are just using the
- *  default encoding from {@link BinaryDocValues}. */
-
-// nocommit remove & add specialized Cached variation only?
-public class TaxonomyFacetCounts extends TaxonomyFacets {
-  private final OrdinalsReader ordinalsReader;
-  private final int[] counts;
-
-  public TaxonomyFacetCounts(OrdinalsReader ordinalsReader, TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
-    super(ordinalsReader.getIndexFieldName(), taxoReader, config);
-    this.ordinalsReader = ordinalsReader;
-    counts = new int[taxoReader.getSize()];
-    count(fc.getMatchingDocs());
-  }
-
-  private final void count(List<MatchingDocs> matchingDocs) throws IOException {
-    IntsRef scratch  = new IntsRef();
-    for(MatchingDocs hits : matchingDocs) {
-      OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        ords.get(doc, scratch);
-        for(int i=0;i<scratch.length;i++) {
-          counts[scratch.ints[scratch.offset+i]]++;
-        }
-        ++doc;
-      }
-    }
-
-    // nocommit we could do this lazily instead:
-
-    // Rollup any necessary dims:
-    for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
-      String dim = ent.getKey();
-      FacetsConfig.DimConfig ft = ent.getValue();
-      if (ft.hierarchical && ft.multiValued == false) {
-        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
-        // It can be -1 if this field was declared in the
-        // config but never indexed:
-        if (dimRootOrd > 0) {
-          counts[dimRootOrd] += rollup(children[dimRootOrd]);
-        }
-      }
-    }
-  }
-
-  private int rollup(int ord) {
-    int sum = 0;
-    while (ord != TaxonomyReader.INVALID_ORDINAL) {
-      int childValue = counts[ord] + rollup(children[ord]);
-      counts[ord] = childValue;
-      sum += childValue;
-      ord = siblings[ord];
-    }
-    return sum;
-  }
-
-  /** Return the count for a specific path.  Returns -1 if
-   *  this path doesn't exist, else the count. */
-  @Override
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    verifyDim(dim);
-    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
-    if (ord < 0) {
-      return -1;
-    }
-    return counts[ord];
-  }
-
-  @Override
-  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    if (topN <= 0) {
-      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
-    }
-    FacetsConfig.DimConfig dimConfig = verifyDim(dim);
-    FacetLabel cp = FacetLabel.create(dim, path);
-    int dimOrd = taxoReader.getOrdinal(cp);
-    if (dimOrd == -1) {
-      //System.out.println("no ord for path=" + path);
-      return null;
-    }
-
-    TopOrdAndIntQueue q = new TopOrdAndIntQueue(Math.min(taxoReader.getSize(), topN));
-    
-    int bottomCount = 0;
-
-    int ord = children[dimOrd];
-    int totCount = 0;
-    int childCount = 0;
-
-    TopOrdAndIntQueue.OrdAndValue reuse = null;
-    while(ord != TaxonomyReader.INVALID_ORDINAL) {
-      if (counts[ord] > 0) {
-        totCount += counts[ord];
-        childCount++;
-        if (counts[ord] > bottomCount) {
-          if (reuse == null) {
-            reuse = new TopOrdAndIntQueue.OrdAndValue();
-          }
-          reuse.ord = ord;
-          reuse.value = counts[ord];
-          reuse = q.insertWithOverflow(reuse);
-          if (q.size() == topN) {
-            bottomCount = q.top().value;
-          }
-        }
-      }
-
-      ord = siblings[ord];
-    }
-
-    if (totCount == 0) {
-      return null;
-    }
-
-    if (dimConfig.multiValued) {
-      if (dimConfig.requireDimCount) {
-        totCount = counts[dimOrd];
-      } else {
-        // Our sum'd count is not correct, in general:
-        totCount = -1;
-      }
-    } else {
-      // Our sum'd dim count is accurate, so we keep it
-    }
-
-    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
-    for(int i=labelValues.length-1;i>=0;i--) {
-      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
-      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
-      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
-    }
-
-    return new SimpleFacetResult(totCount, labelValues, childCount);
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumFloatAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumFloatAssociations.java
deleted file mode 100644
index 337efcb..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumFloatAssociations.java
+++ /dev/null
@@ -1,149 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.facet.simple.SimpleFacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-
-// nocommit jdoc that this assumes/requires the default encoding
-public class TaxonomyFacetSumFloatAssociations extends TaxonomyFacets {
-  private final float[] values;
-
-  public TaxonomyFacetSumFloatAssociations(TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
-    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
-  }
-
-  public TaxonomyFacetSumFloatAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
-    super(indexFieldName, taxoReader, config);
-    values = new float[taxoReader.getSize()];
-    sumValues(fc.getMatchingDocs());
-  }
-
-  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
-    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
-    for(MatchingDocs hits : matchingDocs) {
-      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
-      if (dv == null) { // this reader does not have DocValues for the requested category list
-        continue;
-      }
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      BytesRef scratch = new BytesRef();
-      //System.out.println("count seg=" + hits.context.reader());
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        //System.out.println("  doc=" + doc);
-        // nocommit use OrdinalsReader?  but, add a
-        // BytesRef getAssociation()?
-        dv.get(doc, scratch);
-        byte[] bytes = scratch.bytes;
-        int end = scratch.offset + scratch.length;
-        int offset = scratch.offset;
-        while (offset < end) {
-          int ord = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          int value = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          values[ord] += Float.intBitsToFloat(value);
-        }
-        ++doc;
-      }
-    }
-  }
-
-  /** Return the count for a specific path.  Returns -1 if
-   *  this path doesn't exist, else the count. */
-  @Override
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    verifyDim(dim);
-    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
-    if (ord < 0) {
-      return -1;
-    }
-    return values[ord];
-  }
-
-  @Override
-  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    if (topN <= 0) {
-      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
-    }
-    FacetsConfig.DimConfig dimConfig = verifyDim(dim);
-    FacetLabel cp = FacetLabel.create(dim, path);
-    int dimOrd = taxoReader.getOrdinal(cp);
-    if (dimOrd == -1) {
-      //System.out.println("no ord for path=" + path);
-      return null;
-    }
-
-    TopOrdAndFloatQueue q = new TopOrdAndFloatQueue(Math.min(taxoReader.getSize(), topN));
-    float bottomValue = 0;
-
-    int ord = children[dimOrd];
-    float sumValue = 0;
-    int childCount = 0;
-    TopOrdAndFloatQueue.OrdAndValue reuse = null;
-    while(ord != TaxonomyReader.INVALID_ORDINAL) {
-      if (values[ord] > 0) {
-        sumValue += values[ord];
-        childCount++;
-        if (values[ord] > bottomValue) {
-          if (reuse == null) {
-            reuse = new TopOrdAndFloatQueue.OrdAndValue();
-          }
-          reuse.ord = ord;
-          reuse.value = values[ord];
-          reuse = q.insertWithOverflow(reuse);
-          if (q.size() == topN) {
-            bottomValue = q.top().value;
-          }
-        }
-      }
-
-      ord = siblings[ord];
-    }
-
-    if (sumValue == 0) {
-      //System.out.println("totCount=0 for path=" + path);
-      return null;
-    }
-
-    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
-    for(int i=labelValues.length-1;i>=0;i--) {
-      TopOrdAndFloatQueue.OrdAndValue ordAndValue = q.pop();
-      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
-      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
-    }
-
-    return new SimpleFacetResult(sumValue, labelValues, childCount);
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumIntAssociations.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumIntAssociations.java
deleted file mode 100644
index f6a833e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumIntAssociations.java
+++ /dev/null
@@ -1,150 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.facet.simple.SimpleFacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-
-// nocommit jdoc that this assumes/requires the default encoding
-public class TaxonomyFacetSumIntAssociations extends TaxonomyFacets {
-  private final int[] values;
-
-  public TaxonomyFacetSumIntAssociations(TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
-    this(FacetsConfig.DEFAULT_INDEX_FIELD_NAME, taxoReader, config, fc);
-  }
-
-  public TaxonomyFacetSumIntAssociations(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector fc) throws IOException {
-    super(indexFieldName, taxoReader, config);
-    values = new int[taxoReader.getSize()];
-    sumValues(fc.getMatchingDocs());
-  }
-
-  private final void sumValues(List<MatchingDocs> matchingDocs) throws IOException {
-    //System.out.println("count matchingDocs=" + matchingDocs + " facetsField=" + facetsFieldName);
-    for(MatchingDocs hits : matchingDocs) {
-      BinaryDocValues dv = hits.context.reader().getBinaryDocValues(indexFieldName);
-      if (dv == null) { // this reader does not have DocValues for the requested category list
-        continue;
-      }
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      BytesRef scratch = new BytesRef();
-      //System.out.println("count seg=" + hits.context.reader());
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        //System.out.println("  doc=" + doc);
-        // nocommit use OrdinalsReader?  but, add a
-        // BytesRef getAssociation()?
-        dv.get(doc, scratch);
-        byte[] bytes = scratch.bytes;
-        int end = scratch.offset + scratch.length;
-        int offset = scratch.offset;
-        while (offset < end) {
-          int ord = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          int value = ((bytes[offset]&0xFF) << 24) |
-            ((bytes[offset+1]&0xFF) << 16) |
-            ((bytes[offset+2]&0xFF) << 8) |
-            (bytes[offset+3]&0xFF);
-          offset += 4;
-          values[ord] += value;
-        }
-        ++doc;
-      }
-    }
-  }
-
-  /** Return the count for a specific path.  Returns -1 if
-   *  this path doesn't exist, else the count. */
-  @Override
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    verifyDim(dim);
-    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
-    if (ord < 0) {
-      return -1;
-    }
-    return values[ord];
-  }
-
-  @Override
-  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    if (topN <= 0) {
-      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
-    }
-    verifyDim(dim);
-    FacetLabel cp = FacetLabel.create(dim, path);
-    int dimOrd = taxoReader.getOrdinal(cp);
-    if (dimOrd == -1) {
-      //System.out.println("no ord for path=" + path);
-      return null;
-    }
-
-    TopOrdAndIntQueue q = new TopOrdAndIntQueue(Math.min(taxoReader.getSize(), topN));
-    int bottomValue = 0;
-
-    int ord = children[dimOrd];
-    long sumValue = 0;
-    int childCount = 0;
-
-    TopOrdAndIntQueue.OrdAndValue reuse = null;
-    while(ord != TaxonomyReader.INVALID_ORDINAL) {
-      if (values[ord] > 0) {
-        sumValue += values[ord];
-        childCount++;
-        if (values[ord] > bottomValue) {
-          if (reuse == null) {
-            reuse = new TopOrdAndIntQueue.OrdAndValue();
-          }
-          reuse.ord = ord;
-          reuse.value = values[ord];
-          reuse = q.insertWithOverflow(reuse);
-          if (q.size() == topN) {
-            bottomValue = q.top().value;
-          }
-        }
-      }
-
-      ord = siblings[ord];
-    }
-
-    if (sumValue == 0) {
-      //System.out.println("totCount=0 for path=" + path);
-      return null;
-    }
-
-    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
-    for(int i=labelValues.length-1;i>=0;i--) {
-      TopOrdAndIntQueue.OrdAndValue ordAndValue = q.pop();
-      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
-      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
-    }
-
-    return new SimpleFacetResult(sumValue, labelValues, childCount);
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumValueSource.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumValueSource.java
deleted file mode 100644
index 9732da8..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacetSumValueSource.java
+++ /dev/null
@@ -1,235 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.facet.simple.SimpleFacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.IntsRef;
-
-/** Aggregates sum of values from a {@link ValueSource}, for
- *  each facet label. */
-
-public class TaxonomyFacetSumValueSource extends TaxonomyFacets {
-  private final float[] values;
-  private final OrdinalsReader ordinalsReader;
-
-  /** Aggreggates float facet values from the provided
-   *  {@link ValueSource}, pulling ordinals using {@link
-   *  DocValuesOrdinalsReader} against the default indexed
-   *  facet field {@link
-   *  FacetsConfig#DEFAULT_INDEX_FIELD_NAME}. */
-  public TaxonomyFacetSumValueSource(TaxonomyReader taxoReader, FacetsConfig config,
-                                     SimpleFacetsCollector fc, ValueSource valueSource) throws IOException {
-    this(new DocValuesOrdinalsReader(FacetsConfig.DEFAULT_INDEX_FIELD_NAME), taxoReader, config, fc, valueSource);
-  }
-
-  /** Aggreggates float facet values from the provided
-   *  {@link ValueSource}, and pulls ordinals from the
-   *  provided {@link OrdinalsReader}. */
-  public TaxonomyFacetSumValueSource(OrdinalsReader ordinalsReader, TaxonomyReader taxoReader,
-                                     FacetsConfig config, SimpleFacetsCollector fc, ValueSource valueSource) throws IOException {
-    super(ordinalsReader.getIndexFieldName(), taxoReader, config);
-    this.ordinalsReader = ordinalsReader;
-    values = new float[taxoReader.getSize()];
-    sumValues(fc.getMatchingDocs(), fc.getKeepScores(), valueSource);
-  }
-
-  private static final class FakeScorer extends Scorer {
-    float score;
-    int docID;
-    FakeScorer() { super(null); }
-    @Override public float score() throws IOException { return score; }
-    @Override public int freq() throws IOException { throw new UnsupportedOperationException(); }
-    @Override public int docID() { return docID; }
-    @Override public int nextDoc() throws IOException { throw new UnsupportedOperationException(); }
-    @Override public int advance(int target) throws IOException { throw new UnsupportedOperationException(); }
-    @Override public long cost() { return 0; }
-  }
-
-  private final void sumValues(List<MatchingDocs> matchingDocs, boolean keepScores, ValueSource valueSource) throws IOException {
-    final FakeScorer scorer = new FakeScorer();
-    Map<String, Scorer> context = new HashMap<String, Scorer>();
-    if (keepScores) {
-      context.put("scorer", scorer);
-    }
-    IntsRef scratch = new IntsRef();
-    for(MatchingDocs hits : matchingDocs) {
-      OrdinalsReader.OrdinalsSegmentReader ords = ordinalsReader.getReader(hits.context);
-      FixedBitSet bits = hits.bits;
-    
-      final int length = hits.bits.length();
-      int doc = 0;
-      int scoresIdx = 0;
-      float[] scores = hits.scores;
-
-      FunctionValues functionValues = valueSource.getValues(context, hits.context);
-      while (doc < length && (doc = bits.nextSetBit(doc)) != -1) {
-        ords.get(doc, scratch);
-        if (keepScores) {
-          scorer.docID = doc;
-          scorer.score = scores[scoresIdx++];
-        }
-        float value = (float) functionValues.doubleVal(doc);
-        for(int i=0;i<scratch.length;i++) {
-          values[scratch.ints[i]] += value;
-        }
-        ++doc;
-      }
-    }
-
-    // nocommit we could do this lazily instead:
-
-    // Rollup any necessary dims:
-    for(Map.Entry<String,FacetsConfig.DimConfig> ent : config.getDimConfigs().entrySet()) {
-      String dim = ent.getKey();
-      FacetsConfig.DimConfig ft = ent.getValue();
-      if (ft.hierarchical && ft.multiValued == false) {
-        int dimRootOrd = taxoReader.getOrdinal(new FacetLabel(dim));
-        assert dimRootOrd > 0;
-        values[dimRootOrd] += rollup(children[dimRootOrd]);
-      }
-    }
-  }
-
-  private float rollup(int ord) {
-    float sum = 0;
-    while (ord != TaxonomyReader.INVALID_ORDINAL) {
-      float childValue = values[ord] + rollup(children[ord]);
-      values[ord] = childValue;
-      sum += childValue;
-      ord = siblings[ord];
-    }
-    return sum;
-  }
-
-  @Override
-  public Number getSpecificValue(String dim, String... path) throws IOException {
-    verifyDim(dim);
-    int ord = taxoReader.getOrdinal(FacetLabel.create(dim, path));
-    if (ord < 0) {
-      return -1;
-    }
-    return values[ord];
-  }
-
-  @Override
-  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
-    if (topN <= 0) {
-      throw new IllegalArgumentException("topN must be > 0 (got: " + topN + ")");
-    }
-    FacetsConfig.DimConfig dimConfig = verifyDim(dim);
-    FacetLabel cp = FacetLabel.create(dim, path);
-    int dimOrd = taxoReader.getOrdinal(cp);
-    if (dimOrd == -1) {
-      System.out.println("  no dim ord " + dim);
-      return null;
-    }
-
-    TopOrdAndFloatQueue q = new TopOrdAndFloatQueue(Math.min(taxoReader.getSize(), topN));
-    float bottomValue = 0;
-
-    int ord = children[dimOrd];
-    float sumValues = 0;
-    int childCount = 0;
-
-    TopOrdAndFloatQueue.OrdAndValue reuse = null;
-    while(ord != TaxonomyReader.INVALID_ORDINAL) {
-      if (values[ord] > 0) {
-        sumValues += values[ord];
-        childCount++;
-        if (values[ord] > bottomValue) {
-          if (reuse == null) {
-            reuse = new TopOrdAndFloatQueue.OrdAndValue();
-          }
-          reuse.ord = ord;
-          reuse.value = values[ord];
-          reuse = q.insertWithOverflow(reuse);
-          if (q.size() == topN) {
-            bottomValue = q.top().value;
-          }
-        }
-      }
-
-      ord = siblings[ord];
-    }
-
-    if (sumValues == 0) {
-      System.out.println("  no sum");
-      return null;
-    }
-
-    if (dimConfig.multiValued) {
-      if (dimConfig.requireDimCount) {
-        sumValues = values[dimOrd];
-      } else {
-        // Our sum'd count is not correct, in general:
-        sumValues = -1;
-      }
-    } else {
-      // Our sum'd dim count is accurate, so we keep it
-    }
-
-    LabelAndValue[] labelValues = new LabelAndValue[q.size()];
-    for(int i=labelValues.length-1;i>=0;i--) {
-      TopOrdAndFloatQueue.OrdAndValue ordAndValue = q.pop();
-      FacetLabel child = taxoReader.getPath(ordAndValue.ord);
-      labelValues[i] = new LabelAndValue(child.components[cp.length], ordAndValue.value);
-    }
-
-    return new SimpleFacetResult(sumValues, labelValues, childCount);
-  }
-
-  /** {@link ValueSource} that returns the score for each
-   *  hit; use this to aggregate the sum of all hit scores
-   *  for each facet label.  */
-  public static class ScoreValueSource extends ValueSource {
-    @Override
-    public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
-      final Scorer scorer = (Scorer) context.get("scorer");
-      if (scorer == null) {
-        throw new IllegalStateException("scores are missing; be sure to pass keepScores=true to SimpleFacetsCollector");
-      }
-      return new DoubleDocValues(this) {
-        @Override
-        public double doubleVal(int document) {
-          try {
-            return scorer.score();
-          } catch (IOException exception) {
-            throw new RuntimeException(exception);
-          }
-        }
-      };
-    }
-
-    @Override public boolean equals(Object o) { return o == this; }
-    @Override public int hashCode() { return System.identityHashCode(this); }
-    @Override public String description() { return "score()"; }
-    };
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacets.java
deleted file mode 100644
index fa58fae..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/TaxonomyFacets.java
+++ /dev/null
@@ -1,92 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.facet.taxonomy.ParallelTaxonomyArrays;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/** Base class for all taxonomy-based facets impls. */
-abstract class TaxonomyFacets extends Facets {
-  protected final String indexFieldName;
-  protected final TaxonomyReader taxoReader;
-  protected final FacetsConfig config;
-  protected final int[] children;
-  protected final int[] parents;
-  protected final int[] siblings;
-
-  /** Sole parameter is the field name that holds the facet
-   *  counts. */
-  protected TaxonomyFacets(String indexFieldName, TaxonomyReader taxoReader, FacetsConfig config) throws IOException {
-    this.indexFieldName = indexFieldName;
-    this.taxoReader = taxoReader;
-    this.config = config;
-    ParallelTaxonomyArrays pta = taxoReader.getParallelTaxonomyArrays();
-    children = pta.children();
-    parents = pta.parents();
-    siblings = pta.siblings();
-  }
-
-  protected FacetsConfig.DimConfig verifyDim(String dim) {
-    FacetsConfig.DimConfig dimConfig = config.getDimConfig(dim);
-    if (!dimConfig.indexFieldName.equals(indexFieldName)) {
-      // nocommit get test case to cover this:
-      throw new IllegalArgumentException("dimension \"" + dim + "\" was not indexed into field \"" + indexFieldName);
-    }
-    return dimConfig;
-  }
-
-  @Override
-  public List<SimpleFacetResult> getAllDims(int topN) throws IOException {
-    int ord = children[TaxonomyReader.ROOT_ORDINAL];
-    List<SimpleFacetResult> results = new ArrayList<SimpleFacetResult>();
-    while (ord != TaxonomyReader.INVALID_ORDINAL) {
-      String dim = taxoReader.getPath(ord).components[0];
-      FacetsConfig.DimConfig dimConfig = config.getDimConfig(dim);
-      if (dimConfig.indexFieldName.equals(indexFieldName)) {
-        SimpleFacetResult result = getTopChildren(topN, dim);
-        if (result != null) {
-          results.add(result);
-        }
-      }
-      ord = siblings[ord];
-    }
-
-    // Sort by highest value, tie break by value:
-    Collections.sort(results,
-                     new Comparator<SimpleFacetResult>() {
-                       @Override
-                       public int compare(SimpleFacetResult a, SimpleFacetResult b) {
-                         if (a.value.doubleValue() > b.value.doubleValue()) {
-                           return -1;
-                         } else if (b.value.doubleValue() > a.value.doubleValue()) {
-                           return 1;
-                         } else {
-                           return 0;
-                         }
-                       }
-                     });
-
-    return results;
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TopOrdAndFloatQueue.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TopOrdAndFloatQueue.java
deleted file mode 100644
index c76a951..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/TopOrdAndFloatQueue.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.PriorityQueue;
-
-// nocommit make value a double and merge with TopOrdCountValueQueue?
-
-/** Keeps highest results, first by largest float value,
- *  then tie break by smallest ord. */
-class TopOrdAndFloatQueue extends PriorityQueue<TopOrdAndFloatQueue.OrdAndValue> {
-
-  public static final class OrdAndValue {
-    int ord;
-    float value;
-  }
-
-  public TopOrdAndFloatQueue(int topN) {
-    super(topN, false);
-  }
-
-  @Override
-  protected boolean lessThan(OrdAndValue a, OrdAndValue b) {
-    if (a.value < b.value) {
-      return true;
-    } else if (a.value > b.value) {
-      return false;
-    } else {
-      return a.ord > b.ord;
-    }
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/TopOrdAndIntQueue.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/TopOrdAndIntQueue.java
deleted file mode 100644
index 9bb812d..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/TopOrdAndIntQueue.java
+++ /dev/null
@@ -1,45 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.PriorityQueue;
-
-/** Keeps highest results, first by largest int value,
- *  then tie break by smallest ord. */
-class TopOrdAndIntQueue extends PriorityQueue<TopOrdAndIntQueue.OrdAndValue> {
-
-  public static final class OrdAndValue {
-    int ord;
-    int value;
-  }
-
-  public TopOrdAndIntQueue(int topN) {
-    super(topN, false);
-  }
-
-  @Override
-  protected boolean lessThan(OrdAndValue a, OrdAndValue b) {
-    if (a.value < b.value) {
-      return true;
-    } else if (a.value > b.value) {
-      return false;
-    } else {
-      return a.ord > b.ord;
-    }
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FacetLabel.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FacetLabel.java
index cf6b224..3af4393 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FacetLabel.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/FacetLabel.java
@@ -20,7 +20,7 @@ package org.apache.lucene.facet.taxonomy;
 import java.util.Arrays;
 import java.util.regex.Pattern;
 
-import org.apache.lucene.facet.simple.FacetsConfig;
+import org.apache.lucene.facet.FacetsConfig;
 
 import static org.apache.lucene.util.ByteBlockPool.BYTE_BLOCK_SIZE;
 
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
index 59df2c1..7aad386 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
@@ -21,7 +21,7 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.simple.FacetsConfig;
+import org.apache.lucene.facet.FacetsConfig;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/AssertingSubDocsAtOnceCollector.java b/lucene/facet/src/test/org/apache/lucene/facet/AssertingSubDocsAtOnceCollector.java
new file mode 100644
index 0000000..1caad40
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/AssertingSubDocsAtOnceCollector.java
@@ -0,0 +1,67 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer.ChildScorer;
+import org.apache.lucene.search.Scorer;
+
+/** Verifies in collect() that all child subScorers are on
+ *  the collected doc. */
+class AssertingSubDocsAtOnceCollector extends Collector {
+
+  // TODO: allow wrapping another Collector
+
+  List<Scorer> allScorers;
+
+  @Override
+  public void setScorer(Scorer s) {
+    // Gathers all scorers, including s and "under":
+    allScorers = new ArrayList<Scorer>();
+    allScorers.add(s);
+    int upto = 0;
+    while(upto < allScorers.size()) {
+      s = allScorers.get(upto++);
+      for (ChildScorer sub : s.getChildren()) {
+        allScorers.add(sub.child);
+      }
+    }
+  }
+
+  @Override
+  public void collect(int docID) {
+    for(Scorer s : allScorers) {
+      if (docID != s.docID()) {
+        throw new IllegalStateException("subScorer=" + s + " has docID=" + s.docID() + " != collected docID=" + docID);
+      }
+    }
+  }
+
+  @Override
+  public void setNextReader(AtomicReaderContext context) {
+  }
+
+  @Override
+  public boolean acceptsDocsOutOfOrder() {
+    return false;
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
index 0a30991..cde2115 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
@@ -20,24 +20,16 @@ package org.apache.lucene.facet;
 import java.io.IOException;
 import java.util.Random;
 
-import org.apache.lucene.facet.simple.CachedOrdinalsReader;
-import org.apache.lucene.facet.simple.DocValuesOrdinalsReader;
-import org.apache.lucene.facet.simple.Facets;
-import org.apache.lucene.facet.simple.FacetsConfig;
-import org.apache.lucene.facet.simple.FastTaxonomyFacetCounts;
-import org.apache.lucene.facet.simple.OrdinalsReader;
-import org.apache.lucene.facet.simple.SimpleFacetsCollector;
-import org.apache.lucene.facet.simple.TaxonomyFacetCounts;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.util.LuceneTestCase;
 
 public abstract class FacetTestCase extends LuceneTestCase {
   
-  public Facets getTaxonomyFacetCounts(TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector c) throws IOException {
+  public Facets getTaxonomyFacetCounts(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector c) throws IOException {
     return getTaxonomyFacetCounts(taxoReader, config, c, FacetsConfig.DEFAULT_INDEX_FIELD_NAME);
   }
 
-  public Facets getTaxonomyFacetCounts(TaxonomyReader taxoReader, FacetsConfig config, SimpleFacetsCollector c, String indexFieldName) throws IOException {
+  public Facets getTaxonomyFacetCounts(TaxonomyReader taxoReader, FacetsConfig config, FacetsCollector c, String indexFieldName) throws IOException {
     Facets facets;
     if (random().nextBoolean()) {
       facets = new FastTaxonomyFacetCounts(indexFieldName, taxoReader, config, c);
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestCachedOrdinalsReader.java b/lucene/facet/src/test/org/apache/lucene/facet/TestCachedOrdinalsReader.java
new file mode 100644
index 0000000..c16cbcd
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestCachedOrdinalsReader.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.Test;
+
+public class TestCachedOrdinalsReader extends FacetTestCase {
+
+  @Test
+  public void testWithThreads() throws Exception {
+    // LUCENE-5303: OrdinalsCache used the ThreadLocal BinaryDV instead of reader.getCoreCacheKey().
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    IndexWriter writer = new IndexWriter(indexDir, conf);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    
+    Document doc = new Document();
+    doc.add(new FacetField("A", "1"));
+    writer.addDocument(config.build(doc));
+    doc = new Document();
+    doc.add(new FacetField("A", "2"));
+    writer.addDocument(config.build(doc));
+    
+    final DirectoryReader reader = DirectoryReader.open(writer, true);
+    final CachedOrdinalsReader ordsReader = new CachedOrdinalsReader(new DocValuesOrdinalsReader(FacetsConfig.DEFAULT_INDEX_FIELD_NAME));
+    Thread[] threads = new Thread[3];
+    for (int i = 0; i < threads.length; i++) {
+      threads[i] = new Thread("CachedOrdsThread-" + i) {
+        @Override
+        public void run() {
+          for (AtomicReaderContext context : reader.leaves()) {
+            try {
+              ordsReader.getReader(context);
+            } catch (IOException e) {
+              throw new RuntimeException(e);
+            }
+          }
+        }
+      };
+    }
+
+    long ramBytesUsed = 0;
+    for (Thread t : threads) {
+      t.start();
+      t.join();
+      if (ramBytesUsed == 0) {
+        ramBytesUsed = ordsReader.ramBytesUsed();
+      } else {
+        assertEquals(ramBytesUsed, ordsReader.ramBytesUsed());
+      }
+    }
+    
+    IOUtils.close(writer, taxoWriter, reader, indexDir, taxoDir);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestDrillDownQuery.java b/lucene/facet/src/test/org/apache/lucene/facet/TestDrillDownQuery.java
new file mode 100644
index 0000000..fa6c81d
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestDrillDownQuery.java
@@ -0,0 +1,252 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.QueryUtils;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestDrillDownQuery extends FacetTestCase {
+  
+  private static IndexReader reader;
+  private static DirectoryTaxonomyReader taxo;
+  private static Directory dir;
+  private static Directory taxoDir;
+  private static FacetsConfig config;
+
+  @AfterClass
+  public static void afterClassDrillDownQueryTest() throws Exception {
+    IOUtils.close(reader, taxo, dir, taxoDir);
+    reader = null;
+    taxo = null;
+    dir = null;
+    taxoDir = null;
+    config = null;
+  }
+
+  @BeforeClass
+  public static void beforeClassDrillDownQueryTest() throws Exception {
+    dir = newDirectory();
+    Random r = random();
+    RandomIndexWriter writer = new RandomIndexWriter(r, dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(r, MockTokenizer.KEYWORD, false)));
+    
+    taxoDir = newDirectory();
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    config = new FacetsConfig(taxoWriter);
+
+    // Randomize the per-dim config:
+    config.setHierarchical("a", random().nextBoolean());
+    config.setMultiValued("a", random().nextBoolean());
+    if (random().nextBoolean()) {
+      config.setIndexFieldName("a", "$a");
+    }
+    config.setRequireDimCount("a", true);
+
+    config.setHierarchical("b", random().nextBoolean());
+    config.setMultiValued("b", random().nextBoolean());
+    if (random().nextBoolean()) {
+      config.setIndexFieldName("b", "$b");
+    }
+    config.setRequireDimCount("b", true);
+
+    for (int i = 0; i < 100; i++) {
+      Document doc = new Document();
+      if (i % 2 == 0) { // 50
+        doc.add(new TextField("content", "foo", Field.Store.NO));
+      }
+      if (i % 3 == 0) { // 33
+        doc.add(new TextField("content", "bar", Field.Store.NO));
+      }
+      if (i % 4 == 0) { // 25
+        if (r.nextBoolean()) {
+          doc.add(new FacetField("a", "1"));
+        } else {
+          doc.add(new FacetField("a", "2"));
+        }
+      }
+      if (i % 5 == 0) { // 20
+        doc.add(new FacetField("b", "1"));
+      }
+      writer.addDocument(config.build(doc));
+    }
+    
+    taxoWriter.close();
+    reader = writer.getReader();
+    writer.close();
+    
+    taxo = new DirectoryTaxonomyReader(taxoDir);
+  }
+  
+  public void testAndOrs() throws Exception {
+    IndexSearcher searcher = newSearcher(reader);
+
+    // test (a/1 OR a/2) AND b/1
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("a", "1");
+    q.add("a", "2");
+    q.add("b", "1");
+    TopDocs docs = searcher.search(q, 100);
+    assertEquals(5, docs.totalHits);
+  }
+  
+  public void testQuery() throws IOException {
+    IndexSearcher searcher = newSearcher(reader);
+
+    // Making sure the query yields 25 documents with the facet "a"
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("a");
+    System.out.println("q=" + q);
+    QueryUtils.check(q);
+    TopDocs docs = searcher.search(q, 100);
+    assertEquals(25, docs.totalHits);
+    
+    // Making sure the query yields 5 documents with the facet "b" and the
+    // previous (facet "a") query as a base query
+    DrillDownQuery q2 = new DrillDownQuery(config, q);
+    q2.add("b");
+    docs = searcher.search(q2, 100);
+    assertEquals(5, docs.totalHits);
+
+    // Making sure that a query of both facet "a" and facet "b" yields 5 results
+    DrillDownQuery q3 = new DrillDownQuery(config);
+    q3.add("a");
+    q3.add("b");
+    docs = searcher.search(q3, 100);
+    
+    assertEquals(5, docs.totalHits);
+    // Check that content:foo (which yields 50% results) and facet/b (which yields 20%)
+    // would gather together 10 results (10%..) 
+    Query fooQuery = new TermQuery(new Term("content", "foo"));
+    DrillDownQuery q4 = new DrillDownQuery(config, fooQuery);
+    q4.add("b");
+    docs = searcher.search(q4, 100);
+    assertEquals(10, docs.totalHits);
+  }
+  
+  public void testQueryImplicitDefaultParams() throws IOException {
+    IndexSearcher searcher = newSearcher(reader);
+
+    // Create the base query to start with
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("a");
+    
+    // Making sure the query yields 5 documents with the facet "b" and the
+    // previous (facet "a") query as a base query
+    DrillDownQuery q2 = new DrillDownQuery(config, q);
+    q2.add("b");
+    TopDocs docs = searcher.search(q2, 100);
+    assertEquals(5, docs.totalHits);
+
+    // Check that content:foo (which yields 50% results) and facet/b (which yields 20%)
+    // would gather together 10 results (10%..) 
+    Query fooQuery = new TermQuery(new Term("content", "foo"));
+    DrillDownQuery q4 = new DrillDownQuery(config, fooQuery);
+    q4.add("b");
+    docs = searcher.search(q4, 100);
+    assertEquals(10, docs.totalHits);
+  }
+  
+  public void testScoring() throws IOException {
+    // verify that drill-down queries do not modify scores
+    IndexSearcher searcher = newSearcher(reader);
+
+    float[] scores = new float[reader.maxDoc()];
+    
+    Query q = new TermQuery(new Term("content", "foo"));
+    TopDocs docs = searcher.search(q, reader.maxDoc()); // fetch all available docs to this query
+    for (ScoreDoc sd : docs.scoreDocs) {
+      scores[sd.doc] = sd.score;
+    }
+    
+    // create a drill-down query with category "a", scores should not change
+    DrillDownQuery q2 = new DrillDownQuery(config, q);
+    q2.add("a");
+    docs = searcher.search(q2, reader.maxDoc()); // fetch all available docs to this query
+    for (ScoreDoc sd : docs.scoreDocs) {
+      assertEquals("score of doc=" + sd.doc + " modified", scores[sd.doc], sd.score, 0f);
+    }
+  }
+  
+  public void testScoringNoBaseQuery() throws IOException {
+    // verify that drill-down queries (with no base query) returns 0.0 score
+    IndexSearcher searcher = newSearcher(reader);
+    
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("a");
+    TopDocs docs = searcher.search(q, reader.maxDoc()); // fetch all available docs to this query
+    for (ScoreDoc sd : docs.scoreDocs) {
+      assertEquals(0f, sd.score, 0f);
+    }
+  }
+  
+  public void testTermNonDefault() {
+    String aField = config.getDimConfig("a").indexFieldName;
+    Term termA = DrillDownQuery.term(aField, "a");
+    assertEquals(new Term(aField, "a"), termA);
+    
+    String bField = config.getDimConfig("b").indexFieldName;
+    Term termB = DrillDownQuery.term(bField, "b");
+    assertEquals(new Term(bField, "b"), termB);
+  }
+
+  public void testClone() throws Exception {
+    DrillDownQuery q = new DrillDownQuery(config, new MatchAllDocsQuery());
+    q.add("a");
+    
+    DrillDownQuery clone = q.clone();
+    clone.add("b");
+    
+    assertFalse("query wasn't cloned: source=" + q + " clone=" + clone, q.toString().equals(clone.toString()));
+  }
+  
+  public void testNoDrillDown() throws Exception {
+    Query base = new MatchAllDocsQuery();
+    DrillDownQuery q = new DrillDownQuery(config, base);
+    Query rewrite = q.rewrite(reader).rewrite(reader);
+    assertSame(base, rewrite);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java b/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
new file mode 100644
index 0000000..65f96b1
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestDrillSideways.java
@@ -0,0 +1,1046 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.DrillSideways.DrillSidewaysResult;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.InPlaceMergeSorter;
+import org.apache.lucene.util.InfoStream;
+import org.apache.lucene.util._TestUtil;
+
+public class TestDrillSideways extends FacetTestCase {
+
+  public void testBasic() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setHierarchical("Publish Date", true);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Susan"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Frank"));
+    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    //System.out.println("searcher=" + searcher);
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    DrillSideways ds = new DrillSideways(searcher, config, taxoReader);
+
+    //  case: drill-down on a single field; in this
+    // case the drill-sideways + drill-down counts ==
+    // drill-down of just the query: 
+    DrillDownQuery ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    DrillSidewaysResult r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Same simple case, but no baseQuery (pure browse):
+    // drill-down on a single field; in this case the
+    // drill-sideways + drill-down counts == drill-down of
+    // just the query:
+    ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    r = ds.search(null, ddq, 10);
+
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Another simple case: drill-down on on single fields
+    // but OR of two values
+    ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    ddq.add("Author", "Bob");
+    r = ds.search(null, ddq, 10);
+    assertEquals(3, r.hits.totalHits);
+    // Publish Date is only drill-down: Lisa and Bob
+    // (drill-down) published twice in 2010 and once in 2012:
+    assertEquals("value=3 childCount=2\n  2010 (2)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // More interesting case: drill-down on two fields
+    ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    ddq.add("Publish Date", "2010");
+    r = ds.search(null, ddq, 10);
+    assertEquals(1, r.hits.totalHits);
+    // Publish Date is drill-sideways + drill-down: Lisa
+    // (drill-down) published once in 2010 and once in 2012:
+    assertEquals("value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down:
+    // only Lisa & Bob published (once each) in 2010:
+    assertEquals("value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Even more interesting case: drill down on two fields,
+    // but one of them is OR
+    ddq = new DrillDownQuery(config);
+
+    // Drill down on Lisa or Bob:
+    ddq.add("Author", "Lisa");
+    ddq.add("Publish Date", "2010");
+    ddq.add("Author", "Bob");
+    r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is both drill-sideways + drill-down:
+    // Lisa or Bob published twice in 2010 and once in 2012:
+    assertEquals("value=3 childCount=2\n  2010 (2)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down:
+    // only Lisa & Bob published (once each) in 2010:
+    assertEquals("value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Test drilling down on invalid field:
+    ddq = new DrillDownQuery(config);
+    ddq.add("Foobar", "Baz");
+    r = ds.search(null, ddq, 10);
+    assertEquals(0, r.hits.totalHits);
+    assertNull(r.facets.getTopChildren(10, "Publish Date"));
+    assertNull(r.facets.getTopChildren(10, "Foobar"));
+
+    // Test drilling down on valid term or'd with invalid term:
+    ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    ddq.add("Author", "Tom");
+    r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // LUCENE-4915: test drilling down on a dimension but
+    // NOT facet counting it:
+    ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    ddq.add("Author", "Tom");
+    r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+
+    // Test main query gets null scorer:
+    ddq = new DrillDownQuery(config, new TermQuery(new Term("foobar", "baz")));
+    ddq.add("Author", "Lisa");
+    r = ds.search(null, ddq, 10);
+
+    assertEquals(0, r.hits.totalHits);
+    assertNull(r.facets.getTopChildren(10, "Publish Date"));
+    assertNull(r.facets.getTopChildren(10, "Author"));
+    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testSometimesInvalidDrillDown() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setHierarchical("Publish Date", true);
+
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    writer.addDocument(config.build(doc));
+
+    writer.commit();
+
+    // 2nd segment has no Author:
+    doc = new Document();
+    doc.add(new FacetField("Foobar", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    //System.out.println("searcher=" + searcher);
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    DrillDownQuery ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    DrillSidewaysResult r = new DrillSideways(searcher, config, taxoReader).search(null, ddq, 10);
+
+    assertEquals(1, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("value=1 childCount=1\n  2010 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published once, and Bob
+    // published once:
+    assertEquals("value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testMultipleRequestsPerDim() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setHierarchical("dim", true);
+
+    Document doc = new Document();
+    doc.add(new FacetField("dim", "a", "x"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "a", "y"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "a", "z"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "b"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "c"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("dim", "d"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    //System.out.println("searcher=" + searcher);
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    DrillDownQuery ddq = new DrillDownQuery(config);
+    ddq.add("dim", "a");
+    DrillSidewaysResult r = new DrillSideways(searcher, config, taxoReader).search(null, ddq, 10);
+
+    assertEquals(3, r.hits.totalHits);
+    assertEquals("value=6 childCount=4\n  a (3)\n  b (1)\n  c (1)\n  d (1)\n", r.facets.getTopChildren(10, "dim").toString());
+    assertEquals("value=3 childCount=3\n  x (1)\n  y (1)\n  z (1)\n", r.facets.getTopChildren(10, "dim", "a").toString());
+
+    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
+  }
+
+  private static class Doc implements Comparable<Doc> {
+    String id;
+    String contentToken;
+
+    public Doc() {}
+    
+    // -1 if the doc is missing this dim, else the index
+    // -into the values for this dim:
+    int[] dims;
+
+    // 2nd value per dim for the doc (so we test
+    // multi-valued fields):
+    int[] dims2;
+    boolean deleted;
+
+    @Override
+    public int compareTo(Doc other) {
+      return id.compareTo(other.id);
+    }
+  }
+
+  private double aChance, bChance, cChance;
+
+  private String randomContentToken(boolean isQuery) {
+    double d = random().nextDouble();
+    if (isQuery) {
+      if (d < 0.33) {
+        return "a";
+      } else if (d < 0.66) {
+        return "b";
+      } else {
+        return "c";
+      }
+    } else {
+      if (d <= aChance) {
+        return "a";
+      } else if (d < aChance + bChance) {
+        return "b";
+      } else {
+        return "c";
+      }
+    }
+  }
+
+  public void testRandom() throws Exception {
+
+    boolean canUseDV = defaultCodecSupportsSortedSet();
+
+    while (aChance == 0.0) {
+      aChance = random().nextDouble();
+    }
+    while (bChance == 0.0) {
+      bChance = random().nextDouble();
+    }
+    while (cChance == 0.0) {
+      cChance = random().nextDouble();
+    }
+    //aChance = .01;
+    //bChance = 0.5;
+    //cChance = 1.0;
+    double sum = aChance + bChance + cChance;
+    aChance /= sum;
+    bChance /= sum;
+    cChance /= sum;
+
+    int numDims = _TestUtil.nextInt(random(), 2, 5);
+    //int numDims = 3;
+    int numDocs = atLeast(3000);
+    //int numDocs = 20;
+    if (VERBOSE) {
+      System.out.println("numDims=" + numDims + " numDocs=" + numDocs + " aChance=" + aChance + " bChance=" + bChance + " cChance=" + cChance);
+    }
+    String[][] dimValues = new String[numDims][];
+    int valueCount = 2;
+
+    for(int dim=0;dim<numDims;dim++) {
+      Set<String> values = new HashSet<String>();
+      while (values.size() < valueCount) {
+        String s = _TestUtil.randomRealisticUnicodeString(random());
+        //String s = _TestUtil.randomString(random());
+        if (s.length() > 0) {
+          values.add(s);
+        }
+      } 
+      dimValues[dim] = values.toArray(new String[values.size()]);
+      valueCount *= 2;
+    }
+
+    List<Doc> docs = new ArrayList<Doc>();
+    for(int i=0;i<numDocs;i++) {
+      Doc doc = new Doc();
+      doc.id = ""+i;
+      doc.contentToken = randomContentToken(false);
+      doc.dims = new int[numDims];
+      doc.dims2 = new int[numDims];
+      for(int dim=0;dim<numDims;dim++) {
+        if (random().nextInt(5) == 3) {
+          // This doc is missing this dim:
+          doc.dims[dim] = -1;
+        } else if (dimValues[dim].length <= 4) {
+          int dimUpto = 0;
+          doc.dims[dim] = dimValues[dim].length-1;
+          while (dimUpto < dimValues[dim].length) {
+            if (random().nextBoolean()) {
+              doc.dims[dim] = dimUpto;
+              break;
+            }
+            dimUpto++;
+          }
+        } else {
+          doc.dims[dim] = random().nextInt(dimValues[dim].length);
+        }
+
+        if (random().nextInt(5) == 3) {
+          // 2nd value:
+          doc.dims2[dim] = random().nextInt(dimValues[dim].length);
+        } else {
+          doc.dims2[dim] = -1;
+        }
+      }
+      docs.add(doc);
+    }
+
+    Directory d = newDirectory();
+    Directory td = newDirectory();
+
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setInfoStream(InfoStream.NO_OUTPUT);
+    RandomIndexWriter w = new RandomIndexWriter(random(), d, iwc);
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
+    FacetsConfig config = new FacetsConfig(tw);
+    for(int i=0;i<numDims;i++) {
+      config.setMultiValued("dim"+i, true);
+    }
+
+    boolean doUseDV = canUseDV && random().nextBoolean();
+
+    for(Doc rawDoc : docs) {
+      Document doc = new Document();
+      doc.add(newStringField("id", rawDoc.id, Field.Store.YES));
+      doc.add(newStringField("content", rawDoc.contentToken, Field.Store.NO));
+
+      if (VERBOSE) {
+        System.out.println("  doc id=" + rawDoc.id + " token=" + rawDoc.contentToken);
+      }
+      for(int dim=0;dim<numDims;dim++) {
+        int dimValue = rawDoc.dims[dim];
+        if (dimValue != -1) {
+          if (doUseDV) {
+            doc.add(new SortedSetDocValuesFacetField("dim" + dim, dimValues[dim][dimValue]));
+          } else {
+            doc.add(new FacetField("dim" + dim, dimValues[dim][dimValue]));
+          }
+          doc.add(new StringField("dim" + dim, dimValues[dim][dimValue], Field.Store.YES));
+          if (VERBOSE) {
+            System.out.println("    dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue]));
+          }
+        }
+        int dimValue2 = rawDoc.dims2[dim];
+        if (dimValue2 != -1) {
+          if (doUseDV) {
+            doc.add(new SortedSetDocValuesFacetField("dim" + dim, dimValues[dim][dimValue2]));
+          } else {
+            doc.add(new FacetField("dim" + dim, dimValues[dim][dimValue2]));
+          }
+          doc.add(new StringField("dim" + dim, dimValues[dim][dimValue2], Field.Store.YES));
+          if (VERBOSE) {
+            System.out.println("      dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue2]));
+          }
+        }
+      }
+
+      w.addDocument(config.build(doc));
+    }
+
+    if (random().nextBoolean()) {
+      // Randomly delete a few docs:
+      int numDel = _TestUtil.nextInt(random(), 1, (int) (numDocs*0.05));
+      if (VERBOSE) {
+        System.out.println("delete " + numDel);
+      }
+      int delCount = 0;
+      while (delCount < numDel) {
+        Doc doc = docs.get(random().nextInt(docs.size()));
+        if (!doc.deleted) {
+          if (VERBOSE) {
+            System.out.println("  delete id=" + doc.id);
+          }
+          doc.deleted = true;
+          w.deleteDocuments(new Term("id", doc.id));
+          delCount++;
+        }
+      }
+    }
+
+    if (random().nextBoolean()) {
+      if (VERBOSE) {
+        System.out.println("TEST: forceMerge(1)...");
+      }
+      w.forceMerge(1);
+    }
+    IndexReader r = w.getReader();
+
+    final SortedSetDocValuesReaderState sortedSetDVState;
+    IndexSearcher s = newSearcher(r);
+    
+    if (doUseDV) {
+      sortedSetDVState = new SortedSetDocValuesReaderState(s.getIndexReader());
+    } else {
+      sortedSetDVState = null;
+    }
+
+    if (VERBOSE) {
+      System.out.println("r.numDocs() = " + r.numDocs());
+    }
+
+    // NRT open
+    TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
+
+    int numIters = atLeast(10);
+
+    for(int iter=0;iter<numIters;iter++) {
+
+      String contentToken = random().nextInt(30) == 17 ? null : randomContentToken(true);
+      int numDrillDown = _TestUtil.nextInt(random(), 1, Math.min(4, numDims));
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter=" + iter + " baseQuery=" + contentToken + " numDrillDown=" + numDrillDown + " useSortedSetDV=" + doUseDV);
+      }
+
+      String[][] drillDowns = new String[numDims][];
+
+      int count = 0;
+      boolean anyMultiValuedDrillDowns = false;
+      while (count < numDrillDown) {
+        int dim = random().nextInt(numDims);
+        if (drillDowns[dim] == null) {
+          if (random().nextBoolean()) {
+            // Drill down on one value:
+            drillDowns[dim] = new String[] {dimValues[dim][random().nextInt(dimValues[dim].length)]};
+          } else {
+            int orCount = _TestUtil.nextInt(random(), 1, Math.min(5, dimValues[dim].length));
+            drillDowns[dim] = new String[orCount];
+            anyMultiValuedDrillDowns |= orCount > 1;
+            for(int i=0;i<orCount;i++) {
+              while (true) {
+                String value = dimValues[dim][random().nextInt(dimValues[dim].length)];
+                for(int j=0;j<i;j++) {
+                  if (value.equals(drillDowns[dim][j])) {
+                    value = null;
+                    break;
+                  }
+                }
+                if (value != null) {
+                  drillDowns[dim][i] = value;
+                  break;
+                }
+              }
+            }
+          }
+          if (VERBOSE) {
+            BytesRef[] values = new BytesRef[drillDowns[dim].length];
+            for(int i=0;i<values.length;i++) {
+              values[i] = new BytesRef(drillDowns[dim][i]);
+            }
+            System.out.println("  dim" + dim + "=" + Arrays.toString(values));
+          }
+          count++;
+        }
+      }
+
+      Query baseQuery;
+      if (contentToken == null) {
+        baseQuery = new MatchAllDocsQuery();
+      } else {
+        baseQuery = new TermQuery(new Term("content", contentToken));
+      }
+
+      DrillDownQuery ddq = new DrillDownQuery(config, baseQuery);
+
+      for(int dim=0;dim<numDims;dim++) {
+        if (drillDowns[dim] != null) {
+          int upto = 0;
+          for(String value : drillDowns[dim]) {
+            ddq.add("dim" + dim, value);
+          }
+        }
+      }
+
+      Filter filter;
+      if (random().nextInt(7) == 6) {
+        if (VERBOSE) {
+          System.out.println("  only-even filter");
+        }
+        filter = new Filter() {
+            @Override
+            public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+              int maxDoc = context.reader().maxDoc();
+              final FixedBitSet bits = new FixedBitSet(maxDoc);
+              for(int docID=0;docID < maxDoc;docID++) {
+                // Keeps only the even ids:
+                if ((acceptDocs == null || acceptDocs.get(docID)) && ((Integer.parseInt(context.reader().document(docID).get("id")) & 1) == 0)) {
+                  bits.set(docID);
+                }
+              }
+              return bits;
+            }
+          };
+      } else {
+        filter = null;
+      }
+
+      // Verify docs are always collected in order.  If we
+      // had an AssertingScorer it could catch it when
+      // Weight.scoresDocsOutOfOrder lies!:
+      new DrillSideways(s, config, tr).search(ddq,
+                           new Collector() {
+                             int lastDocID;
+
+                             @Override
+                             public void setScorer(Scorer s) {
+                             }
+
+                             @Override
+                             public void collect(int doc) {
+                               assert doc > lastDocID;
+                               lastDocID = doc;
+                             }
+
+                             @Override
+                             public void setNextReader(AtomicReaderContext context) {
+                               lastDocID = -1;
+                             }
+
+                             @Override
+                             public boolean acceptsDocsOutOfOrder() {
+                               return false;
+                             }
+                           });
+
+      // Also separately verify that DS respects the
+      // scoreSubDocsAtOnce method, to ensure that all
+      // subScorers are on the same docID:
+      if (!anyMultiValuedDrillDowns) {
+        // Can only do this test when there are no OR'd
+        // drill-down values, beacuse in that case it's
+        // easily possible for one of the DD terms to be on
+        // a future docID:
+        new DrillSideways(s, config, tr) {
+          @Override
+          protected boolean scoreSubDocsAtOnce() {
+            return true;
+          }
+        }.search(ddq, new AssertingSubDocsAtOnceCollector());
+      }
+
+      TestFacetResult expected = slowDrillSidewaysSearch(s, docs, contentToken, drillDowns, dimValues, filter);
+
+      Sort sort = new Sort(new SortField("id", SortField.Type.STRING));
+      // nocommit subclass & override to use FacetsTestCase.getFacetCounts
+      DrillSideways ds;
+      if (doUseDV) {
+        ds = new DrillSideways(s, config, sortedSetDVState);
+      } else {
+        ds = new DrillSideways(s, config, tr);
+      }
+
+      // Retrieve all facets:
+      DrillSidewaysResult actual = ds.search(ddq, filter, null, numDocs, sort, true, true);
+
+      TopDocs hits = s.search(baseQuery, numDocs);
+      Map<String,Float> scores = new HashMap<String,Float>();
+      for(ScoreDoc sd : hits.scoreDocs) {
+        scores.put(s.doc(sd.doc).get("id"), sd.score);
+      }
+      if (VERBOSE) {
+        System.out.println("  verify all facets");
+      }
+      verifyEquals(dimValues, s, expected, actual, scores, doUseDV);
+
+      // Make sure drill down doesn't change score:
+      TopDocs ddqHits = s.search(ddq, filter, numDocs);
+      assertEquals(expected.hits.size(), ddqHits.totalHits);
+      for(int i=0;i<expected.hits.size();i++) {
+        // Score should be IDENTICAL:
+        assertEquals(scores.get(expected.hits.get(i).id), ddqHits.scoreDocs[i].score, 0.0f);
+      }
+    }
+
+    IOUtils.close(r, tr, w, tw, d, td);
+  }
+
+  private static class Counters {
+    int[][] counts;
+
+    public Counters(String[][] dimValues) {
+      counts = new int[dimValues.length][];
+      for(int dim=0;dim<dimValues.length;dim++) {
+        counts[dim] = new int[dimValues[dim].length];
+      }
+    }
+
+    public void inc(int[] dims, int[] dims2) {
+      inc(dims, dims2, -1);
+    }
+
+    public void inc(int[] dims, int[] dims2, int onlyDim) {
+      assert dims.length == counts.length;
+      assert dims2.length == counts.length;
+      for(int dim=0;dim<dims.length;dim++) {
+        if (onlyDim == -1 || dim == onlyDim) {
+          if (dims[dim] != -1) {
+            counts[dim][dims[dim]]++;
+          }
+          if (dims2[dim] != -1 && dims2[dim] != dims[dim]) {
+            counts[dim][dims2[dim]]++;
+          }
+        }
+      }
+    }
+  }
+
+  private static class TestFacetResult {
+    List<Doc> hits;
+    int[][] counts;
+    int[] uniqueCounts;
+  }
+  
+  private int[] getTopNOrds(final int[] counts, final String[] values, int topN) {
+    final int[] ids = new int[counts.length];
+    for(int i=0;i<ids.length;i++) {
+      ids[i] = i;
+    }
+
+    // Naive (on purpose, to reduce bug in tester/gold):
+    // sort all ids, then return top N slice:
+    new InPlaceMergeSorter() {
+
+      @Override
+      protected void swap(int i, int j) {
+        int id = ids[i];
+        ids[i] = ids[j];
+        ids[j] = id;
+      }
+
+      @Override
+      protected int compare(int i, int j) {
+        int counti = counts[ids[i]];
+        int countj = counts[ids[j]];
+        // Sort by count descending...
+        if (counti > countj) {
+          return -1;
+        } else if (counti < countj) {
+          return 1;
+        } else {
+          // ... then by label ascending:
+          return new BytesRef(values[ids[i]]).compareTo(new BytesRef(values[ids[j]]));
+        }
+      }
+
+    }.sort(0, ids.length);
+
+    if (topN > ids.length) {
+      topN = ids.length;
+    }
+
+    int numSet = topN;
+    for(int i=0;i<topN;i++) {
+      if (counts[ids[i]] == 0) {
+        numSet = i;
+        break;
+      }
+    }
+
+    int[] topNIDs = new int[numSet];
+    System.arraycopy(ids, 0, topNIDs, 0, topNIDs.length);
+    return topNIDs;
+  }
+
+  private TestFacetResult slowDrillSidewaysSearch(IndexSearcher s, List<Doc> docs,
+                                                        String contentToken, String[][] drillDowns,
+                                                        String[][] dimValues, Filter onlyEven) throws Exception {
+    int numDims = dimValues.length;
+
+    List<Doc> hits = new ArrayList<Doc>();
+    Counters drillDownCounts = new Counters(dimValues);
+    Counters[] drillSidewaysCounts = new Counters[dimValues.length];
+    for(int dim=0;dim<numDims;dim++) {
+      drillSidewaysCounts[dim] = new Counters(dimValues);
+    }
+
+    if (VERBOSE) {
+      System.out.println("  compute expected");
+    }
+
+    nextDoc: for(Doc doc : docs) {
+      if (doc.deleted) {
+        continue;
+      }
+      if (onlyEven != null & (Integer.parseInt(doc.id) & 1) != 0) {
+        continue;
+      }
+      if (contentToken == null || doc.contentToken.equals(contentToken)) {
+        int failDim = -1;
+        for(int dim=0;dim<numDims;dim++) {
+          if (drillDowns[dim] != null) {
+            String docValue = doc.dims[dim] == -1 ? null : dimValues[dim][doc.dims[dim]];
+            String docValue2 = doc.dims2[dim] == -1 ? null : dimValues[dim][doc.dims2[dim]];
+            boolean matches = false;
+            for(String value : drillDowns[dim]) {
+              if (value.equals(docValue) || value.equals(docValue2)) {
+                matches = true;
+                break;
+              }
+            }
+            if (!matches) {
+              if (failDim == -1) {
+                // Doc could be a near-miss, if no other dim fails
+                failDim = dim;
+              } else {
+                // Doc isn't a hit nor a near-miss
+                continue nextDoc;
+              }
+            }
+          }
+        }
+
+        if (failDim == -1) {
+          if (VERBOSE) {
+            System.out.println("    exp: id=" + doc.id + " is a hit");
+          }
+          // Hit:
+          hits.add(doc);
+          drillDownCounts.inc(doc.dims, doc.dims2);
+          for(int dim=0;dim<dimValues.length;dim++) {
+            drillSidewaysCounts[dim].inc(doc.dims, doc.dims2);
+          }
+        } else {
+          if (VERBOSE) {
+            System.out.println("    exp: id=" + doc.id + " is a near-miss on dim=" + failDim);
+          }
+          drillSidewaysCounts[failDim].inc(doc.dims, doc.dims2, failDim);
+        }
+      }
+    }
+
+    Map<String,Integer> idToDocID = new HashMap<String,Integer>();
+    for(int i=0;i<s.getIndexReader().maxDoc();i++) {
+      idToDocID.put(s.doc(i).get("id"), i);
+    }
+
+    Collections.sort(hits);
+
+    TestFacetResult res = new TestFacetResult();
+    res.hits = hits;
+    res.counts = new int[numDims][];
+    res.uniqueCounts = new int[numDims];
+    for (int dim = 0; dim < numDims; dim++) {
+      if (drillDowns[dim] != null) {
+        res.counts[dim] = drillSidewaysCounts[dim].counts[dim];
+      } else {
+        res.counts[dim] = drillDownCounts.counts[dim];
+      }
+      int uniqueCount = 0;
+      for (int j = 0; j < res.counts[dim].length; j++) {
+        if (res.counts[dim][j] != 0) {
+          uniqueCount++;
+        }
+      }
+      res.uniqueCounts[dim] = uniqueCount;
+    }
+
+    return res;
+  }
+
+  void verifyEquals(String[][] dimValues, IndexSearcher s, TestFacetResult expected,
+                    DrillSidewaysResult actual, Map<String,Float> scores, boolean isSortedSetDV) throws Exception {
+    if (VERBOSE) {
+      System.out.println("  verify totHits=" + expected.hits.size());
+    }
+    assertEquals(expected.hits.size(), actual.hits.totalHits);
+    assertEquals(expected.hits.size(), actual.hits.scoreDocs.length);
+    for(int i=0;i<expected.hits.size();i++) {
+      if (VERBOSE) {
+        System.out.println("    hit " + i + " expected=" + expected.hits.get(i).id);
+      }
+      assertEquals(expected.hits.get(i).id,
+                   s.doc(actual.hits.scoreDocs[i].doc).get("id"));
+      // Score should be IDENTICAL:
+      assertEquals(scores.get(expected.hits.get(i).id), actual.hits.scoreDocs[i].score, 0.0f);
+    }
+
+    for(int dim=0;dim<expected.counts.length;dim++) {
+      int topN = random().nextBoolean() ? dimValues[dim].length : _TestUtil.nextInt(random(), 1, dimValues[dim].length);
+      FacetResult fr = actual.facets.getTopChildren(topN, "dim"+dim);
+      if (VERBOSE) {
+        System.out.println("    dim" + dim + " topN=" + topN + " (vs " + dimValues[dim].length + " unique values)");
+        System.out.println("      actual");
+      }
+
+      int idx = 0;
+      Map<String,Integer> actualValues = new HashMap<String,Integer>();
+
+      if (fr != null) {
+        for(LabelAndValue labelValue : fr.labelValues) {
+          actualValues.put(labelValue.label, labelValue.value.intValue());
+          if (VERBOSE) {
+            System.out.println("        " + idx + ": " + new BytesRef(labelValue.label) + ": " + labelValue.value);
+            idx++;
+          }
+        }
+      }
+
+      if (topN < dimValues[dim].length) {
+        int[] topNIDs = getTopNOrds(expected.counts[dim], dimValues[dim], topN);
+        if (VERBOSE) {
+          idx = 0;
+          System.out.println("      expected (sorted)");
+          for(int i=0;i<topNIDs.length;i++) {
+            int expectedOrd = topNIDs[i];
+            String value = dimValues[dim][expectedOrd];
+            System.out.println("        " + idx + ": " + new BytesRef(value) + ": " + expected.counts[dim][expectedOrd]);
+            idx++;
+          }
+        }
+        if (VERBOSE) {
+          System.out.println("      topN=" + topN + " expectedTopN=" + topNIDs.length);
+        }
+
+        if (fr != null) {
+          assertEquals(topNIDs.length, fr.labelValues.length);
+        } else {
+          assertEquals(0, topNIDs.length);
+        }
+        for(int i=0;i<topNIDs.length;i++) {
+          int expectedOrd = topNIDs[i];
+          assertEquals(expected.counts[dim][expectedOrd], fr.labelValues[i].value.intValue());
+          if (isSortedSetDV) {
+            // Tie-break facet labels are only in unicode
+            // order with SortedSetDVFacets:
+            assertEquals("value @ idx=" + i, dimValues[dim][expectedOrd], fr.labelValues[i].label);
+          }
+        }
+      } else {
+
+        if (VERBOSE) {
+          idx = 0;
+          System.out.println("      expected (unsorted)");
+          for(int i=0;i<dimValues[dim].length;i++) {
+            String value = dimValues[dim][i];
+            if (expected.counts[dim][i] != 0) {
+              System.out.println("        " + idx + ": " + new BytesRef(value) + ": " + expected.counts[dim][i]);
+              idx++;
+            } 
+          }
+        }
+
+        int setCount = 0;
+        for(int i=0;i<dimValues[dim].length;i++) {
+          String value = dimValues[dim][i];
+          if (expected.counts[dim][i] != 0) {
+            assertTrue(actualValues.containsKey(value));
+            assertEquals(expected.counts[dim][i], actualValues.get(value).intValue());
+            setCount++;
+          } else {
+            assertFalse(actualValues.containsKey(value));
+          }
+        }
+        assertEquals(setCount, actualValues.size());
+      }
+
+      // nocommit if we add this to FR then re-enable this:
+      // assertEquals("dim=" + dim, expected.uniqueCounts[dim], fr.getNumValidDescendants());
+    }
+  }
+
+  public void testEmptyIndex() throws Exception {
+    // LUCENE-5045: make sure DrillSideways works with an empty index
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    // Count "Author"
+    FacetsConfig config = new FacetsConfig();
+    DrillSideways ds = new DrillSideways(searcher, config, taxoReader);
+    DrillDownQuery ddq = new DrillDownQuery(config);
+    ddq.add("Author", "Lisa");
+    
+    DrillSidewaysResult r = ds.search(ddq, 10); // this used to fail on IllegalArgEx
+    assertEquals(0, r.hits.totalHits);
+
+    r = ds.search(ddq, null, null, 10, new Sort(new SortField("foo", SortField.Type.INT)), false, false); // this used to fail on IllegalArgEx
+    assertEquals(0, r.hits.totalHits);
+    
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+}
+
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestFacetsConfig.java b/lucene/facet/src/test/org/apache/lucene/facet/TestFacetsConfig.java
new file mode 100644
index 0000000..a082276
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestFacetsConfig.java
@@ -0,0 +1,40 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.util._TestUtil;
+
+public class TestFacetsConfig extends FacetTestCase {
+  public void testPathToStringAndBack() throws Exception {
+    int iters = atLeast(1000);
+    for(int i=0;i<iters;i++) {
+      int numParts = _TestUtil.nextInt(random(), 1, 6);
+      String[] parts = new String[numParts];
+      for(int j=0;j<numParts;j++) {
+        parts[j] = _TestUtil.randomUnicodeString(random());
+      }
+
+      String s = FacetsConfig.pathToString(parts);
+      String[] parts2 = FacetsConfig.stringToPath(s);
+      assertTrue(Arrays.equals(parts, parts2));
+    }
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java b/lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java
new file mode 100644
index 0000000..3bc4670
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestMultipleIndexFields.java
@@ -0,0 +1,307 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig.OpenMode;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MultiCollector;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TopScoreDocCollector;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.Test;
+
+public class TestMultipleIndexFields extends FacetTestCase {
+
+  private static final FacetField[] CATEGORIES = new FacetField[] {
+    new FacetField("Author", "Mark Twain"),
+    new FacetField("Author", "Stephen King"),
+    new FacetField("Author", "Kurt Vonnegut"),
+    new FacetField("Band", "Rock & Pop", "The Beatles"),
+    new FacetField("Band", "Punk", "The Ramones"),
+    new FacetField("Band", "Rock & Pop", "U2"),
+    new FacetField("Band", "Rock & Pop", "REM"),
+    new FacetField("Band", "Rock & Pop", "Dave Matthews Band"),
+    new FacetField("Composer", "Bach"),
+  };
+
+  private FacetsConfig getConfig(TaxonomyWriter taxoWriter) {
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    //config.setMultiValued("Author", true);
+    //config.setMultiValued("Band", true);
+    config.setHierarchical("Band", true);
+    return config;
+  }
+  
+  @Test
+  public void testDefault() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    // create and open an index writer
+    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    // create and open a taxonomy writer
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
+    FacetsConfig config = getConfig(tw);
+
+    seedIndex(iw, config);
+
+    IndexReader ir = iw.getReader();
+    tw.commit();
+
+    // prepare index reader and taxonomy.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+
+    // prepare searcher to search against
+    IndexSearcher searcher = newSearcher(ir);
+
+    FacetsCollector sfc = performSearch(tr, ir, searcher);
+
+    // Obtain facets results and hand-test them
+    assertCorrectResults(getTaxonomyFacetCounts(tr, config, sfc));
+
+    assertOrdinalsExist("$facets", ir);
+
+    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
+  }
+
+  @Test
+  public void testCustom() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    // create and open an index writer
+    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    // create and open a taxonomy writer
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
+
+    FacetsConfig config = getConfig(tw);
+    config.setIndexFieldName("Author", "$author");
+    seedIndex(iw, config);
+
+    IndexReader ir = iw.getReader();
+    tw.commit();
+
+    // prepare index reader and taxonomy.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+
+    // prepare searcher to search against
+    IndexSearcher searcher = newSearcher(ir);
+
+    FacetsCollector sfc = performSearch(tr, ir, searcher);
+
+    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    facetsMap.put("Author", getTaxonomyFacetCounts(tr, config, sfc, "$author"));
+    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
+
+    // Obtain facets results and hand-test them
+    assertCorrectResults(facets);
+
+    assertOrdinalsExist("$facets", ir);
+    assertOrdinalsExist("$author", ir);
+
+    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
+  }
+
+  @Test
+  public void testTwoCustomsSameField() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    // create and open an index writer
+    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    // create and open a taxonomy writer
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
+
+    FacetsConfig config = getConfig(tw);
+    config.setIndexFieldName("Band", "$music");
+    config.setIndexFieldName("Composer", "$music");
+    seedIndex(iw, config);
+
+    IndexReader ir = iw.getReader();
+    tw.commit();
+
+    // prepare index reader and taxonomy.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+
+    // prepare searcher to search against
+    IndexSearcher searcher = newSearcher(ir);
+
+    FacetsCollector sfc = performSearch(tr, ir, searcher);
+
+    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    Facets facets2 = getTaxonomyFacetCounts(tr, config, sfc, "$music");
+    facetsMap.put("Band", facets2);
+    facetsMap.put("Composer", facets2);
+    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
+
+    // Obtain facets results and hand-test them
+    assertCorrectResults(facets);
+
+    assertOrdinalsExist("$facets", ir);
+    assertOrdinalsExist("$music", ir);
+    assertOrdinalsExist("$music", ir);
+
+    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
+  }
+
+  private void assertOrdinalsExist(String field, IndexReader ir) throws IOException {
+    for (AtomicReaderContext context : ir.leaves()) {
+      AtomicReader r = context.reader();
+      if (r.getBinaryDocValues(field) != null) {
+        return; // not all segments must have this DocValues
+      }
+    }
+    fail("no ordinals found for " + field);
+  }
+
+  @Test
+  public void testDifferentFieldsAndText() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // create and open an index writer
+    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    // create and open a taxonomy writer
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
+
+    FacetsConfig config = getConfig(tw);
+    config.setIndexFieldName("Band", "$bands");
+    config.setIndexFieldName("Composer", "$composers");
+    seedIndex(iw, config);
+
+    IndexReader ir = iw.getReader();
+    tw.commit();
+
+    // prepare index reader and taxonomy.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+
+    // prepare searcher to search against
+    IndexSearcher searcher = newSearcher(ir);
+
+    FacetsCollector sfc = performSearch(tr, ir, searcher);
+
+    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    facetsMap.put("Band", getTaxonomyFacetCounts(tr, config, sfc, "$bands"));
+    facetsMap.put("Composer", getTaxonomyFacetCounts(tr, config, sfc, "$composers"));
+    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
+
+    // Obtain facets results and hand-test them
+    assertCorrectResults(facets);
+    assertOrdinalsExist("$facets", ir);
+    assertOrdinalsExist("$bands", ir);
+    assertOrdinalsExist("$composers", ir);
+
+    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
+  }
+
+  @Test
+  public void testSomeSameSomeDifferent() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    // create and open an index writer
+    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    // create and open a taxonomy writer
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
+
+    FacetsConfig config = getConfig(tw);
+    config.setIndexFieldName("Band", "$music");
+    config.setIndexFieldName("Composer", "$music");
+    config.setIndexFieldName("Author", "$literature");
+    seedIndex(iw, config);
+
+    IndexReader ir = iw.getReader();
+    tw.commit();
+
+    // prepare index reader and taxonomy.
+    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+
+    // prepare searcher to search against
+    IndexSearcher searcher = newSearcher(ir);
+
+    FacetsCollector sfc = performSearch(tr, ir, searcher);
+
+    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
+    Facets facets2 = getTaxonomyFacetCounts(tr, config, sfc, "$music");
+    facetsMap.put("Band", facets2);
+    facetsMap.put("Composer", facets2);
+    facetsMap.put("Author", getTaxonomyFacetCounts(tr, config, sfc, "$literature"));
+    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
+
+    // Obtain facets results and hand-test them
+    assertCorrectResults(facets);
+    assertOrdinalsExist("$music", ir);
+    assertOrdinalsExist("$literature", ir);
+
+    IOUtils.close(tr, ir, iw, tw);
+    IOUtils.close(indexDir, taxoDir);
+  }
+
+  private void assertCorrectResults(Facets facets) throws IOException {
+    assertEquals(5, facets.getSpecificValue("Band"));
+    assertEquals("value=5 childCount=2\n  Rock & Pop (4)\n  Punk (1)\n", facets.getTopChildren(10, "Band").toString());
+    assertEquals("value=4 childCount=4\n  The Beatles (1)\n  U2 (1)\n  REM (1)\n  Dave Matthews Band (1)\n", facets.getTopChildren(10, "Band", "Rock & Pop").toString());
+    assertEquals("value=3 childCount=3\n  Mark Twain (1)\n  Stephen King (1)\n  Kurt Vonnegut (1)\n", facets.getTopChildren(10, "Author").toString());
+  }
+
+  private FacetsCollector performSearch(TaxonomyReader tr, IndexReader ir, 
+      IndexSearcher searcher) throws IOException {
+    FacetsCollector sfc = new FacetsCollector();
+    Facets.search(searcher, new MatchAllDocsQuery(), 10, sfc);
+    return sfc;
+  }
+
+  private void seedIndex(RandomIndexWriter iw, FacetsConfig config) throws IOException {
+    for (FacetField ff : CATEGORIES) {
+      Document doc = new Document();
+      doc.add(ff);
+      doc.add(new TextField("content", "alpha", Field.Store.YES));
+      iw.addDocument(config.build(doc));
+    }
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestRangeFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/TestRangeFacets.java
new file mode 100644
index 0000000..a520014
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestRangeFacets.java
@@ -0,0 +1,540 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.DoubleDocValuesField;
+import org.apache.lucene.document.DoubleField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FloatDocValuesField;
+import org.apache.lucene.document.FloatField;
+import org.apache.lucene.document.LongField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.DrillSideways.DrillSidewaysResult;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.NumericRangeQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+
+// nocommit rename to TestRangeFacetCounts
+public class TestRangeFacets extends FacetTestCase {
+
+  public void testBasicLong() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      field.setLongValue(l);
+      w.addDocument(doc);
+    }
+    field.setLongValue(Long.MAX_VALUE);
+    w.addDocument(doc);
+
+    IndexReader r = w.getReader();
+    w.close();
+
+    FacetsCollector fc = new FacetsCollector();
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    RangeFacetCounts facets = new RangeFacetCounts("field", fc,
+        new LongRange("less than 10", 0L, true, 10L, false),
+        new LongRange("less than or equal to 10", 0L, true, 10L, true),
+        new LongRange("over 90", 90L, false, 100L, false),
+        new LongRange("90 or above", 90L, true, 100L, false),
+        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, true));
+    
+    FacetResult result = facets.getTopChildren(10, "field");
+    assertEquals("value=101 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (1)\n",
+                 result.toString());
+    
+    r.close();
+    d.close();
+  }
+
+  /** Tests single request that mixes Range and non-Range
+   *  faceting, with DrillSideways and taxonomy. */
+  public void testMixedRangeAndNonRangeTaxonomy() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Directory td = newDirectory();
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig(tw);
+
+    for (long l = 0; l < 100; l++) {
+      Document doc = new Document();
+      // For computing range facet counts:
+      doc.add(new NumericDocValuesField("field", l));
+      // For drill down by numeric range:
+      doc.add(new LongField("field", l, Field.Store.NO));
+
+      if ((l&3) == 0) {
+        doc.add(new FacetField("dim", "a"));
+      } else {
+        doc.add(new FacetField("dim", "b"));
+      }
+      w.addDocument(config.build(doc));
+    }
+
+    final IndexReader r = w.getReader();
+
+    final TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
+
+    IndexSearcher s = newSearcher(r);
+
+    DrillSideways ds = new DrillSideways(s, config, tr) {
+
+        @Override
+        protected Facets buildFacetsResult(FacetsCollector drillDowns, FacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {        
+          // nocommit this is awkward... can we improve?
+          // nocommit is drillDowns allowed to be null?
+          // should it?
+          FacetsCollector dimFC = drillDowns;
+          FacetsCollector fieldFC = drillDowns;
+          if (drillSideways != null) {
+            for(int i=0;i<drillSideways.length;i++) {
+              String dim = drillSidewaysDims[i];
+              if (dim.equals("field")) {
+                fieldFC = drillSideways[i];
+              } else {
+                dimFC = drillSideways[i];
+              }
+            }
+          }
+
+          Map<String,Facets> byDim = new HashMap<String,Facets>();
+          byDim.put("field",
+                    new RangeFacetCounts("field", fieldFC,
+                          new LongRange("less than 10", 0L, true, 10L, false),
+                          new LongRange("less than or equal to 10", 0L, true, 10L, true),
+                          new LongRange("over 90", 90L, false, 100L, false),
+                          new LongRange("90 or above", 90L, true, 100L, false),
+                          new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false)));
+          byDim.put("dim", getTaxonomyFacetCounts(taxoReader, config, dimFC));
+          return new MultiFacets(byDim, null);
+        }
+
+        @Override
+        protected boolean scoreSubDocsAtOnce() {
+          return random().nextBoolean();
+        }
+      };
+
+    // First search, no drill downs:
+    DrillDownQuery ddq = new DrillDownQuery(config);
+    DrillSidewaysResult dsr = ds.search(null, ddq, 10);
+
+    assertEquals(100, dsr.hits.totalHits);
+    assertEquals("value=100 childCount=2\n  b (75)\n  a (25)\n", dsr.facets.getTopChildren(10, "dim").toString());
+    assertEquals("value=100 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 dsr.facets.getTopChildren(10, "field").toString());
+
+    // Second search, drill down on dim=b:
+    ddq = new DrillDownQuery(config);
+    ddq.add("dim", "b");
+    dsr = ds.search(null, ddq, 10);
+
+    assertEquals(75, dsr.hits.totalHits);
+    assertEquals("value=100 childCount=2\n  b (75)\n  a (25)\n", dsr.facets.getTopChildren(10, "dim").toString());
+    assertEquals("value=75 childCount=5\n  less than 10 (7)\n  less than or equal to 10 (8)\n  over 90 (7)\n  90 or above (8)\n  over 1000 (0)\n",
+                 dsr.facets.getTopChildren(10, "field").toString());
+
+    // Third search, drill down on "less than or equal to 10":
+    ddq = new DrillDownQuery(config);
+    ddq.add("field", NumericRangeQuery.newLongRange("field", 0L, 10L, true, true));
+    dsr = ds.search(null, ddq, 10);
+
+    assertEquals(11, dsr.hits.totalHits);
+    assertEquals("value=11 childCount=2\n  b (8)\n  a (3)\n", dsr.facets.getTopChildren(10, "dim").toString());
+    assertEquals("value=100 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 dsr.facets.getTopChildren(10, "field").toString());
+    IOUtils.close(tw, tr, td, w, r, d);
+  }
+
+  public void testBasicDouble() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    DoubleDocValuesField field = new DoubleDocValuesField("field", 0.0);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      field.setDoubleValue(l);
+      w.addDocument(doc);
+    }
+
+    IndexReader r = w.getReader();
+
+    FacetsCollector fc = new FacetsCollector();
+
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+    Facets facets = new RangeFacetCounts("field", fc,
+        new DoubleRange("less than 10", 0.0, true, 10.0, false),
+        new DoubleRange("less than or equal to 10", 0.0, true, 10.0, true),
+        new DoubleRange("over 90", 90.0, false, 100.0, false),
+        new DoubleRange("90 or above", 90.0, true, 100.0, false),
+        new DoubleRange("over 1000", 1000.0, false, Double.POSITIVE_INFINITY, false));
+                                         
+    assertEquals("value=100 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 facets.getTopChildren(10, "field").toString());
+
+    IOUtils.close(w, r, d);
+  }
+
+  public void testBasicFloat() throws Exception {
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    FloatDocValuesField field = new FloatDocValuesField("field", 0.0f);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      field.setFloatValue(l);
+      w.addDocument(doc);
+    }
+
+    IndexReader r = w.getReader();
+
+    FacetsCollector fc = new FacetsCollector();
+
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new RangeFacetCounts("field", fc,
+        new FloatRange("less than 10", 0.0f, true, 10.0f, false),
+        new FloatRange("less than or equal to 10", 0.0f, true, 10.0f, true),
+        new FloatRange("over 90", 90.0f, false, 100.0f, false),
+        new FloatRange("90 or above", 90.0f, true, 100.0f, false),
+        new FloatRange("over 1000", 1000.0f, false, Float.POSITIVE_INFINITY, false));
+    
+    assertEquals("value=100 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
+                 facets.getTopChildren(10, "field").toString());
+    
+    IOUtils.close(w, r, d);
+  }
+
+  public void testRandomLongs() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+
+    int numDocs = atLeast(1000);
+    long[] values = new long[numDocs];
+    for(int i=0;i<numDocs;i++) {
+      Document doc = new Document();
+      long v = random().nextLong();
+      values[i] = v;
+      doc.add(new NumericDocValuesField("field", v));
+      doc.add(new LongField("field", v, Field.Store.NO));
+      w.addDocument(doc);
+    }
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+    FacetsConfig config = new FacetsConfig();
+    
+    int numIters = atLeast(10);
+    for(int iter=0;iter<numIters;iter++) {
+      if (VERBOSE) {
+        System.out.println("TEST: iter=" + iter);
+      }
+      int numRange = _TestUtil.nextInt(random(), 1, 5);
+      LongRange[] ranges = new LongRange[numRange];
+      int[] expectedCounts = new int[numRange];
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        long min = random().nextLong();
+        long max = random().nextLong();
+        if (min > max) {
+          long x = min;
+          min = max;
+          max = x;
+        }
+        boolean minIncl = random().nextBoolean();
+        boolean maxIncl = random().nextBoolean();
+        ranges[rangeID] = new LongRange("r" + rangeID, min, minIncl, max, maxIncl);
+
+        // Do "slow but hopefully correct" computation of
+        // expected count:
+        for(int i=0;i<numDocs;i++) {
+          boolean accept = true;
+          if (minIncl) {
+            accept &= values[i] >= min;
+          } else {
+            accept &= values[i] > min;
+          }
+          if (maxIncl) {
+            accept &= values[i] <= max;
+          } else {
+            accept &= values[i] < max;
+          }
+          if (accept) {
+            expectedCounts[rangeID]++;
+          }
+        }
+      }
+
+      FacetsCollector sfc = new FacetsCollector();
+      s.search(new MatchAllDocsQuery(), sfc);
+      Facets facets = new RangeFacetCounts("field", sfc, ranges);
+      FacetResult result = facets.getTopChildren(10, "field");
+      assertEquals(numRange, result.labelValues.length);
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        if (VERBOSE) {
+          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
+        }
+        LabelAndValue subNode = result.labelValues[rangeID];
+        assertEquals("r" + rangeID, subNode.label);
+        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
+
+        LongRange range = ranges[rangeID];
+
+        // Test drill-down:
+        DrillDownQuery ddq = new DrillDownQuery(config);
+        ddq.add("field", NumericRangeQuery.newLongRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
+        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
+      }
+    }
+
+    IOUtils.close(w, r, dir);
+  }
+
+  public void testRandomFloats() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+
+    int numDocs = atLeast(1000);
+    float[] values = new float[numDocs];
+    for(int i=0;i<numDocs;i++) {
+      Document doc = new Document();
+      float v = random().nextFloat();
+      values[i] = v;
+      doc.add(new FloatDocValuesField("field", v));
+      doc.add(new FloatField("field", v, Field.Store.NO));
+      w.addDocument(doc);
+    }
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+    FacetsConfig config = new FacetsConfig();
+    
+    int numIters = atLeast(10);
+    for(int iter=0;iter<numIters;iter++) {
+      if (VERBOSE) {
+        System.out.println("TEST: iter=" + iter);
+      }
+      int numRange = _TestUtil.nextInt(random(), 1, 5);
+      FloatRange[] ranges = new FloatRange[numRange];
+      int[] expectedCounts = new int[numRange];
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        float min = random().nextFloat();
+        float max = random().nextFloat();
+        if (min > max) {
+          float x = min;
+          min = max;
+          max = x;
+        }
+        boolean minIncl = random().nextBoolean();
+        boolean maxIncl = random().nextBoolean();
+        ranges[rangeID] = new FloatRange("r" + rangeID, min, minIncl, max, maxIncl);
+
+        // Do "slow but hopefully correct" computation of
+        // expected count:
+        for(int i=0;i<numDocs;i++) {
+          boolean accept = true;
+          if (minIncl) {
+            accept &= values[i] >= min;
+          } else {
+            accept &= values[i] > min;
+          }
+          if (maxIncl) {
+            accept &= values[i] <= max;
+          } else {
+            accept &= values[i] < max;
+          }
+          if (accept) {
+            expectedCounts[rangeID]++;
+          }
+        }
+      }
+
+      FacetsCollector sfc = new FacetsCollector();
+      s.search(new MatchAllDocsQuery(), sfc);
+      Facets facets = new RangeFacetCounts("field", sfc, ranges);
+      FacetResult result = facets.getTopChildren(10, "field");
+      assertEquals(numRange, result.labelValues.length);
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        if (VERBOSE) {
+          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
+        }
+        LabelAndValue subNode = result.labelValues[rangeID];
+        assertEquals("r" + rangeID, subNode.label);
+        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
+
+        FloatRange range = ranges[rangeID];
+
+        // Test drill-down:
+        DrillDownQuery ddq = new DrillDownQuery(config);
+        ddq.add("field", NumericRangeQuery.newFloatRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
+        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
+      }
+    }
+
+    IOUtils.close(w, r, dir);
+  }
+
+  public void testRandomDoubles() throws Exception {
+    Directory dir = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
+
+    int numDocs = atLeast(1000);
+    double[] values = new double[numDocs];
+    for(int i=0;i<numDocs;i++) {
+      Document doc = new Document();
+      double v = random().nextDouble();
+      values[i] = v;
+      doc.add(new DoubleDocValuesField("field", v));
+      doc.add(new DoubleField("field", v, Field.Store.NO));
+      w.addDocument(doc);
+    }
+    IndexReader r = w.getReader();
+
+    IndexSearcher s = newSearcher(r);
+    FacetsConfig config = new FacetsConfig();
+    
+    int numIters = atLeast(10);
+    for(int iter=0;iter<numIters;iter++) {
+      if (VERBOSE) {
+        System.out.println("TEST: iter=" + iter);
+      }
+      int numRange = _TestUtil.nextInt(random(), 1, 5);
+      DoubleRange[] ranges = new DoubleRange[numRange];
+      int[] expectedCounts = new int[numRange];
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        double min = random().nextDouble();
+        double max = random().nextDouble();
+        if (min > max) {
+          double x = min;
+          min = max;
+          max = x;
+        }
+        boolean minIncl = random().nextBoolean();
+        boolean maxIncl = random().nextBoolean();
+        ranges[rangeID] = new DoubleRange("r" + rangeID, min, minIncl, max, maxIncl);
+
+        // Do "slow but hopefully correct" computation of
+        // expected count:
+        for(int i=0;i<numDocs;i++) {
+          boolean accept = true;
+          if (minIncl) {
+            accept &= values[i] >= min;
+          } else {
+            accept &= values[i] > min;
+          }
+          if (maxIncl) {
+            accept &= values[i] <= max;
+          } else {
+            accept &= values[i] < max;
+          }
+          if (accept) {
+            expectedCounts[rangeID]++;
+          }
+        }
+      }
+
+      FacetsCollector sfc = new FacetsCollector();
+      s.search(new MatchAllDocsQuery(), sfc);
+      Facets facets = new RangeFacetCounts("field", sfc, ranges);
+      FacetResult result = facets.getTopChildren(10, "field");
+      assertEquals(numRange, result.labelValues.length);
+      for(int rangeID=0;rangeID<numRange;rangeID++) {
+        if (VERBOSE) {
+          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
+        }
+        LabelAndValue subNode = result.labelValues[rangeID];
+        assertEquals("r" + rangeID, subNode.label);
+        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
+
+        DoubleRange range = ranges[rangeID];
+
+        // Test drill-down:
+        DrillDownQuery ddq = new DrillDownQuery(config);
+        ddq.add("field", NumericRangeQuery.newDoubleRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
+        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
+      }
+    }
+
+    IOUtils.close(w, r, dir);
+  }
+
+  // LUCENE-5178
+  public void testMissingValues() throws Exception {
+    assumeTrue("codec does not support docsWithField", defaultCodecSupportsDocsWithField());
+    Directory d = newDirectory();
+    RandomIndexWriter w = new RandomIndexWriter(random(), d);
+    Document doc = new Document();
+    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
+    doc.add(field);
+    for(long l=0;l<100;l++) {
+      if (l % 5 == 0) {
+        // Every 5th doc is missing the value:
+        w.addDocument(new Document());
+        continue;
+      }
+      field.setLongValue(l);
+      w.addDocument(doc);
+    }
+
+    IndexReader r = w.getReader();
+
+    FacetsCollector sfc = new FacetsCollector();
+
+    IndexSearcher s = newSearcher(r);
+    s.search(new MatchAllDocsQuery(), sfc);
+    Facets facets = new RangeFacetCounts("field", sfc,
+        new LongRange("less than 10", 0L, true, 10L, false),
+        new LongRange("less than or equal to 10", 0L, true, 10L, true),
+        new LongRange("over 90", 90L, false, 100L, false),
+        new LongRange("90 or above", 90L, true, 100L, false),
+        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false));
+    
+    assertEquals("value=100 childCount=5\n  less than 10 (8)\n  less than or equal to 10 (8)\n  over 90 (8)\n  90 or above (8)\n  over 1000 (0)\n",
+                 facets.getTopChildren(10, "field").toString());
+
+    IOUtils.close(w, r, d);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestSearcherTaxonomyManager.java b/lucene/facet/src/test/org/apache/lucene/facet/TestSearcherTaxonomyManager.java
new file mode 100644
index 0000000..b3fe6cb
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestSearcherTaxonomyManager.java
@@ -0,0 +1,185 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.SearcherTaxonomyManager.SearcherAndTaxonomy;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestSearcherTaxonomyManager extends FacetTestCase {
+  public void test() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    final IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    final DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+    final FacetsConfig config = new FacetsConfig(tw);
+    config.setMultiValued("field", true);
+    final AtomicBoolean stop = new AtomicBoolean();
+
+    // How many unique facets to index before stopping:
+    final int ordLimit = TEST_NIGHTLY ? 100000 : 6000;
+
+    Thread indexer = new Thread() {
+        @Override
+        public void run() {
+          try {
+            Set<String> seen = new HashSet<String>();
+            List<String> paths = new ArrayList<String>();
+            while (true) {
+              Document doc = new Document();
+              int numPaths = _TestUtil.nextInt(random(), 1, 5);
+              for(int i=0;i<numPaths;i++) {
+                String path;
+                if (!paths.isEmpty() && random().nextInt(5) != 4) {
+                  // Use previous path
+                  path = paths.get(random().nextInt(paths.size()));
+                } else {
+                  // Create new path
+                  path = null;
+                  while (true) {
+                    path = _TestUtil.randomRealisticUnicodeString(random());
+                    if (path.length() != 0 && !seen.contains(path)) {
+                      seen.add(path);
+                      paths.add(path);
+                      break;
+                    }
+                  }
+                }
+                doc.add(new FacetField("field", path));
+              }
+              try {
+                w.addDocument(config.build(doc));
+              } catch (IOException ioe) {
+                throw new RuntimeException(ioe);
+              }
+
+              if (tw.getSize() >= ordLimit) {
+                break;
+              }
+            }
+          } finally {
+            stop.set(true);
+          }
+        }
+      };
+
+    final SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
+
+    Thread reopener = new Thread() {
+        @Override
+        public void run() {
+          while(!stop.get()) {
+            try {
+              // Sleep for up to 20 msec:
+              Thread.sleep(random().nextInt(20));
+
+              if (VERBOSE) {
+                System.out.println("TEST: reopen");
+              }
+
+              mgr.maybeRefresh();
+
+              if (VERBOSE) {
+                System.out.println("TEST: reopen done");
+              }
+            } catch (Exception ioe) {
+              throw new RuntimeException(ioe);
+            }
+          }
+        }
+      };
+    reopener.start();
+
+    indexer.start();
+
+    try {
+      while (!stop.get()) {
+        SearcherAndTaxonomy pair = mgr.acquire();
+        try {
+          //System.out.println("search maxOrd=" + pair.taxonomyReader.getSize());
+          int topN = _TestUtil.nextInt(random(), 1, 20);
+          
+          FacetsCollector sfc = new FacetsCollector();
+          pair.searcher.search(new MatchAllDocsQuery(), sfc);
+          Facets facets = getTaxonomyFacetCounts(pair.taxonomyReader, config, sfc);
+          FacetResult result = facets.getTopChildren(10, "field");
+          if (pair.searcher.getIndexReader().numDocs() > 0) { 
+            //System.out.println(pair.taxonomyReader.getSize());
+            assertTrue(result.childCount > 0);
+            assertTrue(result.labelValues.length > 0);
+          }
+
+          //if (VERBOSE) {
+          //System.out.println("TEST: facets=" + FacetTestUtils.toString(results.get(0)));
+          //}
+        } finally {
+          mgr.release(pair);
+        }
+      }
+    } finally {
+      indexer.join();
+      reopener.join();
+    }
+
+    if (VERBOSE) {
+      System.out.println("TEST: now stop");
+    }
+
+    IOUtils.close(mgr, tw, w, taxoDir, dir);
+  }
+
+  public void testReplaceTaxonomy() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+
+    Directory taxoDir2 = newDirectory();
+    DirectoryTaxonomyWriter tw2 = new DirectoryTaxonomyWriter(taxoDir2);
+    tw2.close();
+
+    SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
+    w.addDocument(new Document());
+    tw.replaceTaxonomy(taxoDir2);
+    taxoDir2.close();
+
+    try {
+      mgr.maybeRefresh();
+      fail("should have hit exception");
+    } catch (IllegalStateException ise) {
+      // expected
+    }
+
+    IOUtils.close(mgr, tw, w, taxoDir, dir);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestSortedSetDocValuesFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/TestSortedSetDocValuesFacets.java
new file mode 100644
index 0000000..8e89276
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestSortedSetDocValuesFacets.java
@@ -0,0 +1,222 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.List;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+
+public class TestSortedSetDocValuesFacets extends FacetTestCase {
+
+  // NOTE: TestDrillSideways.testRandom also sometimes
+  // randomly uses SortedSetDV
+
+  public void testBasic() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    FacetsConfig config = new FacetsConfig();
+    config.setMultiValued("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
+    doc.add(new SortedSetDocValuesFacetField("a", "bar"));
+    doc.add(new SortedSetDocValuesFacetField("a", "zoo"));
+    doc.add(new SortedSetDocValuesFacetField("b", "baz"));
+    writer.addDocument(config.build(doc));
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+    
+    FacetsCollector c = new FacetsCollector();
+
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    assertEquals("value=4 childCount=3\n  foo (2)\n  bar (1)\n  zoo (1)\n", facets.getTopChildren(10, "a").toString());
+    assertEquals("value=1 childCount=1\n  baz (1)\n", facets.getTopChildren(10, "b").toString());
+
+    // DrillDown:
+    DrillDownQuery q = new DrillDownQuery(config);
+    q.add("a", "foo");
+    q.add("b", "baz");
+    TopDocs hits = searcher.search(q, 1);
+    assertEquals(1, hits.totalHits);
+
+    IOUtils.close(writer, searcher.getIndexReader(), dir);
+  }
+
+  // LUCENE-5090
+  public void testStaleState() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
+    writer.addDocument(config.build(doc));
+
+    IndexReader r = writer.getReader();
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(r);
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "bar"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "baz"));
+    writer.addDocument(config.build(doc));
+
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    FacetsCollector c = new FacetsCollector();
+
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    try {
+      new SortedSetDocValuesFacetCounts(state, c);
+      fail("did not hit expected exception");
+    } catch (IllegalStateException ise) {
+      // expected
+    }
+
+    r.close();
+    writer.close();
+    searcher.getIndexReader().close();
+    dir.close();
+  }
+
+  // LUCENE-5333
+  public void testSparseFacets() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
+    doc.add(new SortedSetDocValuesFacetField("b", "bar1"));
+    writer.addDocument(config.build(doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo3"));
+    doc.add(new SortedSetDocValuesFacetField("b", "bar2"));
+    doc.add(new SortedSetDocValuesFacetField("c", "baz1"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+
+    assertEquals(3, results.size());
+    assertEquals("value=3 childCount=3\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
+    assertEquals("value=2 childCount=2\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
+    assertEquals("value=1 childCount=1\n  baz1 (1)\n", results.get(2).toString());
+
+    searcher.getIndexReader().close();
+    dir.close();
+  }
+
+  // nocommit test different delim char & using the default
+  // one in a dim
+
+  // nocommit in the sparse case test that we are really
+  // sorting by the correct dim count
+
+  public void testSlowCompositeReaderWrapper() throws Exception {
+    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
+    Directory dir = newDirectory();
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    FacetsConfig config = new FacetsConfig();
+
+    Document doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+
+    writer.commit();
+
+    doc = new Document();
+    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = new IndexSearcher(SlowCompositeReaderWrapper.wrap(writer.getReader()));
+
+    // Per-top-reader state:
+    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+    Facets facets = new SortedSetDocValuesFacetCounts(state, c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    assertEquals("value=2 childCount=2\n  foo1 (1)\n  foo2 (1)\n", facets.getTopChildren(10, "a").toString());
+
+    IOUtils.close(writer, searcher.getIndexReader(), dir);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetAssociations.java b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetAssociations.java
new file mode 100644
index 0000000..bc41861
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetAssociations.java
@@ -0,0 +1,224 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+
+/** Test for associations */
+public class TestTaxonomyFacetAssociations extends FacetTestCase {
+  
+  private static Directory dir;
+  private static IndexReader reader;
+  private static Directory taxoDir;
+  private static TaxonomyReader taxoReader;
+
+  private static final FacetLabel aint = new FacetLabel("int", "a");
+  private static final FacetLabel bint = new FacetLabel("int", "b");
+  private static final FacetLabel afloat = new FacetLabel("float", "a");
+  private static final FacetLabel bfloat = new FacetLabel("float", "b");
+  private static FacetsConfig config;
+
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    dir = newDirectory();
+    taxoDir = newDirectory();
+    // preparations - index, taxonomy, content
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+
+    // Cannot mix ints & floats in the same indexed field:
+    config = new FacetsConfig(taxoWriter);
+    config.setIndexFieldName("int", "$facets.int");
+    config.setMultiValued("int", true);
+    config.setIndexFieldName("float", "$facets.float");
+    config.setMultiValued("float", true);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    // index documents, 50% have only 'b' and all have 'a'
+    for (int i = 0; i < 110; i++) {
+      Document doc = new Document();
+      // every 11th document is added empty, this used to cause the association
+      // aggregators to go into an infinite loop
+      if (i % 11 != 0) {
+        doc.add(new IntAssociationFacetField(2, "int", "a"));
+        doc.add(new FloatAssociationFacetField(0.5f, "float", "a"));
+        if (i % 2 == 0) { // 50
+          doc.add(new IntAssociationFacetField(3, "int", "b"));
+          doc.add(new FloatAssociationFacetField(0.2f, "float", "b"));
+        }
+      }
+      writer.addDocument(config.build(doc));
+    }
+    
+    taxoWriter.close();
+    reader = writer.getReader();
+    writer.close();
+    taxoReader = new DirectoryTaxonomyReader(taxoDir);
+  }
+  
+  @AfterClass
+  public static void afterClass() throws Exception {
+    reader.close();
+    reader = null;
+    dir.close();
+    dir = null;
+    taxoReader.close();
+    taxoReader = null;
+    taxoDir.close();
+    taxoDir = null;
+  }
+  
+  public void testIntSumAssociation() throws Exception {
+    
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+
+    Facets facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
+    assertEquals("value=350 childCount=2\n  a (200)\n  b (150)\n", facets.getTopChildren(10, "int").toString());
+    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
+    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
+  }
+
+  public void testFloatSumAssociation() throws Exception {
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    
+    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
+    assertEquals("value=59.999996 childCount=2\n  a (50.0)\n  b (9.999995)\n", facets.getTopChildren(10, "float").toString());
+    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
+    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
+  }  
+
+  /** Make sure we can test both int and float assocs in one
+   *  index, as long as we send each to a different field. */
+  public void testIntAndFloatAssocation() throws Exception {
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    
+    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
+    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
+    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
+    
+    facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
+    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
+    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+    FacetsCollector fc = new FacetsCollector();
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    Facets facets = new TaxonomyFacetSumFloatAssociations(taxoReader, config, fc);
+    try {
+      facets.getSpecificValue("float");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "float");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+  }
+
+  public void testMixedTypesInSameIndexField() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntAssociationFacetField(14, "a", "x"));
+    doc.add(new FloatAssociationFacetField(55.0f, "b", "y"));
+    try {
+      writer.addDocument(config.build(doc));
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException exc) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testNoHierarchy() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setHierarchical("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntAssociationFacetField(14, "a", "x"));
+    try {
+      writer.addDocument(config.build(doc));
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException exc) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testRequireDimCount() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setRequireDimCount("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntAssociationFacetField(14, "a", "x"));
+    try {
+      writer.addDocument(config.build(doc));
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException exc) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts.java b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts.java
new file mode 100644
index 0000000..3bf34c9
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts.java
@@ -0,0 +1,648 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayOutputStream;
+import java.io.PrintStream;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.PrintTaxonomyStats;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.similarities.DefaultSimilarity;
+import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
+import org.apache.lucene.search.similarities.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestTaxonomyFacetCounts extends FacetTestCase {
+
+  public void testBasic() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setHierarchical("Publish Date", true);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Susan"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Frank"));
+    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+
+    // Retrieve & verify results:
+    assertEquals("value=5 childCount=3\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", facets.getTopChildren(10, "Publish Date").toString());
+    assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", facets.getTopChildren(10, "Author").toString());
+
+    // Now user drills down on Publish Date/2010:
+    DrillDownQuery q2 = new DrillDownQuery(config);
+    q2.add("Publish Date", "2010");
+    c = new FacetsCollector();
+    searcher.search(q2, c);
+    facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+    assertEquals("value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", facets.getTopChildren(10, "Author").toString());
+
+    assertEquals(1, facets.getSpecificValue("Author", "Lisa"));
+
+    // Smoke test PrintTaxonomyStats:
+    ByteArrayOutputStream bos = new ByteArrayOutputStream();
+    PrintTaxonomyStats.printStats(taxoReader, new PrintStream(bos, false, "UTF-8"), true);
+    String result = bos.toString("UTF-8");
+    assertTrue(result.indexOf("/Author: 4 immediate children; 5 total categories") != -1);
+    assertTrue(result.indexOf("/Publish Date: 3 immediate children; 12 total categories") != -1);
+    // Make sure at least a few nodes of the tree came out:
+    assertTrue(result.indexOf("  /1999") != -1);
+    assertTrue(result.indexOf("  /2012") != -1);
+    assertTrue(result.indexOf("      /20") != -1);
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
+  }
+
+  // LUCENE-5333
+  public void testSparseFacets() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+
+    Document doc = new Document();
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new FacetField("a", "foo2"));
+    doc.add(new FacetField("b", "bar1"));
+    writer.addDocument(config.build(doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new FacetField("a", "foo3"));
+    doc.add(new FacetField("b", "bar2"));
+    doc.add(new FacetField("c", "baz1"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, new FacetsConfig(), c);
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+
+    assertEquals(3, results.size());
+    assertEquals("value=3 childCount=3\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
+    assertEquals("value=2 childCount=2\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
+    assertEquals("value=1 childCount=1\n  baz1 (1)\n", results.get(2).toString());
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setIndexFieldName("a", "$facets2");
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    // Uses default $facets field:
+    Facets facets;
+    if (random().nextBoolean()) {
+      facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
+    } else {
+      OrdinalsReader ordsReader = new DocValuesOrdinalsReader();
+      if (random().nextBoolean()) {
+        ordsReader = new CachedOrdinalsReader(ordsReader);
+      }
+      facets = new TaxonomyFacetCounts(ordsReader, taxoReader, config, c);
+    }
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+    assertTrue(results.isEmpty());
+
+    try {
+      facets.getSpecificValue("a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
+  }
+
+
+  // nocommit in the sparse case test that we are really
+  // sorting by the correct dim count
+
+  public void testReallyNoNormsForDrillDown() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setSimilarity(new PerFieldSimilarityWrapper() {
+        final Similarity sim = new DefaultSimilarity();
+
+        @Override
+        public Similarity get(String name) {
+          assertEquals("field", name);
+          return sim;
+        }
+      });
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path"));
+    writer.addDocument(config.build(doc));
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testMultiValuedHierarchy() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setHierarchical("a", true);
+    config.setMultiValued("a", true);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path", "x"));
+    doc.add(new FacetField("a", "path", "y"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+    FacetResult result = facets.getTopChildren(10, "a");
+    assertEquals(1, result.labelValues.length);
+    assertEquals(1, result.labelValues[0].value.intValue());
+
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testLabelWithDelimiter() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setMultiValued("dim", true);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("dim", "test\u001Fone"));
+    doc.add(new FacetField("dim", "test\u001Etwo"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);
+    
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+    assertEquals(1, facets.getSpecificValue("dim", "test\u001Fone"));
+    assertEquals(1, facets.getSpecificValue("dim", "test\u001Etwo"));
+
+    FacetResult result = facets.getTopChildren(10, "dim");
+    assertEquals("value=-1 childCount=2\n  test\u001Fone (1)\n  test\u001Etwo (1)\n", result.toString());
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testRequireDimCount() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setMultiValued("dim2", true);
+    config.setMultiValued("dim3", true);
+    config.setHierarchical("dim3", true);
+    config.setRequireDimCount("dim", true);
+    config.setRequireDimCount("dim2", true);
+    config.setRequireDimCount("dim3", true);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("dim", "a"));
+    doc.add(new FacetField("dim2", "a"));
+    doc.add(new FacetField("dim2", "b"));
+    doc.add(new FacetField("dim3", "a", "b"));
+    doc.add(new FacetField("dim3", "a", "c"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);
+    
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+    assertEquals(1, facets.getTopChildren(10, "dim").value);
+    assertEquals(1, facets.getTopChildren(10, "dim2").value);
+    assertEquals(1, facets.getTopChildren(10, "dim3").value);
+    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  // LUCENE-4583: make sure if we require > 32 KB for one
+  // document, we don't hit exc when using Facet42DocValuesFormat
+  public void testManyFacetsInOneDocument() throws Exception {
+    assumeTrue("default Codec doesn't support huge BinaryDocValues", _TestUtil.fieldSupportsHugeBinaryDocValues(FacetsConfig.DEFAULT_INDEX_FIELD_NAME));
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setMultiValued("dim", true);
+    
+    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);
+    
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    for (int i = 0; i < numLabels; i++) {
+      doc.add(new FacetField("dim", "" + i));
+    }
+    writer.addDocument(config.build(doc));
+    
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+    
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
+
+    FacetResult result = facets.getTopChildren(Integer.MAX_VALUE, "dim");
+    assertEquals(numLabels, result.labelValues.length);
+    Set<String> allLabels = new HashSet<String>();
+    for (LabelAndValue labelValue : result.labelValues) {
+      allLabels.add(labelValue.label);
+      assertEquals(1, labelValue.value.intValue());
+    }
+    assertEquals(numLabels, allLabels.size());
+    
+    IOUtils.close(searcher.getIndexReader(), taxoWriter, writer, taxoReader, dir, taxoDir);
+  }
+
+  // Make sure we catch when app didn't declare field as
+  // hierarchical but it was:
+  public void testDetectHierarchicalField() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path", "other"));
+    try {
+      config.build(doc);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  // Make sure we catch when app didn't declare field as
+  // multi-valued but it was:
+  public void testDetectMultiValuedField() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+
+    Document doc = new Document();
+    doc.add(newTextField("field", "text", Field.Store.NO));
+    doc.add(new FacetField("a", "path"));
+    doc.add(new FacetField("a", "path2"));
+    try {
+      config.build(doc);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    IOUtils.close(writer, taxoWriter, dir, taxoDir);
+  }
+
+  public void testSeparateIndexedFields() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setIndexFieldName("b", "$b");
+    
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      doc.add(new StringField("f", "v", Field.Store.NO));
+      doc.add(new FacetField("a", "1"));
+      doc.add(new FacetField("b", "1"));
+      iw.addDocument(config.build(doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets1 = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    Facets facets2 = getTaxonomyFacetCounts(taxoReader, config, sfc, "$b");
+    assertEquals(r.maxDoc(), facets1.getTopChildren(10, "a").value.intValue());
+    assertEquals(r.maxDoc(), facets2.getTopChildren(10, "b").value.intValue());
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+  
+  public void testCountRoot() throws Exception {
+    // LUCENE-4882: FacetsAccumulator threw NPE if a FacetRequest was defined on CP.EMPTY
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      doc.add(new FacetField("a", "1"));
+      doc.add(new FacetField("b", "1"));
+      iw.addDocument(config.build(doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    for (FacetResult result : facets.getAllDims(10)) {
+      assertEquals(r.numDocs(), result.value.intValue());
+    }
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testGetFacetResultsTwice() throws Exception {
+    // LUCENE-4893: counts were multiplied as many times as getFacetResults was called.
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+
+    Document doc = new Document();
+    doc.add(new FacetField("a", "1"));
+    doc.add(new FacetField("b", "1"));
+    iw.addDocument(config.build(doc));
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    final FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    List<FacetResult> res1 = facets.getAllDims(10);
+    List<FacetResult> res2 = facets.getAllDims(10);
+    assertEquals("calling getFacetResults twice should return the .equals()=true result", res1, res2);
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+  
+  public void testChildCount() throws Exception {
+    // LUCENE-4885: FacetResult.numValidDescendants was not set properly by FacetsAccumulator
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    for (int i = 0; i < 10; i++) {
+      Document doc = new Document();
+      doc.add(new FacetField("a", Integer.toString(i)));
+      iw.addDocument(config.build(doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    
+    assertEquals(10, facets.getTopChildren(2, "a").childCount);
+
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  private void indexTwoDocs(IndexWriter indexWriter, FacetsConfig config, boolean withContent) throws Exception {
+    for (int i = 0; i < 2; i++) {
+      Document doc = new Document();
+      if (withContent) {
+        doc.add(new StringField("f", "a", Field.Store.NO));
+      }
+      if (config != null) {
+        doc.add(new FacetField("A", Integer.toString(i)));
+        indexWriter.addDocument(config.build(doc));
+      } else {
+        indexWriter.addDocument(doc);
+      }
+    }
+    
+    indexWriter.commit();
+  }
+  
+  public void testSegmentsWithoutCategoriesOrResults() throws Exception {
+    // tests the accumulator when there are segments with no results
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges
+    IndexWriter indexWriter = new IndexWriter(indexDir, iwc);
+
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    indexTwoDocs(indexWriter, config, false); // 1st segment, no content, with categories
+    indexTwoDocs(indexWriter, null, true);         // 2nd segment, with content, no categories
+    indexTwoDocs(indexWriter, config, true);  // 3rd segment ok
+    indexTwoDocs(indexWriter, null, false);        // 4th segment, no content, or categories
+    indexTwoDocs(indexWriter, null, true);         // 5th segment, with content, no categories
+    indexTwoDocs(indexWriter, config, true);  // 6th segment, with content, with categories
+    indexTwoDocs(indexWriter, null, true);         // 7th segment, with content, no categories
+    IOUtils.close(indexWriter, taxoWriter);
+
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher indexSearcher = newSearcher(indexReader);
+    
+    // search for "f:a", only segments 1 and 3 should match results
+    Query q = new TermQuery(new Term("f", "a"));
+    FacetsCollector sfc = new FacetsCollector();
+    indexSearcher.search(q, sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    FacetResult result = facets.getTopChildren(10, "A");
+    assertEquals("wrong number of children", 2, result.labelValues.length);
+    for (LabelAndValue labelValue : result.labelValues) {
+      assertEquals("wrong weight for child " + labelValue.label, 2, labelValue.value.intValue());
+    }
+
+    IOUtils.close(indexReader, taxoReader, indexDir, taxoDir);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts2.java b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts2.java
new file mode 100644
index 0000000..6f775fb
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetCounts2.java
@@ -0,0 +1,373 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+public class TestTaxonomyFacetCounts2 extends FacetTestCase {
+  
+  private static final Term A = new Term("f", "a");
+  private static final String CP_A = "A", CP_B = "B";
+  private static final String CP_C = "C", CP_D = "D"; // indexed w/ NO_PARENTS
+  private static final int NUM_CHILDREN_CP_A = 5, NUM_CHILDREN_CP_B = 3;
+  private static final int NUM_CHILDREN_CP_C = 5, NUM_CHILDREN_CP_D = 5;
+  private static final FacetField[] CATEGORIES_A, CATEGORIES_B;
+  private static final FacetField[] CATEGORIES_C, CATEGORIES_D;
+  static {
+    CATEGORIES_A = new FacetField[NUM_CHILDREN_CP_A];
+    for (int i = 0; i < NUM_CHILDREN_CP_A; i++) {
+      CATEGORIES_A[i] = new FacetField(CP_A, Integer.toString(i));
+    }
+    CATEGORIES_B = new FacetField[NUM_CHILDREN_CP_B];
+    for (int i = 0; i < NUM_CHILDREN_CP_B; i++) {
+      CATEGORIES_B[i] = new FacetField(CP_B, Integer.toString(i));
+    }
+    
+    // NO_PARENTS categories
+    CATEGORIES_C = new FacetField[NUM_CHILDREN_CP_C];
+    for (int i = 0; i < NUM_CHILDREN_CP_C; i++) {
+      CATEGORIES_C[i] = new FacetField(CP_C, Integer.toString(i));
+    }
+    
+    // Multi-level categories
+    CATEGORIES_D = new FacetField[NUM_CHILDREN_CP_D];
+    for (int i = 0; i < NUM_CHILDREN_CP_D; i++) {
+      String val = Integer.toString(i);
+      CATEGORIES_D[i] = new FacetField(CP_D, val, val + val); // e.g. D/1/11, D/2/22...
+    }
+  }
+  
+  private static Directory indexDir, taxoDir;
+  private static Map<String,Integer> allExpectedCounts, termExpectedCounts;
+
+  @AfterClass
+  public static void afterClassCountingFacetsAggregatorTest() throws Exception {
+    IOUtils.close(indexDir, taxoDir); 
+  }
+  
+  private static List<FacetField> randomCategories(Random random) {
+    // add random categories from the two dimensions, ensuring that the same
+    // category is not added twice.
+    int numFacetsA = random.nextInt(3) + 1; // 1-3
+    int numFacetsB = random.nextInt(2) + 1; // 1-2
+    ArrayList<FacetField> categories_a = new ArrayList<FacetField>();
+    categories_a.addAll(Arrays.asList(CATEGORIES_A));
+    ArrayList<FacetField> categories_b = new ArrayList<FacetField>();
+    categories_b.addAll(Arrays.asList(CATEGORIES_B));
+    Collections.shuffle(categories_a, random);
+    Collections.shuffle(categories_b, random);
+
+    ArrayList<FacetField> categories = new ArrayList<FacetField>();
+    categories.addAll(categories_a.subList(0, numFacetsA));
+    categories.addAll(categories_b.subList(0, numFacetsB));
+    
+    // add the NO_PARENT categories
+    categories.add(CATEGORIES_C[random().nextInt(NUM_CHILDREN_CP_C)]);
+    categories.add(CATEGORIES_D[random().nextInt(NUM_CHILDREN_CP_D)]);
+
+    return categories;
+  }
+
+  private static void addField(Document doc) {
+    doc.add(new StringField(A.field(), A.text(), Store.NO));
+  }
+
+  private static void addFacets(Document doc, FacetsConfig config, boolean updateTermExpectedCounts) 
+      throws IOException {
+    List<FacetField> docCategories = randomCategories(random());
+    for (FacetField ff : docCategories) {
+      doc.add(ff);
+      String cp = ff.dim + "/" + ff.path[0];
+      allExpectedCounts.put(cp, allExpectedCounts.get(cp) + 1);
+      if (updateTermExpectedCounts) {
+        termExpectedCounts.put(cp, termExpectedCounts.get(cp) + 1);
+      }
+    }
+    // add 1 to each NO_PARENTS dimension
+    allExpectedCounts.put(CP_B, allExpectedCounts.get(CP_B) + 1);
+    allExpectedCounts.put(CP_C, allExpectedCounts.get(CP_C) + 1);
+    allExpectedCounts.put(CP_D, allExpectedCounts.get(CP_D) + 1);
+    if (updateTermExpectedCounts) {
+      termExpectedCounts.put(CP_B, termExpectedCounts.get(CP_B) + 1);
+      termExpectedCounts.put(CP_C, termExpectedCounts.get(CP_C) + 1);
+      termExpectedCounts.put(CP_D, termExpectedCounts.get(CP_D) + 1);
+    }
+  }
+
+  private static FacetsConfig getConfig(TaxonomyWriter taxoWriter) {
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setMultiValued("A", true);
+    config.setMultiValued("B", true);
+    config.setRequireDimCount("B", true);
+    config.setHierarchical("D", true);
+    return config;
+  }
+
+  private static FacetsConfig getConfig() {
+    return getConfig(null);
+  }
+  
+  private static void indexDocsNoFacets(IndexWriter indexWriter) throws IOException {
+    int numDocs = atLeast(2);
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      addField(doc);
+      indexWriter.addDocument(doc);
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  private static void indexDocsWithFacetsNoTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
+                                                 Map<String,Integer> expectedCounts) throws IOException {
+    Random random = random();
+    int numDocs = atLeast(random, 2);
+    FacetsConfig config = getConfig(taxoWriter);
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      addFacets(doc, config, false);
+      indexWriter.addDocument(config.build(doc));
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  private static void indexDocsWithFacetsAndTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
+                                                  Map<String,Integer> expectedCounts) throws IOException {
+    Random random = random();
+    int numDocs = atLeast(random, 2);
+    FacetsConfig config = getConfig(taxoWriter);
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      addFacets(doc, config, true);
+      addField(doc);
+      indexWriter.addDocument(config.build(doc));
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  private static void indexDocsWithFacetsAndSomeTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
+                                                      Map<String,Integer> expectedCounts) throws IOException {
+    Random random = random();
+    int numDocs = atLeast(random, 2);
+    FacetsConfig config = getConfig(taxoWriter);
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      boolean hasContent = random.nextBoolean();
+      if (hasContent) {
+        addField(doc);
+      }
+      addFacets(doc, config, hasContent);
+      indexWriter.addDocument(config.build(doc));
+    }
+    indexWriter.commit(); // flush a segment
+  }
+  
+  // initialize expectedCounts w/ 0 for all categories
+  private static Map<String,Integer> newCounts() {
+    Map<String,Integer> counts = new HashMap<String,Integer>();
+    counts.put(CP_A, 0);
+    counts.put(CP_B, 0);
+    counts.put(CP_C, 0);
+    counts.put(CP_D, 0);
+    for (FacetField ff : CATEGORIES_A) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    for (FacetField ff : CATEGORIES_B) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    for (FacetField ff : CATEGORIES_C) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    for (FacetField ff : CATEGORIES_D) {
+      counts.put(ff.dim + "/" + ff.path[0], 0);
+    }
+    return counts;
+  }
+  
+  @BeforeClass
+  public static void beforeClassCountingFacetsAggregatorTest() throws Exception {
+    indexDir = newDirectory();
+    taxoDir = newDirectory();
+    
+    // create an index which has:
+    // 1. Segment with no categories, but matching results
+    // 2. Segment w/ categories, but no results
+    // 3. Segment w/ categories and results
+    // 4. Segment w/ categories, but only some results
+    
+    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges, so we can control the index segments
+    IndexWriter indexWriter = new IndexWriter(indexDir, conf);
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+
+    allExpectedCounts = newCounts();
+    termExpectedCounts = newCounts();
+    
+    // segment w/ no categories
+    indexDocsNoFacets(indexWriter);
+
+    // segment w/ categories, no content
+    indexDocsWithFacetsNoTerms(indexWriter, taxoWriter, allExpectedCounts);
+
+    // segment w/ categories and content
+    indexDocsWithFacetsAndTerms(indexWriter, taxoWriter, allExpectedCounts);
+    
+    // segment w/ categories and some content
+    indexDocsWithFacetsAndSomeTerms(indexWriter, taxoWriter, allExpectedCounts);
+    
+    IOUtils.close(indexWriter, taxoWriter);
+  }
+  
+  @Test
+  public void testDifferentNumResults() throws Exception {
+    // test the collector w/ FacetRequests and different numResults
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    TermQuery q = new TermQuery(A);
+    searcher.search(q, sfc);
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_A, CP_A);
+    assertEquals(-1, result.value.intValue());
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(termExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
+    }
+    result = facets.getTopChildren(NUM_CHILDREN_CP_B, CP_B);
+    assertEquals(termExpectedCounts.get(CP_B), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(termExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
+    }
+    
+    IOUtils.close(indexReader, taxoReader);
+  }
+  
+  @Test
+  public void testAllCounts() throws Exception {
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+    
+    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_A, CP_A);
+    assertEquals(-1, result.value.intValue());
+    int prevValue = Integer.MAX_VALUE;
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
+      assertTrue("wrong sort order of sub results: labelValue.value=" + labelValue.value + " prevValue=" + prevValue, labelValue.value.intValue() <= prevValue);
+      prevValue = labelValue.value.intValue();
+    }
+
+    result = facets.getTopChildren(NUM_CHILDREN_CP_B, CP_B);
+    assertEquals(allExpectedCounts.get(CP_B), result.value);
+    prevValue = Integer.MAX_VALUE;
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
+      assertTrue("wrong sort order of sub results: labelValue.value=" + labelValue.value + " prevValue=" + prevValue, labelValue.value.intValue() <= prevValue);
+      prevValue = labelValue.value.intValue();
+    }
+
+    IOUtils.close(indexReader, taxoReader);
+  }
+  
+  @Test
+  public void testBigNumResults() throws Exception {
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+
+    FacetResult result = facets.getTopChildren(Integer.MAX_VALUE, CP_A);
+    assertEquals(-1, result.value.intValue());
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
+    }
+    result = facets.getTopChildren(Integer.MAX_VALUE, CP_B);
+    assertEquals(allExpectedCounts.get(CP_B), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
+    }
+    
+    IOUtils.close(indexReader, taxoReader);
+  }
+  
+  @Test
+  public void testNoParents() throws Exception {
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector sfc = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), sfc);
+
+    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
+
+    FacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_C, CP_C);
+    assertEquals(allExpectedCounts.get(CP_C), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_C + "/" + labelValue.label), labelValue.value);
+    }
+    result = facets.getTopChildren(NUM_CHILDREN_CP_D, CP_D);
+    assertEquals(allExpectedCounts.get(CP_C), result.value);
+    for(LabelAndValue labelValue : result.labelValues) {
+      assertEquals(allExpectedCounts.get(CP_D + "/" + labelValue.label), labelValue.value);
+    }
+    
+    IOUtils.close(indexReader, taxoReader);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetSumValueSource.java b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetSumValueSource.java
new file mode 100644
index 0000000..cdbd2c4
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/TestTaxonomyFacetSumValueSource.java
@@ -0,0 +1,434 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.IntField;
+import org.apache.lucene.document.NumericDocValuesField;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.PrintTaxonomyStats;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.queries.function.FunctionQuery;
+import org.apache.lucene.queries.function.FunctionValues;
+import org.apache.lucene.queries.function.ValueSource;
+import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
+import org.apache.lucene.queries.function.valuesource.IntFieldSource;
+import org.apache.lucene.queries.function.valuesource.LongFieldSource;
+import org.apache.lucene.queries.function.valuesource.QueryValueSource;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MultiCollector;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.TopScoreDocCollector;
+import org.apache.lucene.search.similarities.DefaultSimilarity;
+import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
+import org.apache.lucene.search.similarities.Similarity;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+
+public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
+
+  public void testBasic() throws Exception {
+
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+
+    // Reused across documents, to add the necessary facet
+    // fields:
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("Author", "Bob"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 20, Field.Store.NO));
+    doc.add(new FacetField("Author", "Lisa"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 30, Field.Store.NO));
+    doc.add(new FacetField("Author", "Lisa"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 40, Field.Store.NO));
+    doc.add(new FacetField("Author", "Susan"));
+    writer.addDocument(config.build(doc));
+
+    doc = new Document();
+    doc.add(new IntField("num", 45, Field.Store.NO));
+    doc.add(new FacetField("Author", "Frank"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    // Aggregate the facet counts:
+    FacetsCollector c = new FacetsCollector();
+
+    // MatchAllDocsQuery is for "browsing" (counts facets
+    // for all non-deleted docs in the index); normally
+    // you'd use a "normal" query, and use MultiCollector to
+    // wrap collecting the "normal" hits and also facets:
+    searcher.search(new MatchAllDocsQuery(), c);
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
+
+    // Retrieve & verify results:
+    assertEquals("value=145.0 childCount=4\n  Lisa (50.0)\n  Frank (45.0)\n  Susan (40.0)\n  Bob (10.0)\n", facets.getTopChildren(10, "Author").toString());
+
+    taxoReader.close();
+    searcher.getIndexReader().close();
+    dir.close();
+    taxoDir.close();
+  }
+
+  // LUCENE-5333
+  public void testSparseFacets() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new IntField("num", 20, Field.Store.NO));
+    doc.add(new FacetField("a", "foo2"));
+    doc.add(new FacetField("b", "bar1"));
+    writer.addDocument(config.build(doc));
+
+    if (random().nextBoolean()) {
+      writer.commit();
+    }
+
+    doc = new Document();
+    doc.add(new IntField("num", 30, Field.Store.NO));
+    doc.add(new FacetField("a", "foo3"));
+    doc.add(new FacetField("b", "bar2"));
+    doc.add(new FacetField("c", "baz1"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+
+    assertEquals(3, results.size());
+    assertEquals("value=60.0 childCount=3\n  foo3 (30.0)\n  foo2 (20.0)\n  foo1 (10.0)\n", results.get(0).toString());
+    assertEquals("value=50.0 childCount=2\n  bar2 (30.0)\n  bar1 (20.0)\n", results.get(1).toString());
+    assertEquals("value=30.0 childCount=1\n  baz1 (30.0)\n", results.get(2).toString());
+
+    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testWrongIndexFieldName() throws Exception {
+
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setIndexFieldName("a", "$facets2");
+
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
+
+    Document doc = new Document();
+    doc.add(new IntField("num", 10, Field.Store.NO));
+    doc.add(new FacetField("a", "foo1"));
+    writer.addDocument(config.build(doc));
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    FacetsCollector c = new FacetsCollector();
+    searcher.search(new MatchAllDocsQuery(), c);    
+
+    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, config, c, new IntFieldSource("num"));
+
+    // Ask for top 10 labels for any dims that have counts:
+    List<FacetResult> results = facets.getAllDims(10);
+    assertTrue(results.isEmpty());
+
+    try {
+      facets.getSpecificValue("a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    try {
+      facets.getTopChildren(10, "a");
+      fail("should have hit exc");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+
+    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+
+  public void testSumScoreAggregator() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      if (random().nextBoolean()) { // don't match all documents
+        doc.add(new StringField("f", "v", Field.Store.NO));
+      }
+      doc.add(new FacetField("dim", "a"));
+      iw.addDocument(config.build(doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector fc = new FacetsCollector(true);
+    TopScoreDocCollector topDocs = TopScoreDocCollector.create(10, false);
+    ConstantScoreQuery csq = new ConstantScoreQuery(new MatchAllDocsQuery());
+    csq.setBoost(2.0f);
+    
+    newSearcher(r).search(csq, MultiCollector.wrap(fc, topDocs));
+
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, fc, new TaxonomyFacetSumValueSource.ScoreValueSource());
+    
+    TopDocs td = topDocs.topDocs();
+    int expected = (int) (td.getMaxScore() * td.totalHits);
+    assertEquals(expected, facets.getSpecificValue("dim", "a").intValue());
+    
+    IOUtils.close(iw, taxoWriter, taxoReader, taxoDir, r, indexDir);
+  }
+  
+  public void testNoScore() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    for (int i = 0; i < 4; i++) {
+      Document doc = new Document();
+      doc.add(new NumericDocValuesField("price", (i+1)));
+      doc.add(new FacetField("a", Integer.toString(i % 2)));
+      iw.addDocument(config.build(doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, new LongFieldSource("price"));
+    assertEquals("value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testWithScore() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    for (int i = 0; i < 4; i++) {
+      Document doc = new Document();
+      doc.add(new NumericDocValuesField("price", (i+1)));
+      doc.add(new FacetField("a", Integer.toString(i % 2)));
+      iw.addDocument(config.build(doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    ValueSource valueSource = new ValueSource() {
+      @Override
+      public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
+        final Scorer scorer = (Scorer) context.get("scorer");
+        assert scorer != null;
+        return new DoubleDocValues(this) {
+          @Override
+          public double doubleVal(int document) {
+            try {
+              return scorer.score();
+            } catch (IOException exception) {
+              throw new RuntimeException(exception);
+            }
+          }
+        };
+      }
+
+      @Override public boolean equals(Object o) { return o == this; }
+      @Override public int hashCode() { return System.identityHashCode(this); }
+      @Override public String description() { return "score()"; }
+    };
+    
+    FacetsCollector sfc = new FacetsCollector(true);
+    TopScoreDocCollector tsdc = TopScoreDocCollector.create(10, true);
+    // score documents by their 'price' field - makes asserting the correct counts for the categories easier
+    Query q = new FunctionQuery(new LongFieldSource("price"));
+    newSearcher(r).search(q, MultiCollector.wrap(tsdc, sfc));
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, valueSource);
+    
+    assertEquals("value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testRollupValues() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setHierarchical("a", true);
+    //config.setRequireDimCount("a", true);
+    
+    for (int i = 0; i < 4; i++) {
+      Document doc = new Document();
+      doc.add(new NumericDocValuesField("price", (i+1)));
+      doc.add(new FacetField("a", Integer.toString(i % 2), "1"));
+      iw.addDocument(config.build(doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+
+    ValueSource valueSource = new LongFieldSource("price");
+    FacetsCollector sfc = new FacetsCollector();
+    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
+    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, valueSource);
+    
+    assertEquals("value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
+    
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  public void testCountAndSumScore() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
+    FacetsConfig config = new FacetsConfig(taxoWriter);
+    config.setIndexFieldName("b", "$b");
+    
+    for(int i = atLeast(30); i > 0; --i) {
+      Document doc = new Document();
+      doc.add(new StringField("f", "v", Field.Store.NO));
+      doc.add(new FacetField("a", "1"));
+      doc.add(new FacetField("b", "1"));
+      iw.addDocument(config.build(doc));
+    }
+    
+    DirectoryReader r = DirectoryReader.open(iw, true);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    
+    FacetsCollector sfc = new FacetsCollector(true);
+    TopScoreDocCollector topDocs = TopScoreDocCollector.create(10, false);
+    newSearcher(r).search(new MatchAllDocsQuery(), MultiCollector.wrap(sfc, topDocs));
+    
+    Facets facets1 = getTaxonomyFacetCounts(taxoReader, config, sfc);
+    Facets facets2 = new TaxonomyFacetSumValueSource(new DocValuesOrdinalsReader("$b"), taxoReader, config, sfc, new TaxonomyFacetSumValueSource.ScoreValueSource());
+
+    assertEquals(r.maxDoc(), facets1.getTopChildren(10, "a").value.intValue());
+    double expected = topDocs.topDocs().getMaxScore() * r.numDocs();
+    assertEquals(r.maxDoc(), facets2.getTopChildren(10, "b").value.doubleValue(), 1E-10);
+    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
+  }
+
+  // nocommit in the sparse case test that we are really
+  // sorting by the correct dim count
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/AssertingSubDocsAtOnceCollector.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/AssertingSubDocsAtOnceCollector.java
deleted file mode 100644
index 89f0ab0..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/AssertingSubDocsAtOnceCollector.java
+++ /dev/null
@@ -1,67 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.Scorer.ChildScorer;
-import org.apache.lucene.search.Scorer;
-
-/** Verifies in collect() that all child subScorers are on
- *  the collected doc. */
-class AssertingSubDocsAtOnceCollector extends Collector {
-
-  // TODO: allow wrapping another Collector
-
-  List<Scorer> allScorers;
-
-  @Override
-  public void setScorer(Scorer s) {
-    // Gathers all scorers, including s and "under":
-    allScorers = new ArrayList<Scorer>();
-    allScorers.add(s);
-    int upto = 0;
-    while(upto < allScorers.size()) {
-      s = allScorers.get(upto++);
-      for (ChildScorer sub : s.getChildren()) {
-        allScorers.add(sub.child);
-      }
-    }
-  }
-
-  @Override
-  public void collect(int docID) {
-    for(Scorer s : allScorers) {
-      if (docID != s.docID()) {
-        throw new IllegalStateException("subScorer=" + s + " has docID=" + s.docID() + " != collected docID=" + docID);
-      }
-    }
-  }
-
-  @Override
-  public void setNextReader(AtomicReaderContext context) {
-  }
-
-  @Override
-  public boolean acceptsDocsOutOfOrder() {
-    return false;
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestCachedOrdinalsReader.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestCachedOrdinalsReader.java
deleted file mode 100644
index 4dce38b..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestCachedOrdinalsReader.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-public class TestCachedOrdinalsReader extends FacetTestCase {
-
-  @Test
-  public void testWithThreads() throws Exception {
-    // LUCENE-5303: OrdinalsCache used the ThreadLocal BinaryDV instead of reader.getCoreCacheKey().
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    IndexWriter writer = new IndexWriter(indexDir, conf);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    
-    Document doc = new Document();
-    doc.add(new FacetField("A", "1"));
-    writer.addDocument(config.build(doc));
-    doc = new Document();
-    doc.add(new FacetField("A", "2"));
-    writer.addDocument(config.build(doc));
-    
-    final DirectoryReader reader = DirectoryReader.open(writer, true);
-    final CachedOrdinalsReader ordsReader = new CachedOrdinalsReader(new DocValuesOrdinalsReader(FacetsConfig.DEFAULT_INDEX_FIELD_NAME));
-    Thread[] threads = new Thread[3];
-    for (int i = 0; i < threads.length; i++) {
-      threads[i] = new Thread("CachedOrdsThread-" + i) {
-        @Override
-        public void run() {
-          for (AtomicReaderContext context : reader.leaves()) {
-            try {
-              ordsReader.getReader(context);
-            } catch (IOException e) {
-              throw new RuntimeException(e);
-            }
-          }
-        }
-      };
-    }
-
-    long ramBytesUsed = 0;
-    for (Thread t : threads) {
-      t.start();
-      t.join();
-      if (ramBytesUsed == 0) {
-        ramBytesUsed = ordsReader.ramBytesUsed();
-      } else {
-        assertEquals(ramBytesUsed, ordsReader.ramBytesUsed());
-      }
-    }
-    
-    IOUtils.close(writer, taxoWriter, reader, indexDir, taxoDir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestDrillDownQuery.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestDrillDownQuery.java
deleted file mode 100644
index febf1f2..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestDrillDownQuery.java
+++ /dev/null
@@ -1,252 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.QueryUtils;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestDrillDownQuery extends FacetTestCase {
-  
-  private static IndexReader reader;
-  private static DirectoryTaxonomyReader taxo;
-  private static Directory dir;
-  private static Directory taxoDir;
-  private static FacetsConfig config;
-
-  @AfterClass
-  public static void afterClassDrillDownQueryTest() throws Exception {
-    IOUtils.close(reader, taxo, dir, taxoDir);
-    reader = null;
-    taxo = null;
-    dir = null;
-    taxoDir = null;
-    config = null;
-  }
-
-  @BeforeClass
-  public static void beforeClassDrillDownQueryTest() throws Exception {
-    dir = newDirectory();
-    Random r = random();
-    RandomIndexWriter writer = new RandomIndexWriter(r, dir, 
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(r, MockTokenizer.KEYWORD, false)));
-    
-    taxoDir = newDirectory();
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    config = new FacetsConfig(taxoWriter);
-
-    // Randomize the per-dim config:
-    config.setHierarchical("a", random().nextBoolean());
-    config.setMultiValued("a", random().nextBoolean());
-    if (random().nextBoolean()) {
-      config.setIndexFieldName("a", "$a");
-    }
-    config.setRequireDimCount("a", true);
-
-    config.setHierarchical("b", random().nextBoolean());
-    config.setMultiValued("b", random().nextBoolean());
-    if (random().nextBoolean()) {
-      config.setIndexFieldName("b", "$b");
-    }
-    config.setRequireDimCount("b", true);
-
-    for (int i = 0; i < 100; i++) {
-      Document doc = new Document();
-      if (i % 2 == 0) { // 50
-        doc.add(new TextField("content", "foo", Field.Store.NO));
-      }
-      if (i % 3 == 0) { // 33
-        doc.add(new TextField("content", "bar", Field.Store.NO));
-      }
-      if (i % 4 == 0) { // 25
-        if (r.nextBoolean()) {
-          doc.add(new FacetField("a", "1"));
-        } else {
-          doc.add(new FacetField("a", "2"));
-        }
-      }
-      if (i % 5 == 0) { // 20
-        doc.add(new FacetField("b", "1"));
-      }
-      writer.addDocument(config.build(doc));
-    }
-    
-    taxoWriter.close();
-    reader = writer.getReader();
-    writer.close();
-    
-    taxo = new DirectoryTaxonomyReader(taxoDir);
-  }
-  
-  public void testAndOrs() throws Exception {
-    IndexSearcher searcher = newSearcher(reader);
-
-    // test (a/1 OR a/2) AND b/1
-    SimpleDrillDownQuery q = new SimpleDrillDownQuery(config);
-    q.add("a", "1");
-    q.add("a", "2");
-    q.add("b", "1");
-    TopDocs docs = searcher.search(q, 100);
-    assertEquals(5, docs.totalHits);
-  }
-  
-  public void testQuery() throws IOException {
-    IndexSearcher searcher = newSearcher(reader);
-
-    // Making sure the query yields 25 documents with the facet "a"
-    SimpleDrillDownQuery q = new SimpleDrillDownQuery(config);
-    q.add("a");
-    System.out.println("q=" + q);
-    QueryUtils.check(q);
-    TopDocs docs = searcher.search(q, 100);
-    assertEquals(25, docs.totalHits);
-    
-    // Making sure the query yields 5 documents with the facet "b" and the
-    // previous (facet "a") query as a base query
-    SimpleDrillDownQuery q2 = new SimpleDrillDownQuery(config, q);
-    q2.add("b");
-    docs = searcher.search(q2, 100);
-    assertEquals(5, docs.totalHits);
-
-    // Making sure that a query of both facet "a" and facet "b" yields 5 results
-    SimpleDrillDownQuery q3 = new SimpleDrillDownQuery(config);
-    q3.add("a");
-    q3.add("b");
-    docs = searcher.search(q3, 100);
-    
-    assertEquals(5, docs.totalHits);
-    // Check that content:foo (which yields 50% results) and facet/b (which yields 20%)
-    // would gather together 10 results (10%..) 
-    Query fooQuery = new TermQuery(new Term("content", "foo"));
-    SimpleDrillDownQuery q4 = new SimpleDrillDownQuery(config, fooQuery);
-    q4.add("b");
-    docs = searcher.search(q4, 100);
-    assertEquals(10, docs.totalHits);
-  }
-  
-  public void testQueryImplicitDefaultParams() throws IOException {
-    IndexSearcher searcher = newSearcher(reader);
-
-    // Create the base query to start with
-    SimpleDrillDownQuery q = new SimpleDrillDownQuery(config);
-    q.add("a");
-    
-    // Making sure the query yields 5 documents with the facet "b" and the
-    // previous (facet "a") query as a base query
-    SimpleDrillDownQuery q2 = new SimpleDrillDownQuery(config, q);
-    q2.add("b");
-    TopDocs docs = searcher.search(q2, 100);
-    assertEquals(5, docs.totalHits);
-
-    // Check that content:foo (which yields 50% results) and facet/b (which yields 20%)
-    // would gather together 10 results (10%..) 
-    Query fooQuery = new TermQuery(new Term("content", "foo"));
-    SimpleDrillDownQuery q4 = new SimpleDrillDownQuery(config, fooQuery);
-    q4.add("b");
-    docs = searcher.search(q4, 100);
-    assertEquals(10, docs.totalHits);
-  }
-  
-  public void testScoring() throws IOException {
-    // verify that drill-down queries do not modify scores
-    IndexSearcher searcher = newSearcher(reader);
-
-    float[] scores = new float[reader.maxDoc()];
-    
-    Query q = new TermQuery(new Term("content", "foo"));
-    TopDocs docs = searcher.search(q, reader.maxDoc()); // fetch all available docs to this query
-    for (ScoreDoc sd : docs.scoreDocs) {
-      scores[sd.doc] = sd.score;
-    }
-    
-    // create a drill-down query with category "a", scores should not change
-    SimpleDrillDownQuery q2 = new SimpleDrillDownQuery(config, q);
-    q2.add("a");
-    docs = searcher.search(q2, reader.maxDoc()); // fetch all available docs to this query
-    for (ScoreDoc sd : docs.scoreDocs) {
-      assertEquals("score of doc=" + sd.doc + " modified", scores[sd.doc], sd.score, 0f);
-    }
-  }
-  
-  public void testScoringNoBaseQuery() throws IOException {
-    // verify that drill-down queries (with no base query) returns 0.0 score
-    IndexSearcher searcher = newSearcher(reader);
-    
-    SimpleDrillDownQuery q = new SimpleDrillDownQuery(config);
-    q.add("a");
-    TopDocs docs = searcher.search(q, reader.maxDoc()); // fetch all available docs to this query
-    for (ScoreDoc sd : docs.scoreDocs) {
-      assertEquals(0f, sd.score, 0f);
-    }
-  }
-  
-  public void testTermNonDefault() {
-    String aField = config.getDimConfig("a").indexFieldName;
-    Term termA = SimpleDrillDownQuery.term(aField, "a");
-    assertEquals(new Term(aField, "a"), termA);
-    
-    String bField = config.getDimConfig("b").indexFieldName;
-    Term termB = SimpleDrillDownQuery.term(bField, "b");
-    assertEquals(new Term(bField, "b"), termB);
-  }
-
-  public void testClone() throws Exception {
-    SimpleDrillDownQuery q = new SimpleDrillDownQuery(config, new MatchAllDocsQuery());
-    q.add("a");
-    
-    SimpleDrillDownQuery clone = q.clone();
-    clone.add("b");
-    
-    assertFalse("query wasn't cloned: source=" + q + " clone=" + clone, q.toString().equals(clone.toString()));
-  }
-  
-  public void testNoDrillDown() throws Exception {
-    Query base = new MatchAllDocsQuery();
-    SimpleDrillDownQuery q = new SimpleDrillDownQuery(config, base);
-    Query rewrite = q.rewrite(reader).rewrite(reader);
-    assertSame(base, rewrite);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestFacetsConfig.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestFacetsConfig.java
deleted file mode 100644
index d3f17df..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestFacetsConfig.java
+++ /dev/null
@@ -1,40 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Arrays;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.util._TestUtil;
-
-public class TestFacetsConfig extends FacetTestCase {
-  public void testPathToStringAndBack() throws Exception {
-    int iters = atLeast(1000);
-    for(int i=0;i<iters;i++) {
-      int numParts = _TestUtil.nextInt(random(), 1, 6);
-      String[] parts = new String[numParts];
-      for(int j=0;j<numParts;j++) {
-        parts[j] = _TestUtil.randomUnicodeString(random());
-      }
-
-      String s = FacetsConfig.pathToString(parts);
-      String[] parts2 = FacetsConfig.stringToPath(s);
-      assertTrue(Arrays.equals(parts, parts2));
-    }
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestMultipleIndexFields.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestMultipleIndexFields.java
deleted file mode 100644
index 3309657..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestMultipleIndexFields.java
+++ /dev/null
@@ -1,307 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig.OpenMode;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.MultiCollector;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TopScoreDocCollector;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-public class TestMultipleIndexFields extends FacetTestCase {
-
-  private static final FacetField[] CATEGORIES = new FacetField[] {
-    new FacetField("Author", "Mark Twain"),
-    new FacetField("Author", "Stephen King"),
-    new FacetField("Author", "Kurt Vonnegut"),
-    new FacetField("Band", "Rock & Pop", "The Beatles"),
-    new FacetField("Band", "Punk", "The Ramones"),
-    new FacetField("Band", "Rock & Pop", "U2"),
-    new FacetField("Band", "Rock & Pop", "REM"),
-    new FacetField("Band", "Rock & Pop", "Dave Matthews Band"),
-    new FacetField("Composer", "Bach"),
-  };
-
-  private FacetsConfig getConfig(TaxonomyWriter taxoWriter) {
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    //config.setMultiValued("Author", true);
-    //config.setMultiValued("Band", true);
-    config.setHierarchical("Band", true);
-    return config;
-  }
-  
-  @Test
-  public void testDefault() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // create and open an index writer
-    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    // create and open a taxonomy writer
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
-    FacetsConfig config = getConfig(tw);
-
-    seedIndex(iw, config);
-
-    IndexReader ir = iw.getReader();
-    tw.commit();
-
-    // prepare index reader and taxonomy.
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // prepare searcher to search against
-    IndexSearcher searcher = newSearcher(ir);
-
-    SimpleFacetsCollector sfc = performSearch(tr, ir, searcher);
-
-    // Obtain facets results and hand-test them
-    assertCorrectResults(getTaxonomyFacetCounts(tr, config, sfc));
-
-    assertOrdinalsExist("$facets", ir);
-
-    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
-  }
-
-  @Test
-  public void testCustom() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // create and open an index writer
-    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    // create and open a taxonomy writer
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
-
-    FacetsConfig config = getConfig(tw);
-    config.setIndexFieldName("Author", "$author");
-    seedIndex(iw, config);
-
-    IndexReader ir = iw.getReader();
-    tw.commit();
-
-    // prepare index reader and taxonomy.
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // prepare searcher to search against
-    IndexSearcher searcher = newSearcher(ir);
-
-    SimpleFacetsCollector sfc = performSearch(tr, ir, searcher);
-
-    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
-    facetsMap.put("Author", getTaxonomyFacetCounts(tr, config, sfc, "$author"));
-    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
-
-    // Obtain facets results and hand-test them
-    assertCorrectResults(facets);
-
-    assertOrdinalsExist("$facets", ir);
-    assertOrdinalsExist("$author", ir);
-
-    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
-  }
-
-  @Test
-  public void testTwoCustomsSameField() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // create and open an index writer
-    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    // create and open a taxonomy writer
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
-
-    FacetsConfig config = getConfig(tw);
-    config.setIndexFieldName("Band", "$music");
-    config.setIndexFieldName("Composer", "$music");
-    seedIndex(iw, config);
-
-    IndexReader ir = iw.getReader();
-    tw.commit();
-
-    // prepare index reader and taxonomy.
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // prepare searcher to search against
-    IndexSearcher searcher = newSearcher(ir);
-
-    SimpleFacetsCollector sfc = performSearch(tr, ir, searcher);
-
-    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
-    Facets facets2 = getTaxonomyFacetCounts(tr, config, sfc, "$music");
-    facetsMap.put("Band", facets2);
-    facetsMap.put("Composer", facets2);
-    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
-
-    // Obtain facets results and hand-test them
-    assertCorrectResults(facets);
-
-    assertOrdinalsExist("$facets", ir);
-    assertOrdinalsExist("$music", ir);
-    assertOrdinalsExist("$music", ir);
-
-    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
-  }
-
-  private void assertOrdinalsExist(String field, IndexReader ir) throws IOException {
-    for (AtomicReaderContext context : ir.leaves()) {
-      AtomicReader r = context.reader();
-      if (r.getBinaryDocValues(field) != null) {
-        return; // not all segments must have this DocValues
-      }
-    }
-    fail("no ordinals found for " + field);
-  }
-
-  @Test
-  public void testDifferentFieldsAndText() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // create and open an index writer
-    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    // create and open a taxonomy writer
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
-
-    FacetsConfig config = getConfig(tw);
-    config.setIndexFieldName("Band", "$bands");
-    config.setIndexFieldName("Composer", "$composers");
-    seedIndex(iw, config);
-
-    IndexReader ir = iw.getReader();
-    tw.commit();
-
-    // prepare index reader and taxonomy.
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // prepare searcher to search against
-    IndexSearcher searcher = newSearcher(ir);
-
-    SimpleFacetsCollector sfc = performSearch(tr, ir, searcher);
-
-    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
-    facetsMap.put("Band", getTaxonomyFacetCounts(tr, config, sfc, "$bands"));
-    facetsMap.put("Composer", getTaxonomyFacetCounts(tr, config, sfc, "$composers"));
-    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
-
-    // Obtain facets results and hand-test them
-    assertCorrectResults(facets);
-    assertOrdinalsExist("$facets", ir);
-    assertOrdinalsExist("$bands", ir);
-    assertOrdinalsExist("$composers", ir);
-
-    IOUtils.close(tr, ir, iw, tw, indexDir, taxoDir);
-  }
-
-  @Test
-  public void testSomeSameSomeDifferent() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // create and open an index writer
-    RandomIndexWriter iw = new RandomIndexWriter(random(), indexDir, newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    // create and open a taxonomy writer
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir, OpenMode.CREATE);
-
-    FacetsConfig config = getConfig(tw);
-    config.setIndexFieldName("Band", "$music");
-    config.setIndexFieldName("Composer", "$music");
-    config.setIndexFieldName("Author", "$literature");
-    seedIndex(iw, config);
-
-    IndexReader ir = iw.getReader();
-    tw.commit();
-
-    // prepare index reader and taxonomy.
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // prepare searcher to search against
-    IndexSearcher searcher = newSearcher(ir);
-
-    SimpleFacetsCollector sfc = performSearch(tr, ir, searcher);
-
-    Map<String,Facets> facetsMap = new HashMap<String,Facets>();
-    Facets facets2 = getTaxonomyFacetCounts(tr, config, sfc, "$music");
-    facetsMap.put("Band", facets2);
-    facetsMap.put("Composer", facets2);
-    facetsMap.put("Author", getTaxonomyFacetCounts(tr, config, sfc, "$literature"));
-    Facets facets = new MultiFacets(facetsMap, getTaxonomyFacetCounts(tr, config, sfc));
-
-    // Obtain facets results and hand-test them
-    assertCorrectResults(facets);
-    assertOrdinalsExist("$music", ir);
-    assertOrdinalsExist("$literature", ir);
-
-    IOUtils.close(tr, ir, iw, tw);
-    IOUtils.close(indexDir, taxoDir);
-  }
-
-  private void assertCorrectResults(Facets facets) throws IOException {
-    assertEquals(5, facets.getSpecificValue("Band"));
-    assertEquals("value=5 childCount=2\n  Rock & Pop (4)\n  Punk (1)\n", facets.getTopChildren(10, "Band").toString());
-    assertEquals("value=4 childCount=4\n  The Beatles (1)\n  U2 (1)\n  REM (1)\n  Dave Matthews Band (1)\n", facets.getTopChildren(10, "Band", "Rock & Pop").toString());
-    assertEquals("value=3 childCount=3\n  Mark Twain (1)\n  Stephen King (1)\n  Kurt Vonnegut (1)\n", facets.getTopChildren(10, "Author").toString());
-  }
-
-  private SimpleFacetsCollector performSearch(TaxonomyReader tr, IndexReader ir, 
-      IndexSearcher searcher) throws IOException {
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    Facets.search(searcher, new MatchAllDocsQuery(), 10, sfc);
-    return sfc;
-  }
-
-  private void seedIndex(RandomIndexWriter iw, FacetsConfig config) throws IOException {
-    for (FacetField ff : CATEGORIES) {
-      Document doc = new Document();
-      doc.add(ff);
-      doc.add(new TextField("content", "alpha", Field.Store.YES));
-      iw.addDocument(config.build(doc));
-    }
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestRangeFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestRangeFacets.java
deleted file mode 100644
index 96ab014..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestRangeFacets.java
+++ /dev/null
@@ -1,540 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.DoubleDocValuesField;
-import org.apache.lucene.document.DoubleField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FloatDocValuesField;
-import org.apache.lucene.document.FloatField;
-import org.apache.lucene.document.LongField;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.simple.SimpleDrillSideways.SimpleDrillSidewaysResult;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.NumericRangeQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-
-// nocommit rename to TestRangeFacetCounts
-public class TestRangeFacets extends FacetTestCase {
-
-  public void testBasicLong() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      field.setLongValue(l);
-      w.addDocument(doc);
-    }
-    field.setLongValue(Long.MAX_VALUE);
-    w.addDocument(doc);
-
-    IndexReader r = w.getReader();
-    w.close();
-
-    SimpleFacetsCollector fc = new SimpleFacetsCollector();
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-
-    RangeFacetCounts facets = new RangeFacetCounts("field", fc,
-        new LongRange("less than 10", 0L, true, 10L, false),
-        new LongRange("less than or equal to 10", 0L, true, 10L, true),
-        new LongRange("over 90", 90L, false, 100L, false),
-        new LongRange("90 or above", 90L, true, 100L, false),
-        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, true));
-    
-    SimpleFacetResult result = facets.getTopChildren(10, "field");
-    assertEquals("value=101 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (1)\n",
-                 result.toString());
-    
-    r.close();
-    d.close();
-  }
-
-  /** Tests single request that mixes Range and non-Range
-   *  faceting, with DrillSideways and taxonomy. */
-  public void testMixedRangeAndNonRangeTaxonomy() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Directory td = newDirectory();
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig(tw);
-
-    for (long l = 0; l < 100; l++) {
-      Document doc = new Document();
-      // For computing range facet counts:
-      doc.add(new NumericDocValuesField("field", l));
-      // For drill down by numeric range:
-      doc.add(new LongField("field", l, Field.Store.NO));
-
-      if ((l&3) == 0) {
-        doc.add(new FacetField("dim", "a"));
-      } else {
-        doc.add(new FacetField("dim", "b"));
-      }
-      w.addDocument(config.build(doc));
-    }
-
-    final IndexReader r = w.getReader();
-
-    final TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
-
-    IndexSearcher s = newSearcher(r);
-
-    SimpleDrillSideways ds = new SimpleDrillSideways(s, config, tr) {
-
-        @Override
-        protected Facets buildFacetsResult(SimpleFacetsCollector drillDowns, SimpleFacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {        
-          // nocommit this is awkward... can we improve?
-          // nocommit is drillDowns allowed to be null?
-          // should it?
-          SimpleFacetsCollector dimFC = drillDowns;
-          SimpleFacetsCollector fieldFC = drillDowns;
-          if (drillSideways != null) {
-            for(int i=0;i<drillSideways.length;i++) {
-              String dim = drillSidewaysDims[i];
-              if (dim.equals("field")) {
-                fieldFC = drillSideways[i];
-              } else {
-                dimFC = drillSideways[i];
-              }
-            }
-          }
-
-          Map<String,Facets> byDim = new HashMap<String,Facets>();
-          byDim.put("field",
-                    new RangeFacetCounts("field", fieldFC,
-                          new LongRange("less than 10", 0L, true, 10L, false),
-                          new LongRange("less than or equal to 10", 0L, true, 10L, true),
-                          new LongRange("over 90", 90L, false, 100L, false),
-                          new LongRange("90 or above", 90L, true, 100L, false),
-                          new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false)));
-          byDim.put("dim", getTaxonomyFacetCounts(taxoReader, config, dimFC));
-          return new MultiFacets(byDim, null);
-        }
-
-        @Override
-        protected boolean scoreSubDocsAtOnce() {
-          return random().nextBoolean();
-        }
-      };
-
-    // First search, no drill downs:
-    SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config);
-    SimpleDrillSidewaysResult dsr = ds.search(null, ddq, 10);
-
-    assertEquals(100, dsr.hits.totalHits);
-    assertEquals("value=100 childCount=2\n  b (75)\n  a (25)\n", dsr.facets.getTopChildren(10, "dim").toString());
-    assertEquals("value=100 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
-                 dsr.facets.getTopChildren(10, "field").toString());
-
-    // Second search, drill down on dim=b:
-    ddq = new SimpleDrillDownQuery(config);
-    ddq.add("dim", "b");
-    dsr = ds.search(null, ddq, 10);
-
-    assertEquals(75, dsr.hits.totalHits);
-    assertEquals("value=100 childCount=2\n  b (75)\n  a (25)\n", dsr.facets.getTopChildren(10, "dim").toString());
-    assertEquals("value=75 childCount=5\n  less than 10 (7)\n  less than or equal to 10 (8)\n  over 90 (7)\n  90 or above (8)\n  over 1000 (0)\n",
-                 dsr.facets.getTopChildren(10, "field").toString());
-
-    // Third search, drill down on "less than or equal to 10":
-    ddq = new SimpleDrillDownQuery(config);
-    ddq.add("field", NumericRangeQuery.newLongRange("field", 0L, 10L, true, true));
-    dsr = ds.search(null, ddq, 10);
-
-    assertEquals(11, dsr.hits.totalHits);
-    assertEquals("value=11 childCount=2\n  b (8)\n  a (3)\n", dsr.facets.getTopChildren(10, "dim").toString());
-    assertEquals("value=100 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
-                 dsr.facets.getTopChildren(10, "field").toString());
-    IOUtils.close(tw, tr, td, w, r, d);
-  }
-
-  public void testBasicDouble() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    DoubleDocValuesField field = new DoubleDocValuesField("field", 0.0);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      field.setDoubleValue(l);
-      w.addDocument(doc);
-    }
-
-    IndexReader r = w.getReader();
-
-    SimpleFacetsCollector fc = new SimpleFacetsCollector();
-
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-    Facets facets = new RangeFacetCounts("field", fc,
-        new DoubleRange("less than 10", 0.0, true, 10.0, false),
-        new DoubleRange("less than or equal to 10", 0.0, true, 10.0, true),
-        new DoubleRange("over 90", 90.0, false, 100.0, false),
-        new DoubleRange("90 or above", 90.0, true, 100.0, false),
-        new DoubleRange("over 1000", 1000.0, false, Double.POSITIVE_INFINITY, false));
-                                         
-    assertEquals("value=100 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
-                 facets.getTopChildren(10, "field").toString());
-
-    IOUtils.close(w, r, d);
-  }
-
-  public void testBasicFloat() throws Exception {
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    FloatDocValuesField field = new FloatDocValuesField("field", 0.0f);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      field.setFloatValue(l);
-      w.addDocument(doc);
-    }
-
-    IndexReader r = w.getReader();
-
-    SimpleFacetsCollector fc = new SimpleFacetsCollector();
-
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), fc);
-
-    Facets facets = new RangeFacetCounts("field", fc,
-        new FloatRange("less than 10", 0.0f, true, 10.0f, false),
-        new FloatRange("less than or equal to 10", 0.0f, true, 10.0f, true),
-        new FloatRange("over 90", 90.0f, false, 100.0f, false),
-        new FloatRange("90 or above", 90.0f, true, 100.0f, false),
-        new FloatRange("over 1000", 1000.0f, false, Float.POSITIVE_INFINITY, false));
-    
-    assertEquals("value=100 childCount=5\n  less than 10 (10)\n  less than or equal to 10 (11)\n  over 90 (9)\n  90 or above (10)\n  over 1000 (0)\n",
-                 facets.getTopChildren(10, "field").toString());
-    
-    IOUtils.close(w, r, d);
-  }
-
-  public void testRandomLongs() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
-
-    int numDocs = atLeast(1000);
-    long[] values = new long[numDocs];
-    for(int i=0;i<numDocs;i++) {
-      Document doc = new Document();
-      long v = random().nextLong();
-      values[i] = v;
-      doc.add(new NumericDocValuesField("field", v));
-      doc.add(new LongField("field", v, Field.Store.NO));
-      w.addDocument(doc);
-    }
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = newSearcher(r);
-    FacetsConfig config = new FacetsConfig();
-    
-    int numIters = atLeast(10);
-    for(int iter=0;iter<numIters;iter++) {
-      if (VERBOSE) {
-        System.out.println("TEST: iter=" + iter);
-      }
-      int numRange = _TestUtil.nextInt(random(), 1, 5);
-      LongRange[] ranges = new LongRange[numRange];
-      int[] expectedCounts = new int[numRange];
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        long min = random().nextLong();
-        long max = random().nextLong();
-        if (min > max) {
-          long x = min;
-          min = max;
-          max = x;
-        }
-        boolean minIncl = random().nextBoolean();
-        boolean maxIncl = random().nextBoolean();
-        ranges[rangeID] = new LongRange("r" + rangeID, min, minIncl, max, maxIncl);
-
-        // Do "slow but hopefully correct" computation of
-        // expected count:
-        for(int i=0;i<numDocs;i++) {
-          boolean accept = true;
-          if (minIncl) {
-            accept &= values[i] >= min;
-          } else {
-            accept &= values[i] > min;
-          }
-          if (maxIncl) {
-            accept &= values[i] <= max;
-          } else {
-            accept &= values[i] < max;
-          }
-          if (accept) {
-            expectedCounts[rangeID]++;
-          }
-        }
-      }
-
-      SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-      s.search(new MatchAllDocsQuery(), sfc);
-      Facets facets = new RangeFacetCounts("field", sfc, ranges);
-      SimpleFacetResult result = facets.getTopChildren(10, "field");
-      assertEquals(numRange, result.labelValues.length);
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        if (VERBOSE) {
-          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
-        }
-        LabelAndValue subNode = result.labelValues[rangeID];
-        assertEquals("r" + rangeID, subNode.label);
-        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
-
-        LongRange range = ranges[rangeID];
-
-        // Test drill-down:
-        SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config);
-        ddq.add("field", NumericRangeQuery.newLongRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
-        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
-      }
-    }
-
-    IOUtils.close(w, r, dir);
-  }
-
-  public void testRandomFloats() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
-
-    int numDocs = atLeast(1000);
-    float[] values = new float[numDocs];
-    for(int i=0;i<numDocs;i++) {
-      Document doc = new Document();
-      float v = random().nextFloat();
-      values[i] = v;
-      doc.add(new FloatDocValuesField("field", v));
-      doc.add(new FloatField("field", v, Field.Store.NO));
-      w.addDocument(doc);
-    }
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = newSearcher(r);
-    FacetsConfig config = new FacetsConfig();
-    
-    int numIters = atLeast(10);
-    for(int iter=0;iter<numIters;iter++) {
-      if (VERBOSE) {
-        System.out.println("TEST: iter=" + iter);
-      }
-      int numRange = _TestUtil.nextInt(random(), 1, 5);
-      FloatRange[] ranges = new FloatRange[numRange];
-      int[] expectedCounts = new int[numRange];
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        float min = random().nextFloat();
-        float max = random().nextFloat();
-        if (min > max) {
-          float x = min;
-          min = max;
-          max = x;
-        }
-        boolean minIncl = random().nextBoolean();
-        boolean maxIncl = random().nextBoolean();
-        ranges[rangeID] = new FloatRange("r" + rangeID, min, minIncl, max, maxIncl);
-
-        // Do "slow but hopefully correct" computation of
-        // expected count:
-        for(int i=0;i<numDocs;i++) {
-          boolean accept = true;
-          if (minIncl) {
-            accept &= values[i] >= min;
-          } else {
-            accept &= values[i] > min;
-          }
-          if (maxIncl) {
-            accept &= values[i] <= max;
-          } else {
-            accept &= values[i] < max;
-          }
-          if (accept) {
-            expectedCounts[rangeID]++;
-          }
-        }
-      }
-
-      SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-      s.search(new MatchAllDocsQuery(), sfc);
-      Facets facets = new RangeFacetCounts("field", sfc, ranges);
-      SimpleFacetResult result = facets.getTopChildren(10, "field");
-      assertEquals(numRange, result.labelValues.length);
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        if (VERBOSE) {
-          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
-        }
-        LabelAndValue subNode = result.labelValues[rangeID];
-        assertEquals("r" + rangeID, subNode.label);
-        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
-
-        FloatRange range = ranges[rangeID];
-
-        // Test drill-down:
-        SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config);
-        ddq.add("field", NumericRangeQuery.newFloatRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
-        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
-      }
-    }
-
-    IOUtils.close(w, r, dir);
-  }
-
-  public void testRandomDoubles() throws Exception {
-    Directory dir = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir);
-
-    int numDocs = atLeast(1000);
-    double[] values = new double[numDocs];
-    for(int i=0;i<numDocs;i++) {
-      Document doc = new Document();
-      double v = random().nextDouble();
-      values[i] = v;
-      doc.add(new DoubleDocValuesField("field", v));
-      doc.add(new DoubleField("field", v, Field.Store.NO));
-      w.addDocument(doc);
-    }
-    IndexReader r = w.getReader();
-
-    IndexSearcher s = newSearcher(r);
-    FacetsConfig config = new FacetsConfig();
-    
-    int numIters = atLeast(10);
-    for(int iter=0;iter<numIters;iter++) {
-      if (VERBOSE) {
-        System.out.println("TEST: iter=" + iter);
-      }
-      int numRange = _TestUtil.nextInt(random(), 1, 5);
-      DoubleRange[] ranges = new DoubleRange[numRange];
-      int[] expectedCounts = new int[numRange];
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        double min = random().nextDouble();
-        double max = random().nextDouble();
-        if (min > max) {
-          double x = min;
-          min = max;
-          max = x;
-        }
-        boolean minIncl = random().nextBoolean();
-        boolean maxIncl = random().nextBoolean();
-        ranges[rangeID] = new DoubleRange("r" + rangeID, min, minIncl, max, maxIncl);
-
-        // Do "slow but hopefully correct" computation of
-        // expected count:
-        for(int i=0;i<numDocs;i++) {
-          boolean accept = true;
-          if (minIncl) {
-            accept &= values[i] >= min;
-          } else {
-            accept &= values[i] > min;
-          }
-          if (maxIncl) {
-            accept &= values[i] <= max;
-          } else {
-            accept &= values[i] < max;
-          }
-          if (accept) {
-            expectedCounts[rangeID]++;
-          }
-        }
-      }
-
-      SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-      s.search(new MatchAllDocsQuery(), sfc);
-      Facets facets = new RangeFacetCounts("field", sfc, ranges);
-      SimpleFacetResult result = facets.getTopChildren(10, "field");
-      assertEquals(numRange, result.labelValues.length);
-      for(int rangeID=0;rangeID<numRange;rangeID++) {
-        if (VERBOSE) {
-          System.out.println("  range " + rangeID + " expectedCount=" + expectedCounts[rangeID]);
-        }
-        LabelAndValue subNode = result.labelValues[rangeID];
-        assertEquals("r" + rangeID, subNode.label);
-        assertEquals(expectedCounts[rangeID], subNode.value.intValue());
-
-        DoubleRange range = ranges[rangeID];
-
-        // Test drill-down:
-        SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config);
-        ddq.add("field", NumericRangeQuery.newDoubleRange("field", range.min, range.max, range.minInclusive, range.maxInclusive));
-        assertEquals(expectedCounts[rangeID], s.search(ddq, 10).totalHits);
-      }
-    }
-
-    IOUtils.close(w, r, dir);
-  }
-
-  // LUCENE-5178
-  public void testMissingValues() throws Exception {
-    assumeTrue("codec does not support docsWithField", defaultCodecSupportsDocsWithField());
-    Directory d = newDirectory();
-    RandomIndexWriter w = new RandomIndexWriter(random(), d);
-    Document doc = new Document();
-    NumericDocValuesField field = new NumericDocValuesField("field", 0L);
-    doc.add(field);
-    for(long l=0;l<100;l++) {
-      if (l % 5 == 0) {
-        // Every 5th doc is missing the value:
-        w.addDocument(new Document());
-        continue;
-      }
-      field.setLongValue(l);
-      w.addDocument(doc);
-    }
-
-    IndexReader r = w.getReader();
-
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-
-    IndexSearcher s = newSearcher(r);
-    s.search(new MatchAllDocsQuery(), sfc);
-    Facets facets = new RangeFacetCounts("field", sfc,
-        new LongRange("less than 10", 0L, true, 10L, false),
-        new LongRange("less than or equal to 10", 0L, true, 10L, true),
-        new LongRange("over 90", 90L, false, 100L, false),
-        new LongRange("90 or above", 90L, true, 100L, false),
-        new LongRange("over 1000", 1000L, false, Long.MAX_VALUE, false));
-    
-    assertEquals("value=100 childCount=5\n  less than 10 (8)\n  less than or equal to 10 (8)\n  over 90 (8)\n  90 or above (8)\n  over 1000 (0)\n",
-                 facets.getTopChildren(10, "field").toString());
-
-    IOUtils.close(w, r, d);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSearcherTaxonomyManager.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSearcherTaxonomyManager.java
deleted file mode 100644
index 3acba86..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSearcherTaxonomyManager.java
+++ /dev/null
@@ -1,185 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.simple.SearcherTaxonomyManager.SearcherAndTaxonomy;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestSearcherTaxonomyManager extends FacetTestCase {
-  public void test() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    final IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    final DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-    final FacetsConfig config = new FacetsConfig(tw);
-    config.setMultiValued("field", true);
-    final AtomicBoolean stop = new AtomicBoolean();
-
-    // How many unique facets to index before stopping:
-    final int ordLimit = TEST_NIGHTLY ? 100000 : 6000;
-
-    Thread indexer = new Thread() {
-        @Override
-        public void run() {
-          try {
-            Set<String> seen = new HashSet<String>();
-            List<String> paths = new ArrayList<String>();
-            while (true) {
-              Document doc = new Document();
-              int numPaths = _TestUtil.nextInt(random(), 1, 5);
-              for(int i=0;i<numPaths;i++) {
-                String path;
-                if (!paths.isEmpty() && random().nextInt(5) != 4) {
-                  // Use previous path
-                  path = paths.get(random().nextInt(paths.size()));
-                } else {
-                  // Create new path
-                  path = null;
-                  while (true) {
-                    path = _TestUtil.randomRealisticUnicodeString(random());
-                    if (path.length() != 0 && !seen.contains(path)) {
-                      seen.add(path);
-                      paths.add(path);
-                      break;
-                    }
-                  }
-                }
-                doc.add(new FacetField("field", path));
-              }
-              try {
-                w.addDocument(config.build(doc));
-              } catch (IOException ioe) {
-                throw new RuntimeException(ioe);
-              }
-
-              if (tw.getSize() >= ordLimit) {
-                break;
-              }
-            }
-          } finally {
-            stop.set(true);
-          }
-        }
-      };
-
-    final SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
-
-    Thread reopener = new Thread() {
-        @Override
-        public void run() {
-          while(!stop.get()) {
-            try {
-              // Sleep for up to 20 msec:
-              Thread.sleep(random().nextInt(20));
-
-              if (VERBOSE) {
-                System.out.println("TEST: reopen");
-              }
-
-              mgr.maybeRefresh();
-
-              if (VERBOSE) {
-                System.out.println("TEST: reopen done");
-              }
-            } catch (Exception ioe) {
-              throw new RuntimeException(ioe);
-            }
-          }
-        }
-      };
-    reopener.start();
-
-    indexer.start();
-
-    try {
-      while (!stop.get()) {
-        SearcherAndTaxonomy pair = mgr.acquire();
-        try {
-          //System.out.println("search maxOrd=" + pair.taxonomyReader.getSize());
-          int topN = _TestUtil.nextInt(random(), 1, 20);
-          
-          SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-          pair.searcher.search(new MatchAllDocsQuery(), sfc);
-          Facets facets = getTaxonomyFacetCounts(pair.taxonomyReader, config, sfc);
-          SimpleFacetResult result = facets.getTopChildren(10, "field");
-          if (pair.searcher.getIndexReader().numDocs() > 0) { 
-            //System.out.println(pair.taxonomyReader.getSize());
-            assertTrue(result.childCount > 0);
-            assertTrue(result.labelValues.length > 0);
-          }
-
-          //if (VERBOSE) {
-          //System.out.println("TEST: facets=" + FacetTestUtils.toSimpleString(results.get(0)));
-          //}
-        } finally {
-          mgr.release(pair);
-        }
-      }
-    } finally {
-      indexer.join();
-      reopener.join();
-    }
-
-    if (VERBOSE) {
-      System.out.println("TEST: now stop");
-    }
-
-    IOUtils.close(mgr, tw, w, taxoDir, dir);
-  }
-
-  public void testReplaceTaxonomy() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-
-    Directory taxoDir2 = newDirectory();
-    DirectoryTaxonomyWriter tw2 = new DirectoryTaxonomyWriter(taxoDir2);
-    tw2.close();
-
-    SearcherTaxonomyManager mgr = new SearcherTaxonomyManager(w, true, null, tw);
-    w.addDocument(new Document());
-    tw.replaceTaxonomy(taxoDir2);
-    taxoDir2.close();
-
-    try {
-      mgr.maybeRefresh();
-      fail("should have hit exception");
-    } catch (IllegalStateException ise) {
-      // expected
-    }
-
-    IOUtils.close(mgr, tw, w, taxoDir, dir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java
deleted file mode 100644
index a30393c..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java
+++ /dev/null
@@ -1,1046 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.simple.SimpleDrillSideways.SimpleDrillSidewaysResult;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Collector;
-import org.apache.lucene.search.DocIdSet;
-import org.apache.lucene.search.Filter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.InPlaceMergeSorter;
-import org.apache.lucene.util.InfoStream;
-import org.apache.lucene.util._TestUtil;
-
-public class TestSimpleDrillSideways extends FacetTestCase {
-
-  public void testBasic() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setHierarchical("Publish Date", true);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new FacetField("Author", "Bob"));
-    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Lisa"));
-    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Lisa"));
-    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Susan"));
-    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Frank"));
-    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    //System.out.println("searcher=" + searcher);
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    SimpleDrillSideways ds = new SimpleDrillSideways(searcher, config, taxoReader);
-
-    // Simple case: drill-down on a single field; in this
-    // case the drill-sideways + drill-down counts ==
-    // drill-down of just the query: 
-    SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config);
-    ddq.add("Author", "Lisa");
-    SimpleDrillSidewaysResult r = ds.search(null, ddq, 10);
-    assertEquals(2, r.hits.totalHits);
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
-
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published twice, and Frank/Susan/Bob
-    // published once:
-    assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
-
-    // Same simple case, but no baseQuery (pure browse):
-    // drill-down on a single field; in this case the
-    // drill-sideways + drill-down counts == drill-down of
-    // just the query:
-    ddq = new SimpleDrillDownQuery(config);
-    ddq.add("Author", "Lisa");
-    r = ds.search(null, ddq, 10);
-
-    assertEquals(2, r.hits.totalHits);
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
-
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published twice, and Frank/Susan/Bob
-    // published once:
-    assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
-
-    // Another simple case: drill-down on on single fields
-    // but OR of two values
-    ddq = new SimpleDrillDownQuery(config);
-    ddq.add("Author", "Lisa");
-    ddq.add("Author", "Bob");
-    r = ds.search(null, ddq, 10);
-    assertEquals(3, r.hits.totalHits);
-    // Publish Date is only drill-down: Lisa and Bob
-    // (drill-down) published twice in 2010 and once in 2012:
-    assertEquals("value=3 childCount=2\n  2010 (2)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published twice, and Frank/Susan/Bob
-    // published once:
-    assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
-
-    // More interesting case: drill-down on two fields
-    ddq = new SimpleDrillDownQuery(config);
-    ddq.add("Author", "Lisa");
-    ddq.add("Publish Date", "2010");
-    r = ds.search(null, ddq, 10);
-    assertEquals(1, r.hits.totalHits);
-    // Publish Date is drill-sideways + drill-down: Lisa
-    // (drill-down) published once in 2010 and once in 2012:
-    assertEquals("value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
-    // Author is drill-sideways + drill-down:
-    // only Lisa & Bob published (once each) in 2010:
-    assertEquals("value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
-
-    // Even more interesting case: drill down on two fields,
-    // but one of them is OR
-    ddq = new SimpleDrillDownQuery(config);
-
-    // Drill down on Lisa or Bob:
-    ddq.add("Author", "Lisa");
-    ddq.add("Publish Date", "2010");
-    ddq.add("Author", "Bob");
-    r = ds.search(null, ddq, 10);
-    assertEquals(2, r.hits.totalHits);
-    // Publish Date is both drill-sideways + drill-down:
-    // Lisa or Bob published twice in 2010 and once in 2012:
-    assertEquals("value=3 childCount=2\n  2010 (2)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
-    // Author is drill-sideways + drill-down:
-    // only Lisa & Bob published (once each) in 2010:
-    assertEquals("value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
-
-    // Test drilling down on invalid field:
-    ddq = new SimpleDrillDownQuery(config);
-    ddq.add("Foobar", "Baz");
-    r = ds.search(null, ddq, 10);
-    assertEquals(0, r.hits.totalHits);
-    assertNull(r.facets.getTopChildren(10, "Publish Date"));
-    assertNull(r.facets.getTopChildren(10, "Foobar"));
-
-    // Test drilling down on valid term or'd with invalid term:
-    ddq = new SimpleDrillDownQuery(config);
-    ddq.add("Author", "Lisa");
-    ddq.add("Author", "Tom");
-    r = ds.search(null, ddq, 10);
-    assertEquals(2, r.hits.totalHits);
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published twice, and Frank/Susan/Bob
-    // published once:
-    assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
-
-    // LUCENE-4915: test drilling down on a dimension but
-    // NOT facet counting it:
-    ddq = new SimpleDrillDownQuery(config);
-    ddq.add("Author", "Lisa");
-    ddq.add("Author", "Tom");
-    r = ds.search(null, ddq, 10);
-    assertEquals(2, r.hits.totalHits);
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("value=2 childCount=2\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
-
-    // Test main query gets null scorer:
-    ddq = new SimpleDrillDownQuery(config, new TermQuery(new Term("foobar", "baz")));
-    ddq.add("Author", "Lisa");
-    r = ds.search(null, ddq, 10);
-
-    assertEquals(0, r.hits.totalHits);
-    assertNull(r.facets.getTopChildren(10, "Publish Date"));
-    assertNull(r.facets.getTopChildren(10, "Author"));
-    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
-  }
-
-  public void testSometimesInvalidDrillDown() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setHierarchical("Publish Date", true);
-
-    Document doc = new Document();
-    doc.add(new FacetField("Author", "Bob"));
-    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Lisa"));
-    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
-    writer.addDocument(config.build(doc));
-
-    writer.commit();
-
-    // 2nd segment has no Author:
-    doc = new Document();
-    doc.add(new FacetField("Foobar", "Lisa"));
-    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    //System.out.println("searcher=" + searcher);
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config);
-    ddq.add("Author", "Lisa");
-    SimpleDrillSidewaysResult r = new SimpleDrillSideways(searcher, config, taxoReader).search(null, ddq, 10);
-
-    assertEquals(1, r.hits.totalHits);
-    // Publish Date is only drill-down, and Lisa published
-    // one in 2012 and one in 2010:
-    assertEquals("value=1 childCount=1\n  2010 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
-    // Author is drill-sideways + drill-down: Lisa
-    // (drill-down) published once, and Bob
-    // published once:
-    assertEquals("value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
-
-    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
-  }
-
-  public void testMultipleRequestsPerDim() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setHierarchical("dim", true);
-
-    Document doc = new Document();
-    doc.add(new FacetField("dim", "a", "x"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("dim", "a", "y"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("dim", "a", "z"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("dim", "b"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("dim", "c"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("dim", "d"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    //System.out.println("searcher=" + searcher);
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config);
-    ddq.add("dim", "a");
-    SimpleDrillSidewaysResult r = new SimpleDrillSideways(searcher, config, taxoReader).search(null, ddq, 10);
-
-    assertEquals(3, r.hits.totalHits);
-    assertEquals("value=6 childCount=4\n  a (3)\n  b (1)\n  c (1)\n  d (1)\n", r.facets.getTopChildren(10, "dim").toString());
-    assertEquals("value=3 childCount=3\n  x (1)\n  y (1)\n  z (1)\n", r.facets.getTopChildren(10, "dim", "a").toString());
-
-    IOUtils.close(searcher.getIndexReader(), taxoReader, writer, taxoWriter, dir, taxoDir);
-  }
-
-  private static class Doc implements Comparable<Doc> {
-    String id;
-    String contentToken;
-
-    public Doc() {}
-    
-    // -1 if the doc is missing this dim, else the index
-    // -into the values for this dim:
-    int[] dims;
-
-    // 2nd value per dim for the doc (so we test
-    // multi-valued fields):
-    int[] dims2;
-    boolean deleted;
-
-    @Override
-    public int compareTo(Doc other) {
-      return id.compareTo(other.id);
-    }
-  }
-
-  private double aChance, bChance, cChance;
-
-  private String randomContentToken(boolean isQuery) {
-    double d = random().nextDouble();
-    if (isQuery) {
-      if (d < 0.33) {
-        return "a";
-      } else if (d < 0.66) {
-        return "b";
-      } else {
-        return "c";
-      }
-    } else {
-      if (d <= aChance) {
-        return "a";
-      } else if (d < aChance + bChance) {
-        return "b";
-      } else {
-        return "c";
-      }
-    }
-  }
-
-  public void testRandom() throws Exception {
-
-    boolean canUseDV = defaultCodecSupportsSortedSet();
-
-    while (aChance == 0.0) {
-      aChance = random().nextDouble();
-    }
-    while (bChance == 0.0) {
-      bChance = random().nextDouble();
-    }
-    while (cChance == 0.0) {
-      cChance = random().nextDouble();
-    }
-    //aChance = .01;
-    //bChance = 0.5;
-    //cChance = 1.0;
-    double sum = aChance + bChance + cChance;
-    aChance /= sum;
-    bChance /= sum;
-    cChance /= sum;
-
-    int numDims = _TestUtil.nextInt(random(), 2, 5);
-    //int numDims = 3;
-    int numDocs = atLeast(3000);
-    //int numDocs = 20;
-    if (VERBOSE) {
-      System.out.println("numDims=" + numDims + " numDocs=" + numDocs + " aChance=" + aChance + " bChance=" + bChance + " cChance=" + cChance);
-    }
-    String[][] dimValues = new String[numDims][];
-    int valueCount = 2;
-
-    for(int dim=0;dim<numDims;dim++) {
-      Set<String> values = new HashSet<String>();
-      while (values.size() < valueCount) {
-        String s = _TestUtil.randomRealisticUnicodeString(random());
-        //String s = _TestUtil.randomSimpleString(random());
-        if (s.length() > 0) {
-          values.add(s);
-        }
-      } 
-      dimValues[dim] = values.toArray(new String[values.size()]);
-      valueCount *= 2;
-    }
-
-    List<Doc> docs = new ArrayList<Doc>();
-    for(int i=0;i<numDocs;i++) {
-      Doc doc = new Doc();
-      doc.id = ""+i;
-      doc.contentToken = randomContentToken(false);
-      doc.dims = new int[numDims];
-      doc.dims2 = new int[numDims];
-      for(int dim=0;dim<numDims;dim++) {
-        if (random().nextInt(5) == 3) {
-          // This doc is missing this dim:
-          doc.dims[dim] = -1;
-        } else if (dimValues[dim].length <= 4) {
-          int dimUpto = 0;
-          doc.dims[dim] = dimValues[dim].length-1;
-          while (dimUpto < dimValues[dim].length) {
-            if (random().nextBoolean()) {
-              doc.dims[dim] = dimUpto;
-              break;
-            }
-            dimUpto++;
-          }
-        } else {
-          doc.dims[dim] = random().nextInt(dimValues[dim].length);
-        }
-
-        if (random().nextInt(5) == 3) {
-          // 2nd value:
-          doc.dims2[dim] = random().nextInt(dimValues[dim].length);
-        } else {
-          doc.dims2[dim] = -1;
-        }
-      }
-      docs.add(doc);
-    }
-
-    Directory d = newDirectory();
-    Directory td = newDirectory();
-
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setInfoStream(InfoStream.NO_OUTPUT);
-    RandomIndexWriter w = new RandomIndexWriter(random(), d, iwc);
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
-    FacetsConfig config = new FacetsConfig(tw);
-    for(int i=0;i<numDims;i++) {
-      config.setMultiValued("dim"+i, true);
-    }
-
-    boolean doUseDV = canUseDV && random().nextBoolean();
-
-    for(Doc rawDoc : docs) {
-      Document doc = new Document();
-      doc.add(newStringField("id", rawDoc.id, Field.Store.YES));
-      doc.add(newStringField("content", rawDoc.contentToken, Field.Store.NO));
-
-      if (VERBOSE) {
-        System.out.println("  doc id=" + rawDoc.id + " token=" + rawDoc.contentToken);
-      }
-      for(int dim=0;dim<numDims;dim++) {
-        int dimValue = rawDoc.dims[dim];
-        if (dimValue != -1) {
-          if (doUseDV) {
-            doc.add(new SortedSetDocValuesFacetField("dim" + dim, dimValues[dim][dimValue]));
-          } else {
-            doc.add(new FacetField("dim" + dim, dimValues[dim][dimValue]));
-          }
-          doc.add(new StringField("dim" + dim, dimValues[dim][dimValue], Field.Store.YES));
-          if (VERBOSE) {
-            System.out.println("    dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue]));
-          }
-        }
-        int dimValue2 = rawDoc.dims2[dim];
-        if (dimValue2 != -1) {
-          if (doUseDV) {
-            doc.add(new SortedSetDocValuesFacetField("dim" + dim, dimValues[dim][dimValue2]));
-          } else {
-            doc.add(new FacetField("dim" + dim, dimValues[dim][dimValue2]));
-          }
-          doc.add(new StringField("dim" + dim, dimValues[dim][dimValue2], Field.Store.YES));
-          if (VERBOSE) {
-            System.out.println("      dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue2]));
-          }
-        }
-      }
-
-      w.addDocument(config.build(doc));
-    }
-
-    if (random().nextBoolean()) {
-      // Randomly delete a few docs:
-      int numDel = _TestUtil.nextInt(random(), 1, (int) (numDocs*0.05));
-      if (VERBOSE) {
-        System.out.println("delete " + numDel);
-      }
-      int delCount = 0;
-      while (delCount < numDel) {
-        Doc doc = docs.get(random().nextInt(docs.size()));
-        if (!doc.deleted) {
-          if (VERBOSE) {
-            System.out.println("  delete id=" + doc.id);
-          }
-          doc.deleted = true;
-          w.deleteDocuments(new Term("id", doc.id));
-          delCount++;
-        }
-      }
-    }
-
-    if (random().nextBoolean()) {
-      if (VERBOSE) {
-        System.out.println("TEST: forceMerge(1)...");
-      }
-      w.forceMerge(1);
-    }
-    IndexReader r = w.getReader();
-
-    final SortedSetDocValuesReaderState sortedSetDVState;
-    IndexSearcher s = newSearcher(r);
-    
-    if (doUseDV) {
-      sortedSetDVState = new SortedSetDocValuesReaderState(s.getIndexReader());
-    } else {
-      sortedSetDVState = null;
-    }
-
-    if (VERBOSE) {
-      System.out.println("r.numDocs() = " + r.numDocs());
-    }
-
-    // NRT open
-    TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
-
-    int numIters = atLeast(10);
-
-    for(int iter=0;iter<numIters;iter++) {
-
-      String contentToken = random().nextInt(30) == 17 ? null : randomContentToken(true);
-      int numDrillDown = _TestUtil.nextInt(random(), 1, Math.min(4, numDims));
-      if (VERBOSE) {
-        System.out.println("\nTEST: iter=" + iter + " baseQuery=" + contentToken + " numDrillDown=" + numDrillDown + " useSortedSetDV=" + doUseDV);
-      }
-
-      String[][] drillDowns = new String[numDims][];
-
-      int count = 0;
-      boolean anyMultiValuedDrillDowns = false;
-      while (count < numDrillDown) {
-        int dim = random().nextInt(numDims);
-        if (drillDowns[dim] == null) {
-          if (random().nextBoolean()) {
-            // Drill down on one value:
-            drillDowns[dim] = new String[] {dimValues[dim][random().nextInt(dimValues[dim].length)]};
-          } else {
-            int orCount = _TestUtil.nextInt(random(), 1, Math.min(5, dimValues[dim].length));
-            drillDowns[dim] = new String[orCount];
-            anyMultiValuedDrillDowns |= orCount > 1;
-            for(int i=0;i<orCount;i++) {
-              while (true) {
-                String value = dimValues[dim][random().nextInt(dimValues[dim].length)];
-                for(int j=0;j<i;j++) {
-                  if (value.equals(drillDowns[dim][j])) {
-                    value = null;
-                    break;
-                  }
-                }
-                if (value != null) {
-                  drillDowns[dim][i] = value;
-                  break;
-                }
-              }
-            }
-          }
-          if (VERBOSE) {
-            BytesRef[] values = new BytesRef[drillDowns[dim].length];
-            for(int i=0;i<values.length;i++) {
-              values[i] = new BytesRef(drillDowns[dim][i]);
-            }
-            System.out.println("  dim" + dim + "=" + Arrays.toString(values));
-          }
-          count++;
-        }
-      }
-
-      Query baseQuery;
-      if (contentToken == null) {
-        baseQuery = new MatchAllDocsQuery();
-      } else {
-        baseQuery = new TermQuery(new Term("content", contentToken));
-      }
-
-      SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config, baseQuery);
-
-      for(int dim=0;dim<numDims;dim++) {
-        if (drillDowns[dim] != null) {
-          int upto = 0;
-          for(String value : drillDowns[dim]) {
-            ddq.add("dim" + dim, value);
-          }
-        }
-      }
-
-      Filter filter;
-      if (random().nextInt(7) == 6) {
-        if (VERBOSE) {
-          System.out.println("  only-even filter");
-        }
-        filter = new Filter() {
-            @Override
-            public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
-              int maxDoc = context.reader().maxDoc();
-              final FixedBitSet bits = new FixedBitSet(maxDoc);
-              for(int docID=0;docID < maxDoc;docID++) {
-                // Keeps only the even ids:
-                if ((acceptDocs == null || acceptDocs.get(docID)) && ((Integer.parseInt(context.reader().document(docID).get("id")) & 1) == 0)) {
-                  bits.set(docID);
-                }
-              }
-              return bits;
-            }
-          };
-      } else {
-        filter = null;
-      }
-
-      // Verify docs are always collected in order.  If we
-      // had an AssertingScorer it could catch it when
-      // Weight.scoresDocsOutOfOrder lies!:
-      new SimpleDrillSideways(s, config, tr).search(ddq,
-                           new Collector() {
-                             int lastDocID;
-
-                             @Override
-                             public void setScorer(Scorer s) {
-                             }
-
-                             @Override
-                             public void collect(int doc) {
-                               assert doc > lastDocID;
-                               lastDocID = doc;
-                             }
-
-                             @Override
-                             public void setNextReader(AtomicReaderContext context) {
-                               lastDocID = -1;
-                             }
-
-                             @Override
-                             public boolean acceptsDocsOutOfOrder() {
-                               return false;
-                             }
-                           });
-
-      // Also separately verify that DS respects the
-      // scoreSubDocsAtOnce method, to ensure that all
-      // subScorers are on the same docID:
-      if (!anyMultiValuedDrillDowns) {
-        // Can only do this test when there are no OR'd
-        // drill-down values, beacuse in that case it's
-        // easily possible for one of the DD terms to be on
-        // a future docID:
-        new SimpleDrillSideways(s, config, tr) {
-          @Override
-          protected boolean scoreSubDocsAtOnce() {
-            return true;
-          }
-        }.search(ddq, new AssertingSubDocsAtOnceCollector());
-      }
-
-      SimpleTestFacetResult expected = slowDrillSidewaysSearch(s, docs, contentToken, drillDowns, dimValues, filter);
-
-      Sort sort = new Sort(new SortField("id", SortField.Type.STRING));
-      // nocommit subclass & override to use FacetsTestCase.getFacetCounts
-      SimpleDrillSideways ds;
-      if (doUseDV) {
-        ds = new SimpleDrillSideways(s, config, sortedSetDVState);
-      } else {
-        ds = new SimpleDrillSideways(s, config, tr);
-      }
-
-      // Retrieve all facets:
-      SimpleDrillSidewaysResult actual = ds.search(ddq, filter, null, numDocs, sort, true, true);
-
-      TopDocs hits = s.search(baseQuery, numDocs);
-      Map<String,Float> scores = new HashMap<String,Float>();
-      for(ScoreDoc sd : hits.scoreDocs) {
-        scores.put(s.doc(sd.doc).get("id"), sd.score);
-      }
-      if (VERBOSE) {
-        System.out.println("  verify all facets");
-      }
-      verifyEquals(dimValues, s, expected, actual, scores, doUseDV);
-
-      // Make sure drill down doesn't change score:
-      TopDocs ddqHits = s.search(ddq, filter, numDocs);
-      assertEquals(expected.hits.size(), ddqHits.totalHits);
-      for(int i=0;i<expected.hits.size();i++) {
-        // Score should be IDENTICAL:
-        assertEquals(scores.get(expected.hits.get(i).id), ddqHits.scoreDocs[i].score, 0.0f);
-      }
-    }
-
-    IOUtils.close(r, tr, w, tw, d, td);
-  }
-
-  private static class Counters {
-    int[][] counts;
-
-    public Counters(String[][] dimValues) {
-      counts = new int[dimValues.length][];
-      for(int dim=0;dim<dimValues.length;dim++) {
-        counts[dim] = new int[dimValues[dim].length];
-      }
-    }
-
-    public void inc(int[] dims, int[] dims2) {
-      inc(dims, dims2, -1);
-    }
-
-    public void inc(int[] dims, int[] dims2, int onlyDim) {
-      assert dims.length == counts.length;
-      assert dims2.length == counts.length;
-      for(int dim=0;dim<dims.length;dim++) {
-        if (onlyDim == -1 || dim == onlyDim) {
-          if (dims[dim] != -1) {
-            counts[dim][dims[dim]]++;
-          }
-          if (dims2[dim] != -1 && dims2[dim] != dims[dim]) {
-            counts[dim][dims2[dim]]++;
-          }
-        }
-      }
-    }
-  }
-
-  private static class SimpleTestFacetResult {
-    List<Doc> hits;
-    int[][] counts;
-    int[] uniqueCounts;
-  }
-  
-  private int[] getTopNOrds(final int[] counts, final String[] values, int topN) {
-    final int[] ids = new int[counts.length];
-    for(int i=0;i<ids.length;i++) {
-      ids[i] = i;
-    }
-
-    // Naive (on purpose, to reduce bug in tester/gold):
-    // sort all ids, then return top N slice:
-    new InPlaceMergeSorter() {
-
-      @Override
-      protected void swap(int i, int j) {
-        int id = ids[i];
-        ids[i] = ids[j];
-        ids[j] = id;
-      }
-
-      @Override
-      protected int compare(int i, int j) {
-        int counti = counts[ids[i]];
-        int countj = counts[ids[j]];
-        // Sort by count descending...
-        if (counti > countj) {
-          return -1;
-        } else if (counti < countj) {
-          return 1;
-        } else {
-          // ... then by label ascending:
-          return new BytesRef(values[ids[i]]).compareTo(new BytesRef(values[ids[j]]));
-        }
-      }
-
-    }.sort(0, ids.length);
-
-    if (topN > ids.length) {
-      topN = ids.length;
-    }
-
-    int numSet = topN;
-    for(int i=0;i<topN;i++) {
-      if (counts[ids[i]] == 0) {
-        numSet = i;
-        break;
-      }
-    }
-
-    int[] topNIDs = new int[numSet];
-    System.arraycopy(ids, 0, topNIDs, 0, topNIDs.length);
-    return topNIDs;
-  }
-
-  private SimpleTestFacetResult slowDrillSidewaysSearch(IndexSearcher s, List<Doc> docs,
-                                                        String contentToken, String[][] drillDowns,
-                                                        String[][] dimValues, Filter onlyEven) throws Exception {
-    int numDims = dimValues.length;
-
-    List<Doc> hits = new ArrayList<Doc>();
-    Counters drillDownCounts = new Counters(dimValues);
-    Counters[] drillSidewaysCounts = new Counters[dimValues.length];
-    for(int dim=0;dim<numDims;dim++) {
-      drillSidewaysCounts[dim] = new Counters(dimValues);
-    }
-
-    if (VERBOSE) {
-      System.out.println("  compute expected");
-    }
-
-    nextDoc: for(Doc doc : docs) {
-      if (doc.deleted) {
-        continue;
-      }
-      if (onlyEven != null & (Integer.parseInt(doc.id) & 1) != 0) {
-        continue;
-      }
-      if (contentToken == null || doc.contentToken.equals(contentToken)) {
-        int failDim = -1;
-        for(int dim=0;dim<numDims;dim++) {
-          if (drillDowns[dim] != null) {
-            String docValue = doc.dims[dim] == -1 ? null : dimValues[dim][doc.dims[dim]];
-            String docValue2 = doc.dims2[dim] == -1 ? null : dimValues[dim][doc.dims2[dim]];
-            boolean matches = false;
-            for(String value : drillDowns[dim]) {
-              if (value.equals(docValue) || value.equals(docValue2)) {
-                matches = true;
-                break;
-              }
-            }
-            if (!matches) {
-              if (failDim == -1) {
-                // Doc could be a near-miss, if no other dim fails
-                failDim = dim;
-              } else {
-                // Doc isn't a hit nor a near-miss
-                continue nextDoc;
-              }
-            }
-          }
-        }
-
-        if (failDim == -1) {
-          if (VERBOSE) {
-            System.out.println("    exp: id=" + doc.id + " is a hit");
-          }
-          // Hit:
-          hits.add(doc);
-          drillDownCounts.inc(doc.dims, doc.dims2);
-          for(int dim=0;dim<dimValues.length;dim++) {
-            drillSidewaysCounts[dim].inc(doc.dims, doc.dims2);
-          }
-        } else {
-          if (VERBOSE) {
-            System.out.println("    exp: id=" + doc.id + " is a near-miss on dim=" + failDim);
-          }
-          drillSidewaysCounts[failDim].inc(doc.dims, doc.dims2, failDim);
-        }
-      }
-    }
-
-    Map<String,Integer> idToDocID = new HashMap<String,Integer>();
-    for(int i=0;i<s.getIndexReader().maxDoc();i++) {
-      idToDocID.put(s.doc(i).get("id"), i);
-    }
-
-    Collections.sort(hits);
-
-    SimpleTestFacetResult res = new SimpleTestFacetResult();
-    res.hits = hits;
-    res.counts = new int[numDims][];
-    res.uniqueCounts = new int[numDims];
-    for (int dim = 0; dim < numDims; dim++) {
-      if (drillDowns[dim] != null) {
-        res.counts[dim] = drillSidewaysCounts[dim].counts[dim];
-      } else {
-        res.counts[dim] = drillDownCounts.counts[dim];
-      }
-      int uniqueCount = 0;
-      for (int j = 0; j < res.counts[dim].length; j++) {
-        if (res.counts[dim][j] != 0) {
-          uniqueCount++;
-        }
-      }
-      res.uniqueCounts[dim] = uniqueCount;
-    }
-
-    return res;
-  }
-
-  void verifyEquals(String[][] dimValues, IndexSearcher s, SimpleTestFacetResult expected,
-                    SimpleDrillSidewaysResult actual, Map<String,Float> scores, boolean isSortedSetDV) throws Exception {
-    if (VERBOSE) {
-      System.out.println("  verify totHits=" + expected.hits.size());
-    }
-    assertEquals(expected.hits.size(), actual.hits.totalHits);
-    assertEquals(expected.hits.size(), actual.hits.scoreDocs.length);
-    for(int i=0;i<expected.hits.size();i++) {
-      if (VERBOSE) {
-        System.out.println("    hit " + i + " expected=" + expected.hits.get(i).id);
-      }
-      assertEquals(expected.hits.get(i).id,
-                   s.doc(actual.hits.scoreDocs[i].doc).get("id"));
-      // Score should be IDENTICAL:
-      assertEquals(scores.get(expected.hits.get(i).id), actual.hits.scoreDocs[i].score, 0.0f);
-    }
-
-    for(int dim=0;dim<expected.counts.length;dim++) {
-      int topN = random().nextBoolean() ? dimValues[dim].length : _TestUtil.nextInt(random(), 1, dimValues[dim].length);
-      SimpleFacetResult fr = actual.facets.getTopChildren(topN, "dim"+dim);
-      if (VERBOSE) {
-        System.out.println("    dim" + dim + " topN=" + topN + " (vs " + dimValues[dim].length + " unique values)");
-        System.out.println("      actual");
-      }
-
-      int idx = 0;
-      Map<String,Integer> actualValues = new HashMap<String,Integer>();
-
-      if (fr != null) {
-        for(LabelAndValue labelValue : fr.labelValues) {
-          actualValues.put(labelValue.label, labelValue.value.intValue());
-          if (VERBOSE) {
-            System.out.println("        " + idx + ": " + new BytesRef(labelValue.label) + ": " + labelValue.value);
-            idx++;
-          }
-        }
-      }
-
-      if (topN < dimValues[dim].length) {
-        int[] topNIDs = getTopNOrds(expected.counts[dim], dimValues[dim], topN);
-        if (VERBOSE) {
-          idx = 0;
-          System.out.println("      expected (sorted)");
-          for(int i=0;i<topNIDs.length;i++) {
-            int expectedOrd = topNIDs[i];
-            String value = dimValues[dim][expectedOrd];
-            System.out.println("        " + idx + ": " + new BytesRef(value) + ": " + expected.counts[dim][expectedOrd]);
-            idx++;
-          }
-        }
-        if (VERBOSE) {
-          System.out.println("      topN=" + topN + " expectedTopN=" + topNIDs.length);
-        }
-
-        if (fr != null) {
-          assertEquals(topNIDs.length, fr.labelValues.length);
-        } else {
-          assertEquals(0, topNIDs.length);
-        }
-        for(int i=0;i<topNIDs.length;i++) {
-          int expectedOrd = topNIDs[i];
-          assertEquals(expected.counts[dim][expectedOrd], fr.labelValues[i].value.intValue());
-          if (isSortedSetDV) {
-            // Tie-break facet labels are only in unicode
-            // order with SortedSetDVFacets:
-            assertEquals("value @ idx=" + i, dimValues[dim][expectedOrd], fr.labelValues[i].label);
-          }
-        }
-      } else {
-
-        if (VERBOSE) {
-          idx = 0;
-          System.out.println("      expected (unsorted)");
-          for(int i=0;i<dimValues[dim].length;i++) {
-            String value = dimValues[dim][i];
-            if (expected.counts[dim][i] != 0) {
-              System.out.println("        " + idx + ": " + new BytesRef(value) + ": " + expected.counts[dim][i]);
-              idx++;
-            } 
-          }
-        }
-
-        int setCount = 0;
-        for(int i=0;i<dimValues[dim].length;i++) {
-          String value = dimValues[dim][i];
-          if (expected.counts[dim][i] != 0) {
-            assertTrue(actualValues.containsKey(value));
-            assertEquals(expected.counts[dim][i], actualValues.get(value).intValue());
-            setCount++;
-          } else {
-            assertFalse(actualValues.containsKey(value));
-          }
-        }
-        assertEquals(setCount, actualValues.size());
-      }
-
-      // nocommit if we add this to SimpleFR then re-enable this:
-      // assertEquals("dim=" + dim, expected.uniqueCounts[dim], fr.getNumValidDescendants());
-    }
-  }
-
-  public void testEmptyIndex() throws Exception {
-    // LUCENE-5045: make sure DrillSideways works with an empty index
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    // Count "Author"
-    FacetsConfig config = new FacetsConfig();
-    SimpleDrillSideways ds = new SimpleDrillSideways(searcher, config, taxoReader);
-    SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(config);
-    ddq.add("Author", "Lisa");
-    
-    SimpleDrillSidewaysResult r = ds.search(ddq, 10); // this used to fail on IllegalArgEx
-    assertEquals(0, r.hits.totalHits);
-
-    r = ds.search(ddq, null, null, 10, new Sort(new SortField("foo", SortField.Type.INT)), false, false); // this used to fail on IllegalArgEx
-    assertEquals(0, r.hits.totalHits);
-    
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-}
-
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSortedSetDocValuesFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSortedSetDocValuesFacets.java
deleted file mode 100644
index 2b8bb76..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSortedSetDocValuesFacets.java
+++ /dev/null
@@ -1,222 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.List;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-
-public class TestSortedSetDocValuesFacets extends FacetTestCase {
-
-  // NOTE: TestDrillSideways.testRandom also sometimes
-  // randomly uses SortedSetDV
-
-  public void testBasic() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory dir = newDirectory();
-
-    FacetsConfig config = new FacetsConfig();
-    config.setMultiValued("a", true);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
-    doc.add(new SortedSetDocValuesFacetField("a", "bar"));
-    doc.add(new SortedSetDocValuesFacetField("a", "zoo"));
-    doc.add(new SortedSetDocValuesFacetField("b", "baz"));
-    writer.addDocument(config.build(doc));
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // Per-top-reader state:
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
-    
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
-
-    assertEquals("value=4 childCount=3\n  foo (2)\n  bar (1)\n  zoo (1)\n", facets.getTopChildren(10, "a").toString());
-    assertEquals("value=1 childCount=1\n  baz (1)\n", facets.getTopChildren(10, "b").toString());
-
-    // DrillDown:
-    SimpleDrillDownQuery q = new SimpleDrillDownQuery(config);
-    q.add("a", "foo");
-    q.add("b", "baz");
-    TopDocs hits = searcher.search(q, 1);
-    assertEquals(1, hits.totalHits);
-
-    IOUtils.close(writer, searcher.getIndexReader(), dir);
-  }
-
-  // LUCENE-5090
-  public void testStaleState() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory dir = newDirectory();
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo"));
-    writer.addDocument(config.build(doc));
-
-    IndexReader r = writer.getReader();
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(r);
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "bar"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "baz"));
-    writer.addDocument(config.build(doc));
-
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    try {
-      new SortedSetDocValuesFacetCounts(state, c);
-      fail("did not hit expected exception");
-    } catch (IllegalStateException ise) {
-      // expected
-    }
-
-    r.close();
-    writer.close();
-    searcher.getIndexReader().close();
-    dir.close();
-  }
-
-  // LUCENE-5333
-  public void testSparseFacets() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory dir = newDirectory();
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
-    writer.addDocument(config.build(doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
-    doc.add(new SortedSetDocValuesFacetField("b", "bar1"));
-    writer.addDocument(config.build(doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo3"));
-    doc.add(new SortedSetDocValuesFacetField("b", "bar2"));
-    doc.add(new SortedSetDocValuesFacetField("c", "baz1"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // Per-top-reader state:
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-    SortedSetDocValuesFacetCounts facets = new SortedSetDocValuesFacetCounts(state, c);
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<SimpleFacetResult> results = facets.getAllDims(10);
-
-    assertEquals(3, results.size());
-    assertEquals("value=3 childCount=3\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
-    assertEquals("value=2 childCount=2\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
-    assertEquals("value=1 childCount=1\n  baz1 (1)\n", results.get(2).toString());
-
-    searcher.getIndexReader().close();
-    dir.close();
-  }
-
-  // nocommit test different delim char & using the default
-  // one in a dim
-
-  // nocommit in the sparse case test that we are really
-  // sorting by the correct dim count
-
-  public void testSlowCompositeReaderWrapper() throws Exception {
-    assumeTrue("Test requires SortedSetDV support", defaultCodecSupportsSortedSet());
-    Directory dir = newDirectory();
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    FacetsConfig config = new FacetsConfig();
-
-    Document doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo1"));
-    writer.addDocument(config.build(doc));
-
-    writer.commit();
-
-    doc = new Document();
-    doc.add(new SortedSetDocValuesFacetField("a", "foo2"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = new IndexSearcher(SlowCompositeReaderWrapper.wrap(writer.getReader()));
-
-    // Per-top-reader state:
-    SortedSetDocValuesReaderState state = new SortedSetDocValuesReaderState(searcher.getIndexReader());
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-    Facets facets = new SortedSetDocValuesFacetCounts(state, c);
-
-    // Ask for top 10 labels for any dims that have counts:
-    assertEquals("value=2 childCount=2\n  foo1 (1)\n  foo2 (1)\n", facets.getTopChildren(10, "a").toString());
-
-    IOUtils.close(writer, searcher.getIndexReader(), dir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetAssociations.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetAssociations.java
deleted file mode 100644
index 186060d..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetAssociations.java
+++ /dev/null
@@ -1,224 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-
-/** Test for associations */
-public class TestTaxonomyFacetAssociations extends FacetTestCase {
-  
-  private static Directory dir;
-  private static IndexReader reader;
-  private static Directory taxoDir;
-  private static TaxonomyReader taxoReader;
-
-  private static final FacetLabel aint = new FacetLabel("int", "a");
-  private static final FacetLabel bint = new FacetLabel("int", "b");
-  private static final FacetLabel afloat = new FacetLabel("float", "a");
-  private static final FacetLabel bfloat = new FacetLabel("float", "b");
-  private static FacetsConfig config;
-
-  @BeforeClass
-  public static void beforeClass() throws Exception {
-    dir = newDirectory();
-    taxoDir = newDirectory();
-    // preparations - index, taxonomy, content
-    
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-
-    // Cannot mix ints & floats in the same indexed field:
-    config = new FacetsConfig(taxoWriter);
-    config.setIndexFieldName("int", "$facets.int");
-    config.setMultiValued("int", true);
-    config.setIndexFieldName("float", "$facets.float");
-    config.setMultiValued("float", true);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    // index documents, 50% have only 'b' and all have 'a'
-    for (int i = 0; i < 110; i++) {
-      Document doc = new Document();
-      // every 11th document is added empty, this used to cause the association
-      // aggregators to go into an infinite loop
-      if (i % 11 != 0) {
-        doc.add(new IntAssociationFacetField(2, "int", "a"));
-        doc.add(new FloatAssociationFacetField(0.5f, "float", "a"));
-        if (i % 2 == 0) { // 50
-          doc.add(new IntAssociationFacetField(3, "int", "b"));
-          doc.add(new FloatAssociationFacetField(0.2f, "float", "b"));
-        }
-      }
-      writer.addDocument(config.build(doc));
-    }
-    
-    taxoWriter.close();
-    reader = writer.getReader();
-    writer.close();
-    taxoReader = new DirectoryTaxonomyReader(taxoDir);
-  }
-  
-  @AfterClass
-  public static void afterClass() throws Exception {
-    reader.close();
-    reader = null;
-    dir.close();
-    dir = null;
-    taxoReader.close();
-    taxoReader = null;
-    taxoDir.close();
-    taxoDir = null;
-  }
-  
-  public void testIntSumAssociation() throws Exception {
-    
-    SimpleFacetsCollector fc = new SimpleFacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-
-    Facets facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
-    assertEquals("value=350 childCount=2\n  a (200)\n  b (150)\n", facets.getTopChildren(10, "int").toString());
-    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
-    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
-  }
-
-  public void testFloatSumAssociation() throws Exception {
-    SimpleFacetsCollector fc = new SimpleFacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
-    assertEquals("value=59.999996 childCount=2\n  a (50.0)\n  b (9.999995)\n", facets.getTopChildren(10, "float").toString());
-    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
-    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
-  }  
-
-  /** Make sure we can test both int and float assocs in one
-   *  index, as long as we send each to a different field. */
-  public void testIntAndFloatAssocation() throws Exception {
-    SimpleFacetsCollector fc = new SimpleFacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    Facets facets = new TaxonomyFacetSumFloatAssociations("$facets.float", taxoReader, config, fc);
-    assertEquals("Wrong count for category 'a'!", 50f, facets.getSpecificValue("float", "a").floatValue(), 0.00001);
-    assertEquals("Wrong count for category 'b'!", 10f, facets.getSpecificValue("float", "b").floatValue(), 0.00001);
-    
-    facets = new TaxonomyFacetSumIntAssociations("$facets.int", taxoReader, config, fc);
-    assertEquals("Wrong count for category 'a'!", 200, facets.getSpecificValue("int", "a").intValue());
-    assertEquals("Wrong count for category 'b'!", 150, facets.getSpecificValue("int", "b").intValue());
-  }
-
-  public void testWrongIndexFieldName() throws Exception {
-    SimpleFacetsCollector fc = new SimpleFacetsCollector();
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    Facets facets = new TaxonomyFacetSumFloatAssociations(taxoReader, config, fc);
-    try {
-      facets.getSpecificValue("float");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    try {
-      facets.getTopChildren(10, "float");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-  }
-
-  public void testMixedTypesInSameIndexField() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new IntAssociationFacetField(14, "a", "x"));
-    doc.add(new FloatAssociationFacetField(55.0f, "b", "y"));
-    try {
-      writer.addDocument(config.build(doc));
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException exc) {
-      // expected
-    }
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-
-  public void testNoHierarchy() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setHierarchical("a", true);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new IntAssociationFacetField(14, "a", "x"));
-    try {
-      writer.addDocument(config.build(doc));
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException exc) {
-      // expected
-    }
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-
-  public void testRequireDimCount() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setRequireDimCount("a", true);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new IntAssociationFacetField(14, "a", "x"));
-    try {
-      writer.addDocument(config.build(doc));
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException exc) {
-      // expected
-    }
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts.java
deleted file mode 100644
index c07d849..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts.java
+++ /dev/null
@@ -1,648 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.PrintStream;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.PrintTaxonomyStats;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.similarities.DefaultSimilarity;
-import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
-import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestTaxonomyFacetCounts extends FacetTestCase {
-
-  public void testBasic() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setHierarchical("Publish Date", true);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new FacetField("Author", "Bob"));
-    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Lisa"));
-    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Lisa"));
-    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Susan"));
-    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new FacetField("Author", "Frank"));
-    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    // Aggregate the facet counts:
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
-
-    // Retrieve & verify results:
-    assertEquals("value=5 childCount=3\n  2010 (2)\n  2012 (2)\n  1999 (1)\n", facets.getTopChildren(10, "Publish Date").toString());
-    assertEquals("value=5 childCount=4\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", facets.getTopChildren(10, "Author").toString());
-
-    // Now user drills down on Publish Date/2010:
-    SimpleDrillDownQuery q2 = new SimpleDrillDownQuery(config);
-    q2.add("Publish Date", "2010");
-    c = new SimpleFacetsCollector();
-    searcher.search(q2, c);
-    facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
-    assertEquals("value=2 childCount=2\n  Bob (1)\n  Lisa (1)\n", facets.getTopChildren(10, "Author").toString());
-
-    assertEquals(1, facets.getSpecificValue("Author", "Lisa"));
-
-    // Smoke test PrintTaxonomyStats:
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintTaxonomyStats.printStats(taxoReader, new PrintStream(bos, false, "UTF-8"), true);
-    String result = bos.toString("UTF-8");
-    assertTrue(result.indexOf("/Author: 4 immediate children; 5 total categories") != -1);
-    assertTrue(result.indexOf("/Publish Date: 3 immediate children; 12 total categories") != -1);
-    // Make sure at least a few nodes of the tree came out:
-    assertTrue(result.indexOf("  /1999") != -1);
-    assertTrue(result.indexOf("  /2012") != -1);
-    assertTrue(result.indexOf("      /20") != -1);
-
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
-  }
-
-  // LUCENE-5333
-  public void testSparseFacets() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-
-    Document doc = new Document();
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(config.build(doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new FacetField("a", "foo2"));
-    doc.add(new FacetField("b", "bar1"));
-    writer.addDocument(config.build(doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new FacetField("a", "foo3"));
-    doc.add(new FacetField("b", "bar2"));
-    doc.add(new FacetField("c", "baz1"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    Facets facets = getTaxonomyFacetCounts(taxoReader, new FacetsConfig(), c);
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<SimpleFacetResult> results = facets.getAllDims(10);
-
-    assertEquals(3, results.size());
-    assertEquals("value=3 childCount=3\n  foo1 (1)\n  foo2 (1)\n  foo3 (1)\n", results.get(0).toString());
-    assertEquals("value=2 childCount=2\n  bar1 (1)\n  bar2 (1)\n", results.get(1).toString());
-    assertEquals("value=1 childCount=1\n  baz1 (1)\n", results.get(2).toString());
-
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
-  }
-
-  public void testWrongIndexFieldName() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setIndexFieldName("a", "$facets2");
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    // Uses default $facets field:
-    Facets facets;
-    if (random().nextBoolean()) {
-      facets = new FastTaxonomyFacetCounts(taxoReader, config, c);
-    } else {
-      OrdinalsReader ordsReader = new DocValuesOrdinalsReader();
-      if (random().nextBoolean()) {
-        ordsReader = new CachedOrdinalsReader(ordsReader);
-      }
-      facets = new TaxonomyFacetCounts(ordsReader, taxoReader, config, c);
-    }
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<SimpleFacetResult> results = facets.getAllDims(10);
-    assertTrue(results.isEmpty());
-
-    try {
-      facets.getSpecificValue("a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    try {
-      facets.getTopChildren(10, "a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, taxoDir, dir);
-  }
-
-
-  // nocommit in the sparse case test that we are really
-  // sorting by the correct dim count
-
-  public void testReallyNoNormsForDrillDown() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setSimilarity(new PerFieldSimilarityWrapper() {
-        final Similarity sim = new DefaultSimilarity();
-
-        @Override
-        public Similarity get(String name) {
-          assertEquals("field", name);
-          return sim;
-        }
-      });
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("a", "path"));
-    writer.addDocument(config.build(doc));
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-
-  public void testMultiValuedHierarchy() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setHierarchical("a", true);
-    config.setMultiValued("a", true);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("a", "path", "x"));
-    doc.add(new FacetField("a", "path", "y"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    // Aggregate the facet counts:
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
-    SimpleFacetResult result = facets.getTopChildren(10, "a");
-    assertEquals(1, result.labelValues.length);
-    assertEquals(1, result.labelValues[0].value.intValue());
-
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-
-  public void testLabelWithDelimiter() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setMultiValued("dim", true);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("dim", "test\u001Fone"));
-    doc.add(new FacetField("dim", "test\u001Etwo"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);
-    
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
-    assertEquals(1, facets.getSpecificValue("dim", "test\u001Fone"));
-    assertEquals(1, facets.getSpecificValue("dim", "test\u001Etwo"));
-
-    SimpleFacetResult result = facets.getTopChildren(10, "dim");
-    assertEquals("value=-1 childCount=2\n  test\u001Fone (1)\n  test\u001Etwo (1)\n", result.toString());
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-
-  public void testRequireDimCount() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setMultiValued("dim2", true);
-    config.setMultiValued("dim3", true);
-    config.setHierarchical("dim3", true);
-    config.setRequireDimCount("dim", true);
-    config.setRequireDimCount("dim2", true);
-    config.setRequireDimCount("dim3", true);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("dim", "a"));
-    doc.add(new FacetField("dim2", "a"));
-    doc.add(new FacetField("dim2", "b"));
-    doc.add(new FacetField("dim3", "a", "b"));
-    doc.add(new FacetField("dim3", "a", "c"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);
-    
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
-    assertEquals(1, facets.getTopChildren(10, "dim").value);
-    assertEquals(1, facets.getTopChildren(10, "dim2").value);
-    assertEquals(1, facets.getTopChildren(10, "dim3").value);
-    IOUtils.close(writer, taxoWriter, searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-
-  // LUCENE-4583: make sure if we require > 32 KB for one
-  // document, we don't hit exc when using Facet42DocValuesFormat
-  public void testManyFacetsInOneDocument() throws Exception {
-    assumeTrue("default Codec doesn't support huge BinaryDocValues", _TestUtil.fieldSupportsHugeBinaryDocValues(FacetsConfig.DEFAULT_INDEX_FIELD_NAME));
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, iwc);
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setMultiValued("dim", true);
-    
-    int numLabels = _TestUtil.nextInt(random(), 40000, 100000);
-    
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    for (int i = 0; i < numLabels; i++) {
-      doc.add(new FacetField("dim", "" + i));
-    }
-    writer.addDocument(config.build(doc));
-    
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    // Aggregate the facet counts:
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, c);
-
-    SimpleFacetResult result = facets.getTopChildren(Integer.MAX_VALUE, "dim");
-    assertEquals(numLabels, result.labelValues.length);
-    Set<String> allLabels = new HashSet<String>();
-    for (LabelAndValue labelValue : result.labelValues) {
-      allLabels.add(labelValue.label);
-      assertEquals(1, labelValue.value.intValue());
-    }
-    assertEquals(numLabels, allLabels.size());
-    
-    IOUtils.close(searcher.getIndexReader(), taxoWriter, writer, taxoReader, dir, taxoDir);
-  }
-
-  // Make sure we catch when app didn't declare field as
-  // hierarchical but it was:
-  public void testDetectHierarchicalField() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("a", "path", "other"));
-    try {
-      config.build(doc);
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-
-  // Make sure we catch when app didn't declare field as
-  // multi-valued but it was:
-  public void testDetectMultiValuedField() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-
-    Document doc = new Document();
-    doc.add(newTextField("field", "text", Field.Store.NO));
-    doc.add(new FacetField("a", "path"));
-    doc.add(new FacetField("a", "path2"));
-    try {
-      config.build(doc);
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    IOUtils.close(writer, taxoWriter, dir, taxoDir);
-  }
-
-  public void testSeparateIndexedFields() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setIndexFieldName("b", "$b");
-    
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      doc.add(new StringField("f", "v", Field.Store.NO));
-      doc.add(new FacetField("a", "1"));
-      doc.add(new FacetField("b", "1"));
-      iw.addDocument(config.build(doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-    Facets facets1 = getTaxonomyFacetCounts(taxoReader, config, sfc);
-    Facets facets2 = getTaxonomyFacetCounts(taxoReader, config, sfc, "$b");
-    assertEquals(r.maxDoc(), facets1.getTopChildren(10, "a").value.intValue());
-    assertEquals(r.maxDoc(), facets2.getTopChildren(10, "b").value.intValue());
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-  
-  public void testCountRoot() throws Exception {
-    // LUCENE-4882: FacetsAccumulator threw NPE if a FacetRequest was defined on CP.EMPTY
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      doc.add(new FacetField("a", "1"));
-      doc.add(new FacetField("b", "1"));
-      iw.addDocument(config.build(doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
-    for (SimpleFacetResult result : facets.getAllDims(10)) {
-      assertEquals(r.numDocs(), result.value.intValue());
-    }
-    
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  public void testGetFacetResultsTwice() throws Exception {
-    // LUCENE-4893: counts were multiplied as many times as getFacetResults was called.
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-
-    Document doc = new Document();
-    doc.add(new FacetField("a", "1"));
-    doc.add(new FacetField("b", "1"));
-    iw.addDocument(config.build(doc));
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    final SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
-    List<SimpleFacetResult> res1 = facets.getAllDims(10);
-    List<SimpleFacetResult> res2 = facets.getAllDims(10);
-    assertEquals("calling getFacetResults twice should return the .equals()=true result", res1, res2);
-    
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-  
-  public void testChildCount() throws Exception {
-    // LUCENE-4885: FacetResult.numValidDescendants was not set properly by FacetsAccumulator
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    for (int i = 0; i < 10; i++) {
-      Document doc = new Document();
-      doc.add(new FacetField("a", Integer.toString(i)));
-      iw.addDocument(config.build(doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
-    
-    assertEquals(10, facets.getTopChildren(2, "a").childCount);
-
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  private void indexTwoDocs(IndexWriter indexWriter, FacetsConfig config, boolean withContent) throws Exception {
-    for (int i = 0; i < 2; i++) {
-      Document doc = new Document();
-      if (withContent) {
-        doc.add(new StringField("f", "a", Field.Store.NO));
-      }
-      if (config != null) {
-        doc.add(new FacetField("A", Integer.toString(i)));
-        indexWriter.addDocument(config.build(doc));
-      } else {
-        indexWriter.addDocument(doc);
-      }
-    }
-    
-    indexWriter.commit();
-  }
-  
-  public void testSegmentsWithoutCategoriesOrResults() throws Exception {
-    // tests the accumulator when there are segments with no results
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges
-    IndexWriter indexWriter = new IndexWriter(indexDir, iwc);
-
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    indexTwoDocs(indexWriter, config, false); // 1st segment, no content, with categories
-    indexTwoDocs(indexWriter, null, true);         // 2nd segment, with content, no categories
-    indexTwoDocs(indexWriter, config, true);  // 3rd segment ok
-    indexTwoDocs(indexWriter, null, false);        // 4th segment, no content, or categories
-    indexTwoDocs(indexWriter, null, true);         // 5th segment, with content, no categories
-    indexTwoDocs(indexWriter, config, true);  // 6th segment, with content, with categories
-    indexTwoDocs(indexWriter, null, true);         // 7th segment, with content, no categories
-    IOUtils.close(indexWriter, taxoWriter);
-
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher indexSearcher = newSearcher(indexReader);
-    
-    // search for "f:a", only segments 1 and 3 should match results
-    Query q = new TermQuery(new Term("f", "a"));
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    indexSearcher.search(q, sfc);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, config, sfc);
-    SimpleFacetResult result = facets.getTopChildren(10, "A");
-    assertEquals("wrong number of children", 2, result.labelValues.length);
-    for (LabelAndValue labelValue : result.labelValues) {
-      assertEquals("wrong weight for child " + labelValue.label, 2, labelValue.value.intValue());
-    }
-
-    IOUtils.close(indexReader, taxoReader, indexDir, taxoDir);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts2.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts2.java
deleted file mode 100644
index b45c588..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetCounts2.java
+++ /dev/null
@@ -1,373 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-public class TestTaxonomyFacetCounts2 extends FacetTestCase {
-  
-  private static final Term A = new Term("f", "a");
-  private static final String CP_A = "A", CP_B = "B";
-  private static final String CP_C = "C", CP_D = "D"; // indexed w/ NO_PARENTS
-  private static final int NUM_CHILDREN_CP_A = 5, NUM_CHILDREN_CP_B = 3;
-  private static final int NUM_CHILDREN_CP_C = 5, NUM_CHILDREN_CP_D = 5;
-  private static final FacetField[] CATEGORIES_A, CATEGORIES_B;
-  private static final FacetField[] CATEGORIES_C, CATEGORIES_D;
-  static {
-    CATEGORIES_A = new FacetField[NUM_CHILDREN_CP_A];
-    for (int i = 0; i < NUM_CHILDREN_CP_A; i++) {
-      CATEGORIES_A[i] = new FacetField(CP_A, Integer.toString(i));
-    }
-    CATEGORIES_B = new FacetField[NUM_CHILDREN_CP_B];
-    for (int i = 0; i < NUM_CHILDREN_CP_B; i++) {
-      CATEGORIES_B[i] = new FacetField(CP_B, Integer.toString(i));
-    }
-    
-    // NO_PARENTS categories
-    CATEGORIES_C = new FacetField[NUM_CHILDREN_CP_C];
-    for (int i = 0; i < NUM_CHILDREN_CP_C; i++) {
-      CATEGORIES_C[i] = new FacetField(CP_C, Integer.toString(i));
-    }
-    
-    // Multi-level categories
-    CATEGORIES_D = new FacetField[NUM_CHILDREN_CP_D];
-    for (int i = 0; i < NUM_CHILDREN_CP_D; i++) {
-      String val = Integer.toString(i);
-      CATEGORIES_D[i] = new FacetField(CP_D, val, val + val); // e.g. D/1/11, D/2/22...
-    }
-  }
-  
-  private static Directory indexDir, taxoDir;
-  private static Map<String,Integer> allExpectedCounts, termExpectedCounts;
-
-  @AfterClass
-  public static void afterClassCountingFacetsAggregatorTest() throws Exception {
-    IOUtils.close(indexDir, taxoDir); 
-  }
-  
-  private static List<FacetField> randomCategories(Random random) {
-    // add random categories from the two dimensions, ensuring that the same
-    // category is not added twice.
-    int numFacetsA = random.nextInt(3) + 1; // 1-3
-    int numFacetsB = random.nextInt(2) + 1; // 1-2
-    ArrayList<FacetField> categories_a = new ArrayList<FacetField>();
-    categories_a.addAll(Arrays.asList(CATEGORIES_A));
-    ArrayList<FacetField> categories_b = new ArrayList<FacetField>();
-    categories_b.addAll(Arrays.asList(CATEGORIES_B));
-    Collections.shuffle(categories_a, random);
-    Collections.shuffle(categories_b, random);
-
-    ArrayList<FacetField> categories = new ArrayList<FacetField>();
-    categories.addAll(categories_a.subList(0, numFacetsA));
-    categories.addAll(categories_b.subList(0, numFacetsB));
-    
-    // add the NO_PARENT categories
-    categories.add(CATEGORIES_C[random().nextInt(NUM_CHILDREN_CP_C)]);
-    categories.add(CATEGORIES_D[random().nextInt(NUM_CHILDREN_CP_D)]);
-
-    return categories;
-  }
-
-  private static void addField(Document doc) {
-    doc.add(new StringField(A.field(), A.text(), Store.NO));
-  }
-
-  private static void addFacets(Document doc, FacetsConfig config, boolean updateTermExpectedCounts) 
-      throws IOException {
-    List<FacetField> docCategories = randomCategories(random());
-    for (FacetField ff : docCategories) {
-      doc.add(ff);
-      String cp = ff.dim + "/" + ff.path[0];
-      allExpectedCounts.put(cp, allExpectedCounts.get(cp) + 1);
-      if (updateTermExpectedCounts) {
-        termExpectedCounts.put(cp, termExpectedCounts.get(cp) + 1);
-      }
-    }
-    // add 1 to each NO_PARENTS dimension
-    allExpectedCounts.put(CP_B, allExpectedCounts.get(CP_B) + 1);
-    allExpectedCounts.put(CP_C, allExpectedCounts.get(CP_C) + 1);
-    allExpectedCounts.put(CP_D, allExpectedCounts.get(CP_D) + 1);
-    if (updateTermExpectedCounts) {
-      termExpectedCounts.put(CP_B, termExpectedCounts.get(CP_B) + 1);
-      termExpectedCounts.put(CP_C, termExpectedCounts.get(CP_C) + 1);
-      termExpectedCounts.put(CP_D, termExpectedCounts.get(CP_D) + 1);
-    }
-  }
-
-  private static FacetsConfig getConfig(TaxonomyWriter taxoWriter) {
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setMultiValued("A", true);
-    config.setMultiValued("B", true);
-    config.setRequireDimCount("B", true);
-    config.setHierarchical("D", true);
-    return config;
-  }
-
-  private static FacetsConfig getConfig() {
-    return getConfig(null);
-  }
-  
-  private static void indexDocsNoFacets(IndexWriter indexWriter) throws IOException {
-    int numDocs = atLeast(2);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      addField(doc);
-      indexWriter.addDocument(doc);
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  private static void indexDocsWithFacetsNoTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
-                                                 Map<String,Integer> expectedCounts) throws IOException {
-    Random random = random();
-    int numDocs = atLeast(random, 2);
-    FacetsConfig config = getConfig(taxoWriter);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      addFacets(doc, config, false);
-      indexWriter.addDocument(config.build(doc));
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  private static void indexDocsWithFacetsAndTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
-                                                  Map<String,Integer> expectedCounts) throws IOException {
-    Random random = random();
-    int numDocs = atLeast(random, 2);
-    FacetsConfig config = getConfig(taxoWriter);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      addFacets(doc, config, true);
-      addField(doc);
-      indexWriter.addDocument(config.build(doc));
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  private static void indexDocsWithFacetsAndSomeTerms(IndexWriter indexWriter, TaxonomyWriter taxoWriter, 
-                                                      Map<String,Integer> expectedCounts) throws IOException {
-    Random random = random();
-    int numDocs = atLeast(random, 2);
-    FacetsConfig config = getConfig(taxoWriter);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      boolean hasContent = random.nextBoolean();
-      if (hasContent) {
-        addField(doc);
-      }
-      addFacets(doc, config, hasContent);
-      indexWriter.addDocument(config.build(doc));
-    }
-    indexWriter.commit(); // flush a segment
-  }
-  
-  // initialize expectedCounts w/ 0 for all categories
-  private static Map<String,Integer> newCounts() {
-    Map<String,Integer> counts = new HashMap<String,Integer>();
-    counts.put(CP_A, 0);
-    counts.put(CP_B, 0);
-    counts.put(CP_C, 0);
-    counts.put(CP_D, 0);
-    for (FacetField ff : CATEGORIES_A) {
-      counts.put(ff.dim + "/" + ff.path[0], 0);
-    }
-    for (FacetField ff : CATEGORIES_B) {
-      counts.put(ff.dim + "/" + ff.path[0], 0);
-    }
-    for (FacetField ff : CATEGORIES_C) {
-      counts.put(ff.dim + "/" + ff.path[0], 0);
-    }
-    for (FacetField ff : CATEGORIES_D) {
-      counts.put(ff.dim + "/" + ff.path[0], 0);
-    }
-    return counts;
-  }
-  
-  @BeforeClass
-  public static void beforeClassCountingFacetsAggregatorTest() throws Exception {
-    indexDir = newDirectory();
-    taxoDir = newDirectory();
-    
-    // create an index which has:
-    // 1. Segment with no categories, but matching results
-    // 2. Segment w/ categories, but no results
-    // 3. Segment w/ categories and results
-    // 4. Segment w/ categories, but only some results
-    
-    IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // prevent merges, so we can control the index segments
-    IndexWriter indexWriter = new IndexWriter(indexDir, conf);
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-
-    allExpectedCounts = newCounts();
-    termExpectedCounts = newCounts();
-    
-    // segment w/ no categories
-    indexDocsNoFacets(indexWriter);
-
-    // segment w/ categories, no content
-    indexDocsWithFacetsNoTerms(indexWriter, taxoWriter, allExpectedCounts);
-
-    // segment w/ categories and content
-    indexDocsWithFacetsAndTerms(indexWriter, taxoWriter, allExpectedCounts);
-    
-    // segment w/ categories and some content
-    indexDocsWithFacetsAndSomeTerms(indexWriter, taxoWriter, allExpectedCounts);
-    
-    IOUtils.close(indexWriter, taxoWriter);
-  }
-  
-  @Test
-  public void testDifferentNumResults() throws Exception {
-    // test the collector w/ FacetRequests and different numResults
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    TermQuery q = new TermQuery(A);
-    searcher.search(q, sfc);
-    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
-    SimpleFacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_A, CP_A);
-    assertEquals(-1, result.value.intValue());
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(termExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
-    }
-    result = facets.getTopChildren(NUM_CHILDREN_CP_B, CP_B);
-    assertEquals(termExpectedCounts.get(CP_B), result.value);
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(termExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  @Test
-  public void testAllCounts() throws Exception {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), sfc);
-
-    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
-    
-    SimpleFacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_A, CP_A);
-    assertEquals(-1, result.value.intValue());
-    int prevValue = Integer.MAX_VALUE;
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
-      assertTrue("wrong sort order of sub results: labelValue.value=" + labelValue.value + " prevValue=" + prevValue, labelValue.value.intValue() <= prevValue);
-      prevValue = labelValue.value.intValue();
-    }
-
-    result = facets.getTopChildren(NUM_CHILDREN_CP_B, CP_B);
-    assertEquals(allExpectedCounts.get(CP_B), result.value);
-    prevValue = Integer.MAX_VALUE;
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
-      assertTrue("wrong sort order of sub results: labelValue.value=" + labelValue.value + " prevValue=" + prevValue, labelValue.value.intValue() <= prevValue);
-      prevValue = labelValue.value.intValue();
-    }
-
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  @Test
-  public void testBigNumResults() throws Exception {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), sfc);
-
-    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
-
-    SimpleFacetResult result = facets.getTopChildren(Integer.MAX_VALUE, CP_A);
-    assertEquals(-1, result.value.intValue());
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_A + "/" + labelValue.label), labelValue.value);
-    }
-    result = facets.getTopChildren(Integer.MAX_VALUE, CP_B);
-    assertEquals(allExpectedCounts.get(CP_B), result.value);
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_B + "/" + labelValue.label), labelValue.value);
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  @Test
-  public void testNoParents() throws Exception {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), sfc);
-
-    Facets facets = getTaxonomyFacetCounts(taxoReader, getConfig(), sfc);
-
-    SimpleFacetResult result = facets.getTopChildren(NUM_CHILDREN_CP_C, CP_C);
-    assertEquals(allExpectedCounts.get(CP_C), result.value);
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_C + "/" + labelValue.label), labelValue.value);
-    }
-    result = facets.getTopChildren(NUM_CHILDREN_CP_D, CP_D);
-    assertEquals(allExpectedCounts.get(CP_C), result.value);
-    for(LabelAndValue labelValue : result.labelValues) {
-      assertEquals(allExpectedCounts.get(CP_D + "/" + labelValue.label), labelValue.value);
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetSumValueSource.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetSumValueSource.java
deleted file mode 100644
index 709d0a4..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacetSumValueSource.java
+++ /dev/null
@@ -1,434 +0,0 @@
-package org.apache.lucene.facet.simple;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.IntField;
-import org.apache.lucene.document.NumericDocValuesField;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.PrintTaxonomyStats;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.queries.function.FunctionQuery;
-import org.apache.lucene.queries.function.FunctionValues;
-import org.apache.lucene.queries.function.ValueSource;
-import org.apache.lucene.queries.function.docvalues.DoubleDocValues;
-import org.apache.lucene.queries.function.valuesource.IntFieldSource;
-import org.apache.lucene.queries.function.valuesource.LongFieldSource;
-import org.apache.lucene.queries.function.valuesource.QueryValueSource;
-import org.apache.lucene.search.ConstantScoreQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.MultiCollector;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Scorer;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.search.TopScoreDocCollector;
-import org.apache.lucene.search.similarities.DefaultSimilarity;
-import org.apache.lucene.search.similarities.PerFieldSimilarityWrapper;
-import org.apache.lucene.search.similarities.Similarity;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-
-public class TestTaxonomyFacetSumValueSource extends FacetTestCase {
-
-  public void testBasic() throws Exception {
-
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-
-    // Reused across documents, to add the necessary facet
-    // fields:
-    Document doc = new Document();
-    doc.add(new IntField("num", 10, Field.Store.NO));
-    doc.add(new FacetField("Author", "Bob"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new IntField("num", 20, Field.Store.NO));
-    doc.add(new FacetField("Author", "Lisa"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new IntField("num", 30, Field.Store.NO));
-    doc.add(new FacetField("Author", "Lisa"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new IntField("num", 40, Field.Store.NO));
-    doc.add(new FacetField("Author", "Susan"));
-    writer.addDocument(config.build(doc));
-
-    doc = new Document();
-    doc.add(new IntField("num", 45, Field.Store.NO));
-    doc.add(new FacetField("Author", "Frank"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    // Aggregate the facet counts:
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-
-    // MatchAllDocsQuery is for "browsing" (counts facets
-    // for all non-deleted docs in the index); normally
-    // you'd use a "normal" query, and use MultiCollector to
-    // wrap collecting the "normal" hits and also facets:
-    searcher.search(new MatchAllDocsQuery(), c);
-
-    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
-
-    // Retrieve & verify results:
-    assertEquals("value=145.0 childCount=4\n  Lisa (50.0)\n  Frank (45.0)\n  Susan (40.0)\n  Bob (10.0)\n", facets.getTopChildren(10, "Author").toString());
-
-    taxoReader.close();
-    searcher.getIndexReader().close();
-    dir.close();
-    taxoDir.close();
-  }
-
-  // LUCENE-5333
-  public void testSparseFacets() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-
-    Document doc = new Document();
-    doc.add(new IntField("num", 10, Field.Store.NO));
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(config.build(doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new IntField("num", 20, Field.Store.NO));
-    doc.add(new FacetField("a", "foo2"));
-    doc.add(new FacetField("b", "bar1"));
-    writer.addDocument(config.build(doc));
-
-    if (random().nextBoolean()) {
-      writer.commit();
-    }
-
-    doc = new Document();
-    doc.add(new IntField("num", 30, Field.Store.NO));
-    doc.add(new FacetField("a", "foo3"));
-    doc.add(new FacetField("b", "bar2"));
-    doc.add(new FacetField("c", "baz1"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, new FacetsConfig(), c, new IntFieldSource("num"));
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<SimpleFacetResult> results = facets.getAllDims(10);
-
-    assertEquals(3, results.size());
-    assertEquals("value=60.0 childCount=3\n  foo3 (30.0)\n  foo2 (20.0)\n  foo1 (10.0)\n", results.get(0).toString());
-    assertEquals("value=50.0 childCount=2\n  bar2 (30.0)\n  bar1 (20.0)\n", results.get(1).toString());
-    assertEquals("value=30.0 childCount=1\n  baz1 (30.0)\n", results.get(2).toString());
-
-    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-
-  public void testWrongIndexFieldName() throws Exception {
-
-    Directory dir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Writes facet ords to a separate directory from the
-    // main index:
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
-
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setIndexFieldName("a", "$facets2");
-
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);
-
-    Document doc = new Document();
-    doc.add(new IntField("num", 10, Field.Store.NO));
-    doc.add(new FacetField("a", "foo1"));
-    writer.addDocument(config.build(doc));
-
-    // NRT open
-    IndexSearcher searcher = newSearcher(writer.getReader());
-    writer.close();
-
-    // NRT open
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    taxoWriter.close();
-
-    SimpleFacetsCollector c = new SimpleFacetsCollector();
-    searcher.search(new MatchAllDocsQuery(), c);    
-
-    TaxonomyFacetSumValueSource facets = new TaxonomyFacetSumValueSource(taxoReader, config, c, new IntFieldSource("num"));
-
-    // Ask for top 10 labels for any dims that have counts:
-    List<SimpleFacetResult> results = facets.getAllDims(10);
-    assertTrue(results.isEmpty());
-
-    try {
-      facets.getSpecificValue("a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    try {
-      facets.getTopChildren(10, "a");
-      fail("should have hit exc");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-
-    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
-  }
-
-  public void testSumScoreAggregator() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      if (random().nextBoolean()) { // don't match all documents
-        doc.add(new StringField("f", "v", Field.Store.NO));
-      }
-      doc.add(new FacetField("dim", "a"));
-      iw.addDocument(config.build(doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    SimpleFacetsCollector fc = new SimpleFacetsCollector(true);
-    TopScoreDocCollector topDocs = TopScoreDocCollector.create(10, false);
-    ConstantScoreQuery csq = new ConstantScoreQuery(new MatchAllDocsQuery());
-    csq.setBoost(2.0f);
-    
-    newSearcher(r).search(csq, MultiCollector.wrap(fc, topDocs));
-
-    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, fc, new TaxonomyFacetSumValueSource.ScoreValueSource());
-    
-    TopDocs td = topDocs.topDocs();
-    int expected = (int) (td.getMaxScore() * td.totalHits);
-    assertEquals(expected, facets.getSpecificValue("dim", "a").intValue());
-    
-    IOUtils.close(iw, taxoWriter, taxoReader, taxoDir, r, indexDir);
-  }
-  
-  public void testNoScore() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    for (int i = 0; i < 4; i++) {
-      Document doc = new Document();
-      doc.add(new NumericDocValuesField("price", (i+1)));
-      doc.add(new FacetField("a", Integer.toString(i % 2)));
-      iw.addDocument(config.build(doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, new LongFieldSource("price"));
-    assertEquals("value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
-    
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  public void testWithScore() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    for (int i = 0; i < 4; i++) {
-      Document doc = new Document();
-      doc.add(new NumericDocValuesField("price", (i+1)));
-      doc.add(new FacetField("a", Integer.toString(i % 2)));
-      iw.addDocument(config.build(doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    ValueSource valueSource = new ValueSource() {
-      @Override
-      public FunctionValues getValues(@SuppressWarnings("rawtypes") Map context, AtomicReaderContext readerContext) throws IOException {
-        final Scorer scorer = (Scorer) context.get("scorer");
-        assert scorer != null;
-        return new DoubleDocValues(this) {
-          @Override
-          public double doubleVal(int document) {
-            try {
-              return scorer.score();
-            } catch (IOException exception) {
-              throw new RuntimeException(exception);
-            }
-          }
-        };
-      }
-
-      @Override public boolean equals(Object o) { return o == this; }
-      @Override public int hashCode() { return System.identityHashCode(this); }
-      @Override public String description() { return "score()"; }
-    };
-    
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector(true);
-    TopScoreDocCollector tsdc = TopScoreDocCollector.create(10, true);
-    // score documents by their 'price' field - makes asserting the correct counts for the categories easier
-    Query q = new FunctionQuery(new LongFieldSource("price"));
-    newSearcher(r).search(q, MultiCollector.wrap(tsdc, sfc));
-    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, valueSource);
-    
-    assertEquals("value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
-    
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  public void testRollupValues() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setHierarchical("a", true);
-    //config.setRequireDimCount("a", true);
-    
-    for (int i = 0; i < 4; i++) {
-      Document doc = new Document();
-      doc.add(new NumericDocValuesField("price", (i+1)));
-      doc.add(new FacetField("a", Integer.toString(i % 2), "1"));
-      iw.addDocument(config.build(doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-
-    ValueSource valueSource = new LongFieldSource("price");
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector();
-    newSearcher(r).search(new MatchAllDocsQuery(), sfc);
-    Facets facets = new TaxonomyFacetSumValueSource(taxoReader, config, sfc, valueSource);
-    
-    assertEquals("value=10.0 childCount=2\n  1 (6.0)\n  0 (4.0)\n", facets.getTopChildren(10, "a").toString());
-    
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  public void testCountAndSumScore() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    DirectoryTaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    IndexWriter iw = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())));
-    FacetsConfig config = new FacetsConfig(taxoWriter);
-    config.setIndexFieldName("b", "$b");
-    
-    for(int i = atLeast(30); i > 0; --i) {
-      Document doc = new Document();
-      doc.add(new StringField("f", "v", Field.Store.NO));
-      doc.add(new FacetField("a", "1"));
-      doc.add(new FacetField("b", "1"));
-      iw.addDocument(config.build(doc));
-    }
-    
-    DirectoryReader r = DirectoryReader.open(iw, true);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
-    
-    SimpleFacetsCollector sfc = new SimpleFacetsCollector(true);
-    TopScoreDocCollector topDocs = TopScoreDocCollector.create(10, false);
-    newSearcher(r).search(new MatchAllDocsQuery(), MultiCollector.wrap(sfc, topDocs));
-    
-    Facets facets1 = getTaxonomyFacetCounts(taxoReader, config, sfc);
-    Facets facets2 = new TaxonomyFacetSumValueSource(new DocValuesOrdinalsReader("$b"), taxoReader, config, sfc, new TaxonomyFacetSumValueSource.ScoreValueSource());
-
-    assertEquals(r.maxDoc(), facets1.getTopChildren(10, "a").value.intValue());
-    double expected = topDocs.topDocs().getMaxScore() * r.numDocs();
-    assertEquals(r.maxDoc(), facets2.getTopChildren(10, "b").value.doubleValue(), 1E-10);
-    IOUtils.close(taxoWriter, iw, taxoReader, taxoDir, r, indexDir);
-  }
-
-  // nocommit in the sparse case test that we are really
-  // sorting by the correct dim count
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java
index 4d1865c..70f572b 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java
@@ -8,9 +8,9 @@ import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetField;
 import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.simple.FacetField;
-import org.apache.lucene.facet.simple.FacetsConfig;
+import org.apache.lucene.facet.FacetsConfig;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
 import org.apache.lucene.facet.taxonomy.writercache.cl2o.Cl2oTaxonomyWriterCache;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
index 80b176e..e5f8848 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
@@ -10,10 +10,10 @@ import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.analysis.MockAnalyzer;
 import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetField;
 import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.simple.FacetField;
-import org.apache.lucene.facet.simple.FacetsConfig;
-import org.apache.lucene.facet.simple.SimpleDrillDownQuery;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.DrillDownQuery;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
@@ -459,7 +459,7 @@ public class TestDirectoryTaxonomyWriter extends FacetTestCase {
     DirectoryReader indexReader = DirectoryReader.open(indexDir);
     TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
     IndexSearcher searcher = new IndexSearcher(indexReader);
-    SimpleDrillDownQuery ddq = new SimpleDrillDownQuery(new FacetsConfig());
+    DrillDownQuery ddq = new DrillDownQuery(new FacetsConfig());
     ddq.add("dim", bigs);
     assertEquals(1, searcher.search(ddq, 10).totalHits);
     
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java b/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java
index b3758a5..c54fc16 100644
--- a/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java
+++ b/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyReplicationClientTest.java
@@ -26,18 +26,19 @@ import java.util.concurrent.Callable;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.params.FacetIndexingParams;
-import org.apache.lucene.facet.params.FacetSearchParams;
-import org.apache.lucene.facet.search.CountFacetRequest;
-import org.apache.lucene.facet.search.DrillDownQuery;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.DrillDownQuery;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.FastTaxonomyFacetCounts;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexDocument;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.SnapshotDeletionPolicy;
@@ -63,11 +64,14 @@ public class IndexAndTaxonomyReplicationClientTest extends ReplicatorTestCase {
     private final Directory indexDir, taxoDir;
     private DirectoryReader indexReader;
     private DirectoryTaxonomyReader taxoReader;
+    private FacetsConfig config;
     private long lastIndexGeneration = -1;
     
     public IndexAndTaxonomyReadyCallback(Directory indexDir, Directory taxoDir) throws IOException {
       this.indexDir = indexDir;
       this.taxoDir = taxoDir;
+      config = new FacetsConfig();
+      config.setHierarchical("A", true);
       if (DirectoryReader.indexExists(indexDir)) {
         indexReader = DirectoryReader.open(indexDir);
         lastIndexGeneration = indexReader.getIndexCommit().getGeneration();
@@ -102,14 +106,14 @@ public class IndexAndTaxonomyReplicationClientTest extends ReplicatorTestCase {
         
         // verify faceted search
         int id = Integer.parseInt(indexReader.getIndexCommit().getUserData().get(VERSION_ID), 16);
-        FacetLabel cp = new FacetLabel("A", Integer.toString(id, 16));
         IndexSearcher searcher = new IndexSearcher(indexReader);
-        FacetsCollector fc = FacetsCollector.create(new FacetSearchParams(new CountFacetRequest(cp, 10)), indexReader, taxoReader);
+        FacetsCollector fc = new FacetsCollector();
         searcher.search(new MatchAllDocsQuery(), fc);
-        assertEquals(1, (int) fc.getFacetResults().get(0).getFacetResultNode().value);
+        Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, fc);
+        assertEquals(1, facets.getSpecificValue("A", Integer.toString(id, 16)).intValue());
         
-        DrillDownQuery drillDown = new DrillDownQuery(FacetIndexingParams.DEFAULT);
-        drillDown.add(cp);
+        DrillDownQuery drillDown = new DrillDownQuery(config);
+        drillDown.add("A", Integer.toString(id, 16));
         TopDocs docs = searcher.search(drillDown, 10);
         assertEquals(1, docs.totalHits);
       }
@@ -130,6 +134,7 @@ public class IndexAndTaxonomyReplicationClientTest extends ReplicatorTestCase {
   private ReplicationHandler handler;
   private IndexWriter publishIndexWriter;
   private SnapshotDirectoryTaxonomyWriter publishTaxoWriter;
+  private FacetsConfig config;
   private IndexAndTaxonomyReadyCallback callback;
   private File clientWorkDir;
   
@@ -175,11 +180,10 @@ public class IndexAndTaxonomyReplicationClientTest extends ReplicatorTestCase {
     return new IndexAndTaxonomyRevision(publishIndexWriter, publishTaxoWriter);
   }
   
-  private Document newDocument(TaxonomyWriter taxoWriter, int id) throws IOException {
+  private IndexDocument newDocument(TaxonomyWriter taxoWriter, int id) throws IOException {
     Document doc = new Document();
-    FacetFields facetFields = new FacetFields(taxoWriter);
-    facetFields.addFields(doc, Collections.singleton(new FacetLabel("A", Integer.toString(id, 16))));
-    return doc;
+    doc.add(new FacetField("A", Integer.toString(id, 16)));
+    return config.build(doc);
   }
   
   @Override
@@ -201,6 +205,8 @@ public class IndexAndTaxonomyReplicationClientTest extends ReplicatorTestCase {
     conf.setIndexDeletionPolicy(new SnapshotDeletionPolicy(conf.getIndexDeletionPolicy()));
     publishIndexWriter = new IndexWriter(publishIndexDir, conf);
     publishTaxoWriter = new SnapshotDirectoryTaxonomyWriter(publishTaxoDir);
+    config = new FacetsConfig(publishTaxoWriter);
+    config.setHierarchical("A", true);
   }
   
   @After
diff --git a/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyRevisionTest.java b/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyRevisionTest.java
index 2c3b227..c1f2de2 100644
--- a/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyRevisionTest.java
+++ b/lucene/replicator/src/test/org/apache/lucene/replicator/IndexAndTaxonomyRevisionTest.java
@@ -21,13 +21,15 @@ import java.io.IOException;
 import java.io.InputStream;
 import java.util.Collections;
 import java.util.List;
-import java.util.Map;
 import java.util.Map.Entry;
+import java.util.Map;
 
 import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetsConfig;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.index.IndexDocument;
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
@@ -41,11 +43,11 @@ import org.junit.Test;
 
 public class IndexAndTaxonomyRevisionTest extends ReplicatorTestCase {
   
-  private Document newDocument(TaxonomyWriter taxoWriter) throws IOException {
+  private IndexDocument newDocument(TaxonomyWriter taxoWriter) throws IOException {
+    FacetsConfig config = new FacetsConfig(taxoWriter);
     Document doc = new Document();
-    FacetFields ff = new FacetFields(taxoWriter);
-    ff.addFields(doc, Collections.singleton(new FacetLabel("A")));
-    return doc;
+    doc.add(new FacetField("A", "1"));
+    return config.build(doc);
   }
   
   @Test

